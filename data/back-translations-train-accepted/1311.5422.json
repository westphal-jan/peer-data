{"id": "1311.5422", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2013", "title": "Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis", "abstract": "Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multi- subject fMRI studies in which functional activity is classified using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso.", "histories": [["v1", "Wed, 20 Nov 2013 16:45:51 GMT  (448kb,D)", "https://arxiv.org/abs/1311.5422v1", "To appear in Advances in Neural Information Processing Systems, 2013"], ["v2", "Fri, 22 Nov 2013 04:49:54 GMT  (450kb,D)", "http://arxiv.org/abs/1311.5422v2", "To appear in Advances in Neural Information Processing Systems, 2013"]], "COMMENTS": "To appear in Advances in Neural Information Processing Systems, 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nikhil s rao", "christopher r cox", "robert d nowak", "timothy t rogers"], "accepted": true, "id": "1311.5422"}, "pdf": {"name": "1311.5422.pdf", "metadata": {"source": "CRF", "title": "Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis", "authors": ["Nikhil S. Rao", "Christopher R. Cox", "Robert D. Nowak", "Timothy T. Rogers"], "emails": ["nrao2@wisc.edu", "crcox@wisc.edu", "nowak@ece.wisc.edu", "ttrogers@wisc.edu"], "sections": [{"heading": "1 Introduction", "text": "The group lasso [21, 10] is, of course, suitable for this situation: when a feature is selected for a task, it is selected for all tasks. This may be too restrictive in many applications, and this motivates a less rigid approach to selecting multitask features. In other words, a feature that is useful for a task suggests that the subset to which it belongs may contain the features that are useful in other tasks (Figure 1), and that the features that are useful in a task may be similar, but not necessarily identical, to those that are most suitable for other tasks. In this paper, we introduce the sparse overlap between the individual tasks (SOSlasso), a convex program for recreating the saving patterns explained above."}, {"heading": "1.1 Sparse Overlapping Sets", "text": "This is achieved by breaking down the characteristics of each task into groups G1... GM, where M is equal but not identical for each task, and Gi is a set of characteristics that can be considered similar across tasks. Conceptually, SOSlasso first selects subsets that are most useful for all tasks, and then identifies a unique, sparse solution for each task based only on characteristics in the selected subsets. In the fMRI application discussed later, the subsets are simple clusters of adjacent spatial data points (voxels) in the brains of several subjects. Figure 1 shows an example of the patterns that typically occur in sparse multi-task learning applications, where rows refer to characteristics and columns that correspond to tasks. Previous work focused on restoring variables that exist within and across groups when the groups do not overlap [16], on the application of genetics, the climate, the timing handwriting, and [17] and so on."}, {"heading": "1.2 fMRI Applications", "text": "In psychological studies with fMRI, multiple participants are scanned while undergoing exactly the same experimental manipulations. Cognitive scientists are interested in identifying the activity patterns associated with different cognitive states and creating a model of activity that accurately predicts the cognitive state induced in novel studies. In these datasets, it is reasonable to expect that the same general brain areas respond to the manipulation in each participant. In short, the specific activity patterns in these regions vary, both because neural codes may vary depending on the participant [4] and because brains vary in size and shape, and neuroanatomy is only an approximate guide for locating relevant information between individuals. In short, a voxel useful for predicting in a participant suggests that the general anatomical neighborhood in which useful vowels can be found, but not the precise vowel while the logistic [19] and [15] are not applied in one of the macelic predictions."}, {"heading": "1.3 Organization", "text": "The rest of the paper is structured as follows: In Section 2, we outline the notations we will use and formalize the problem; we also present the SOSlasso regularizer; in Section 3, we derive certain key characteristics of the regularizer; in Section 4, we specialize in linear regression adjustment (2) and derive consistency rates for the same beneficial ideas [11]; in Section 5, we outline experiments performed with simulated data. In this section, we also perform logistical regressions on fMRI data and argue that the use of the SOSlasso in comparison with Glasso and Lasso leads to interpretable multivariate solutions."}, {"heading": "2 Sparse Overlapping Sets Lasso", "text": "We assume that there is a vector x? t, so that the measurements obtained from the form yt = p? t + p? p p? p p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p p? p? p? p? p p p? p? p? p? p? p p p? p? p p? p p p p? p p p p? p? p p? p? p p? p p p? p p? p p p p? p? p p p p p? p? p p p? p p p? p p p? p p p p p? p p? p p p? p? p p? p p? p p p? p? p p p p? p? p p p p? p p p p p? p p? p p p? p p? p p? p p? p p p p p? p p p p? p p? p p p p? p p p p? p p p p p? p p p p? p? p? p p p p p p? p p? p? p? p p p p? p p? p p? p p p p p? p? p p p? p? p p p p p p p? p p p p p p? p p p p p p p? p p p p p? p? p p p p p? p p p? p"}, {"heading": "3 Error Bounds for SOSlasso with General Loss Functions", "text": "We derive certain key characteristics of the G group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: G-group: group: G-group: G-group: G-group: group: G-group: G-group: group: G-group: G-group: G-group: group: G-group: G-group: G-group: G-group: G-group: group: G-group: group: G-group: G-group: group: G-group: G-group: G-group:"}, {"heading": "4 Consistency of SOSlasso with Squared Error Loss", "text": "We must first include the double norm of the course of the loss function, so that we must adhere to the double norm of the course of the loss function. (2) Let us consider L: = Lp: (2) = 12n (2) = 12n (2).Our goal now is to find an upper limit for the quantity h (2), which ranges from (4) = 1np (2) = 1np (2) = 12n (2).G: 1 (2).G: 1 (2).G: 1 (2) = 12n (2).G: 2 (2).G: 2 (2).G: 2 (2).G: 2 (2).G: 2 (2).G: 2 (2).G: 2 (2): G (2): (G).G (2):.G (2).G (2): (2).G (2)."}, {"heading": "5 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Synthetic data, Gaussian Linear Regression", "text": "For T = 20 tasks, we define a N = 2002 element vector divided into M = 500 groups of size B = 6. Each group overlaps with its neighboring groups (G1 = {1, 2,.., 6}, G2 = {5, 6,.., 10}, G3 = {9, 10,.., 14},..). 20 of these groups were randomly activated and settled according to a uniform [\u2212 1, 1] distribution. A fraction \u03b1 of these coefficients with the highest magnitude was retained as a true signal. For each task, we get 250 linear measurements using a N (0, 1250I) matrix. Subsequently, we corrupt each measurement with additive white Gaussian noise (AWGN) and evaluate the signal recovery with respect to the mean square error (MSE). The regulation parameter was selected clairvoyantly to minimize the MSE value over a range of parameters. The results of these b measurements are distributed so that the results of a group of 12 are different (ctub)."}, {"heading": "5.2 The SOSlasso for fMRI", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "6 Conclusions and Extensions", "text": "We introduced SOSlasso, a feature that restores splinter patterns that are a mixture of overlapping group-sparse and sparse patterns when used as a regulator in convex programs, and proves their theoretical convergence rates in minimizing smallest squares. SOSlasso is successful in multitascale fMRI analysis, where it both allows for better conclusions and discovers theoretically more plausible brain regions such as lasso and glass. Future work includes experimenting with different parameters for the group and l1 penalties, as well as using other similarity groups, such as functional connectivity in fMRI.2The irregular group size compensates for voxels being larger and scanner coverage being smaller in the z dimension (only 8 sections compared to 64 in the x and y dimensions)."}, {"heading": "7 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Proofs of Lemmas and other Results", "text": "At this point we will sketch evidence for lemmas and results, which we have put aside in the main work. Before we prove the results, we should remember that we assume definieh (x) = inf W-G-G (\u03b1G-wG-wG-wG-wG-1) s.t. \u2211 G-G-wG = xAs in the work, \u03b1G = 1-G-G-G."}, {"heading": "7.1.1 Proof of Lemma 3.1", "text": "Evidence It is trivial to show that h (x) \u2265 0 with equality iff x = 0. We now show positive homogeneity. Let us assume, prejudice, G-G is an optimal decomposition of x, and let \u03b3-G-G-MLA = x-G-G-MLA-MLA-MLA-MLA. This leads to the following series of inequalities: h (x) = \u2211 G-G-G-MLA-1 = 1-G-G-MLA-MLA-MLA-1) = 1-G-G-MLA-MLA-MLA-G (6) Now we have the assumption that vG-G-G-G-G-MLA-MLA-G is an optimal decomposition, the definition G-G-G-MLA-MLA-G-MLA-1 = 1-G-MLA-MLA-MLA-MLA-MLA-G-G (6) Now we have the assumption that vG-G-G-G-MLA-MLA-MLA-G is an optimal decomposition, the definition G-G-MLA-MLA-MLA-G-MLA-G-MLA-G-G-MLA-G-MLA-G-G-G-G-MLA-G-G-G-G-G-G-MLA-G-G-G-G-G-MLA-G-G-G-G-G-G-G-MLA-G-G-G-G-G-G-G, the definition G-G-G-G-G-G-G-MLA-G-G-G-MLA-G-G-G-G-G-G-G-G-G-MLA-G-G-G-G-G-G-G-G-G-G-G-G-MLA-G-G-G-G-G-G-G is an optimal decomposition, the definition G-MLA-G-G-MLA-G-G-MLA-G-G-G-G-MLA-G-G-G-MLA-G-G-G-G"}, {"heading": "7.1.2 Proof of Lemma 3.3", "text": "Note that the vectors wA and wB are the optimal decomposition factors, so that none of the pillars of the vectors wA overlap with those in wB. It follows that h (a) + h (b) = \u2211 G-G? (HAG + HAG-wBG-1) + \u2211 G-G (HAG-wBG-1) = \u2211 G-G (HAG-wAG-1 + HAG-wBG-1) = h (a + b) This proves the decomposition of h (\u00b7) over the subsets sA and sB."}, {"heading": "7.2 More Motivation and Results for the Neuroscience Application", "text": "The analysis of the fMRI data raises a number of computational and conceptual challenges. Healthy brains have much in common: anatomically, they have many of the same structures; functionally, there is a rough correlation between the structures that hide behind which processes. However, despite these great similarities, no two brains are identical, neither in their physical form nor in their functional activity. Thus, in order to benefit from the handling of a multidisciplinary fMRI data set as a multifunctional learning problem, a balance must be established between similarity in the macrostructure and dissimilarity in the microstructure. (a) LASSO (b) Glass soStandard multiple examinations involve clever \"massively univariate\" statistical methods that are applied explicitly, regardless of any datapoint in space, when this point responds in the same way to the presence of a stimulus."}, {"heading": "7.2.1 Additional Experimental Results", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}], "references": [{"title": "Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression", "author": ["Francis Bach"], "venue": "arXiv preprint arXiv:1303.6149,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Sparse group lasso for regression on land climate variables", "author": ["S. Chatterjee", "A. Banerjee", "A. Ganguly"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "A concentration theorem for projections", "author": ["S. Dasgupta", "D. Hsu", "N. Verma"], "venue": "arXiv preprint arXiv:1206.6813,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "The neural bases of the short-term storage of verbal information are anatomically variable across individuals", "author": ["Eva Feredoes", "Giulio Tononi", "Bradley R Postle"], "venue": "The Journal of Neuroscience,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Distributed and overlapping representations of faces and objects in ventral temporal cortex", "author": ["James V Haxby", "M Ida Gobbini", "Maura L Furey", "Alumit Ishai", "Jennifer L Schouten", "Pietro Pietrini"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J.P. Vert"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A dirty model for multi-task learning", "author": ["A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Proximal methods for hierarchical sparse coding", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "arXiv preprint arXiv:1009.2139,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Multiscale mining of fmri data with hierarchical structured sparsity", "author": ["Rodolphe Jenatton", "Alexandre Gramfort", "Vincent Michel", "Guillaume Obozinski", "Evelyn Eger", "Francis Bach", "Bertrand Thirion"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Taking advantage of sparsity in multi-task learning", "author": ["K. Lounici", "M. Pontil", "A.B. Tsybakov", "S. van de Geer"], "venue": "arXiv preprint arXiv:0903.1468,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["S.N. Negahban", "P. Ravikumar", "M. J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Group lasso with overlaps: The latent group lasso approach", "author": ["G. Obozinski", "L. Jacob", "J.P. Vert"], "venue": "arXiv preprint arXiv:1110.0413,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Universal measurement bounds for structured sparse signal recovery", "author": ["N. Rao", "B. Recht", "R. Nowak"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Sparse regression analysis of task-relevant information distribution in the brain", "author": ["Irina Rish", "Guillermo A Cecchia", "Kyle Heutonb", "Marwan N Balikic", "A Vania Apkarianc"], "venue": "In Proceedings of SPIE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Sparse logistic regression for whole brain classification of fmri data", "author": ["Srikanth Ryali", "Kaustubh Supekar", "Daniel A Abrams", "Vinod Menon"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "A sparse-group lasso", "author": ["N. Simon", "J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational and Graphical Statistics, (just-accepted),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Collaborative hierarchical sparse modeling", "author": ["P. Sprechmann", "I. Ramirez", "G. Sapiro", "Y. Eldar"], "venue": "In Information Sciences and Systems (CISS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "Interpreting single trial data using groupwise regularisation", "author": ["Marcel van Gerven", "Christian Hesse", "Ole Jensen", "Tom Heskes"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Using machine learning to detect cognitive states across multiple subjects", "author": ["X. Wang", "T. M Mitchell", "R. Hutchinson"], "venue": "CALD KDD project paper,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Malsar: Multi-task learning via structural regularization", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Exclusive lasso for multi-task feature selection", "author": ["Y. Zhou", "R. Jin", "S.C. Hoi"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}], "referenceMentions": [{"referenceID": 20, "context": "The group lasso (Glasso) [21, 10] is naturally suited for this situation: if a feature is selected for one task, then it is selected for all tasks.", "startOffset": 25, "endOffset": 33}, {"referenceID": 9, "context": "The group lasso (Glasso) [21, 10] is naturally suited for this situation: if a feature is selected for one task, then it is selected for all tasks.", "startOffset": 25, "endOffset": 33}, {"referenceID": 17, "context": "SOSlasso generalizes lasso [18] and Glasso, effectively spanning the range between these two well-known procedures.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Sparse group lasso [16] is a special case of SOSlasso that only applies to disjoint sets, a significant limitation when features cannot be easily partitioned, as is the case of our motivating example in fMRI.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "The main contribution of this paper is a theoretical analysis of SOSlasso, which also covers sparse group lasso as a special case (further differentiating us from [16]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "Past work has focused on recovering variables that exhibit within and across group sparsity, when the groups do not overlap [16], finding application in genetics, handwritten character recognition [17] and climate and oceanography [2].", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "Past work has focused on recovering variables that exhibit within and across group sparsity, when the groups do not overlap [16], finding application in genetics, handwritten character recognition [17] and climate and oceanography [2].", "startOffset": 197, "endOffset": 201}, {"referenceID": 1, "context": "Past work has focused on recovering variables that exhibit within and across group sparsity, when the groups do not overlap [16], finding application in genetics, handwritten character recognition [17] and climate and oceanography [2].", "startOffset": 231, "endOffset": 234}, {"referenceID": 22, "context": "Along related lines, the exclusive lasso [23] can be used when it is explicitly known that variables in certain sets are negatively correlated.", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "An example of group sparse patterns promoted by Glasso [21] is shown in (b).", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "In (c), we show the patterns considered in [7].", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "However, the specific patterns of activity in these regions will vary, both because neural codes can vary by participant [4] and because brains vary in size and shape, rendering neuroanatomy only an approximate guide to the location of relevant information across individuals.", "startOffset": 121, "endOffset": 124}, {"referenceID": 18, "context": "While logistic Glasso [19], lasso [15], and the elastic net penalty [14] have been applied to neuroimaging data, these methods do not exclusively take into account both the common macrostructure and the differences in microstructure across brains.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "While logistic Glasso [19], lasso [15], and the elastic net penalty [14] have been applied to neuroimaging data, these methods do not exclusively take into account both the common macrostructure and the differences in microstructure across brains.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "While logistic Glasso [19], lasso [15], and the elastic net penalty [14] have been applied to neuroimaging data, these methods do not exclusively take into account both the common macrostructure and the differences in microstructure across brains.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "In Section 4, we specialize the problem to the multitask linear regression setting (2), and derive consistency rates for the same, leveraging ideas from [11].", "startOffset": 153, "endOffset": 157}, {"referenceID": 9, "context": "We use this reformulation for ease of exposition (see also [10] and references therein).", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "As the \u03b1G \u2192 \u221e the `1 term becomes redundant, reducing h(x) to the overlapping group lasso penalty introduced in [6], and studied in [12, 13].", "startOffset": 112, "endOffset": 115}, {"referenceID": 11, "context": "As the \u03b1G \u2192 \u221e the `1 term becomes redundant, reducing h(x) to the overlapping group lasso penalty introduced in [6], and studied in [12, 13].", "startOffset": 132, "endOffset": 140}, {"referenceID": 12, "context": "As the \u03b1G \u2192 \u221e the `1 term becomes redundant, reducing h(x) to the overlapping group lasso penalty introduced in [6], and studied in [12, 13].", "startOffset": 132, "endOffset": 140}, {"referenceID": 5, "context": "To solve (1) and (2) with the regularizer proposed in (3), we use the covariate duplication method of [6], to reduce the problem to a non overlapping sparse group lasso problem.", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "We then use proximal point methods [8] in conjunction with the MALSAR [22] package to solve the optimization problem.", "startOffset": 35, "endOffset": 38}, {"referenceID": 21, "context": "We then use proximal point methods [8] in conjunction with the MALSAR [22] package to solve the optimization problem.", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "Since h(\u00b7) is a norm, we can apply methods developed in [11] to derive consistency rates for the optimization problems (1) and (2).", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "We will use the same notations as in [11] wherever possible.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Based on the results above, we can now apply a result from [11] to the SOSlasso:", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "7 (Corollary 1 in [11]) Consider a convex and differentiable loss function such that RSC holds with constants \u03ba and \u03c4 = 0 over (5), and a norm h(\u00b7) decomposable over sets sA and sB.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Convergence for logistic regression settings may be derived using methods in [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "Proof From the chi-squared tail bound in [3], P(zi \u2265 cd) \u2264 exp ( \u2212 (c\u22121) d 2 ) .", "startOffset": 41, "endOffset": 44}, {"referenceID": 10, "context": "From [11], we see that the convergence rate matches that of the group lasso, with an additional multiplicative factor \u03b1.", "startOffset": 5, "endOffset": 9}, {"referenceID": 5, "context": "The results of applying lasso, standard latent group lasso [6, 12], and our SOSlasso to these data are plotted in Figures 2(a), varying \u03c3, \u03b1 = 0.", "startOffset": 59, "endOffset": 66}, {"referenceID": 11, "context": "The results of applying lasso, standard latent group lasso [6, 12], and our SOSlasso to these data are plotted in Figures 2(a), varying \u03c3, \u03b1 = 0.", "startOffset": 59, "endOffset": 66}, {"referenceID": 19, "context": "In this experiment, we compared SOSlasso, lasso, and Glasso in analysis of the star-plus dataset [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "[20] showed that there exists cross-subject consistency in the cortical regions useful for prediction in this task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Note that, unlike [20], we do not aim to learn a solution that generalizes to a withheld subject.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "Finally, if SOSlasso is finding useful cross-individual structure, the features it selects should align at least somewhat with the expert-identified ROIs shown by [20] to carry consistent information.", "startOffset": 163, "endOffset": 167}, {"referenceID": 19, "context": "To assess how well these clusters align with the anatomical regions thought a-priori to be involved in sentence and picture representation, we calculated the proportion of selected voxels falling within the 7 ROIs identified by [20] as relevant to the classification task (Table 2).", "startOffset": 228, "endOffset": 232}], "year": 2013, "abstractText": "Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classified using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso.", "creator": "LaTeX with hyperref package"}}}