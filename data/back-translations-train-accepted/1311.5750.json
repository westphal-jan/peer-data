{"id": "1311.5750", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2013", "title": "Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization", "abstract": "Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantee and impressive numerical performance. In this paper, we generalize HTP from compressive sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard thresholding step with or without debiasing. We prove that our method enjoys the strong guarantees analogous to HTP in terms of rate of convergence and parameter estimation accuracy. Numerical evidences show that our method is superior to the state-of-the-art greedy selection methods in sparse logistic regression and sparse precision matrix estimation tasks.", "histories": [["v1", "Fri, 22 Nov 2013 13:52:07 GMT  (296kb)", "http://arxiv.org/abs/1311.5750v1", "arXiv admin note: text overlap witharXiv:1102.2233by other authors"], ["v2", "Mon, 25 Nov 2013 04:19:39 GMT  (296kb)", "http://arxiv.org/abs/1311.5750v2", null]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1102.2233by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.NA stat.ML", "authors": ["xiaotong yuan", "ping li 0001", "tong zhang 0001"], "accepted": true, "id": "1311.5750"}, "pdf": {"name": "1311.5750.pdf", "metadata": {"source": "CRF", "title": "Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization", "authors": ["Xiao-Tong Yuan", "Ping Li", "Tong Zhang"], "emails": ["{xtyuan1980@gmail.com,", "pingli@stat.rutgers.edu,", "tzhang@stat.rutgers.edu}"], "sections": [{"heading": null, "text": "ar Xiv: 131 1.57 50v1 [Keywords. Thrift, greedy selection, hard threshold, steep descent."}, {"heading": "1 Introduction", "text": "In the last decade, we have been looking for new ways to mislead us. (...) In the second decade, we have received a broad research interest in data mining and scientific discoveries. (...) In the third decade, we are dealing with a very complex system. (...) In the third decade, we are dealing with a very complex system. (...) In the third decade, we are dealing with a very complex system. (...) In the third decade, we are dealing with a very complex system. (...) In the fourth decade, we are dealing with a system. (...) In the fourth decade, we are dealing with a system. (...) In the fourth decade, we are dealing with a system. (...) In the fourth decade, we are dealing with a system. (...) In the fourth decade, we are dealing with a system. (...) In the fourth decade, we are dealing with a system."}, {"heading": "1.1 Our Contribution", "text": "In this paper, inspired by the success of the Hard Thresholding Pursuit (HTP) (Foucart, 2011, 2012) in the field of compressive sampling, we propose the Gradient Hard Thresholding Pursuit (GraHTP) method to capture the sparse estimation problems arising from applications using general nonlinear models. At each iteration, GraHTP performs a standard gradient descent followed by a hard threshold drop that first selects the uppermost k (in the order of magnitude) entries of the resulting vector and then (optionally) balances them out. We demonstrate that GraHTP (with or without equalization) has strong theoretical guarantees, analogous to HTP in terms of convergence rate and parameter estimation. We have applied GraHTP to the frugal logistic regression model and the sparse precision matrix model that are ineffective in both of these models."}, {"heading": "1.2 Notation", "text": "Following, x-Rp is a vector, F is an index set and A is a matrix. The following notations are used in the text. \u2022 [x] i: the ith input of vector x. \u2022 xF: the restriction of x to index sets F, i.e., [xF] i = [x] i if i-F, and [xF] i = 0 otherwise. \u2022 xk: the restriction of x to the top k entries (in modulus). We simplify xFk to xk, without ambiguity in context. \u2022 x-x-x: the Euclidean norm of x. \u2022 x-x-x-i = 1 | xi-xi |: the 1 norm of x. \u2022 x-x-0: the number of unequal entries of x. \u2022 supp (x): the index set of unequal entries of x. \u2022 supp (x, k): the index of rick in the top norm of x."}, {"heading": "1.3 Paper Organization", "text": "This work proceeds as follows: we present the GraHTP algorithm in \u00a7 2; the convergence guarantees of GraHTP are provided for in \u00a7 3; the specialisation of GraHTP in logistic regression and Gaussian learning of graphical models is examined in \u00a7 4; Monte Carlo simulations and experimental results to real data are presented in \u00a7 5; and we conclude this work in \u00a7 6."}, {"heading": "2 Gradient Hard Thresholding Pursuit", "text": "In the third stage, we will find a vector with this support that reduces the function that we have often referred to as step. (...) We have the largest support that we have. (...) We have the largest support that we have chosen as support to pursue minimization most effectively. (...) In the third stage, we find a vector x. (...) We have the largest support that we have. (...) We have the largest support that we have. (...) We have the largest support that we have chosen for minimization. (...) In the third stage, we find a vector x. (...) We have the largest support in which we have the largest support. (...) We have the largest support in which we pursue minimization most effectively. (...) In the third stage, we will find a vector with this support that reduces the target. (...) We have chosen the largest support in which we find minimization most effective. (...)"}, {"heading": "3 Theoretical Analysis", "text": "In this section we analyze the theoretical properties of GraHTP and FGraHTP."}, {"heading": "3.1 Convergence", "text": "We will now analyze the convergence properties of GraHTP and FGraHTP. First, we make a simple observation about GraHTP: Since there is only a finite number of subsets of {1,..., p} of quantity k, the sequence defined by GraHTP is ultimately periodic. The significance of this observation lies in the fact that once the convergence of GraHTP is established, we can confirm that the limit is exactly reached after a finite number of iterations. In Theorem 1, we determine the convergence of GraHTP and FGraHTP under proper conditions. Proof this theorem is provided in Appendix A.2.Theorem 1. Suppose that f the condition C (2k, \u04122k) and the step size of 1 < (1 + IC2k) converge the sequence {x (t)} of the size of GraHTP."}, {"heading": "3.2 Sparse Recovery Performance", "text": "The following theorem is our main result on the parameter estimation of GraHTP and FGraHTP, if the target solution is sparing. < p > p > p > p > p < p > p < p > p < p > p (0 \u2212 p) p > p (0 \u2212 p) p > p (0 \u2212 p) p > p (0 \u2212 p) p > p (1 \u2212 p) p < p > p (0 \u2212 p) p > p (0 \u2212 p) p > p (\"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\") p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" \"\" p \"\" p \"\" \"\" \"\" p \"\" \"\" \"\" \"\" \"\" \"\" \"p\" p \"\" \"\" \"\" \"p\" \"\""}, {"heading": "4 Applications", "text": "In this section we will specialize in two popular statistical learning models: the sparse logistic regression (in \u00a7 4.1) and the sparse precision matrix estimation (in \u00a7 4.2)."}, {"heading": "4.1 Sparsity-Constrained \u21132-Regularized Logistic Regression", "text": "Logistic regression is one of the most popular models in statistics and machine learning (Bishop, 2006).In this model, the relationship between the random character vector u-Rp and its random binary label v-1, + 1 \u00b2 is determined by the conditional probability P (v-u; w-p) = exp (2vw-u) 1 + exp (2vw-u) 1 + exp (2vw-u), where w-Rp denotes a parameter vector. In the face of a series of n independently drawn data samples {(u (i), v (i)} ni = 1, logistic regression learns the parameters w to minimize the logistic log probability."}, {"heading": "4.1.2 Bounding the Estimation Error", "text": "In the following deviation, we assume that the common density of the random vector (u, v) and Rp + 1 is given by the following exponential family distribution: P (u, v; w) = exp (vw) and B (u) \u2212 A (w)), (4,3), whereas A (w): = log (v = {\u2212 1,1) and p (vw) = exp (u). The term B (u) characterizes the marginal behavior of u (w). Obviously, the conditional distribution of v is given u, P (v | u; w)."}, {"heading": "4.2 Sparsity-Constrained Precision Matrix Estimation", "text": "An important class of sparse learning problems involves estimating the precision (inverse covariance) matrix of high-dimensional random vectors under the assumption that the true precision matrix is sparse. \u2212 This problem arises in a variety of applications, including computational biology, natural language processing, and document analysis, where the model dimension can be comparable or substantially larger than the sample size. \u2212 This problem is equivalent to learning the structure of the Gaussian Markov Random Field (GMRF) (Edwards, 2000).Let x be a p-variate random vector with zero-mean Gaussian distribution N (0, \u03a3).Let x is parameterized by the precision matrix."}, {"heading": "4.2.2 Bounding the Estimation Error.", "text": "It is known from theorem 2 that the error of estimation is controlled by the number 2. Since the number 1 + 2 = 1 + 1 + 1 = n, we have a probability of at least 1 \u2212 c0p \u2212 c1 for some positive constants c0 and c1 and a sufficiently large number of n (see e.g. Ravikumar et al., 2011, Lemma 1). Therefore, we have an overwhelming probability of 2 = O if n is sufficiently large."}, {"heading": "4.2.3 A Modified GraHTP", "text": "Unfortunately, GraHTP is not directly applicable to the problem (4,6) due to the presence of the constraint \"I\" and \"I\" in addition to the constraint. \"To solve this problem, we need to modify the discharge step (S3) of GraHTP to minimize the problem\" L \"(4,7) versus the constraint\" I \"and the support rate\" F \"(t). (4,7) Since this problem is\" convex, \"any\" off-the-shelf \"convex solver can be applied for optimization. In our implementation, we resort to the\" alternate \"(ADM) method to solve this sub-problem based on its reported efficiency (Boyd et al., 2010; Yuan, 2012). The implementation details of ADM for the solution (4,7) are moved to\" Appendix B. \"The modified GraHTP for the precision matrix estimation problem is described as part.\""}, {"heading": "5 Experimental Results", "text": "In this section, we show the empirical performance of GraHTP and FGraHTP with sparse logistical regression and sparse precision matrix estimation. We do not report on the results of our algorithms for compression sensor tasks, since GraHTP and FGraHTP are reduced to the well-studied HTP (Foucart, 2011) and IHT (Blumensath & Davies, 2009) for these tasks, respectively. Our algorithms are implemented in Matlab 7.12, which runs on a desktop with Intel Core i7 3.2G CPU and 16G RAM."}, {"heading": "5.1 Sparsity-Constrained \u21132-Regularized Logistic Regression.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 Monte-Carlo Simulation", "text": "We are looking at a synthetic data model that is identical to what is in (Bahmani et al., 2013).The sparse parameters w = 1000 dimensional vector that is able to distance itself from the standard gas distribution are indispensable for an independent selection of the underlying methods that are generated by an authoritarian system.The data that are able to hide are indispensable for an indispensable selection of the underlying methods.The data that are able to identify oneself, the data that are able to identify oneself are indispensable, the data that are able to identify oneself. The data that are able to identify oneself are indispensable. The data that are able to identify oneself are indispensable. The data that are able to identify oneself, the data that are able to identify oneself."}, {"heading": "5.1.2 Real Data", "text": "The algorithms are also compared on the rcv1.binary dataset (p = 47.236), which is a popular dataset for the binary classification of sparse data. We test with the sparsity parameters k (100, 200,..., 1000) and set the regularization parameter \u03bb = 10 \u2212 5. The initial vector is w (0) = 0 for all considered algorithms. We set the stop criterion as \"w (t) - w (t \u2212 1) - w (t \u2212 1) - 0\" - 0 for all considered algorithms. From this figure we can deduce that GraHTP and GraSP are comparable in terms of convergence rate and that they are superior to FGraHTP and FBS."}, {"heading": "5.2 Sparsity-Constrained Precision Matrix Estimation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Monte-Carlo Simulation", "text": "The fact is that we will be able to fulfil the changed conditions and that we will be able to overtake them."}, {"heading": "5.2.2 Real Data", "text": "We consider the task of the LDA (linear discriminant analysis) classification of tumors using the breast cancer data sets (Hess et al., 2006), which are available at http: / / bioinformatics.mdanderson.org /. This data set consists of 133 subjects, each of whom is associated with 22,283 gene expression levels, of which 34 are with pathological complete response (pCR) and 99 with residual disease (RD). The pCR subjects are considered to have a high chance of long-term cancer-free survival. Based on the estimated precision matrix of gene expression levels, we apply LDA to predict whether a subject can achieve the pCR state or the RD state. In our experiment, we follow the protocol used by (Cai et al). The data is used as references."}, {"heading": "6 Conclusion", "text": "In this paper, we propose GraHTP as a generalization of the HTP from compressive scanning to the generic problem of sparsity-confined optimization, the main idea being to force the gradient descent iteration by hard threshold loding. Theoretically, we prove that GraHTP converges geometrically in finite steps of iteration under mild conditions and its estimation error is controlled by the limited standard of the gradient at the sparse target solution. We also propose and analyze the FGraHTP algorithm as a fast variant of GraHTP without the biased step. Empirically, we compare GraHTP and FGraHTP with several representative, greedy selection methods when applied to economical logistic regression and sparse precision matrix estimation tasks. Our theoretical results and empirical evidence show that simple combing of gradient descent with hardening tasks leads to optimization or degradation efficiency."}, {"heading": "Acknowledgment", "text": "Xiao-Tong Yuan was a postdoctoral fellow supported by nsf-dms 0808864 and nsf-busily 1249316. Ping Li is supported by ONR-N00014-13-1-0764, AFOSR-FA9550-13-1-0137 and NSF-BigData 1250914. Tong Zhang is supported by nsf-iis 1016061, nsf-dms 1007527 and nsf-iis 1250985."}, {"heading": "A Technical Proofs", "text": "The second Inequality can be derived by replacing the first and integration f (x) \u2212 f (y) \u2212 f (y) \u2212 f (y) \u2212 f (a), x \u2212 f (b): By swapping two copies of the inequality (3.1) with x and y and the theorem 2.1.5 in (Nesterov, 2004), we know that (x \u2212 y) f (x \u2212 x), x \u2212 f (x), x \u2212 f (x), x \u2212 f (x) that the x-x x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}], "references": [{"title": "Fast global convergence rates of gradient methods for high-dimensional statistical recovery", "author": ["A. Agarwal", "S. Negahban", "M. Wainwright"], "venue": "In Proceedings of the 24th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "Greedy sparsity-constrained optimization", "author": ["S. Bahmani", "B. Raj", "P. Boufounos"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bahmani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bahmani et al\\.", "year": 2013}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "Teboulle", "Marc"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "ISBN 978-0-387-31073-2", "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath and Davies,? \\Q2009\\E", "shortCiteRegEx": "Blumensath and Davies", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2010}, {"title": "A constrained l1 minimization approach to sparse precision matrix estimation", "author": ["T. Cai", "W. liu", "X. Luo"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Cai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2011}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E.J. Cand\u00e8s", "J.K. Romberg", "T. Tao"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Dai and Milenkovic,? \\Q2009\\E", "shortCiteRegEx": "Dai and Milenkovic", "year": 2009}, {"title": "Introduction to Graphical Modelling", "author": ["D.M. Edwards"], "venue": null, "citeRegEx": "Edwards,? \\Q2000\\E", "shortCiteRegEx": "Edwards", "year": 2000}, {"title": "Hard thresholding pursuit: An algorithm for compressive sensing", "author": ["S. Foucart"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Foucart,? \\Q2011\\E", "shortCiteRegEx": "Foucart", "year": 2011}, {"title": "Sparse recovery algorithms: sufficient conditions in terms of restricted isometry constants", "author": ["S. Foucart"], "venue": "In Approximation Theory XIII: San Antonio 2010,", "citeRegEx": "Foucart,? \\Q2012\\E", "shortCiteRegEx": "Foucart", "year": 2012}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval Res. Logist. Quart.,", "citeRegEx": "Frank and Wolfe,? \\Q1956\\E", "shortCiteRegEx": "Frank and Wolfe", "year": 1956}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Pharmacogenomic predictor of snesitivity to preoperative chemotherapy with paclitaxel and fluorouracil, doxorubicin, and cyclophosphamide in breast cancer", "author": ["K.R. Hess", "K. Anderson", "W.F. Symmans"], "venue": "Journal of Clinical Oncology,", "citeRegEx": "Hess et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hess et al\\.", "year": 2006}, {"title": "Sparse convex optimization methods for machine learning", "author": ["M. Jaggi"], "venue": "Technical report, PhD thesis in Theoretical Computer Science, ETH Zurich,", "citeRegEx": "Jaggi,? \\Q2011\\E", "shortCiteRegEx": "Jaggi", "year": 2011}, {"title": "On learning discrete graphical models using greedy methods", "author": ["A. Jalali", "C.C. Johnson", "P.K. Ravikumar"], "venue": "In Proceedings of the 25th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Jalali et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2011}, {"title": "Gradient lasso for feature selection", "author": ["Kim", "Yongdai", "Jinseog"], "venue": "In Proceedings of the Twentyfirst International Conference on Machine Learning", "citeRegEx": "Kim et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2004}, {"title": "Smooth optimization approach for sparse covariance selection", "author": ["Z. Lu"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lu,? \\Q2009\\E", "shortCiteRegEx": "Lu", "year": 2009}, {"title": "Sparse principal component analysis and iterative thresholding", "author": ["Z. Ma"], "venue": "Annals of Statistics,", "citeRegEx": "Ma,? \\Q2013\\E", "shortCiteRegEx": "Ma", "year": 2013}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S. Mallat", "Zhang", "Zhifeng"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Mallat et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Mallat et al\\.", "year": 1993}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Natarajan,? \\Q1995\\E", "shortCiteRegEx": "Natarajan", "year": 1995}, {"title": "Cosamp: iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Needell and Tropp,? \\Q2009\\E", "shortCiteRegEx": "Needell and Tropp", "year": 2009}, {"title": "Introductory Lectures on Convex Optimization: A", "author": ["Y. Nesterov"], "venue": "Basic Course. Kluwer,", "citeRegEx": "Nesterov,? \\Q2004\\E", "shortCiteRegEx": "Nesterov", "year": 2004}, {"title": "High-dimensional covariance estimation by minimizing l1-penalized log-determinant divergence", "author": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Ravikumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2011}, {"title": "Trading accuracy for sparsity in optimization problems with sparsity constraints", "author": ["Shalev-Shwartz", "Shai", "Srebro", "Nathan", "Zhang", "Tong"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Greedy algorithms for structurally constrained high dimensional problems", "author": ["A. Tewari", "P. Ravikumar", "I.S. Dhillon"], "venue": "In Proceedings of the 25th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Tewari et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tewari et al\\.", "year": 2011}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J. Tropp", "A. Gilbert"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Tropp and Gilbert,? \\Q2007\\E", "shortCiteRegEx": "Tropp and Gilbert", "year": 2007}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Vershynin", "Roman"], "venue": null, "citeRegEx": "Vershynin and Roman.,? \\Q2011\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2011}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Alternating direction method of multipliers for covariance selection models", "author": ["X.M. Yuan"], "venue": "Journal of Scientific Computing,", "citeRegEx": "Yuan,? \\Q2012\\E", "shortCiteRegEx": "Yuan", "year": 2012}, {"title": "Forward basis selection for sparse approximation over dictionary", "author": ["Yuan", "X.-T", "S. Yan"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201912),", "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}, {"title": "Forward basis selection for pursuing sparse representations over a dictionary", "author": ["Yuan", "X.-T", "S. Yan"], "venue": "IEEE Transactions on Pattern Analysis And Machine Intelligence,", "citeRegEx": "Yuan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2013}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["Yuan", "X.-T", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yuan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2013}, {"title": "Adative forward-backward greedy algorithm for sparse learning with linear models", "author": ["T. Zhang"], "venue": "In Proceedings of the 22nd Annual Conference on Neural Information Processing Systems", "citeRegEx": "Zhang,? \\Q2008\\E", "shortCiteRegEx": "Zhang", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Among others, several examples falling into this model include: (i) Sparsity-constrained linear regression model (Tropp & Gilbert, 2007) where the residual error is used to measure data reconstruction error; (ii) Sparsity-constrained logistic regression model (Bahmani et al., 2013) where the sigmoid loss is used to measure prediction error; (iii) Sparsity-constrained graphical model learning (Jalali et al.", "startOffset": 260, "endOffset": 282}, {"referenceID": 16, "context": ", 2013) where the sigmoid loss is used to measure prediction error; (iii) Sparsity-constrained graphical model learning (Jalali et al., 2011) where the likelihood of samples drawn from an underlying probabilistic model is used to measure data fidelity.", "startOffset": 120, "endOffset": 141}, {"referenceID": 21, "context": "1) is generally NP-hard even for the quadratic cost function (Natarajan, 1995).", "startOffset": 61, "endOffset": 78}, {"referenceID": 10, "context": "A vast body of greedy selection algorithms for compressing sensing have been proposed including matching pursuit (Mallat & Zhang, 1993), orthogonal matching pursuit (Tropp & Gilbert, 2007), compressive sampling matching pursuit (Needell & Tropp, 2009), hard thresholding pursuit (Foucart, 2011), iterative hard thresholding (Blumensath & Davies, 2009) and subspace pursuit (Dai & Milenkovic, 2009) to name a few.", "startOffset": 279, "endOffset": 294}, {"referenceID": 0, "context": "Comparing to those first-order convex optimization methods developed for l1-regularized sparse learning (Beck & Teboulle, 2009; Agarwal et al., 2010), these greedy selection algorithms often exhibit similar accuracy guarantees but more attractive computational efficiency.", "startOffset": 104, "endOffset": 149}, {"referenceID": 3, "context": "For example, in statistical machine learning the log-likelihood function is commonly used in logistic regression problems (Bishop, 2006) and graphical models learning (Jalali et al.", "startOffset": 122, "endOffset": 136}, {"referenceID": 16, "context": "For example, in statistical machine learning the log-likelihood function is commonly used in logistic regression problems (Bishop, 2006) and graphical models learning (Jalali et al., 2011; Ravikumar et al., 2011).", "startOffset": 167, "endOffset": 212}, {"referenceID": 24, "context": "For example, in statistical machine learning the log-likelihood function is commonly used in logistic regression problems (Bishop, 2006) and graphical models learning (Jalali et al., 2011; Ravikumar et al., 2011).", "startOffset": 167, "endOffset": 212}, {"referenceID": 25, "context": "To this end, several forward selection algorithms have been proposed to select the nonzero entries in a sequential fashion (Kim & Kim, 2004; Shalev-Shwartz et al., 2010; Yuan & Yan, 2013; Jaggi, 2011).", "startOffset": 123, "endOffset": 200}, {"referenceID": 15, "context": "To this end, several forward selection algorithms have been proposed to select the nonzero entries in a sequential fashion (Kim & Kim, 2004; Shalev-Shwartz et al., 2010; Yuan & Yan, 2013; Jaggi, 2011).", "startOffset": 123, "endOffset": 200}, {"referenceID": 26, "context": "The forward greedy selection method has also been generalized to minimize a convex objective over the linear hull of a collection of atoms (Tewari et al., 2011; Yuan & Yan, 2012).", "startOffset": 139, "endOffset": 178}, {"referenceID": 19, "context": "The hard-threshholding-type methods have also been shown to be statistically and computationally efficient for sparse principal component analysis (Yuan & Zhang, 2013; Ma, 2013).", "startOffset": 147, "endOffset": 177}, {"referenceID": 23, "context": "The forward greedy selection method has also been generalized to minimize a convex objective over the linear hull of a collection of atoms (Tewari et al., 2011; Yuan & Yan, 2012). To make the greedy selection procedure more adaptive, Zhang (2008) proposed a forward-backward algorithm which takes backward steps adaptively whenever beneficial.", "startOffset": 140, "endOffset": 247}, {"referenceID": 15, "context": "Jalali et al. (2011) have applied this forward-backward selection method to learn the structure of a sparse graphical model.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "More recently, Bahmani et al. (2013) proposed a gradient hard-thresholding method which generalizes the compressive sampling matching pursuit method (Needell & Tropp, 2009) from compressive sensing to the general sparsity-constrained optimization problem.", "startOffset": 15, "endOffset": 37}, {"referenceID": 10, "context": "We close this section by pointing out that, in the special case where the squared error f(x) = 1 2\u2016y\u2212Ax\u2016 is the cost function, GraHTP reduces to HTP (Foucart, 2011).", "startOffset": 149, "endOffset": 164}, {"referenceID": 34, "context": "We may establish the connections between condition C(s, \u03b6, \u03c1s) and the conditions of restricted strong convexity/smoothness which are key to the analysis of several previous greedy selection methods (Zhang, 2008; Shalev-Shwartz et al., 2010; Yuan & Yan, 2013; Bahmani et al., 2013).", "startOffset": 199, "endOffset": 281}, {"referenceID": 25, "context": "We may establish the connections between condition C(s, \u03b6, \u03c1s) and the conditions of restricted strong convexity/smoothness which are key to the analysis of several previous greedy selection methods (Zhang, 2008; Shalev-Shwartz et al., 2010; Yuan & Yan, 2013; Bahmani et al., 2013).", "startOffset": 199, "endOffset": 281}, {"referenceID": 1, "context": "We may establish the connections between condition C(s, \u03b6, \u03c1s) and the conditions of restricted strong convexity/smoothness which are key to the analysis of several previous greedy selection methods (Zhang, 2008; Shalev-Shwartz et al., 2010; Yuan & Yan, 2013; Bahmani et al., 2013).", "startOffset": 199, "endOffset": 281}, {"referenceID": 7, "context": "This condition of \u03c1s is analogous to the RIP condition for estimation from noisy measurements in compressive sensing (Cand\u00e8s et al., 2006; Needell & Tropp, 2009; Foucart, 2011).", "startOffset": 117, "endOffset": 176}, {"referenceID": 10, "context": "This condition of \u03c1s is analogous to the RIP condition for estimation from noisy measurements in compressive sensing (Cand\u00e8s et al., 2006; Needell & Tropp, 2009; Foucart, 2011).", "startOffset": 117, "endOffset": 176}, {"referenceID": 1, "context": "For the general sparsity-constrained optimization problem, we note that a similar estimation error bound has been established for the GraSP (Gradient Support Pursuit) method (Bahmani et al., 2013) which is another hard-thresholding-type method.", "startOffset": 174, "endOffset": 196}, {"referenceID": 3, "context": "1 Sparsity-Constrained l2-Regularized Logistic Regression Logistic regression is one of the most popular models in statistics and machine learning (Bishop, 2006).", "startOffset": 147, "endOffset": 161}, {"referenceID": 9, "context": "Specifically, for multivariate Gaussian distribution, precision matrix estimation is equivalent to learning the structure of Gaussian Markov random field (GMRF) (Edwards, 2000).", "startOffset": 161, "endOffset": 176}, {"referenceID": 5, "context": "In our implementation, we resort to alternating direction method (ADM) for solving this subproblem because of its reported efficiency (Boyd et al., 2010; Yuan, 2012).", "startOffset": 134, "endOffset": 165}, {"referenceID": 30, "context": "In our implementation, we resort to alternating direction method (ADM) for solving this subproblem because of its reported efficiency (Boyd et al., 2010; Yuan, 2012).", "startOffset": 134, "endOffset": 165}, {"referenceID": 10, "context": "Here we do not report the results of our algorithms in compressive sensing tasks because in these tasks GraHTP and FGraHTP reduce to the well studied HTP (Foucart, 2011) and IHT (Blumensath & Davies, 2009), respectively.", "startOffset": 154, "endOffset": 169}, {"referenceID": 1, "context": "1 Monte-Carlo Simulation We consider a synthetic data model identical to the one used in (Bahmani et al., 2013).", "startOffset": 89, "endOffset": 111}, {"referenceID": 1, "context": "For each case, we compare GraHTP and FGraHTP with two state-of-the-art greedy selection methods: GraSP (Bahmani et al., 2013) and FBS (Forward Basis Selection) (Yuan & Yan, 2013).", "startOffset": 103, "endOffset": 125}, {"referenceID": 13, "context": "Also, we compare GraHTP with GLasso (Graphical Lasso) which is a representative convex method for l1-penalized log-determinant program (Friedman et al., 2008).", "startOffset": 135, "endOffset": 158}, {"referenceID": 18, "context": "2 Sparsity-Constrained Precision Matrix Estimation 5.2.1 Monte-Carlo Simulation Our simulation study employs the sparse precision matrix model \u03a9\u0304 = B+\u03c3I where each off-diagonal entry in B is generated independently and equals 1 with probability P = 0.1 or 0 with probability 1 \u2212 P = 0.9. B has zeros on the diagonal, and \u03c3 is chosen so that the condition number of \u03a9\u0304 is p. We generate a training sample of size n = 100 from N (0, \u03a3\u0304), and an independent sample of size 100 from the same distribution for tuning the parameter k. We compare performance for different values of p \u2208 {30, 60, 120, 200}, replicated 100 times each. We compare the modified GraHTP (see Algorithm 2) with GraSP and FBS. To adopt GraSP to sparse precision matrix estimation, we modify the algorithm with a similar two-stage strategy as used in the modified GraHTP such that it can handle the eigenvalue bounding constraint in addition to the sparsity constraint. In the work of Yuan & Yan (2013), FBS has already been applied to sparse precision matrix estimation.", "startOffset": 33, "endOffset": 971}, {"referenceID": 14, "context": "2 Real Data We consider the task of LDA (linear discriminant analysis) classification of tumors using the breast cancer dataset (Hess et al., 2006), available at http://bioinformatics.", "startOffset": 128, "endOffset": 147}, {"referenceID": 6, "context": "In our experiment, we follow the same protocol used by (Cai et al., 2011) as well as references therein.", "startOffset": 55, "endOffset": 73}, {"referenceID": 13, "context": "We also compare GraHTP with GLasso (Graphical Lasso) (Friedman et al., 2008).", "startOffset": 53, "endOffset": 76}, {"referenceID": 6, "context": "Mathews correlation coefficient (MCC) criteria as in (Cai et al., 2011):", "startOffset": 53, "endOffset": 71}, {"referenceID": 23, "context": "5 in (Nesterov, 2004), we know that (x\u2212 y)(\u2207f(x)\u2212\u2207f(y)) \u2265 ms\u2016x\u2212 y\u2016, \u2016\u2207F f(x)\u2212\u2207F f(y)\u2016 \u2264 Ms\u2016x\u2212 y\u2016.", "startOffset": 5, "endOffset": 21}], "year": 2017, "abstractText": "Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantee and impressive numerical performance. In this paper, we generalize HTP from compressive sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard thresholding step with or without debiasing. We prove that our method enjoys the strong guarantees analogous to HTP in terms of rate of convergence and parameter estimation accuracy. Numerical evidences show that our method is superior to the state-of-the-art greedy selection methods in sparse logistic regression and sparse precision matrix estimation tasks.", "creator": "LaTeX with hyperref package"}}}