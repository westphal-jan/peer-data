{"id": "1311.7184", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2013", "title": "Using multiple samples to learn mixture models", "abstract": "In the mixture models problem it is assumed that there are $K$ distributions $\\theta_{1},\\ldots,\\theta_{K}$ and one gets to observe a sample from a mixture of these distributions with unknown coefficients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same $K$ underlying distributions, but with different mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sample, we prove that it is possible to use the differences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data.", "histories": [["v1", "Thu, 28 Nov 2013 01:36:49 GMT  (284kb,D)", "http://arxiv.org/abs/1311.7184v1", "Published in Neural Information Processing Systems (NIPS) 2013"]], "COMMENTS": "Published in Neural Information Processing Systems (NIPS) 2013", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jason d lee", "ran gilad-bachrach", "rich caruana"], "accepted": true, "id": "1311.7184"}, "pdf": {"name": "1311.7184.pdf", "metadata": {"source": "CRF", "title": "Using multiple samples to learn mixture models", "authors": ["Jason Lee", "Ran Gilad-Bachrach"], "emails": ["jdl17@stanford.edu", "rang@microsoft.com", "rcaruana@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The mixing model has been extensively studied from several directions. In one setting, it is assumed that there is a single sample, that is, a single collection of instances from which to recover the hidden information. A series of studies on cluster theory, starting from [?] has proposed to address this problem by finding a projection onto a low dimensional space and solving the problem in that space. The aim of this projection is to reduce the dimension while maintaining the distances as far as possible between the means of the underlying distributions. However, we will refer to this line as MM (Mixture Models). At the other end of the spectrum, Topic Modeling (TM) assumes that several samples (documents) that are mixtures with different weights of the underlying distributions (topics) versus words."}, {"heading": "1.1 Definitions and Notations", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "1.2 Examples", "text": "Before we delve further into the discussion about our methods and how they are compared with the previous art, we would like to point out that the model we are starting from is realistic in many cases. Consider the following example: one would assume that one would like to divide medical records into groups in order to identify subtypes of diseases (e.g. different types of heart disease). In the classical cluster situation (MM), one would take a sample of patients and try to divide them into groups based on some similarity criteria. However, in many cases, one has access to data from different hospitals in different geographical locations. Communities served by different hospitals can vary in socioeconomic status, demographic backgrounds and exposure to climate and environmental hazards. Different disease subtypes are likely to appear in different relationships in different hospitals. However, if patients in two hospitals have acquired the same subtype of disease, parts of their medical records will be similar. Another example is object classification in images."}, {"heading": "2 Projection for overlapping components", "text": "In this section we present a method of using multiple samples to project high-dimensional mixtures into a low-dimensional space while maintaining the averages of the mixture components. Algorithm 1 Multi Sample Projection (MSP)"}, {"heading": "Inputs:", "text": "Samples S1,.., Sm from mixtures D1,.., Dm Outputs: Vectors v-1,.., v-1, which include the projected space: 1,..,.,.,.,..,...,...,...,...,...,..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3 Disjoint supports and the Double Sample Clustering (DSC) algorithm", "text": "Dre rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "Inputs:", "text": "\u2022 Samples S1, S2 \u2022 A binary learning algorithm L that gives samples S1, S2 with weights w1, w2 a classifier h and an estimator of error of h. \u2022 A threshold \u03c4 > 0."}, {"heading": "Outputs:", "text": "\u2022 A tree of classifiers"}, {"heading": "Algorithm:", "text": "It is not as if the DSC algorithms for the DSC algorithms on S \u2212 1 and S \u2212 2 (e) return a tree with a single leaf. < 0} (c) Let T + be the tree return to S \u2212 1 (x) < 0} (c) Let T + be the tree return to the DSC algorithm returned to S + 1 and S + 2 (d) Let T \u2212 be the tree to the DSC algorithm applied to S \u2212 1 and S \u2212 2 (e) return a tree in which c is at the root node and T \u2212 is its left subtree."}, {"heading": "4 Empirical evidence", "text": "We performed several experiments with synthetic data to compare different methods of clustering in high-dimensional spaces. Synthetic data were generated by three Gaussians with centers at points (0, 0), (3, 0) and (-3, + 3). In addition, additional dimensions with normally distributed noise were selected. In the first experiment, we used variance for all dimensions. In the second experiment, we sketched the distribution so that the variance in the other characteristics is 5. Two sets of mixing coefficients were chosen at random 100 times, using three uniform values from [0, 1] and normalized them at sums."}, {"heading": "5 Conclusions", "text": "The mixing problem studied here is closely related to the problem of clustering; most cluster data can be considered points generated from multiple underlying distributions or generating functions, and clustering can be considered a process of restoring the structure or mapping to those distributions. We presented two algorithms for the mixing problem that can be considered cluster algorithms; the MSP algorithm uses multiple samples to find a low dimensional space into which the data is projected; the DSC algorithm forms a cluster tree assuming that the clusters are fragmented; we have proven that these algorithms work under milder assumptions than the methods currently known; and the key message of this work is that, when multiple samples are available, it is often best not to bundle the data into one large sample, but to use the structure in different samples to improve cluster performance."}, {"heading": "A Supplementary Material", "text": "Here we present a detailed analysis of the results presented in the paper, which could not fit due to space constraints."}, {"heading": "A.1 Proof of Lemma 1", "text": "Proof. 1. Let A be a measurable quantity. Let Ai = A = A = Ci. D1 (A) \u2212 D2 (A) = \u2211 i D1 (Ai) \u2212 D2 (Ai) = \u2211 i \u03c61i \u03b8i (Ai) \u2212 \u03c62i \u03b8i (Ai) = \u2211 i \u03b8i (Ai) (\u03c61i \u2212 \u03c62i) \u2264 i max (insp1i \u2212 insp2i, 0).2. Let A \u0445 = insp1 i = inspi i = inspi i, 0).3. Suppose that i, inspi thenD1 (A) \u2212 D2 (sp. 1) and A is such that D1 (A) \u2212 inspi (sp.) & 2p \u2212 inspi (sp.) & lti (sp.) & lti (sp.) & lsp. & lti. & lti; & lti & lti; & lti; & lti."}, {"heading": "A.2 Proof of MSP Analysis theorem", "text": "The MSP algorithm first calculates the expected value for each sample. For each sample, we assume nNj. \u2212 Once the expected values have been calculated, the calculation of each v-j-vector is 2n operation.2. Let us remember that Dj = 0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-"}, {"heading": "A.3 Proof of Theorem 2", "text": "To simplify the notation, we define the following assumptions: Acceptance (1): Acceptance (1): Acceptance (2): Acceptance (1): Acceptance (1): Acceptance (1): Acceptance (1): Acceptance (2): Acceptance (1): Acceptance (1): Acceptance (1): Acceptance (2): Acceptance (2): Acceptance (2): Acceptance (2): Acceptance (2): Acceptance (2): Acceptance (2): Acceptance (2): Acceptance (2): Acceptance (2): (2): (2): (2): (2): (2): (2): (2): (2) (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (1:): (2): (2:): (2:): (2:): (2:): (1:): (2:): (1: (2:): (1:): (1:): (1:): (2: (1:): (1:): (1:): (1:): (2: (2:): (1:): (1: (1:): (1:): (2: (1:): (2: (1:): (1:): (2: (1: (1:): (1:): (2: (1: (1:): (2:): (1: (2: (1:): (2: (2: (2: (2:): (2: (1: (2:): (2: (1: (1:)): (1: (2): (1: (2:): (1: (1: (2: (1: ("}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "In the mixture models problem it is assumed that there are K distributions \u03b81, . . . , \u03b8K and one gets to observe a sample from a mixture of these distributions with unknown coefficients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same K underlying distributions, but with different mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sample, we prove that it is possible to use the differences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data.", "creator": "LaTeX with hyperref package"}}}