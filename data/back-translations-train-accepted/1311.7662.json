{"id": "1311.7662", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2013", "title": "The Power of Asymmetry in Binary Hashing", "abstract": "When approximating binary similarity using the hamming distance between short binary hashes, we show that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between $x$ and $x'$ as the hamming distance between $f(x)$ and $g(x')$, for two distinct binary codes $f,g$, rather than as the hamming distance between $f(x)$ and $f(x')$.", "histories": [["v1", "Fri, 29 Nov 2013 18:53:32 GMT  (118kb,D)", "http://arxiv.org/abs/1311.7662v1", "Accepted to NIPS 2013, 9 pages, 5 figures"]], "COMMENTS": "Accepted to NIPS 2013, 9 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.IR", "authors": ["behnam neyshabur", "nati srebro", "ruslan salakhutdinov", "yury makarychev", "payman yadollahpour"], "accepted": true, "id": "1311.7662"}, "pdf": {"name": "1311.7662.pdf", "metadata": {"source": "CRF", "title": "The Power of Asymmetry in Binary Hashing", "authors": ["Behnam Neyshabur", "Payman Yadollahpour", "Yury Makarychev"], "emails": ["btavakoli@ttic.edu", "pyadolla@ttic.edu", "yury@ttic.edu", "rsalakhu@cs.toronto.edu", "nati@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "It is therefore desirable to find short-term approaches that point to a rapid approach to reality. (...) It is not only possible for there to be a reduction in costs, but also for there to be a reduction in costs. (...) It is also possible for there to be a reduction in costs. (...) \"It is only a fraction of the time in which there is a reduction in costs.\" (...) \"It is only a fraction of the time in which there is a reduction in costs. (...)\" (...) \"(It is. (...)\" (It is.) \"(It is. (...)\" (It is. (...) \"(It is.)\" (It is.) \"(It is.\" (It is.) \"(It is. (It is.)\" (It is.) \"(It is.\" (It is.) \"(It is. (It is.)\" (It is. (It is.) \"(It is. (It is.)\" (It is. \"(It is.)\" (It is. \"(It is.)\" (It is. (It is.) \"(It is.\" (It is.)"}, {"heading": "2 Minimum Code Lengths and the Power of Asymmetry", "text": "Let us S: X: X \u00b7 X > V = > K = > K = > K = > K = > K = > K = > K = > K = > K = > K = > K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K"}, {"heading": "3 Approximate Binary Codes", "text": "When we turn to real data sets, we must also deviate from the search for a binary encoding that accurately captures the similarity matrix. Rather, we usually content ourselves with merely approximating S, and for each fixed code length k we look for the (symmetrical or asymmetrical) k-bit code that \"best captures\" the similarity matrix S. This is captured by the following optimization problem: min U, V, \u03b8L (Y; S), \u03b2 \u2211 i, j: (Yij) + (1 \u2212 \u03b2) \u2211 i, j: Sij = \u2212 1 \"(\u2212 Yij) s.t. U, V: {\u00b1 1} k \u00b7 n \u03b8, U > V \u2212 \u03b81n (3), where\" (z) = 1z \u2264 0 of the zero-one error and \u03b2 error is a parameter that allows us to weigh positive and negative errors differently. Such a weighting can compensate the imbalance between Sij points not typically comparable with many other pairs."}, {"heading": "4 Out of Sample Generalization: Learning a Mapping", "text": "So we focused on learning binary codes about a fixed set of objects by associating any code word with each object and completely ignoring the input representation of the objects. We only discussed how well binary hashing can approximate similarity, but would not consider generalizing additional new database objects. However, in most applications we would consider such an out-of-sample generalization, that is, we would like to apply a mapping f: X \u2192 1) k over an infinite domain X with only a finite number of objects, and then apply the mapping of objects to obtain binary codes f (x) for future objects encountered, so that S (x) x. \""}, {"heading": "5 Optimization", "text": "We focus on x-X-Rd and the linear thresholds of the form f (x), where we (Wx), where we (Wx), where we (Wx), where we (Wqx) and g (x) (Wdx), (Wdx), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (W), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), S (S), S, S, S, S (S, S, S, S, S, S (S), S (S, S, S, S, S, S (S), S (S), S (S, S, S (S), S (S), S (S, S, S (S), S (S), S (S, S, S (S), S (S, S, S, S (S), S (S), S (S, S, S (S), S (S, S), S (S, S (S), S (S, S, S (S), S (S, S (S), S, S (S, S), S (S (S), S (S, S (S, S, S, S, S, S), S (S (S), S (S, S), S (S, S, S (S, S), S (S (S, S (S, S, S, S), S (S, S (S, S), S (S, S, S (S, S (S), S (S (S (S, S, S, S, S, S), S (S), S (S (S (S, S, S, S, S), S (S (S, S, S (S, S), S (S, S, S, S, S"}, {"heading": "6 Empirical Evaluation", "text": "To empirically evaluate the benefits of asymmetry in hashing, we replicate the experiments of [8], which in turn are based on [5], six sets of data that use learned (symmetric) linear thresholds. These data sets include: LabelMe and Peekaboom, however, are collections of images that are presented as 512D GIST characteristics [13], Photo Tourism is a database of image fields that are presented as 128 SIFT characteristics [12], MNIST is a collection of 785D handwritten images, and Nursery contains 8D characteristics. Similar to [8, 5] we also construct a synthetic 10D Uniform Dataset that uniformly contains 4000 points for a 10D Hypercube. We used 1000 points for training and 3000 for testing. For each dataset, we find the Euclidean distance at which each point has an average of 50 neighbors."}, {"heading": "7 Summary", "text": "The main point we would like to make is that when looking at binary hash values to approximate similarity, even if the measure of similarity is fully symmetrical and \"well-educated,\" a lot of power can be gained by looking at asymmetrical codes. We support this claim both by theoretically analyzing the possible power of asymmetrical codes, and by proving that asymmetrical codes are better in relatively direct experimental replication than the state-of-the-art obtained for symmetrical codes. The optimization approach we use is very rough, but even with this rough approach we could find asymmetrical codes that outperform well-optimized symmetrical codes. It should certainly be possible to develop much better and more substantiated training and optimization approaches. Although we have shown our results in a specific environment using linear threshold codes, we believe that the power of asymmetrical hash is far more binding."}, {"heading": "Acknowledgments", "text": "This research was partially supported by the NSF CAREER Prize CCF-1150062 and the NSF Prize IIS-1302662."}], "references": [{"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni"], "venue": "Proceedings of the twentieth annual symposium on Computational geometry, pages 253\u2013262. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Asymmetric distance estimation with sketches for similarity search in highdimensional spaces", "author": ["W. Dong", "M. Charikar"], "venue": "SIGIR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "TPAMI", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Asymmetric distances for binary embeddings", "author": ["A. Gordo", "F. Perronnin"], "venue": "CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "R", "author": ["W. Liu"], "venue": "Ji J. Wang, Y.-G. Jiang, and S.-F. Chang. Supervised hashing with kernels. CVPR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "ICML", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Hamming distance metric learning", "author": ["M. Norouzi", "D.J. Fleet", "R. Salakhutdinov"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "S", "author": ["N. Snavely"], "venue": "M. Seitz, and R.Szeliski. Photo tourism: Exploring photo collections in 3d. In Proc. SIGGRAPH", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Small codes and large image databases for recognition", "author": ["A. Torralba", "R. Fergus", "Y. Weiss"], "venue": "CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["J. Wang", "S. Kumar", "S. Chang"], "venue": "ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "Indeed, it only takes a fraction of a second to retrieve a shortlist of similar items from a corpus containing billions of data points, which is important in image, video, audio, and document retrieval tasks [11, 9, 10, 13].", "startOffset": 208, "endOffset": 223}, {"referenceID": 8, "context": "Indeed, it only takes a fraction of a second to retrieve a shortlist of similar items from a corpus containing billions of data points, which is important in image, video, audio, and document retrieval tasks [11, 9, 10, 13].", "startOffset": 208, "endOffset": 223}, {"referenceID": 9, "context": "Indeed, it only takes a fraction of a second to retrieve a shortlist of similar items from a corpus containing billions of data points, which is important in image, video, audio, and document retrieval tasks [11, 9, 10, 13].", "startOffset": 208, "endOffset": 223}, {"referenceID": 12, "context": "Indeed, it only takes a fraction of a second to retrieve a shortlist of similar items from a corpus containing billions of data points, which is important in image, video, audio, and document retrieval tasks [11, 9, 10, 13].", "startOffset": 208, "endOffset": 223}, {"referenceID": 0, "context": "Pioneering work on Locality Sensitive Hashing used random linear thresholds for obtaining bits of the hash [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 14, "context": "Later work suggested learning hash functions attuned to the distribution of the data [15, 11, 5, 7, 3].", "startOffset": 85, "endOffset": 102}, {"referenceID": 10, "context": "Later work suggested learning hash functions attuned to the distribution of the data [15, 11, 5, 7, 3].", "startOffset": 85, "endOffset": 102}, {"referenceID": 4, "context": "Later work suggested learning hash functions attuned to the distribution of the data [15, 11, 5, 7, 3].", "startOffset": 85, "endOffset": 102}, {"referenceID": 6, "context": "Later work suggested learning hash functions attuned to the distribution of the data [15, 11, 5, 7, 3].", "startOffset": 85, "endOffset": 102}, {"referenceID": 2, "context": "Later work suggested learning hash functions attuned to the distribution of the data [15, 11, 5, 7, 3].", "startOffset": 85, "endOffset": 102}, {"referenceID": 13, "context": "More recent work focuses on learning hash functions so as to optimize agreement with the target similarity measure on specific datasets [14, 8, 9, 6] .", "startOffset": 136, "endOffset": 149}, {"referenceID": 7, "context": "More recent work focuses on learning hash functions so as to optimize agreement with the target similarity measure on specific datasets [14, 8, 9, 6] .", "startOffset": 136, "endOffset": 149}, {"referenceID": 8, "context": "More recent work focuses on learning hash functions so as to optimize agreement with the target similarity measure on specific datasets [14, 8, 9, 6] .", "startOffset": 136, "endOffset": 149}, {"referenceID": 5, "context": "More recent work focuses on learning hash functions so as to optimize agreement with the target similarity measure on specific datasets [14, 8, 9, 6] .", "startOffset": 136, "endOffset": 149}, {"referenceID": 1, "context": "This has become known as \u201casymmetric hashing\u201d [2, 4], but even with such asymmetry, both mappings are based on the same fractional mapping f\u0303(\u00b7).", "startOffset": 46, "endOffset": 52}, {"referenceID": 3, "context": "This has become known as \u201casymmetric hashing\u201d [2, 4], but even with such asymmetry, both mappings are based on the same fractional mapping f\u0303(\u00b7).", "startOffset": 46, "endOffset": 52}, {"referenceID": 7, "context": "For example, when X = R, we can consider linear threshold mappings fW (x) = sign(Wx), where W \u2208 Rk\u00d7d and sign(\u00b7) operates elementwise, as in Minimal Loss Hashing [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 10, "context": "Or, we could also consider more complex classes, such as multilayer networks [11, 9].", "startOffset": 77, "endOffset": 84}, {"referenceID": 8, "context": "Or, we could also consider more complex classes, such as multilayer networks [11, 9].", "startOffset": 77, "endOffset": 84}, {"referenceID": 7, "context": "[8]), would be to find a single parametric mapping f : X \u2192 {\u00b11} such that S(x, xi) \u2248 sign(\u3008f(x), f(xi)\u3009 \u2212 \u03b8) for future queries x and database objects xi, calculate f(xi) for all database objects xi, and store these hashes (perhaps in a hash table allowing for fast retrieval of codes within a short hamming distance).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "In order to empirically evaluate the benefits of asymmetry in hashing, we replicate the experiments of [8], which were in turn based on [5], on six datasets using learned (symmetric) linear threshold codes.", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "In order to empirically evaluate the benefits of asymmetry in hashing, we replicate the experiments of [8], which were in turn based on [5], on six datasets using learned (symmetric) linear threshold codes.", "startOffset": 136, "endOffset": 139}, {"referenceID": 12, "context": "These datasets include: LabelMe and Peekaboom are collections of images, represented as 512D GIST features [13], Photo-tourism is a database of image patches, represented as 128 SIFT features [12], MNIST is a collection of 785D greyscale handwritten images, and Nursery contains 8D features.", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "These datasets include: LabelMe and Peekaboom are collections of images, represented as 512D GIST features [13], Photo-tourism is a database of image patches, represented as 128 SIFT features [12], MNIST is a collection of 785D greyscale handwritten images, and Nursery contains 8D features.", "startOffset": 192, "endOffset": 196}, {"referenceID": 7, "context": "Similar to [8, 5], we also constructed a synthetic 10D Uniform dataset, containing uniformly sampled 4000 points for a 10D hypercube.", "startOffset": 11, "endOffset": 17}, {"referenceID": 4, "context": "Similar to [8, 5], we also constructed a synthetic 10D Uniform dataset, containing uniformly sampled 4000 points for a 10D hypercube.", "startOffset": 11, "endOffset": 17}, {"referenceID": 7, "context": "Based on these n training points, [8] present a sophisticated optimization approach for learning a thresholded linear hash function of the form f(x) = sign(Wx), whereW \u2208 Rk\u00d7d.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "[8] evaluate the quality of the hash by considering an independent set of test points and comparing S(x, xi) to sign(\u3008f(x), f(xi)\u3009\u2212 \u03b8) on the test points x and the database objects (i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "However, as in the experiments of [8], we actually learn a code (i.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "In all of our experiments, in addition to Minimal Loss Hashing (MLH), we also compare our approach to three other widely used methods: Kernel-Based Supervised Hashing (KSH) of [6], Binary Reconstructive Embedding (BRE)", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "of [5], and Locality-Sensitive Hashing (LSH) of [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "of [5], and Locality-Sensitive Hashing (LSH) of [1].", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "These results are similar to ones reported by [8], where MLH yields higher precision compared to BRE and LSH.", "startOffset": 46, "endOffset": 49}, {"referenceID": 12, "context": "We now consider a much larger LabelMe dataset [13], called Semantic 22K LabelMe.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "As argued by [8], hash functions learned using semantic labels should be more useful for content-based image retrieval compared to Euclidean distances.", "startOffset": 13, "endOffset": 16}], "year": 2013, "abstractText": "When approximating binary similarity using the hamming distance between short binary hashes, we show that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x\u2032 as the hamming distance between f(x) and g(x\u2032), for two distinct binary codes f, g, rather than as the hamming distance between f(x) and f(x\u2032).", "creator": "LaTeX with hyperref package"}}}