{"id": "1312.1099", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2013", "title": "Multiscale Dictionary Learning for Estimating Conditional Distributions", "abstract": "Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efficiently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.", "histories": [["v1", "Wed, 4 Dec 2013 10:44:01 GMT  (135kb,D)", "http://arxiv.org/abs/1312.1099v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["francesca petralia", "joshua t vogelstein", "david b dunson"], "accepted": true, "id": "1312.1099"}, "pdf": {"name": "1312.1099.pdf", "metadata": {"source": "CRF", "title": "Multiscale Dictionary Learning for Estimating Conditional Distributions", "authors": ["Francesca Petralia", "Joshua Vogelstein", "David B. Dunson"], "emails": ["francesca.petralia@mssm.edu", "jo.vo@duke.edu", "dunson@stat.duke.edu"], "sections": [{"heading": null, "text": "It is important to allow not only the mean, but also the variance and shape of response density to be flexibly altered with features that are massively dimensional. We propose a multi-scale dictionary learning model that expresses conditional response density as a convex combination of dictionary densities, with the densities and weights used depending on the path through a tree decomposition of the attribute space. To obtain tree decomposition, a fast diagram partitioning algorithm is used, using Bayesian methods to softly adaptively prune and cut various subtrees in a likely manner. The algorithm efficiently scales to about one million features. State-of-the-art prediction results are demonstrated for toy examples and two neuroscientific applications with up to one million features."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Setting", "text": "Let X: \u03b7 \u2192 X Rp be a p-dimensional euclidean vector value predictor random variable (e.g. Y R). For inferential expediency, we postulate the existence of a latent variable \u043e \u2192 M X, where M is only d \"dimensional\" and d p. Note that M does not have to be a linear subspace of X, but M could be, for example, a union or affine subspaces (or a smooth compact compact swissdimensional p). Regardless of the nature of M, we assume that we can decompose the common distribution approximately as follows, fX, Y | \u03b7 = fY | fY | fY = fY-dimensional x-dimensional x-dimensional x-dimensional x-dimensional x-dimensional x-dimensional distribution."}, {"heading": "3 Goal", "text": "Our goal is to develop an approach to derive from n pairs of observations, which we assume to be interchangeable samples from the common distribution (xi, yi) \u0445 fX, Y-F. Let us leave Dn = {(xi, yi)} i [n], where [n] = {1,..., n}. Specifically, we strive to obtain a posterior via fY | X. We insist that our approach fulfills several desiderates, including the most important: (i) scales up to p \u2248 106 in reasonable time, (ii) provides good empirical results, and (iii) automatically adapts to the complexity of the data body. To our knowledge, no sufficient approach to estimate conditional densities or posteriors thereof fulfills even our first criterion."}, {"heading": "4 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Ms. Deeds Framework", "text": "Here we propose a general modular approach, which we call multi-scale dictatorial learning for estimating conditional distributions (\"Ms. Deeds\"). \"Ms. Deeds\" consists of two components: (i) a tree structure in which the data live, and (ii) an assumed form of conditional probability calculation in which FW is a Borel probability measure, and (W) a multi-scale distribution of the data in which the data live. \"Let\" (W) is a measurable metric space in which FW is a Borel probability measure. \"W\" W \"W\" R is a metric measure for W. Let \"BWr (w) is the analog space within W.\" For example, we could be the data corpus, or it could be a tree, as in [2, 6]."}, {"heading": "4.2 Inference", "text": "We introduce the latent variable \"i-Z\" for i = [n], which denotes the multi-scale plane used by the ith observation. Let nj, k be the number of observations in Cj, k. Let kh (xi) be a variable indicating the set at level h at which xi was assigned. Each Gibbs sampler iteration can be summarized in the following steps: (i) Update \"i by scanning from the multinomial complete condition: Pr (\" i = j | \u00b7) = \u03c0j, kj (xi) fj, kj (xi) (yi | xi) / \u0445Z \u03c0s, ks (xi) fs, ks (xi) (yi | xi) (ii) Update the random variable Vj, k, for all j-Z and k-Kj, from beta (\u03b2 \u00b2, \u03b1 \u00b2) to the standard (\u03b2 \u00b2)."}, {"heading": "4.3 Simulation Studies", "text": "In order to assess the predictive power of the proposed model, we considered the four different simulation scenarios described below. (1) Nonlinear Mixture: First, we consider the following nonlinear combination as a model. (2) In the simulations, we leave (2), (2), (2), (2), (2), (3), (3), (3), (3), (3), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), and (5)."}, {"heading": "4.4 Neuroscience Applications", "text": "For all analyses, each variable was normalized by subtracting its mean and dividing it by its standard deviation, using the previous specification and the Gibbs sampler described in Sections 4.1 and 4.2. In the first experiment, we investigated the extent to which we could predict creativity (measured by the Composite Creativity Index [3]) using a structural connectome dataset collected in the Mind Research Network (data were collected as described in Jung et al. [18]). For each subject, we estimate a 70-vertex-undirected brain graph using the Magnetic Resonance Connectome Automated Pipeline (MRCAP) [12] from diffusion sensor image data [20]. Since our graphs are unaligned and have no self-loops, we have a total of p = (70 2) = 415 potentially weighted edges."}, {"heading": "4.5 Evaluation Criteria", "text": "To compare algorithmic performance, we looked at the rAm definition asrAm = \u03c6 (MSB) / \u03c6 (A), where \u03c6 is the magnitude of interest (for example, the CPU time in seconds or the mean square error), MSB is our approach, and A is the competing algorithm. To get estimates of the mean square error from MSB, we select our rear mean as the point estimate (the comparison algorithms do not generate rear predictions, but only point estimates). For each simulation scenario, we sampled several sets of data and calculated the identical distribution of rAm. In other words, instead of running simulations and reporting the power distribution for each algorithm, we compare the algorithms per simulation. This provides a much more informative indication of algorithmic performance by specifying the fraction of the simulations that one algorithm on one metric exceeds another, and for each example, we have obtained 20 sets of data."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Illustrative Example", "text": "The middle and right panels of Figure 1 represent the quality of partitioning and density estimation for the example described in paragraph 2, with the ambient dimension p = 1000 and the predictive multiple dimension d = 1. We have taken n = 104 samples for this figure. On scale 3, we have 4 partitions, and on scale 4, we have 8 (note that the partition tree generally does not have to be binary). The upper panels are color-coded to indicate which partition xi falls. Although imperfect, it should be clear that the data is very well partitioned. The lower panels show the resulting estimate of the rear panels on the two scales. These rear panels are piecemeal constant, as they are invariant for the manifold coordinates within a given partition.To avoid the need to choose a scale to make a prediction, we choose a Bayesian approach and integrate it across the modals of the previous 2 (we show the two observations)."}, {"heading": "5.2 Quantitative Comparisons for Simulated Data", "text": "Figure 3 compares the numerical performance of our algorithm (MSB) with lasso (black), CART (red) and PC regression (green) in terms of mean error (top) and CPU time (bottom) in the models (2), (3) and (4) in the left, middle and right panels. These figures show the relative performance per simulation, allowing a much more powerful comparison than the average performance for each algorithm over a series of simulations. Note that these three simulations include a wide range of models, including nonlinear, smooth manifolds such as the Swissroll (model 2), relatively simple linear partial space manifolds (model 3) and a union of linear sub-space models (model 4; which is neither linear nor varied). In terms of predictive accuracy, the top panels show that the suterip for all three simulations, in each dimension we consider to be function - including MYP = 106 \u00d7 MYS - is much more accurate than MYS - or MYP = 106 \u00d7 SARB."}, {"heading": "5.3 Quantitative Comparisons for Neuroscience Applications", "text": "Table 1 shows the mean and standard deviation of point estimates per subject (using leave-one-out) for the two neuroscience applications we studied: (i) the prediction of creativity by diffusion MRI (creativity) and (ii) the prediction of head movements based on functional MRI (motion). For the application of creativity p was relatively small, \"only\" 2, 415, allowing us to perform lasso, CART and random forest (RF) [4]. For the application of motion p was almost one million. In both applications, MSB provided improved predictive accuracy compared to all competitors. Although CART and Lasso were faster than MSB on the relatively low-dimensional predictor example (creativity), their computational scaling was poor, so that in the higher-dimensional case CART yielded a memory error and lasso took much more time than MSB."}, {"heading": "6 Discussion", "text": "In this paper, we have introduced a general formalism to estimate conditional distributions over multidimensional dictatorial learning processes. An important feature of any such strategy is the ability to scale to ultra-high dimensional predictors. We have considered simulations and real data examples in which the dimensionality of the predictor space approaches a million people. To our knowledge, no other approach to learning conditional distributions can be performed on this scale."}], "references": [{"title": "Multiscale geometric methods for data sets II: geometric wavelets", "author": ["W.K. Allard", "G. Chen", "M. Maggioni"], "venue": "Applied and Computational Harmonic Analysis, 32:435\u2013462", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Neuroimaging creativity: a psychometric view", "author": ["R. Arden", "R.S. Chavez", "R. Grazioplene", "R.E. Jung"], "venue": "Behavioural brain research, 214:143\u2013156", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical Modeling : The Two Cultures", "author": ["Leo Breiman"], "venue": "Statistical Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "An automated stopping rule for mcmc convergence assessment", "author": ["Didier Chauveau", "Jean Diebolt"], "venue": "Computational Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "A fast multiscale framework for data in high-dimensions: Measure estimation", "author": ["G. Chen", "M. Iwen", "S. Chin", "M. Maggioni"], "venue": "anomaly detection, and compressive measurements. In VCIP, 2012 IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonparametric Bayes conditional distribution modeling with variable selection", "author": ["Y. Chung", "D.B. Dunson"], "venue": "Journal of the American Statistical Association, 104:1646\u20131660", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Ten Lectures on Wavelets (CBMS-NSF Regional Conference Series in Applied Mathematics)", "author": ["Ingrid Daubechies"], "venue": "SIAM: Society for Industrial and Applied Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "Bayesian density regression", "author": ["D.B. Dunson", "N. Pillai", "J.H. Park"], "venue": "Journal of the Royal Statistical Society Series B-Statistical Methodology, 69:163\u2013183", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Estimation of conditional densities and sensitivity measures in nonlinear dynamical systems", "author": ["J.Q. Fan", "Q.W. Yao", "H. Tong"], "venue": "Biometrika, 83:189\u2013206", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "A kernel-based parametric method for conditional density estimation", "author": ["G. Fu", "F.Y. Shih", "H. Wang"], "venue": "Pattern recognition, 44:284\u2013294", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Magnetic resonance connectome automated pipeline: an overview", "author": ["W.R\u0303. Gray", "J.\u00c3. Bogovic", "J.T\u0303. Vogelstein", "B.\u00c3. Landman", "J \u0307 L. Prince", "R.J\u0303. Vogelstein"], "venue": "IEEE pulse,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Order-based dependent Dirichlet processes", "author": ["J.E. Griffin", "M.F.J. Steel"], "venue": "Journal of the American Statistical Association, 101:179\u2013194", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Fast kernel conditional density estimation: a dual-tree Monte Carlo approach", "author": ["M.P. Holmes", "G.A. Gray", "C.L. Isbell"], "venue": "Computational statistics & data analysis, 54:1707\u20131718", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive mixture of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural Computation, 3:79\u201387", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "Hierarchical mixtures-of-experts for exponential family regression models: approximation and maximum likelihood estimation", "author": ["W.X. Jiang", "M.A. Tanner"], "venue": "Annals of Statistics, 27:987\u20131011", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "A", "author": ["R.\u1ebc. Jung", "R. Grazioplene"], "venue": "Caprihan, R.S\u0303. Chavez, and R.J\u0303. Haier. White matter integrity, creativity, and psychopathology: Disentangling constructs with diffusion tensor imaging. PloS one, 5(3):e9818", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM Journal on Scientific Computing 20, 1:359\u00d0392", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Principles of diffusion tensor imaging and its applications to basic neuroscience research", "author": ["Susumu Mori", "Jiangyang Zhang"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Sparse bayesian hierarchical mixture of experts", "author": ["I. Mossavat", "O. Amft"], "venue": "IEEE Statistical Signal Processing Workshop (SSP)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian modeling of joint and conditional distributions", "author": ["A. Norets", "J. Pelenis"], "venue": "Journal of Econometrics, 168:332\u2013346", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Regression density estimation with variational methods and stochastic approximation", "author": ["D.J. Nott", "S.L. Tan", "M. Villani", "R. Kohn"], "venue": "Journal of Computational and Graphical Statistics, 21:797\u2013820", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion", "author": ["J.D. Power", "K.A. Barnes", "C.J. Stone", "R.A. Olshen"], "venue": "Neuroimage, 59:2142\u20132154", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiscale representations for manifold- valued data", "author": ["I.U. Rahman", "I. Drori", "V.C. Stodden", "D.L. Donoho"], "venue": "SIAM J. Multiscale Model, 4:1201\u20131232", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "A constructive de\u00denition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica, 4:639\u2013650", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "Towards Automated Analysis of Connectomes: The Configurable Pipeline for the Analysis of Connectomes", "author": ["S. Sikka", "J.T\u0303. Vogelstein", "M.P\u0303. Milham"], "venue": "(C- PAC). Neuroinformatics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Bayesian density regression with logistic Gaussian process and subspace projection", "author": ["S.T. Tokdar", "Y.M. Zhu", "J.K. Ghosh"], "venue": "Bayesian Analysis, 5:319\u2013344", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Simultaneous variable selection and component selection for regression density estimation with mixtures of heteroscedastic experts", "author": ["M.N. Tran", "D.J. Nott", "R. Kohn"], "venue": "Electronic Journal of Statistics, 6:1170\u20131199", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Y-F\u1e86ang", "author": ["Q-H. Zou", "C-Z. Zhu", "Y. Yang", "X-N. Zuo", "X-Y. Long", "Q-J. Cao"], "venue": "and Y-F. Zang. An improved approach to detection of amplitude of low-frequency fluctuation (ALFF) for resting-state fMRI: fractional ALFF. Journal of neuroscience methods, 172", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 23, "context": "A plethora of methods are emerging to estimate such lower-dimensional subspaces [25, 2].", "startOffset": 80, "endOffset": 87}, {"referenceID": 0, "context": "A plethora of methods are emerging to estimate such lower-dimensional subspaces [25, 2].", "startOffset": 80, "endOffset": 87}, {"referenceID": 14, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 59, "endOffset": 67}, {"referenceID": 15, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 59, "endOffset": 67}, {"referenceID": 8, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 84, "endOffset": 96}, {"referenceID": 13, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 84, "endOffset": 96}, {"referenceID": 9, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 84, "endOffset": 96}, {"referenceID": 21, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 129, "endOffset": 141}, {"referenceID": 27, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 129, "endOffset": 141}, {"referenceID": 20, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 129, "endOffset": 141}, {"referenceID": 11, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 170, "endOffset": 184}, {"referenceID": 7, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 170, "endOffset": 184}, {"referenceID": 5, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 170, "endOffset": 184}, {"referenceID": 26, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 170, "endOffset": 184}, {"referenceID": 27, "context": "However, there has been limited consideration of scaling to large p settings, with the variational Bayes approach of [29] being a notable exception.", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "For dimensionality reduction, [29] follow a greedy variable selection algorithm.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "To improve efficiency, sparse extensions relying on different variable selection algorithms have been proposed [21].", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "However, performing variable selection in high dimensions is effectively intractable: algorithms need to efficiently search for the best subsets of predictors to include in weight and mean functions within a mixture model, an NP-hard problem [14].", "startOffset": 242, "endOffset": 246}, {"referenceID": 17, "context": "This tree is efficiently learned in a first stage using a fast and scalable graph partitioning algorithm applied to the high-dimensional observations [19].", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "We define a tree decomposition as in [2, 6].", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "We define a tree decomposition as in [2, 6].", "startOffset": 37, "endOffset": 43}, {"referenceID": 6, "context": ", in wavelets [8]), we choose to learn \u03c4 from the data.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "[6] developed a multiscale measure estimation strategy, and proved that there exists a scale j such that the approximate measure is within some bound of the true measure, under certain relatively general assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Specifically, we employ METIS [19], a wellknown relatively efficient multiscale partitioning algorithm with demonstrably good empirical performance on a wide range of graphs.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "(1) be generated by a stick-breaking process [26].", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "This is a pragmatically useful feature of the Bayesian formulation, in addition to the alleviation of the need to choose a scale [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "Gibbs sampler chains were stopped testing normality of normalized averages of functions of the Markov chain [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "In the first experiment we investigated the extent to which we could predict creativity (as measured via the Composite Creativity Index [3]) via a structural", "startOffset": 136, "endOffset": 139}, {"referenceID": 16, "context": "[18]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "For each subject, we estimate a 70 vertex undirected weighted brain-graph using the Magnetic Resonance Connectome Automated Pipeline (MRCAP) [12] from diffusion tensor imaging data [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 18, "context": "For each subject, we estimate a 70 vertex undirected weighted brain-graph using the Magnetic Resonance Connectome Automated Pipeline (MRCAP) [12] from diffusion tensor imaging data [20].", "startOffset": 181, "endOffset": 185}, {"referenceID": 25, "context": "Each brain-image was processed using the Configurable Pipeline for Analysis of Connectomes (CPAC) [27].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "For each subject, we computed a measure of normalized power at each voxel called fALFF [30].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "For the creativity application, p was relatively small, \u201cmerely\u201d 2, 415, so we could run Lasso, CART, and random forests (RF) [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "Indeed, while multiscale methods benefit from a rich theoretical foundation [2], the relative advantages and disadvantages of a fully Bayesian approach, in which one can estimate posteriors over all functionals of fY |X at all scales, remains relatively unexplored.", "startOffset": 76, "endOffset": 79}], "year": 2013, "abstractText": "Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different subtrees in a soft probabilistic manner. The algorithm scales efficiently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.", "creator": "LaTeX with hyperref package"}}}