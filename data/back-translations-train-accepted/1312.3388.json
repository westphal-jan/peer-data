{"id": "1312.3388", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2013", "title": "Online Bayesian Passive-Aggressive Learning", "abstract": "Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This pa- per presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts.", "histories": [["v1", "Thu, 12 Dec 2013 02:46:07 GMT  (273kb,D)", "http://arxiv.org/abs/1312.3388v1", "10 Pages. ICML 2014, Beijing, China"]], "COMMENTS": "10 Pages. ICML 2014, Beijing, China", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tianlin shi", "jun zhu"], "accepted": true, "id": "1312.3388"}, "pdf": {"name": "1312.3388.pdf", "metadata": {"source": "META", "title": "Online Bayesian Passive-Aggressive Learning", "authors": ["Tianlin Shi", "Jun Zhu"], "emails": ["STL501@GMAIL.COM", "DCSZJ@MAIL.TSINGHUA.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is as if most of them are able to surpass themselves by taking responsibility for themselves. (...) In fact, it is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. \"(...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves.\" (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves."}, {"heading": "2. Bayesian Passive-Aggressive Learning", "text": "In this section, we present a general perspective on the Bayesian conclusion of maximum online margin."}, {"heading": "2.1. Online PA Learning", "text": "The goal of online supervised learning is to minimize the cumulative loss for a specific prediction task from the successively incoming training samples. Online Passive Aggressive (PA) algorithms (Crammer et al., 2006) achieve this goal by updating some parametrized patterns w (e.g. the weights of a linear SVM) online with the current losses from incoming data {xt} t \u2265 0 and the corresponding responses {yt} t \u2265 0. In their opinion, the losses (w; xt, yt) could be the hinge loss (\u2212 ytw > xt) + for binary classification or the -insensitive loss (| yt \u2212 w > xt | \u2212) + for regression, where it is a hyperparameter and (x) + = max (0, x) + the passive-aggressive update rule is then derived by defining the new weight sample + 1 as the solution to the following optimization problem (walgorithmically walgorithmical, walgorithmically walgorithmic (), walgorithmical (), walgorithmical (), walgorithm ()."}, {"heading": "2.2. Online BayesPA Learning", "text": "In other words, we find a posterior distribution qt + 1 (w)] s.t.: \"[q (w); xt, yt] = 0, (3) where Ft is any distribution family, e.g. the probability simplex P. In other words, we find a posterior distribution qt + 1 (w) s.t., which is not only close to qt (w) due to the commonly used KL divergence, but also a high probability of explaining new data."}, {"heading": "2.3. Learning with Latent Structures", "text": "The latent structures could typically be characterized by two types of latent variables - local latent variables hd (d \u2265 0), which characterize the hidden structures of each observed data set xd, and global variables M, which capture the common properties of all data. Therefore, the goal of Bayesian PA learning with latent structures is to update the distribution of M as well as the weights w based on each incoming mini-batch (Xt, Yt) and its corresponding latent variables Ht = {hd} d, Bt. Due to the uncertainty in Ht, we are expanding BayesPA to infer the common posterior distribution, qt + 1 (w, M, Ht), as solvingmin q, FtL (w, M, Ht) variables in the next form we are using."}, {"heading": "3. Online Max-Margin Topic Models", "text": "We apply the theory of online BayesPA to topic modeling and develop online learning algorithms for max margin topic models. In addition, in the next section we present a non-parametric generalization to resolve the number of topics."}, {"heading": "3.1. Batch MedLDA", "text": "A Max Margin Model consists of a latent Dirichlet Assignment (LDA) = q = q =. (Lead et al., 2003) Model to describe the underlying themes and a Max Margin Classifier to predict responses. Specifically, LDA is a hierarchical Bayesian model that treats each document as an admixture of themes. The generative process of document d is described as the assimilation of themes, with Dir (\u03b1), zdi \u00b2 Kk = 1, each topic representing a multinomial distribution via a W-word vocabulary. The generative process of document d is an assimilation of Dir (\u03b1), zdi \u00b2 Mult \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2, referring to the distribution variable, and Mult (\u00b7) is a multinomial distribution. In Bayesian LDA, the themes are drawn from a Dirichlet d distribution, i.e. z \u00b2 d \u00b2."}, {"heading": "3.2. Online MedLDA", "text": "We consider Gibbs MedLDA because, as shown in (Zhu et al., 2013a), it allows efficient inference algorithms by exploring the data augmentation (6), the optimal solution of the problem (6), qt + 1 (w, Ht), isqt (w, isqt), p0 (Ht) p (Xt), Ht (Ht), w), the optimal solution of the problem (6), qt + 1 (w, Ht), isqt (w, M) p0 (Ht) p (Ht) p0 (Ht), Ht (Xt), w), Yt (Yt), Yw (Ht), Yt (optimum), Ht (Hw), Xt (Ht), Xt (Ht)."}, {"heading": "4. Online Nonparametric MedLDA", "text": "We present online non-parametric MedLDA to solve the unknown number of topics based on the Hierarchical Dirichlet Process (HDP) theory (Teh et al., 2006a)."}, {"heading": "4.1. Batch MedHDP", "text": "HDP provides an extension to LDA that allows a non-parametric conclusion to be drawn from the unknown q-numbers. HDP's generation process can be summarized by a pitch-breaking construction (Wang & Lead, 2012) in which the stick lengths \u03c0 = {\u03c0k} \u221e k = 1 as: \u03c0k = \u03c0 \u00b2 k \u00b2 i < k (1 \u2212 \u03bd i), \u03c0 \u00b2 k \u00b2 beta (1, \u03b3), for k = 1,..., and the subject where the proportions are mixed as \u03b8d \u00b2 Dir (\u03b1), for d = 1,..., D. After mixing the topic, word generation is the same as in the standard LDA. To expand the HDP theme model for predictive tasks, we introduce a classifier w and define the linear discrimination function in the same form as (7), with the topic q = finite numbers."}, {"heading": "4.2. Online MedHDP", "text": "To apply the global distribution, we need to apply the global variables M = (\u03c0) and the local variables Ht = (q) and the local variables Ht = (q). (c) We need to refocus on the expected differences (9) in this paper. (c) We need to apply the same data augmentation techniques with the augmented variables. (c) We need to use the auxiliary latent variables St = (sd) d, where sdk represents the number of occupied tables served in a Chinese restaurant. (Teh et al., 2006a; Wang & Blei, 2012) We have by definition the auxiliary latent variable St = (sd) d, where we need to specify the number of occupied tables in a Chinese restaurant. (sd) We need to adjust the number of occupied tables. (sd) and specify the number of occupied tables. (sd) We need to adjust the number of occupied tables (d)."}, {"heading": "5. Experiments", "text": "We demonstrate the efficiency and predictive accuracy of online MedLDA and MedHDP, which are called paMedLDA and paMedHDP, on the 20Newsgroup (20NG) and a large Wikipedia dataset. Sensitivity analysis of key parameters is also available. After the same setting in (Zhu et al., 2012) we remove a standard list of stop words. All experiments are performed on a normal computer with core clock speed up to 2.4 GHz."}, {"heading": "5.1. Classification on 20Newsgroup", "text": "We perform a multi-class classification across the entire 20NG dataset with all 20 categories; the training set contains 11,269 documents, with the smallest category having 376 documents and the largest category having 599 documents; the test set contains 7,505 documents, with the smallest and largest categories having 259 and 399 documents respectively; we adopt the \"one-vs-all\" strategy (Rifkin & Klautau, 2004) to combine binary classifiers for multi-class prediction tasks; we compare paMedLDA and paMedHDP with their counterparts, known as bMedLDA and bMedHDP, which are achieved by using batch size."}, {"heading": "5.2. Further Discussions", "text": "First, we analyze the sensitivity of the models to some key parameters. Second, we illustrate an application with a large Wikipedia dataset of 1.1 million documents in which class names are not exclusive."}, {"heading": "5.2.1. SENSITIVITY ANALYSIS", "text": "Batch size | B |: Figure 4 shows the test errors of the BayesPA theme models as a function of training time on the entire 20NG dataset with different batch sizes, where K = 40. We can see that convergence speeds of different batch sizes (| B | = 64, 256) converge faster. For example, we choose a batch size that is too small, and are therefore much slower than the online alternatives. Second, we could observe that algorithms with medium batch sizes (| B | = 64, 256) converge faster. If we choose a batch size that is too small, for example | B | = 1, any iteration would not provide sufficient evidence for updating global variables; if the batch size is too large, each mini-batch becomes redundant and the convergence rate is reduced. Number of iterations I and samples J: Since the time complexity of the algorithm 1 is linear, we can know both in the batch size and the pre-J parameters that we would like to predict the J, as well as the pre-J pairs that we would."}, {"heading": "5.2.2. MULTI-TASK CLASSIFICATION", "text": "For multi-task classification, a number of binary classifiers are trained, each of which determines whether a document xd belongs to a particular task / category y\u03c4d, {+ 1, \u2212 1}. These binary classifiers may share common latent representations and could therefore be achieved via a modified BayesPA update equation: min q, Ft L (q (w, M, Ht) + 2c T, 1 '(q (w, M, Ht); Xt, Y \u03c4t), where T is the total number of tasks. We can then derive the multi-task version from passive-aggressive topic models designated by paMedLDA-mt and paMedHDP-mt, similar to Section 3.2 and 4.2.We test paMedLDA-mt and paMedHDP-mt, as well as the comparison models bMedLDA-mt, bMedHDP-ingsmt and gBayLDA-mt."}, {"heading": "6. Conclusions and Future Work", "text": "We show that BayesPA subsumes the online PA and, more importantly, generalizes it to incorporate latent variables and perform non-parametric Bayesian conclusions, creating great flexibility for exploratory analysis. Based on BayesPA's ideas, we develop efficient online learning algorithms for maximum-margin topic models and their non-parametric extensions. Empirical experiments on multiple real-world datasets show significant improvements in time efficiency while maintaining comparable results. As future work, we are interested in demonstrating detectable boundaries in their convergence and constraints. In addition, a better understanding of their mathematical structure would allow more involved BayesPA algorithms to be designed for different models. In addition, we are interested in developing highly scalable, distributed (Broderick et al., 2013) BayesPA learning paradigms that better meet the demand for massive data available today."}, {"heading": "Appendix A: Proof of Lemma 2.2", "text": "In this section we prove Lemma 2.2. We should point out that our deviations below also provide insights for the development of BayesPA online algorithms using the average classifiers. \u2212 Proof. We prove for the more general soft margin version of BayesPA learning, which can be reformulated using a Flack variable: qt + 1 (w) = argmin q (w). \u2212 PKL [q (w) | qt (w)] + c.s.t.: ytEq [w > xt] \u2265 \u2212 0. (24) Similar to Korollar5 in (Zhu et al., 2012), the optimal solution q (w) to the above problem can be derived from its functional Lagrangian form and has the following form: q (w) = qt (w) qt (w) exp (w) exp (w > xt)."}, {"heading": "Appendix B:", "text": "We show that the goal in (11) is an upper limit of its in (6), i.e., L (q (w, \u03a6, Zt, \u03bbt) \u2212 Eq (log), where L (q) = KL (w, 4) q0 (Zt)].Proof. We first have L (w, 4, 4, 5, 5) = Eq (g, 5) = Eq (g, 5, 5, 6) qt (w, 5) qt (w, 5) qt (Zt).We first have L (w, 5, 5, 5) = Eq (w, 5, 5) qt (w, 5, 5) qt (w, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, b, 5, 5, 5, 5, 5, b, 5, 5, 5, 5, b, 5, 5, 5, 5, 5, b, 5, 5, 5, 5, 5, b, 5, 5, 5, 5, 5, 5 (b, 5, 5, 5, 5, 5, 5, 5, 5, 5, b, 5, 5, 5, 5, 5, 5, b, 5, 5, 5, 5, 5, 5, 5, b, 5, 5, 5, 5, 5, 5, 5, b, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, b, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5) qt"}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "In ICML,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems", "author": ["C.E. Antoniak"], "venue": "The annals of statistics,", "citeRegEx": "Antoniak,? \\Q1974\\E", "shortCiteRegEx": "Antoniak", "year": 1974}, {"title": "Supervised topic models", "author": ["D.M. Blei", "J.D. McAuliffe"], "venue": "In NIPS,", "citeRegEx": "Blei and McAuliffe,? \\Q2010\\E", "shortCiteRegEx": "Blei and McAuliffe", "year": 2010}, {"title": "Streaming variational Bayes", "author": ["T. Broderick", "N. Boyd", "A. Wibisono", "A.C. Wilson", "M.I. Jordan"], "venue": "arXiv preprint arXiv:1307.6769,", "citeRegEx": "Broderick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Broderick et al\\.", "year": 2013}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.C. Chang", "Lin", "C.-J"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Online large-margin training of syntactic and structural translation features", "author": ["D. Chiang", "Y. Marton", "P. Resnik"], "venue": "In EMNLP,", "citeRegEx": "Chiang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2008}, {"title": "Non-Uniform Random Variate Generation", "author": ["L. Devroye"], "venue": null, "citeRegEx": "Devroye,? \\Q1986\\E", "shortCiteRegEx": "Devroye", "year": 1986}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["Z. Ghahramani", "T.L. Griffiths"], "venue": "In NIPS,", "citeRegEx": "Ghahramani and Griffiths,? \\Q2005\\E", "shortCiteRegEx": "Ghahramani and Griffiths", "year": 2005}, {"title": "Bayesian Nonparametrics", "author": ["N.L. Hjort"], "venue": null, "citeRegEx": "Hjort,? \\Q2010\\E", "shortCiteRegEx": "Hjort", "year": 2010}, {"title": "Online learning for latent Dirichlet allocation", "author": ["M. Hoffman", "F.R. Bach", "D.M. Blei"], "venue": "In NIPS,", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Maximum entropy discrimination", "author": ["T. Jaakkola", "M. Meila", "T. Jebara"], "venue": "In NIPS,", "citeRegEx": "Jaakkola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1999}, {"title": "Multitask sparsity via maximum entropy discrimination", "author": ["T. Jebara"], "venue": "JMLR, 12:75\u2013110,", "citeRegEx": "Jebara,? \\Q2011\\E", "shortCiteRegEx": "Jebara", "year": 2011}, {"title": "Monte Carlo Methods for Maximum Margin Supervised Topic Models", "author": ["Q. Jiang", "J. Zhu", "M. Sun", "E.P. Xing"], "venue": "In NIPS,", "citeRegEx": "Jiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2012}, {"title": "An Introduction to Variational Methods for Graphical Models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": null, "citeRegEx": "Jordan et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1998}, {"title": "DiscLDA: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "In NIPS,", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2008}, {"title": "Online largemargin training of dependency parsers", "author": ["R. McDonald", "K. Crammer", "F. Pereira"], "venue": "In ACL,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Generating random variates using transformations with multiple roots", "author": ["J.R. Michael", "W.R. Schucany", "R.W. Haas"], "venue": "The American Statistician,", "citeRegEx": "Michael et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Michael et al\\.", "year": 1976}, {"title": "Sparse stochastic inference for latent dirichlet allocation", "author": ["D. Mimno", "M. Hoffman", "D.M. Blei"], "venue": null, "citeRegEx": "Mimno et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2012}, {"title": "Hierarchical Dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation", "author": ["Y.W. Teh", "D. Newman", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Truncation-free online variational inference for Bayesian nonparametric models", "author": ["C. Wang", "D.M. Blei"], "venue": "In NIPS,", "citeRegEx": "Wang and Blei,? \\Q2012\\E", "shortCiteRegEx": "Wang and Blei", "year": 2012}, {"title": "Online variational inference for the hierarchical Dirichlet process", "author": ["C. Wang", "J.W. Paisley", "D.M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Bayesian nonparametric maximum margin matrix factorization for collaborative prediction", "author": ["M. Xu", "J. Zhu", "B. Zhang"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Maximum entropy discrimination", "author": ["J. Zhu", "E.P. Xing"], "venue": "Markov networks. JMLR,", "citeRegEx": "Zhu and Xing,? \\Q2009\\E", "shortCiteRegEx": "Zhu and Xing", "year": 2009}, {"title": "Infinite SVM: a Dirichlet process mixture of large-margin kernel machines", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}, {"title": "MedLDA: maximum margin supervised topic models", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "JMLR, 13:2237\u20132278,", "citeRegEx": "Zhu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2012}, {"title": "Gibbs max-margin topic models with fast sampling algorithms", "author": ["J. Zhu", "N. Chen", "H. Perkins", "B. Zhang"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}, {"title": "Scalable inference in max-margin topic models", "author": ["J. Zhu", "X. Zheng", "L. Zhou", "B. Zhang"], "venue": "In SIGKDD,", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": ", 2006) provides a generic framework for online large-margin learning, with many applications (McDonald et al., 2005; Chiang et al., 2008).", "startOffset": 94, "endOffset": 138}, {"referenceID": 5, "context": ", 2006) provides a generic framework for online large-margin learning, with many applications (McDonald et al., 2005; Chiang et al., 2008).", "startOffset": 94, "endOffset": 138}, {"referenceID": 8, "context": "Moreover, the recent progress on nonparametric Bayesian methods (Hjort, 2010; Teh et al., 2006a) further provides an increasingly important framework that allows the Bayesian models to have an unbounded model complexity, e.", "startOffset": 64, "endOffset": 96}, {"referenceID": 8, "context": ", an infinite number of components in a mixture model (Hjort, 2010) or an infinite number of units in a latent feature model (Ghahramani & Griffiths, 2005), and to adapt when the learning environment changes.", "startOffset": 54, "endOffset": 67}, {"referenceID": 9, "context": "To scale up Bayesian inference, much progress has been made on developing online variational Bayes (Hoffman et al., 2010; Mimno et al., 2012) and online Monte Carlo (Ahn et al.", "startOffset": 99, "endOffset": 141}, {"referenceID": 17, "context": "To scale up Bayesian inference, much progress has been made on developing online variational Bayes (Hoffman et al., 2010; Mimno et al., 2012) and online Monte Carlo (Ahn et al.", "startOffset": 99, "endOffset": 141}, {"referenceID": 0, "context": ", 2012) and online Monte Carlo (Ahn et al., 2012) methods.", "startOffset": 31, "endOffset": 49}, {"referenceID": 10, "context": "For example, maximum entropy discrimination (MED) (Jaakkola et al., 1999) made a significant advance in conjoining maxmargin learning and Bayesian generative models, mainly in the context of supervised learning and structured output prediction (Zhu & Xing, 2009).", "startOffset": 50, "endOffset": 73}, {"referenceID": 25, "context": "Recently, much attention has been focused on generalizing MED to incorporate latent variables and perform nonparametric Bayesian inference, in many contexts including topic modeling (Zhu et al., 2012), matrix factorization (Xu et al.", "startOffset": 182, "endOffset": 200}, {"referenceID": 22, "context": ", 2012), matrix factorization (Xu et al., 2012), and multi-task learning (Jebara, 2011; Zhu et al.", "startOffset": 30, "endOffset": 47}, {"referenceID": 11, "context": ", 2012), and multi-task learning (Jebara, 2011; Zhu et al., 2011).", "startOffset": 33, "endOffset": 65}, {"referenceID": 24, "context": ", 2012), and multi-task learning (Jebara, 2011; Zhu et al., 2011).", "startOffset": 33, "endOffset": 65}, {"referenceID": 25, "context": "As concrete examples, we apply the theory of online BayesPA to topic modeling and derive efficient online learning algorithms for max-margin supervised topic models (Zhu et al., 2012).", "startOffset": 165, "endOffset": 183}, {"referenceID": 21, "context": "We further develop efficient online learning algorithms for the nonparametric max-margin topic models, an extension of the nonparametric topic models (Teh et al., 2006a; Wang et al., 2011) for predictive tasks.", "startOffset": 150, "endOffset": 188}, {"referenceID": 25, "context": "Although in some cases we can marginalize out the local variables Ht, in general we would not obtain a closed-form posterior distribution qt+1(w,M) for the next optimization round, especially in dealing with some involved models like MedLDA (Zhu et al., 2012).", "startOffset": 241, "endOffset": 259}, {"referenceID": 25, "context": "For supervised topic models (Blei & McAuliffe, 2010; Zhu et al., 2012), such a regularization term could be a loss function of a prediction model w on the data X = {xd}d=1 and response signals Y = {yd}d=1.", "startOffset": 28, "endOffset": 70}, {"referenceID": 12, "context": "As a regularized Bayesian (RegBayes) model (Jiang et al., 2012), MedLDA infers a distribution of the latent variables Z as well as classification weights w by solving the problem:", "startOffset": 43, "endOffset": 63}, {"referenceID": 13, "context": "With the mild mean-field assumption that q(w,\u03a6,Zt,\u03bbt) = q(w)q(\u03a6)q(Zt,\u03bbt), we can solve (11) via an iterative procedure that alternately updates each factor distribution (Jordan et al., 1998), as detailed below.", "startOffset": 169, "endOffset": 190}, {"referenceID": 17, "context": "This hybrid strategy has shown promising performance for LDA (Mimno et al., 2012).", "startOffset": 61, "endOffset": 81}, {"referenceID": 6, "context": "a generalized inverse gaussian distribution (Devroye, 1986).", "startOffset": 44, "endOffset": 59}, {"referenceID": 16, "context": "Therefore, \u03bb\u22121 d follows an inverse gaussian distribution IG(\u03bb\u22121 d ; 1 c \u221a \u03b6\u03042 d+z\u0304 > d \u03a3 \u2217z\u0304d , 1), from which we can draw a sample in constant time (Michael et al., 1976).", "startOffset": 150, "endOffset": 172}, {"referenceID": 1, "context": "where S(a, b) are unsigned Stirling numbers of the first kind (Antoniak, 1974).", "startOffset": 62, "endOffset": 78}, {"referenceID": 25, "context": "Following the same setting in (Zhu et al., 2012), we remove a standard list of stop words.", "startOffset": 30, "endOffset": 48}, {"referenceID": 17, "context": "We also consider online unsupervised topic models as baselines, including sparse inference for LDA (spLDA) (Mimno et al., 2012), which has been demonstrated to be superior than online variational LDA (Hoffman et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 9, "context": ", 2012), which has been demonstrated to be superior than online variational LDA (Hoffman et al., 2010) in performance, and truncation-free online variational HDP (tfHDP) (Wang & Blei, 2012), which", "startOffset": 80, "endOffset": 102}, {"referenceID": 14, "context": "The performances of other batch supervised topic models, such as sLDA (Blei & McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008), are reported in (Zhu et al.", "startOffset": 107, "endOffset": 136}, {"referenceID": 25, "context": ", 2008), are reported in (Zhu et al., 2012).", "startOffset": 25, "endOffset": 43}], "year": 2013, "abstractText": "Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This paper presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts.", "creator": "LaTeX with hyperref package"}}}