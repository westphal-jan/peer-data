{"id": "1312.3393", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2013", "title": "Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem", "abstract": "This paper proposes a new method for the K-armed dueling bandit problem, a variation on the regular K-armed bandit problem that offers only relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying Upper Confidence Bound with the winner as a benchmark. We prove a finite-time regret bound of order O(log t). In addition, our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art.", "histories": [["v1", "Thu, 12 Dec 2013 03:08:46 GMT  (1952kb,D)", "http://arxiv.org/abs/1312.3393v1", "13 pages, 6 figures"], ["v2", "Tue, 17 Dec 2013 10:30:42 GMT  (1952kb,D)", "http://arxiv.org/abs/1312.3393v2", "13 pages, 6 figures"]], "COMMENTS": "13 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["masrour zoghi", "shimon whiteson", "r\u00e9mi munos", "maarten de rijke"], "accepted": true, "id": "1312.3393"}, "pdf": {"name": "1312.3393.pdf", "metadata": {"source": "META", "title": "Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem", "authors": ["Masrour Zoghi", "Shimon Whiteson"], "emails": ["M.Zoghi@uva.nl", "S.A.Whiteson@uva.nl", "remi.munos@inria.fr", "M.deRijke@uva.nl"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that most of us will be able to put ourselves in the role of real people, \"he said in an interview with the German Press Agency.\" We have it in hand, \"he said,\" but we have it in hand, \"he added:\" We have it in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand"}, {"heading": "2. Problem Setting", "text": "The K question that we have to ask ourselves is not only a problem, but also a problem that we can solve in each and every step, with the probability that we will receive a reward that depends on a constant distribution with the expected value. K question is a variation in which we select a single arm (ai, aj) and receive a reward that depends on a constant distribution with the expected value. K question is a variation in which we select a single arm (ai, aj) and in which we consider one of the two to be a better choice, with the probability that it will be selected equal to a constant distribution with the expected value, and which affects in a constant way in which we define the preference matrix P = [pij], the ij entry of which in this paper we assume there is a condorcet winner (Urvoy et al.): an arm that we select in a constant way that is used in a constant way."}, {"heading": "3. Related Work", "text": "The first method for the K-armed duel-bandit problem is that the first method for the K-armed duel-bandit problem is interleaved filter (IF) (Yue et al., 2012), which was designed for a scenario with a limited horizon and which also depends on the selection of a reference arm to compare against the rest and use it to eliminate other arms until the reference arm is eliminated by a better arm, in which case the latter becomes the reference arm and the algorithm continues as before. The algorithm ends either when all other weapons are eliminated or when the exploration horizon T is reached. More recently, the mean value (BTM) of the has been shown to exceed IF (Yue & Joachims, 2011), while imposing less restrictive assumptions on the K-armed duel problem. BTM focuses on researching the weapons involved in the duel-west comparisons."}, {"heading": "4. Method", "text": "We now introduce Relative Upper Confidence Bound (RUCB), which winner.Algorithm 1 Relative Upper Confidence Bound Input: \u03b1 > 12, T-arc = 1,...). (D-arc) 1: W + WT + WT \u00b7 K / / 2D Array of wins: wij is the number of times ai beat aj 2: for t = 1,..., T-do3: U: = [uij] = W + WT + WT = 1 WT / / / All operations are elementary; x0: = 1 for each x. UCloop it is for each i = 1,.. K. 5: Pick any c satisfactory ucj = 12 for all j. If no such c, we choose random from {1,."}, {"heading": "5. Theoretical Results", "text": "In this section, we will prove the final probability and the expected regret with respect to RUCB = > > we will assume that Lemma 1 and it will be used to prove a high probability bound in Theorem 2, from which we will derive an expected regret expressed in Theorem 3.To simplify the notation, we will assume that a1 is the optimal arm in the following. We will also define uij (t), wij (t), wij (t), wij (t), wij (t), wij (t) to be the number of Ai beaten in the first t iterations of the algorithm. We will also define uij (t): wij (t), wij (t), wij (t), wij (t), wij (t)."}, {"heading": "6. Experiments", "text": "In order to evaluate the RUCB data, we apply it to the problem of ranking from the data query area. (IR) We use this data (IR et al., 2008). A ranking is a function that is used as an input to a user's search query and ranks the documents in a collection according to their relevance to that query. Ranker evaluation aims to determine which of a number of ranking points works best. An effective way to achieve this is to use interleaved comparisons (Radlinski et al, 2008), which interleave the documents proposed by two different ranking points and present the resulting list to the user whose resulting click feedback is used to derive a noisy preference for one of the ranking points. Given a number of ranking points, the problem of finding the best ranking points can then be modeled as a K-armed dual problem, with each arm corresponding to a ranking point."}, {"heading": "7. Conclusions", "text": "In this paper, a new method called Relative Upper Confidence Bound (RUCB) was proposed for the K-Armed Duel Bandit Problem, which extends the Upper Confidence Bound (UCB) algorithm to the relative environment by making optimistic estimates of the paired probabilities in order to select a potential champion and perform regular UCB with the champion as a yardstick. We have demonstrated apocalyptic maximum probability and expectation limits of the order O (log t) for our algorithm and evaluated these empirically in an application for retrieval of information. In contrast to existing results, our regret limits hold for all time steps and not just a specific horizon T input for the algorithm. Furthermore, they rely on less restrictive assumptions or have better multiplicative constants than existing methods. Finally, the empirical results showed that RUCB significantly exceeds the state of the bandit methods."}, {"heading": "8. Appendix", "text": "Here we present some of the details alluded to in the main part of the essay."}, {"heading": "8.1. The Condorcet Assumption", "text": "As mentioned in Section 3, IF and BTM require the likelihood of comparison to meet certain difficult-to-verify conditions."}, {"heading": "8.2. Proof of Lemma 1", "text": "In this section, we perform Lemma 1, the statement of which is repeated here for convenience. Callback from section 5, that we assume without loss of generality that a1 is the optimal arm. Furthermore, we define wij (t) + wji (t) as the number of times that arm ai has beaten in the first t iteration of the algorithm. We also define uij (t): = wij (t) wij (t) + wji (t) + wji (t), where there is any positive contant, and lij (t): = 1 \u2212 uji (t). Furthermore, we define C (4\u03b1 \u2212 1) K2 (2\u03b1 \u2212 1)."}], "references": [{"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "In Conference on Learning Theory, pp", "citeRegEx": "Agrawal and Goyal,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal", "year": 2012}, {"title": "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits", "author": ["Audibert", "J.-Y", "R. Munos", "C. Szepesv\u00e1ri"], "venue": null, "citeRegEx": "Audibert et al\\.,? \\Q1876\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 1876}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Kullback-Leibler upper confidence bounds for optimal sequential allocation", "author": ["O. Capp\u00e9", "A. Garivier", "Maillard", "O.-A", "R. Munos", "G. Stoltz"], "venue": "Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "An experimental comparison of click position-bias models", "author": ["N. Craswell", "O. Zoeter", "M. Taylor", "B. Ramsey"], "venue": "In WSDM", "citeRegEx": "Craswell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Craswell et al\\.", "year": 2008}, {"title": "Exponential regret bounds for Gaussian process bandits with deterministic observations", "author": ["N. de Freitas", "A. Smola", "M. Zoghi"], "venue": "In ICML,", "citeRegEx": "Freitas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Freitas et al\\.", "year": 2012}, {"title": "Towards preference-based reinforcement learning", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier", "W. Cheng", "S.H. Park"], "venue": "Machine Learning,", "citeRegEx": "F\u00fcrnkranz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "F\u00fcrnkranz et al\\.", "year": 2012}, {"title": "Efficient multiple-click models in web search", "author": ["F. Guo", "C. Liu", "Y. Wang"], "venue": "In WSDM", "citeRegEx": "Guo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "A probabilistic method for inferring preferences from clicks", "author": ["K. Hofmann", "S. Whiteson", "M. de Rijke"], "venue": "In CIKM", "citeRegEx": "Hofmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2011}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In KDD", "citeRegEx": "Joachims,? \\Q2002\\E", "shortCiteRegEx": "Joachims", "year": 2002}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins", "year": 1985}, {"title": "Letor: Benchmark dataset for research on learning to rank for information retrieval", "author": ["Liu", "T.-Y", "J. Xu", "T. Qin", "W. Xiong", "H. Li"], "venue": "In LR4IR \u201907,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Introduction to Information Retrieval", "author": ["C. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Optimistic optimization of a deterministic function without the knowledge of its smoothness", "author": ["R. Munos"], "venue": "In NIPS,", "citeRegEx": "Munos,? \\Q2011\\E", "shortCiteRegEx": "Munos", "year": 2011}, {"title": "How does clickthrough data reflect retrieval quality", "author": ["F. Radlinski", "M. Kurup", "T. Joachims"], "venue": "In CIKM", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "In ICML,", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Generic exploration and k-armed voting bandits", "author": ["T. Urvoy", "F. Clerot", "R. F\u00e9raud", "S. Naamane"], "venue": "In ICML,", "citeRegEx": "Urvoy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urvoy et al\\.", "year": 2013}, {"title": "Stochastic simultaneous optimistic optimization", "author": ["M. Valko", "A. Carpentier", "R. Munos"], "venue": "In ICML,", "citeRegEx": "Valko et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Valko et al\\.", "year": 2013}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Y. Yue", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Yue and Joachims,? \\Q2009\\E", "shortCiteRegEx": "Yue and Joachims", "year": 2009}, {"title": "Beat the mean bandit", "author": ["Y. Yue", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Yue and Joachims,? \\Q2011\\E", "shortCiteRegEx": "Yue and Joachims", "year": 2011}, {"title": "The K-armed dueling bandits problem", "author": ["Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Yue et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 23, "context": "In this paper, we propose and analyze a new algorithm, called Relative Upper Confidence Bound (RUCB), for the K-armed dueling bandit problem (Yue et al., 2012), a variation on the K-armed bandit problem, where the feedback comes in the form of pairwise preferences.", "startOffset": 141, "endOffset": 159}, {"referenceID": 11, "context": "We assess the performance of this algorithm using one of the main current applications of the K-armed dueling bandit problem, ranker evaluation (Hofmann et al., 2013; Joachims, 2002; Yue & Joachims, 2011), which is used in information retrieval, ad placement and recommender systems, among others.", "startOffset": 144, "endOffset": 204}, {"referenceID": 8, "context": "The K-armed dueling bandit problem is part of the general framework of preference learning (F\u00fcrnkranz & H\u00fcllermeier, 2010; F\u00fcrnkranz et al., 2012), where the goal is to learn, not from real-valued feedback, but from relative feedback, which specifies only which of two alternatives is preferred.", "startOffset": 91, "endOffset": 146}, {"referenceID": 8, "context": ", because it is provided by a human) and specifying real-valued feedback instead would be arbitrary or inefficient (F\u00fcrnkranz et al., 2012).", "startOffset": 115, "endOffset": 139}, {"referenceID": 23, "context": "Other algorithms proposed for this problem are Interleaved Filter (IF) (Yue et al., 2012), Beat the Mean (BTM) (Yue & Joachims, 2011), and SAVAGE (Urvoy et al.", "startOffset": 71, "endOffset": 89}, {"referenceID": 19, "context": ", 2012), Beat the Mean (BTM) (Yue & Joachims, 2011), and SAVAGE (Urvoy et al., 2013).", "startOffset": 64, "endOffset": 84}, {"referenceID": 2, "context": "comes, and (1) use these estimates to select a potential champion, which is an arm that has a chance of being the best arm, and (2) select an arm to compare to this potential champion by performing regular Upper Confidence Bound (Auer et al., 2002) relative to it.", "startOffset": 229, "endOffset": 248}, {"referenceID": 23, "context": "The K-armed dueling bandit problem (Yue et al., 2012) is a modification of the K-armed bandit problem (Auer et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 2, "context": ", 2012) is a modification of the K-armed bandit problem (Auer et al., 2002): the latter considers K arms {a1, .", "startOffset": 56, "endOffset": 75}, {"referenceID": 19, "context": "In this paper, we assume that there exists a Condorcet winner (Urvoy et al., 2013): an arm, which without loss of generality we label a1, such that p1i > 1 2 for all i > 1.", "startOffset": 62, "endOffset": 82}, {"referenceID": 23, "context": "Given a Condorcet winner, we define regret for each time-step as follows (Yue et al., 2012): if arms ai and aj were chosen for comparison at time t, then regret at that time is set to be rt := \u22061i+\u22061j 2 , with \u2206k := p1k \u2212 12 for all k \u2208 {1, .", "startOffset": 73, "endOffset": 91}, {"referenceID": 19, "context": "The Condorcet winner is different in a subtle but important way from the Borda winner (Urvoy et al., 2013), which is an arm ab that satisfies \u2211 j pbj \u2265 \u2211 j pij , for all i = 1, .", "startOffset": 86, "endOffset": 106}, {"referenceID": 19, "context": "In this setting, the algorithm can be assessed on its accuracy, the probability that a given run of the algorithm reports the Condorcet winner as the best arm (Urvoy et al., 2013), which is related to expected simple regret : the regret associated with the algorithm\u2019s choice of the best arm, i.", "startOffset": 159, "endOffset": 179}, {"referenceID": 3, "context": ", rT+1 (Bubeck et al., 2009).", "startOffset": 7, "endOffset": 28}, {"referenceID": 23, "context": "mulated by the explore-then-exploit problem formulation (Yue et al., 2012).", "startOffset": 56, "endOffset": 74}, {"referenceID": 23, "context": "The first method for the K-armed dueling bandit problem is interleaved filter (IF) (Yue et al., 2012), which was designed for a finite-horizon scenario and which proceeds by picking a reference arm to compare against the rest and using it to eliminate other arms, until the reference arm is eliminated by a better arm, in which case the latter becomes the reference arm and the algorithm continues as before.", "startOffset": 83, "endOffset": 101}, {"referenceID": 23, "context": "Under these conditions, theoretical results have been proven for IF and BTM in (Yue et al., 2012) and (Yue & Joachims, 2011).", "startOffset": 79, "endOffset": 97}, {"referenceID": 19, "context": "Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al., 2013) is a recently proposed algorithm that outperforms both IF and BTM by a wide margin when the number of arms is of moderate size.", "startOffset": 67, "endOffset": 87}, {"referenceID": 19, "context": "Moreover, one version of SAVAGE, called Condorcet SAVAGE, makes the Condorcet assumption and performed the best experimentally (Urvoy et al., 2013).", "startOffset": 127, "endOffset": 147}, {"referenceID": 2, "context": "Note that RUCB uses the upperconfidence bounds (Line 3 of Algorithm 1) introduced in the original version of UCB (Auer et al., 2002) (up to the \u03b1 factor).", "startOffset": 113, "endOffset": 132}, {"referenceID": 4, "context": ", 2009) or KL-UCB (Capp\u00e9 et al., 2013)) have improved performance for the regular K-armed bandit problem.", "startOffset": 18, "endOffset": 38}, {"referenceID": 14, "context": "To evaluate RUCB, we apply it to the problem of ranker evaluation from the field of information retrieval (IR) (Manning et al., 2008).", "startOffset": 111, "endOffset": 133}, {"referenceID": 16, "context": "One effective way to achieve this is to use interleaved comparisons (Radlinski et al., 2008), which interleave the documents proposed by two different rankers and presents the resulting list to the user, whose resulting click feedback is used to infer a noisy preference for one of the rankers.", "startOffset": 68, "endOffset": 92}, {"referenceID": 13, "context": "Our experimental setup is built on real IR data, namely the LETOR NP2004 dataset (Liu et al., 2007).", "startOffset": 81, "endOffset": 99}, {"referenceID": 10, "context": "To compare a pair of rankers, we use probabilistic interleave (PI) (Hofmann et al., 2011), a recently developed method for interleaved comparisons.", "startOffset": 67, "endOffset": 89}, {"referenceID": 6, "context": "To model the user\u2019s click behavior on the resulting interleaved lists, we employ a probabilistic user model (Craswell et al., 2008; Hofmann et al., 2011) that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the LETOR NP2004 dataset.", "startOffset": 108, "endOffset": 153}, {"referenceID": 10, "context": "To model the user\u2019s click behavior on the resulting interleaved lists, we employ a probabilistic user model (Craswell et al., 2008; Hofmann et al., 2011) that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the LETOR NP2004 dataset.", "startOffset": 108, "endOffset": 153}, {"referenceID": 19, "context": "We did not include an evaluation of IF, since both BTM and Condocet SAVAGE were shown to outperform it (Urvoy et al., 2013; Yue & Joachims, 2011).", "startOffset": 103, "endOffset": 145}, {"referenceID": 15, "context": "First, building off extensions of UCB to the continuous bandit setting (Bubeck et al., 2011; de Freitas et al., 2012; Munos, 2011; Srinivas et al., 2010; Valko et al., 2013), we aim to extend RUCB to the continuous dueling bandit setting, without a convexity assumption as in (Yue & Joachims, 2009).", "startOffset": 71, "endOffset": 173}, {"referenceID": 17, "context": "First, building off extensions of UCB to the continuous bandit setting (Bubeck et al., 2011; de Freitas et al., 2012; Munos, 2011; Srinivas et al., 2010; Valko et al., 2013), we aim to extend RUCB to the continuous dueling bandit setting, without a convexity assumption as in (Yue & Joachims, 2009).", "startOffset": 71, "endOffset": 173}, {"referenceID": 20, "context": "First, building off extensions of UCB to the continuous bandit setting (Bubeck et al., 2011; de Freitas et al., 2012; Munos, 2011; Srinivas et al., 2010; Valko et al., 2013), we aim to extend RUCB to the continuous dueling bandit setting, without a convexity assumption as in (Yue & Joachims, 2009).", "startOffset": 71, "endOffset": 173}, {"referenceID": 18, "context": "Second, building off Thompson Sampling (Agrawal & Goyal, 2012; Kauffmann et al., 2012; Thompson, 1933), an elegant and effective sampling-based alternative to UCB, we will investigate whether a sampling-based extension to RUCB would be amenable to theoretical analysis.", "startOffset": 39, "endOffset": 102}], "year": 2017, "abstractText": "This paper proposes a new method for the K-armed dueling bandit problem, a variation on the regular K-armed bandit problem that offers only relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying Upper Confidence Bound with the winner as a benchmark. We prove a finite-time regret bound of order O(log t). In addition, our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art.", "creator": "LaTeX with hyperref package"}}}