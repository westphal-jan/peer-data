{"id": "1312.4400", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Network In Network", "abstract": "We propose a novel network structure called \"Network In Network\" (NIN) to enhance the model discriminability for local receptive fields. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to handle the variance of the local receptive fields. We instantiate the micro neural network with a nonlinear multiple layer structure which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner of CNN and then fed into the next layer. The deep NIN is thus implemented as stacking of multiple sliding micro neural networks. We demonstrated state-of-the-art classification performances with NIN on CIFAR-10/100, SVHN and MINST datasets.", "histories": [["v1", "Mon, 16 Dec 2013 15:34:13 GMT  (501kb,D)", "http://arxiv.org/abs/1312.4400v1", null], ["v2", "Wed, 18 Dec 2013 09:30:27 GMT  (509kb,D)", "http://arxiv.org/abs/1312.4400v2", "9 pages, 4 figures, for iclr2014"], ["v3", "Tue, 4 Mar 2014 05:15:42 GMT  (445kb,D)", "http://arxiv.org/abs/1312.4400v3", "10 pages, 4 figures, for iclr2014"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["min lin", "qiang chen", "shuicheng yan"], "accepted": true, "id": "1312.4400"}, "pdf": {"name": "1312.4400.pdf", "metadata": {"source": "CRF", "title": "Network In Network", "authors": ["Min Lin", "Qiang Chen", "Shuicheng Yan"], "emails": ["linmin@nus.edu.sg", "chenqiang@nus.edu.sg", "eleyans@nus.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "The convolution layers take internal product of the linear filter and the underlying receptive field followed by a nonlinear activation function at each local part of the input, the resulting results are referred to as attribute cards. Visualization of CNN [2] indicates that the activations in the attribute card correspond to some latent concepts, a higher value in the attribute card indicates a higher probability that the corresponding input field contains the latent concept. Thus, we can consider the activations in the attribute cards as trust values of latent concepts. The revolutionary filter can also be considered a linear binary classifier for local patches, and the separation is defined by the filter.The above process is sufficient if the latent concepts are more linear separable."}, {"heading": "2 Convolutional Neural Networks", "text": "The revolutionary layers generate characteristics followed by a linear revolutionary filter followed by a nonlinear activation function (rectifier, sigmoid, tanh, etc.). Using linear rectifiers as an example, the characteristic map can be calculated as in the equation 1.fi, j, k = relu (w T k xi, j). Here, i and j are the pixel indexes in the characteristic map centered for the input patch at the site (i, j), and k is used to index the channels of the characteristic map. This linear evolution is sufficient if the instances of the latent concepts are separable. However, the trust of the latent concepts in the latent concepts concepts is generally highly nonlinear functions of the input data."}, {"heading": "3 Network In Network", "text": "First, we look at the key components of our proposed \"Network In Network\" structure, i.e. the MLP convolution layer and the global average pooling layer. Then, we explain the general NIN in Section 3.3."}, {"heading": "3.1 MLP Convolution Layers", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the re"}, {"heading": "3.2 Global Average Pooling", "text": "Conventional Convolutionary Neural Networks perform folding in the lower layers of the network. For classification, the characteristic maps of the last revolutionary layer are vectorized and fed into fully connected layers, followed by a softmax logistic regression layer [7] [8]. This structure bridges the revolutionary structure with traditional classifiers of neural networks. It treats the revolutionary layers as trait extractors, and the resulting trait is classified in the traditional way. However, the fully connected layers are prone to overload, hampering the generalization capacity of the entire network. Dropouts are by Hinton et al. [9] as a regulator that randomly sets half of the activations from the fully connected layers to zero during training."}, {"heading": "3.3 Network In Network Structure", "text": "The overall structure of NIN consists of a stack of mlpconv layers containing the global average pooling layer and the objective cost layer. Sub-sampling layers can be added between the mlpconv layers, as is the case with CNN and maxout networks. Figure 2 shows a NIN with three mlpconv layers, and within each mlpconv layer there is a three-layer perceptron. The number of layers in either NIN or the micro networks is flexible and can be changed for specific tasks."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Overview", "text": "We evaluate NIN using four benchmark data sets: CIFAR-10, CIFAR-100, SVHN, and MNIST. The networks used for the data sets all consist of three stacked mlpconv layers, the mlpconv layers in all experiments followed by a spatial max pooling layer that tries out the input image by a factor of two downwards. As a regulator, the weight drop used by Krizhevsky et al. [7]. Figure 2 illustrates the overall structure of the NIN network used in this section."}, {"heading": "4.2 CIFAR-10", "text": "The CIFAR 10 dataset [10] consists of 10 classes of natural images with a total of 50,000 images and 10,000 test images. Each image is an RGB image of the size 32x32. For this dataset, we apply the same global contrast normalization and ZCA whitening used by Goodfellow et al. in the maxout network. [6] We use the last 10,000 images of the training set as validation data. The network used for CIFAR-10 contains three mlpconv layers, each of which uses a three-layer perceptron to weave their input. The number of function cards for each mlpconv layer is set to the same number as in the corresponding maxout network. Two hyperparameters are matched using the validation set, i.e. the local receptive field size and weight loss. Then the hyperparameters are fixed and we retrain the network from ground to ground."}, {"heading": "4.3 CIFAR-100", "text": "The CIFAR-100 dataset [10] has the same size and format as the CIFAR-10 dataset, but contains 100 classes. Thus, the number of images per class is only one-tenth of the CIFAR-10 dataset. For CIFAR-100, we do not adjust the hyperparameters, but use the same settings as for the CIFAR-10 dataset. The only difference is that the last mlpconv layer prints 100 feature maps. For CIFAR-100, there is a test error of 35.68%, which exceeds the current best performance without data augmentation by more than two percent."}, {"heading": "4.4 Street View House Numbers", "text": "The SVHN dataset [14] consists of 630420 32 x 32 color images, divided into training set, test set and an additional set. The task for this dataset is to classify the digit located in the center of each image. Goodfellow et al. [6] will follow the training and testing procedure, namely 400 samples per class selected from the training set, and 200 samples per class from the additional set will be used for validation. The rest of the training set and the additional set will be used for training. The validation set will only be used as a guide for hyperparameter selection, but never for training the model. Pre-processing of the dataset will in turn follow Goodfellow et al., which was a local contrast normalization. Structure and parameters used in SVHN are similar to those used for CIFAR-10, which consists of three mlpconv layers, followed by a global average spooling. For this new set, we will get a 2.35% error rate in the data set compared with the 2.3 in our data set."}, {"heading": "4.5 MNIST", "text": "The MNIST [1] dataset consists of handwritten digits 0-9, measuring 28x28. There are a total of 60,000 training images and 10,000 test images. This dataset adopts the same network structure as for cifar10. However, the number of feature maps generated by each mlpconv layer is decreasing. Since MNIST is a simpler dataset than CIFAR-10 and therefore requires fewer parameters, we test our method on this dataset without data augmentation. The result is compared to previous work that has adopted revolutionary structures, and is shown in Table 4. We do not achieve a better performance than the current best for the MNIST dataset. However, since MNIST has been set to a very low error rate of 0.45%, we believe that 0.47% already represents a comparable result."}, {"heading": "4.6 Global Average Pooling as a Regularizer", "text": "The difference is in the transformation matrix. For the global average pooling, the transformation matrix is prefixed, and it is not zero only on block diagonal elements that share the same value. Fully connected layers can have dense transformation matrices, and the values are subject to backpropagation optimization. To study the regulating effect of global average pooling, we replace the global average pool layer with a fully connected layer, while the other parts of the model remain the same. We evaluated this model with and without dropout before the fully connected linear layer. Both models are tested on the CIFAR 10 dataset, a comparison of performance is shown in Table 5. As shown in Table 5, the fully connected layer without dropout regulation has the worst performance (11.59%)."}, {"heading": "4.7 Visualization of NIN", "text": "We explicitly force function boards in the last mlpconv layer of the NIN to be trust cards of the categories by means of global average pooling activities, which is only possible with stronger local receptive field modeling, e.g. mlpconv in NIN. To understand how much this purpose is achieved, we extract and visualize the function boards from the last mlpconv layer of the trained model for CIFAR-10. Figure 3 shows some examplar images and their corresponding function boards for each of the ten categories selected from the CIFAR-10 test set. It is expected that the largest activations will be observed in the function card corresponding to the soil truth category of the input image; as this is forced by global average pooling forces. Within the function card of the soil truth category, we observed that the strongest activations in the same region of the object appear in the original image."}, {"heading": "5 Conclusion", "text": "We propose a new structure called \"Network In Network\" (NIN) for classification tasks. This new structure consists of mlpconv layers that use multi-layer perceptrons to pool input, and a global average pooling layer that replaces the fully connected layers in conventional CNN. Mlpconv layers better model local patches, and global average pooling acts as a structural regulator that prevents global overlapping. With these two components of NIN, we demonstrate the performance on CIFAR-10, CIFAR-100, SVHN, and MNIST datasets. By visualizing the feature maps, we show that feature maps from the last mlpconv layer of NIN are similar to confidence maps of the categories, and this shows an ability to perform object recognition in NIN."}], "references": [{"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "arXiv preprint arXiv:1311.2901,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y Bengio", "A Courville", "P Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Ica with reconstruction cost for efficient overcomplete feature learning", "author": ["Quoc V Le", "Alexandre Karpenko", "Jiquan Ngiam", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Piecewise linear multilayer perceptrons and dropout", "author": ["Ian J Goodfellow"], "venue": "arXiv preprint arXiv:1301.5088,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "arXiv preprint arXiv:1301.3557,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Master\u2019s thesis,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1206.2944,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Learnable pooling regions for image classification", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "arXiv preprint arXiv:1301.3516,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Improving neural networks with dropout", "author": ["Nitish Srivastava"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Learning hierarchical features for scene labeling", "author": ["Clement Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Convolution neural networks (CNNs) [1] consist of alternating convolutional layers and pooling layers.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "The visualization of CNN [2] gives hint that the activations in the feature map correspond to some latent concepts, a bigger value in the feature map indicates a higher probability that the corresponding input patch contains the latent concept.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "However, the confidence of a latent concept is often a highly nonlinear function of the input data[3], leading to complex separation hyperplanes.", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "In conventional CNN, this might be compensated by utilizing an over-complete set of filters [4] to cover all variations latent concepts.", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "However, having too many filters for a single concept imposes extra burden on the next layer, which needs to consider all combinations of variations from the previous layer[5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 2, "context": "Multilayer perceptron can be a deep model itself, which is consistent with the spirit of feature re-use [3].", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "For classification, the feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax logistic regression layer [7] [6] [8].", "startOffset": 168, "endOffset": 171}, {"referenceID": 6, "context": "For classification, the feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax logistic regression layer [7] [6] [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 7, "context": "[9] as a regularizer which randomly sets half of the activations from the fully connected layers to zero during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "It has successfully shown improvements on the generalization ability and largely prevents overfitting [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We implemente our network on the super fast cuda-convnet code developed by Alex Krizhevsky [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The CIFAR-10 dataset [10] is composed of 10 classes of natural images with 50,000 images in total, and 10,000 testing images.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "Method Test Error Stochastic Pooling [8] 15.", "startOffset": 37, "endOffset": 40}, {"referenceID": 9, "context": "13% CNN + Spearmint [11] 14.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "41% CNN + Spearmint + Data Augmentation [11] 9.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "38% DropConnect + 12 networks + Data Augmentation [12] 9.", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "The CIFAR-100 dataset [10] is the same in size and format as the CIFAR-10 dataset, but contains 100 classes.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Method Test Error Learned Pooling [13] 43.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "71% Stochastic Pooling [8] 42.", "startOffset": 23, "endOffset": 26}, {"referenceID": 12, "context": "The SVHN dataset [14] is composed of 630420 32x32 color images, divided into training set, testing set and an extra set.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "Method Test Error Stochastic Pooling [8] 2.", "startOffset": 37, "endOffset": 40}, {"referenceID": 13, "context": "80% Rectifier + Dropout [15] 2.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "78% Rectifier + Dropout + Synthetic Translation [15] 2.", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "The MNIST [1] dataset consists of hand written digits 0-9 which are 28x28 in size.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Method Test Error 2-Layer CNN + 2-Layer NN [8] 0.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "53% Stochastic Pooling [8] 0.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "[9], which consists of three convolutional layers and one local connection layer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] for convectional CNN with fully connected layer and dropout.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[16]", "startOffset": 0, "endOffset": 4}], "year": 2013, "abstractText": "We propose a novel network structure called \u201cNetwork In Network\u201d(NIN) to enhance the model discriminability for local receptive fields. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to handle the variance of the local receptive fields. We instantiate the micro neural network with a nonlinear multiple layer structure which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner of CNN and then fed into the next layer. The deep NIN is thus implemented as stacking of multiple sliding micro neural networks. We demonstrated state-of-the-art classification performances with NIN on CIFAR 10/100, SVHN and MINST datasets.", "creator": "LaTeX with hyperref package"}}}