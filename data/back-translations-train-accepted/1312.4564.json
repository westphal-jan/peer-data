{"id": "1312.4564", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Adaptive Stochastic Alternating Direction Method of Multipliers", "abstract": "The Alternating Direction Method of Multipliers (ADMM) has been studied for years, since it can be applied to many large-scale and data-distributed machine learning tasks. The traditional ADMM algorithm needs to compute an (empirical) expected loss function on all the training examples for each iteration, which results in a computational complexity propositional to the number of training examples. To reduce the time complexity, stochastic ADMM algorithm is proposed to replace the expected loss function by a random loss function associated with one single uniformly drawn example and Bregman divergence for a second order proximal function. The Bregman divergence in the original stochastic ADMM algorithm is derived from half squared norm, which could be a suboptimal choice. In this paper, we present a new stochastic ADMM algorithm, using Bregman divergence derived from second order proximal functions associated with iteratively updated matrices. Our new stochastic ADMM produces a new family of adaptive subgradient methods. We theoretically prove that their regret bounds are as good as the bounds achieved by the best proximal functions that can be chosen in hindsight. Encouraging results confirm the effectiveness and efficiency of the proposed algorithms.", "histories": [["v1", "Mon, 16 Dec 2013 21:22:46 GMT  (42kb)", "https://arxiv.org/abs/1312.4564v1", "12 pages"], ["v2", "Sun, 22 Dec 2013 01:59:05 GMT  (0kb,I)", "http://arxiv.org/abs/1312.4564v2", "This paper has been withdrawn by the author, because this paper is not well-prepared"], ["v3", "Thu, 5 Jun 2014 07:03:48 GMT  (43kb)", "http://arxiv.org/abs/1312.4564v3", "12 pages"], ["v4", "Mon, 9 Jun 2014 09:31:13 GMT  (43kb)", "http://arxiv.org/abs/1312.4564v4", "13 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["peilin zhao", "jinwei yang", "tong zhang", "ping li 0001"], "accepted": true, "id": "1312.4564"}, "pdf": {"name": "1312.4564.pdf", "metadata": {"source": "CRF", "title": "Adaptive Stochastic Alternating Direction Method of Multipliers", "authors": ["Peilin Zhao", "Jinwei Yang", "Tong Zhang", "Ping Li"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 131 2.45 64v4 [st at.M L] 9J un2 014In this thesis we present a new family of stochastic ADMM algorithms with optimal second-order proximal functions that produce a new family of adaptive subgradient methods. Theoretically, we prove that their remorse limits are as good as the limits that can be achieved by the best proximal function that can be selected retrospectively. Encouraging empirical results on a variety of real data sets confirm the effectiveness and efficiency of the proposed algorithms."}, {"heading": "1 Introduction", "text": "In fact, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "2 Adaptive Stochastic Alternating Direction Method of Multipli-", "text": ""}, {"heading": "2.1 Problem Formulation", "text": "In this thesis, we will examine a family of convex optimization problems in which our objective functions are composed. In particular, we are interested in the following equality-bound optimization task: min w, v, Vf ((w, v, v): = E, (w,) + E, (v), S, T, Aw + Bv = b, (1), where w, Rd1, v, Rd2, A, Rm, d1, B, Rm, d2, b, Rm, W, and V are convex sets. For the sake of simplicity, the notation is used both for the instance function value (w,) and for its expectation (w) = E, (w,). It is assumed that a sequence of identical and independent (i.i.d.) observations are derived from the random vector, x, which satisfies a fixed but unknown distribution."}, {"heading": "2.2 Algorithm", "text": "To solve the problem (1), a popular method is the Alternating Direction Multipliers Method (ADMM = \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2). (ADMM splits the optimizations in relation to w and v by using the extended Lagrangian: min w, vL\u03b2 (w, \u03b8): = (w) +. (v). (Aw + Bv \u2212 b >. (b). (wt). (wt). (wt). (wt). (v). (v). (v). (v). (v). (v). (v). (v). (v). (v). (v). (v). (v). (v). (v). (v). (v.). (v.). (v.). (v.). (v. (v.). (v.). (v. (v.). (v. (v.). (v.). (v. (v.). (v.). (v. (v.). (v.). (v.). (v. (v.). (v.). (v.). (v.). (v. (v.). (v.). (v.). (v. (v.). (v.). (v.). (v. (v.). (v.). (v. (v.). (v). (v. (v). (v. (v.). (v.). (v. (v.). (v. (v.). (v.). (v. (v.). (v. (v. (v.). (v.). (v.). (v. (v. (v.). (v.). (v. (v. (v.). (v. (v.). (v.). (v. (v.). (v. (v.). (v.). (v. (v.).). (v. (v. (v.). (v.). (v.)."}, {"heading": "2.3 Analysis", "text": "In this subsection, we will analyze the performance of the proposed algorithm for general Ht, t = > b = > b = >., T. (w) Specifically, we will provide an expected convergence rate of iterative solutions. \u2212 To achieve this goal, we will first start with a technical problem that facilitates later analysis. Lemma 1. Let's (w) and the results (w) be convex, and we will be positive for t \u2212 1. Then, for algorithm 1, we will have the following inequality (wt) + 1) + 1) \u2212 t (w) \u2212 t (w) \u2212 t (w), wt (wt) \u2212 t (v), (wt), (wt), (wt) \u2212 t (wt), \u2212 t (t) (z) \u2212 f (t). \u2212 f"}, {"heading": "2.4 Diagonal Matrix Proximal Functions", "text": "In this subsection, we restrict Ht as a diagonal matrix for two reasons: (i) the diagonal matrix will yield results that are easier to understand than the general matrix; (ii) for high dimensional problems, the general matrix can lead to prohibitive costs that are not desirable. (i) First, we note that the upper limit in Theorem 1 is based on the upper level. (ii) For each g1, g2,., gT that we know in advance, we could minimize this term by setting Ht = diag (s). We will use the following proposition. (ii) For each g1, g2,., gT, we havemin diag (s) 0, 1) s, cT (s) s, 2diag (s) = 1c (d1)."}, {"heading": "2.5 Full Matrix Proximal Functions", "text": "In this subsection, we will use the pseudo-forecasters to replace the minimizers in the S = cG 1 / 2 T. (D = 1) We will use the results of the analysis for the diagonal S 0, tr (S) \u2264 1, tr (S) \u2264 1, tr (D), tr (D), t (S), t (S), t (S), T), T (S), T (S), T (S), T (S), T (S), T (S), T (S), T (S), T (T), T (S), T (S), T (S), T (S), T (T), T (T), T (S)."}, {"heading": "3 Experiment", "text": "In this section, we will evaluate the empirical performance of the proposed adaptive stochastic ADMM algorithms for solving GGSVM tasks, which is formulated as the following problem [15]: min w, v1nn \u2211 i = 1 [1 \u2212 yix i w] + \u03b32 \u04412 + \u03bd \u0442 2 + \u03bd \u0432 v \u0445 1, s.t. Fw \u2212 v = 0, where [z] + = max (0, z) and the matrix F is constructed on the basis of a diagram G = {V, E}. For this diagram, V = {w1,.., wd1} is a series of variables and E = {e1,..., e | E |}, where ek = {i, j} is assigned with a weight \u03b1ij. And the associated F is in the form: Fki = \u03b1ij and Fkj = \u2212 \u03b1ij. To construct a diagram for a given dataset, we assume the sparse sign, the verance [ij = 1 and ij = i1] ij of the variance."}, {"heading": "3.1 Experimental Testbed and Setup", "text": "To check performance, we test all algorithms against 6 real data sets from Web Machine Learning Repositories listed in Table 1. \"news20\" is the \"20 newsgroups\" downloaded from 1, while the other data sets can be downloaded from the LIBSVM website.2 For each data set, we randomly divide it into two folds: the training parameter with 80% of the examples and the test set with the remainder. To make a fair comparison, all algorithms use the same experimental setup. In particular, we set the penalty parameter \u03b3 = \u03bd = 1 / n, where n is the number of training examples, and the compromise parameter \u03b2 = 1. In addition, we set the step size parameter \u03b7t = 1 / (\u03b3t) for SADMM according to Theorem 2 in [15]. Finally, the smooth parameter a is set as 1, and the step size for adaptive stochastic ADMM algorithms is measured from 2 to 5."}, {"heading": "3.2 Performance Evaluation", "text": "Figure 1 shows the performance of all algorithms compared to studies from which we can draw several observations. Firstly, the left column shows the objective values of the three algorithms. We can observe that the two adaptive stochastic ADMM algorithms converge much faster than SADMM, demonstrating the effectiveness of researching adaptive (sub) gradients to accelerate stochastic ADMM. Secondly, compared to Ada-SADMMdiag, Ada-SADMMfull achieves slightly smaller objective values on most datasets, suggesting that the complete matrix is slightly more informative than the diagonal one. Thirdly, the central column provides test error rates of three algorithms, where we observe that the two adaptive algorithms achieve significantly lower or comparable test error rates at 0.25-th epoch than SADMM at 2-th epoch."}, {"heading": "4 Conclusion", "text": "ADMM is a popular technique in machine learning. This paper explored how to accelerate stochastic ADMM with adaptive subgradients by replacing fixed proximal function with adaptive proximal function. Compared to traditional stochastic ADMM, we show that the proposed adaptive algorithms converge significantly faster through the proposed adaptive strategies. Promising experimental results on a variety of real data sets further confirm the effectiveness of our techniques."}], "references": [{"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["Wei Deng", "Wotao Yin"], "venue": "Technical report, DTIC Document,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "On the douglasrachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["Jonathan Eckstein", "Dimitri P Bertsekas"], "venue": "Mathematical Programming,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Chapter ix applications of the method of multipliers to variational inequalities", "author": ["Daniel Gabay"], "venue": "Studies in mathematics and its applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1983}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["Daniel Gabay", "Bertrand Mercier"], "venue": "Computers & Mathematics with Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1976}, {"title": "Sur lapproximation, par elements nis dordre un, et la resolution, par penalisationdualite, dune classe de problems de dirichlet non lineares", "author": ["R. Glowinski", "A. Marroco"], "venue": "Revue Francaise dAutomatique, Informatique,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1975}, {"title": "Augmented Lagrangian and operator-splitting methods in nonlinear mechanics, volume", "author": ["Roland Glowinski", "Patrick Le Tallec"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Fast alternating linearization methods for minimizing the sum of two convex functions", "author": ["Donald Goldfarb", "Shiqian Ma", "Katya Scheinberg"], "venue": "Math. Program.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "The split bregman method for l1-regularized problems", "author": ["Tom Goldstein", "Stanley Osher"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "On the o(1/n) convergence rate of the douglas-rachford alternating direction method", "author": ["Bingsheng He", "Xiaoming Yuan"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["Zhi-Quan Luo"], "venue": "arXiv preprint arXiv:1208.3922,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Iteration-complexity of block-decomposition algorithms and the alternating direction method of multipliers", "author": ["Renato D.C. Monteiro", "Benar Fux Svaiter"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Stochastic alternating direction method of multipliers", "author": ["Hua Ouyang", "Niao He", "Long Tran", "Alexander G Gray"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Dual averaging and proximal gradient descent for online alternating direction multiplier method", "author": ["Taiji Suzuki"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Online alternating direction method", "author": ["Huahua Wang", "Arindam Banerjee"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Alternating direction algorithms for l1-problems in compressive sensing", "author": ["Junfeng Yang", "Yin Zhang"], "venue": "SIAM journal on scientific computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 40, "endOffset": 46}, {"referenceID": 6, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 40, "endOffset": 46}, {"referenceID": 5, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 215, "endOffset": 224}, {"referenceID": 8, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 215, "endOffset": 224}, {"referenceID": 3, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 215, "endOffset": 224}, {"referenceID": 13, "context": "Recent studies have shown that ADMM achieves a convergence rate of O(1/T ) [14, 12] (where T is number of iterations of ADMM), when the objective function is generally convex.", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "Recent studies have shown that ADMM achieves a convergence rate of O(1/T ) [14, 12] (where T is number of iterations of ADMM), when the objective function is generally convex.", "startOffset": 75, "endOffset": 83}, {"referenceID": 12, "context": "Furthermore, ADMM enjoys a convergence rate of O(\u03b1 ), for some \u03b1 \u2208 (0, 1), when the objective function is strongly convex and smooth [13, 2].", "startOffset": 133, "endOffset": 140}, {"referenceID": 1, "context": "Furthermore, ADMM enjoys a convergence rate of O(\u03b1 ), for some \u03b1 \u2208 (0, 1), when the objective function is strongly convex and smooth [13, 2].", "startOffset": 133, "endOffset": 140}, {"referenceID": 17, "context": "ADMM has shown attractive performance in a wide range of real-world problems such as compressed sensing [18], image restoration [11], video processing, and matrix completion [10], etc.", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "ADMM has shown attractive performance in a wide range of real-world problems such as compressed sensing [18], image restoration [11], video processing, and matrix completion [10], etc.", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "ADMM has shown attractive performance in a wide range of real-world problems such as compressed sensing [18], image restoration [11], video processing, and matrix completion [10], etc.", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "The online ADMM (OADMM) algorithm [17] was proposed to tackle the computational challenge.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "Interestingly, although the optimization of the loss function is assumed to be easy in the analysis of [17], it is actually not necessarily easy in practice.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "To address this issue, the stochastic ADMM algorithm was proposed, by linearizing the the online loss function [15, 16].", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "To address this issue, the stochastic ADMM algorithm was proposed, by linearizing the the online loss function [15, 16].", "startOffset": 111, "endOffset": 119}, {"referenceID": 14, "context": "In the previous work [15, 16] the Bregman divergence is derived from a simple second order function, i.", "startOffset": 21, "endOffset": 29}, {"referenceID": 15, "context": "In the previous work [15, 16] the Bregman divergence is derived from a simple second order function, i.", "startOffset": 21, "endOffset": 29}, {"referenceID": 2, "context": ", the half squared norm, which could be a suboptimal choice [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "When \u03be is deterministic, the above optimization becomes the traditional problem formulation of ADMM [1].", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "It is easy to see that this proposed approximation includes the one proposed by [15] as a special case when Ht = I.", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "To provide an upper bound for the first term Lt, taking D(u,v) = B\u03c6t(u,v) = 1 2\u2016u \u2212 v\u2016Ht and applying Lemma 1 in [15] to the step of getting wt+1 in the Algorithm 1, we will have \u3008l(wt, \u03bet) +A[\u03b2(Awt+1 +Bvt \u2212 b)\u2212 \u03b8t],wt+1 \u2212w\u3009 \u2264 1 \u03b7 [B\u03c6t(wt,w)\u2212 B\u03c6t(wt+1,w)\u2212 B\u03c6t(wt+1,wt)].", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "where the last inequality used the Lemma 4 in [3], which implies this update is a nearly optimal update method for the diagonal matrix case.", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "Similar with the analysis for the diagonal case, we first introduce the following proposition (Lemma 15 in [3]).", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "where the last inequality used the Lemma 10 in [3], which implies this update is a nearly optimal update method for the full matrix case.", "startOffset": 47, "endOffset": 50}, {"referenceID": 14, "context": "3 Experiment In this section, we will evaluate the empirical performance of the proposed adaptive stochastic ADMM algorithms for solving GGSVM tasks, which is formulated as the following problem [15]:", "startOffset": 195, "endOffset": 199}, {"referenceID": 4, "context": "To construct a graph for a given dataset, we adopt the sparse inverse covariance estimation [5] and determine the sparsity pattern of the inverse covariance matrix \u03a3\u22121.", "startOffset": 92, "endOffset": 95}, {"referenceID": 14, "context": "In addition, we set the step size parameter \u03b7t = 1/(\u03b3t) for SADMM according to the theorem 2 in [15].", "startOffset": 96, "endOffset": 100}], "year": 2014, "abstractText": "The Alternating Direction Method of Multipliers (ADMM) has been studied for years. The traditional ADMM algorithm needs to compute, at each iteration, an (empirical) expected loss function on all training examples, resulting in a computational complexity proportional to the number of training examples. To reduce the time complexity, stochastic ADMM algorithms were proposed to replace the expected function with a random loss function associated with one uniformly drawn example plus a Bregman divergence. The Bregman divergence, however, is derived from a simple second order proximal function, the half squared norm, which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms with optimal second order proximal functions, which produce a new family of adaptive subgradient methods. We theoretically prove that their regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms.", "creator": "LaTeX with hyperref package"}}}