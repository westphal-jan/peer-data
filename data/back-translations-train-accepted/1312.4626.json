{"id": "1312.4626", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2013", "title": "Compact Random Feature Maps", "abstract": "Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results.", "histories": [["v1", "Tue, 17 Dec 2013 03:33:08 GMT  (4447kb,D)", "http://arxiv.org/abs/1312.4626v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["raffay hamid", "ying xiao", "alex gittens", "dennis decoste"], "accepted": true, "id": "1312.4626"}, "pdf": {"name": "1312.4626.pdf", "metadata": {"source": "META", "title": "Compact Random Feature Maps", "authors": ["Raffay Hamid", "Ying Xiao"], "emails": ["raffay@gmail.com", "ying.xiao@gatech.edu", "agittens@ebay.com", "ddecoste@ebay.com"], "sections": [{"heading": "1. Introduction", "text": "This year it has come to the point that it will be able to retaliate, \"he said in an interview with the German Press Agency.\" We have never hesitated so long, \"he said.\" We have never hesitated so long until we were able to retaliate, \"he said."}, {"heading": "2. Related Work", "text": "Extending the core machine framework to large-scale learning processes has been explored in a variety of ways (Bottou et al., 2007), the most popular of which are decomposition methods for solving support vector machines (Platt, 1999) (Chang & Lin, 2011).While these methods are generally extremely useful, problems with more than a few hundred thousand data points are not always easily scaled.To solve this challenge, several schemes have been proposed to explicitly approximate the core matrix (Blum, 2006) (Bach & Jordan, 2005), to stomp individual entries (Achlioptas et al., 2002), or to discard entire lines (Drineas & Mahoney, 2005).Fast neighbor search methods have also been used to approach multiplication operations with the core matrix."}, {"heading": "3. Compact Random Feature Maps", "text": "We begin by demonstrating that earlier approaches to the approximation of polynomial cores (Kar & Karnick, 2012) (Pham & Pagh, 2013) construct rank-deficient spaces. As a solution to these challenges, we present the framework of CRAFTMaps, followed by proof of its algorithm 1 - Random Feature Maps (RFM) Input: Kernel parameters q and r, output dimensionality D, sampling parameters p > 0 Output: Random Feature Map Z: Rd \u2192 RD such that < Z (x), Z (y) > \u2248 K (x, y) 1: Set f (x) = 0 anx n, where an = fn (0) n! 2: for each i = 1 to D 3: Set N-N0 for P [N = n] = 1pn + 1 4: Sample w1, \u00b7 \u00b7 \u00b7, wN = 1 box: Set Zi: 7 \u00b0 aJ N \u00b7 N \u00b7 N \u00b7 1 \u00b7 T1 = 1 efficient."}, {"heading": "3.1. Preliminaries", "text": "Following (Kar & Karnick, 2012), a positively defined kernel K is considered: (x, y) 7 \u2192 f (< x, y >), where f allows Maclaurin expansion with only non-negative coefficients, i.e., f (x) = -0 anxn, where \u2265 0. An example of such a kernel is the polynomial kernel K (x, y) = (< x, y > + q) r, with q + -0 and r +. By defining estimates for each individual term of kernel expansion, one can approximate the exact kernel dot products. (For this purpose, w-Rd will be a wheel-maker vector, i.e. each of its components is independently selected from the set {\u2212 1, 1} -D using a fair coin."}, {"heading": "3.2. Limitations of Random Feature Maps", "text": "Random character maps are an efficient means of approximating the underlying structure of the exact core space, but their efficiency can come at the expense of their rank deficit. Consider, for example, Figure 2 (a), where the black chart shows the log-scree diagram of the 7th order polynomial nucleus (q = 1), which was determined using 1000 randomly selected points from MNIST data. The red chart shows the log-scree diagram for the random feature chart (Kar & Karnick, 2012) in a 212-dimensional space. It can be observed that the red chart is substantially lower than the black chart for most of the spectrum. Note that this rank deficit also applies to the space generated by random tensor products (Pham & Pagh, 2013), whose log-scree diagram is shown in green in Figure 2 (a)."}, {"heading": "3.3. CRAFTMaps using Up/Down Projections", "text": "The intuition behind CRAFTMaps is to first comprehensively grasp the eigenstructure of the exact core space, followed by its representation in a more precise form. CRAFTMaps are therefore generated in the following two steps: Up Projection: Since the difference between < Z (x), Z (y) > and K (x, y) decreases exponentially as a function of the dimensionality of Z (Kar & Karnick, 2012) (Pham & Pagh, 2013), we first project the original data non-linear from Rd to a much higher dimensional space RD in order to maximize the underlying eigenstructure of the exact core space. Down Projection: Since the randomized feature map Z: Rd \u2192 RD 2013 is created as a result of the high-projection, the sketch of RD is projected to a much higher dimensional space RD in order to maximize the underlying eigenstructure of the exact core space (\u00a7 RD) > RIP (RIP)."}, {"heading": "3.4. Error Bounds for CRAFTMaps", "text": "Remember that the following result obtained with an application of Hoeffding inequality (Hoeffding, 1963) is central to the analysis of (Kar & Karnick, 2012): < Z (x), Z (y) > \u2212 K (x, y), y > r for all points on the sphere of unity. In this case, we have, C2o = (pR 2), 2 = (12r + 1), 2 d2r (2), where R = max. x, y > r for all points on the sphere of unity. In this case, we have, C2o = (pR 2), 2 = (12r + 1), 2 d2r (2), where R = max. x), and a suitable choice for p is 1 / 2. We only get a non-trivial binding when D & 2d2r."}, {"heading": "3.5. Efficient CRAFTMaps Generation", "text": "Remember that for Hessian-based optimization of linear regression problems, the dominant costs of O (nD2) are spent to calculate the Hessian. By compact representation of random features in RE as opposed to RD for E < D, CRAFTMaps provide a factor of D2 / E2gain in the complexity of Hessian calculation. A simple version of CRAFTMaps would incur additional costs for O (nDE) for the downward step. However, since for problems on the n > > D scale, the gains CRAFTMaps provide for classification metrics via random feature maps are worth the relatively small additional cost they incur. These gains can be further improved by using structured random matrices for the up / down projections of CRAFTMaps. One way to do this is to use the Hadamard matrix as a set of orthonorthonic bases, as opposed to a mean distribution."}, {"heading": "4. Classification Using ECOCs", "text": "To solve multi-level classification problems, we use error correction output codes (ECOCs) (Dietterich & Bakiri, 1994) that use a unique binary \"codeword\" of length c for each of the k-classes, and learn c binary functions, one for each bit position in the code words. For training, the required output of the c-binary functions is assigned by the codeword for class i. In a test instance x, each of the c-binary functions is evaluated to calculate a c-bit string s. This string is compared with the k-codewords, where x is assigned to the class whose codeword is1As Hadamards exist in potencies of 2, usually x must be padded to the nearest higher performance of 2.0, corresponding to a certain distance. Overall, d-dimensional data from k-classes are compared, relying first on / ab projections to classify all the COCRT2 data we use in order to classify the first two-dimensional framework of the RE."}, {"heading": "5. Experiments and Results", "text": "We now present reconstruction and classification results of CRAFTMaps on several datasets."}, {"heading": "5.1. Reconstruction Error", "text": "Figure 4 shows the errors in the mean value of the root square (Nrms) obtained during the reconstruction of the polynomial nucleus with r = 7 and q = 1 compared to their respective CRAFTMap representations using random character sketches (Kar & Karnick, 2012) and tensor sketches (Pham & Pagh, 2013). Results from 6 different data sets are presented. All diagrams in each diagram were obtained using 10 folds of 1000 randomly selected data points from a particular dataset. As shown, CRAFTMaps provide a significant reconstruction improvement for random maps and tensor sketches. Figure 5 shows the reconstruction improvements due to CRAFTMaps depending on the polynomic degree. These results were obtained using 10 sets of 1000 randomly selected points from MNIST data."}, {"heading": "5.2. Classification Error", "text": "Table 1 shows the classification errors obtained with the help of random maps (Kar & Karnick, 2012) and tensor sketches (Pham & Pagh, 2013) compared to their CRAFTMap representations. Results across 4 different datasets are presented, on which CRAFTMaps delivered consistently improved classification results. Table 1-a shows the results of CRAFTMaps on MNIST data for small and substantially large projected characteristics. Note that for RE < d (which is for MNIST 784) the random characteristic cards cannot use the H-0 / 1 heuristic values of (Kar & Karnick, 2012)."}, {"heading": "5.3. Run-Time Analysis", "text": "Figure 7 shows the log scatter plot of the calculation times (Projection + Hessen) for random feature maps (Kar & Karnick, 2012), tensor sketches (Pham & Pagh, 2013) and CRAFTMaps using random feature maps (with H-01 heuristics) recorded for MNIST data using a 40-core machine. Note that CRAFTMaps show significant improvements in classification per unit at the right end of the x-axis, as the Hessian calculation costs dominate with increasing size of the projected space."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper, we proposed CRAFTMaps to approximate the polynomial cores more precisely and accurately compared to previous approaches. We have theoretically demonstrated the error limits of CRAFTMaps and presented empirical results to demonstrate its effectiveness. An important context in which CRAFTMaps are particularly useful is the attitude to map reduction. By calculating a single Hessian matrix (with different gradients for each ECOC) in a compact feature room, CRAFTMaps provide an effective way to learn multiple class classifiers in a single pass over large amounts of data. Furthermore, CRAFTMaps is suitable for smaller applications such as mobile phone apps due to its ability to capture its own structure in a compact way."}], "references": [{"title": "Sampling Techniques for Kernel Methods", "author": ["D. Achlioptas", "F. McSherry", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Achlioptas et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Achlioptas et al\\.", "year": 2002}, {"title": "Theoretical foundations of the potential function method in pattern recognition learning", "author": ["A. Aizerman", "E.M. Braverman", "L.I. Rozoner"], "venue": "Automation and Remote Control,", "citeRegEx": "Aizerman et al\\.,? \\Q1964\\E", "shortCiteRegEx": "Aizerman et al\\.", "year": 1964}, {"title": "Predictive low-rank decomposition for kernel methods", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Bach and Jordan,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2005}, {"title": "The Curse of Highly Variable Functions for Local Kernel Machines", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Random Projection, Margins, Kernels, and Feature-Selection", "author": ["A. Blum"], "venue": "In Subspace, Latent Structure and Feature Selection. Springer,", "citeRegEx": "Blum,? \\Q2006\\E", "shortCiteRegEx": "Blum", "year": 2006}, {"title": "Sharp inequalities for martingales and stochastic integrals", "author": ["D.L. Burkholder"], "venue": "Asterisque,", "citeRegEx": "Burkholder,? \\Q1988\\E", "shortCiteRegEx": "Burkholder", "year": 1988}, {"title": "LIBSVM: A Library for Support Vector Machines", "author": ["C. Chang", "C. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin", "year": 2011}, {"title": "Solving Multiclass Learning Problems via Error-Correcting Output Codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri,? \\Q1994\\E", "shortCiteRegEx": "Dietterich and Bakiri", "year": 1994}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney", "year": 2005}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan,? \\Q2012\\E", "shortCiteRegEx": "Golub and Loan", "year": 2012}, {"title": "Probability Inequalities for Sums of Bounded Random Variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Satistical Association,", "citeRegEx": "Hoeffding,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "In Proceedings of the 30th annual ACM Symposium on the Theory of Computing,", "citeRegEx": "Indyk and Motwani,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani", "year": 1998}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "In Conference on Modern Analysis and Probability,", "citeRegEx": "Johnson and Lindenstrauss,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss", "year": 1984}, {"title": "Random Feature Maps for Dot Product Kernels", "author": ["P. Kar", "H. Karnick"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kar and Karnick,? \\Q2012\\E", "shortCiteRegEx": "Kar and Karnick", "year": 2012}, {"title": "Random Fourier Approximations for Skewed Multiplicative Histogram Kernels", "author": ["F. Li", "C. Ionescu", "C. Sminchisescu"], "venue": "In Pattern Recognition. Springer,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Max-Margin Additive Classiers for Detection", "author": ["S. Maji", "A.C. Berg"], "venue": "In IEEE 12th International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Maji and Berg,? \\Q2009\\E", "shortCiteRegEx": "Maji and Berg", "year": 2009}, {"title": "Fast and Scalable Polynomial Kernels via Explicit Feature Maps", "author": ["N. Pham", "R. Pagh"], "venue": "In Proceedings of the 19th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Pham and Pagh,? \\Q2013\\E", "shortCiteRegEx": "Pham and Pagh", "year": 2013}, {"title": "Using Analytic QP and Sparseness to Speed Training of Support Vector Machines", "author": ["J.C. Platt"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "Random Features for LargeScale Kernel Machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Rahimi and Recht,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "Advances in Kernel Methods: Support Vector Learning", "author": ["B. Sch\u00f6lkopf", "C.J.C. Burges", "Smola", "A.J. (eds"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "Fast Gaussian Process Regression using KD-Trees", "author": ["Y. Shen", "A. Ng", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Shen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2005}, {"title": "Sparseness of Support Vector Machines", "author": ["I. Steinwart"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Steinwart,? \\Q2003\\E", "shortCiteRegEx": "Steinwart", "year": 2003}, {"title": "Improved analysis of the subsampled randomized Hadamard transform", "author": ["J.A. Tropp"], "venue": "Advances in Adaptive Data Analysis,", "citeRegEx": "Tropp,? \\Q2011\\E", "shortCiteRegEx": "Tropp", "year": 2011}, {"title": "Core vector machines: Fast SVM training on very large data sets", "author": ["I.W. Tsang", "J.T. Kwok", "P. Cheung"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsang et al\\.", "year": 2006}, {"title": "Generalized RBF feature maps for efficient detection", "author": ["S. Vempati", "A. Vedaldi", "A. Zisserman", "C.V. Jawahar"], "venue": "In 21st British Machine Vision Conference,", "citeRegEx": "Vempati et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vempati et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": "Kernel methods allow implicitly learning nonlinear functions using explicit linear feature spaces (Sch\u00f6lkopf et al., 1999).", "startOffset": 98, "endOffset": 122}, {"referenceID": 1, "context": "One solution to this problem is the well known kernel trick (Aizerman et al., 1964), where instead of directly learning a hyperplane classifier in R, one considers a non-linear mapping \u03a6 : R \u2192 H, such that for all x,y \u2208 R, \u3008\u03a6(x),\u03a6(y)\u3009H = K(x,y) for some kernel K(x,y).", "startOffset": 60, "endOffset": 83}, {"referenceID": 21, "context": "It has been observed however that with increase in training data size, the support of the vector w can undergo unbounded growth, which can result in increased training as well as testing time (Steinwart, 2003) (Bengio et al.", "startOffset": 192, "endOffset": 209}, {"referenceID": 3, "context": "It has been observed however that with increase in training data size, the support of the vector w can undergo unbounded growth, which can result in increased training as well as testing time (Steinwart, 2003) (Bengio et al., 2006).", "startOffset": 210, "endOffset": 231}, {"referenceID": 22, "context": "Note that while down-projection using structured random matrices is straight forward (Tropp, 2011), we need to incorporate a few novel modifications to previous structured random projection approaches before they can be used for the up-projection step (see \u00a7 3.", "startOffset": 85, "endOffset": 98}, {"referenceID": 17, "context": "The most popular of these approaches are decomposition methods for solving Support Vector Machines (Platt, 1999) (Chang & Lin, 2011).", "startOffset": 99, "endOffset": 112}, {"referenceID": 4, "context": "To solve this challenge, several schemes have been proposed to explicitly approximate the kernel matrix, including low-rank approximations (Blum, 2006) (Bach & Jordan, 2005), sampling individual entries (Achlioptas et al.", "startOffset": 139, "endOffset": 151}, {"referenceID": 0, "context": "To solve this challenge, several schemes have been proposed to explicitly approximate the kernel matrix, including low-rank approximations (Blum, 2006) (Bach & Jordan, 2005), sampling individual entries (Achlioptas et al., 2002), or discarding entire rows (Drineas & Mahoney, 2005).", "startOffset": 203, "endOffset": 228}, {"referenceID": 20, "context": "Similarly, fast nearest neighbor lookup methods have been used to approximate multiplication operations with the kernel matrix (Shen et al., 2005).", "startOffset": 127, "endOffset": 146}, {"referenceID": 23, "context": "Moreover, formulations leveraging concepts from computational geometry have been explored to obtain efficient approximate solutions for SVM learning (Tsang et al., 2006).", "startOffset": 149, "endOffset": 169}, {"referenceID": 14, "context": "Besides (Rahimi & Recht, 2007), there have been several approaches proposed to approximate other kernels such as group invariant (Li et al., 2010), intersection (Maji & Berg, 2009), and RBF kernels (Vempati et al.", "startOffset": 129, "endOffset": 146}, {"referenceID": 24, "context": ", 2010), intersection (Maji & Berg, 2009), and RBF kernels (Vempati et al., 2010).", "startOffset": 59, "endOffset": 81}, {"referenceID": 10, "context": "Recall that the following result obtained using an application of the Hoeffding inequality (Hoeffding, 1963) is central to the analysis of (Kar & Karnick, 2012):", "startOffset": 91, "endOffset": 108}, {"referenceID": 5, "context": "Assuming k \u2265 2, and using Marcinkiewicz\u2013Zygmund inequality (Burkholder, 1988) we have:", "startOffset": 59, "endOffset": 77}, {"referenceID": 22, "context": "To employ Hadmard matrices for efficient CRAFTMaps generation, we use the sub-sampled randomized Hadamard transform (SRHT) (Tropp, 2011).", "startOffset": 123, "endOffset": 136}], "year": 2013, "abstractText": "Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results.", "creator": "LaTeX with hyperref package"}}}