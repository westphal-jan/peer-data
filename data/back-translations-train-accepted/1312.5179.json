{"id": "1312.5179", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited", "abstract": "Hypergraphs allow one to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions. In this paper, we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element is a family of regularization functionals based on the total variation on hypergraphs.", "histories": [["v1", "Wed, 18 Dec 2013 15:35:32 GMT  (62kb,D)", "http://arxiv.org/abs/1312.5179v1", "Long version of paper accepted at NIPS 2013"]], "COMMENTS": "Long version of paper accepted at NIPS 2013", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["matthias hein 0001", "simon setzer", "leonardo jost", "syama sundar rangapuram"], "accepted": true, "id": "1312.5179"}, "pdf": {"name": "1312.5179.pdf", "metadata": {"source": "CRF", "title": "The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited", "authors": ["Matthias Hein", "Simon Setzer", "Leonardo Jost"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2 The Total Variation on Hypergraphs", "text": "A large class of graph-based algorithms in semi-supervised learning and clustering is based either explicitly or implicitly on the intersection. Therefore, in Section 2.1, we will first discuss the hypergraph section and its approximations. In Section 2.2, we will introduce the total variation of the hypergraph as a Lovasz extension of the hypergraph section, analogous to the diagrams."}, {"heading": "2.1 Hypergraphs, Graphs and Cuts", "text": "In fact, most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves."}, {"heading": "2.2 The Total Variation on Hypergraphs", "text": "In this area, we are able to respond to the question of whether we will be able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "3 Semi-supervised Learning", "text": "With the regularization functions derived in the last section, we can immediately write a formulation for two-class, semi-supervised learning on hypergraphs similar to the known approaches of [19, 20]. Given the label set L, we construct the vector Y-Rn with Yi = 0 if i / o Land Yi equals the label in {\u2212 1, 1} if i-L. We propose to solve the problem of convex optimization efficiently for cases p = 1 and p = 2. Note, however, that loss functions other than the square loss could be used, (3) where \u03bb > 0 is the regularization parameter. In Section 5, we discuss how this problem of convex optimization can be efficiently solved for cases p = 1 and p = 2. Note, however, that loss functions other than the square loss could be used. However, the regularizer aims to merge the function, and we use the labeling {-1 is that \u2212 1."}, {"heading": "4 Balanced Hypergraph Cuts", "text": "In fact, it is such that it is a matter of a way in which people act in the real world in the real world in which they live, in the real world in which they live, in the real world in which they live, in the real world in which they live, in which they live, in the real world in which they live, in which they live, in which they live, in the real world in which they live, in the real world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, in the real world, in the real world, in the real world, in the real world, in the real world, in which they live, in the real world, in which they live, in the real world, in the real world, in which they live, in the real world, in which they"}, {"heading": "5 Algorithms for the Total Variation on Hypergraphs", "text": "The problem (3) we want to solve is the problem (4) of RatioDCA, which has a common structure. (4) They are the sum of the convex functionalities, in which one of them is the new regulatory term in the objective function. (4) We propose to solve these problems using a primary dual algorithm called PDHG in this paper. (4) Its main idea is to calculate the proximal map proxg (x) = arg min x Rn {1 2-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-proximal map proxg-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "6 Experiments", "text": "It is the only solution we can afford if we are able to quantify the numerical characteristics in 10 bins of the same size. Two datasets are created each with two classes (4.5 and 6.7) of the original dataset."}, {"heading": "Acknowledgments", "text": "M.H. thanks for the support by the ERC Starting Grant NOLEPRO and L.J. thanks for the support by the DFG SPP-1324."}], "references": [{"title": "Video object segmentation by hypergraph cut", "author": ["Y. Huang", "Q. Liu", "D. Metaxas"], "venue": "In CVPR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Higher order motion models and spectral clustering", "author": ["P. Ochs", "T. Brox"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Hypergraphs and cellular networks", "author": ["S. Klamt", "U.-U. Haus", "F. Theis"], "venue": "PLoS Computational Biology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "A hypergraph-based learning algorithm for classifying gene expression and arraycgh data with prior knowledge", "author": ["Z. Tian", "T. Hwang", "R. Kuang"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Clustering categorical data: an approach based on dynamical systems", "author": ["D. Gibson", "J. Kleinberg", "P. Raghavan"], "venue": "VLDB Journal,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Music recommendation by unified hypergraph: Combining social media information and music content", "author": ["J. Bu", "S. Tan", "C. Chen", "C. Wang", "H. Wu", "L. Zhang", "X. He"], "venue": "In Proc. of the Int. Conf. on Multimedia (MM),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Multi-way clustering using super-symmetric non-negative tensor factorization", "author": ["A. Shashua", "R. Zass", "T. Hazan"], "venue": "In ECCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Pellilo. A game-theoretic approach to hypergraph clustering", "author": ["M.S. Rota Bulo"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Efficient hypergraph clustering", "author": ["M. Leordeanu", "C. Sminchisescu"], "venue": "In AISTATS, pages 676\u2013684,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Beyond pairwise clustering", "author": ["S. Agarwal", "J. Lim", "L. Zelnik-Manor", "P. Petrona", "D.J. Kriegman", "S. Belongie"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Learning with hypergraphs: Clustering, classification, and embedding", "author": ["D. Zhou", "J. Huang", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Higher order learning with graphs", "author": ["S. Agarwal", "K. Branson", "S. Belongie"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Modeling hypergraphs by graphs with the same mincut properties", "author": ["E. Ihler", "D. Wagner", "F. Wagner"], "venue": "Information Processing Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "An inverse power method for nonlinear eigenproblems with applications in 1spectral clustering and sparse PCA", "author": ["M. Hein", "T. B\u00fchler"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Total variation and Cheeger cuts", "author": ["A. Szlam", "X. Bresson"], "venue": "In ICML, pages 1039\u20131046,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Beyond spectral clustering - tight relaxations of balanced graph cuts", "author": ["M. Hein", "S. Setzer"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Constrained fractional set programs and their application in local clustering and community detection", "author": ["T. B\u00fchler", "S. Rangapuram", "S. Setzer", "M. Hein"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Learning with submodular functions: A convex optimization perspective", "author": ["F. Bach"], "venue": "CoRR, abs/1111.6453,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Semi-supervised learning on manifolds", "author": ["M. Belkin", "P. Niyogi"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. Patt. Anal. Mach. Intell.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science", "author": ["E. Esser", "X. Zhang", "T.F. Chan"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["A. Chambolle", "T. Pock"], "venue": "J. of Math. Imaging and Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "A primaldual splitting method for convex optimization involving lipschitzian, proximable and linear composite terms", "author": ["L. Condat"], "venue": "J. Optimization Theory and Applications,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "On Linear-Time algorithms for the continuous quadratic knapsack problem", "author": ["K. Kiwiel"], "venue": "J. Opt. Theory Appl.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Signal recovery by proximal forward-backward splitting", "author": ["P.L. Combettes", "V.R. Wajs"], "venue": "Multiscale Modeling and Simulation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.", "startOffset": 76, "endOffset": 82}, {"referenceID": 1, "context": "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.", "startOffset": 76, "endOffset": 82}, {"referenceID": 2, "context": "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.", "startOffset": 99, "endOffset": 105}, {"referenceID": 3, "context": "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.", "startOffset": 99, "endOffset": 105}, {"referenceID": 4, "context": "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.", "startOffset": 132, "endOffset": 138}, {"referenceID": 5, "context": "It has been recognized in several application areas such as computer vision [1, 2], bioinformatics [3, 4] and information retrieval [5, 6] that such higher-order relations are available and help to improve the learning performance.", "startOffset": 132, "endOffset": 138}, {"referenceID": 6, "context": "The first one uses tensor methods for clustering as the higher-order extension of matrix (spectral) methods for graphs [7, 8, 9].", "startOffset": 119, "endOffset": 128}, {"referenceID": 7, "context": "The first one uses tensor methods for clustering as the higher-order extension of matrix (spectral) methods for graphs [7, 8, 9].", "startOffset": 119, "endOffset": 128}, {"referenceID": 8, "context": "The first one uses tensor methods for clustering as the higher-order extension of matrix (spectral) methods for graphs [7, 8, 9].", "startOffset": 119, "endOffset": 128}, {"referenceID": 9, "context": "The second main approach can deal with arbitrary hypergraphs [10, 11].", "startOffset": 61, "endOffset": 69}, {"referenceID": 10, "context": "The second main approach can deal with arbitrary hypergraphs [10, 11].", "startOffset": 61, "endOffset": 69}, {"referenceID": 11, "context": "The two main ways of approximating the hypergraph by a standard graph are the clique and the star expansion which were compared in [12].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "One can summarize [12] by stating that no approximation fully encodes the hypergraph structure.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "Earlier, [13] have proven that an exact representation of the hypergraph via a graph retaining its cut properties is impossible.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "In Section 4, we show in line of recent research [14, 15, 16, 17] that there exists a tight relaxation of the normalized hypergraph cut.", "startOffset": 49, "endOffset": 65}, {"referenceID": 14, "context": "In Section 4, we show in line of recent research [14, 15, 16, 17] that there exists a tight relaxation of the normalized hypergraph cut.", "startOffset": 49, "endOffset": 65}, {"referenceID": 15, "context": "In Section 4, we show in line of recent research [14, 15, 16, 17] that there exists a tight relaxation of the normalized hypergraph cut.", "startOffset": 49, "endOffset": 65}, {"referenceID": 16, "context": "In Section 4, we show in line of recent research [14, 15, 16, 17] that there exists a tight relaxation of the normalized hypergraph cut.", "startOffset": 49, "endOffset": 65}, {"referenceID": 10, "context": "In order to handle hypergraphs with existing methods developed for graphs, the focus in previous works [11, 12] has been on transforming the hypergraph into a graph.", "startOffset": 103, "endOffset": 111}, {"referenceID": 11, "context": "In order to handle hypergraphs with existing methods developed for graphs, the focus in previous works [11, 12] has been on transforming the hypergraph into a graph.", "startOffset": 103, "endOffset": 111}, {"referenceID": 10, "context": "In [11], they suggest using the clique expansion (CE), i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "We omit the discussion of the star graph approximation of hypergraphs discussed in [12] as it is shown there that the star graph expansion is very similar to the clique expansion.", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "[13] which states that in general there exists no graph with the same vertex set V which has for every partition (C,C) the same cut value as the hypergraph cut.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "It is well-known that the Lovasz extension S is a convex function if and only if \u015c is submodular [18].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "For graphs G = (V,W ), the total variation on graphs is defined as the Lovasz extension of the graph cut [18] given as TVG : R|V | \u2192 R, TVG(f) = 1 2 \u2211n i,j=1 wij |fi \u2212 fj |.", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "With the regularization functionals derived in the last section, we can immediately write down a formulation for two-class semi-supervised learning on hypergraphs similar to the well-known approaches of [19, 20].", "startOffset": 203, "endOffset": 211}, {"referenceID": 19, "context": "With the regularization functionals derived in the last section, we can immediately write down a formulation for two-class semi-supervised learning on hypergraphs similar to the well-known approaches of [19, 20].", "startOffset": 203, "endOffset": 211}, {"referenceID": 20, "context": "Clearly, this difference carries over to the famous normalized cut criterion introduced in [21, 22] for clustering of graphs with applications in image segmentation.", "startOffset": 91, "endOffset": 99}, {"referenceID": 21, "context": "Clearly, this difference carries over to the famous normalized cut criterion introduced in [21, 22] for clustering of graphs with applications in image segmentation.", "startOffset": 91, "endOffset": 99}, {"referenceID": 13, "context": "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.", "startOffset": 42, "endOffset": 58}, {"referenceID": 14, "context": "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.", "startOffset": 42, "endOffset": 58}, {"referenceID": 15, "context": "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.", "startOffset": 42, "endOffset": 58}, {"referenceID": 16, "context": "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.", "startOffset": 42, "endOffset": 58}, {"referenceID": 21, "context": "Thus, we follow a recent line of research [14, 15, 16, 17] where it has been shown that the standard spectral relaxation of the normalized cut used in spectral clustering [22] is loose and that a tight, in fact exact, relaxation can be formulated in terms of a nonlinear eigenproblem.", "startOffset": 171, "endOffset": 175}, {"referenceID": 15, "context": "Other examples of balancing functions can be found in [16].", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "Our following result shows that the balanced hypergraph cut also has an exact relaxation into a continuous nonlinear eigenproblem [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "Noting that both cutH(C,C) and \u015c(C) vanish on the full set V , the proof then follows from the recent result [17], which shows in this case the equivalence between the set problem and the continuous problem written in terms of the Lovasz extensions.", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "As discussed in [16], every Lovasz extension S can be written as a difference of convex positively 1-homogeneous functions1 S = S1 \u2212 S2.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "We employ the RatioDCA algorithm [16] shown in Algorithm 1.", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "For the general case, see [16].", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "We propose to solve these problems using a primal-dual algorithm, denoted PDHG in this paper, which was proposed in [23, 24].", "startOffset": 116, "endOffset": 124}, {"referenceID": 23, "context": "We propose to solve these problems using a primal-dual algorithm, denoted PDHG in this paper, which was proposed in [23, 24].", "startOffset": 116, "endOffset": 124}, {"referenceID": 22, "context": "For convergence proofs we refer to [23, 24].", "startOffset": 35, "endOffset": 43}, {"referenceID": 23, "context": "For convergence proofs we refer to [23, 24].", "startOffset": 35, "endOffset": 43}, {"referenceID": 0, "context": "Algorithm 2 PDHG 1: Initialization: f (0) = f\u0304 (0) = 0, \u03b8 \u2208 [0, 1], \u03c3, \u03c4 > 0 with \u03c3\u03c4 < 1/\u2016K\u20162 2: repeat 3: \u03b1 = prox\u03c3F\u2217(\u03b1 (k) + \u03c3Kf\u0304 ) 4: f (k+1) = prox\u03c4G(f)(f (k) \u2212 \u03c4K(\u03b1)) 5: f\u0304 (k+1) = f (k+1) + \u03b8(f (k+1) \u2212 f ) 6: until relative duality gap < 7: Output: f .", "startOffset": 60, "endOffset": 66}, {"referenceID": 24, "context": "However, note that smooth convex terms can also be directly exploited [25].", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": ", [26].", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "Algorithm 3 PDHG for \u03a9H,1 1: Initialization: f (0) = f\u0304 (0) = 0, \u03b8 \u2208 [0, 1], \u03c3, \u03c4 > 0 with \u03c3\u03c4 < 1/(2 maxi=1,.", "startOffset": 69, "endOffset": 75}, {"referenceID": 0, "context": "Algorithm 5 PDHG for \u03a9H,2 1: Initialization: f (0) = f\u0304 (0) = 0, \u03b8 \u2208 [0, 1], \u03c3, \u03c4 > 0 with \u03c3\u03c4 < 1/maxi=1,.", "startOffset": 69, "endOffset": 75}, {"referenceID": 10, "context": "The method of Zhou et al [11] seems to be the standard algorithm for clustering and SSL on hypergraphs.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "Zoo, Mushrooms and 20Newsgroups2 have been used also in [11] and contain only categorical features.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "As in [11], a hyperedge of weight one is created by all data points which have the same value of a categorical feature.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "In [11], they suggest using a regularizer induced by the normalized Laplacian LCE arising from the clique expansion LCE = I\u2212D \u2212 12 CEHW \u2032HTD \u2212 1 2 CE , where DCE is a diagonal matrix with entries dEC(i) = \u2211 e\u2208E Hi,e we |e| and W \u2032 \u2208 R|E|\u00d7|E| is a diagonal matrix with entries w\u2032(e) = we/|e|.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Our SSL methods based on \u03a9H,p, p = 1, 2 outperform consistently the clique expansion technique of Zhou et al [11] on all datasets except 20newsgroups3.", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "For comparison we use the normalized spectral clustering approach based on the Laplacian LCE [11](clique expansion).", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "The number k is chosen as the smallest number for which the graph becomes connected and we compare results of normalized 1-spectral clustering [14] and the standard spectral clustering [22].", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "The number k is chosen as the smallest number for which the graph becomes connected and we compare results of normalized 1-spectral clustering [14] and the standard spectral clustering [22].", "startOffset": 185, "endOffset": 189}, {"referenceID": 10, "context": "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "Clustering Error % Hypergraph Ncut Graph(CE) Ncut Clustering Error % kNN-Graph Ncut Dataset Ours [11] Ours [11] Ours [11] [14] [22] [14] [22] Mushrooms 10.", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "Communications with the authors of [11] could not clarify the difference to their results on 20newsgroups", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "Moreover, our method sometimes has even smaller cuts on the graphs resulting from the clique expansion, although it does not directly optimize this objective in contrast to [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "Again, we could not run the method of [11] on covertype (6,7) since the weight matrix is very dense.", "startOffset": 38, "endOffset": 42}], "year": 2013, "abstractText": "Hypergraphs allow one to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions. In this paper, we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element is a family of regularization functionals based on the total variation on hypergraphs.", "creator": "LaTeX with hyperref package"}}}