{"id": "1312.5258", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "On the Challenges of Physical Implementations of RBMs", "abstract": "Restricted Boltzmann machines (RBMs) are powerful machine learning models, but learning and some kinds of inference in the model require sampling-based approximations, which, in classical digital computers, are implemented using expensive MCMC. Physical computation offers the opportunity to reduce the cost of sampling by building physical systems whose natural dynamics correspond to drawing samples from the desired RBM distribution. Such a system avoids the burn-in and mixing cost of a Markov chain. However, hardware implementations of this variety usually entail limitations such as low-precision and limited range of the parameters and restrictions on the size and topology of the RBM. We conduct software simulations to determine how harmful each of these restrictions is. Our simulations are designed to reproduce aspects of the D-Wave quantum computer, but the issues we investigate arise in most forms of physical computation.", "histories": [["v1", "Wed, 18 Dec 2013 18:30:51 GMT  (720kb,D)", "http://arxiv.org/abs/1312.5258v1", "Conference, prepared for ICLR 2014"], ["v2", "Fri, 24 Oct 2014 19:16:14 GMT  (630kb,D)", "http://arxiv.org/abs/1312.5258v2", null]], "COMMENTS": "Conference, prepared for ICLR 2014", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["vincent dumoulin", "ian j goodfellow", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1312.5258"}, "pdf": {"name": "1312.5258.pdf", "metadata": {"source": "META", "title": "On the Challenges of Physical Implementations of RBMs", "authors": [], "emails": [], "sections": [{"heading": null, "text": "000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 030 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054055 056 057 058 059 060 061 062 063 064 065 066 067 068 070 071 072 073 074 075 076 077 078 079 080 081 082 084 085 086 087 088 089 090 092 093 094 095 096 098 099 100 101 102 103 104 105 107 108 109 072 073 075 077 079 079 080 081 082 085 086 088 089 090 092 092 092 093 094 095 096 096 097 098 098 099 099 100 101 103 104 105 107 107 109 071 072 072 072 073 074 047 047 047 046 047 047 047 047 047 047 047 048 048 048 053 053 053 053 053 053 053 053 053 053 053 053 053 053 056 053 053 053 053 035 035 035 035 035 036 036 036 036 036 036 036 036 036 036 036 036 036 036 036 036 036 038 038 042 042 042 042 042 042 042 043 044 043"}, {"heading": "1. Introduction", "text": "One of the main difficulties limiting its effectiveness is that the log probability of the model is only insoluble (Long and Servedio, 2010).The model can be trained to gradient the log probability using sample-based approaches (Younes, 1998; Tieleman, 2008).However, drawing a fair sample from the model is also intractable (Long and Servedio, 2010).Drawing samples from an RBM on a classical digital computer is an active area of research (Salakhutdinov, 2010a; Desjardins et al., 2010; Cho et al., 2010).Existing approaches are based on Markov chain Monte Carlo methods.The cost of drawing a fair sample using a Monte-Preliminary work."}, {"heading": "2. Background and related work", "text": "In this section, we provide some background information on RBMs and the D-Wave Two system, including previous approaches to implementing RBMs using D-Wave systems."}, {"heading": "2.1. Restricted Boltzmann machines", "text": "A restricted expectation after 100,000 milliseconds (RBM) is a probability distribution over a vector of visible units v = 458 (RBM). (RBM) It is a probability distribution over a vector of visible units v = 458 (RBM). (RBM uses an energy function E to represent a probability mass function: 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7. (RBM) An energy function is used which is calculated by sampling with no additional parameter noise from RBMs. (RBM) -1 0.4 0.6 0.7. (M) 5 models were trained with the same hyperparameters but with different seeds. (a) Trains = 0.10 (b) Trains = 0.30 (c) Trains = 0.70 figures 6. Conditional expectation samples after 100,000 milliseconds. (RBM) Three RBMs were trained with different parameters but with different moves (SM = 0.70 moves). (BM = 0.70 moves)."}, {"heading": "2.2. The D-Wave machine", "text": "Dre rf\u00fc nde nlrfhUeae\u00fccnlrrf\u00fc ide nlrrfhUeaeae\u00fccnlrg\u00dfe\u00fccnlhsrtee\u00fccnllrrrf\u00fc rfhsrrrf\u00fc ide rfhsrc\u00fc\u00fceaeeeFnlrh ni rfhree\u00fccrrrrrrrrrrfteerni nlrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrlrlrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "3. Methodological notes", "text": "All models in this paper were trained with PCD-5 with the standard MNIST training set. Training examples were binarized each time they were presented by samples from a binomial distribution. Unless explicitly stated, all models were trained with the same hyperparameters. The negative logic probability of all models in this paper is approximately calculated using AIS. If you add noise to the parameters, the expected AIS of Monte Carlo is calculated using the same binarized method as for training examples. 660 661 662 663 664 665 666 667 667 668 670 672 673 674 675 676 676 676 676 677 678 678 679 680 681 682 683 684 685 685 686 687 689 691 691 691 691 691 691 692 662 664 664 665 667 67 67 67 67 67 67 67 669 69 669 69 69 69 69 69 69 69 69 69 69 69 69 672 675 74 74 74 74 74 74 74 74 76 76 676 676 676 677 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 70 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 771 770 770 770 770 770 770 770 770 770 770 770 770 771 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 771 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 771 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770 770"}, {"heading": "4. Simulating limitations", "text": "We now describe our experiments using simulations of physical calculations."}, {"heading": "4.1. Noisy parameters", "text": "In this case, we know that the parameters we acquire are not to be surpassed in terms of selection results in this area. \"We have to be prepared for the selection results we acquire,\" he says, \"but we have to be prepared for the fact that the selection results we acquire are not capable of rethinking the selection results in terms of selection results.\" It is about the selection results in this area: \"It is about the question of whether the selection results we acquire, which we have to take care of.\" \"It is about the selection results we take care of, which we take care of, which we take care of, which we have to take care of.\""}, {"heading": "4.2. Limited parameter range", "text": "We are now turning our attention to parameter range constraint. We trained RBMs by forcing their parameter size to remain below a certain threshold, and observed the effect of this value on the NLL test (Fig. 7). Whenever parameter updates would bring a parameter outside this range, it was cut to the threshold. We find that a size constraint greater than or equal to 1.0 has little or no effect on performance, but that the size of a parameter must be smaller than this quickly degrades performance. Samples tend to be blurred by narrowing the range of allowed parameter values (Fig. 8)."}, {"heading": "4.3. Combining noise and limited parameter range", "text": "We combined noise and size restrictions with each other to see how they interact The two restrictions seem to work well together. In fact, a model with high noise and small parameter values works almost as well as a standard RBM. We believe that the restriction of parameter values can actually be helpful because they force the RBM to find good weight vector directions that generalize well, rather than simply scale its weights to override the noise.770 771 772 773 774 775 776 778 779 780 781 782 783 784 785 786 788 789 790 792 793 764 795 797 798 798 755 800 801 802 803 804 865 857 806 860 860 835 881 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 828 850 850 850 850 850 64 850 64 850 850 850 850 850 850 850 850 850 850 850 850 842 850 850 850 850 850 850 850 850 850 850 850 850 842 870 870 870 828 870 870 870 870 828 870 870 870 870 870 842 842 850 850 842 850 850 850 850 850 850 842 850 850 842 850 850 850 850 850 842 850 850 850 850 850 850 842 850 850 842 850 850 850 850 850 842 850 850 850 850 842 850 850 850 850 850 828 870 870 842 870 870 870 870 870 870 828 870 870 870 870 842 870 870 870 870 870 850 842 850 850 850 842 842 850 850 842 850 850 850 842 850 850 850 850 842 850 850 850 850 850 8"}, {"heading": "4.4. Limited connectivity", "text": "We trained RBMs by setting a random subset of weights to zero and observed how this would affect the structure of test NLL (Fig. 10). It turned out that the RBM can cope with a reasonable amount of removed connections: even if half of the weights are forced to be zero, test NLL only increases by about 6.6%. However, physical implementations are likely to have very poor connectivity; for example, the connectivity pattern of a D-Wave machine (Fig. 1) in samples (Fig. 11) causes over 99% of their connections to be removed. In the aforementioned experiment, 99% of distant connections lead to disappointing 199.2 \u00b1 0.1 test NLLL.When looking at samples (Fig. 11), we find that the representative power of the RBM decreases as we force more weights to be zero to a point where samples do not look like digits."}, {"heading": "5. Conclusion", "text": "In this paper, we conducted a series of simulation experiments to determine the feasibility of implementing an RBM using physical calculation. We assessed the impact of three obstacles to the success of physical calculation: noise on the model parameters, limited range on the model parameters, and limited topology of the model. We found that noise on the parameters moderately impairs the standard learning algorithms, although this can be mitigated by training with the same sampler in the negative phase with which samples are taken during the test period. We found that the limits on the range of the parameters do not significantly impair the performance of the RBM. Finally, and most importantly, we found that limitations on the model's topology can seriously impair its performance."}], "references": [{"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Y. Bengio"], "venue": "JMLR W&CP: Proc. Unsupervised and Transfer Learning challenge and workshop, volume 27, pages 17\u201336.", "citeRegEx": "Bengio,? 2012", "shortCiteRegEx": "Bengio", "year": 2012}, {"title": "Parallel tempering is efficient for learning restricted Boltzmann machines", "author": ["K. Cho", "T. Raiko", "A. Ilin"], "venue": "Proceedings of the International Joint Conference on Neural Networks (IJCNN 2010), Barcelona, Spain.", "citeRegEx": "Cho et al\\.,? 2010", "shortCiteRegEx": "Cho et al\\.", "year": 2010}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "ICML\u20192011.", "citeRegEx": "Coates and Ng,? 2011", "shortCiteRegEx": "Coates and Ng", "year": 2011}, {"title": "Toward the implementation of a quantum rbm", "author": ["M. Denil", "N. de Freitas"], "venue": "NIPS*2011 Workshop on Deep Learning and Unsupervised Feature Learning.", "citeRegEx": "Denil and Freitas,? 2011", "shortCiteRegEx": "Denil and Freitas", "year": 2011}, {"title": "Tempered Markov chain Monte Carlo for training of restricted Boltzmann machine", "author": ["G. Desjardins", "A. Courville", "Y. Bengio"], "venue": "JMLR W&CP: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010), vol-", "citeRegEx": "Desjardins et al\\.,? 2010", "shortCiteRegEx": "Desjardins et al\\.", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, 18, 1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Beitrag zur Theorie des Ferromagnetismus", "author": ["E. Ising"], "venue": "Zeitschrift fur Physik, 31, 253\u2013258.", "citeRegEx": "Ising,? 1925", "shortCiteRegEx": "Ising", "year": 1925}, {"title": "Restricted Boltzmann machines are hard to approximately evaluate or simulate", "author": ["P.M. Long", "R.A. Servedio"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML\u201910).", "citeRegEx": "Long and Servedio,? 2010", "shortCiteRegEx": "Long and Servedio", "year": 2010}, {"title": "Learning deep Boltzmann machines using adaptive MCMC", "author": ["R. Salakhutdinov"], "venue": "L. Bottou and M. Littman, editors, Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML10), volume 1, pages 943\u2013950. ACM.", "citeRegEx": "Salakhutdinov,? 2010a", "shortCiteRegEx": "Salakhutdinov", "year": 2010}, {"title": "Learning in Markov random fields using tempered transitions", "author": ["R. Salakhutdinov"], "venue": "NIPS\u20192010.", "citeRegEx": "Salakhutdinov,? 2010b", "shortCiteRegEx": "Salakhutdinov", "year": 2010}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing, volume 1, chapter 6, pages 194\u2013 281. MIT Press, Cambridge.", "citeRegEx": "Smolensky,? 1986", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008, pages 1064\u20131071. ACM.", "citeRegEx": "Tieleman,? 2008", "shortCiteRegEx": "Tieleman", "year": 2008}, {"title": "Using fast weights to improve persistent contrastive divergence", "author": ["T. Tieleman", "G. Hinton"], "venue": "ICML\u20192009.", "citeRegEx": "Tieleman and Hinton,? 2009", "shortCiteRegEx": "Tieleman and Hinton", "year": 2009}, {"title": "On the convergence of Markovian stochastic algorithms with rapidly decreasing ergodicity rates", "author": ["L. Younes"], "venue": "Stochastics and Stochastics Models, pages 177\u2013 228.", "citeRegEx": "Younes,? 1998", "shortCiteRegEx": "Younes", "year": 1998}], "referenceMentions": [{"referenceID": 10, "context": "A restricted Boltzmann machine (Smolensky, 1986) is a generative model that has found widespread application (Hinton et al.", "startOffset": 31, "endOffset": 48}, {"referenceID": 7, "context": "One of the main difficulties limiting its effectiveness is that the log likelihood of the model is intractable (Long and Servedio, 2010).", "startOffset": 111, "endOffset": 136}, {"referenceID": 13, "context": "The model may be trained using sampling-based approximations to the gradient of the log likelihood (Younes, 1998; Tieleman, 2008) however, drawing a fair sample from the model is also intractable (Long and Servedio, 2010).", "startOffset": 99, "endOffset": 129}, {"referenceID": 11, "context": "The model may be trained using sampling-based approximations to the gradient of the log likelihood (Younes, 1998; Tieleman, 2008) however, drawing a fair sample from the model is also intractable (Long and Servedio, 2010).", "startOffset": 99, "endOffset": 129}, {"referenceID": 7, "context": "The model may be trained using sampling-based approximations to the gradient of the log likelihood (Younes, 1998; Tieleman, 2008) however, drawing a fair sample from the model is also intractable (Long and Servedio, 2010).", "startOffset": 196, "endOffset": 221}, {"referenceID": 8, "context": "Drawing samples from an RBM on a classical digital computer is an active area of research (Salakhutdinov, 2010a; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 90, "endOffset": 155}, {"referenceID": 4, "context": "Drawing samples from an RBM on a classical digital computer is an active area of research (Salakhutdinov, 2010a; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 90, "endOffset": 155}, {"referenceID": 1, "context": "Drawing samples from an RBM on a classical digital computer is an active area of research (Salakhutdinov, 2010a; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 90, "endOffset": 155}, {"referenceID": 13, "context": "The persistent contrastive divergence (PCD) algorithm (also known as stochastic maximum likelihood) (Younes, 1998; Tieleman, 2008) uses a persistent Gibbs (MCMC) sampling scheme that sequentially samples from the conditionals p(h | v) and p(v | h) to recover samples from the joint distribution.", "startOffset": 100, "endOffset": 130}, {"referenceID": 11, "context": "The persistent contrastive divergence (PCD) algorithm (also known as stochastic maximum likelihood) (Younes, 1998; Tieleman, 2008) uses a persistent Gibbs (MCMC) sampling scheme that sequentially samples from the conditionals p(h | v) and p(v | h) to recover samples from the joint distribution.", "startOffset": 100, "endOffset": 130}, {"referenceID": 12, "context": "ing issue include the use of auxilliary parameters (Tieleman and Hinton, 2009) and tempering methods (Salakhutdinov, 2010b; Desjardins et al.", "startOffset": 51, "endOffset": 78}, {"referenceID": 9, "context": "ing issue include the use of auxilliary parameters (Tieleman and Hinton, 2009) and tempering methods (Salakhutdinov, 2010b; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 101, "endOffset": 166}, {"referenceID": 4, "context": "ing issue include the use of auxilliary parameters (Tieleman and Hinton, 2009) and tempering methods (Salakhutdinov, 2010b; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 101, "endOffset": 166}, {"referenceID": 1, "context": "ing issue include the use of auxilliary parameters (Tieleman and Hinton, 2009) and tempering methods (Salakhutdinov, 2010b; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 101, "endOffset": 166}, {"referenceID": 6, "context": "The D-Wave Two system implements an Ising model (Ising, 1925).", "startOffset": 48, "endOffset": 61}], "year": 2017, "abstractText": "Restricted Boltzmann machines (RBMs) are powerful machine learning models, but learning and some kinds of inference in the model require sampling-based approximations, which, in classical digital computers, are implemented using expensive MCMC. Physical computation offers the opportunity to reduce the cost of sampling by building physical systems whose natural dynamics correspond to drawing samples from the desired RBM distribution. Such a system avoids the burn-in and mixing cost of a Markov chain. However, hardware implementations of this variety usually entail limitations such as lowprecision and limited range of the parameters and restrictions on the size and topology of the RBM. We conduct software simulations to determine how harmful each of these restrictions is. Our simulations are designed to reproduce aspects of the D-Wave quantum computer, but the issues we investigate arise in most forms of physical computation.", "creator": "LaTeX with hyperref package"}}}