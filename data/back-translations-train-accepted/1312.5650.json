{"id": "1312.5650", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Zero-Shot Learning by Convex Combination of Semantic Embeddings", "abstract": "Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the semantic embedding space is trained jointly with the image transformation, while in other cases the semantic embedding space is established independently by a separate task, such as a natural language processing task on a text corpus, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional n-way classification framing of image understanding, particularly in terms of the promise of zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. Here we propose a simple method for constructing an image embedding system from any existing n-way image classification mechanism and any existing semantic embedding space which contains the n class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional learning. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.", "histories": [["v1", "Thu, 19 Dec 2013 17:30:31 GMT  (18kb)", "http://arxiv.org/abs/1312.5650v1", null], ["v2", "Fri, 20 Dec 2013 23:30:47 GMT  (92kb,D)", "http://arxiv.org/abs/1312.5650v2", null], ["v3", "Fri, 21 Mar 2014 23:47:20 GMT  (93kb,D)", "http://arxiv.org/abs/1312.5650v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammad norouzi", "tomas mikolov", "samy bengio", "yoram singer", "jonathon shlens", "andrea frome", "greg s corrado", "jeffrey dean"], "accepted": true, "id": "1312.5650"}, "pdf": {"name": "1312.5650.pdf", "metadata": {"source": "CRF", "title": "Zero-Shot Learning by Convex Combination of Semantic Embeddings", "authors": ["Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S. Corrado", "Jeffrey Dean"], "emails": ["norouzi@cs.toronto.edu,", "singer}@google.com", "jeff}@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.56 50v1 [cs.LG]"}, {"heading": "1 Introduction", "text": "In fact, you have to be able to go to another world, where you can go to another world, where you can go to another world, where you can go to another world."}, {"heading": "2 Formulation", "text": "Suppose a designated training data set of Figures D0 \u2261 {(xi, yi)} mi = 1 is given unambiguously, each image being represented by a p-dimensional feature vector designated xi-X \u2261 Rp, and there are n0-different class markings available for training, i.e., yi-Y0 \u2261 {1,., n0}. In addition, a test data set designated D1 \u2261 (x-j, y-j) m \u2032 j = 1 is provided, where x-j as above, but y-j-Y1 \u2261 (n0 + 1,., n0 + n1} tic marking of the tests contains n1 different class markings that are omitted from the training set. Let n = n0 + n1 stand for the total number of markings in the training and the test sets Yy. The goal of zero-shot learning is to train a classical on the training set that is well visible on the D0 scale."}, {"heading": "3 Previous work", "text": "A key component of zero-shot learning is the way in which semantic embedding of class labels is defined. In computer vision, there has been a series of works on the use of human-labeled attributes [4, 9] to detect invisible objects. Binary attributes are most commonly used to encode the presence and absence of a set of visual attributes within an object category. Examples of the attributes include different types of materials, colors, textures and object parts. The main problem with classification based on monitored attributes is its poor scalability for large tasks. Clearly, the annotation of hundreds of attributes for tens of thousands of object classes is an ambiguous and challenging task itself, limiting the applicability of monitored attributes to large-scale zero-shot object learning. There have been recent works that include impressive zero-shot performance data bases based on these knowledge tasks [14], but also on the use of knowledge tasks."}, {"heading": "4 ConSE: Convex combination of semantic embeddings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Model Description", "text": "In contrast to the previous paper, which considered zero-point learning as a regression problem from the entrance space to the semantic embedding of the space, this paper is not explicitly a regression function f: X \u2192 S. Rather, we follow the classic machine learning approach and learn a classifier from the training inputs to the training labels. For this purpose, a classifier p0 is trained to D0 to estimate the probability of an image x belonging to a class label y. Y0 (y | x), in which there is a p0 (y = 1 p0) (y | x) = 1 (p0), we propose a method to transfer the probable predictions of the classifier beyond the training labels to a set of test labels."}, {"heading": "4.2 Difference with DeViSE", "text": "The main difference between the ConSE and the DeViSE model [6] is that the DeViSE model replaces the last layer of a Convolutionary Neural Network Classifier [7], the Softmax layer, with a linear transformation layer. Subsequently, the new transformation layer is trained using a ranking lens to map training inputs to continuous semantic embedding for the correct labels. Subsequently, the lower layers of the Convolutionary Neural Network are fine-tuned using the ranking lens to achieve better results. In contrast, the ConSE model keeps the Softmax layer of the Convolutionary Network intact and does not train the neural network any further. Considering a test image, the ConSE simply executes the classifier by Krizhevsky et al. [7] and considers the top-T predictions of the model. Subsequently, the convex combination of the corresponding T-vectors is embedded in the soft-sect space (see below) (see Equal Space)."}, {"heading": "5 Experiments", "text": "We compare our approach to the \"convex combination of semantic embedding\" (ConSE) with a \"state-of-the-art\" method called \"Deep Visual-Semantic Embedding\" (DeViSE), which is capable of surpassing itself. \"The two models\" ConSE \"and\" DeViSE, \"which define the semantic label\" Embedding space, \"each encompass the way in which they are used.\" The skipgram model was trained on 5.4 billion words from Wikipedia.org to construct 500-dimensional semantic embedding vectors. \"The embedding vectors are then standardized to have a unit called\" norm. \"In both\" ConSE \"and\" DeViSE \"the Convolutionary Neural Network of Krizhevsky et al al al al al\" is used. This neural network is used as an image classifier."}, {"heading": "6 Conclusion", "text": "The ConSE approach to allocating images to a semantic embedding space is also deceptively simple. Treating classification results as weights in a convex combination of word vectors is perhaps the most direct method imaginable to reform an n-way image classification system as an embedding system. However, this method surpasses more elaborate common approaches to both the null and performance indicators that are based on semantic quality. Undoubtedly, the success of this method lies in its ability to harness the strengths of the state-of-the-art image classifier, as well as the state-of-the-art text embedding system from which it was constructed. While it draws on its strengths, we have no reason to believe that the visual and text models from which it is constructed."}], "references": [{"title": "Cross-generalization: learning novel classes from a single example by feature replacement", "author": ["E. Bart", "S. Ullman"], "venue": "CVPR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "One-shot learning of object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "IEEE Trans. PAMI, 28:594\u2013 611", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": "NIPS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "One shot learning of simple visual concepts", "author": ["B.M. Lake", "R. Salakhutdinov", "J. Gross", "J.B. Tenenbaum"], "venue": "CogSci", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "ECCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell"], "venue": "NIPS", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Evaluating knowledge transfer and zero-shot learning in a largescale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluating knowledge transfer and zero-shot learning in a largescale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "CVPR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "H. Sridhar", "O. Bastani", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "There have been continued efforts in collecting larger image corpora with a broader coverage of object categories [3], thereby enabling image classification with many class labels.", "startOffset": 114, "endOffset": 117}, {"referenceID": 11, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 3, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 8, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 5, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 14, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 11, "context": "Our model fits into the zero-shot learning framework [12], which recently received a growing amount of attention [13, 6, 15].", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "Our model fits into the zero-shot learning framework [12], which recently received a growing amount of attention [13, 6, 15].", "startOffset": 113, "endOffset": 124}, {"referenceID": 5, "context": "Our model fits into the zero-shot learning framework [12], which recently received a growing amount of attention [13, 6, 15].", "startOffset": 113, "endOffset": 124}, {"referenceID": 14, "context": "Our model fits into the zero-shot learning framework [12], which recently received a growing amount of attention [13, 6, 15].", "startOffset": 113, "endOffset": 124}, {"referenceID": 3, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 85, "endOffset": 91}, {"referenceID": 8, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 85, "endOffset": 91}, {"referenceID": 5, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 164, "endOffset": 175}, {"referenceID": 14, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 164, "endOffset": 175}, {"referenceID": 10, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 164, "endOffset": 175}, {"referenceID": 6, "context": "By employing a state-of-the-art convolutional neural network [7] trained only on 1000 object categories from ImageNet, the ConSE model is able to achieve 9.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "When the test object classes are farther from the training classes in the ImageNet category hierarchy, the zero-shot classification results get worse, as expected, but still the ConSE model significantly outperforms the state-of-the-art model [6] applied to the same task.", "startOffset": 243, "endOffset": 246}, {"referenceID": 5, "context": ", [6, 15]) has addressed zero-shot classification by learning a mapping from input features to semantic label embedding vectors using a multivariate regression model.", "startOffset": 2, "endOffset": 9}, {"referenceID": 14, "context": ", [6, 15]) has addressed zero-shot classification by learning a mapping from input features to semantic label embedding vectors using a multivariate regression model.", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": "In computer vision, there has been a body of work on the use of human-labeled attributes [4, 9] to help detecting unseen objects.", "startOffset": 89, "endOffset": 95}, {"referenceID": 8, "context": "In computer vision, there has been a body of work on the use of human-labeled attributes [4, 9] to help detecting unseen objects.", "startOffset": 89, "endOffset": 95}, {"referenceID": 13, "context": "There has been more recent work showing impressive zero-shot performance on visual recognition tasks [14, 10], but these methods also rely on the use of knowledge bases containing descriptive properties of object classes, and the WordNet hierarchy.", "startOffset": 101, "endOffset": 109}, {"referenceID": 9, "context": "There has been more recent work showing impressive zero-shot performance on visual recognition tasks [14, 10], but these methods also rely on the use of knowledge bases containing descriptive properties of object classes, and the WordNet hierarchy.", "startOffset": 101, "endOffset": 109}, {"referenceID": 1, "context": "A more scalable approach to semantic embeddings of class labels builds upon the recent advances in unsupervised neural language modeling [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 10, "context": "The word embeddings are optimized to increase the predictability of each word given its context [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "[6] and Socher et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] exploit word embeddings to embed textual names of object class labels into a rich semantic space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In this work, we also use the skip-gram model [11] to learn class label embeddings.", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "Zero-shot learning is closely related to one-shot learning [5, 1, 8], where the goal is to learn classifiers for object categories based on a few labeled training exemplars.", "startOffset": 59, "endOffset": 68}, {"referenceID": 0, "context": "Zero-shot learning is closely related to one-shot learning [5, 1, 8], where the goal is to learn classifiers for object categories based on a few labeled training exemplars.", "startOffset": 59, "endOffset": 68}, {"referenceID": 7, "context": "Zero-shot learning is closely related to one-shot learning [5, 1, 8], where the goal is to learn classifiers for object categories based on a few labeled training exemplars.", "startOffset": 59, "endOffset": 68}, {"referenceID": 5, "context": "The key difference between the ConSE and the DeViSE [6] models is that, the DeViSE model replaces the last layer of a convolutional neural network classifier [7], the Softmax layer, with a linear transformation layer.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "The key difference between the ConSE and the DeViSE [6] models is that, the DeViSE model replaces the last layer of a convolutional neural network classifier [7], the Softmax layer, with a linear transformation layer.", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "[7], and considers the top T predictions of the model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We compare our approach, \u201cconvex combination of semantic embedding\u201d (ConSE), with a stateof-the-art method called \u201cDeep Visual-Semantic Embedding\u201d (DeViSE) [6] on the ImageNet dataset [3].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "We compare our approach, \u201cconvex combination of semantic embedding\u201d (ConSE), with a stateof-the-art method called \u201cDeep Visual-Semantic Embedding\u201d (DeViSE) [6] on the ImageNet dataset [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 10, "context": "Both of the models ConSE and DeViSE, use the same skipgram text model [11] to define the semantic label embedding space.", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": "[7] is used as the image classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We mirror the ImageNet zero-shot learning experiments of [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "As in [6], we quantify the zero-shot generalization performance of the models on three test datasets with increasing degree of difficulty.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Table 1: Flat hit@k performance of DeViSE [6] and ConSE (T ) for T = 1, 10, 1000 on ImageNet zero-shot learning task.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "Table 2: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Table 2: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task.", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "For a detailed definition of hierarchical precision@k please refer to the supplementary material of [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "[15] because Frome et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] reported that the ranking loss used in the DeViSE significantly outperforms the the squared loss used in [15].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[6] reported that the ranking loss used in the DeViSE significantly outperforms the the squared loss used in [15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "Table 2 shows hierarchical precision@k results for the Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Table 2 shows hierarchical precision@k results for the Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task.", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "We did not compare against the Softmax baseline [7] on the flat hit@k measure, because the Softmax model cannot predict any of the test labels.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "Table 3: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Table 3: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Table 4: Flat hit@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Table 4: Flat hit@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.", "startOffset": 64, "endOffset": 67}], "year": 2017, "abstractText": "Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the semantic embedding space is trained jointly with the image transformation, while in other cases the semantic embedding space is established independently by a separate task, such as a natural language processing task on a text corpus, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional n-way classification framing of image understanding, particularly in terms of the promise of zero-shot learning \u2013 the ability to correctly annotate images of previously unseen object categories. Here we propose a simple method for constructing an image embedding system from any existing n-way image classification mechanism and any existing semantic embedding space which contains the n class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional learning. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}