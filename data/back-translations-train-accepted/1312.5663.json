{"id": "1312.5663", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "k-Sparse Autoencoders", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "histories": [["v1", "Thu, 19 Dec 2013 17:46:46 GMT  (1073kb)", "http://arxiv.org/abs/1312.5663v1", null], ["v2", "Sat, 22 Mar 2014 17:12:07 GMT  (1155kb)", "http://arxiv.org/abs/1312.5663v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alireza makhzani", "brendan frey"], "accepted": true, "id": "1312.5663"}, "pdf": {"name": "1312.5663.pdf", "metadata": {"source": "META", "title": "k-Sparse Autoencoders", "authors": ["Alireza Makhzani"], "emails": ["makhzani@psi.utoronto.ca", "frey@psi.utoronto.ca"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.56 63v1 [cs.LG] 1 9D ec2 01"}, {"heading": "1. Introduction", "text": "In fact, most of us are able to set out in search of new paths to follow."}, {"heading": "2. Description of the Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. The Basic Autoencoder", "text": "This can be achieved by a hidden representation of the activities associated with the function z = f (Px + b), which is based on the generation of data. (Px + b) The hidden results are then included in the evaluations. (Px + b) The hidden ones are then included in the evaluations. (Px + b) The hidden ones are then included in the evaluations. (Px + b) The hidden ones are included in the evaluations. (Px + b) The hidden ones are included in the evaluations. (Px + b) The hidden ones are included in the evaluations. (Px + b) The hidden ones are included in the evaluations. (Px + b) The hidden ones are included in the evaluations. (Px + b)"}, {"heading": "3.3. Importance of Incoherence", "text": "The coherence of a dictionary indicates the degree of similarity between different atoms or different aggregations of atoms. As the dictionary is overcomplete, we can represent each column of the dictionary as a linear combination of other columns. However, what incoherence means is that we should not be able to represent a column as a sparse linear combination of other columns and the coefficients of the linear combination. A na\u00efve measure of coherence proposed in the literature is the mutual coherence defined as the maximum absolute inner product of all possible pairs of atoms."}, {"heading": "4. Experiments", "text": "In this section, we evaluate the performance of k - sparse auto encoders in both unsupervised learning and shallow and deeply discriminatory learning tasks."}, {"heading": "4.1. Datasets", "text": "We use the handwritten MNIST dataset, which consists of 60,000 training images and 10,000 test images. We randomly separate the training set into 50,000 training cases and 10,000 cases for validation. We also use the small standardized NORB dataset (LeCun et al., 2004), which contains 24,300 training examples and 24,300 test examples. This database contains images of 50 toys in 5 generic categories: quadrupeds, human figures, airplanes, trucks and cars. Each image consists of two channels of 96 x 96 pixels each. We take the inner 64 x 64 pixels of each channel and reduce them to the size of 32 x 32 pixels using bicubic interpolation, from which we form a vector with 2048 dimensions as input. The data points are subtracted by the mean and divided by the standard deviation along each input dimension to normalize contrast."}, {"heading": "4.2.1. Scheduling of the Sparsity Level", "text": "If we enforce a low savings density level in k-sparse autocoders (e.g. k = 15 in MNIST), a problem might arise that the algorithm greedily assigns groups of training cases to individual hidden units in the first epochs, similar to clustering Kmeans. In later epochs, these hidden units are selected and re-enforced, and other hidden units are not adapted. That is, too much sparseness can prevent the gradient backspread from adjusting the weights of these other \"dead\" hidden units. We can solve this problem by dividing the savings density level across epochs as subsequent epochs. Suppose we aim for a savings density level of k = 15. Then we start with a large savings density level (e.g. k = 100), for which the k-sparse autocoder can train all hidden units. Then, we link the savings level from k = 100 to k = 15 in the first half of the epoch."}, {"heading": "4.2.2. Training Hyper-parameters", "text": "We optimized the model parameters by stochastic gradient reduction with impulse as follows: vk + 1 = mkvk \u2212 \u03b7k \u0445f (xk) xk + 1 = xk + vk (11) Here we use different impulse values, learning rates and initializations based on the task and the dataset, and validation is used for selecting hyperparameters. In the unattended MNIST task, the values \u03c3 = 0.01, mk = 0.9 and \u03b7k = 0.01, for 5000 epochs. In the monitored MNIST task, training began with mk = 0.25 and \u041ak = 1, and then the learning rate was reduced linearly to 0.001 over 200 epochs. In the monitored task NORK was 0.001 = 0.001 and in the monitored task NORK 0.001 = 0.001 and in the given time 0.001 = 0.000km."}, {"heading": "4.2.3. Implementations", "text": "While most conventional sparse encoding algorithms require complex matrix operations such as matrix inversion or SVD decomposition, k-sparse autoencoders require only matrix multiplications and sorting operations during both the learning and sparse encoding phases. (For parallel, distributed implementation, the sorting process can be replaced by a method that recursively applies a threshold value until k values remain.) We used efficient GPU implementation achieved on a single Nvidia GTX 680 GPU using the publicly available Gnumpy library (Tieleman, 2010)."}, {"heading": "4.3. Effect of Sparsity Level", "text": "In k-sparse autoencoders, we are able to adjust the value of k to obtain the desirable sparsity level that makes the algorithm suitable for a variety of datasets. For example, an application could preform a flat or deep discriminatory neural network. For large values of k (e.g. k = 100 on MNIST), the algorithm tends to learn very local characteristics, as shown in Figure 1a and 2a. These characteristics are too primitive to be used for classification using a flat architecture, since a naive linear classifier does not have enough capacity to combine these characteristics and achieve a good classification rate. However, these characteristics could be used for pre-training deep neural networks. As we reduce the sparsity level (e.g. k = 40 on MNIST), the output is reconstructed with a smaller number of hidden units, and hence the characteristics are more global than in Figure 1b and Figure 1b."}, {"heading": "4.4. Unsupervised Feature Learning Results", "text": "To compare the quality of the characteristics learned by our algorithm with those of other unattended learning methods, we first extracted characteristics using each unattended learning algorithm. We then fixed the characteristics and trained a logistic regression classifier using these characteristics. The usefulness of the characteristics is then evaluated by examining the error rate of the classification. We trained a number of architectures on the MNIST and NORB datasets, including RBM, dropout autoencoder and denoising autoencoder. When dropout, after we found the characteristics by dropout regulation with a dropout rate of 50%, we used all the hidden units as characteristics (this worked best). In the dropout autoencoder, after training the network by dropping the input pixels at a rate of 20%, we used all the uninput units as characteristics of a WIS-K to find the autocoxel that works best for the WIS-K."}, {"heading": "4.5. Shallow Supervised Learning Results", "text": "In the field of supervised learning, it is common practice to use encoder weights learned through an unsupervised learning method to initialize the early layers of a multi-layered discrimination model (Erhan et al., 2010).In this section, we report on results with uncontrolled learning algorithms such as RBMs, DBNs (Salakhutdinov & Larochelle, 2010), DBMs (Salakhutdinov & Larochelle, 2010), third-order RBMs (Nair & Hinton, 2009), dropout encoders, denoising autoencoders and k -sparse autoencoders to initialize the shallow discriminative neural network for the MNIST and NORB datasets the fine-tuning method."}, {"heading": "4.6. Deep Supervised Learning Results", "text": "The k-sparse autoencoder can be used as a building block of a deep neural network by carrying out greedy layer-by-layer pre-training (Bengio et al., 2007). First, we train a flat k-sparse autoencoder and obtain the hidden codes. Then, we repair the properties and train another k-sparse autoencoder over them to obtain another set of hidden codes. Then, we use the parameters of these autoencoders to initialize a discriminatory neural network with two hidden layers. At the fine-tuning stage of the deep neural network, we first fix the parameters of the first and second layers and train a Softmax classifier on the second layer. Then, we hold the weights of the first layer and train the second layer and Softmax together using the initialization of the softmax we found in the previous step. Finally, together, we refine all layers with the previous initialization."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed a very fast, sparse encoding method called k -sparse autoencoder, which achieves an exact spareness in the hidden representation. The main message of this paper is that we can use the resulting representations to achieve state-of-the-art classification results, exclusively by asserting the spareness in the hidden units and without any other nonlinearity or regularization. We also discussed how the k -sparse autoencoder could be used for pre-training of flat and more deeply monitored architectures."}], "references": [{"title": "K-svd: Design of dictionaries for sparse representation", "author": ["Aharon", "Michal", "Elad", "Michael", "Bruckstein", "Alfred"], "venue": "Proceedings of SPARS,", "citeRegEx": "Aharon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2005}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["Blumensath", "Thomas", "Davies", "Mike E"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath et al\\.", "year": 2009}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Coates", "Adam", "Ng", "Andrew"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization", "author": ["Donoho", "David L", "Elad", "Michael"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Donoho et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Donoho et al\\.", "year": 2003}, {"title": "Method of optimal directions for frame design", "author": ["Engan", "Kjersti", "Aase", "Sven Ole", "J. Hakon Husoy"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Engan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Engan et al\\.", "year": 1999}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Learning fast approximations of sparse coding", "author": ["Gregor", "Karol", "LeCun", "Yann"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Gregor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1010.3467,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["LeCun", "Yann", "Huang", "Fu Jie", "Bottou", "Leon"], "venue": "In Computer Vision and Pattern Recognition, CVPR,", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "Sparse deep belief net model for visual area v2", "author": ["Lee", "Honglak", "Ekanadham", "Chaitanya", "Ng", "Andrew"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Coherence analysis of iterative thresholding algorithms", "author": ["Maleki", "Arian"], "venue": "47th Annual Allerton Conference on,", "citeRegEx": "Maleki and Arian.,? \\Q2009\\E", "shortCiteRegEx": "Maleki and Arian.", "year": 2009}, {"title": "3d object recognition with deep belief nets", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nair et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2009}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Olshausen", "Bruno A", "Field", "David J"], "venue": "Vision research,", "citeRegEx": "Olshausen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 1997}, {"title": "Efficient learning of deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Larochelle", "Hugo"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2010}, {"title": "Gnumpy: an easy way to use gpu boards in python", "author": ["Tieleman", "Tijmen"], "venue": "Department of Computer Science, University of Toronto,", "citeRegEx": "Tieleman and Tijmen.,? \\Q2010\\E", "shortCiteRegEx": "Tieleman and Tijmen.", "year": 2010}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["Tropp", "Joel A", "Gilbert", "Anna C"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Tropp et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tropp et al\\.", "year": 2007}, {"title": "Kernel codebooks for scene categorization", "author": ["Van Gemert", "Jan C", "Geusebroek", "Jan-Mark", "Veenman", "Cor J", "Smeulders", "Arnold WM"], "venue": "In Computer Vision\u2013 ECCV", "citeRegEx": "Gemert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gemert et al\\.", "year": 2008}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 12, "context": "Sparse feature learning algorithms range from sparse coding approaches (Olshausen & Field, 1997) to training neural networks with sparsity penalties (Nair & Hinton, 2009; Lee et al., 2007).", "startOffset": 149, "endOffset": 188}, {"referenceID": 6, "context": "The current sparse codes are then used to update the dictionary, using techniques such as the method of optimal directions (MOD) (Engan et al., 1999) or K-SVD (Aharon et al.", "startOffset": 129, "endOffset": 149}, {"referenceID": 0, "context": ", 1999) or K-SVD (Aharon et al., 2005).", "startOffset": 17, "endOffset": 38}, {"referenceID": 10, "context": "To achieve speedups, in (Gregor & LeCun, 2010; Kavukcuoglu et al., 2010), a parameterized non-linear encoder function is trained to explicitly predict sparse codes using a soft thresholding operator.", "startOffset": 24, "endOffset": 72}, {"referenceID": 12, "context": "For example, in (Lee et al., 2007; Nair & Hinton, 2009), a \u201clifetime sparsity\u201d penalty function proportional to the negative of the KL divergence between the hidden unit marginals and the target sparsity probability is added to the cost function.", "startOffset": 16, "endOffset": 55}, {"referenceID": 20, "context": "(iii) We show that by solely relying on sparsity as the regularizer and as the only nonlinearity, we can achieve much better results than the other methods, including RBMs (Hinton & Salakhutdinov, 2006), denoising autoencoders (Vincent et al., 2008) and dropout (Hinton et al.", "startOffset": 227, "endOffset": 249}, {"referenceID": 20, "context": "This is related to the denoising autoencoder (Vincent et al., 2008) in which we achieve invariant features by trying to reconstruct the original signal from its noisy versions.", "startOffset": 45, "endOffset": 67}, {"referenceID": 11, "context": "We also use the small NORB normalized-uniform dataset (LeCun et al., 2004), which contains 24,300 training examples and 24,300 test examples.", "startOffset": 54, "endOffset": 74}, {"referenceID": 3, "context": "This preprocessing pipeline is the same as the one used in (Coates et al., 2011) for feature extraction.", "startOffset": 59, "endOffset": 80}, {"referenceID": 7, "context": "In supervised learning, it is a common practice to use the encoder weights learnt by an unsupervised learning method to initialize the early layers of a multilayer discriminative model (Erhan et al., 2010).", "startOffset": 185, "endOffset": 205}, {"referenceID": 1, "context": "The k -sparse autoencoder can be used as a building block of a deep neural network, using greedy layerwise pre-training (Bengio et al., 2007).", "startOffset": 120, "endOffset": 141}], "year": 2013, "abstractText": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the \u201ck -sparse autoencoder\u201d, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k -sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "creator": "LaTeX with hyperref package"}}}