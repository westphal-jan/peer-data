{"id": "1312.5770", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Consistency of Causal Inference under the Additive Noise Model", "abstract": "We analyze a family of methods for statistical causal inference from sample under the so-called Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting.", "histories": [["v1", "Thu, 19 Dec 2013 22:15:40 GMT  (55kb,D)", "https://arxiv.org/abs/1312.5770v1", null], ["v2", "Mon, 23 Dec 2013 16:59:04 GMT  (55kb,D)", "http://arxiv.org/abs/1312.5770v2", null], ["v3", "Wed, 5 Feb 2014 03:37:30 GMT  (65kb,D)", "http://arxiv.org/abs/1312.5770v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["samory kpotufe", "eleni sgouritsa", "dominik janzing", "bernhard sch\u00f6lkopf"], "accepted": true, "id": "1312.5770"}, "pdf": {"name": "1312.5770.pdf", "metadata": {"source": "META", "title": "Consistency of Causal Inference under the Additive Noise Model", "authors": ["Samory Kpotufe", "Eleni Sgouritsa", "Dominik Janzig", "Bernhard Sch\u00f6lkopf"], "emails": ["SAMORY@TTIC.EDU", "ELENI.SGOURITSA@TUEBINGEN.MPG.DE", "DOMINIK.JANZIG@TUEBINGEN.MPG.DE", "BS@TUEBINGEN.MPG.DE"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is not the case that these methods can distinguish between the two graphs that trigger the same set of conditional dependencies as the so-called Markov equivalents, which favor, for example, the case of only two observed random variables. Conditional independence is based on methods that are unable to obtain the causal graphs of X and Y. An elegant basis for causal graphs is the framework of structural causal models (Pearl, 2000), in which every observable function of their parents and an unobserved independent notion of intoxication is present."}, {"heading": "1.1. Inference Methods Under the Additive Noise Model", "text": "Assuming f and g are the best functional adjustments under some risk, or Y \u2248 f (X) and X \u2248 g (Y): Fit Y as function f (X), get the remainders \u03b7Y, f = Y \u2212 f (X), fit X as function g (Y), get the remainders \u03b7X, f = Y \u2212 f (X), abstain otherwise. Instances therefore vary in the regression procedures used for the functional adjustment, and in the applied independence measures. Our analysis concerns procedures that apply an entropy-based independence measure that is cheaper than standard independence tests. These procedures vary in the regression and entropy estimators used. They are described in detail in section 3."}, {"heading": "1.2. Towards Consistency: Main Difficulties", "text": "Suppose (i) and (ii) hold so that X causes Y under the CAM. We want to prove this from sufficiently large finite samples. This is consistency in a broad sense. Determining the consistency of the above meta-methods faces many subtle difficulties. However, the algorithmic approach outlined above consists of four interdependent statistical estimates, namely two regression problems and two independence tests. The main difficulty is that although we observe X and Y, we do not observe the residuals of Y, f and g, but empirical approximations of Y, fn and g, the successful regression obtained by estimating f and g. The main difficulty is that although we observe X and Y, we do not observe the residuals of Y, f and g, but empirical approximations of Y, fn and g, gn obtained by estimating f and g."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Setup and Notation", "text": "We let H and I specify differential entropy and mutual information respectively (Cover et al., 1994). Given a density p, we will sometimes use the (abusive) notation H (p) if an R.v. is not specified. Definition 1. For a function f: R 7 \u2192 R, we will consider one of the residuals: \u03b7Y, f, Y \u2212 f (X) and \u03b7X, f, X \u2212 f (Y). The causal additive noise model is captured as follows: Definition 2 (CAM). Given r.v.'sX, Y, we will consider a function f: R \u2192 R and a function f: R and an R.v. \u03b7, we will write X f \u2212 \u2212 \u2192 Y if the following applies: (P.X, function X, P.PY, and.P.PY)."}, {"heading": "3. Causal Inference Procedures", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Main Intuition", "text": "Lemma 1. Consider an absolutely continuous joint distribution PX, Y to X, Y, Y, R. For two functions f, g: R 7 \u2192 R we have H (X) + H (\u03b7Y, f) = H (Y) + H (\u03b7X, g) \u2212 {I (\u03b7X, g, Y) \u2212 I (\u03b7Y, f, X) \u2212 proof. According to the chain rule of differential entropy we have H (X, Y) = H (X) + H (Y | X) = H (X, g, Y) + H (\u03b7Y, f | X) = H (X) + H (AgenY, f) \u2212 I (\u03b7Y, f, X), similar to H (\u03b7X, g) \u2212 I (\u03b7X, g) \u2212 I (\u03b7X, g, Y).Equate the two R.h.s above and reorder them. Note that we always implicitly Y, f, X, X, H + H (X) = 0, then Y, when the distribution is less dependent on the.R.H."}, {"heading": "3.2. Meta-Algorithm", "text": "Let {(Xi, Yi) n1 = \u2192 (x1, y1),. > Procedure, (xn, yn)} be a finite sample drawn from PX, Y. Let Hn (X) and Hn (Y) each be estimators of H (X) and H (Y), based on the sample {(Xi, Yi) n1. Let us consider the following family of inference procedures: Give an i.i.d sample {(Xi, Yi) n1 from PX, Y, let fn be returned by an algorithm that matches Y as fn (X) and returns gn by an algorithm that fitsX as gn (Y). LetHn denotes an entropy estimator."}, {"heading": "4. Technical overview of results", "text": "This year, it has come to the point where it will be able to retaliate, \"he said.\" We've never done so much in history, \"he said.\" We've never done so much, \"he said.\" We've never done so much, \"he said.\" We've never done as much as we set out to do, \"he said.\" We've never done as much as we want to do. \""}, {"heading": "5. Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Consistency for Decoupled-estimation", "text": "In this section, we present a general consistency result for the meta-procedure, the most important technical aspects being to refer the differential entropy of the residuals to the L2 norms of the residuals (i.e. to the error made in the function assessment), and from now on to the Lebesgue measurement, the analysis in this section assumes the following polynomial final assumption: Adoption 2 meets the identifiability conditions of (Zhang & Hyva \ufffd rinen, 2009) Adoption 2 (Tail). PX, Y is generated as follows: Xf, \u03b7 \u2212 Z for a limited function f, with a limited derivative of R. PX has limited support, and both PX and PX have densities pX, PX with limited derivatives on R. Furthermore we assume that we have limited variance, and p2 is satisfied, for some > T, > C, > 1 and PX."}, {"heading": "5.2. Coupled Regression and Residual-entropy Estimation", "text": "Here we look at a coupled version of the meta algorithm where fn and gn are kernel regressors, as described in the next subsection."}, {"heading": "5.2.1. KERNEL INSTANTIATION OF THE META-ALGORITHM", "text": "Regression: Although each kernel located outside a limited region works for regression, we will focus here (for simplicity) on the specific case where fn and gn box kernel regressors are defined as follows (exchange X and Y to get gn (y): fn (x) = 1nx, h n \u2211 i = 1 Yi1 {| Xi \u2212 x | < h}, (7) where nx, h = | i: | Xi \u2212 x | < h |, for a bandwidth h.Entropy estimate: For a sequence of = {i} ni = 1 and a bandwidth size, we define pn as follows: pn, (t) = 1nh n = i = 1 K (i \u2212 t), where K (u) du = 1, K (u) du = 1, K (u) dduK (u) and K (u = 0) for the Y (u), Y (t), and Xi (\u2212 n)."}, {"heading": "5.2.2. CONSISTENCY RESULT FOR COUPLED-ESTIMATION", "text": "We assume that the noise in this section decreases exponentially: Definition 7. A r.v. Z has exponentially decreasing tail if there is C, C, C, > 0."}, {"heading": "6. Final Remarks", "text": "This paper focuses on the case of two R.v.s X and Y, which capture the inherent difficulties of consistency. However, we believe that the findings developed should be extended to the case of random vectors under appropriate tail conditions. However, the details will be left to future work. Another interesting multivariate situation is that of a causal network of R.v.s. as discussed in Peters et al. (2011b) earlier. Extending our consistency results to this particular multivariate case would primarily consist in extending our distributional tail conditions to the tails of distributions resulting from the conditioning of suitable variable sets in the network. However, this is a non-trivial extension such as it implies, e.g. for convergence-conditioned entropies, some additional integration steps that need to be carefully explained."}, {"heading": "A. Omitted figures from Section 4", "text": "Some additional experimental results were omitted from the main work for space and are shown in Figure 2."}, {"heading": "B. Omitted Proofs: Section 5.1", "text": "The proof for Lemma 2. Note that, assuming both pX and p\u03b7 = > Y are limited. (9) Therefore, we have ddxpX, Y (x, y) = pX (x) \u00b7 pY (y) = pX (x) \u00b7 p\u03b7 (y \u00b7 f (x)). (9) Therefore, ddxpX, Y (x, y) = pX (x) \u00b7 p\u03b7 (y \u2212 f (x)) \u2212 d dx f (x, y) \u2212 d dx (x) < d dx (x, y) < p (x, y) < p (Y) < p; p (x) < p (x, y) < p (X) < < p; < < X < < < X < (Y) < p; p; p; p; p; p; p; p; p; p; p; p; p; p (X) < < < X < < p (X < p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p;"}, {"heading": "C. Omitted Proofs: Section 5.2", "text": "We call the random n samples Xn, {Xi} n1 and Y n1 (both) n1 (all) n1 (all) n1 (all) n1 (all) n1 (all) n1 (all) n1 (all) n1 (all) n1 (all) n (all) n1 (all) n (all) n2 (all) n2 (all) nX (1) nX (1) nX) n. We find that the R.v.'s X and Y are interchangeable in the two lemmas 5 and 6 (both) nX) nX (both) nX) nX) nX (Xi) n. We just have to show that E 1n (1) n (Xi) n (Xi) n."}], "references": [{"title": "Nonparametric entropy estimation: An overview", "author": ["Beirlant", "Jan", "Dudewicz", "Edward J", "Gy\u00f6rfi", "L\u00e1szl\u00f3", "Van der Meulen", "Edward C"], "venue": "International Journal of Mathematical and Statistical Sciences,", "citeRegEx": "Beirlant et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Beirlant et al\\.", "year": 1997}, {"title": "Cam: Causal additive models, high-dimensional order search and penalized regression", "author": ["P. Buhlmann", "J. Peters", "J. Ernest"], "venue": null, "citeRegEx": "Buhlmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Buhlmann et al\\.", "year": 2013}, {"title": "Elements of information theory", "author": ["Cover", "Thomas M", "Thomas", "Joy A", "Kieffer", "John"], "venue": "SIAM Review,", "citeRegEx": "Cover et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cover et al\\.", "year": 1994}, {"title": "A Distribution Free Theory of Nonparametric Regression", "author": ["L. Gyorfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": null, "citeRegEx": "Gyorfi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gyorfi et al\\.", "year": 2002}, {"title": "Nonlinear causal discovery with additive noise models", "author": ["Hoyer", "Patrik O", "Janzing", "Dominik", "JM Mooij", "Peters", "Jonas", "Sch\u00f6lkopf", "Bernhard"], "venue": "Proceedings of Advances in Neural Processing Information Systems,", "citeRegEx": "Hoyer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hoyer et al\\.", "year": 2009}, {"title": "Causality: models, reasoning and inference, volume 29", "author": ["Pearl", "Judea"], "venue": null, "citeRegEx": "Pearl and Judea.,? \\Q2000\\E", "shortCiteRegEx": "Pearl and Judea.", "year": 2000}, {"title": "Causal inference on discrete data using additive noise models", "author": ["Peters", "Jonas", "Janzing", "Dominik", "Scholkopf", "Bernhard"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Peters et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2011}, {"title": "Identifiability of causal graphs using functional models", "author": ["Peters", "Jonas", "Mooij", "Joris", "Janzing", "Dominik", "Sch\u00f6lkopf", "Bernhard"], "venue": "Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Peters et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2011}, {"title": "A linear non-gaussian acyclic model for causal discovery", "author": ["Shimizu", "Shohei", "Hoyer", "Patrik O", "Hyv\u00e4rinen", "Aapo", "Kerminen", "Antti"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Shimizu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shimizu et al\\.", "year": 2006}, {"title": "Causation Prediction & Search 2e, volume 81", "author": ["Spirtes", "Peter", "Glymour", "Clark N", "Scheines", "Richard"], "venue": "MIT press,", "citeRegEx": "Spirtes et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Spirtes et al\\.", "year": 2000}, {"title": "Nonlinear directed acyclic structure learning with weakly additive noise models", "author": ["Tillman", "Robert", "Gretton", "Arthur", "Spirtes", "Peter"], "venue": "Proceedings of Advances in Neural Processing Information Systems,", "citeRegEx": "Tillman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tillman et al\\.", "year": 2009}, {"title": "On the identifiability of the post-nonlinear causal model", "author": ["Zhang", "Kun", "Hyv\u00e4rinen", "Aapo"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "Conditional-independence-based methods (Pearl, 2000; Spirtes et al., 2000) estimate a set of directed acyclic graphs, all entailing the same conditional independences, from the data.", "startOffset": 39, "endOffset": 74}, {"referenceID": 8, "context": "Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian.", "startOffset": 14, "endOffset": 36}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al.", "startOffset": 8, "endOffset": 851}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al.", "startOffset": 8, "endOffset": 877}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and P\u03b7 are absolutely continuous on R, with P\u03b7 having support R.", "startOffset": 8, "endOffset": 903}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and P\u03b7 are absolutely continuous on R, with P\u03b7 having support R. Note that Zhang & Hyv\u00e4rinen (2009) also introduces a generalization of the CAM termed post-nonlinear models.", "startOffset": 8, "endOffset": 1140}, {"referenceID": 4, "context": ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + \u03b7 is linear, provided the independent noise \u03b7 is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyv\u00e4rinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and P\u03b7 are absolutely continuous on R, with P\u03b7 having support R. Note that Zhang & Hyv\u00e4rinen (2009) also introduces a generalization of the CAM termed post-nonlinear models. Further work by Peters et al. (2011b) showed how to reduce causal inference for a network of multiple variables under the CAM to the case of two variables X and Y discussed so far, by properly extending the conditions (i) and (ii) to conditional distributions instead of marginals.", "startOffset": 8, "endOffset": 1252}, {"referenceID": 8, "context": "Various works (Shimizu et al., 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.", "startOffset": 14, "endOffset": 78}, {"referenceID": 4, "context": "Various works (Shimizu et al., 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.", "startOffset": 14, "endOffset": 78}, {"referenceID": 3, "context": ", 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.1 below) on a mix of artificial and real-world datasets where the causal structure to be inferred is clear. However, on the theoretical side, it remains unclear whether these procedures can infer causality from samples in general situations where the CAM is identifiable. In the particular case where the functional relation between X and Y is linear, Hyv\u00e4rinen et al. (2008) proposed a successful method shown to be consistent.", "startOffset": 8, "endOffset": 505}, {"referenceID": 1, "context": "In a recent Arxived result appearing after our initial submission, Buhlmann et al. (2013) showed the consistency of a maximum log-likelihood approach to causal inference under the multi-variable network extension of Peters et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 1, "context": "In a recent Arxived result appearing after our initial submission, Buhlmann et al. (2013) showed the consistency of a maximum log-likelihood approach to causal inference under the multi-variable network extension of Peters et al. (2011b). While consistency has been shown for particular procedures, in this paper we are rather interested in general conditions under which common approaches, with various algorithmic instantiations, are consistent.", "startOffset": 67, "endOffset": 238}, {"referenceID": 3, "context": "(Gyorfi et al., 2002), Theorem 3.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Setup and Notation We letH and I denote respectively differential entropy, and mutual information (Cover et al., 1994).", "startOffset": 98, "endOffset": 118}, {"referenceID": 3, "context": "kernel, k-NN, Kernel-SVM, spline regressors) are consistent in the above sense (Gyorfi et al., 2002).", "startOffset": 79, "endOffset": 100}, {"referenceID": 0, "context": "plug-in entropy estimators) is well established (Beirlant et al., 1997).", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": "For entropy estimation we employ a resubstitution estimate using a kernel density estimator tuned against log-likelihood (Beirlant et al., 1997) and for regression estimator we use kernel regression (KR).", "startOffset": 121, "endOffset": 144}, {"referenceID": 0, "context": "(Beirlant et al., 1997)).", "startOffset": 0, "endOffset": 23}], "year": 2014, "abstractText": "We analyze a family of methods for statistical causal inference from sample under the socalled Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting.", "creator": "LaTeX with hyperref package"}}}