{"id": "1312.6055", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Unit Tests for Stochastic Optimization", "abstract": "Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difficulty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on a dozen established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms.", "histories": [["v1", "Fri, 20 Dec 2013 17:44:06 GMT  (2443kb)", "http://arxiv.org/abs/1312.6055v1", "Initial submission to ICLR 2014"], ["v2", "Tue, 7 Jan 2014 20:43:40 GMT  (2923kb)", "http://arxiv.org/abs/1312.6055v2", "Submission to ICLR 2014 (revised for minor improvements, pre-reviews)"], ["v3", "Tue, 25 Feb 2014 18:16:54 GMT  (7536kb,D)", "http://arxiv.org/abs/1312.6055v3", "Final submission to ICLR 2014 (revised according to reviews, additional results added)"]], "COMMENTS": "Initial submission to ICLR 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tom schaul", "ioannis antonoglou", "david silver"], "accepted": true, "id": "1312.6055"}, "pdf": {"name": "1312.6055.pdf", "metadata": {"source": "CRF", "title": "Unit Tests for Stochastic Optimization", "authors": ["Tom Schaul"], "emails": ["tom.schaul@deepmind.com", "ioannis@deepmind.com", "david@deepmind.com"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.60 55v1 [cs.LG] 2 0"}, {"heading": "1 Introduction", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "2 Unit test Construction", "text": "Our test frame is an open source library with a collection of unit tests and visualization tools. Each unit test is defined by a prototype function to be optimized, a prototype scale, a noise prototype, and optionally a non-stationary prototype. A prototype function is the concatenation of one or more local mold prototypes. A multi-dimensional unit test is a composition of one-dimensional unit tests, alternatively with a rotation prototype or a gradient curl prototype."}, {"heading": "2.1 Shape Prototypes", "text": "Local shape prototypes are functions defined in an interval, and our collection includes linear slopes (zero curvature), square curves (fixed curvature), convex or concave curves (different curvature), and curves with exponentially increasing or decreasing slope. In addition, there are a number of non-differentiable local shape prototypes (absolute value, straight-linear or cliffs), all of which occur in realistic learning scenarios, e.g. in logistic regression, the loss area is partly concave and partly convex, a MSE loss is the prototype square shell, but then a regulation such as L1 introduces non-differentiable curvatures (such as rectified linear or maxout units in deep learning [15, 16])."}, {"heading": "2.2 One-dimensional Concatenation", "text": "Within our framework, we can concatenate a number of mold prototypes in such a way that the resulting function is continuous and differentiable at all nodes. Thus, we can create many prototype functions that closely mimic existing functions, such as the Laplace function, sinusoids, saddle points, step functions, etc. See the lower rows of Figure 1 for some examples. A single scale parameter determines the scaling of a concatenated function across all its shapes using the nodes. Varying the scales is an important aspect in testing robustness, as it is not possible to guarantee well-scaled gradients without significant overhead. On many learning problems, effort is invested in proper normalization [17], but this is not enough to ensure homogeneous scaling across all layers of a deep neural network."}, {"heading": "2.3 Noise Prototypes", "text": "The distinguishing feature of stochastic gradient optimization (compared to batch methods) is that it is based on sample gradients (derived from a subset of even a single element of the dataset) that are inherently noisy. In our unit tests, we model this using four types of stochasticity: \u2022 scale-independent additive Gaussian noise on the gradients, which corresponds to a random translation of input in a linear model with MSE loss. Note that this type of noise twists the sign of the gradient near the optimum and makes it difficult to accurately approach it. \u2022 Multiplicative (scale-dependent) Gaussian noise on the gradients, which multiplies the gradients by a positive random number (signs remain). This corresponds to a learning scenario in which the loss curvature for different samples near the current point noise unit will be different for each of the gradients (the gradients are different for the gradients)."}, {"heading": "2.4 Multi-dimensional Composition", "text": "High-dimensional unit tests are constructed by assembling one-dimensional unit tests. For example, for two one-dimensional prototypes La and Lb in combination with a p-standard, the composition L (a, b) (\u03b8) = (La (\u03b81) p + Lb (\u03b82) p) 1 is obtained. Noise prototypes exist independently of mold prototypes. While the 1D prototypes they are composed of can be chains, high-dimensional prototypes themselves are not linked. Different degrees of conditioning can be achieved by having dramatically different scales in different component dimensions. In addition to the combined prototypes and their scaling, we allow rotation in the entrance space that links the dimensions together and avoids axis alignment. These rotations are particularly important for testing diagonal / element optimization algorithms."}, {"heading": "2.5 Gradient Curl", "text": "In amplification learning, a value function (the expected discounted reward for each state) can be learned using Temporal Difference Learning (TD), an update method that uses bootstrapping: that is, it pulls the value of the current state toward the value of its successor state [19]. These stochastic update directions are not real gradients of any scalar energy field [20], but they still form a (more general) vector field with a curve that does not deviate from zero, with the goal of the optimization algorithm being to align with its fixed point (its fixed points). See Figure 4 for a detailed example. We have implemented this aspect by placing different amounts of curl over a multidimensional vector field using a fixed rotation matrix in our unit tests."}, {"heading": "2.6 Non-stationarity", "text": "In many settings, it is necessary to optimize a non-stationary lens function. This can typically occur in a non-stationary task where the problem to be solved changes over time. However, non-stationary optimization can be important even for large stationary tasks, if the algorithm itself decides to pursue a particular aspect of the problem rather than solving the problem globally.21 In addition, reinforcement learning tasks often involve non-stationary optimization.For example, many RL algorithms use the TD algorithm described in the previous paragraph when evaluating the value function. This results in two sources of non-stationary loss: the target value changes with each step (which leads to the previously described curve); and also the state distribution changes as the value function is improved and better actions are selected. These algorithms can therefore be considered a non-stationary loss function, the optimum of which is to check the current parameter values. In order to test the instationary sequences, we need to optimize the position of the various motion stimuli for different movements."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Setup and Algorithms", "text": "For our experiments, we test the candidate algorithms for 1003 unit tests. Each test pairing of algorithm and unit is repeated ten times, but the same 10 random seeds are reused in all algorithms and configurations. For each run k, we calculate the truly expected loss in the parameter value achieved after 100 update steps L (k) = E [L (\u03b8 (k) 100)). The evaluated algorithms are fixed learning rate SGD \u03b70 [10 \u2212 6, 10], SGD with annealing with decay factor in [10 \u2212 2, 1] and start rates \u03b70, SGD with momentum (regular or Nesterov variant [22]) [0,1, 0,999] and start rates \u03b70, SGD with parameter average [] with decay factor in [10 \u2212 4, 0.5] and exponent in {12 \u2212 3, 4 \u2212 1}, GD with initial rates Nitrate 12 \u2212 10} and \u2212 AGRAD \u2212 103 [10 \u2212 1 \u2212 10]."}, {"heading": "3.2 Reference performance", "text": "Each unit test is associated with a reference performance Lsgd and a corresponding reference learning rate \u03b7best, which is determined by a parameter overview of all set learning rates for SGD (34 evenly distributed values on the log scale between 10 \u2212 10 and 10), while retaining the highest performing values. In our aggregated charts, unit tests (per group) are sorted by their reference learning rate, i.e. those requiring small steps on the left and those requiring large steps on the right. Algorithm structures are also sorted on the vertical axis according to their median performance on a reference unit test (square, additive noise)."}, {"heading": "3.3 Qualitative Evaluation", "text": "The algorithm power L (k) is converted to a normalized value L (k) norm = L (k) \u2212 Linit Lsgd \u2212 Linitwhere Linit = E [L (\u03b80)]. In other words, a normalized value near zero does not correspond to progress, negative means divergence, and a value close to one corresponds to the best SGD. Based on these results, we assign a qualitative color value to the performance of each algorithm setup for each unit test so that it can be represented in the resulting numbers in a single pixel: \u2022 Red: divergence or numerical instability in all runs. \u2022 Violet: divergence or numerical instability in at least one run. \u2022 Orange: Insufficient progress: median (norm) < 0.1 \u2022 Yellow: Good progress: median (norm) > 0.1 and high variability: Lnorm < 0.1 for at least 14 of runs and 0.1 of progress: median (very good)."}, {"heading": "3.4 Results", "text": "Figure 5 shows the qualitative results of all algorithm variants on all 1003 unit tests. There is a wealth of information in these visualizations. For example, the relatively sparse amount of blueFigure 5: Qualitative results for all algorithm variants (350) on all unit tests (800 one-dimensional and 200 multi-dimensional). Each column is a unit test, ordered by type, and each row group is an algorithm with one set of hyperparameters per line. The color code is: red / violet = divergence, orange = slow, yellow = variability, green = acceptable, blue = excellent (see main text for details) indicating that it is difficult to beat well-matched SGD in performance on most unit tests. Another unsurprising conclusion is that matching hyperparameters for the adaptive algorithms (ADAGRAD, ADADELTA, RPROMRP, SRP) is not as important for some SRP adaptive tests as it is for SD."}, {"heading": "4 Realism and Future Work", "text": "We do not expect this to replace the real predictive function of a one-dimensional autocoder, but rather to complement it with our series of unit tests. However, it is important to have sufficient coverage of the types of potential difficulties that occur in realistic environments. To a much lesser extent, we may not want to overload the test series with unit tests that measure problems that never occur in realistic problems.It is not easy to map very high-dimensional real loss functions with an MLP [27]. We are postponing a broader study of this kind, namely obtaining statistics on how often different prototypes occur, to future work. However, unit tests capture the characteristics of some examples that can be analyzed, one of which is the predictive function of a one-dimensional autodictator unit."}, {"heading": "4.1 Algorithm Dynamics", "text": "Our long-term goal is to be able to perform systematic tests and a complete study of optimization dynamics for a given algorithm. Of course, it is not possible to test it definitively for all possible loss functions (because there are infinitely many), but a sharing and conquest approach could be the next best one. To this end, we introduce the concept of the algorithm state that changes during optimization (e.g. the current step size or dynamics).Now, a long optimization process can be considered a concatenation of a number of unit tests while maintaining the algorithm state in between. Our hypothesis is that the totality of all possible chains of unit tests in our collection covers most of the qualitatively different (stationary or non-stationary) loss functions of an optimization algorithm. To evaluate the robustness of an algorithm (and not its expected performance), we can assume that an opponent selects an algorithm at each step in sequential dynamics."}, {"heading": "5 Conclusion", "text": "This work established a large collection of simple benchmarks to evaluate stochastic optimization algorithms across a wide range of small-scale, isolated, and well-understood difficulties, and this approach helps unravel problems that tend to get confused in real-world scenarios when they have realistic characteristics. Our initial findings on a dozen established algorithms (under a variety of different hyperparameter settings) show that robustness is not trivial, and that different algorithms struggle to perform different unit tests. The test framework is open source, expandable to new function classes, and easy to use to evaluate the robustness of new algorithms."}, {"heading": "A Appendix: Framework Software", "text": "In fact, most of them will be able to play by the rules."}], "references": [{"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1951}, {"title": "Online Algorithms and Stochastic Approximations", "author": ["L\u00e9on Bottou"], "venue": "Online Learning and Neural Networks", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Large Scale Online Learning", "author": ["L\u00e9on Bottou", "Yann LeCun"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "The Tradeoffs of Large Scale Learning", "author": ["L\u00e9on Bottou", "Olivier Bousquet"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Adaptive Algorithms and Stochastic Approximations", "author": ["A. Benveniste", "M. Metivier", "P. Priouret"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Topmoumoute online natural gradient", "author": ["N. Le Roux", "P.A. Manzagol", "Y. Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent", "author": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent", "author": ["Wei Xu"], "venue": "ArXiv-CoRR, abs/1107.2490,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "No More Pesky Learning Rates", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Real-parameter black-box optimization benchmarking 2010", "author": ["Nikolaus Hansen", "Anne Auger", "Steffen Finck", "Raymond Ros"], "venue": "Experimental setup", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Comparing results of 31 algorithms from the black-box optimization benchmarking BBOB-2009", "author": ["Nikolaus Hansen", "Anne Auger", "Raymond Ros", "Steffen Finck", "Petr Po\u0161\u0131\u0301k"], "venue": "In Proceedings of the 12th annual conference companion on Genetic and evolutionary computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Efficient BackProp", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Temporal-difference methods and Markov models", "author": ["Etienne Barnard"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "On the role of tracking in stationary environments", "author": ["Richard S. Sutton", "Anna Koop", "David Silver"], "venue": "In Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Interior-point polynomial algorithms in convex programming, volume 13", "author": ["Yurii Nesterov", "Arkadii Semenovich Nemirovskii"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Adapting bias by gradient descent: An incremental version of delta-bardelta", "author": ["Richard S Sutton"], "venue": "In AAAI,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1992}, {"title": "A direct adaptive method for faster backpropagation learning: The RPROP algorithm", "author": ["Martin Riedmiller", "Heinrich Braun"], "venue": "In Neural Networks,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1993}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T Tieleman", "G Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Stochastic optimization [1] is among the most widely used components in large-scale machine learning, thanks to its linear complexity, efficient data usage, and often superior generalization [2, 3, 4].", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "Stochastic optimization [1] is among the most widely used components in large-scale machine learning, thanks to its linear complexity, efficient data usage, and often superior generalization [2, 3, 4].", "startOffset": 191, "endOffset": 200}, {"referenceID": 2, "context": "Stochastic optimization [1] is among the most widely used components in large-scale machine learning, thanks to its linear complexity, efficient data usage, and often superior generalization [2, 3, 4].", "startOffset": 191, "endOffset": 200}, {"referenceID": 3, "context": "Stochastic optimization [1] is among the most widely used components in large-scale machine learning, thanks to its linear complexity, efficient data usage, and often superior generalization [2, 3, 4].", "startOffset": 191, "endOffset": 200}, {"referenceID": 4, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 5, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 6, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 7, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 8, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 9, "context": "These algorithms may derive from simplifying assumptions on the optimization landscape [10], but in practice, they tend to be used as general-purpose tools, often outside of the space of assumptions their designers intended.", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "The troublesome conclusion is that practitioners find it difficult to discern where potential weaknesses of new (or old) algorithms may lie [11], and when they are applicable \u2013 an issue that is separate from raw performance.", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "This results in essentially a trial-and-error procedure for finding the appropriate algorithm variant and hyper-parameter settings, every time that the dataset, loss function, regularization parameters, or model architecture change [12].", "startOffset": 232, "endOffset": 236}, {"referenceID": 12, "context": "This is a similar approach to the very fruitful one taken by the black-box optimization community [13, 14].", "startOffset": 98, "endOffset": 106}, {"referenceID": 13, "context": "This is a similar approach to the very fruitful one taken by the black-box optimization community [13, 14].", "startOffset": 98, "endOffset": 106}, {"referenceID": 14, "context": "All of these occur in realistic learning scenarios, for example in logistic regression the loss surface is part concave and part convex, an MSE loss is the prototypical quadratic bowl, but then regularization such as L1 introduces non-differentiable bends (as do rectified-linear or maxout units in deep learning [15, 16]).", "startOffset": 313, "endOffset": 321}, {"referenceID": 10, "context": "Steep cliffs in the loss surface are a common occurrence when training recurrent neural networks, as discussed in [11].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "In many learning problems, effort is put into proper normalization [17], but that is insufficient to guarantee homogeneous scaling all throughout the layers of a deep neural network.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "This mimics both training with drop-out [18], and scenarios with rectified linear units where a unit will be inactive for some input samples, but not for others.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "it pulls the value of the current state towards the value of its successor state [19].", "startOffset": 81, "endOffset": 85}, {"referenceID": 18, "context": "These stochastic update directions are not proper gradients of any scalar energy field [20], but they still form a (more general) vector field with non-zero curl, where the objective for the optimization algorithm is to converge to its fixed-point(s).", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "However, nonstationary optimization can even be important in large stationary tasks, when the algorithm itself chooses to track a particular aspect of the problem, rather than solve the problem globally [21].", "startOffset": 203, "endOffset": 207}, {"referenceID": 9, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 63, "endOffset": 71}, {"referenceID": 9, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 63, "endOffset": 71}, {"referenceID": 9, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 113, "endOffset": 120}, {"referenceID": 0, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 113, "endOffset": 120}, {"referenceID": 20, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 192, "endOffset": 196}, {"referenceID": 9, "context": "5] and exponent in { 1 2 , 3 4 , 1}, ADAGRAD [10] with initial rates \u03b70, ADADELTA [23] with decay parameter (1 \u2212 \u03b3) \u2208 [10, 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "5] and exponent in { 1 2 , 3 4 , 1}, ADAGRAD [10] with initial rates \u03b70, ADADELTA [23] with decay parameter (1 \u2212 \u03b3) \u2208 [10, 0.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 194, "endOffset": 202}, {"referenceID": 9, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 194, "endOffset": 202}], "year": 2013, "abstractText": "Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difficulty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on a dozen established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms.", "creator": "LaTeX with hyperref package"}}}