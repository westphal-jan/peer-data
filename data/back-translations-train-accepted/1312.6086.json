{"id": "1312.6086", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "The return of AdaBoost.MH: multi-class Hamming trees", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "histories": [["v1", "Fri, 20 Dec 2013 19:33:26 GMT  (97kb,D)", "http://arxiv.org/abs/1312.6086v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bal\\'azs k\\'egl"], "accepted": true, "id": "1312.6086"}, "pdf": {"name": "1312.6086.pdf", "metadata": {"source": "CRF", "title": "The return of ADABOOST.MH: multi-class Hamming trees", "authors": ["Bal\u00e1zs K\u00e9gl"], "emails": ["BALAZS.KEGL@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "The idea behind it is that it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, and in which it is about a way, and in which it is about a way, and in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which way it is about a way, and in which it is about a way, in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about a way and which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is"}, {"heading": "2. ADABOOST.MH", "text": "In this section, we first present the general structure of multiclass learning (Section 2.1), then describe ADABOOST.MH in detail (Section 2.2), then explain the general requirements for basic learning in ADABOOST.MH and introduce the concept of factorized vector-weighted base learner (Section 2.3), and finally explain the general objective for factorized base learners and the algorithmic structure for optimizing this goal (Section 2.4)."}, {"heading": "2.1. The multi-class setup: single-label and multi-label/multi-task", "text": "For the formal description of ADABOOST.MH, leave the training data D = {x1, y1),.., (xn, yn), where xi-Rd are the observation vectors, and yi-K, where the terminology of the individual conceptual features (x1, x1,.) and the conceptual features of the individual conceptual features (x1,.) and the conceptual features of the individual conceptual features (x1,.) of the individual conceptual features (x) of the observation x x (x1,.) is when the conceptual features Y = (y1,.,. K} instead of the set of the pairs D.1 In the multi-level classification, the single conceptual matrix (x) of the observation x will come from a finite set of x. Without loss of generality, we will assume that \"L = {1,.,., K.}. The conceptual vectors y is a one-level representation of the correct class,\" we are the correct class (x)."}, {"heading": "2.2. ADABOOST.MH", "text": "The goal of the ADABOOST.MH algorithm (Schapire & Singer 1999; Figure 1) is to return a vector-weighted discriminant function f (T): Rd \u2192 RK with a low hamming loss R-H (f, W) (2) by minimizing the weighted multi-class exponential margin-based error R-EXP (f (T), W) = 1n n n \u2211 i = 1 K \u2211 '= 1 wi,' exp (\u2212 f (T) '(xi) yi,'). (4) Since exp (\u2212 \u03c1) \u2265 I {< 0}, (4) limits the hamming loss R-H (f (T), W) (2) upwards. ADABOOST.MH returns the final discriminant function f (T) (x) = \u2211 T = 1 h (x) as the sum of the T base classifiers h (t): X \u2192 RK after an algorithm BAST (T)."}, {"heading": "2.3. Base learning for ADABOOST.MH", "text": "The objective of multi-level basic teaching is to objectiveZ (t) = min h Z (h, W (t)) = n \u2211 i = 1 K \u2211 '= 1 w (t) i, \"e \u2212 h\" (xi) yi, \"(5) It is easy to show (Schapire & Singer, 1999) that i) the one-level discretionary difference R (f) I (t) becomes iteration0. Generally, any vector-rated multi-level learning algorithm can be used to minimize (5).Although this goal is clearly defined in (Schapire & Singer, 1999), efficient basic learning algorithms have never been described in detail."}, {"heading": "2.4. Casting the votes", "text": "The intuitive semantics of (6) is as follows: The binary classifier (x) cuts the entrance space into a positive and a negative region. In binary classification, this is the end of the story: We need a good correlation with the binary class name y (x). In multi-class classification, it is possible that the (x) correlation with some of the class names y \"and anti-correlates with others. This free choice is expressed by the binary\" vote \"v.\" We say that the (x) votes for the class \"when v '1 and it votes against the class\" when v '1. As in binary classification, the general quality of the classification (x) expresses itself."}, {"heading": "3. Hamming trees", "text": "They are particularly efficient when used as the basis for their education, but when averaged, this problem largely disappears. Quinlan (1993) \"s C4.5 is the most commonly used tree learning method. Whereas this tree implementation is a perfect choice for binary ADABOOST, it is suboptimal for ADABOST.MH Since there is no guarantee of a positive multi-class edge, this problem can be solved in practice by building large single-class trees, it seems to be suboptimal (Section 4)."}, {"heading": "4. Experiments", "text": "Full reproducibility was one of the main motivations when we conceived our experimental arrangements. (All experiments were carried out with the open source software by Benbouzid et al. (2012) (All experiments were carried out by us) (All experiments were carried out by us in the manner in which we validate the hyperparameters. We carried out experiments on five medium levels, which we overlap with most of the most recent multifold papers. (Ke) gl & Busa-Fekete, 2009; Li, 2009a; b; Zhu et al., 2009; Mukherjee al, 2013. (Ke) gl & Busa-Fekete, 2009; Li, 2009a; Zhu et al. (Zhu et al.)"}, {"heading": "5. Conclusion", "text": "We have shown that ADABOOST.MH is one of the best standard classification algorithms for multiple classes without this restriction, which is often considered mandatory. It retains the conceptual elegance, performance and computational efficiency of binary ADABOOST.The use of decision errors on the inner nodes and leaves of the tree is a natural choice due to the efficiency of the learning algorithm. Nevertheless, the general structure described in this paper allows the use of any binary classifier. One of the possibilities being investigated for future work is to attempt stronger classification mechanisms, such as SVMs, as binary sections. The formal structure described in Section 2.1 does not restrict the algorithm to single-label problems; another direction for future work is to measure it against standard problems such as multiple classification and the sequence of 2008."}, {"heading": "A. Showing (9)", "text": "Z (h, W) = n \u00b5 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 n \u00b2 = 1 wi, 'exp (\u2212 h' (xi) yi ',') = n \u2211 i = 1 K \u2211 '= 1 wi,' exp (\u2212 \u03b1v ',' xi) yi ',') (16) = n \u2211 i = 1 K \u2211 '= 1 (wi', 'I {v', (xi) yi, '= 1} e \u2212 \u03b1 + wi,' I {v ', (xi) yi,' = \u2212 1} e\u03b1) = K \u2211 '= 1 (\u00b5' + I {v '= + 1} + \u00b5' \u2212 I {v '= \u2212 1}) e' (e ', e', e ', e', e ', e', e '.'"}, {"heading": "B. Multi-class decision stumps", "text": "The simplest basic scalar learner applied in practice to numerical features is the decision stump, a one-sided two-sided decision tree of the formj, b (x) = {1 if x (j) \u2265 b, \u2212 1 otherwise, where j is the index of the selected feature and b is the decision threshold. If the feature values (x (j) 1,.., x (j) n) are pre-ordered before the first increasing iteration, a decision stump that maximizes the edge (11) (or minimizes the energy (16) 5) can be found within the (ndK) line. The pseudo code of the algorithm is shown in Figure 3. \u2212 STUMPBASE first calculates the edge vector (0) of the constant classification h (0) (x) the best energy (1) that serves as the output alarm edge vector for each characteristic edge maximizer."}, {"heading": "C. Cutting the data set", "text": "The basic procedure for adding a tree node with a scalar binary classifier (intersection) is to separate the data matrices X, Y, and W according to the sign of classification \u0442 (xi) for all xi-X. Figure 5 contains the pseudo code of this simple procedure."}], "references": [{"title": "Reducing multiclass to binary: a unifying approach for margin classifiers", "author": ["E.L. Allwein", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2001}, {"title": "MultiBoost: a multi-purpose boosting package", "author": ["D. Benbouzid", "R. Busa-Fekete", "N. Casagrande", "Collin", "F.-D", "B. K\u00e9gl"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Benbouzid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Benbouzid et al\\.", "year": 2012}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B. Boser", "I. Guyon", "V. Vapnik"], "venue": "In Fifth Annual Workshop on Computational Learning Theory, pp", "citeRegEx": "Boser et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Boser et al\\.", "year": 1992}, {"title": "eds.). Yahoo! Learning-to-Rank Challenge, volume 14 of JMLR W&CP", "author": ["O. Chapelle", "Y. Chang", "T.Y. Liu"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Introduction to Algorithms", "author": ["T. Cormen", "C. Leiserson", "R. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2009}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri", "year": 1995}, {"title": "Gradient tree boosting for training conditional random fields", "author": ["T.G. Dietterich", "Hao", "Guohua", "A. Ashenfelter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dietterich et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 2008}, {"title": "Make it cheap: Learning with O(nd) complexity", "author": ["W. Duch", "N. Jankowski", "T. Maszczyk"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Duch et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duch et al\\.", "year": 2012}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1997}, {"title": "Multiclass boosting with hinge loss based on output coding", "author": ["T. Gao", "D. Koller"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Gao and Koller,? \\Q2011\\E", "shortCiteRegEx": "Gao and Koller", "year": 2011}, {"title": "Multiclass learning, boosting, and error-correcting codes", "author": ["V. Guruswami", "A. Sahai"], "venue": "In Conference on Computational Learning Theory,", "citeRegEx": "Guruswami and Sahai,? \\Q1999\\E", "shortCiteRegEx": "Guruswami and Sahai", "year": 1999}, {"title": "Boosting products of base classifiers", "author": ["B. K\u00e9gl", "R. Busa-Fekete"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "K\u00e9gl and Busa.Fekete,? \\Q2009\\E", "shortCiteRegEx": "K\u00e9gl and Busa.Fekete", "year": 2009}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Multiclass boosting with repartitioning", "author": ["Li", "Ling"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li and Ling.,? \\Q2006\\E", "shortCiteRegEx": "Li and Ling.", "year": 2006}, {"title": "ABC-Boost: Adaptive base class boost for multiclass classification", "author": ["P. Li"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li,? \\Q2009\\E", "shortCiteRegEx": "Li", "year": 2009}, {"title": "ABC-LogitBoost for multi-class classification", "author": ["P. Li"], "venue": "Technical Report arXiv:0908.4144, Arxiv preprint,", "citeRegEx": "Li,? \\Q2009\\E", "shortCiteRegEx": "Li", "year": 2009}, {"title": "A theory of multiclass boosting", "author": ["I. Mukherjee", "R.E. Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mukherjee and Schapire,? \\Q2013\\E", "shortCiteRegEx": "Mukherjee and Schapire", "year": 2013}, {"title": "Obtaining calibrated probabilities from boosting", "author": ["A. Niculescu-Mizil", "R. Caruana"], "venue": "In Proceedings of the 21st International Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Niculescu.Mizil and Caruana,? \\Q2005\\E", "shortCiteRegEx": "Niculescu.Mizil and Caruana", "year": 2005}, {"title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt,? \\Q2000\\E", "shortCiteRegEx": "Platt", "year": 2000}, {"title": "Induction of decision trees", "author": ["J. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "Quinlan,? \\Q1986\\E", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "Programs for Machine Learning", "author": ["Quinlan", "J. C"], "venue": null, "citeRegEx": "Quinlan and C4.5,? \\Q1993\\E", "shortCiteRegEx": "Quinlan and C4.5", "year": 1993}, {"title": "Bagging, boosting and C4.5", "author": ["J. Quinlan"], "venue": "In Proceedings of the 13th National Conference on Artificial Intelligence, pp", "citeRegEx": "Quinlan,? \\Q1996\\E", "shortCiteRegEx": "Quinlan", "year": 1996}, {"title": "Using output codes to boost multiclass learing problems", "author": ["R.E. Schapire"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Schapire,? \\Q1997\\E", "shortCiteRegEx": "Schapire", "year": 1997}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer", "year": 1999}, {"title": "AOSO-LogitBoost: Adaptive one-vs-one LogitBoost for multi-class problem", "author": ["P. Sun", "M.D. Reid", "J. Zhou"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Sun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "Unifying the error-correcting and output-code AdaBoost within the margin framework", "author": ["Y. Sun", "S. Todorovic", "J. Li", "D. Wu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Sun et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2005}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M. Jones"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola and Jones,? \\Q2004\\E", "shortCiteRegEx": "Viola and Jones", "year": 2004}, {"title": "Multi-class AdaBoost", "author": ["J. Zhu", "H. Zou", "S. Rosset", "T. Hastie"], "venue": "Statistics and its Interface,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "It is especially the method of choice when any-time solutions are required on large data sets, so it has been one of the most successful techniques in recent large-scale classification and ranking challenges (Dror et al., 2009; Chapelle et al., 2011).", "startOffset": 208, "endOffset": 250}, {"referenceID": 27, "context": "As a consequence, several recent papers concentrate on replacing the boosting objective and the engine that optimizes this objective (Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013).", "startOffset": 133, "endOffset": 210}, {"referenceID": 24, "context": "As a consequence, several recent papers concentrate on replacing the boosting objective and the engine that optimizes this objective (Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013).", "startOffset": 133, "endOffset": 210}, {"referenceID": 24, "context": "MH with Hamming trees performs on par with the best existing multiclass boosting algorithm AOSOLOGITBOOST (Sun et al., 2012) and with support vector machines (SVMs; Boser et al.", "startOffset": 106, "endOffset": 124}, {"referenceID": 2, "context": ", 2012) and with support vector machines (SVMs; Boser et al. 1992).", "startOffset": 41, "endOffset": 66}, {"referenceID": 27, "context": "MH (Zhu et al., 2009; Mukherjee & Schapire, 2013).", "startOffset": 3, "endOffset": 49}, {"referenceID": 1, "context": "In experiments (carried out using an open source package of Benbouzid et al. (2012) for reproducibility) we found that ADABOOST.", "startOffset": 60, "endOffset": 84}, {"referenceID": 27, "context": "In most recent papers (Zhu et al., 2009; Mukherjee & Schapire, 2013) where ADABOOST.", "startOffset": 22, "endOffset": 68}, {"referenceID": 22, "context": "Although it is not described in detail, it seems that the base classifier used in the original paper of Schapire & Singer (1999) is a vector ofK independent decision stumps h(x) = ( h1(x), .", "startOffset": 104, "endOffset": 129}, {"referenceID": 0, "context": "In the simplest setup this matrix is fixed beforehand by maximizing the error correcting capacity of the matrix (Dietterich & Bakiri, 1995; Allwein et al., 2001).", "startOffset": 112, "endOffset": 161}, {"referenceID": 22, "context": "A slightly better solution (Schapire, 1997; Guruswami & Sahai, 1999; Sun et al., 2005) is to wait until the given iteration to pick v by maximizing", "startOffset": 27, "endOffset": 86}, {"referenceID": 25, "context": "A slightly better solution (Schapire, 1997; Guruswami & Sahai, 1999; Sun et al., 2005) is to wait until the given iteration to pick v by maximizing", "startOffset": 27, "endOffset": 86}, {"referenceID": 25, "context": "and then to choose the optimal binary classifier \u03c6 with this fixed vote (or code) vector v\u2217 (although in practice it seems to be better to fix v to a random binary vector; Sun et al. 2005).", "startOffset": 92, "endOffset": 188}, {"referenceID": 19, "context": "Classification trees (Quinlan, 1986) have been widely used for multivariate classification since the 80s.", "startOffset": 21, "endOffset": 36}, {"referenceID": 21, "context": "They are especially efficient when used as base learners in ADABOOST (Caruana & Niculescu-Mizil, 2006; Quinlan, 1996).", "startOffset": 69, "endOffset": 117}, {"referenceID": 19, "context": "Classification trees (Quinlan, 1986) have been widely used for multivariate classification since the 80s. They are especially efficient when used as base learners in ADABOOST (Caruana & Niculescu-Mizil, 2006; Quinlan, 1996). Their main disadvantage is their variance with respect to the training data, but when averaged over T different runs, this problem largely disappears. The most commonly used tree learner is C4.5 of Quinlan (1993). Whereas this tree implementation is a perfect choice for binary ADABOOST, it is suboptimal for ADABOOST.", "startOffset": 22, "endOffset": 438}, {"referenceID": 4, "context": "The main idea is to maintain a priority queue, a data structure that allows inserting objects with numerical keys into a set, and extracting the object with the maximum key (Cormen et al., 2009).", "startOffset": 173, "endOffset": 194}, {"referenceID": 4, "context": "When the priority queue is implemented as a heap, both the insertion and the extraction of the maximum takes O(logN) time (Cormen et al., 2009), so the total running time of the procedure is O ( N(TBASE +n+ logN) ) , where TBASE is the running time of the base learner.", "startOffset": 122, "endOffset": 143}, {"referenceID": 1, "context": "All experiments were done using the open source multiboost software of Benbouzid et al. (2012), version 1.", "startOffset": 71, "endOffset": 95}, {"referenceID": 27, "context": "The five sets were chosen to overlap with the selections of most of the recent multi-class boosting papers (K\u00e9gl & Busa-Fekete, 2009; Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013), The small data sets were selected for comparing ADABOOST.", "startOffset": 107, "endOffset": 210}, {"referenceID": 24, "context": "The five sets were chosen to overlap with the selections of most of the recent multi-class boosting papers (K\u00e9gl & Busa-Fekete, 2009; Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013), The small data sets were selected for comparing ADABOOST.", "startOffset": 107, "endOffset": 210}, {"referenceID": 7, "context": "MH with SVMs using Gaussian kernels, taking the results of a recent paper (Duch et al., 2012) whose experimental setup we adopted.", "startOffset": 74, "endOffset": 93}, {"referenceID": 27, "context": "Some report results with fixed hyperparameters (Zhu et al., 2009; Sun et al., 2012) and others give the full table of test errors for a grid of hyperparameters (K\u00e9gl & BusaFekete, 2009; Li, 2009a;b; Mukherjee & Schapire, 2013).", "startOffset": 47, "endOffset": 83}, {"referenceID": 24, "context": "Some report results with fixed hyperparameters (Zhu et al., 2009; Sun et al., 2012) and others give the full table of test errors for a grid of hyperparameters (K\u00e9gl & BusaFekete, 2009; Li, 2009a;b; Mukherjee & Schapire, 2013).", "startOffset": 47, "endOffset": 83}, {"referenceID": 24, "context": "1 (Sun et al., 2012) 3.", "startOffset": 2, "endOffset": 20}, {"referenceID": 27, "context": "5 SAMME w single-label trees (Zhu et al., 2009) 2.", "startOffset": 29, "endOffset": 47}, {"referenceID": 27, "context": "MH w single-label trees (Zhu et al., 2009) 2.", "startOffset": 24, "endOffset": 42}, {"referenceID": 7, "context": "On the small data sets, Duch et al. (2012) used the exact same protocol, so, although the folds are not the same, the results are directly comparable.", "startOffset": 24, "endOffset": 43}, {"referenceID": 24, "context": "For methods where this choice would unfairly bias the comparison (AOSOLOGITBOOST (Sun et al., 2012), ABCLOGITBOOST, LOGITBOOST, and ABCMART (Li, 2009a;b)), we chose the best overall hyperparameter J = 20 and \u03bd = 0.", "startOffset": 81, "endOffset": 99}, {"referenceID": 1, "context": "MH with decision products (K\u00e9gl & BusaFekete, 2009) is also implemented in multiboost (Benbouzid et al., 2012), for this method we re-ran experiments with the protocol described above.", "startOffset": 86, "endOffset": 110}, {"referenceID": 21, "context": "MH (SAMME of Zhu et al. (2009) and ADABOOST.", "startOffset": 13, "endOffset": 31}, {"referenceID": 19, "context": "MM of Mukherjee & Schapire (2013)) we report the post-validated best error which may be significantly lower than the error corresponding to the hyperparameter choice selected by proper validation.", "startOffset": 18, "endOffset": 34}, {"referenceID": 24, "context": "The overall conclusion is that AOSOLOGITBOOST (Sun et al., 2012) and ADABOOST.", "startOffset": 46, "endOffset": 64}, {"referenceID": 24, "context": "MH with decision products (K\u00e9gl & BusaFekete, 2009) and ABCLOGITBOOST are slightly weaker, as also noted by (Sun et al., 2012).", "startOffset": 108, "endOffset": 126}, {"referenceID": 27, "context": "SAMME (Zhu et al., 2009) and ADABOOST.", "startOffset": 6, "endOffset": 24}, {"referenceID": 27, "context": "MH in (Zhu et al., 2009; Mukherjee & Schapire, 2013), assumably implemented using single-label trees (the errors reported by Mukherjee & Schapire (2013) are especially conspicuous).", "startOffset": 6, "endOffset": 52}, {"referenceID": 22, "context": "MM (Mukherjee & Schapire, 2013) perform below the rest of the methods on the two data sets shared among all the papers (even though we give post-validated results). Another important conclusion is that ADABOOST.MH with Hamming trees is significantly better then other implementations of ADABOOST.MH in (Zhu et al., 2009; Mukherjee & Schapire, 2013), assumably implemented using single-label trees (the errors reported by Mukherjee & Schapire (2013) are especially conspicuous).", "startOffset": 16, "endOffset": 449}, {"referenceID": 12, "context": "85%, comparable to classical convolutional nets (LeCun et al., 1998).", "startOffset": 48, "endOffset": 68}, {"referenceID": 18, "context": "MH with Hamming trees, usually combined with calibration (Platt, 2000; Niculescu-Mizil & Caruana, 2005) and model averaging, has been also successful in recent data challenges.", "startOffset": 57, "endOffset": 103}, {"referenceID": 3, "context": "In the Yahoo! Learning-to-Rank Challenge (Chapelle et al., 2011) it achieved top ten performances with results not significantly different from the winning scores.", "startOffset": 41, "endOffset": 64}, {"referenceID": 6, "context": "1 does not restrict the algorithm to single-label problems; another direction for future work is to benchmark it on standard multi-label and sequence-to-sequence classification problems (Dietterich et al., 2008).", "startOffset": 186, "endOffset": 211}], "year": 2013, "abstractText": "Within the framework of ADABOOST.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem toK binary one-againstall classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length K and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary ADABOOST. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLOGITBOOST, and it is significantly better than other known implementations of ADABOOST.MH.", "creator": "LaTeX with hyperref package"}}}