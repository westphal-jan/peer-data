{"id": "1312.6098", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "On the number of response regions of deep feed forward networks with piece-wise linear activations", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $\\Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "histories": [["v1", "Fri, 20 Dec 2013 20:22:31 GMT  (65kb,D)", "http://arxiv.org/abs/1312.6098v1", null], ["v2", "Mon, 6 Jan 2014 19:53:34 GMT  (242kb,D)", "http://arxiv.org/abs/1312.6098v2", null], ["v3", "Mon, 27 Jan 2014 22:13:09 GMT  (244kb,D)", "http://arxiv.org/abs/1312.6098v3", null], ["v4", "Mon, 10 Feb 2014 17:24:12 GMT  (291kb,D)", "http://arxiv.org/abs/1312.6098v4", null], ["v5", "Fri, 14 Feb 2014 17:52:12 GMT  (2248kb,D)", "http://arxiv.org/abs/1312.6098v5", "17 pages, 9 figures"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["razvan pascanu", "guido montufar", "yoshua bengio"], "accepted": true, "id": "1312.6098"}, "pdf": {"name": "1312.6098.pdf", "metadata": {"source": "CRF", "title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "authors": ["Razvan Pascanu", "Guido Mont\u00fafar"], "emails": ["r.pascanu@gmail.com", "montufar@mis.mpg.de", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Preliminaries", "text": "rE \"s rf\u00fc ide for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five.\" rE \"s rf\u00fc ide for the five for the five for the five for the five for the six for the six for the six for the six for the six for the six for the six for the six for the six for the six for the six for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight for the eight"}, {"heading": "3 One layer rectifier MLP", "text": "Let us look at how many linear regions we have for a single layer of MLP with n0 input units and n1 hidden units. First, we reformulate the arrangement of the individual layers of MLP as the function f (1) x + W (out) 1 diag I (1) 0: x + b (1) 0: x + b (1) 2: x + b of the individual layer MLP as the function f (1) x + W (out) 1 diag I (1) 0: x + b (1) 0: 1 (1) n1: x (1) n1: x (1) b (1) b (out)."}, {"heading": "4 Several layers", "text": "Leave r (n0, n1), b (n0, n1), u (n0, n1), and u (n1) is the number of regions, limited regions, and unlimited regions of a generic arrangement of n1 hyperplanes in a n0-dimensional space. Proposal 8. The number of regions of linearity of an internal layer with a width of m (n) denotes the number of regions and unlimited regions with negative values of the previous layer. In particular, R (n0, n1, n2, ny) is more than large asr + (m \u2212 1) u, where r and u are the number of regions with negative values of the previous layer. In particular, R (n1, n2, ny) is more than 0 + (n1 k) + (n0 \u2212 1) (n0 \u2212 1) k = 0 (n0 (n1) k = 0 (n1 k) (n1 k) - (n1 k) regions and unlimited regions with negative values of the previous layer."}, {"heading": "5 Discussion and Conclusions", "text": "In this paper, we have introduced a novel way of understanding the meaningfulness of a model, provided that it is a piecemeal linear function. We argue that by counting the number of linear regions that this model can represent, we can, for example, look at how well it can approximate any curved surface. This is no limitation, as the output activation itself is not parameterized. Therefore, we argue that if there is a target function that we want to model with a rectifier MLP with its output activation function, then there is a function f'targ, so that such a piece (f'targ) = ftarg is not parameterized. If the inverse function has (such as sigmoid), a model with a rectifier MLP with its output activation function can be selected."}], "references": [{"title": "On the expressive power of deep architectures", "author": ["Y. Bengio", "O. Delalleau"], "venue": "Algorithmic Learning Theory,", "citeRegEx": "Bengio and Delalleau.,? \\Q2011\\E", "shortCiteRegEx": "Bengio and Delalleau.", "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Almost optimal lower bounds for small depth circuits", "author": ["J. H\u00e5stad"], "venue": "In Proceedings of the 18th annual ACM Symposium on Theory of Computing,", "citeRegEx": "H\u00e5stad.,? \\Q1986\\E", "shortCiteRegEx": "H\u00e5stad.", "year": 1986}, {"title": "On the power of small-depth threshold circuits", "author": ["J. H\u00e5stad", "M. Goldmann"], "venue": "Computational Complexity,", "citeRegEx": "H\u00e5stad and Goldmann.,? \\Q1991\\E", "shortCiteRegEx": "H\u00e5stad and Goldmann.", "year": 1991}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinv"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Montreal (Qc), Canada,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": null, "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "Poon and Domingos.,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2011}, {"title": "An introduction to hyperplane arrangements", "author": ["R. Stanley"], "venue": "In Lect. notes, IAS/Park City Math. Inst.,", "citeRegEx": "Stanley.,? \\Q2004\\E", "shortCiteRegEx": "Stanley.", "year": 2004}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Technical report,", "citeRegEx": "Zeiler and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "Such a behaviour is empirically illustrated, for instance, in Zeiler and Fergus (2013); Lee et al.", "startOffset": 62, "endOffset": 87}, {"referenceID": 4, "context": "Such a behaviour is empirically illustrated, for instance, in Zeiler and Fergus (2013); Lee et al. (2009). On the other hand, a shallow model has to construct detectors of target objects based only on the detectors learnt by the first layer.", "startOffset": 88, "endOffset": 106}, {"referenceID": 2, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011).", "startOffset": 192, "endOffset": 206}, {"referenceID": 3, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011).", "startOffset": 259, "endOffset": 286}, {"referenceID": 0, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011).", "startOffset": 302, "endOffset": 330}, {"referenceID": 8, "context": "Bengio and Delalleau (2011) shows that deep sum-product networks (Poon and Domingos, 2011) can use exponentially less nodes to express some families of polynomials compared to the shallow ones.", "startOffset": 65, "endOffset": 90}, {"referenceID": 1, "context": "Rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), and in general piece-wise linear activation function (for example the maxout unit (Goodfellow et al.", "startOffset": 16, "endOffset": 60}, {"referenceID": 7, "context": "Rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), and in general piece-wise linear activation function (for example the maxout unit (Goodfellow et al.", "startOffset": 16, "endOffset": 60}, {"referenceID": 0, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011). Bengio and Delalleau (2011) shows that deep sum-product networks (Poon and Domingos, 2011) can use exponentially less nodes to express some families of polynomials compared to the shallow ones.", "startOffset": 303, "endOffset": 360}, {"referenceID": 0, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011). Bengio and Delalleau (2011) shows that deep sum-product networks (Poon and Domingos, 2011) can use exponentially less nodes to express some families of polynomials compared to the shallow ones. This note analyzes the representational power of deep MLP with rectifier units. Rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), and in general piece-wise linear activation function (for example the maxout unit (Goodfellow et al., 2013)), are becoming a popular choice in designing deep models, and most current state-of-the-art results involve using one of such activations (Goodfellow et al., 2013; Hinton et al., 2012b). Glorot et al. (2011) show that rectifier units have several properties, that makes the optimization problem easier than the more traditional case of using smooth and bounded activations, such as tanh or sigmoid.", "startOffset": 303, "endOffset": 984}], "year": 2017, "abstractText": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has 2n hidden units and n0 inputs, then the number of linear regions is O(n0). A two layer model with n number of hidden units on each layer has \u03a9(n0). We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "creator": "LaTeX with hyperref package"}}}