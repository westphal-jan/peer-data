{"id": "1312.6173", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Multilingual Distributed Representations without Word Alignment", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "histories": [["v1", "Fri, 20 Dec 2013 23:13:38 GMT  (107kb,D)", "http://arxiv.org/abs/1312.6173v1", null], ["v2", "Fri, 21 Feb 2014 20:24:06 GMT  (121kb,D)", "http://arxiv.org/abs/1312.6173v2", "Submitted to ICLR 2014"], ["v3", "Mon, 17 Mar 2014 17:52:13 GMT  (121kb,D)", "http://arxiv.org/abs/1312.6173v3", "To appear at ICLR 2014"], ["v4", "Thu, 20 Mar 2014 13:55:02 GMT  (122kb,D)", "http://arxiv.org/abs/1312.6173v4", "To appear at ICLR 2014"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karl moritz hermann", "phil blunsom"], "accepted": true, "id": "1312.6173"}, "pdf": {"name": "1312.6173.pdf", "metadata": {"source": "CRF", "title": "A Simple Model for Learning Multilingual Compositional Semantics", "authors": ["Karl Moritz Hermann"], "emails": ["karl.moritz.hermann@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "Distributed representations of meaning are a natural way of encoding covariance relationships between words and phrases in NLP. By overcoming problems of data economy and providing information about semantic relationships that are not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can be successfully applied to a number of monolingual applications such as mood analysis. At the same time, there has been initial success in the work on learning common word-like representations in different languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign aligned sentences similar embeddings and sentences that are not aligned but do not require word alignment. We show that our representations are semantically informative and apply them to a cross-language classification task that we encounter at the previous state of art."}, {"heading": "1 Introduction", "text": "Distributed representations of words are increasingly used to achieve a high degree of generalization within language models. Successful applications of this approach include the disamination of the word, the similarity of words, and the recognition of synonyms (e.g. [9, 24]). Subsequently, attempts have been made to learn distributed semantics of larger structures that allow us to apply distributed representations to tasks such as sensitivity analysis or the recognition of paraphrases. At the same time, a second strand of work has focused on the transmission of linguistic knowledge in languages, in particular English in low-resource languages, by means of distributed representations at the word level."}, {"heading": "2 Models of Compositional Distributed Semantics", "text": "In the case of the presentation of individual words as vectors, the distribution report of semantics provides a plausible explanation of what is encoded in a word vector. It follows the idea that the meaning of a word can be determined by \"the company that holds it,\" [10] i.e. by the context in which it appears. Such a context can easily be encoded in vectors, and it is also based on other methods of word embedding [7, 18]. For a number of important problems, semantic representations of individual words are not enough; instead, a semantic representation of a larger structure - e.g. a phrase or a sentence - is required. This was highlighted in [9] when a mechanism for modifying the representation of a word based on its individual context was proposed."}, {"heading": "2.1 Multilingual Embeddings", "text": "Klementiev et al. [15] described a form of multi-agent learning based on word-oriented parallel data to transfer embedding from one language to another. Previous work, Haghighi et al. [12], proposed a method for generating cross-language lexicographies using monolingual character representations and a small initial lexicon to boot-strap. This approach was recently expanded to include [16, 17], which developed a method for learning transformation matrices to convert semantic vectors from one language to another. [26] It has been shown that this approach can be applied to improve machine translation tasks. Their CBOW model is also noteworthy for its similarities to the composition function used in this paper. Using a slightly different approach, [26] larger steps are also needed for machine translation."}, {"heading": "3 Model Description", "text": "Language acquisition in humans is widely considered to be based on sensory motor experience [20, 2]. Based on this idea, there have been some attempts to use multimodal data to learn better vector representations of words (e.g. [23]), but such methods are not easily scalable across languages or large amounts of data for which there could be no secondary or tertiary representation. We are taking the underlying principle a step further and trying to learn semantics from multilingual data. The idea is that a common representation with sufficiently parallel data would be forced to capture the common elements between sentences from different languages. What two parallel sentences have in common, of course, is the semantics of these two sentences. On the basis of these data, we propose a novel method for learning vector representations at word level and beyond."}, {"heading": "3.1 Bilingual Signal", "text": "Using the semantic similarity of parallel sentences between languages, we can define a simple bilingual (and trivially multilingual) error function as follows: If we use a compositional sentence model (CVM) MA that maps a sentence to a vector, we can train a second CVM MB using a corpus of CA, B parallel data from the language pair A, B. For each pair of parallel sentences (a, b), we try to minimize Ebi (a, b) = \"aroot \u2212 broot,\" where aroot is the vector that represents sentence a, and broot is the vector that represents sentence b."}, {"heading": "3.2 The BICVM Model", "text": "We assume that individual words are represented by vectors (x * Rn).Previous methods use binary parse trees on the data (e.g. [13, 22]) and use weighted or multiplicative composition functions. Under such a setup, where each node in the tree is terminal or has two children (p \u2192 c0, c1), a binary composition function could take the following form: p = g (W e [c0; c1] + e), where [c0; c1] is the concatenation of the two child vectors, W e-Rn and be-Rn are the coding matrix and bias, respectively g an elementary activation function such as the hyperbolic tangent. For the purpose of evaluating the bilingual signal above, we simplify this composition by setting all identifiers."}, {"heading": "3.3 Model Learning", "text": "Considering the objective function as defined above, the same techniques can be applied to model learning as to any monolingual CVM. Since the objective function is differentiated, we can particularly apply standard techniques such as stochastic gradient descent, L-BFGS or the adaptive gradient algorithm (AdaGrad). Within each monolingual CVM, we use backpropagation by structure after applying the common error to each sentence level node."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data and Parameters", "text": "All model weights were randomly initialized using a Gaussian distribution. There are a number of parameters that can influence the model training. In future work, we will investigate the effects of these parameters in more detail. Initially, we used the following values: L2 regularization (1), step size (0.1), number of noise elements (50), border size (50), embedding dimensionality (d = 128). Noise element samples were randomly taken from the corpus during the training. We use the Europarl corpus (v7) 1 to form the bilingual model. The corpus was pre-processed with the cdec2 tool set for tokenization and reduction of the data. Furthermore, all empty sentences and their translations were removed from the corpus. We present the results from two experiments. The BICVM model was trained on 500-sentence pairs of the English-German part of the Europarl corpus."}, {"heading": "4.2 Cross-Lingual Document Classification", "text": "We evaluate our model on the basis of the task of Cross-lingual Document Classification (CLDC) by Klementiev et al. [15]. This task involves learning language-independent embedding, which is then used to classify documents within the English-German language pair. CLDC uses a special type of supervision for this purpose, namely the use of monitored training data in one language and the evaluation without supervision in another language. CLDC is therefore a good task to determine whether our learned representations are useful semantically across multiple languages.We follow the experimental setup described in [15], except that we learn our embedding exclusively on the basis of Europarl data and only use the Reuters RCV1 / RCV2 corpora during the classification trainings and examination phases.Each document in the classification task is represented by the average of d-dimensional representations of all its phrases.We train the multiclassifier classifier on the basis of the same perception settings and the average implementation in English [6]."}, {"heading": "4.3 Visualization", "text": "While the CLDC experiment focused on determining the semantic content of the sentence-level representations, we will also briefly examine the induced word embeddings. In particular, the BICVM + model is interesting for this purpose, as it allows us to evaluate our approach of using English as a pivot language in a multilingual arrangement. In Figure 3, we show the t-SNE projections for a number of English, French and German words. Of particular interest should be the correct diagram highlighting bilingual embeddings between French and German words. Although the model did not use parallel French-German data during the training, it was nevertheless possible to learn semantic word-word similarity between these two languages."}, {"heading": "5 Conclusions", "text": "Using a very simple method of semantic composition, however, we were able to achieve state-of-the-art results in the CLDC task, which was specifically designed to evaluate semantic transmissions between languages. By broadening our approach to include multilingual training data in the BICVM + model, we were able to show that adding additional languages further improves the model. Furthermore, we demonstrated through some qualitative experiments and visualizations that our approach also allows us to learn semantically related embedding between languages without direct training data.Our approach offers great flexibility in training data and requires little to no comment. After demonstrating the successful training of semantic representations using sentence alignment data, a plausible next step is to perform the training with document-oriented data or even corporations of comparable documents."}, {"heading": "Acknowledgements", "text": "The authors thank Alexandre Klementiev and his co-authors for providing their data sets and average Perceptron implementation and for answering a number of questions related to their work on this task."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Precis of how children learn the meanings of words", "author": ["Paul Bloom"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Combining symbolic and distributional models of meaning", "author": ["Stephen Clark", "Stephen Pulman"], "venue": "In Proceedings of AAAI Spring Symposium on Quantum Interaction. AAAI Press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": "Lambek Festschrift. Linguistic Analysis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Machine translation by triangulation: Making effective use of multi-parallel corpora", "author": ["Trevor Cohn", "Mirella Lapata"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "A structured vector space model for word meaning in context", "author": ["K. Erk", "S. Pad\u00f3"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP \u201908,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A synopsis of linguistic theory 1930-55", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1952}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "The Role of Syntax in Vector Space Models of Compositional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Recurrent convolutional neural networks for discourse compositionality", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai"], "venue": "In Proceedings of the International Conference on Computational Linguistics (COLING),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever"], "venue": "CoRR, abs/1309.4168,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Proceedings of ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Grounded spoken language acquisition: Experiments in word learning", "author": ["D. Roy"], "venue": "Trans. Multi.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP-CoNLL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Lawrence K. Saul"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "[9, 24]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "[9, 24]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[1, 3, 11, 13, 19, 22]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "[1, 3, 11, 13, 19, 22]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "[1, 3, 11, 13, 19, 22]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "[1, 3, 11, 13, 19, 22]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 18, "context": "[1, 3, 11, 13, 19, 22]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 21, "context": "[1, 3, 11, 13, 19, 22]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "At the same time a second strand of work has focused on transferring linguistic knowledge across languages, and particularly from English into low-resource languages, by means of distributed representations at the word level [12, 15].", "startOffset": 225, "endOffset": 233}, {"referenceID": 14, "context": "At the same time a second strand of work has focused on transferring linguistic knowledge across languages, and particularly from English into low-resource languages, by means of distributed representations at the word level [12, 15].", "startOffset": 225, "endOffset": 233}, {"referenceID": 9, "context": "This follows the idea that the meaning of a word can be determined by \u201cthe company it keeps\u201d [10], that is by the context it appears in.", "startOffset": 93, "endOffset": 97}, {"referenceID": 6, "context": "Such context can easily be encoded in vectors using collocational methods, and is also underlying other methods of learning word embeddings [7, 18].", "startOffset": 140, "endOffset": 147}, {"referenceID": 17, "context": "Such context can easily be encoded in vectors using collocational methods, and is also underlying other methods of learning word embeddings [7, 18].", "startOffset": 140, "endOffset": 147}, {"referenceID": 8, "context": "This was highlighted in [9], who proposed a mechanism for modifying a word\u2019s representation based on its individual context.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "A notable exception perhaps is Baroni and Zamparelli [1], who learned distributional representations for adjective noun pairs using a collocational approach on a corpus of unprecedented size.", "startOffset": 53, "endOffset": 56}, {"referenceID": 18, "context": "[19] provides an evaluation of a number of simple composition functions applied to bigrams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Notably, [3] propose a tensor-based model for semantic composition and, similarly, [4] develop a framework for semantic composition by combining distributional theory with pregroup grammars.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "Notably, [3] propose a tensor-based model for semantic composition and, similarly, [4] develop a framework for semantic composition by combining distributional theory with pregroup grammars.", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "The latter framework was empirically evaluated and supported by the results in [11].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "Such models include recursive autoencoders [21], matrix-vector recursive neural networks [22], untied recursive neural networks [13] or convolutional networks [14].", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "Such models include recursive autoencoders [21], matrix-vector recursive neural networks [22], untied recursive neural networks [13] or convolutional networks [14].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "Such models include recursive autoencoders [21], matrix-vector recursive neural networks [22], untied recursive neural networks [13] or convolutional networks [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "Such models include recursive autoencoders [21], matrix-vector recursive neural networks [22], untied recursive neural networks [13] or convolutional networks [14].", "startOffset": 159, "endOffset": 163}, {"referenceID": 14, "context": "[15] described a form of multi-agent learning on word-aligned parallel data to transfer embeddings from one language to another.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12], proposed a method for inducing cross-lingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "This approach has recently been extended by [16, 17], who developed a method for learning transformation matrices to convert semantic vectors of one language into those of another.", "startOffset": 44, "endOffset": 52}, {"referenceID": 16, "context": "This approach has recently been extended by [16, 17], who developed a method for learning transformation matrices to convert semantic vectors of one language into those of another.", "startOffset": 44, "endOffset": 52}, {"referenceID": 25, "context": "Using a slightly different approach, [26], also learned bilingual embeddings for machine translation.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "Language acquisition in humans is widely seen as grounded in sensory-motor experience [20, 2].", "startOffset": 86, "endOffset": 93}, {"referenceID": 1, "context": "Language acquisition in humans is widely seen as grounded in sensory-motor experience [20, 2].", "startOffset": 86, "endOffset": 93}, {"referenceID": 22, "context": "[23]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13, 22]) and use weighted or multiplicative composition functions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[13, 22]) and use weighted or multiplicative composition functions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "While this can be a useful objective for transferring linguistic knowledge into low-resource languages [15], this precondition is not helpful when there is no model to learn from in first place.", "startOffset": 103, "endOffset": 107}, {"referenceID": 24, "context": "12 in [25]): Enoise(a, b, n) = [1\u2212 Ebi(a, b) + Ebi(a, n)]+ (6)", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "Cross-lingual compositional representations (BICVM and BICVM+), cross-lingual representations using learned embeddings and an interaction matrix (I-Matrix) [15] translated (MT) and glossed (Glossed) words, and the majority class baseline.", "startOffset": 156, "endOffset": 160}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "This is similar to prior work in machine translation where English was used as a pivot for translation between low-resource languages [5].", "startOffset": 134, "endOffset": 137}, {"referenceID": 7, "context": "We use the adaptive gradient method, AdaGrad [8], for updating the weights of our models, and terminate training for the BICVM model after 10 iterations.", "startOffset": 45, "endOffset": 48}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We follow the experimental setup described in [15], with the exception that we learn our embeddings using solely the Europarl data and only use the Reuters RCV1/RCV2 corpora during the classifier training and testing stages.", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "We train the multiclass classifier using the same settings and implementation of the averaged perceptron classifier [6] as used in [15].", "startOffset": 116, "endOffset": 119}, {"referenceID": 14, "context": "We train the multiclass classifier using the same settings and implementation of the averaged perceptron classifier [6] as used in [15].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "Using the data splits provided by [15], we used varying training data sizes from 100 to 10,000 documents for training the multiclass classifier.", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "We will investigate more complex compositional vector models such as MV-RNN [22] or tensor-based approaches, to see whether these can further improve results on both mono- and multilingual tasks when used in conjunction with our cross-lingual objective function.", "startOffset": 76, "endOffset": 80}], "year": 2017, "abstractText": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "creator": "LaTeX with hyperref package"}}}