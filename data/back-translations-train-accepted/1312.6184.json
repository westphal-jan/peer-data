{"id": "1312.6184", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Do Deep Nets Really Need to be Deep?", "abstract": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method on TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available.", "histories": [["v1", "Sat, 21 Dec 2013 00:47:43 GMT  (21kb)", "http://arxiv.org/abs/1312.6184v1", null], ["v2", "Fri, 3 Jan 2014 03:32:10 GMT  (12kb)", "http://arxiv.org/abs/1312.6184v2", "updated the discussion and the conclusion sections"], ["v3", "Mon, 6 Jan 2014 20:49:04 GMT  (12kb)", "http://arxiv.org/abs/1312.6184v3", "updated author info"], ["v4", "Wed, 8 Jan 2014 17:34:30 GMT  (12kb)", "http://arxiv.org/abs/1312.6184v4", "minor revision"], ["v5", "Fri, 21 Feb 2014 20:04:00 GMT  (13kb)", "http://arxiv.org/abs/1312.6184v5", "revision"], ["v6", "Tue, 7 Oct 2014 21:12:27 GMT  (13kb)", "http://arxiv.org/abs/1312.6184v6", "final revision coming soon"], ["v7", "Sat, 11 Oct 2014 00:19:10 GMT  (67kb,D)", "http://arxiv.org/abs/1312.6184v7", "final revision coming soon"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jimmy ba", "rich caruana"], "accepted": true, "id": "1312.6184"}, "pdf": {"name": "1312.6184.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jimmy@psi.utoronto.ca", "rcaurana@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.61 84v1 [cs.LG] 2 1D ec2 01Currently, deep neural networks are state-of-the-art in problems such as speech recognition and computer vision. In this extended abstract, we show that flat feedback networks can learn the complex functions previously learned from deep networks and can achieve accuracies previously only achieved with deep models. Furthermore, flat neural networks can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method for the phoneme recognition task TIMIT and are able to train flat, fully interconnected networks that function similarly complex, mature, deep winding architectures."}, {"heading": "1 Introduction", "text": "If you train a flat neural network as in [2], which consists of coil layers, bundling layers and several fully interconnected feed layers on the same data, you get 90% accuracy on the test set.What is the source of this magic? Is the 5% increase in the accuracy of the deep mesh over the flat mesh because: a) the deep mesh has more parameters than the flat mesh; b) the deep mesh is deeper than the flat mesh; c) non-coiled meshes cannot learn what coiled meshes can learn; d) current learning algorithms and regulation procedures work better with deep architectures than with flat architectures; e) all or some of the above models are flawed to make the model flawed."}, {"heading": "2 Model compression", "text": "The main idea behind the compression model [3] is to use a compact model to approximate the function learned by a more complex model. For example, it has been shown that a single medium-sized neural network can often be trained to imitate a much larger complex ensemble with thousands to tens of thousands of models; the small neural network can contain 1,000 times fewer parameters and run 1,000 times faster, but is often just as accurate as the complex ensemble on which it is trained; the compression of the model works by passing blank data through the large, precise model to collect the values predicted by this model; these values capture the function that the complex model has learned at the sampled points; this synthetically labeled data set is then used to train the compressive model. Note that the mimic model is not trained on the original labels; instead, it is trained to learn the function that is likely to be predetermined by the large model."}, {"heading": "3 TIMIT dataset", "text": "The TIMIT speech corpus includes 462 speakers in the training set. There is a separate development set of 50 speakers for cross-validation and a final test set of 24 speakers. The raw waveform audio data was pre-processed with 25ms hamming window shift of 10ms to extract Fourier transform-based filter banks of 40 coefficients (plus energy) distributed on a mel scale, along with their first and second time derivatives. We included + / - 7 nearby frames to formulate the final input vector for the 1845 dimension. Data input functions were normalized by subtracting the mean and dividing by the standard deviation for each dimension. All 61 phonemes represented in tri-state, 3 states for each of the 61 phonemes and 183 dimension target label vector were used during the training and subsequently decoded for classes [as in 6]."}, {"heading": "4 Deep Learning on TIMIT", "text": "We follow the same framework and train two deep models on TIMIT, DNN and CNN. DNN is a deep neural network consisting of three fully connected hidden layers with feedback, consisting of 2000 linear rectified units (ReLU) [8] per layer. CNN is a deep neural network consisting of a revolutionary layer and a max pooling layer followed by three hidden layers containing 2000 ReLU units. [1] Dropout is applied to both models to prevent overmatching. DNN and CNN's accuracy on the final test set is shown in Table 1. The error rate of the revolutionary deep network is about 2.1% better than the deep network. The table also shows the accuracy of a flat neural network with 8000 hidden units (SNN). Although the flat model has a similar number of parameters to the DNN, it is 2% less accurate than the DNN and 4.1% less accurate than CNN."}, {"heading": "5 Training an SNN to Mimic a Deeper Model", "text": "We train the compressive mimic SNN model using the data labeled by an ensemble of deep meshes. the deep models are trained in the usual way using Softmax output and Cross-Entropy cost function. However, the flat SNN, instead of being trained with Cross-Entropy on the 183 p values, where epk = e zk / \u2211 j ezj is generated from the deep model, is trained directly on the log probability values z before Softmax activation.The training on the log probability value facilitates learning for the flat neural network by placing more emphasis on reliable predictions, so that the flat network will attempt to correct data points with high confidence predictions first during learning.We formulate the learning object function as a regression problem in light of the training data {(x (1), z (1)),..., (x), z (T), z (T))}: L (W, \u03b2 = S \u00b7 T (W = T), W (T = T), W (T = T = T)."}, {"heading": "6 Speed up learning by including a linear layer", "text": "In order to meet the similar number of parameters as in deep networks, flat networks must place all non-linear hidden units in a single layer, resulting in a large weight matrix W. In forming a large flat neural network with tens of thousands of hidden units, we find out that it is very slow to learn the large number of parameters in the weight matrix between input and hidden layer of size O (HD), where D is the dimension of the input characteristics and H is the number of hidden units. It is difficult to estimate this large weight matrix with gradient descend methods, as many highly correlated parameters are used, so that the learning algorithm runs slowly. We also notice that during learning flat networks spend most of the compilation in the costly matrix multiplication of input data vectors and the large weight matrix. In order to limit the flow of information through the network and the effective matrix to a non-linear number of the matrix."}, {"heading": "7 Experimental results", "text": "We trained flat neural networks and deep neural networks using the cross-entropy cost function on the original 0 / 1 labels. In Table 1 we show a comparison of these models with the mimic flat neural networks trained using models for phoneme compression. We used the same architecture for the Convolutionary Neural Network as in [4]. The precise DNN and CNN models in the table are used to label the training data for the mimic SNNs. The protocol probability of predictive values of the training input characteristics from the deep models are used as a regression target to train mimic SNNs. In addition, we trained a large SNN with 400k of non-linear hidden units combining a linear layer with 250 linear units. This large model was trained to the goal produced by an ensemble of different CNN models. During our evaluation, the insertion penalty and the weighting of the language model can not be further specified on the basis of these parameters or both. We trained flat neural networks and deep neural networks using the cross-entropy cost function on the original 0 / 1 labels."}, {"heading": "7.1 Discussion", "text": "Normally, deep neural networks are much more competitive than flat networks in training with current learning algorithms and crossentropy cost function. We see that by increasing the number of hidden layers from one to three, phoneme recognition performance jumped by 2%, from 23.6% to 21.6%. In addition, a five-layer CNN with sophisticated first and second layers continues to improve performance to 19.5%. Even with failure regulation [5] and intelligent weight initialization [9], the flat networks trained on the original data cannot come close to the performance of the deep models and suffer from revision in increasing the model size. If we train the SNNs using targets produced by other, more accurate models, namely DNN and CNN, the mixed SNNs are able to adjust performance and just as well have a deep, fully networked neural network with comparable number of parameters we can grasp with our size."}, {"heading": "8 Conclusions", "text": "We found that when training on log probability using compression technology and applying certain constraints on model parameters, shallow neural networks can be learned to achieve performance that previously could only be achieved by deep models. We evaluated our phoneme detection method using TIMIT datasets. The flat, fully networked network trained with our method behaves similarly to mature complex, deeply winding architectures. Finally, our strong results on TIMIT suggest new learning algorithms to train feed-forward networks for better performance."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel-rahman Mohamed", "Hui Jiang", "Gerald Penn"], "venue": "In Acoustics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Recent advances in deep learning for speech research at microsoft", "author": ["Li Deng", "Jinyu Li", "Jui-Ting Huang", "Kaisheng Yao", "Dong Yu", "Frank Seide", "Michael Seltzer", "Geoff Zweig", "Xiaodong He", "Jason Williams"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["K-F Lee", "H-W Hon"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "Acoustic modeling using deep belief networks. Audio, Speech, and Language Processing", "author": ["Abdel-rahman Mohamed", "George E Dahl", "Geoffrey Hinton"], "venue": "IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proc. 27th International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "When you train a deeper neural net as in [2] consisting of convolutional layers, pooling layers, and multiple fully-connected feedforward layers on the same data you obtain 90% accuracy on the test set.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "All 61 phoneme labels represented in tri-state, 3 states for each one of the 61 phoneme and 183 dimensions target label vector, are were used during the training and decoding, then mapped to 39 classes as in [6] for scoring.", "startOffset": 208, "endOffset": 211}, {"referenceID": 4, "context": "Deep learning is first successfully applied to speech recognition in [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "DNN is a deep neural net consisting of three fully-connected feedforward hidden layers consisting of 2000 rectified linear units (ReLU) [8] per layer.", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "We used the same architecture for the convolutional neural net as in [4].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "Even with dropout regularization [5] and smart weight initialization [9], the shallow nets trained on the original data are not able to come close to the performance of the deep models and suffer from overfitting when increasing model size.", "startOffset": 33, "endOffset": 36}], "year": 2013, "abstractText": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method on TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available.", "creator": "LaTeX with hyperref package"}}}