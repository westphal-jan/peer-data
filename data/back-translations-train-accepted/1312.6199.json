{"id": "1312.6199", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Intriguing properties of neural networks", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.", "histories": [["v1", "Sat, 21 Dec 2013 03:36:08 GMT  (9313kb,D)", "http://arxiv.org/abs/1312.6199v1", null], ["v2", "Fri, 3 Jan 2014 04:37:34 GMT  (9312kb,D)", "http://arxiv.org/abs/1312.6199v2", null], ["v3", "Thu, 13 Feb 2014 17:40:08 GMT  (9313kb,D)", "http://arxiv.org/abs/1312.6199v3", null], ["v4", "Wed, 19 Feb 2014 16:33:14 GMT  (9313kb,D)", "http://arxiv.org/abs/1312.6199v4", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["christian szegedy", "wojciech zaremba", "ilya sutskever", "joan bruna", "dumitru erhan", "ian goodfellow", "rob fergus"], "accepted": true, "id": "1312.6199"}, "pdf": {"name": "1312.6199.pdf", "metadata": {"source": "CRF", "title": "Intriguing properties of neural networks", "authors": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 Framework", "text": "We first examine the properties of the \u03c6 (x) image and then look for its blind spots. We conduct a series of experiments with a few different networks and three sets of data: \u2022 For the MNIST dataset, we used the following architectures [11] - a simple, fully connected, single-layer network with a Softmax classifier over it. We refer to this network as \"Softmax.\" - A simple, fully connected network with two hidden layers and a classifier. We refer to this network as \"FC.\" - a classifier trained on an auto encoder. We refer to this network as \"AE.\" - a standard convolutionary network that performs well on this dataset: it has a conversion layer, followed by a maximum pooling layer, a fully connected layer, and a final softmax classifier."}, {"heading": "3 Units of: \u03c6(x)", "text": "Traditional computer vision systems rely on feature extraction: Often a single feature is easy to interpret, such as a histogram of colors or quantified local derivatives, which allows to inspect the individual coordinates of the feature space and associate them with meaningful variations in the input area. Similar considerations have been applied in previous work to analyze neural networks applied to computer vision problems, which interpret activation of a hidden unit as a useful feature, looking for input images that maximize the activation value of that single feature [6, 13, 7, 4]. The above-mentioned technique can formally be called visual inspection of images x that have a satisfactory (or near-maximum achievable) value."}, {"heading": "4 Blind Spots in Neural Networks", "text": "So far, unit-level inspection methods have had relatively little use beyond confirming certain intuitions in terms of the complexity of representations learned through a deep neural network. [6, 13, 7, 4] Globally, network-level inspection methods can be useful in the context of explaining classification decisions made by a model [1] and can be used, for example, to better understand the parts of the input-to-output mapping that have led to a correct classification of a particular visual instance (in other words, one can use a trained model for poorly monitored localization). Such global analyses are useful in helping us better understand the input-to-output mapping mapping that is represented by the trained networking. Generally speaking, the output-layer unit of a neural network is a highly nonlinear function of its input. When trained with cross-entropy loss (using the max activation function, it is represented by a)."}, {"heading": "4.1 Formal description", "text": "We also assume that f has an associated continuous loss function, referred to as lossf: Rm \u00b7 {1... k} \u2212 \u2192 R +. For a given x-Rm image and the target label l \u00b2 {1.... k}, we aim to solve the following box-limited optimization problem: \u2022 Minimize the number of x-R \u00b2 objects provided: 1. f (x + r) = l 2. x + r \u00b2 [0, 1] mThe minimizer r may not be unique, but we designate such an x + r for an arbitrarily selected minimizer by D (x, l). Informally, x + r is the next image classified by f. Obviously, D (x, f (x) = f (x), so this task is not trivial only if f (x) 6 = l =. In general, the problem we have classified as l by f > GS is approximate (l)."}, {"heading": "4.2 Experimental results", "text": "Our \"Minimimum Distortion\" function D has the following fascinating properties, which we will also demonstrate with qualitative and quantitative experiments in this field: 1. For all the networks we have studied (MNIST, QuocNet [10], AlexNet [9]), we will always be able to generate very close, visually indistinguishable, contradictory examples that are misclassified by the original network (see Figure 5 for examples).2 Generalizations of examples are misclassified by networks formed by different hyper-parametric parameters (number of levels, regulations, or output weights).The above observations suggest that adversarial examples are somewhat universal and not just the results of overmatching to a particular model or to the specific selection of training sets."}, {"heading": "4.3 Spectral Analysis of Instability", "text": "The previous section showed that the networks resulting from supervised learning are unstable in relation to a particular family of disorders. Instability is mathematically expressed as a collection of pairs (xi, \u0445) i in such a way that the class of unstable perturbations ni depends on the architecture of the phenomenon, but does not depend on the specific training parameters W. The instability of the phenomenon (x; W) can be partially explained by looking at the upper lipschitz constant of each layer, defined as Lk > 0, so that it is x, x \u00b2, x \u00b2), x \u00b2), that the instability of the phenomenon c (x; W), the reflected layers (both convolutionally or fully connected), is a constant c."}, {"heading": "5 Discussion", "text": "We showed that deep neural networks have counterintuitive properties both in terms of the semantic meaning of individual units and in terms of their discontinuities, and the existence of negative negatives seems to be at odds with the network's ability to achieve high generalization power. If the network can generalize well, how can it be confused by these negative negatives, which are indistinguishable from the regular examples? The explanation for this is that the number of negative negatives is extremely low and therefore never (or rarely) observed in the test set, but is dense (similar to rational numbers, and therefore present in virtually every test case)."}], "references": [{"title": "How to explain individual classification decisions", "author": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus- Robert M\u00fcller"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations and trends\u00ae in Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Technical Report 1341,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["Pedro Felzenszwalb", "David McAllester", "Deva Ramanan"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "arXiv preprint arXiv:1311.2524,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Measuring invariances in deep networks", "author": ["Ian Goodfellow", "Quoc Le", "Andrew Saxe", "Honglak Lee", "Andrew Y Ng"], "venue": "Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey E. Hinton", "Li Deng", "Dong Yu", "George E. Dahl", "Abdel rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le", "Marc\u2019Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg S Corrado", "Jeff Dean", "Andrew Y Ng"], "venue": "arXiv preprint arXiv:1112.6209,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "arXiv preprint arXiv:1311.2901,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "Deep neural networks are powerful learning models that achieve excellent performance on visual and speech recognition problems [9, 8].", "startOffset": 127, "endOffset": 133}, {"referenceID": 7, "context": "Deep neural networks are powerful learning models that achieve excellent performance on visual and speech recognition problems [9, 8].", "startOffset": 127, "endOffset": 133}, {"referenceID": 5, "context": "Previous works [6, 13, 7] analyzed the semantic meaning of various units by finding the set of inputs that maximally activate a given unit.", "startOffset": 15, "endOffset": 25}, {"referenceID": 12, "context": "Previous works [6, 13, 7] analyzed the semantic meaning of various units by finding the set of inputs that maximally activate a given unit.", "startOffset": 15, "endOffset": 25}, {"referenceID": 6, "context": "Previous works [6, 13, 7] analyzed the semantic meaning of various units by finding the set of inputs that maximally activate a given unit.", "startOffset": 15, "endOffset": 25}, {"referenceID": 11, "context": "[12] for word representations, where the various directions in the vector space representing the words are shown to give rise to a surprisingly rich semantic encoding of relations and analogies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "\u2022 For the MNIST dataset, we used the following architectures [11] \u2013 A simple fully connected, single layer network with a softmax classifier on top of it.", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "\u2022 The ImageNet dataset [3].", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "al architecture [9].", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "\u2022 \u223c 10M image samples from Youtube (see [10]) \u2013 Unsupervised trained network with \u223c 1 billion learnable parameters.", "startOffset": 40, "endOffset": 44}, {"referenceID": 5, "context": "They look for input images which maximize the activation value of this single feature [6, 13, 7, 4].", "startOffset": 86, "endOffset": 99}, {"referenceID": 12, "context": "They look for input images which maximize the activation value of this single feature [6, 13, 7, 4].", "startOffset": 86, "endOffset": 99}, {"referenceID": 6, "context": "They look for input images which maximize the activation value of this single feature [6, 13, 7, 4].", "startOffset": 86, "endOffset": 99}, {"referenceID": 3, "context": "They look for input images which maximize the activation value of this single feature [6, 13, 7, 4].", "startOffset": 86, "endOffset": 99}, {"referenceID": 5, "context": "So far, unit-level inspection methods had relatively little utility beyond confirming certain intuitions regarding the complexity of the representations learned by a deep neural network [6, 13, 7, 4].", "startOffset": 186, "endOffset": 199}, {"referenceID": 12, "context": "So far, unit-level inspection methods had relatively little utility beyond confirming certain intuitions regarding the complexity of the representations learned by a deep neural network [6, 13, 7, 4].", "startOffset": 186, "endOffset": 199}, {"referenceID": 6, "context": "So far, unit-level inspection methods had relatively little utility beyond confirming certain intuitions regarding the complexity of the representations learned by a deep neural network [6, 13, 7, 4].", "startOffset": 186, "endOffset": 199}, {"referenceID": 3, "context": "So far, unit-level inspection methods had relatively little utility beyond confirming certain intuitions regarding the complexity of the representations learned by a deep neural network [6, 13, 7, 4].", "startOffset": 186, "endOffset": 199}, {"referenceID": 0, "context": "decisions made by a model [1] and can be used to, for instance, identify the parts of the input which led to a correct classification of a given visual input instance (in other words, one can use a trained model for weakly-supervised localization).", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "It has been argued [2] that the deep stack of non-linear layers in between the input and the output unit of a neural network are a way for the model to encode a non-local generalization prior over the input space.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "Figure 5: Adversarial examples generated for AlexNet [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 8, "context": "Already, a variety of recent state of the art computer vision models employ input deformations during training for increasing the robustness and convergence speed of the models [9, 13].", "startOffset": 177, "endOffset": 184}, {"referenceID": 12, "context": "Already, a variety of recent state of the art computer vision models employ input deformations during training for increasing the robustness and convergence speed of the models [9, 13].", "startOffset": 177, "endOffset": 184}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "x+ r \u2208 [0, 1]", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "\u2022 Minimize c|r|+ lossf (x+ r, l) subject to x+ r \u2208 [0, 1]", "startOffset": 51, "endOffset": 57}, {"referenceID": 9, "context": "Figure 6: Adversarial examples for QuocNet [10].", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "For all the networks we studied (MNIST, QuocNet [10], AlexNet [9]), for each sample, we always manage to generate very close, visually indistinguishable, adversarial examples that are misclassified by the original network (see figure 5 for examples).", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "For all the networks we studied (MNIST, QuocNet [10], AlexNet [9]), for each sample, we always manage to generate very close, visually indistinguishable, adversarial examples that are misclassified by the original network (see figure 5 for examples).", "startOffset": 62, "endOffset": 65}, {"referenceID": 0, "context": "The pixel intensities are scaled to be in the range [0, 1].", "startOffset": 52, "endOffset": 58}, {"referenceID": 8, "context": "Table 5 shows the upper and lower bounds computed from the ImageNet deep convolutional network of [9].", "startOffset": 98, "endOffset": 101}], "year": 2013, "abstractText": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network\u2019s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "creator": "LaTeX with hyperref package"}}}