{"id": "1312.6203", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Spectral Networks and Locally Connected Networks on Graphs", "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with $O(1)$ parameters, resulting in efficient deep architectures.", "histories": [["v1", "Sat, 21 Dec 2013 04:25:53 GMT  (1303kb,D)", "http://arxiv.org/abs/1312.6203v1", "11 pages"], ["v2", "Thu, 20 Feb 2014 23:23:06 GMT  (1782kb,D)", "http://arxiv.org/abs/1312.6203v2", "14 pages"], ["v3", "Wed, 21 May 2014 16:27:09 GMT  (1782kb,D)", "http://arxiv.org/abs/1312.6203v3", "14 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["joan bruna", "wojciech zaremba", "arthur szlam", "yann lecun"], "accepted": true, "id": "1312.6203"}, "pdf": {"name": "1312.6203.pdf", "metadata": {"source": "CRF", "title": "Spectral Networks and Locally Connected Networks on Graphs", "authors": ["Joan Bruna", "Wojciech Zaremba"], "emails": ["bruna@cims.nyu.edu", "woj.zaremba@gmail.com", "aszlam@ccny.cuny.edu", "yann@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Convolutionary neural networks have been extremely successful in machine learning problems where the coordinates of the underlying data representation have a grid structure (in 1.2 and 3 dimensions) and the data studied in these coordinates have a translational equivariance / invariance. On a regular grid, a CNN is able to use multiple structures that work well together to substantially reduce the number of parameters in the system, resulting in subsampling.If there are n coordinates on a grid of dimension d, a fully connected network would require O (n2) parameters for each function board, enabling compactly supported filters.3. The multi-scale dyadic clustering of the grid, the subsampling.If there are n coordinates on a grid of dimension d, a fully connected network would require O (n2) parameters for each function scale that we use to form an arbitrary (arbitrary) per (complex) filter layer (O) reduced to one (complex) layer (O)."}, {"heading": "1.1 Locality via W", "text": "In fact, the weights in a diagram determine a concept of locality. A simple way to define neighborhoods on W, for example, is to set a threshold \u03b4 and take neighborhoods N\u03b4 (j) = {i \u0109\u0442: Wij > \u03b4}. We can limit attention to sparse \"filters\" with receptive fields given by these neighborhoods to obtain locally connected networks, and reduce the number of parameters in a filter layer to O (n) if the neighborhoods are sparse."}, {"heading": "1.2 Harmonic analysis on weighted graphs", "text": "The combinatorial Laplacian L = D \u2212 W or diagram Laplacian L = I \u2212 D \u2212 D \u2212 1 / 2WD \u2212 1 / 2 are generalizations of the Laplacian on the grid; and frequency and smoothness w.r.t. W are linked by these operators [1, 14]. For the sake of simplicity, here we use the combinatorial Laplacian. If f is an m vector, then a natural definition of smoothness is functional | 2 W at a node i | 2 W (i) = property filter of the property j Wij [f (i) \u2212 f (j)] 2, and | 2 W = [f (i) \u2212 f (j)] 2, with this definition the smoothest vector is a constant: v0 = property filter of the property j Wij [f (i) \u2212 f (j)]] 2, and the Laplacian vectors of the property of the Laplacian."}, {"heading": "1.3 Multiresolution Analysis on Graphs", "text": "CNNs reduce the size of the network by pooling and subsampling layers. These layers are possible due to the natural, multi-level clustering of the network: they enter all the characteristic maps of a cluster and output a single characteristic for that cluster. On the network, dyadic clustering behaves well with metrics and Lapland (and thus with the translation structure); there is a large literature on the formation of multi-level clusters on diagrams, see for example [10, 14, 4, 8]; the search for multi-level clusters that have been proven to work well is still an open field of research. In this work we will apply a naive agglomerative method."}, {"heading": "1.4 Contributions", "text": "Our contributions are summarized as follows: \u2022 We use robust / simple geometry to construct O (n) networks: We show that it is possible to obtain efficient architectures from a very weak geometric structure using O (n) parameters, which we validate using low-dimensional graph data sets. \u2022 We introduce a construction using O (1) parameters, which we verify empirically, and we discuss their connections with a harmonic analysis problem on graphs."}, {"heading": "2 Spatial and Spectral Constructions", "text": "As described above, W is supposed to be a weighted graph with an index set denoted by \u0430, and V is supposed to be the eigenvectors of the graph Laplacian, ordered by eigenvalue."}, {"heading": "2.1 Spectral construction", "text": "Considering a weighted graph, we can try to generalize a revolutionary mesh by operating on the spectrum of weights (i.e., operating on the laplacian). Suppose we have a real estimated nonlinearity h: R \u2192 R; we can replace standard average pooling / subsampling by dropping a specified number of coefficients in V. That is, a layer of the network consists of ofyk + 1 \u2190 Dkh (UkFkyk) (2.1) with y0 = V Tx, and where x is the input into the network. HereFk = Fk, 1.1... Fk \u2212 1, fk \u2212 1... Fk, 1... Fk, 1... Fk, fk, fk \u2212 1 \u2212 fk \u2212 1, (2.2) where Fk, i, j is a dk \u00d7 dk diagonal matrix with the spectral matrix that is on the diagonal matrix, not on the diagonal plane, fk, the number of the filters, and the number of the fk on the plane is high."}, {"heading": "2.1.1 Rediscovering standard CNN\u2019s", "text": "A simple, in a sense universal, weight matrix in this construction is essentially the covariance of the data. Let X = (xk) k is the input data distribution with which each coordinate j = 1. While it is known that the main components of the image of a fixed magnitude are (experimentally) the Fourier functions organized by frequency. This can be explained by noting that the images are invariable, and satisfying the covariance operator operators (j) = E (x) \u2212 E (j) \u2212 E (x) \u2212 E (j)))))."}, {"heading": "2.2 Spatial construction (locally connected networks)", "text": "Instead of generalizing a CNN by finding spectral multipliers, one can work completely in space. We need a multi-scale clustering of k and neighborhoods on each scale, which is indexed by the rows of matrices Wk (the rows and columns of Wk are indexed by the clusters in k-1, where 0 = 1. With these in hand is one layer of the mesh isxk + 1 \u2190 Lkh (Fkxk). (2,3) In this construction, xk = Fk, 1.1... Fk, 1, fk \u2212 1 vector, where dk is the number of clusters at level k, fk the number of filters at level k, and xk the filter responses are vertically linked. Besides, Fk = Fk, 1.1... Fk, 1... Fk, fk \u2212 1... Fk, fk \u2212 1... Fk, 1... Fk, 1... Fk, 1... Fk, fk, fk, fk \u2212 1, fk, fk \u2212 1, and fk \u2212 1 are linked to the filters on level \u2212 4 (the number of the filters \u2212 1)."}, {"heading": "3 Relationship with previous work", "text": "There is a large literature on the structure of waves on graphs, see for example [12, 5, 2, 3, 7]. A wave base on a grid, in the language of neural networks, is a linear autoencoder with certain detectable regularity properties (especially when coding different classes of smooth functions, the sparseness is measured).Forward propagation in a classical waveform is very similar to forward propagation in a neural network, except that there is only one filter card at each level (and it is usually the same filter at each level), and the output of each layer is maintained, rather than just the output of the last layer. Classically, the filter is not learned, but constructed to facilitate regularity. In the case of graphs, the goal is the same; except that the smoothness on the grid is replaced by smoothness on the graph. As in the classic case, most works have tried to explicitly construct the waveform (some of the work is based on the correctness of the graph)."}, {"heading": "3.1 Multigrid", "text": "We could improve both constructions and unify them to some extent by making a multi-scale clustering of the graph that plays nicely with Lapland. As already mentioned, the standard dyadic cubes have the property that the sample of the Fourier functions on the grid is identical to a coarser grid, like finding the Fourier functions on the coarser grid. This property would eliminate the annoying need to map the spectral construction to the finest grid at each level to achieve nonlinearity; and would allow us (by interpolation) to interpret the local filters in deeper layers of the spatial construction as low frequency. This type of clustering is the basis of the multicultural method for solving discredited PDE's (and linear systems in general) [13]."}, {"heading": "4 Numerical Experiments", "text": "We show experiments with two variations of the MNIST dataset. In the first, we sample the normal 28 x 28 grid to get 400 coordinates. These coordinates still have a 2d structure, but it is not possible to use standard waves. We then create a dataset by placing d = 4096 points on the 2d unit sphere to get the set S. We project random MNIST images onto a random rotation of S by means of bicubic interpolation. In all experiments, we use reflected linear units as nonlinearity. We train the models with cross entropy loss, using a fixed learning rate of 0.1 with momentum of 0.9."}, {"heading": "4.1 Subsampled MNIST", "text": "We apply the constructions from Sections 2.1 and 2.2 to the subsampled MNIST datasets. Figure 4.1 shows examples of the resulting input signals, and Figures 4.1, 4.1 show the hierarchical clustering of the graph and some eigenfunctions of the graph Laplacian, respectively. Performance of the various graph architectures is reported in Table 1. To serve as a basis, we calculate the default Nearest Neighbor classifier, which is slightly inferior to the full MNIST dataset. A two-layer cross-linking reduces the error to 1.8%. The geometric structure of the Nearest Neighbor classifier can be exploited with the full MNIST datasets. Local Receptive fields matching structure exceed the fully connected spectrum (2.8%)."}, {"heading": "4.2 MNIST on the sphere", "text": "In this section, we test the constructions of the CNN graph on another low-dimensional graph. In this case, however, we raise the MNIST digits considerably lower into the sphere. The data set is constructed as follows: We first sample 4096 random points S = {sj} j \u2264 4096 from the unit sphere S2-R3. We then look at a reference frame E = (e1, e2, e3) with the difference that for each signal xi from the original MNIST data set, we sample a covariance operator i from the previous distribution = (E + W) T (E + W), where W defines a Gaussian iid matrix with the variance \u03c32 < 1 for each signal xi from the original MNIST data set, we sample a covariance operator i from the previous distribution and consider its PCA base Ui (E + W) as probable. This basis defines a point of view and the rotation in S. jxi, which we use to 4.xi."}, {"heading": "5 Conclusion", "text": "Using graph-based analogies of wave architectures can significantly reduce the number of parameters in a neural network without exacerbating (and often improving) the test error, while at the same time allowing for faster propagation forward. These methods can be scaled to data with a large number of coordinates that have an idea of locality. There is much to do here. We suspect that with more careful training and deeper networks, we can consistently improve fully connected networks on \"manifold\" graphs such as the tested sphere. Furthermore, we intend to apply these techniques to fewer artificial problems, such as recommendation problems in networks where there is biclustering of data and coordinates. Finally, the fact that smoothness on the naive arrangement of eigenvectors leads to improved results and localized filters suggests that it may be possible to produce \"dual\" constructions with O (1) parameters per filter in a much more general manner than the grid."}], "references": [{"title": "Diffusion wavelets", "author": ["R.R. Coifman", "M. Maggioni"], "venue": "Appl. Comp. Harm. Anal., 21(1):53\u201394, July", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Graph wavelets for spatial traffic analysis", "author": ["Mark Crovella", "Eric D. Kolaczyk"], "venue": "In INFOCOM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Weighted graph cuts without eigenvectors 10  Table 3: Classification results on the MNIST-sphere dataset generated using uniformly random rotations, for different architectures method Parameters Error Nearest Neighbors  NA 80 4096-FC2048-FC512-9", "author": ["Inderjit S. Dhillon", "Yuqiang Guan", "Brian Kulis"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2048}, {"title": "Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi supervised learning", "author": ["Matan Gavish", "Boaz Nadler", "Ronald R. Coifman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Emergence of complex-like cells in a temporal product network with local receptive fields", "author": ["Karol Gregor", "Yann LeCun"], "venue": "CoRR, abs/1006.0448,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Multiresolution signal processing for meshes", "author": ["I. Guskov", "W. Sweldens", "P. Schr\u00f6der"], "venue": "Computer Graphics Proceedings (SIGGRAPH 99), pages 325\u2013334,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Metis - unstructured graph partitioning and sparse matrix ordering system, version 2.0", "author": ["George Karypis", "Vipin Kumar"], "venue": "Technical report,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Efficient multilevel eigensolvers with applications to data analysis tasks", "author": ["D. Kushnir", "M. Galun", "A. Brandt"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(8):1377\u20131391,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast multiscale clustering and manifold identification", "author": ["Dan Kushnir", "Meirav Galun", "Achi Brandt"], "venue": "Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Tiled convolutional neural networks", "author": ["Quoc V. Le", "Jiquan Ngiam", "Zhenghao Chen", "Daniel Chia", "Pang Wei Koh", "Andrew Y. Ng"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Wavelets on graphs via deep learning", "author": ["Raif M. Rustamov", "Leonidas Guibas"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Technical Report 149,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "Using arbitrary filters instead of generic fully connected layers reduces the complexity to O(n) parameters per feature map, as does using the metric structure by building a \u201clocally connected\u201d net [6, 11].", "startOffset": 198, "endOffset": 205}, {"referenceID": 9, "context": "Using arbitrary filters instead of generic fully connected layers reduces the complexity to O(n) parameters per feature map, as does using the metric structure by building a \u201clocally connected\u201d net [6, 11].", "startOffset": 198, "endOffset": 205}, {"referenceID": 11, "context": "W are interrelated through these operators [1, 14].", "startOffset": 43, "endOffset": 50}, {"referenceID": 8, "context": "There is a large literature on forming multiscale clusterings on graphs, see for example [10, 14, 4, 8]; finding multiscale clusterings that are provably guaranteed to behave well w.", "startOffset": 89, "endOffset": 103}, {"referenceID": 11, "context": "There is a large literature on forming multiscale clusterings on graphs, see for example [10, 14, 4, 8]; finding multiscale clusterings that are provably guaranteed to behave well w.", "startOffset": 89, "endOffset": 103}, {"referenceID": 2, "context": "There is a large literature on forming multiscale clusterings on graphs, see for example [10, 14, 4, 8]; finding multiscale clusterings that are provably guaranteed to behave well w.", "startOffset": 89, "endOffset": 103}, {"referenceID": 6, "context": "There is a large literature on forming multiscale clusterings on graphs, see for example [10, 14, 4, 8]; finding multiscale clusterings that are provably guaranteed to behave well w.", "startOffset": 89, "endOffset": 103}, {"referenceID": 10, "context": "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].", "startOffset": 76, "endOffset": 92}, {"referenceID": 3, "context": "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].", "startOffset": 76, "endOffset": 92}, {"referenceID": 0, "context": "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].", "startOffset": 76, "endOffset": 92}, {"referenceID": 1, "context": "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].", "startOffset": 76, "endOffset": 92}, {"referenceID": 5, "context": "There is a large literature on building wavelets on graphs, see for example [12, 5, 2, 3, 7].", "startOffset": 76, "endOffset": 92}, {"referenceID": 10, "context": "In this work, and the recent work [12], the \u201cfilters\u201d are constrained by construction to have some of the regularity properties of wavelets, but are also trained so that they are appropriate for a task separate from (but perhaps related to) the smoothness on the graph.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "Whereas [12] still builds a (sparse) linear autoencoder that keeps the basic wavelet transform setup, this work focuses on nonlinear constructions; and in particular, tries to build analogues of CNN\u2019s.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "There have been several papers extending the multigrid method, and in particular, the multiscale clustering(s) associated to the multigrid method, in settings more general than regular grids, see for example [10, 9] for situations as in this paper, and see [13] for the algebraic multigrid method in general.", "startOffset": 208, "endOffset": 215}, {"referenceID": 7, "context": "There have been several papers extending the multigrid method, and in particular, the multiscale clustering(s) associated to the multigrid method, in settings more general than regular grids, see for example [10, 9] for situations as in this paper, and see [13] for the algebraic multigrid method in general.", "startOffset": 208, "endOffset": 215}], "year": 2017, "abstractText": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with O(1) parameters, resulting in efficient deep architectures.", "creator": "LaTeX with hyperref package"}}}