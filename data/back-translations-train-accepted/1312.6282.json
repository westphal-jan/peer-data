{"id": "1312.6282", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning", "abstract": "Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution. These methods rely on a singular value decomposition of a matrix $H_S$, called the Hankel matrix, that records the frequencies of (some of) the observed strings. The accuracy of the learned distribution depends both on the quantity of information embedded in $H_S$ and on the distance between $H_S$ and its mean $H_r$. Existing concentration bounds seem to indicate that the concentration over $H_r$ gets looser with the size of $H_r$, suggesting to make a trade-off between the quantity of used information and the size of $H_r$. We propose new dimension-free concentration bounds for several variants of Hankel matrices. Experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. These results suggest that the concentration rate of the Hankel matrix around its mean does not constitute an argument for limiting its size.", "histories": [["v1", "Sat, 21 Dec 2013 18:10:59 GMT  (65kb,D)", "http://arxiv.org/abs/1312.6282v1", "Extended version of a paper to appear at ICML 2014"]], "COMMENTS": "Extended version of a paper to appear at ICML 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fran\u00e7ois denis", "mattias gybels", "amaury habrard"], "accepted": true, "id": "1312.6282"}, "pdf": {"name": "1312.6282.pdf", "metadata": {"source": "META", "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning", "authors": ["Fran\u00e7ois Denis", "Mattias Gybels", "Amaury Habrard"], "emails": ["FRANCOIS.DENIS@LIF.UNIV-MRS.FR", "MATTIAS.GYBELS@LIF.UNIV-MRS.FR", "AMAURY.HABRARD@UNIV-ST-ETIENNE.FR"], "sections": [{"heading": "1. Introduction", "text": "Many applications in natural language processing, text analysis, or computer-aided biology require learning probability models on finite variable size concepts (2009). However, the weighted models of the rational series and their algebraic properties are widely used in this context (Droste et al., 2009). In particular, they acknowledge that algebraic representations, which can be characterized by a series of finite linear operators whose rank corresponds to the minimum number of states necessary to define automation, have the goal of deriving good estimates of these linear operators from finite samples from a machine learning perspective. In this paper, we look at the problem of linear representation of a weighted automaton, consisting of a finite sample of variable strands i.i.d. from an unknown target distribution.Recently, the seminal papers of Hsu al."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Singular Values, Eigenvalues and Matrix Norms", "text": "The singular values of M are the square roots of the eigenvalues of the matrix MTM, where MT denotes the transposition of M: \u03c3max (M) and \u03c3min (M) each denote the largest and smallest singular values of M. In this essay, we mainly use the spectral norms | | | | k, induced by the corresponding vector norms to Rn and induced by | | M | | k = maxx 6 = 0 | | Mx | k: \u2022 | | M | | 1 = Max1 \u2264 j = 1 | M [i, j] |, \u2022 | M | 1 = Max1 \u2264 j = 1 | | | M = Max1 \u2264 i, j] |, \u2022 | Max1 \u2264 i = 1 \u2264 i \u2264 m \u00b2."}, {"heading": "2.2. Rational stochastic languages and Hankel matrices", "text": "The set of all finite strings placed above us is defined by the number of strings. (For each string, the length of the string w is defined by the number of strings w. (For each string w), the number of strings of length n (resp.) is defined. (For each string w (w), the number of strings is defined by the number of strings w = uv}.A row is a figure r: 7 \u2192 R. A row r is convergent if the sequence r (resp.) = number of strings w (w) is convergent; its boundary is specified by the number of strings. (n) A stochastic language p is a probability distribution over the number of strings. (i.e.) A row that does not include negative values and convergence to 1.Let n."}, {"heading": "2.3. Spectral Algorithm for Learning Rational Stochastic Languages", "text": "The Rational Series allow a canonical linear representation, which is determined by their Hankel matrix. Let us have a rational series of rankings and rankings of rankings and rankings of rankings of rankings and rankings of rankings of rankings and rankings of rankings and rankings of rankings and rankings of rankings and rankings of rankings and rankings of rankings and rankings of rankings and rankings of rankings of rankings and rankings of rankings of rankings and rankings of rankings of rankings and rankings of rankings of rankings, their rankings and rankings of rankings of rankings and rankings of rankings of rankings and rankings of rankings of rankings and rankings of rankings of rankings and rankings of rankings of rankings of rankings and rankings of rankings of rankings of rankings and rankings of rankings of rankings of rankings of rankings and rankings of rankings of rankings of rankings of rankings and rankings of rankings of rankings of rankings of rankings and rankings of rankings of rankings of and rankings of rankings of rankings of rankings of and rankings of rankings of rankings of and rankings of rankings of rankings of and rankings of rankings of rankings of rankings of and rankings of rankings of rankings of rankings of and rankings of rankings of rankings of rankings of rankings of rankings of and of rankings of rankings of rankings of rankings of and rankings of rankings of rankings of rankings of rankings of and rankings of rankings of rankings of rankings of rankings of rankings of and rankings of rankings of rankings of and rankings of rankings of rankings of rankings of rankings of rankings of rankings of rankings of and rankings of rankings of rankings of rankings of rankings of and rankings of rankings of rankings of rankings of rankings of rankings of rankings of."}, {"heading": "3. Concentration Bounds for Hankel Matrices", "text": "Let us have a rational stochastic language about the size of S, d is the minimum dimension of the matrix."}, {"heading": "3.3. Bound for the factor Hankel Matrix Hp\u0302U,V", "text": "Random Matrix Z > is a minimum linear representation of p then < I >. Random Matrix Z > is a minimum linear representation of p >. Random Matrix Z > is a minimum linear representation of p >. Random Matrix Z >. Random Matrix Z >."}, {"heading": "4. Experiments", "text": "The proposed limits are evaluated on the basis of the PAutomaC benchmark (Verwer et al., 2012), which provides examples of strings generated from multiple probabilistic automatons designed to evaluate probabilistic automatic learning. Eleven problems were selected from this benchmark, for which the economy of Hankel matrices allows the use of standard SVD algorithms from NumPy or SciPy. Table 1 provides some information about the selected problems. Figure 1 shows the typical behavior of S (1) and S (1) p, similar for all problems. For each problem, the exact value of | HU, VS \u2212 HU, Vp | 2 is calculated for the sentences U and V of the \u2264 l form, attempting to maximize l according to our computational resources. It is compared with the limits provided by Theorem 3 and Equation (1), with a distance of 0.05 (Table 2)."}, {"heading": "5. Conclusion", "text": "We have provided dimensional-free concentration imbalances for Hankel matrices in the context of spectral learning of rational stochastic languages. These limits include 3 cases, each of which corresponds to a specific way of using the observed strings, paying attention to the strings themselves, their prefixes, or their factors. In the latter two cases, we have introduced parameterized variants that allow for a trade-off between the concentration rate and the use of the information contained in data.One consequence of these results is that, apart from resource constraints, there is no good reason to limit the size of Hankel matrices from the outset, indicating immediate future work consisting of studying newer random techniques (Halko et al., 2011) to calculate singular values that disassemble Hankel matrices in order to deal with giant matrices. Then, a second aspect is to assess the impact of these methods on the quality of the models (Halko et al.)."}], "references": [{"title": "A spectral algorithm for latent dirichlet allocation", "author": ["A. Anandkumar", "D.P. Foster", "D. Hsu", "S. Kakade", "Liu", "Y.-K"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Learning mixtures of tree graphical models", "author": ["A. Anandkumar", "D. Hsu", "F. Huang", "S. Kakade"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "Proceedings of COLT - Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "M\u00e9thodes spectrales pour l\u2019inf\u00e9rence grammaticale probabiliste de langages stochastiques rationnels", "author": ["R. Bailly"], "venue": "PhD thesis, Aix-Marseille Universite\u0301,", "citeRegEx": "Bailly,? \\Q2011\\E", "shortCiteRegEx": "Bailly", "year": 2011}, {"title": "Grammatical inference as a principal component analysis problem", "author": ["R. Bailly", "F. Denis", "L. Ralaivola"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Bailly et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bailly et al\\.", "year": 2009}, {"title": "A spectral approach for probabilistic grammatical inference on trees", "author": ["R. Bailly", "A. Habrard", "F. Denis"], "venue": "In Proceedings of ALT,", "citeRegEx": "Bailly et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bailly et al\\.", "year": 2010}, {"title": "Spectral learning of general weighted automata via constrained matrix completion", "author": ["B. Balle", "M. Mohri"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Balle and Mohri,? \\Q2012\\E", "shortCiteRegEx": "Balle and Mohri", "year": 2012}, {"title": "A spectral learning algorithm for finite state transducers", "author": ["B. Balle", "A. Quattoni", "X. Carreras"], "venue": "In Proceedings of ECML/PKDD", "citeRegEx": "Balle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2011}, {"title": "Local loss optimization in operator models: A new insight into spectral learning", "author": ["B. Balle", "A. Quattoni", "X. Carreras"], "venue": "In Proceedings of ICML,", "citeRegEx": "Balle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2012}, {"title": "Spectral learning for non-deterministic dependency parsing", "author": ["F.M. Luque", "A. Quattoni", "B. Balle", "X. Carreras"], "venue": "In Proceedings of EACL,", "citeRegEx": "Luque et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Luque et al\\.", "year": 2012}, {"title": "A spectral algorithm for latent tree graphical models", "author": ["A.P. Parikh", "L. Song", "E.P. Xing"], "venue": "In Proceedings of ICML,", "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "Reduced-rank hidden Markov models", "author": ["S. Siddiqi", "B. Boots", "G.J. Gordon"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),", "citeRegEx": "Siddiqi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Siddiqi et al\\.", "year": 2010}, {"title": "Hilbert space embeddings of hidden markov models", "author": ["L. Song", "B. Boots", "S.M. Siddiqi", "G.J. Gordon", "A.J. Smola"], "venue": "In Proceedings of ICML,", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Perturbation theory for the singular value decomposition", "author": ["G.W. Stewart"], "venue": "In SVD and Signal Processing II: Algorithms, Analysis and Applications,", "citeRegEx": "Stewart,? \\Q1990\\E", "shortCiteRegEx": "Stewart", "year": 1990}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["J.A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Tropp,? \\Q2012\\E", "shortCiteRegEx": "Tropp", "year": 2012}, {"title": "Results of the PAutomaC probabilistic automaton learning competition", "author": ["S. Verwer", "R. Eyraud", "C. de la Higuera"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Verwer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Verwer et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "(2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012).", "startOffset": 180, "endOffset": 262}, {"referenceID": 12, "context": "(2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012).", "startOffset": 180, "endOffset": 262}, {"referenceID": 8, "context": "(2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012).", "startOffset": 180, "endOffset": 262}, {"referenceID": 5, "context": "Extensions to probabilistic models for treestructured data (Bailly et al., 2010; Parikh et al., 2011; Cohen et al., 2012), transductions (Balle et al.", "startOffset": 59, "endOffset": 121}, {"referenceID": 10, "context": "Extensions to probabilistic models for treestructured data (Bailly et al., 2010; Parikh et al., 2011; Cohen et al., 2012), transductions (Balle et al.", "startOffset": 59, "endOffset": 121}, {"referenceID": 7, "context": ", 2012), transductions (Balle et al., 2011) or other graphical models (Anandkumar et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 9, "context": ", 2011) or other graphical models (Anandkumar et al., 2012c;b;a; Luque et al., 2012) have also attracted a lot of interest.", "startOffset": 34, "endOffset": 84}, {"referenceID": 0, "context": "(2009) for learning HMM and Bailly et al. (2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "It can be shown that the above learning scheme, or slight variants of it, are consistent as soon as the matrix H S has full rank (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||H S \u2212H p ||2 between the empirical Hankel matrix and its mean (Hsu et al.", "startOffset": 129, "endOffset": 181}, {"referenceID": 8, "context": "It can be shown that the above learning scheme, or slight variants of it, are consistent as soon as the matrix H S has full rank (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||H S \u2212H p ||2 between the empirical Hankel matrix and its mean (Hsu et al.", "startOffset": 129, "endOffset": 181}, {"referenceID": 3, "context": ", 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||H S \u2212H p ||2 between the empirical Hankel matrix and its mean (Hsu et al., 2009; Bailly, 2011).", "startOffset": 169, "endOffset": 201}, {"referenceID": 9, "context": "In order to limit the loss of information when dealing with restricted sets U and V , a general trend is to work with other functions than the target p, such as the prefix function p(u) = \u2211 v\u2208\u03a3\u2217 p(uv) or the factor function p\u0302 = \u2211 v,w\u2208\u03a3\u2217 p(vuw) (Balle et al., 2013; Luque et al., 2012).", "startOffset": 245, "endOffset": 285}, {"referenceID": 15, "context": "These bounds are evaluated on a benchmark made of 11 problems extracted from the PAutomaC challenge (Verwer et al., 2012).", "startOffset": 100, "endOffset": 121}, {"referenceID": 4, "context": "Then, \u3008RE, (RTxR)x\u2208\u03a3, RP \u3009 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).", "startOffset": 59, "endOffset": 132}, {"referenceID": 3, "context": "Then, \u3008RE, (RTxR)x\u2208\u03a3, RP \u3009 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).", "startOffset": 59, "endOffset": 132}, {"referenceID": 8, "context": "Then, \u3008RE, (RTxR)x\u2208\u03a3, RP \u3009 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).", "startOffset": 59, "endOffset": 132}, {"referenceID": 13, "context": "The Stewart formula (Stewart, 1990) bounds the principle angle \u03b8 between the spaces spanned by the right singular vectors of R and RS : |sin(\u03b8)| \u2264 ||HU\u00d7V S \u2212HU\u00d7V r ||2 \u03c3min(H U\u00d7V r ) .", "startOffset": 20, "endOffset": 35}, {"referenceID": 14, "context": "We then use recent results (Tropp, 2012; Hsu et al., 2011) to obtain dimension-free concentration bounds for Hankel matrices.", "startOffset": 27, "endOffset": 58}, {"referenceID": 15, "context": "Experiments The proposed bounds are evaluated on the benchmark of PAutomaC (Verwer et al., 2012) which provides samples of strings generated from several probabilistic automata, designed to evaluate probabilistic automata learning.", "startOffset": 75, "endOffset": 96}], "year": 2013, "abstractText": "Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution. These methods rely on a singular value decomposition of a matrix HS , called the Hankel matrix, that records the frequencies of (some of) the observed strings. The accuracy of the learned distribution depends both on the quantity of information embedded in HS and on the distance betweenHS and its mean Hr. Existing concentration bounds seem to indicate that the concentration over Hr gets looser with the size of Hr, suggesting to make a tradeoff between the quantity of used information and the size of Hr. We propose new dimensionfree concentration bounds for several variants of Hankel matrices. Experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. These results suggest that the concentration rate of the Hankel matrix around its mean does not constitute an argument for limiting its size.", "creator": "LaTeX with hyperref package"}}}