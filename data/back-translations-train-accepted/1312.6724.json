{"id": "1312.6724", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2013", "title": "Local algorithms for interactive clustering", "abstract": "We study the design of interactive clustering algorithms for data sets satisfying natural stability assumptions. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable features in many applications. We show that in this constrained setting one can still design provably efficient algorithms that produce accurate clusterings. We also show that our algorithms perform well on real-world data.", "histories": [["v1", "Tue, 24 Dec 2013 00:16:37 GMT  (2189kb,D)", "https://arxiv.org/abs/1312.6724v1", null], ["v2", "Fri, 7 Feb 2014 05:12:20 GMT  (2189kb,D)", "http://arxiv.org/abs/1312.6724v2", null], ["v3", "Thu, 19 Mar 2015 23:45:54 GMT  (2403kb,D)", "http://arxiv.org/abs/1312.6724v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["pranjal awasthi", "maria-florina balcan", "konstantin voevodski"], "accepted": true, "id": "1312.6724"}, "pdf": {"name": "1312.6724.pdf", "metadata": {"source": "CRF", "title": "Local algorithms for interactive clustering", "authors": ["Pranjal Awasthi", "Maria Florina Balcan", "Konstantin Voevodski"], "emails": ["PAWASTHI@CS.CMU.EDU", "NINAMF@CS.CMU.EDU", "KVODSKI@GOOGLE.COM"], "sections": [{"heading": null, "text": "Assumptions. Our algorithms start with each initial cluster and only make local changes in each step; both are desirable characteristics in many applications. We show that in this constrained environment, it is still possible to design efficient algorithms that produce precise clustering. We also show that our algorithms work well on real data."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Notation and Preliminaries", "text": "Given a dataset of n points, we define C = {C1, C2,. Ck} as a k clustering of X, where the Ci's represent the individual clusters. Given two clusters C and C, \"we define the distance between a cluster Ci\" C \"and clustering C\" as: dist (Ci, \"C\"). It values 0 if all the points in Ci are contained in a single cluster in C. Of course, we can then define the distance between C \"and C\" as: dist (C \") = Ci\" dist (Ci, \"C\"). Notice that this notion of cluster removal is asymmetrical: dist (C, \"C\")."}, {"heading": "2.1 Generalized clustering error", "text": "We observe that the clustering errors defined in the previous section can be generalized by abstracting their common properties. We define the following properties of a natural clustering error, which is any integer error that decreases as we improve the proposed cluster locally. Definition 7 We say that a clustering error is natural if it fulfills the following properties: \u2022 If there is a cluster Ci containing points of Ci, and some other ground truth cluster (s), then that cluster must be divided into two clusters. Ci, 1 = Ci, C, which only contain points of C, and Ci, 2 = Ci \u2212 Ci, 1 (which contains the other points) must reduce the error. If there are two clusters containing only points from the same target cluster, then they must merge into one cluster. \u2022 The error is integer-valued.We expect a lot of definitions of the clustering error to meet the above criteria."}, {"heading": "3. The \u03b7-merge model", "text": "In this section, we describe and analyze the algorithms in Figure 1 and Figure 2. To implement Figure 1 in Figure 1, we first have to go through the individual processing steps for all of our algorithms before we can calculate the hierarchical linkage algorithms for all points in the database that we use to globally link the two most similar nodes, where the similarity between two nodes N1 and N2 is the average similarity between the points in N2.We assign a \"impure\" to each cluster in the initial cluster formation; these labels are used by the merge procedure."}, {"heading": "3.1 Algorithms for correlation-clustering error", "text": "To limit the number of beriberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiberiber"}, {"heading": "3.2 Algorithms under stronger assumptions", "text": "If the data complies with the strict separation regulations of Balcan and al (2008), we can modify the splitting and merging of algorithms to apply the local error threshold constructed by only the points in the processing request. Furthermore, if the data complies with strict separation regulations, we can remove the restrictions on splitting and merging algorithms that are correct for each threshold > 0.Theorem 17 Suppose the target clustering complies with strict separation, and the initial clustering results in errors and underclustering errors. Within the framework of the systems merge model, the algorithms in Figure 4 and Figure 5 may require strict separation."}, {"heading": "4. The unrestricted-merge model", "text": "In this section, we will relax the assumptions about the nature of oracle queries further. As previously, the oracle may require that a cluster be split if it contains dots from two or more target clusters. Therefore, for mergers, the oracle may require that Ci and Cj be merged if both clusters contain only a single point from the same ground truth cluster. We note that this is a minimal set of assumptions for a local algorithm to make progress, otherwise, the oracle may always propose irrelevant splits or mergers that cannot reduce the cluster error. For this model, we propose the merger of algorithms described in Figure 7. The shared algorithms remain the same as in Figure 1. To prove the truth accumulated in this setting, we must select any merge requirement in order to achieve a random number of realizable mergers, which is consistent with the observations in Awasthi and the unlimited 2010 model."}, {"heading": "5. Experimental Results", "text": "We are conducting two experiments: First, we are testing the proposed splitting process for the aggregation of corporate records managed by Google, and then we are testing the proposed framework in its entirety against the much smaller set of newsgroup documents."}, {"heading": "5.1 Clustering business listings", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4 -117 -117", "text": "is clean), while 2-median is able to reduce the cc error even if the resulting partition is not clean. Overall, the two algorithms result in a tie in performance in 12 cases; in 4 cases, CleanSplit makes greater progress in reducing the correlation clustering error; and in 4 cases, 2-median makes greater progress. Note also that clean-split does not reduce the cc error only once; while 2-median does not reduce the cc error four times."}, {"heading": "5.2 Clustering newsgroup documents", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "5.2.2 EXPERIMENTS IN THE UNRESTRICTED-MERGE MODEL", "text": "We also experiment with algorithms in the unrestricted merge model. Here, we use the same algorithm to perform the splits, but we use the algorithm in Figure 7. We show the results on data set A in Figure 9. Full experimental results are in the appendix. We find that our results for larger settings of \u03b7 are better than our theoretical analysis (we only show results for \u03b7 \u2265 0.5), and performance continues to improve for truncated data sets. Our research shows that for untruncated data sets and smaller settings of \u03b7, we are still able to quickly approach the target clustering, but the algorithms are unable to approach the target due to inconsistencies in the average link tree. We can fix some of these inconsistencies by constructing the tree in a more robust way, which in fact results in improved performance for untruncated data sets."}, {"heading": "5.2.3 EXPERIMENTS WITH SMALL INITIAL ERROR", "text": "To simulate this scenario in the calculation of the initial cluster, we maintain for each document the cluster mapping between reason and truth with a probability of 0.95 and otherwise assign it to one of the other clusters randomly selected. This procedure usually gives us initial clusters with a subarcluster and subarcluster error between 5 and 20 and a correlation cluster error between 500 and 1000. As expected, our interactive algorithms work much better in this setting, especially on printed data sets. Figure 10 shows the results; we see that in these cases it often takes less than a hundred processing requests to find the target clustering in both models."}, {"heading": "5.2.4 IMPROVED PERFORMANCE USING A ROBUST AVERAGE-LINKAGE TREE", "text": "When we examine the inconsistencies in the average linkage trees, we find that there are \"outliers\" located near the root of the tree that the algorithm incorrectly splits off and merges back together without making progress in finding the target cluster. We can counter these outliers by constructing the average linkage tree in a more robust way: First, we find groups (\"blobs\") of similar points of a certain minimum size, calculate an average linkage tree for each group, and then merge these trees using the average linkage. The tree constructed in this way can then be used by our algorithms. We tried this approach using algorithm 2 from Balcan and Gupta (2010) to calculate the \"blobs.\""}, {"heading": "6. Discussion", "text": "In this thesis, we have developed a new framework and algorithms for interactive clustering. Our framework models practical limitations of the algorithms: We start with an initial cluster that we cannot change arbitrarily and that can only perform local processing in accordance with user requirements. In this setting, we develop several simple but effective algorithms under different assumptions about the nature of the processing requirements and the structure of the data. We present theoretical analyses that show that our algorithms converge after a small number of processing requests to the target cluster. We also present experimental evidence that our algorithms work well in practice. Several directions result from this work. It would be interesting to loosen the condition in the \u03b7-merge model and the assumption about the requirement sequences in the unrestricted merge model. It is important to study additional properties of an interactive cluster algorithm. In particular, it is often desirable that the algorithm never increases the current error of the cluster."}, {"heading": "Appendix A. Complete Experimental Results", "text": "The following figures show the complete experimental results for all algorithms. Figure 12 and Figure 13 provide the results in the \u03b7 merger model. Figure 14 and Figure 15 provide the results in the \u03b7 merger model for the algorithms in Figure 1 and Figure 3 (for the correlation clustering target). Figure 16 and Figure 17 provide the results in the unrestricted merge model. Dataset AData Set BData Set CData Set AData Set BData Set CData Set AData Set BData Set BData Set C"}], "references": [{"title": "On spectral learning of mixtures of distributions", "author": ["D. Achlioptas", "F. McSherry"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "Queries and concept learning", "author": ["D. Angluin"], "venue": "Machine Learning,", "citeRegEx": "Angluin.,? \\Q1998\\E", "shortCiteRegEx": "Angluin.", "year": 1998}, {"title": "Learning mixtures of arbitrary Gaussians", "author": ["S. Arora", "R. Kannan"], "venue": "In Proceedings of the 33rd ACM Symposium on Theory of Computing,", "citeRegEx": "Arora and Kannan.,? \\Q2001\\E", "shortCiteRegEx": "Arora and Kannan.", "year": 2001}, {"title": "Supervised clustering", "author": ["Pranjal Awasthi", "Reza Bosagh Zadeh"], "venue": "In NIPS,", "citeRegEx": "Awasthi and Zadeh.,? \\Q2010\\E", "shortCiteRegEx": "Awasthi and Zadeh.", "year": 2010}, {"title": "Clustering with interactive feedback", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "In ALT,", "citeRegEx": "Balcan and Blum.,? \\Q2008\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2008}, {"title": "Robust hierarchical clustering", "author": ["Maria-Florina Balcan", "Pramod Gupta"], "venue": "In COLT,", "citeRegEx": "Balcan and Gupta.,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Gupta.", "year": 2010}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "In Proceedings of the 40th annual ACM symposium on Theory of computing,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Correlation clustering", "author": ["Nikhil Bansal", "Avrim Blum", "Shuchi Chawla"], "venue": "Machine Learning,", "citeRegEx": "Bansal et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2004}, {"title": "Active semisupervision for pairwise constrained clustering", "author": ["Sugato Basu", "A. Banjeree", "ER. Mooney", "Arindam Banerjee", "Raymond J. Mooney"], "venue": "Proceedings of the 2004 SIAM International Conference on Data Mining (SDM-04,", "citeRegEx": "Basu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2004}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In FOCS,", "citeRegEx": "Belkin and Sinha.,? \\Q2010\\E", "shortCiteRegEx": "Belkin and Sinha.", "year": 2010}, {"title": "Combining multiple clustering systems", "author": ["Constantinos Boulis", "Mari Ostendorf"], "venue": "European conference on Principles and Practice of Knowledge Discovery in Databases(PKDD),", "citeRegEx": "Boulis and Ostendorf.,? \\Q2004\\E", "shortCiteRegEx": "Boulis and Ostendorf.", "year": 2004}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S. Charles Brubaker", "Santosh Vempala"], "venue": "CoRR, abs/0804.3575,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "A structured family of clustering and tree construction methods", "author": ["David Bryant", "Vincent Berry"], "venue": "Adv. Appl. Math.,", "citeRegEx": "Bryant and Berry.,? \\Q2001\\E", "shortCiteRegEx": "Bryant and Berry.", "year": 2001}, {"title": "Bayesian maximum margin clustering", "author": ["Bo Dai", "Baogang Hu", "Gang Niu"], "venue": "In Proceedings of the 2010 IEEE International Conference on Data Mining, ICDM", "citeRegEx": "Dai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2010}, {"title": "Learning mixtures of Gaussians", "author": ["S. Dasgupta"], "venue": "In Proceedings of the 40th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Hierarchical sampling for active learning", "author": ["Sanjoy Dasgupta", "Daniel Hsu"], "venue": "In ICML,", "citeRegEx": "Dasgupta and Hsu.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Hsu.", "year": 2008}, {"title": "Bayesian hierarchical clustering", "author": ["Katherine A. Heller", "Zoubin Ghahramani"], "venue": "In ICML,", "citeRegEx": "Heller and Ghahramani.,? \\Q2005\\E", "shortCiteRegEx": "Heller and Ghahramani.", "year": 2005}, {"title": "Efficiently learning mixtures of two Gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "In STOC,", "citeRegEx": "Kalai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2010}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory,", "citeRegEx": "Kannan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2005}, {"title": "Efficient active algorithms for hierarchical clustering", "author": ["Akshay Krishnamurthy", "Sivaraman Balakrishnan", "Min Xu", "Aarti Singh"], "venue": null, "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "Comparing clusterings - an information based distance", "author": ["Marina Meil\u0103"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Meil\u0103.,? \\Q2007\\E", "shortCiteRegEx": "Meil\u0103.", "year": 2007}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In FOCS,", "citeRegEx": "Moitra and Valiant.,? \\Q2010\\E", "shortCiteRegEx": "Moitra and Valiant.", "year": 2010}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley"], "venue": "Information processing and management,", "citeRegEx": "Salton and Buckley.,? \\Q1988\\E", "shortCiteRegEx": "Salton and Buckley.", "year": 1988}, {"title": "Active clustering of biological sequences", "author": ["Konstantin Voevodski", "Maria-Florina Balcan", "Heiko R\u00f6glin", "Shang-Hua Teng", "Yu Xia"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Voevodski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Voevodski et al\\.", "year": 2012}, {"title": "Generative model-based document clustering: a comparative study", "author": ["Shi Zhong"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Zhong.,? \\Q2005\\E", "shortCiteRegEx": "Zhong.", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al.", "startOffset": 119, "endOffset": 150}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al.", "startOffset": 119, "endOffset": 172}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al.", "startOffset": 119, "endOffset": 189}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al.", "startOffset": 119, "endOffset": 214}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al.", "startOffset": 119, "endOffset": 243}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al. (2010); Moitra and Valiant (2010); Belkin and Sinha (2010).", "startOffset": 119, "endOffset": 264}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al. (2010); Moitra and Valiant (2010); Belkin and Sinha (2010).", "startOffset": 119, "endOffset": 291}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al. (2010); Moitra and Valiant (2010); Belkin and Sinha (2010). Although this helps define the \u201cright\u201d clustering one should be looking for, real-world data rarely comes from such well-behaved probabilistic models.", "startOffset": 119, "endOffset": 316}, {"referenceID": 7, "context": "We also develop algorithms for the well-known correlation-clustering objective function Bansal et al. (2004), which considers pairs of points that are clustered inconsistently with respect to the target clustering (See Section 2).", "startOffset": 88, "endOffset": 109}, {"referenceID": 9, "context": "We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005); Dasgupta and Hsu (2008); Dai et al.", "startOffset": 161, "endOffset": 177}, {"referenceID": 9, "context": "We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005); Dasgupta and Hsu (2008); Dai et al.", "startOffset": 161, "endOffset": 207}, {"referenceID": 9, "context": "We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005); Dasgupta and Hsu (2008); Dai et al.", "startOffset": 161, "endOffset": 232}, {"referenceID": 9, "context": "We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005); Dasgupta and Hsu (2008); Dai et al. (2010); Boulis and Ostendorf (2004); Zhong (2005).", "startOffset": 233, "endOffset": 251}, {"referenceID": 7, "context": "(2010); Boulis and Ostendorf (2004); Zhong (2005).", "startOffset": 8, "endOffset": 36}, {"referenceID": 7, "context": "(2010); Boulis and Ostendorf (2004); Zhong (2005). Still, we find that our algorithms perform fairly well; for larger settings of \u03b7 we are able find the target clustering after a limited number of edit requests.", "startOffset": 8, "endOffset": 50}, {"referenceID": 2, "context": "Related work Interactive models for clustering studied in previous works Balcan and Blum (2008); Awasthi and Zadeh (2010) were inspired by an analogous model for learning under feedback Angluin (1998).", "startOffset": 73, "endOffset": 96}, {"referenceID": 2, "context": "Related work Interactive models for clustering studied in previous works Balcan and Blum (2008); Awasthi and Zadeh (2010) were inspired by an analogous model for learning under feedback Angluin (1998).", "startOffset": 97, "endOffset": 122}, {"referenceID": 1, "context": "Related work Interactive models for clustering studied in previous works Balcan and Blum (2008); Awasthi and Zadeh (2010) were inspired by an analogous model for learning under feedback Angluin (1998). In this model, the algorithm can propose a hypothesis to the user (in this case, a clustering of the data) and get some feedback regarding the correctness of the current hypothesis.", "startOffset": 186, "endOffset": 201}, {"referenceID": 7, "context": "Basu et al. Basu et al. (2004) study the problem of minimizing the k-means objective in the presence of limited supervision.", "startOffset": 0, "endOffset": 31}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001).", "startOffset": 174, "endOffset": 195}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict threshold separation Balcan et al.", "startOffset": 174, "endOffset": 220}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict threshold separation Balcan et al. (2008); Krishnamurthy et al.", "startOffset": 174, "endOffset": 381}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict threshold separation Balcan et al. (2008); Krishnamurthy et al. (2012). This property is known to hold for real-world data.", "startOffset": 174, "endOffset": 410}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict threshold separation Balcan et al. (2008); Krishnamurthy et al. (2012). This property is known to hold for real-world data. In particular, Voevodski et al. (2012) observed that this property holds for protein sequence data, where similarities are computed with sequence alignment and ground truth clusters correspond to evolutionary-related proteins.", "startOffset": 174, "endOffset": 502}, {"referenceID": 6, "context": "In addition, we also study the following stronger properties of a clustering, which were first introduced in Balcan et al. (2008).", "startOffset": 109, "endOffset": 130}, {"referenceID": 20, "context": "For additional discussion about comparing clusterings see Meil\u0103 (2007). Note that several criteria discussed in Meil\u0103 (2007) satisfy our first two properties (for a similarity measure we may replace \u201dmust decrease the error\u201d with \u201dmust increase the similarity\u201d).", "startOffset": 58, "endOffset": 71}, {"referenceID": 20, "context": "For additional discussion about comparing clusterings see Meil\u0103 (2007). Note that several criteria discussed in Meil\u0103 (2007) satisfy our first two properties (for a similarity measure we may replace \u201dmust decrease the error\u201d with \u201dmust increase the similarity\u201d).", "startOffset": 58, "endOffset": 125}, {"referenceID": 20, "context": "For additional discussion about comparing clusterings see Meil\u0103 (2007). Note that several criteria discussed in Meil\u0103 (2007) satisfy our first two properties (for a similarity measure we may replace \u201dmust decrease the error\u201d with \u201dmust increase the similarity\u201d). In addition, the Rand and Mirkin criteria discussed in Meil\u0103 (2007) are closely related to the correlation clustering error defined here (all three measures are a function of the number of pairs of points that are clustered incorrectly).", "startOffset": 58, "endOffset": 331}, {"referenceID": 6, "context": "Proof The proof of this statement can be found in Balcan et al. (2008). The intuition is that if there is a node in T that is not laminar w.", "startOffset": 50, "endOffset": 71}, {"referenceID": 6, "context": "In particular, if the data satisfies the strict separation property from Balcan et al. (2008), we may change the split and merge algorithms to use the local averagelinkage tree, which is constructed from only the points in the edit request.", "startOffset": 73, "endOffset": 94}, {"referenceID": 3, "context": "This assumption is consistent with the observation in Awasthi and Zadeh (2010) that in the unrestricted-merge model with arbitrary request sequences, even very simple cases (ex.", "startOffset": 54, "endOffset": 79}, {"referenceID": 22, "context": "Each post/document is represented by a term frequency - inverse document frequency (tf-idf) vector Salton and Buckley (1988). We use cosine similarity to compare these vectors, which gives a similarity measure between 0 and 1 (inclusive).", "startOffset": 99, "endOffset": 125}, {"referenceID": 14, "context": "This observation was also made in other clustering studies that report that the hierarchical trees constructed from these data have low purity Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005).", "startOffset": 157, "endOffset": 173}, {"referenceID": 14, "context": "This observation was also made in other clustering studies that report that the hierarchical trees constructed from these data have low purity Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005). These observations suggest that these data are quite challenging for clustering algorithms.", "startOffset": 157, "endOffset": 203}, {"referenceID": 5, "context": "We tried this approach, using Algorithm 2 from Balcan and Gupta (2010) to compute the \u201cblobs\u201d.", "startOffset": 47, "endOffset": 71}], "year": 2015, "abstractText": "We study the design of interactive clustering algorithms for data sets satisfying natural stability assumptions. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable features in many applications. We show that in this constrained setting one can still design provably efficient algorithms that produce accurate clusterings. We also show that our algorithms perform well on real-world data.", "creator": "LaTeX with hyperref package"}}}