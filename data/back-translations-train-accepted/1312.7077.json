{"id": "1312.7077", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Dec-2013", "title": "Language Modeling with Power Low Rank Ensembles", "abstract": "We present power low rank ensembles, a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. Our method is a generalization of n-gram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. On English and Russian evaluation sets, we obtain noticeably lower perplexities relative to state-of-the-art modified Kneser-Ney and class-based n-gram models.", "histories": [["v1", "Thu, 26 Dec 2013 09:45:02 GMT  (38kb)", "https://arxiv.org/abs/1312.7077v1", null], ["v2", "Fri, 3 Oct 2014 08:28:03 GMT  (157kb,D)", "http://arxiv.org/abs/1312.7077v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["ankur p parikh", "avneesh saluja", "chris dyer", "eric p xing"], "accepted": true, "id": "1312.7077"}, "pdf": {"name": "1312.7077.pdf", "metadata": {"source": "CRF", "title": "Language Modeling with Power Low Rank Ensembles", "authors": ["Ankur P. Parikh", "Avneesh Saluja", "Eric P. Xing"], "emails": ["apparikh@cs.cmu.edu", "avneesh@cs.cmu.edu", "cdyer@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "The probability that the word sequences were not observed in formation is enormously high. (\"We have a high probability that rare events will occur and legitimate word sequences will occur that were not observed.\") (\"We have massively overestimated the probability that rare events will occur.\" (\"We have the probability that rare events will occur and legitimate word sequences will occur.\") (\"We have the probability that rare events will occur and rare events will occur.\") (\"We have the probability that rare events will occur.\") (\"We have the probability that rare events will occur.\") (\"We have the probability that rare events will occur.\") (\"We have the probability that rare events will occur.\") (\"We have the probability that rare events will occur.\" (\"We have the probability that rare events will occur.\") (\"We have the probability that rare events will occur.\") (\"We have the probability that rare events will occur.\""}, {"heading": "2 Discount-based Smoothing", "text": "First, we provide background information on absolute discounting (Ney et al., 1994) and Kneser-Ney smoothing (Kneser and Ney, 1995), two common n-gram smoothing methods. Both methods can be formulated as backoff or interpolated models; the latter is described here as this is the basis of our low-level approach."}, {"heading": "2.1 Notation", "text": "Let c (w) be the number of words w, and similarly c (w, wi \u2212 1) for the common number of words w and wi \u2212 1. For the abbreviation, we will define w j i to denote the word order {wi, wi + 1,..., wj \u2212 1, wj}. Let P (wi) refer to the maximum probability estimate (MLE) of the probability of the word wi, and similarly P (wi | wi \u2212 1) to the probability based on a story, or more generally, P (wi \u2212 1i \u2212 n + 1). Let N \u2212 (wi): = | {w: c (wi, w) > 0} | be the number of unique words that occur before wi. More generally, let N \u2212 (wii \u2212 n + 1) = | {w: c (wii \u2212 n \u2212 n + 1, w) > 0} |."}, {"heading": "2.2 Absolute Discounting", "text": "Absolute discounting is based on the idea of interpolating higher-order n-gram models with lower-order n-gram models. Specifically, a certain probability mass must first be \"subtracted\" from the higher-order n-gram models so that the remaining probability can be mapped to lower-order n-gram models. Then, define the following discounted conditional probability: P-D (wi | wi \u2212 1i \u2212 n + 1) = max {c (wi, wi \u2212 1i \u2212 n + 1) \u2212 D, 0} c (wi \u2212 1i \u2212 n + 1) Then, Pabs absolute discounting (\u00b7) uses the following (recursive) equation: Pabs (wi | wi \u2212 1i \u2212 n \u2212 n + 1) = P-D (wi \u2212 w i \u2212 1 i \u2212 n + 1) + \u03b3 (wi \u2212 1i \u2212 n \u2212 1) + \u03b3 (wi \u2212 1i \u2212 n \u2212 n \u2212 n)."}, {"heading": "2.3 Kneser Ney Smoothing", "text": "Ideally, the smoothed probability should maintain the observed universal probability: P: (wi) = (wi) = (wi) = (wi) wi \u2212 (1i \u2212 n) P: (w i \u2212 1 i \u2212 n + 1) (1) where Psm (wi | wi \u2212 1i \u2212 n + 1) is the smoothed conditional probability that a model will yield results. Unfortunately, absolute discounting does not fulfill this property, as it uses only the unchanged MLE universal model as a lower order model. In practice, the lower word distribution is used only when we are not sure whether the higher job distribution (i.e., if (\u00b7) is large. Therefore, the universal model should be changed in relation to this fact. This is the inspiration behind Kneser-Ney (KN), smoothing, an elegant algorithm with robust performance in language modelling that is n \u2212 g."}, {"heading": "3 Power Low Rank Ensembles", "text": "In n-gram smoothing methods, when a bigram count c (wi, wi \u2212 1) is zero, the unigram probabilities are used, which amounts to the assumption that wi and wi \u2212 1 are independent (and similar for general n). Unfortunately, in this situation, instead of relying on a 1-gram scheme, we can resort to a \"1.5-gram\" or, more generally, an order between 1 and 2 that covers a coarser level of dependence between wi and wi \u2212 1 and does not assume full dependence. Inspired by this intuition, our strategy is to construct an ensemble of matrices and tensors that not only consist of MLE-based counter information, but also contain quantities representing the level of dependence between the different orders in the model. We call these combinations low-ranked ensembles (PLRE), and they can be considered n-gram models with non-integer."}, {"heading": "3.1 Rank", "text": "We first show how the number of amounts paid is between an n-gram and an n-gram value compared to another (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) value (n-m) -value (n-m) -value (n-m) -value (n-m) -value (n-m) -value (n-m) -value (n-m) value (n-m) -value (n-m) -value (n-m) -value (n-m) -value (n-m) -value (n) (n-m) -value (n) (n-n) value (n (n-n) (n-n) (n-n) value (n-m) -m (n-m) -value (n-m) -value (n-m) -value (n-m) (n-m) -value (n-m) -value (n-m) -value (n-m) -value (n-m (n-m) -value (n-m) (n-m) value (n-m (n-m) (n-m) (n-m (n-m) -value (n-m) (n-m (n-m) (n-m) value (n-m (n-m) (n-m (n-m) (n-m) (n-m) value (n-m (n-m) (n-m (n-m) (n-m) (n-m) -value (n-m) value (n-m (n-m) (n-m) -value (n-m) (n-m (n-m) (n-m) (n-m (n-m) (n-m) value (n-m) (n-m) (n-m (n-m) (n-m) (n-m) (n-m"}, {"heading": "3.2 Power", "text": "Since the CN smoothing alters the distributions of the lower order, rather than simply using the MLE > wi sum, it is not sufficient to vary the rank to generalize this series of techniques. Thus, PLRE \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 n \u2212 calculates approximate values of the changed number matrices. Let us consider the elementary power of the matrix of the bigram number, which is denoted by B \u00b7 \u03c1. (For example: the observed bigram number matrix and the associated row sum: B \u00b7 1 = (1.0 2.0 1.0 0 0 0 0 0 0 0 0 0) row sum \u2192 (3.4 2.2 1.4) Let us consider how the row sum of the number series (4.0 5.0 2.0) is as expected according to the number sequence of the unigram number (which we call u). Let us now consider B \u00b7 0.5: B \u00b7 0.5 = (1.0 1.0 0 0 0 2.2 0) row sum of the row sum \u2192 i (3.4 2.2 1.4)."}, {"heading": "4 Creating the Ensemble", "text": "Remember our general formulation in Eq. 3 \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi = wi \u2212 wi (wi = wi = wi (wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi = wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi (wi = wi = wi = wi = wi = wi = wi = wi = wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi \u2212 wi"}, {"heading": "4.1 Step 1: Computing the Discounts", "text": "To ensure the constraint that Ppwr (wi \u2212 1) = Ppwr (wi \u2212 1) = Ppwr (wi \u2212 1) = Ppwr (wi \u2212 1) = Ppwr (wi \u2212 1) = Ppwr (wi \u2212 1) = Ppwr (wi \u2212 1) = Ppsp (wi \u2212 1) = Psp (wi \u2212 1) + Psp (wi \u2212 1) = Psp (wi \u2212 1) + Pkt. (wi \u2212 1) + Pkt. (wi \u2212 1) + Pkt. (wi \u2212 1) = Pkt. (wi \u2212 1) + Pkt. (wi \u2212 1) + Pkt. (wi \u2212 1) + Pkt. (wi \u2212 1) + 1 kt. \u2212 Pwi. + 1 kt. \u2212 Pkt. (wi \u2212 1) = Pt. (wi \u2212 1) = Pt. (wi \u2212 1) = Pt. (wi \u2212 1) = Pt."}, {"heading": "4.2 Step 2: Computing Low Rank Quantities", "text": "The next step is to calculate low marginal approximations of Y (2) Dj in order to obtain ZDj in such a way that the mean marginal constraint in Eq.7. This constraint applies trivially to the mean total ensemble Ppwr (wi | wi \u2212 1) due to the way in which the discounts in \u00a7 4.1. For our ongoing bigram example, define Z (wi \u2212 1) Dj as the best marginal approximation to Y (wi \u2212 1) Dj according to gKL and letZ \u03c1j, \u03baj Dj (wi | 1) = Z \u03c1j, \u03baj Dj (wi \u2212 1), wi \u2212 1), \u0432j (wi \u2212 1) \u0432jsum \u0421yyj (wi \u2212 1) Dj (wi \u2212 1) is a valid (discounted) conditional probability, since gKL \u2212 the row / slit sum remains unchanged, so that the denominator remains below the lower margKL."}, {"heading": "4.3 More general algorithm", "text": "In general, the principles outlined in the previous sections apply to n-grams of higher order. Suppose that the discounts according to algorithm 1 are calculated using the parameters d * and Z (\u03c1j, \u03baj) djis calculated according to algorithm 2. Note that, as shown in algorithm 2, for n-grams of higher order, the Z (\u03c1j, \u03baj) dj are created by approximating the proportions of the (driven) counters of lower order (see Lemma 2 for intuition). Equation 3 can now be glossed over: Plot (wi | wi \u2212 1i \u2212 n + 1) = P alt D0 (wi | w i \u2212 1 i \u2212 n + 1) + \u03b30 (w i \u2212 1 i \u2212 n + 1) + 1 (z \u2212 1 \u2212 n + 1) (z \u2212 1 \u2212 1 \u2212 1 \u2212 n + 1) (z \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 \u2212 p \u2212 n)."}, {"heading": "4.4 Links with KN Smoothing", "text": "In this section, we explicitly show the relationship between PLRE and CN, which is smoothing out. Furthermore, Equation 12 is rewritten as follows: Pplre (wi | wi \u2212 1i \u2212 n + 1) = P-Terme plre (wi | wi \u2212 1i \u2212 n + 1) + \u03b30: \u03b7 (w i \u2212 1 i \u2212 n + 1) Pplre (wi | w i \u2212 1 i \u2212 n + 2) (13), where P termsplre (wi | w i \u2212 1 i \u2212 n + 1) contains the terms in Equation 12, except the last one, and \u03b30: \u03b7 (wi \u2212 1i \u2212 n + 1) = \u0432 h = 0 \u03b3h (w i \u2212 1 i \u2212 n + 1), we can use the form of the discount and take advantage of the fact that equality is + 1 = 2.0: \u03b7 (w i \u2212 1 \u2212 n \u2212 1) -D density, which has been replaced by equality (w i \u2212 1 \u2212 n \u2212 n) -D structure."}, {"heading": "4.5 Computational Considerations", "text": "PLRE scales well, even if the order n increases. To calculate a low-rank bigram, a low-rank approach to a V \u00b7 V matrix is required. To calculate a low-rank trigram, we must calculate a low-rank approach to each disc Cn, (\u00b7 p) D (:, w \u0435i \u2212 1,:), and a low-rank approach. Although this may seem scary at first glance, in practice the size of each disc (number of non-zero rows / columns) is usually much, much smaller than V, while the calculation remains traceable. Similarly, PLRE also evaluates conditional probabilities efficiently in the evaluation time. As shown in Algorithm 2, the normalizer can be calculated on the sparsely driven matrix / tensor. As a result, our test complexity O (\u0432\u043e\u043b\u0435i = 1 pi), where the total number of matrices within the ensemble is."}, {"heading": "5 Experiments", "text": "To evaluate PLRE, we compared its performance at English and Russian corporations with several vari-2for derivation see proof of Lemma 4 in the supplementary materialants of KN smoothing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007). We evaluated with helplessness in most of our experiments, but also provided results evaluated with BLEU (Papineni et al., 2002) at a downstream machine translation task (MT). We made the code for our approach publicly available 3. To build the hard class-based LMs, we used mkcls4, a word class formation tool that uses the maximum probability criterion (Och., 1995) for classification, and then trained trim class language models in these classes (according to 2nd order HMMs) using SRILM (Stolcke, 2002), with N-smoothing the abilities."}, {"heading": "5.1 Datasets", "text": "For the perplexity experiments, we evaluated our proposed approach using 4 sets of data, 2 in English and 2 in Russian. In all cases, the singletons were replaced by \"\" tokens in the training corpus, and any word not in the vocabulary was replaced by this token during evaluation. \u2022 Small-English: APNews corpus (Bengio et al., 2003): Train - 14 million words, Dev - 963,000, Test - 963,000. Vocabulary - 18,000 types. \u2022 Small-Russian: Subset of Russian News com-mentary data from 2013 WMT translation task5: Training - 3.5 million words, Dev - 400,000, Test - 96,000. Vocabulary - 18,000 types. \u2022 Small-Russian: Subset of Russian News com-mentary data from 2013 WMT Translation task5: Training - 3.5 million words, Dev - 400,000."}, {"heading": "5.2 Small Corpora", "text": "For class-based LMs, the number of classes was chosen from {32, 64, 128, 256, 512, 1024) and {512, 1024 (Small-Russian). We could not go higher because the process of persistent linking (int-MKN) was changed."}, {"heading": "5.3 Large Corpora", "text": "The results of the larger corpora for the two best execution methods \"PLRE\" and \"int-MKN\" are shown in Table 2. Due to the larger training size, we use 4-gram models in these experiments. However, the 4-gram tensor, including the low one, has brought little profit and therefore the 4-gram tensor has only additional low-rank bigrams and low-rank trigram matrices / tensors. As described above, the ranks were matched to the development set. For SmallRussian, the ranks {1e \u2212 4, 5e \u2212 4, 1e \u2212 3} (a fraction of the vocabulary size) were for both the low-rank Kbigramm and low-rank trigram models. For SmallRussian, the ranks were {1e \u2212 5, 5e \u2212 5, 1e \u2212 4} for both the low-rank bigrams and the low-rank trigram models."}, {"heading": "6 Machine Translation Task", "text": "We used MIRA (Chiang et al., 2008) to learn the characteristic weights. In order to control randomness in MIRA, we avoid a re-adjustment when changing LMs - the number of characteristic weights determined by int-MKN is the same, only the language model changes. The6As already described, only the ranks need to be adjusted so that only 2-3 low-rank bigrams and 2-3 low-rank trigrams need to be calculated (and combined depending on the setting). In contrast to other recent approaches, where an additional characteristic weight is matched to the proposed model and used in conjunction with the KN smoothing (Vaswani et al., 2013), our goal is to demonstrate the improvements PLRE offers as a substitute for KN, so that the BLRE values are never inferior to the EU values on average (Vaswani et al., 2013)."}, {"heading": "7 Related Work", "text": "Recent attempts to rethink the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the link between interpolated Kneser Ney and the hierarchical Pitman-Yor process, which has led to generalizations that take into account domain effects (Wood and Teh, 2009) and unlimited contexts (Wood et al., 2009).The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive feats.These methods can be quite expensive to train and question word modeling (especially as word size increases).Techniques such as noise-contrast estimation (Gutmann and Hyva \ufffd rinen, 2012; Mnih and Tesubh, 2012; Vampulal, 2013)."}, {"heading": "8 Conclusion", "text": "By using ensembles of sparse and low matrices and tensors, our method captures both fine-grained and coarse structures in word sequences. Our discounting strategy preserves the boundary constraint and generalizes Kneser Ney, and can also extend other smoothing methods, such as deleted interpolation / JelinekMercer smoothing, if slightly modified. Experimentally, PLRE performs convincingly better than Kneser-Ney smoothing and class-based baselines."}, {"heading": "Acknowledgements", "text": "This work was supported by NSF IIS1218282, NSF IIS1218749, NSF IIS1111142, NIH R01GM093156, the US Army Research Laboratory and the US Army Research Office under contract / grant number W911NF-10-1-0533, the NSF Graduate Research Fellowship Program under grant number 0946825 (NSF Fellowship to APP) and a grant from Ebay Inc. (to AS)."}], "references": [{"title": "Large vocabulary speech recognition with multispan statistical language models", "author": ["Jerome R. Bellegarda."], "venue": "IEEE Transactions on Speech and Audio Processing, 8(1):76\u201384.", "citeRegEx": "Bellegarda.,? 2000", "shortCiteRegEx": "Bellegarda.", "year": 2000}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res., 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Findings of the 2013 Workshop on Statistical Machine Translation", "author": ["Ond\u0159ej Bojar", "Christian Buck", "Chris Callison-Burch", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Christof Monz", "Matt Post", "Radu Soricut", "Lucia Specia."], "venue": "Pro-", "citeRegEx": "Bojar et al\\.,? 2013", "shortCiteRegEx": "Bojar et al\\.", "year": 2013}, {"title": "A singular value thresholding algorithm", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht."], "venue": "Foundations of Computational mathematics, 9(6):717\u2013 772.", "citeRegEx": "Cand\u00e8s and Recht.,? 2009", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman."], "venue": "Computer Speech & Language, 13(4):359\u2013393.", "citeRegEx": "Chen and Goodman.,? 1999", "shortCiteRegEx": "Chen and Goodman.", "year": 1999}, {"title": "A survey of smoothing techniques for me models", "author": ["Stanley F Chen", "Ronald Rosenfeld."], "venue": "Speech and Audio Processing, IEEE Transactions on, 8(1):37\u2013", "citeRegEx": "Chen and Rosenfeld.,? 2000", "shortCiteRegEx": "Chen and Rosenfeld.", "year": 2000}, {"title": "Shrinking exponential language models", "author": ["Stanley F. Chen."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL \u201909, pages", "citeRegEx": "Chen.,? 2009", "shortCiteRegEx": "Chen.", "year": 2009}, {"title": "Online large-margin training of syntactic and structural translation features", "author": ["David Chiang", "Yuval Marton", "Philip Resnik."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 224\u2013233. Association for Com-", "citeRegEx": "Chiang et al\\.,? 2008", "shortCiteRegEx": "Chiang et al\\.", "year": 2008}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Comput. Linguist., 33(2):201\u2013228, June.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Jonathan Weese", "Hendra Setiawan", "Adam Lopez", "Ferhan Ture", "Vladimir Eidelman", "Juri Ganitkevitch", "Phil Blunsom", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Interpolating between types and tokens by estimating power-law generators", "author": ["Sharon Goldwater", "Thomas Griffiths", "Mark Johnson."], "venue": "Advances in Neural Information Processing Systems, volume 18.", "citeRegEx": "Goldwater et al\\.,? 2006", "shortCiteRegEx": "Goldwater et al\\.", "year": 2006}, {"title": "Classes for fast maximum entropy training", "author": ["Joshua Goodman."], "venue": "Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP\u201901). 2001 IEEE International Conference on, volume 1, pages 561\u2013564. IEEE.", "citeRegEx": "Goodman.,? 2001", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "Journal of Machine Learning Research, 13:307\u2013 361.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2012", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2012}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187\u2013197, Edinburgh, Scotland, United Kingdom, July.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Nonnegative matrix factorization with fixed row and column sums", "author": ["Ngoc-Diep Ho", "Paul Van Dooren."], "venue": "Linear Algebra and its Applications, 429(5):1020\u20131025.", "citeRegEx": "Ho and Dooren.,? 2008", "shortCiteRegEx": "Ho and Dooren.", "year": 2008}, {"title": "Low rank language models for small training sets", "author": ["Brian Hutchinson", "Mari Ostendorf", "Maryam Fazel."], "venue": "Signal Processing Letters, IEEE, 18(9):489\u2013 492.", "citeRegEx": "Hutchinson et al\\.,? 2011", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2011}, {"title": "Interpolated estimation of markov source parameters from sparse data", "author": ["Frederick Jelinek", "Robert Mercer."], "venue": "Pattern recognition in practice.", "citeRegEx": "Jelinek and Mercer.,? 1980", "shortCiteRegEx": "Jelinek and Mercer.", "year": 1980}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney."], "venue": "Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pages 181\u2013184. IEEE.", "citeRegEx": "Kneser and Ney.,? 1995", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press, New York, NY, USA, 1st edition.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky."], "venue": "Computer, 42(8):30\u201337.", "citeRegEx": "Koren et al\\.,? 2009", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Daniel D. Lee", "H. Sebastian Seung."], "venue": "Advances in Neural Information Processing Systems, 13:556\u2013562.", "citeRegEx": "Lee and Seung.,? 2001", "shortCiteRegEx": "Lee and Seung.", "year": 2001}, {"title": "Divide-and-conquer matrix factorization", "author": ["Lester Mackey", "Ameet Talwalkar", "Michael I Jordan."], "venue": "arXiv preprint arXiv:1107.0789.", "citeRegEx": "Mackey et al\\.,? 2011", "shortCiteRegEx": "Mackey et al\\.", "year": 2011}, {"title": "Foundations of statistical natural language processing, volume 999", "author": ["Christopher D Manning", "Hinrich Sch\u00fctze."], "venue": "MIT Press.", "citeRegEx": "Manning and Sch\u00fctze.,? 1999", "shortCiteRegEx": "Manning and Sch\u00fctze.", "year": 1999}, {"title": "Natural language processing with modular pdp networks and distributed lexicon", "author": ["Risto Miikkulainen", "Michael G. Dyer."], "venue": "Cognitive Science, 15:343\u2013 399.", "citeRegEx": "Miikkulainen and Dyer.,? 1991", "shortCiteRegEx": "Miikkulainen and Dyer.", "year": 1991}, {"title": "Recurrent neural network based language model", "author": ["Tom Mikolov", "Martin Karafit", "Luk Burget", "Jan ernock", "Sanjeev Khudanpur."], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTER-", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proceedings of the 24th international conference on Machine learning, pages 641\u2013648. ACM.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Structured penalties for log-linear language models", "author": ["Anil Kumar Nelakanti", "Cedric Archambeau", "Julien Mairal", "Francis Bach", "Guillaume Bouchard."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Nelakanti et al\\.,? 2013", "shortCiteRegEx": "Nelakanti et al\\.", "year": 2013}, {"title": "On Structuring Probabilistic Dependencies in Stochastic Language Modelling", "author": ["Hermann Ney", "Ute Essen", "Reinhard Kneser."], "venue": "Computer Speech and Language, 8:1\u201338.", "citeRegEx": "Ney et al\\.,? 1994", "shortCiteRegEx": "Ney et al\\.", "year": 1994}, {"title": "Maximum-likelihoodsch\u00e4tzung von wortkategorien mit verfahren der kombinatorischen optimierung", "author": ["Franz Josef Och."], "venue": "Bachelor\u2019s thesis (Studienarbeit), University of Erlangen.", "citeRegEx": "Och.,? 1995", "shortCiteRegEx": "Och.", "year": 1995}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei jing Zhu."], "venue": "pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Fundamentals of speech recognition", "author": ["Lawrence Rabiner", "Biing-Hwang Juang"], "venue": null, "citeRegEx": "Rabiner and Juang.,? \\Q1993\\E", "shortCiteRegEx": "Rabiner and Juang.", "year": 1993}, {"title": "Smoothed marginal distribution constraints for language modeling", "author": ["Brian Roark", "Cyril Allauzen", "Michael Riley."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 43\u201352.", "citeRegEx": "Roark et al\\.,? 2013", "shortCiteRegEx": "Roark et al\\.", "year": 2013}, {"title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo", "author": ["Ruslan Salakhutdinov", "Andriy Mnih."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 880\u2013887. ACM.", "citeRegEx": "Salakhutdinov and Mnih.,? 2008", "shortCiteRegEx": "Salakhutdinov and Mnih.", "year": 2008}, {"title": "Aggregate and mixed-order markov models for statistical language processing", "author": ["Lawrence Saul", "Fernando Pereira."], "venue": "Proceedings of the second conference on empirical methods in natural language processing, pages 81\u201389. Somerset, New Jer-", "citeRegEx": "Saul and Pereira.,? 1997", "shortCiteRegEx": "Saul and Pereira.", "year": 1997}, {"title": "SRILM - An Extensible Language Modeling Toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of the International Conference in Spoken Language Processing.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "A survey of collaborative filtering techniques", "author": ["Xiaoyuan Su", "Taghi M Khoshgoftaar."], "venue": "Advances in artificial intelligence, 2009:4.", "citeRegEx": "Su and Khoshgoftaar.,? 2009", "shortCiteRegEx": "Su and Khoshgoftaar.", "year": 2009}, {"title": "A hierarchical bayesian language model based on pitman-yor processes", "author": ["Yee Whye Teh."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Lin-", "citeRegEx": "Teh.,? 2006", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "Decoding with largescale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation", "author": ["F. Wood", "Y.W. Teh."], "venue": "Artificial Intelligence and Statistics, pages 607\u2013614.", "citeRegEx": "Wood and Teh.,? 2009", "shortCiteRegEx": "Wood and Teh.", "year": 2009}, {"title": "A stochastic memoizer for sequence data", "author": ["Frank Wood", "C\u00e9dric Archambeau", "Jan Gasthaus", "Lancelot James", "Yee Whye Teh."], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 1129\u20131136. ACM.", "citeRegEx": "Wood et al\\.,? 2009", "shortCiteRegEx": "Wood et al\\.", "year": 2009}, {"title": "Efficient training methods for maximum entropy language modeling", "author": ["Jun Wu", "Sanjeev Khudanpur."], "venue": "Interspeech, pages 114\u2013118.", "citeRegEx": "Wu and Khudanpur.,? 2000", "shortCiteRegEx": "Wu and Khudanpur.", "year": 2000}, {"title": "Efficient subsampling for training complex language models", "author": ["Puyang Xu", "Asela Gunawardana", "Sanjeev Khudanpur."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911, pages 1128\u20131136,", "citeRegEx": "Xu et al\\.,? 2011", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "Human behaviour and the principle of least-effort", "author": ["George Zipf."], "venue": "Addison-Wesley, Cambridge, MA.", "citeRegEx": "Zipf.,? 1949", "shortCiteRegEx": "Zipf.", "year": 1949}], "referenceMentions": [{"referenceID": 34, "context": "Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010).", "startOffset": 185, "endOffset": 210}, {"referenceID": 20, "context": "Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010).", "startOffset": 235, "endOffset": 248}, {"referenceID": 46, "context": "Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have been observed in the training data (Manning and Sch\u00fctze, 1999).", "startOffset": 40, "endOffset": 52}, {"referenceID": 24, "context": "Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have been observed in the training data (Manning and Sch\u00fctze, 1999).", "startOffset": 259, "endOffset": 286}, {"referenceID": 5, "context": "These reassign probability mass (generally from overestimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models (Chen and Goodman, 1999).", "startOffset": 196, "endOffset": 220}, {"referenceID": 21, "context": "Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand\u00e8s and Recht, 2009; Cai et al.", "startOffset": 179, "endOffset": 226}, {"referenceID": 39, "context": "Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand\u00e8s and Recht, 2009; Cai et al.", "startOffset": 179, "endOffset": 226}, {"referenceID": 4, "context": ", 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand\u00e8s and Recht, 2009; Cai et al., 2010).", "startOffset": 56, "endOffset": 98}, {"referenceID": 3, "context": ", 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand\u00e8s and Recht, 2009; Cai et al., 2010).", "startOffset": 56, "endOffset": 98}, {"referenceID": 22, "context": "In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011).", "startOffset": 86, "endOffset": 158}, {"referenceID": 36, "context": "In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011).", "startOffset": 86, "endOffset": 158}, {"referenceID": 23, "context": "In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011).", "startOffset": 86, "endOffset": 158}, {"referenceID": 1, "context": "This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance.", "startOffset": 117, "endOffset": 183}, {"referenceID": 28, "context": "This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance.", "startOffset": 117, "endOffset": 183}, {"referenceID": 26, "context": "This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance.", "startOffset": 117, "endOffset": 183}, {"referenceID": 37, "context": ", 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance.", "startOffset": 29, "endOffset": 53}, {"referenceID": 6, "context": "While this is larger than Kneser Ney\u2019s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant.", "startOffset": 140, "endOffset": 202}, {"referenceID": 7, "context": "While this is larger than Kneser Ney\u2019s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant.", "startOffset": 140, "endOffset": 202}, {"referenceID": 30, "context": "While this is larger than Kneser Ney\u2019s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant.", "startOffset": 140, "endOffset": 202}, {"referenceID": 28, "context": "We also include a comparison to the log-bilinear neural language model (Mnih and Hinton, 2007) and evaluate performance on a downstream machine translation task (\u00a76) where our method achieves consistent improvements in BLEU.", "startOffset": 71, "endOffset": 94}, {"referenceID": 31, "context": "We first provide background on absolute discounting (Ney et al., 1994) and Kneser-Ney smoothing (Kneser and Ney, 1995), two common n-gram smoothing methods.", "startOffset": 52, "endOffset": 70}, {"referenceID": 19, "context": ", 1994) and Kneser-Ney smoothing (Kneser and Ney, 1995), two common n-gram smoothing methods.", "startOffset": 33, "endOffset": 55}, {"referenceID": 19, "context": "1 (Kneser and Ney, 1995).", "startOffset": 2, "endOffset": 24}, {"referenceID": 22, "context": "While this statement is not true under the `2 norm, it is true under generalized KL divergence (Lee and Seung, 2001): gKL(A||B) = \u2211", "startOffset": 95, "endOffset": 116}, {"referenceID": 18, "context": "deleted-interpolation/Jelinek Mercer smoothing (Jelinek and Mercer, 1980).", "startOffset": 47, "endOffset": 73}, {"referenceID": 28, "context": "ants of KN smoothing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007).", "startOffset": 85, "endOffset": 108}, {"referenceID": 33, "context": "We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU (Papineni et al., 2002) on a downstream machine translation (MT) task.", "startOffset": 102, "endOffset": 125}, {"referenceID": 32, "context": "To build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing.", "startOffset": 127, "endOffset": 138}, {"referenceID": 38, "context": "We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities.", "startOffset": 117, "endOffset": 132}, {"referenceID": 9, "context": "For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al.", "startOffset": 66, "endOffset": 80}, {"referenceID": 11, "context": "For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010).", "startOffset": 99, "endOffset": 118}, {"referenceID": 15, "context": "The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011).", "startOffset": 71, "endOffset": 87}, {"referenceID": 1, "context": "\u2022 Small-English: APNews corpus (Bengio et al., 2003): Train - 14 million words, Dev - 963,000, Test - 963,000.", "startOffset": 31, "endOffset": 52}, {"referenceID": 28, "context": "Comparison to LBL (Mnih and Hinton, 2007): Mnih and Hinton (2007) evaluate on the Small-English dataset (but remove end markers and concatenate the sentences).", "startOffset": 18, "endOffset": 41}, {"referenceID": 28, "context": "Comparison to LBL (Mnih and Hinton, 2007): Mnih and Hinton (2007) evaluate on the Small-English dataset (but remove end markers and concatenate the sentences).", "startOffset": 19, "endOffset": 66}, {"referenceID": 8, "context": "We used MIRA (Chiang et al., 2008) to learn the feature weights.", "startOffset": 13, "endOffset": 34}, {"referenceID": 2, "context": "9 (Bojar et al., 2013)", "startOffset": 2, "endOffset": 22}, {"referenceID": 10, "context": "procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011).", "startOffset": 68, "endOffset": 88}, {"referenceID": 41, "context": "Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN.", "startOffset": 140, "endOffset": 162}, {"referenceID": 42, "context": "These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 43, "context": "These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009).", "startOffset": 110, "endOffset": 129}, {"referenceID": 39, "context": "Teh (2006) and Goldwater et al.", "startOffset": 0, "endOffset": 11}, {"referenceID": 12, "context": "Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process.", "startOffset": 15, "endOffset": 39}, {"referenceID": 25, "context": "The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al.", "startOffset": 67, "endOffset": 96}, {"referenceID": 28, "context": "The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance.", "startOffset": 117, "endOffset": 162}, {"referenceID": 26, "context": "The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance.", "startOffset": 117, "endOffset": 162}, {"referenceID": 14, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al.", "startOffset": 48, "endOffset": 119}, {"referenceID": 29, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al.", "startOffset": 48, "endOffset": 119}, {"referenceID": 41, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al.", "startOffset": 48, "endOffset": 119}, {"referenceID": 45, "context": ", 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types.", "startOffset": 21, "endOffset": 38}, {"referenceID": 44, "context": ", 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types.", "startOffset": 114, "endOffset": 138}, {"referenceID": 13, "context": "An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost of exact normalization to O( \u221a V ).", "startOffset": 47, "endOffset": 84}, {"referenceID": 27, "context": "An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost of exact normalization to O( \u221a V ).", "startOffset": 47, "endOffset": 84}, {"referenceID": 13, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al.", "startOffset": 49, "endOffset": 539}, {"referenceID": 13, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation.", "startOffset": 49, "endOffset": 565}, {"referenceID": 37, "context": "There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.", "startOffset": 36, "endOffset": 103}, {"referenceID": 0, "context": "There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.", "startOffset": 36, "endOffset": 103}, {"referenceID": 17, "context": "There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.", "startOffset": 36, "endOffset": 103}, {"referenceID": 0, "context": "There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off parameters for heavilypruned language models, whereas we use this concept to estimate n-gram specific discounts.", "startOffset": 61, "endOffset": 306}], "year": 2014, "abstractText": "We present power low rank ensembles (PLRE), a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. Our method can be understood as a generalization of ngram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task.", "creator": "TeX"}}}