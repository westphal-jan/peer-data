{"id": "1312.7302", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Dec-2013", "title": "Learning Human Pose Estimation Features with Convolutional Networks", "abstract": "This paper introduces a new architecture for human pose estimation using a multi- layer convolutional network architecture and a modified learning technique that learns low-level features and higher-level weak spatial models. Unconstrained human pose estimation is one of the hardest problems in computer vision, and our new architecture and learning schema shows significant improvement over the current state-of-the-art results. The main contribution of this paper is showing, for the first time, that a specific variation of deep learning is able to outperform all existing traditional architectures on this task. The paper also discusses several lessons learned while researching alternatives, most notably, that it is possible to learn strong low-level feature detectors on features that might even just cover a few pixels in the image. Higher-level spatial models improve somewhat the overall result, but to a much lesser extent then expected. Many researchers previously argued that the kinematic structure and top-down information is crucial for this domain, but with our purely bottom up, and weak spatial model, we could improve other more complicated architectures that currently produce the best results. This mirrors what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced.", "histories": [["v1", "Fri, 27 Dec 2013 17:41:13 GMT  (3122kb,D)", "https://arxiv.org/abs/1312.7302v1", null], ["v2", "Mon, 30 Dec 2013 04:29:34 GMT  (3097kb,D)", "http://arxiv.org/abs/1312.7302v2", null], ["v3", "Fri, 3 Jan 2014 20:56:34 GMT  (3206kb,D)", "http://arxiv.org/abs/1312.7302v3", null], ["v4", "Tue, 18 Feb 2014 16:22:38 GMT  (3212kb,D)", "http://arxiv.org/abs/1312.7302v4", null], ["v5", "Tue, 25 Feb 2014 05:32:32 GMT  (3212kb,D)", "http://arxiv.org/abs/1312.7302v5", null], ["v6", "Wed, 23 Apr 2014 19:23:46 GMT  (3213kb,D)", "http://arxiv.org/abs/1312.7302v6", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["arjun jain", "jonathan tompson", "mykhaylo", "riluka", "graham w taylor", "christoph bregler"], "accepted": true, "id": "1312.7302"}, "pdf": {"name": "1312.7302.pdf", "metadata": {"source": "CRF", "title": "Learning Human Pose Estimation Features with Convolutional Networks", "authors": ["Arjun Jain", "Jonathan Tompson", "Mykhaylo Andriluka"], "emails": ["ajain@nyu.edu", "tompson@cims.nyu.edu", "andriluk@mpi-inf.mpg.de", "gwtaylor@uoguelph.ca", "chris.bregler@nyu.edu"], "sections": [{"heading": null, "text": "Figure 1: The green cross is the wrist locator of our new technique, the red cross the most advanced CVPR13 MODEC detector [38] in the FLIC database."}, {"heading": "1 Introduction", "text": "One of the most difficult tasks in computer vision is to determine the high degree of freedom configuration of a human body with all its extremities, complex self-closures, self-similar parts and large variations due to clothing, body type, lighting and many other factors. However, the most difficult scenario for this problem is from a monocular RGB image and without any prior assumptions, which are performed with motion models, background models or other common heuristics using current state-of-the-art systems. Finding a face in frontal or lateral view is relatively simple, but determining thear Xiv: 131 2.73 02v6 [cs.CV] 2 3A prexact location of body parts as hands, elbows, hips, knees and feet, each of which sometimes only occupy some pixels in the image in front of an arbitrary cluttered background, is significant harder.The best performing pose estimation methods, including those based on the deformable models, typical."}, {"heading": "2 Related Work", "text": "In fact, most of them will be able to play by the rules that they need for their work, and they will be able to play by the rules that they need for their work."}, {"heading": "3 Model", "text": "The most obvious approach would be to map the image input directly to a vector that encodes the articulated pose: i.e., the type of labels found in the Pose datasets. The vvnet output would represent the unlimited 2-D or 3-D position of joints, or alternatively a hierarchy of joint angles. However, we found that this worked very poorly. Another problem is that while pooling is useful for improving translation invariance during object recognition, it destroys precise spatial information needed for accurate prediction. Conventional networks that generate segmentation maps completely avoid the consistency of the body [47, 13]. Another problem is that the direct mapping from the entrance space to the kinematic body represents coefficients that are highly nonlinear and not one-to-one."}, {"heading": "3.1 Convolutional Network Architecture", "text": "The lowest level of our two-stage feature detection pipeline is therefore based on a standard Convexnet architecture, the overview of which is shown in Figure 2. Convnets, like their fully connected, deep neural network counterparts, perform end-to-end feature learning and are trained with the back-propagation algorithm. However, they differ in a number of aspects, especially in terms of local connectivity, weight distribution and local pooling. The first two properties significantly reduce the number of free parameters and reduce the need to learn repeated feature detectors at different locations of the input. The third property makes the learned representation indispensable for small translations of the input. The Convnet pipeline shown in Figure 2 significantly reduces the number of free parameters and reduces the need to learn repeated feature detectors at different locations of the input."}, {"heading": "3.2 Enforcing Global Pose Consistency with a Spatial Model", "text": "We believe that this is due to two factors: 1), 2), 3), 3), 4), 4), 4), 4), 4), 5), 5 (2), 5 (2), 5 (2), 5 (4), 5 (4), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), 6 (5), 6 (5), 6 (5), 7 (5), 7 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5), 8 (5 (5), 8 (5), 8 (5), 8 (5 (5), 8, 8 (5), 8 (5), 8 (5 (5), 8, 8 (5), 8, 8 (5), 8 (5 (5), 8, 8 (5), 8 (5 (5), 8, 8, 8, 8 (8), 8, 8 (8, 8, 8 (8), 8 (8, 8, 8, 8 (8), 8 (8, 8, 8 (8), 8, 8 (8, 8, 8 (8), 8 (8, 8, 8, 8, 8, 8 (8), 8 (8, 8, 8 (8), 8 (8, 8, 8 (8), 8 (8, 8, 8, 8, 8 (5), 8, 8, 8 (5 (8), 8, 8, 8, 8 (5 (8), 8, 8, 8 (5 (5), 8, 8, 8, 8"}, {"heading": "4 Results", "text": "We evaluated our architecture on the FLIC [38] dataset, which consists of 5003 RGB images taken from a selection of Hollywood movies. Each frame in the dataset contains at least one person in a frontal pose (facing the camera), and each frame has been processed by Amazon Mechanical Turk to obtain truth labels for the common positions of a single person's torso. FLIC dataset is very challenging for state-of-the-art pose estimation methods, because poses are often obscured, and clothing and background are not consistent. We use 3987 workout images from the dataset, which we also mirror horizontally, to obtain a total of 3987 \u00d7 2 = 7974 examples. As the workout images are not on the same scale, we will manually address the Bounding Box for the head in these workout set images, bringing them to canonical scale."}, {"heading": "4.1 Evaluation", "text": "To evaluate our model on the FLIC dataset, we use a measure of joint accuracy proposed by Sapp et al. [38]: for a certain precision radius of the joints, we specify the percentage of joints in the test set within the radius threshold (where distance is defined as 2D-euclidean distance in pixels). In Figure 4.1, we evaluate this measure for the wrist, elbow, and shoulder joints. We also compare our detector with the DPM [15] and MODEC [38] architectures. Note that we use the same subset of 351 images for testing all detectors. Figure 4.1 shows that our architecture surpasses or is equal to the MODEC and DPM detectors for all three body parts. For wrist and elbow joints, our simple spatial model improves joint localization for about 5% of test sets (at a threshold of 5 pixels), allowing all detectors to match each other."}, {"heading": "5 Conclusion", "text": "We have successfully demonstrated how to improve the state of the art in one of the most complex computer vision tasks: the unrestricted estimation of human poses. Conventional networks are impressive low-level detectors that, when combined with a previous global position, are capable of surpassing much more complex and popular models. We examined many different higher-level structural models with the aim of further improving the results, but the most generic higher-level spatial model achieved the best results. As mentioned in the introduction, this is counterintuitive for human kinematic structures, but mirrors the results in other areas. For example, in speech recognition, researchers observed that when the learned transition probabilities (higher structure) are reset to equal probabilities, the detection power, which is now mainly driven by the emission probabilities, is not significantly reduced [27]. Other areas are discussed in more detail by [26]. We expect further improvement by expanding the training set to include a new multidimensional distortion, which we are currently examining as well."}, {"heading": "6 Acknowledgements", "text": "This research was partially funded by the Office of Naval Research ONR Award N000141210327 and a Google Award."}], "references": [{"title": "Recovering 3D human pose from monocular images", "author": ["A. Agarwal", "B. Triggs", "I. Rhone-Alpes", "F. Montbonnot"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(1):44\u201358,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Pictorial structures revisited: People detection and articulated pose estimation", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Monocular 3d pose estimation and tracking by detection", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 623\u2013630. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Scape: shape completion and animation of people", "author": ["D. Anguelov", "P. Srinivasan", "D. Koller", "S. Thrun", "J. Rodgers", "J. Davis"], "venue": "ACM Transactions on Graphics (TOG), volume 24, pages 408\u2013416. ACM,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Boostmap: A method for efficient approximate similarity rankings", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "CVPR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A.C. Courville", "P. Vincent"], "venue": "Technical report, University of Montreal,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), June", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Co-registration \u2013 simultaneous alignment and modeling of articulated 3D shapes", "author": ["M. Black", "D. Hirshberg", "M. Loper", "E. Rachlin", "A. Weiss"], "venue": "European patent application EP12187467.1 and US Provisional Application, Oct.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Poselets: Body part detectors trained using 3d human pose annotations", "author": ["L. Bourdev", "J. Malik"], "venue": "ICCV, sep", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning sign language by watching TV (using weakly aligned subtitles)", "author": ["P. Buehler", "A. Zisserman", "M. Everingham"], "venue": "CVPR,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886\u2013893. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Vision-based hand pose estimation: A review", "author": ["A. Erol", "G. Bebis", "M. Nicolescu", "R.D. Boyle", "X. Twombly"], "venue": "Computer Vision and Image Understanding, 108(1):52\u201373,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Scene parsing with multiscale feature learning, purity trees, and optimal covers", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer Learning in Sign language", "author": ["A. Farhadi", "D. Forsyth", "R. White"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P. Felzenszwalb", "D. McAllester", "D. Ramanan"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Pose search: Retrieving people using their pose", "author": ["V. Ferrari", "M. Marin-Jimenez", "A. Zisserman"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume, volume 15, pages 315\u2013323,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Inferring 3d structure with a statistical image-based shape model", "author": ["K. Grauman", "G. Shakhnarovich", "T. Darrell"], "venue": "ICCV, pages 641\u2013648,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "A statistical model of human pose and body shape", "author": ["N. Hasler", "C. Stoll", "M. Sunkel", "B. Rosenhahn", "H.-P. Seidel"], "venue": "P. Dutr\u2019e and M. Stamminger, editors, Computer Graphics Forum (Proc. Eurographics 2008), volume 2, Munich, Germany, Mar.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "In Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1106\u20131114,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, 86(11):2278\u20132324,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150\u20131157. Ieee,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "Are spatial and global constraints really necessary for segmentation? In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 9\u201316", "author": ["A. Lucchi", "Y. Li", "X. Boix", "K. Smith", "P. Fua"], "venue": "IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating human body configurations using shape context matching", "author": ["G. Mori", "J. Malik"], "venue": "ECCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient non-maximum suppression", "author": ["A. Neubeck", "L. Van Gool"], "venue": "Proceedings of the 18th International Conference on Pattern Recognition - Volume 03, ICPR \u201906, pages 850\u2013855, Washington, DC, USA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "A convolutional neural network hand tracker", "author": ["S.J. Nowlan", "J.C. Platt"], "venue": "Advances in Neural Information Processing Systems, pages 901\u2013908,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1995}, {"title": "Synergistic face detection and pose estimation with energybased models", "author": ["M. Osadchy", "Y.L. Cun", "M.L. Miller"], "venue": "The Journal of Machine Learning Research, 8:1197\u20131215,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Why is real-world visual object recognition hard", "author": ["N. Pinto", "D.D. Cox", "J.J. DiCarlo"], "venue": "PLoS computational biology,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Vision-based human motion analysis: An overview", "author": ["R. Poppe"], "venue": "Computer Vision and Image Understanding, 108(1-2):4\u201318,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Strike a pose: Tracking people by finding stylized poses", "author": ["D. Ramanan", "D. Forsyth", "A. Zisserman"], "venue": "CVPR,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive pose priors for pictorial structures", "author": ["B. Sapp", "C. Jordan", "B.Taskar"], "venue": "In CVPR,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "ICCV, pages 750\u2013759,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2003}, {"title": "Realtime human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A. Fitzgibbon", "M. Finocchio", "A. Blake", "M. Cook", "R. Moore"], "venue": "Communications of the ACM, 56(1):116\u2013 124,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human", "author": ["L. Sigal", "A. Balan", "B.M. J"], "venue": "motion. IJCV,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Fast articulated motion tracking using a sums of gaussians body model", "author": ["C. Stoll", "N. Hasler", "J. Gall", "H. Seidel", "C. Theobalt"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 951\u2013958. IEEE,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Pose-sensitive embedding by nonlinear NCA regression", "author": ["G. Taylor", "R. Fergus", "I. Spiro", "G. Williams", "C. Bregler"], "venue": "Advances in Neural Information Processing Systems 23 (NIPS), pages 2280\u20132288,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamical binary latent variable models for 3d human pose tracking", "author": ["G. Taylor", "L. Sigal", "D. Fleet", "G. Hinton"], "venue": "Proc. of the 23rd IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Convolutional networks can learn to generate affinity graphs for image segmentation", "author": ["S.C. Turaga", "J.F. Murray", "V. Jain", "F. Roth", "M. Helmstaedter", "K. Briggman", "W. Denk", "H.S. Seung"], "venue": "Neural Computation, 22:511\u2013538,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Real-time hand-tracking with a color glove", "author": ["R.Y. Wang", "J. Popovi\u0107"], "venue": "ACM Transactions on Graphics (TOG), volume 28, page 63. ACM,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Pfinder: Real-time tracking of the human body", "author": ["C. Wren", "A. Azarbayejani", "T. Darrell", "A. Pentland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):780\u2013785,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1997}, {"title": "Articulated pose estimation with flexible mixtures-of-parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1385\u20131392. IEEE,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1311.2901,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 24, "context": "This echos what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced [26].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "The first stage of processing in a typical pipeline consists of extracting sets of low-level features such as SIFT [25], HoG [11], or other filters that describe orientation statistics in local image patches.", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "The first stage of processing in a typical pipeline consists of extracting sets of low-level features such as SIFT [25], HoG [11], or other filters that describe orientation statistics in local image patches.", "startOffset": 125, "endOffset": 129}, {"referenceID": 5, "context": "For a recent review, see [6].", "startOffset": 25, "endOffset": 28}, {"referenceID": 21, "context": "Several such techniques have used unsupervised or semi-supervised learning to extract multi-layer domain-specific invariant representations, however, it is purely supervised techniques that have won several recent challenges by large margins, including ImageNet LSVRC 2012 and 2013 [23, 51].", "startOffset": 282, "endOffset": 290}, {"referenceID": 44, "context": "Several such techniques have used unsupervised or semi-supervised learning to extract multi-layer domain-specific invariant representations, however, it is purely supervised techniques that have won several recent challenges by large margins, including ImageNet LSVRC 2012 and 2013 [23, 51].", "startOffset": 282, "endOffset": 290}, {"referenceID": 22, "context": "While our approach is based on convolutional networks (convnets) [24], we want to stress that the na\u0131\u0308ve implementation of applying this model \u201coff-the-shelf\u201d will not work.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Examples include [14, 49, 5, 30].", "startOffset": 17, "endOffset": 32}, {"referenceID": 42, "context": "Examples include [14, 49, 5, 30].", "startOffset": 17, "endOffset": 32}, {"referenceID": 4, "context": "Examples include [14, 49, 5, 30].", "startOffset": 17, "endOffset": 32}, {"referenceID": 27, "context": "Examples include [14, 49, 5, 30].", "startOffset": 17, "endOffset": 32}, {"referenceID": 30, "context": "We refer to [35] for a complete survey of this era.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "Examples include \u201cshape-context\u201d edge-based histograms from the human body [28, 1] or just silhouette features [19].", "startOffset": 75, "endOffset": 82}, {"referenceID": 0, "context": "Examples include \u201cshape-context\u201d edge-based histograms from the human body [28, 1] or just silhouette features [19].", "startOffset": 75, "endOffset": 82}, {"referenceID": 17, "context": "Examples include \u201cshape-context\u201d edge-based histograms from the human body [28, 1] or just silhouette features [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 33, "context": "[39] learn a parameter sensitive hash function to perform example-based pose estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "Many relevant techniques have also been applied to hand tracking such as [48].", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "A more general survey of the large field of hand tracking can be found in [12].", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Some use a combination of local detectors and structural reasoning (see [36] for coarse tracking and [10] for person-dependent tracking).", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "Some use a combination of local detectors and structural reasoning (see [36] for coarse tracking and [10] for person-dependent tracking).", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 72, "endOffset": 94}, {"referenceID": 2, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 72, "endOffset": 94}, {"referenceID": 15, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 72, "endOffset": 94}, {"referenceID": 32, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 72, "endOffset": 94}, {"referenceID": 8, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 107, "endOffset": 110}, {"referenceID": 43, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 134, "endOffset": 142}, {"referenceID": 35, "context": "Further examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8].", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "Further examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 18, "context": "Further examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8].", "startOffset": 162, "endOffset": 169}, {"referenceID": 7, "context": "Further examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8].", "startOffset": 162, "endOffset": 169}, {"referenceID": 36, "context": "Also many of these techniques work on images from a single camera, but need video sequence input (not single images) to achieve impressive results [42, 52].", "startOffset": 147, "endOffset": 155}, {"referenceID": 34, "context": "\u2019s Kinect based body part detector [40] uses a random forest of decision trees trained on synthetic depth data to create simple body part detectors.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "One of the earliest examples of this type was Nowlan and Platt\u2019s convolutional neural network hand tracker [30], which tracked a single hand.", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "applied a convolutional network to simultaneously detect and estimate the pitch, yaw and roll of a face [31].", "startOffset": 104, "endOffset": 108}, {"referenceID": 37, "context": "[44] trained a convolutional neural network to learn an embedding in which images of people in similar pose lie nearby.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "\u2019s work on tracking people in video [45], augmenting a particle filter with a structured prior over human pose and dynamics based on learning representations.", "startOffset": 36, "endOffset": 40}, {"referenceID": 35, "context": "While they estimated a posterior over the whole body (60 joint angles), their experiments were limited to the HumanEva dataset [41], which was collected in a controlled laboratory setting.", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "To perform pose estimation with a convolutional network architecture [24] (convnet), the most obvious approach would be to map the image input directly to a vector coding the articulated pose: i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 40, "context": "Convnets that produce segmentation maps, for example, avoid pooling completely [47, 13].", "startOffset": 79, "endOffset": 87}, {"referenceID": 12, "context": "Convnets that produce segmentation maps, for example, avoid pooling completely [47, 13].", "startOffset": 79, "endOffset": 87}, {"referenceID": 20, "context": "The convnet pipeline shown in Figure 2 starts with a 64\u00d764 pixel RGB input patch which has been local contrast normalized (LCN) [22] to emphasize geometric discontinuities and improve generalization performance [32].", "startOffset": 128, "endOffset": 132}, {"referenceID": 29, "context": "The convnet pipeline shown in Figure 2 starts with a 64\u00d764 pixel RGB input patch which has been local contrast normalized (LCN) [22] to emphasize geometric discontinuities and improve generalization performance [32].", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "The input is then processed by three convolution and subsampling layers, which use rectified linear units (ReLUs) [18] and max-pooling.", "startOffset": 114, "endOffset": 118}, {"referenceID": 39, "context": "We used Nesterov momentum [43] as well as RMSPROP [46] to accelerate learning and we used L2 regularization and dropout [21] on the input to each of the fully-connected linear stages to reduce over-fitting the restricted-size training set.", "startOffset": 50, "endOffset": 54}, {"referenceID": 19, "context": "We used Nesterov momentum [43] as well as RMSPROP [46] to accelerate learning and we used L2 regularization and dropout [21] on the input to each of the fully-connected linear stages to reduce over-fitting the restricted-size training set.", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "For datasets containing examples with multiple persons (known a priori), we use non-maximal suppression [29] to find multiple local maxima across the filtered response-maps from each scale, and we then take the top n most likely joint candidates from each person in the scene.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "[15], at test time we run our model on images with only one person (351 images", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "For training the convnet we use Theano [7], which provides a Python-based framework for efficient GPU processing and symbolic differentiation of complex compound functions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 14, "context": "We also compare our detector to the DPM [15] and MODEC [38] architectures.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "Other domains are discussed in more detail by [26].", "startOffset": 46, "endOffset": 50}], "year": 2014, "abstractText": "This paper introduces a new architecture for human pose estimation using a multilayer convolutional network architecture and a modified learning technique that learns low-level features and a higher-level weak spatial model. Unconstrained human pose estimation is one of the hardest problems in computer vision, and our new architecture and learning schema shows improvement over the current stateof-the-art. The main contribution of this paper is showing, for the first time, that a specific variation of deep learning is able to meet the performance, and in many cases outperform, existing traditional architectures on this task. The paper also discusses several lessons learned while researching alternatives, most notably, that it is possible to learn strong low-level feature detectors on regions that might only cover a few pixels in the image. Higher-level spatial models improve somewhat the overall result, but to a much lesser extent than expected. Many researchers previously argued that the kinematic structure and top-down information are crucial for this domain, but with our purely bottom-up, and weak spatial model, we improve on other more complicated architectures that currently produce the best results. This echos what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced [26]. Figure 1: The green cross is our new technique\u2019s wrist locator, the red cross is the state-of-the-art CVPR13 MODEC detector [38] on the FLIC database.", "creator": "LaTeX with hyperref package"}}}