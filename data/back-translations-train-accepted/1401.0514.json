{"id": "1401.0514", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jan-2014", "title": "Structured Generative Models of Natural Source Code", "abstract": "We study the problem of building generative models of natural source code (NSC); that is, source code written and understood by humans. Our primary contribution is to describe a family of generative models for NSC that have three key properties: First, they incorporate both sequential and hierarchical structure. Second, we learn a distributed representation of source code elements. Finally, they integrate closely with a compiler, which allows leveraging compiler logic and abstractions when building structure into the model. We also develop an extension that includes more complex structure, refining how the model generates identifier tokens based on what variables are currently in scope. Our models can be learned efficiently, and we show empirically that including appropriate structure greatly improves the models, measured by the probability of generating test programs.", "histories": [["v1", "Thu, 2 Jan 2014 19:35:31 GMT  (415kb,D)", "https://arxiv.org/abs/1401.0514v1", null], ["v2", "Fri, 20 Jun 2014 08:12:20 GMT  (629kb,D)", "http://arxiv.org/abs/1401.0514v2", null]], "reviews": [], "SUBJECTS": "cs.PL cs.LG stat.ML", "authors": ["chris j maddison", "daniel tarlow"], "accepted": true, "id": "1401.0514"}, "pdf": {"name": "1401.0514.pdf", "metadata": {"source": "META", "title": "Structured Generative Models of Natural Source Code", "authors": ["Chris J. Maddison", "Daniel Tarlow"], "emails": ["CMADDIS@CS.TORONTO.EDU", "DTARLOW@MICROSOFT.COM"], "sections": [{"heading": "1. Introduction", "text": "An important goal is to develop tools that make the development of source code easier, faster and less error-prone, and to develop tools that are able to better understand existing source codes. In fact, this problem has been largely researched outside of machine learning. Many problems in this area do not seem to be well suited to current machine learning technologies (Huang et al., 2013). However, source codes are some of the most widely used data with many public online repositories. Moreover, massive open online courses have begun to collect source codes for homework by tens of thousands of students (Huang et al., 2013). At the same time, the software engineering community has recently observed that it is useful to write source code by humans and means to be understood by other people. (Hindle et al., 2012) This natural source code (NSC) has a large degree of statistical regularity that is ripe for study."}, {"heading": "2. Modelling Source Code", "text": "In this section, we will discuss the challenges of building a generative model of code. In this section, we will motivate our selection of representation and model structures and introduce terminology that is used throughout. (The first step in the compilation is to insert the code into a sequence of tokens, (\u03b1t) Tt = 1 = \u03b1. Tokens are strings like \"sum,\" \"int\" or \"int\" that serve as atomic syntactical elements of a programming language. However, the representation of code leads to very inefficient descriptions. In a C # loop, for example, there must be a sequence that contains the tokens for, (an initialization statement, a conditional expression, the token), a representation that is essentially flat, this structure cannot be represented, because it can be nested for loops. Instead, it is more efficient to use the hierarchical structure that comes from the programming language."}, {"heading": "3. Log-bilinear Tree-Traversal Models", "text": "It is a family of probable models that generate ASTs at a depth level (algorithm 1 = 1). Firstly, the stack is initialized and the children are pushed onto the stack (lines 10-11). If a token node is cracked until it is empty (line 14), this procedure has the effect of generating nodes at a depth level. In addition to the tree that is generated in a recursive manner, traverse variables Hi are updated whenever an internal node is cracked (line 9). Thus, they traverse the tree by evolving sequentially with each tree of the final AST. This sequential view allows us to explore the context, such as variable scoping in intermediate stages of the process."}, {"heading": "4. Extending LTTs", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "5. Inference and Learning in LTTs", "text": "In this section, we will briefly consider how to calculate gradients and probabilities in LTTs. Deterministic transversal variables only. If all transverse variables can be calculated deterministically from the current subtree, we will use the compiler to calculate the full AST corresponding to the program \u03b1m. From the AST, we will calculate the only valid setting of the transverse variables. Since both the AST and the transverse variables can be calculated deterministically from the token sequence, all variables in the model can be treated as observed. Since LTTs are oriented models, this means that the total protocol probability is a sum of protocol probabilities for each production, and learning breaks down into independent problems for each production. Thus, we can easily stack all productions into a single training set and follow standard gradient-based procedures to form log bilinear models."}, {"heading": "6. Related Work", "text": "The LTTs described here are closely related to several existing models. Firstly, a Hidden Markov Model (HMM) can be restored by using all child tuples containing a token and a next node, or just one token (which will terminate the sequence), and using a single discrete latent traverse variable. However, if the traverse variable has only one state and the children's distributions all have finite support, then an LTT is equated to probabilistic context-free grammar (PCFG). PCFGs and their variants are components of the state-of-the-art English parser (McClosky et al., 2006), and many variants have been researched: internal node annotation charniak (1997) and latent annotations Matsuzaki et al al al al al al. (2005) Apart from the question of the order of the traverse variables, the all variables are a specific model of the LTTs."}, {"heading": "7. Experimental Analysis", "text": "This year it is more than ever before."}, {"heading": "8. Discussion", "text": "We have developed probabilistic models that capture some of the structure that occurs in NPCs. A key to our approach is to leverage the large amount of work that has gone into building compilers, resulting in models that not only bring great improvements in quantitative metrics beyond baselines, but also produce much more realistic quality.There are also a number of remaining modeling problems that are ripe for further studies. One question is how to encode the notion that the point of the source code is to do something. Relatively, how do we represent and discover high-level structures in the context of trying to perform such tasks? There are also a number of specific sub-problems that are ripe for further studies. Our model of block statements is naive, and we see that it makes a significant contribution to logging probabilities. It would be interesting to apply more complex sequence models to children who use tuples of blocks to type them in our model, if others would find it interesting to use them on our model."}, {"heading": "Acknowledgments", "text": "We thank John Winn, Andy Gordon, Tom Minka and Thore Graepel for helpful discussions and suggestions and Miltos Allamanis and Charles Sutton for hints on related work."}, {"heading": "EM Learning for Latent Traversal Variable LTTs", "text": "We describe learning the LTTs with latently limited data probability following the following evaluations: \"It becomes clear that the sum is always the same.\" We describe learning the LTTs with latently limited data probability. \"It is not that we can calculate the sum using the forward-looking algorithms.\" \"It becomes clear that we can calculate the sum using the forward-looking algorithms.\" \"We follow the standard formula of the EM formulation and the subordinate data probability with the following energy.\" \"It becomes clear that the sum can always be 1.\" \"We will calculate the learning probability using the forward-looking algorithms.\" It becomes clear that we are tracking the learning probability with the backward algorithms and the subordinate data probability with a free evaluation variable of the following way. \""}], "references": [{"title": "Relating probabilistic grammars and automata", "author": ["Abney", "Steven", "McAllester", "David", "Pereira", "Fernando"], "venue": "In ACL,", "citeRegEx": "Abney et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Abney et al\\.", "year": 1999}, {"title": "Mining source code repositories at massive scale using language modeling", "author": ["Allamanis", "Miltiadis", "Sutton", "Charles"], "venue": "In MSR,", "citeRegEx": "Allamanis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2013}, {"title": "Learning natural coding conventions", "author": ["Allamanis", "Miltiadis", "Barr", "Earl T", "Sutton", "Charles"], "venue": "arXiv preprint arXiv:1402.4182,", "citeRegEx": "Allamanis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2014}, {"title": "Learning from examples to improve code completion systems", "author": ["Bruch", "Marcel", "Monperrus", "Martin", "Mezini", "Mira"], "venue": "In ESEC/FSE,", "citeRegEx": "Bruch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bruch et al\\.", "year": 2009}, {"title": "Statistical parsing with a context-free grammar and word statistics", "author": ["Charniak", "Eugene"], "venue": "AAAI/IAAI, 2005:598\u2013603,", "citeRegEx": "Charniak and Eugene.,? \\Q1997\\E", "shortCiteRegEx": "Charniak and Eugene.", "year": 1997}, {"title": "A maximum-entropy-inspired parser", "author": ["Charniak", "Eugene"], "venue": "In ACL, pp", "citeRegEx": "Charniak and Eugene.,? \\Q2000\\E", "shortCiteRegEx": "Charniak and Eugene.", "year": 2000}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Automating string processing in spreadsheets using input-output examples", "author": ["Gulwani", "Sumit"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Gulwani and Sumit.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani and Sumit.", "year": 2011}, {"title": "Programming by example", "author": ["Halbert", "Daniel Conrad"], "venue": "PhD thesis, University of California, Berkeley,", "citeRegEx": "Halbert and Conrad.,? \\Q1984\\E", "shortCiteRegEx": "Halbert and Conrad.", "year": 1984}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["Henderson", "James", "Titov", "Ivan"], "venue": "JMLR, 11:3541\u20133570,", "citeRegEx": "Henderson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2010}, {"title": "On the naturalness of software", "author": ["Hindle", "Abram", "Barr", "Earl T", "Su", "Zhendong", "Gabel", "Mark", "Devanbu", "Premkumar"], "venue": "In ICSE,", "citeRegEx": "Hindle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hindle et al\\.", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": "Science,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Syntactic and functional variability of a million code submissions in a machine learning MOOC", "author": ["Huang", "Jonathan", "Piech", "Chris", "Nguyen", "Andy", "Guibas", "Leonidas"], "venue": "In AIED,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["Jaakkola", "Tommi", "Haussler", "David"], "venue": null, "citeRegEx": "Jaakkola et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1998}, {"title": "Fast exact inference with a factored model for natural language parsing", "author": ["Klein", "Dan", "Manning", "Christopher D"], "venue": "In NIPS, pp", "citeRegEx": "Klein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2002}, {"title": "A factor graph model for software bug finding", "author": ["Kremenek", "Ted", "Ng", "Andrew Y", "Engler", "Dawson R"], "venue": "In IJCAI,", "citeRegEx": "Kremenek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kremenek et al\\.", "year": 2007}, {"title": "Learning programs: A hierarchical Bayesian approach", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In ICML, pp", "citeRegEx": "Liang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2010}, {"title": "Probabilistic cfg with latent annotations", "author": ["Matsuzaki", "Takuya", "Miyao", "Yusuke", "Tsujii", "Jun\u2019ichi"], "venue": "In ACL,", "citeRegEx": "Matsuzaki et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Matsuzaki et al\\.", "year": 2005}, {"title": "Effective self-training for parsing", "author": ["McClosky", "David", "Charniak", "Eugene", "Johnson", "Mark"], "venue": "In ACL, pp. 152\u2013159", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In ICML, pp", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "In ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Graph-based pattern-oriented, contextsensitive source code completion", "author": ["Nguyen", "Anh Tuan", "Tung Thanh", "Hoan Anh", "Tamrawi", "Ahmed", "Hung Viet", "Al-Kofahi", "Jafar", "Tien N"], "venue": "In ICSE,", "citeRegEx": "Nguyen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2012}, {"title": "A statistical semantic language model for source code", "author": ["Nguyen", "Tung Thanh", "Anh Tuan", "Hoan Anh", "Tien N"], "venue": "In ESEC/FSE,", "citeRegEx": "Nguyen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2013}, {"title": "Aggregate and mixedorder Markov models for statistical language processing", "author": ["Saul", "Lawrence", "Pereira", "Fernando"], "venue": "In EMNLP,", "citeRegEx": "Saul et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1997}, {"title": "Incremental Bayesian networks for structure prediction", "author": ["Titov", "Ivan", "Henderson", "James"], "venue": "In ICML,", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Mining succinct and high-coverage api usage patterns from source code", "author": ["Wang", "Jue", "Dang", "Yingnong", "Zhang", "Hongyu", "Chen", "Kai", "Xie", "Tao", "Dongmei"], "venue": "In MSR,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Additionally, massive open online courses (MOOCs) have begun to collect source code homework assignments from tens of thousands of students (Huang et al., 2013).", "startOffset": 140, "endOffset": 160}, {"referenceID": 10, "context": "At the same time, the software engineering community has recently observed that it is useful to think of source code as natural\u2014written by humans and meant to be understood by other humans (Hindle et al., 2012).", "startOffset": 189, "endOffset": 210}, {"referenceID": 15, "context": "Other related applications include finding bugs (Kremenek et al., 2007), mining and suggesting API usage patterns (Bruch et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 3, "context": ", 2007), mining and suggesting API usage patterns (Bruch et al., 2009; Nguyen et al., 2012; Wang et al., 2013), as a basis for code complexity metrics (Allamanis & Sutton, 2013), and to help with enforcing coding conventions (Allamanis et al.", "startOffset": 50, "endOffset": 110}, {"referenceID": 21, "context": ", 2007), mining and suggesting API usage patterns (Bruch et al., 2009; Nguyen et al., 2012; Wang et al., 2013), as a basis for code complexity metrics (Allamanis & Sutton, 2013), and to help with enforcing coding conventions (Allamanis et al.", "startOffset": 50, "endOffset": 110}, {"referenceID": 25, "context": ", 2007), mining and suggesting API usage patterns (Bruch et al., 2009; Nguyen et al., 2012; Wang et al., 2013), as a basis for code complexity metrics (Allamanis & Sutton, 2013), and to help with enforcing coding conventions (Allamanis et al.", "startOffset": 50, "endOffset": 110}, {"referenceID": 2, "context": ", 2013), as a basis for code complexity metrics (Allamanis & Sutton, 2013), and to help with enforcing coding conventions (Allamanis et al., 2014).", "startOffset": 122, "endOffset": 146}, {"referenceID": 7, "context": "Indeed, Hindle et al. (2012) have shown that even simple n-gram models are useful for improving code completion tools, and Nguyen et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 7, "context": "Indeed, Hindle et al. (2012) have shown that even simple n-gram models are useful for improving code completion tools, and Nguyen et al. (2013) have extended these ideas.", "startOffset": 8, "endOffset": 144}, {"referenceID": 0, "context": "Thus, LTTs can be viewed as a Markov model equipped with a stack\u2014a special case of a Probabilistic Pushdown Automata (PPDA) (Abney et al., 1999).", "startOffset": 124, "endOffset": 144}, {"referenceID": 18, "context": "PCFGs and their variants are components of state-of-the-art parsers of English (McClosky et al., 2006), and many variants have been explored: internal node annotation Charniak (1997) and latent annotations Matsuzaki et al.", "startOffset": 79, "endOffset": 102}, {"referenceID": 16, "context": "PCFGs and their variants are components of state-of-the-art parsers of English (McClosky et al., 2006), and many variants have been explored: internal node annotation Charniak (1997) and latent annotations Matsuzaki et al.", "startOffset": 80, "endOffset": 183}, {"referenceID": 16, "context": ", 2006), and many variants have been explored: internal node annotation Charniak (1997) and latent annotations Matsuzaki et al. (2005). Aside from the question of the order of the traversals, the traversal variables make LTTs special cases of Probabilistic Pushdown Automata (PPDA) (for definition and weak equivalence to PCFGs, see Abney et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 0, "context": "Aside from the question of the order of the traversals, the traversal variables make LTTs special cases of Probabilistic Pushdown Automata (PPDA) (for definition and weak equivalence to PCFGs, see Abney et al. (1999)).", "startOffset": 197, "endOffset": 217}, {"referenceID": 10, "context": "We previously mentioned Hindle et al. (2012) and Allamanis & Sutton (2013), which tackle the same task as us but with simple NLP models.", "startOffset": 24, "endOffset": 45}, {"referenceID": 10, "context": "We previously mentioned Hindle et al. (2012) and Allamanis & Sutton (2013), which tackle the same task as us but with simple NLP models.", "startOffset": 24, "endOffset": 75}, {"referenceID": 10, "context": "We previously mentioned Hindle et al. (2012) and Allamanis & Sutton (2013), which tackle the same task as us but with simple NLP models. Very recently, Allamanis & Sutton (2014) explores more sophisticated nonparametric Bayesian grammar models of source code for the purpose of learning code idioms.", "startOffset": 24, "endOffset": 178}, {"referenceID": 10, "context": "We previously mentioned Hindle et al. (2012) and Allamanis & Sutton (2013), which tackle the same task as us but with simple NLP models. Very recently, Allamanis & Sutton (2014) explores more sophisticated nonparametric Bayesian grammar models of source code for the purpose of learning code idioms. Liang et al. (2010) use a sophisticated non-parametric model to encode the prior that programs should factorize repeated computation, but there is no learning from existing source code, and the prior is only applicable to a functional programming language with quite simple syntax rules.", "startOffset": 24, "endOffset": 320}, {"referenceID": 6, "context": "For the gradientbased optimization, we used AdaGrad (Duchi et al., 2011) with stochastic minibatches.", "startOffset": 52, "endOffset": 72}], "year": 2014, "abstractText": "We study the problem of building generative models of natural source code (NSC); that is, source code written by humans and meant to be understood by humans. Our primary contribution is to describe new generative models that are tailored to NSC. The models are based on probabilistic context free grammars (PCFGs) and neuro-probabilistic language models (Mnih & Teh, 2012), which are extended to incorporate additional source code-specific structure. These models can be efficiently trained on a corpus of source code and outperform a variety of less structured baselines in terms of predictive log likelihoods on held-out data.", "creator": "LaTeX with hyperref package"}}}