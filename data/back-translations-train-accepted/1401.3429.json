{"id": "1401.3429", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Latent Tree Models and Approximate Inference in Bayesian Networks", "abstract": "We propose a novel method for approximate inference in Bayesian networks (BNs). The idea is to sample data from a BN, learn a latent tree model (LTM) from the data offline, and when online, make inference with the LTM instead of the original BN. Because LTMs are tree-structured, inference takes linear time. In the meantime, they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good. Empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost.", "histories": [["v1", "Wed, 15 Jan 2014 04:46:37 GMT  (228kb)", "http://arxiv.org/abs/1401.3429v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yi wang 0006", "nevin lianwen zhang", "tao chen"], "accepted": true, "id": "1401.3429"}, "pdf": {"name": "1401.3429.pdf", "metadata": {"source": "CRF", "title": "Latent Tree Models and Approximate Inference in Bayesian Networks", "authors": ["Yi Wang", "Nevin L. Zhang", "Tao Chen"], "emails": ["wangyi@cse.ust.hk", "lzhang@cse.ust.hk", "csct@cse.ust.hk"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Latent Tree Model", "text": "The first component m denotes the collection of parameters in M. It contains a conditional probability table for each node that contains its parents. Let X and Y be the set of manifest variables and the set of latent variables in M. We use P (X, Y | m, \u03b8m) or PM (X, Y) in short to denote the common distribution represented by M. Two LTMs M and M \u2032 are marginally equivalent if they have the same set of manifest variables X and PM (X) = PM \u2032 (X). A model m contains another model m \u00b2, if for each model m \u00b2 there is a model called Z \u00b2 and (m \u00b2, stem \u00b2) is marginally equivalent."}, {"heading": "3. Approximating Bayesian Networks with Latent Tree Models", "text": "In this section, we will examine the problem of approaching a BN to an LTM. Let N be the approximate BN. Let X be the set of variables in N. For an LTM M to be an approximation to N, it should use X as its manifest variables, and the cardinalities of its latent variables should not exceed a predetermined threshold. Figure 1 (b), 1 (c), and 1 (d) show three examples of LTMs approaching the BN in Figure 1 (a). They are used to illustrate various steps in our method.Let PN (X) be the joint distribution represented by N. Approaching M is of high quality when PM (X) is close to PN (X). We measure the quality of the approximation using the KL divergence (Cover & Thomas, 1991) D [PN (X): Approaching M is of high quality when PM (X) is close to a PX (our target)."}, {"heading": "3.1 Parameter Optimization", "text": "We start by tackling the second partial problem. In view of a model m, our goal is to find the second problem in this area. (PN (X).P (X | m, \u03b8m).It turns out that due to the presence of latent variables, it is difficult to directly minimize the KL divergence, which can be seen by extending the KL divergence as follows: D [PN (X).P (X | m, \u03b8m)] = X PN (X | m).P (X | m).P (X).P (X).P (X).P (X).P (X).P).P (X).M).P (X).P (X).P (X).M).P (X).M (X).X).M (X).M (X).M).M (X).M (X).M)."}, {"heading": "3.2 Exhaustive Search for the Optimal Model", "text": "Now we look at the first partial problem, i.e. the search for the best model m. One simple way to solve this problem is to exploit all possible models, find the optimal parameters \u03b8 m for each model m, calculate the KL divergence D [PN (X) VP (X | m, \u03b8 m)] and then return a model m with the minimum KL divergence. The problem with this solution is its high computational complexity. In a set of manifest variables X, there are an infinite number of models. New models can always be obtained by inserting latent variables into an existing model. As we will show in Section 3.5, it is sufficient to consider a limited subspace, i.e. the subspace of regular models. Nevertheless, there are still superexponentially many regular models (Zhang, 2004). For each model, we need to optimize its parameters by performing EM, which is a time-consuming process."}, {"heading": "3.3 Heuristic Construction of Model Structure", "text": "We first present a heuristic to determine the model structure. In an LTM IM, two manifest variables are called siblings if they share the same parent. Our heuristics are based on two ideas: (1) In an LTM M, siblings are generally more closely correlated with each other than variables that are far apart from each other. (2) If M is a good approximation of N, then two variables Xi and Xj are closely correlated in M, if and only if they are closely correlated in N. Thus, we can examine any pair of variables in N, select the two variables that are most closely correlated, and introduce a latent variable as parent. We measure the strength of the correlation between a pair of variables Xi and Xj based on the mutual information. (Cover & Thomas, 1991) IN (Xi; Xj) = Xj YJ PN (Xi, Xj), we log PN (Xj) PN (Xj)."}, {"heading": "3.4 Cardinalities of Latent Variables", "text": "After we have obtained a model structure, the next step is to determine the cardinalities of the latent variables. We set the cardinalities of all latent variables to a given value. In the following, we will discuss how the choice of C affects the quality of the approximation and the resulting efficiency. In this case, we will first discuss the effects of the value of C on the approximate quality. We will start by looking at the case when C corresponds to the manifest variables, when C corresponds to the value of X, i.e., the product of the cardinalities of all manifest variables. In this case, any latent variable can be considered a common variable of all manifest variables. We can therefore specify the parameters of the model so that P (X) is the P (X, m) = PN (X). That is, m can capture all interactions between the manifest variables. What happens if we reduce C? It can be shown that the approximation quality is degraded."}, {"heading": "3.5 Model Regularization", "text": "Suppose we have obtained a model that uses the technique described in Section 3.3 and adjusts the cardinalities of the latent variables to a certain value. In the following two subsections, we will show that it is sometimes possible to simplify the model without compromising the approximation quality. First, we note that it may be irregular. As an example, we consider the model in Figure 1 (b). It is an approximation of Figure 1 (a) with C = 8. Checking the latent variables, we find that Y5 violates the regularity state. It has only two neighbors and Figure 1 (b)."}, {"heading": "3.6 Further Simplifications", "text": "Let us take model m \u2032 in picture 1 (c) as an example. It contains two adjacent latent variables Y1 and Y2. Both variables are saturated. & 2p; & 2p # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3.7 The Algorithm LTAB", "text": "In summary, we have outlined an algorithm for approximating BNs using LTMs. We call the LTAB algorithm, an abbreviation for Latent Tree Approximation of Bayesian Network. It has 3 inputs: a BN, a specified cardinality C for latent variables, and a sample size N. The output of LTAB is an LTM that approaches PN (X), the common probability distribution represented by N. LTAB is briefly described as follows: 1. Generate a dataset D of N i.i.d. samples from PN (X). (Section 3.1) 2. Obtain an LTM structure by performing hierarchical clustering of variables, useful mutual information based on D as a measure of similarity. (Section 3.3) 3. Set cardinalities of latent variables at C and simplify the model. (Section 3.4 - 3.6) 4. Optimize the parameters by executing Section 3.5."}, {"heading": "4. LTM-based Approximate Inference", "text": "We propose the following two-phase method: 1. Offline: Use LTAB to construct an approximation; sample size N should be set as large as possible, while cardinality C should be determined to meet the requirement of inferential complexity; 2. Online: Make inferences in M instead of N. Specifically, return Q PM (Q | E = e) as approximation to PN (Q | E = e) for evidence and a querying variable."}, {"heading": "5. Empirical Results", "text": "In this section, we evaluate empirically our approximate sequence method. First, we examine the influence of sample size N and cardinality C on the performance of our method. Then, we compare our method with CTP, LBP, the CL-based method and the LCM-based method. We used 8 networks in our experiments. They are shown in Table 3. CPCS54 is a subset of the CPCS network (Pradhan et al., 1994). The other networks are available at http: / / www.cs.huji.ac.il / labs / compbio / Repository /. Table 3 also reports on the properties of the networks, including the number of nodes, the average / maximum index and cardinality of the nodes, as well as the inferential complexity (i.e. the sum of clique sizes in the clique tree). The networks are sorted in ascending order in terms of inferential complexity."}, {"heading": "5.1 Impact of N and C", "text": "The aforementioned lcihsrc\u00fc\u00fcehsrcS nvo edn rf\u00fc ide rf\u00fc ide nlrf\u00fc \u00fceegnlrcehnlrcnlrrc\u00fcehcnlrf\u00fc ide rf\u00fc \u00fc\u00fceegnlrteeegnln rf\u00fc ide rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rfu rf\u00fc rfu rf\u00fc rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu"}, {"heading": "5.2 Comparison with CTP", "text": "The first concern is how accurate our method is. By examining Figure 4, we argue that our method always achieves a good approximation accuracy: for HAILFINDER, CPCS54, WATER, the average KL divergence of our method is about 10 \u2212 3; for the other networks, the average KL divergence is about 10 \u2212 2. Next, we compare the inferential efficiency of our method and the CTP algorithm. The runtime of the CTP is indicated by dashed horizontal lines in the diagrams of Figure 5. In summary, our method is more efficient than the CTP algorithm. In particular, for the five networks with the highest inferential complexity, our method is two to three orders of magnitude faster than CTP."}, {"heading": "5.3 Comparison with LBP", "text": "We are now comparing our method with LBP. The latter is an iterative algorithm. It can be used as an inference method available at any time by performing a certain number of iterations. In our first experiment, we run LBP as long as our method and compare its approximation accuracy. We have this for each network and each value of C. The accuracy of LBP is illustrated by the curves referred to as LBP in Figure 4. By comparing these curves with the LTM curves for N = 100k, we see that in most cases our method achieves a much higher accuracy than LBP: For WATER, the difference in the average KL divergence shows up to three orders of magnitude; for the other networks, the difference is up to an order of magnitude. For HAILFINDER with C = 32, LBP is two times more accurate than our method. However, our method LLBP also shows a good approximation accuracy of KL curves. The average KL divergence is smaller than 10 times, and finally, we find that for two curves each, we have two - LBP."}, {"heading": "5.4 Comparison with CL-based Method", "text": "In this subsection, we compare our method with the CL-based method. Specifically, for each network, we learn a tree model from the 100k samples using the highly exciting tree algorithm developed by Chow and Liu (1968). Compared to the CL-based method, our method achieves higher accuracy in all networks except MILDEW. In INSURANCE, WATER, and BARLEY, the differences are significant. In MILDEW, our method competes with the CL-based method. Meanwhile, we note that the CL-based method achieves good approximations in all networks except MILDEW. The average KL divergence is about 10 \u2212 2. An obvious advantage of the CL-based method is its high efficiency. This can be derived from the summary of the results below the CILLEY line, with most of the results of the second line CL-based."}, {"heading": "5.5 Comparison with LCM-based Method", "text": "Lowd and Domingos (2005) have already investigated the use of LCM for density estimation. Using a data set, they determine the cardinality of the latent variables using hold-out validation and optimize the parameters using EM. It turns out that the learned LCM achieves a good model fit on a separate test set. LCM was also used to answer simulated probabilistic queries and the results turn out to be good. Inspired by their work, we also learned a number of LCMs from the 100k samples and compared them with LTMs on the approximate follow-up task. Our learning strategy is slightly different. As LCM is a special case of LTM, its inferential complexity can also be controlled by modifying the cardinality of the latent variables. In our experiments, we use cardinality in such a way that the sum of LCS sizes in the clique tree of the LLCM-C formation is roughly comparable to that of the LCM models used for the WIND."}, {"heading": "6. Related Work", "text": "The idea of approaching complex BNs through simple models and using the latter to draw conclusions has been studied before. Existing work falls mainly into two categories; the work in the first category approaches the common distributions of the BNs and uses the approach to answer all probable questions; in contrast, the work in the second category is query-specific. It assumes that the evidence is known and directly determines the posterior distribution of the interrogating nodes. Our method falls into the first category. We are investigating the use of LTMs within this framework. This possibility has also been explored by Pearl (1988) and Sarkar (1995). Pearl (1988) develops an algorithm for constructing an LTM that is marginally equivalent to a common distribution."}, {"heading": "7. Concluding Remarks", "text": "With our scheme, you can balance approximation accuracy against inferential complexity. Our scheme achieves good accuracy at low cost across all the networks studied, and in particular consistently outperforms LBP. We also show that LTMs are superior when used for approximate conclusions. The current bottleneck in the offline phase is parameter learning. We used the EM algorithm to optimize parameters, which is notoriously time consuming. The problem is particularly serious when the parameter C and sample size are large. One way to speed up parameter learning is to adjust the agglomerative cluster technique to learn the cardinality of a latent variable from data (Elidan & Friedman, 2001).The basic idea is to complete the training data by setting the cardinality of the latent variables large enough and assigning each dataset to a latent state with a high probability of repeating the problem."}, {"heading": "Acknowledgments", "text": "We thank Haipeng Guo and Yiping Ke for revealing discussions and the anonymous reviewers for their valuable comments and suggestions on the earlier version of this essay. Research on this work was supported by Hong Kong Grants Council Grants # 622105 and # 622307 and the National Basic Research Program of China (also known as the 973 Program) as part of Project # 2003CB517106. Work was completed when the first author was placed on leave at HKUST Fok Ying Tung Graduate School, Guangzhou, China."}], "references": [{"title": "Approximating posterior distributions in belief networks using mixtures", "author": ["C.M. Bishop", "N. Lawrence", "T. Jaakkola", "M.I. Jordan"], "venue": "In Proceedings of the 10th Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Bishop et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1997}, {"title": "Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables", "author": ["D.M. Chickering", "D. Heckerman"], "venue": "Machine Learning,", "citeRegEx": "Chickering and Heckerman,? \\Q1997\\E", "shortCiteRegEx": "Chickering and Heckerman", "year": 1997}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K. Chow", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Chow and Liu,? \\Q1968\\E", "shortCiteRegEx": "Chow and Liu", "year": 1968}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.R. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Learning the dimensionality of hidden variables", "author": ["G. Elidan", "N. Friedman"], "venue": "In Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Elidan and Friedman,? \\Q2001\\E", "shortCiteRegEx": "Elidan and Friedman", "year": 2001}, {"title": "Data perturbation for escaping local maxima in learning", "author": ["G. Elidan", "M. Ninio", "N. Friedman", "D. Schuurmans"], "venue": "In Proceedings of the 18th National Conference on Artificial Intelligence,", "citeRegEx": "Elidan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Elidan et al\\.", "year": 2002}, {"title": "A revolution: belief propagation in graphs with cycles", "author": ["B.J. Frey", "D.J.C. MacKay"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Frey and MacKay,? \\Q1997\\E", "shortCiteRegEx": "Frey and MacKay", "year": 1997}, {"title": "Applied Latent Class Analysis", "author": ["J.A. Hagenaars", "A.L. McCutcheon"], "venue": null, "citeRegEx": "Hagenaars and McCutcheon,? \\Q2002\\E", "shortCiteRegEx": "Hagenaars and McCutcheon", "year": 2002}, {"title": "Propagating uncertainty in Bayesian networks by probabilistic logic sampling", "author": ["M. Henrion"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "Henrion,? \\Q1988\\E", "shortCiteRegEx": "Henrion", "year": 1988}, {"title": "The behavior of maximum likelihood estimates under nonstandard conditions", "author": ["P.J. Huber"], "venue": "In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "Huber,? \\Q1967\\E", "shortCiteRegEx": "Huber", "year": 1967}, {"title": "Naive Bayes models for probability estimation", "author": ["D. Lowd", "P. Domingos"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Lowd and Domingos,? \\Q2005\\E", "shortCiteRegEx": "Lowd and Domingos", "year": 2005}, {"title": "Loopy belief propagation for approximate inference: an empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "In Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Murphy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Knowledge engineering for large belief networks", "author": ["M. Pradhan", "G. Provan", "B. Middleton", "M. Henrion"], "venue": "In Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Pradhan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 1994}, {"title": "Modeling uncertainty using enhanced tree structures in expert systems", "author": ["S. Sarkar"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Sarkar,? \\Q1995\\E", "shortCiteRegEx": "Sarkar", "year": 1995}, {"title": "Mean field theory for sigmoid belief networks", "author": ["L.K. Saul", "T. Jaakkola", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Saul et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1996}, {"title": "Exploiting tractable substructures in intractable networks", "author": ["L.K. Saul", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Saul and Jordan,? \\Q1996\\E", "shortCiteRegEx": "Saul and Jordan", "year": 1996}, {"title": "Severity of local maxima for the em algorithm: Experiences with hierarchical latent class models", "author": ["Y. Wang", "N.L. Zhang"], "venue": "In Proceedings of the 3rd European Workshop on Probabilistic Graphical Models,", "citeRegEx": "Wang and Zhang,? \\Q2006\\E", "shortCiteRegEx": "Wang and Zhang", "year": 2006}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}], "referenceMentions": [{"referenceID": 18, "context": "They are previously known as hierarchical latent class models (Zhang, 2004).", "startOffset": 62, "endOffset": 75}, {"referenceID": 12, "context": "Pearl (1988) was the first to identify LTMs as a potentially useful class of models.", "startOffset": 0, "endOffset": 13}, {"referenceID": 12, "context": "We compared our method with loopy belief propagation (LBP) (Pearl, 1988), a standard approximate inference method which has been successfully used in many real world domains (Frey & MacKay, 1997; Murphy, Weiss, & Jordan, 1999).", "startOffset": 59, "endOffset": 72}, {"referenceID": 18, "context": "It can be reduced to a regular model m\u2032 that is marginally equivalent to and contains fewer parameters than m (Zhang, 2004).", "startOffset": 110, "endOffset": 123}, {"referenceID": 9, "context": "It is well known that \u03b8\u0302m converges almost surely to \u03b8 m as the sample size N approaches infinity (Huber, 1967).", "startOffset": 98, "endOffset": 111}, {"referenceID": 8, "context": "Since PN (X) is represented by BN N , we use logic sampling (Henrion, 1988) for this task.", "startOffset": 60, "endOffset": 75}, {"referenceID": 3, "context": "We thus use the EM algorithm (Dempster et al., 1977).", "startOffset": 29, "endOffset": 52}, {"referenceID": 5, "context": "In practice, one can also use various techniques such as multiple restart (Chickering & Heckerman, 1997) and data permutation (Elidan et al., 2002) to alleviate this issue.", "startOffset": 126, "endOffset": 147}, {"referenceID": 18, "context": "However, there are still super-exponentially many regular models (Zhang, 2004).", "startOffset": 65, "endOffset": 78}, {"referenceID": 18, "context": "As shown by Zhang (2004), for any parameters \u03b8m of m, there exists parameters \u03b8m\u2032 of m\u2032 such that (m,\u03b8m) and (m\u2032,\u03b8m\u2032) are marginally equivalent.", "startOffset": 12, "endOffset": 25}, {"referenceID": 18, "context": "As shown by Zhang (2004), a model is marginally equivalent to its unrooted version.", "startOffset": 12, "endOffset": 25}, {"referenceID": 13, "context": "CPCS54 is a subset of the CPCS network (Pradhan et al., 1994).", "startOffset": 39, "endOffset": 61}, {"referenceID": 1, "context": "The multiple restarting strategy by Chickering and Heckerman (1997) was used to avoid local maxima.", "startOffset": 36, "endOffset": 68}, {"referenceID": 2, "context": "More specifically, for each network, we learn a tree model from the 100k samples using the maximum spanning tree algorithm developed by Chow and Liu (1968). We then use the learned tree model to answer the queries.", "startOffset": 136, "endOffset": 156}, {"referenceID": 2, "context": "More specifically, for each network, we learn a tree model from the 100k samples using the maximum spanning tree algorithm developed by Chow and Liu (1968). We then use the learned tree model to answer the queries. The approximation accuracy of the CL-based method are shown as solid horizontal lines in the plots in Figure 4. Comparing with the CL-based method, our method achieves higher accuracy in all the networks except for MILDEW. For INSURANCE, WATER, and BARLEY, the differences are significant. For MILDEW, our method is competitive with the CL-based method. In the meantime, we notice that the CL-based method achieves good approximations in all the networks except for BARLEY. The average KL divergence is around or less than 10\u22122. An obvious advantage of CL-based method is its high efficiency. This can be seen from the plots in Figure 5. In most of the plots, the CL line locates below the second data point on the LTM curve. The exception is MILDEW, for which the running time of the CL-based method is as long as our method with C = 16. In summary, the results suggest that the CL-based method is a good choice for approximate inference if the online inference time is very limited. Otherwise, our method is more attractive because it is able to produce more accurate results when more time is allowed. 5.5 Comparison with LCM-based Method Lowd and Domingos (2005) have previously investigated the use of LCM for density estimation.", "startOffset": 136, "endOffset": 1382}, {"referenceID": 9, "context": "This possibility has also been studied by Pearl (1988) and Sarkar (1995).", "startOffset": 42, "endOffset": 55}, {"referenceID": 9, "context": "This possibility has also been studied by Pearl (1988) and Sarkar (1995). Pearl (1988) develops an algorithm for constructing an LTM that is marginally equivalent to a joint distribution P (X), assuming such an LTM exists.", "startOffset": 42, "endOffset": 73}, {"referenceID": 9, "context": "This possibility has also been studied by Pearl (1988) and Sarkar (1995). Pearl (1988) develops an algorithm for constructing an LTM that is marginally equivalent to a joint distribution P (X), assuming such an LTM exists.", "startOffset": 42, "endOffset": 87}, {"referenceID": 9, "context": "This possibility has also been studied by Pearl (1988) and Sarkar (1995). Pearl (1988) develops an algorithm for constructing an LTM that is marginally equivalent to a joint distribution P (X), assuming such an LTM exists. Sarkar (1995) studies how to build good LTMs when only approximations are amenable.", "startOffset": 42, "endOffset": 237}, {"referenceID": 1, "context": "Chow and Liu (1968) consider tree-structured BNs without latent variables.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Chow and Liu (1968) consider tree-structured BNs without latent variables. They develop a maximum spanning tree algorithm to efficiently construct the tree model that is closest to the original BN in terms of KL divergence. Lowd and Domingos (2005) learn an LCM to summarize a data set.", "startOffset": 0, "endOffset": 249}, {"referenceID": 0, "context": "Bishop et al. (1997) consider another improvement, i.", "startOffset": 0, "endOffset": 21}], "year": 2008, "abstractText": "We propose a novel method for approximate inference in Bayesian networks (BNs). The idea is to sample data from a BN, learn a latent tree model (LTM) from the data offline, and when online, make inference with the LTM instead of the original BN. Because LTMs are tree-structured, inference takes linear time. In the meantime, they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good. Empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost.", "creator": "dvips(k) 5.94b Copyright 2004 Radical Eye Software"}}}