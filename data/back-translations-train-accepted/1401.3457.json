{"id": "1401.3457", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Learning Document-Level Semantic Properties from Free-Text Annotations", "abstract": "This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations. Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as a real bargain or good value. These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with joint inference. We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.", "histories": [["v1", "Wed, 15 Jan 2014 05:14:31 GMT  (513kb)", "http://arxiv.org/abs/1401.3457v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["s r k branavan", "harr chen", "jacob eisenstein", "regina barzilay"], "accepted": true, "id": "1401.3457"}, "pdf": {"name": "1401.3457.pdf", "metadata": {"source": "CRF", "title": "Learning Document-Level Semantic Properties from Free-Text Annotations", "authors": ["S.R.K. Branavan", "Harr Chen", "Jacob Eisenstein", "Regina Barzilay"], "emails": ["BRANAVAN@CSAIL.MIT.EDU", "HARR@CSAIL.MIT.EDU", "JACOBE@CSAIL.MIT.EDU", "REGINA@CSAIL.MIT.EDU"], "sections": [{"heading": null, "text": "These notes are representative of the underlying semantic properties, but unlike expert comments, they are loud: laymen may use different labels to denote the same property, and some labels may be missing. To learn how to use such vocal annotations, we find a hidden paraphrase structure clustering the keyphrases. The paraphrase structure is linked to a latent theme model of review texts, which allows the system to predict the properties of uncommented documents and effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with common conclusions."}, {"heading": "1. Introduction", "text": "In fact, it is such that most of them are able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, that they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "2. Related Work", "text": "The material presented in this section covers three lines of related work. First, we will discuss work on Bayesian theme modeling related to our technique for learning from free-text annotations. Second, we will discuss current methods for identifying and analyzing product properties from the overview text. Finally, we will place our summary work in the landscape of previous research on summarizing multiple documents."}, {"heading": "2.1 Bayesian Topic Modeling", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves, and that we will be able, that we will be able, that we will be able."}, {"heading": "2.2 Property Assessment for Review Analysis", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2.3 Multidocument Summarization", "text": "In fact, it is customary for the repetition of information relating to its meaning (Barzilay, & Elhadad, 1999) to be crucial, because the majority of documents have the same facts. Indeed, it is customary to assume the repetition of information relating to its meaning (Barzilay, & Elhadad, 1999)."}, {"heading": "3. Analysis of Free-Text Keyphrase Annotations", "text": "The results of this analysis motivate the development of the learning algorithm described in Section 4. We are conducting this research in the area of online restaurant reviews, using documents downloaded from the popular Epinions1 website. Users of this website rate products by providing both a textual description of their opinion and concise lists of key terms (pros and cons) that summarize the rating. Pros / cons key terms are an appealing source of comments for online review texts, but they are provided independently of multiple users and are therefore unlikely to be as clean as expert comments. In our analysis, we focus on two characteristics of free text comments: incompleteness and inconsistency. Measuring the incompleteness of these characteristics of annotations quantifies the extent of label omission in free text comments, while inconsistency reflects the variation of key terms."}, {"heading": "3.1 Incompleteness", "text": "To measure incompleteness, we compare the attributes given by the review authors in terms of advantages and disadvantages with those indicated only in the review text as judged by expert commentators. This comparison is performed with precision, memory and F score. In this setting, the proportion of semantic attributes in the text for which the review author also provided at least one keyphrase for annotations; precision is the proportion of keyphrases conveying properties that are considered supported by the text; and F score is their harmonious mean. The results of the comparison are summarized in the left half of Table 1. These incompleteness results show the significant discrepancy between user and expert comments. As expected, the recollection is quite low; more than 40% of property occurrences are indicated in the review text without explicitly being mentioned in the annotations. The precision indicators indicate that the reverse would be true, even if some of the intuitive ones are not mentioned in the Restoration text."}, {"heading": "3.2 Inconsistency", "text": "To quantify this phenomenon, the judges manually grouped a subset of keyphrases associated with the six previously mentioned properties. Specifically, 121 keyphrases associated with the six main properties were selected, taking into account 10.8% of all keyphrase occurrences. We use these manually grouped annotations to examine the distribution pattern of keyphrases describing the same underlying property, using two different statistics. First, the number of different keyphrases for each property gives a lower limit on the number of possible paraphrases. Second, we measure how often the most common keyphrase is used to comment on each property, i.e. the actual coverage of this property is a multiple property that we use within each phrase."}, {"heading": "4. Model Description", "text": "We present a generative Bayesian model for documents commented with free-text keyphrases. Rather, our model assumes that each commented document is generated from a set of underlying semantic topics. Semantic topics generate the document text by indexing a language model; in our approach, they are also associated with clusters of keyphrases. In this way, the model can be considered an extension of latent dirichlet allocation (Blei et al., 2003), with latent topics tending to be distorted in addition to the keyphrases that appear in the training data. However, this coupling is flexible as some words can be extracted from topics that are not represented by the keyphrase annotations, allowing the model to effectively learn in the presence of incomplete annotations while encouraging keyphrase clustering on topics that are supported by the document text. Another critical aspect of our model is that we want the ability to use arbitrary keyword comparisons between the keywords."}, {"heading": "4.1 Keyphrase Clustering", "text": "To handle the hidden paraphrase structure of keyphrases, one component of the model estimates clustering over keyphrases. The goal is to obtain clusters in which each cluster corresponds to a well-defined semantic theme - for example, both \"healthy\" and \"good nutrition\" should be grouped into a single cluster. As our common overall model is generative, a generative model for clustering could easily be integrated into the larger framework. Such an approach would treat all keyphrases in each cluster as if they were generated from a parametric distribution. Therefore, this representation would not allow many powerful features to assess the similarity of keyphrase pairs, such as string overlaps or keyphrase co-occurrences in a corpus (McCallum, Bellare, & Pereira, 2005). For this reason, we present each keyphrase as a real vector rather than its surface shape."}, {"heading": "4.2 Document Topic Modeling", "text": "Our analysis of the text of the document is based on probabilistic topic models such as LDA (Lead et al., 2003). Within the framework of the LDA, each word is generated from a language model indexed by the topic assignment of the word. Thus, instead of identifying a single topic for a document, LDA identifies a distribution across the topics. The assignment of a topic with high probability identifies compact language models with low entropy, so that the probability mass of the language model for each topic is divided into a relatively small vocabulary. Our model works in a similar manner and identifies for each word a topic designated by z in Figure 4. However, where LDA learns a distribution over the topics for each document, we deterministically construct a document-specific topic distribution from the clusters represented by the key phrases of the document - this is the rule in the illustration."}, {"heading": "4.3 Generative Process", "text": "Our model assumes that all observed data are generated by a stochastic process with hidden parameters. \u2022 In this section, we formally define this generative process. This specification draws conclusions about the hidden parameters based on the observed data, which include the following topic: \u2022 For each of the L keyphrases, a vector s of length L is formed, which represents a paired similarity value in the interval [0, 1] to each other keyphrase. \u2022 For each document d, its wordpocket wd of length Nd. The nth word d is wd, n.2. Note that we model each similarity as an independent model; clearly, this assumption is too strong, due to symmetry and transitivity. Models that make similar assumptions about the independence of related hidden variables have previously been proven successful (for example, Toutanova & Johnson, 2008). \u2022 For each document d, a set of keyphrase annotations containing an index containing a document phrase."}, {"heading": "5. Parameter Estimation", "text": "In Bayesian conclusions, we estimate the distribution for each parameter depending on the observed data and hyperparameters. Such conclusions are generally unfeasible, but sampling approaches allow us to consider approximate distributions for each parameter of interest. It is possible to construct a Markov chain whose stationary distribution is the posterior distribution of the model parameters (Gelman, Carlin, Stern, & Rubin, 2004). The use of sampling techniques in natural language processing has previously been investigated by many researchers, including Finkel, Grenager and Manning (2005)."}, {"heading": "6. Evaluation of Summarization Quality", "text": "The goal of PRE \u0301 CIS is to provide users with effective access to review data via mobile devices. PRE \u0301 CIS contains information on 49,490 reviews of everything from childcare products to restaurants and movies. For each of these products, the system includes a collection of reviews downloaded from consumer websites such as Epinions, CNET and Amazon. PRE \u0301 CIS compresses data for each product into a short list of pros and cons supported by the majority of reviews. An example of a summary of 27 reviews for the film Pirates of the Caribbean is shown in Figure 6. Unlike traditional multi-document summarizers, the output of the system is not a sequence of sentences, but a list of phrases indicating product characteristics."}, {"heading": "6.1 Single-Document Evaluation", "text": "First, we evaluate our model for its ability to reflect the annotations contained in individual documents based on the text of the document. We compare it with a variety of baselines and variations of our model, demonstrating the appropriateness of our approach to this task. In addition, we explicitly evaluate the quality of the etiquette induced by our model by comparing it to a gold standard cluster of keyphrases provided by experienced annotators."}, {"heading": "6.1.1 EXPERIMENTAL SETUP", "text": "This year it is more than ever before."}, {"heading": "6.1.2 RESULTS", "text": "In fact, it is such that most of them will be able to find themselves in a position to move to another world, in which they are able to integrate themselves, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, that they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live in which they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they"}, {"heading": "6.2 Summarizing Multiple Reviews", "text": "Our latest experiment examines the ability of our system to summarize multiple documents. We examine the ability of our model to aggregate properties across a range of reviews, compared to baselines that aggregate through direct use of free-text annotations."}, {"heading": "6.2.1 DATA AND EVALUATION", "text": "We selected 50 restaurants, with five custom reviews for each restaurant. Ten commenters were asked to comment on the reviews for five restaurants, each with 25 reviews per commentator, using the same six outstanding features and the same annotation guidelines as in the previous restaurant review experiment (see Section 3). In creating the basic truth, we highlight properties that are supported in at least three of the five valuations.We generate real estate forecasts based on the same ratings using our model and baselines shown below. For the automatic methods, we register a prediction when the system rates the property as supported in at least two of the five ratings. 13 The recall, accuracy and F score are calculated using these aggregated predictions against the six outstanding properties highlighted by commenters."}, {"heading": "6.2.2 AGGREGATION APPROACHES", "text": "The most obvious starting point for summarizing multiple reviews would be to directly aggregate their free-text keyphrases. These annotations are probably representative of the semantic properties of the review, and unlike review text, keyphrases can be directly compared with each other. Our first starting line applies this term directly: \u2022 Keyphrase aggregation: A keyphrase is supported for a restaurant if at least two out of five reviews are commented verbatim with this keyphrase. This simple aggregation approach has the obvious downside that a very strict match between independently written reviews is required. That's why we are considering extensions of this aggregation approach that allow annotation paraphrasing: \u2022 Model cluster aggregation: A keyphrase is supported for a restaurant if at least two out of five reviews are forecasted or if our gold paraphrasing model is only used for clustering."}, {"heading": "6.2.3 RESULTS", "text": "Table 8 compares the baselines with our model. Our model outperforms all comment-based baselines, even though it does not have access to the keyphrase annotations. It is noteworthy that the keyphrase aggregation performs very poorly because it makes very few predictions due to its requirement for exact keyphrase string match. As before, the inclusion of keyphrase clusters improves the performance of baseline models. However, the incompleteness of the keyphrase annotations (see Section 3) explains why the recall scores are still low compared to our model. By including document text, our model achieves dramatically improved memory at the expense of lower precision and ultimately a significantly improved F-Score. These results show that the review summary benefits greatly from our common model of review text and keyphrases."}, {"heading": "7. Conclusions and Future Work", "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the support of the National Science Foundation (NSF) CAREER grant IIS0448168, the Microsoft Research New Faculty Fellowship, the U.S. Office of Naval Research (ONR), Quanta Computer, and Nokia Corporation. Harr Chen is supported by the National Defense Science and Engineering and NSF Graduate Fellowships. Thanks to Michael Collins, Zoran Dzunic, Amir Globerson, Aria Haghighi, Dina Katabi, Kristian Kersting, Terry Koo, Yoong Keok Lee, Brian Milch, Tahira Naseem, Dan Roy, Christina Sauper, Benjamin Snyder, Luke Zettlemoyer, and the magazine reviewers for helpful comments and suggestions. We also thank Marcia Davidson and members of the NLP Group at MIT for helping with expert comments. Any opinions, findings, conclusions, or recommendations expressed in this article are those of the authors and do not necessarily reflect the views of NSF, ONR, Microsoft, Quanta, or Nokia."}, {"heading": "Appendix A. Development and Test Set Statistics", "text": "Table 9 lists the semantic properties for each domain and the number of documents used to evaluate each of these properties. As mentioned in Section 6.1.1, the gold standard rating is complete, with each property being tested with each document. Conversely, the free-text ratings for each property only use documents with the property or its antagonist - for this reason, the number of documents for each semantic property differs."}, {"heading": "Appendix B. Additional Multiple Review Summarization Results", "text": "Table 10 lists the results of the multi-document experiment with a variation in aggregation - we require that each automatic method predicts a property for three out of five ratings to predict this property for the product, instead of two as shown in Section 6.2. For base systems, this change causes a steep drop in recall, resulting in significantly worse F-score results than in Section 6.2.3. In contrast, the F-score for our model is consistent in both ratings."}, {"heading": "Appendix C. Hyperparameter Settings", "text": "Table 11 lists the hyperparameters \u03b80, \u0445 0 and \u03c60 used in all experiments for each domain. These values were reached by tuning to the development theorem. In all cases \u03bb0 was set to (1, 1), which makes beta (\u03bb0) a uniform distribution."}], "references": [{"title": "Information fusion in the context of multidocument summarization", "author": ["R. Barzilay", "K. McKeown", "M. Elhadad"], "venue": "In Proceedings of ACL,", "citeRegEx": "Barzilay et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 1999}, {"title": "Extracting paraphrases from a parallel corpus", "author": ["R. Barzilay", "K.R. McKeown"], "venue": "In Proceedings of ACL,", "citeRegEx": "Barzilay and McKeown,? \\Q2001\\E", "shortCiteRegEx": "Barzilay and McKeown", "year": 2001}, {"title": "A latent Dirichlet model for unsupervised entity resolution", "author": ["I. Bhattacharya", "L. Getoor"], "venue": "In Proceedings of the SIAM International Conference on Data Mining", "citeRegEx": "Bhattacharya and Getoor,? \\Q2006\\E", "shortCiteRegEx": "Bhattacharya and Getoor", "year": 2006}, {"title": "Correlated Topic Models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "In Advances in NIPS,", "citeRegEx": "Blei and Lafferty,? \\Q2006\\E", "shortCiteRegEx": "Blei and Lafferty", "year": 2006}, {"title": "Supervised topic models", "author": ["D.M. Blei", "J. McAuliffe"], "venue": "In Advances in NIPS,", "citeRegEx": "Blei and McAuliffe,? \\Q2008\\E", "shortCiteRegEx": "Blei and McAuliffe", "year": 2008}, {"title": "A topic model for word sense disambiguation", "author": ["J. Boyd-Graber", "D. Blei", "X. Zhu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Boyd.Graber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Boyd.Graber et al\\.", "year": 2007}, {"title": "Learning document-level semantic properties from free-text annotations", "author": ["S.R.K. Branavan", "H. Chen", "J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of ACL,", "citeRegEx": "Branavan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2008}, {"title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "In Proceedings of ACM SIGIR,", "citeRegEx": "Carbonell and Goldstein,? \\Q1998\\E", "shortCiteRegEx": "Carbonell and Goldstein", "year": 1998}, {"title": "Statistical significance of MUC-6 results", "author": ["N. Chinchor"], "venue": "In Proceedings of the 6th Conference on Message Understanding,", "citeRegEx": "Chinchor,? \\Q1995\\E", "shortCiteRegEx": "Chinchor", "year": 1995}, {"title": "Evaluating message understanding systems: An analysis of the third message understanding conference (MUC-3)", "author": ["N. Chinchor", "D.D. Lewis", "L. Hirschman"], "venue": "Computational Linguistics,", "citeRegEx": "Chinchor et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Chinchor et al\\.", "year": 1993}, {"title": "A coefficient of agreement for nominal scales", "author": ["J. Cohen"], "venue": "Educational and Psychological Measurement,", "citeRegEx": "Cohen,? \\Q1960\\E", "shortCiteRegEx": "Cohen", "year": 1960}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["I. Dagan", "O. Glickman", "B. Magnini"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Towards generating patient specific summaries of medical articles", "author": ["N. Elhadad", "K.R. McKeown"], "venue": "In Proceedings of NAACL Workshop on Automatic Summarization,", "citeRegEx": "Elhadad and McKeown,? \\Q2001\\E", "shortCiteRegEx": "Elhadad and McKeown", "year": 2001}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Bayesian Data Analysis (2nd edition). Texts in Statistical Science", "author": ["A. Gelman", "J.B. Carlin", "H.S. Stern", "D.B. Rubin"], "venue": null, "citeRegEx": "Gelman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2004}, {"title": "Contextual dependencies in unsupervised word segmentation", "author": ["S. Goldwater", "T.L. Griffiths", "M. Johnson"], "venue": "In Proceedings of ACL,", "citeRegEx": "Goldwater et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2006}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "In Proceedings of SIGKDD,", "citeRegEx": "Hu and Liu,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Making Large-Scale Support Vector Machine Learning Practical, pp. 169\u2013184", "author": ["T. Joachims"], "venue": null, "citeRegEx": "Joachims,? \\Q1999\\E", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Automatic identification of pro and con reasons in online reviews", "author": ["Kim", "S.-M", "E. Hovy"], "venue": "In Proceedings of COLING/ACL,", "citeRegEx": "Kim et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2006}, {"title": "Discovery of inference rules for question-answering", "author": ["D. Lin", "P. Pantel"], "venue": "Natural Language Engineering,", "citeRegEx": "Lin and Pantel,? \\Q2001\\E", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "Opinion observer: Analyzing and comparing opinions on the web", "author": ["B. Liu", "M. Hu", "J. Cheng"], "venue": "In Proceedings of WWW,", "citeRegEx": "Liu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2005}, {"title": "Opinion integration through semi-supervised topic modeling", "author": ["Y. Lu", "C. Zhai"], "venue": "In Proceedings of WWW,", "citeRegEx": "Lu and Zhai,? \\Q2008\\E", "shortCiteRegEx": "Lu and Zhai", "year": 2008}, {"title": "Multi-document summarization by graph search and matching", "author": ["I. Mani", "E. Bloedorn"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Mani and Bloedorn,? \\Q1997\\E", "shortCiteRegEx": "Mani and Bloedorn", "year": 1997}, {"title": "Explorations in sentence fusion", "author": ["E. Marsi", "E. Krahmer"], "venue": "In Proceedings of the European Workshop on Natural Language Generation,", "citeRegEx": "Marsi and Krahmer,? \\Q2005\\E", "shortCiteRegEx": "Marsi and Krahmer", "year": 2005}, {"title": "A conditional random field for discriminativelytrained finite-state string edit distance", "author": ["A. McCallum", "K. Bellare", "F. Pereira"], "venue": "In Proceedings of UAI,", "citeRegEx": "McCallum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2005}, {"title": "A compositional context sensitive multidocument summarizer: exploring the factors that influence summarization", "author": ["A. Nenkova", "L. Vanderwende", "K. McKeown"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "Nenkova et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2006}, {"title": "Computer-Intensive Methods for Testing Hypotheses: An Introduction", "author": ["E. Noreen"], "venue": null, "citeRegEx": "Noreen,? \\Q1989\\E", "shortCiteRegEx": "Noreen", "year": 1989}, {"title": "OPINE: Extracting product features and opinions from reviews", "author": ["Popescu", "A.-M", "B. Nguyen", "O. Etzioni"], "venue": "In Proceedings of HLT/EMNLP,", "citeRegEx": "Popescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Popescu et al\\.", "year": 2005}, {"title": "Unsupervised topic modelling for multi-party spoken discourse", "author": ["M. Purver", "K.P. K\u00f6rding", "T.L. Griffiths", "J.B. Tenenbaum"], "venue": "In Proceedings of COLING/ACL,", "citeRegEx": "Purver et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Purver et al\\.", "year": 2006}, {"title": "Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation and user studies", "author": ["D. Radev", "H. Jing", "M. Budzikowska"], "venue": "In Proceedings of ANLP/NAACL Summarization Workshop", "citeRegEx": "Radev et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2000}, {"title": "Generating natural language summaries from multiple on-line sources", "author": ["D. Radev", "K. McKeown"], "venue": "Computational Linguistics,", "citeRegEx": "Radev and McKeown,? \\Q1998\\E", "shortCiteRegEx": "Radev and McKeown", "year": 1998}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W.M. Rand"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rand,? \\Q1971\\E", "shortCiteRegEx": "Rand", "year": 1971}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "On some pitfalls in automatic evaluation and significance testing for MT", "author": ["S. Riezler", "J.T. Maxwell"], "venue": "In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,", "citeRegEx": "Riezler and Maxwell,? \\Q2005\\E", "shortCiteRegEx": "Riezler and Maxwell", "year": 2005}, {"title": "Multiple aspect ranking using the good grief algorithm", "author": ["B. Snyder", "R. Barzilay"], "venue": "In Proceedings of NAACL/HLT,", "citeRegEx": "Snyder and Barzilay,? \\Q2007\\E", "shortCiteRegEx": "Snyder and Barzilay", "year": 2007}, {"title": "A joint model of text and aspect ratings for sentiment summarization", "author": ["I. Titov", "R. McDonald"], "venue": "In Proceedings of ACL,", "citeRegEx": "Titov and McDonald,? \\Q2008\\E", "shortCiteRegEx": "Titov and McDonald", "year": 2008}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["I. Titov", "R. McDonald"], "venue": "In Proceedings of WWW,", "citeRegEx": "Titov and McDonald,? \\Q2008\\E", "shortCiteRegEx": "Titov and McDonald", "year": 2008}, {"title": "A Bayesian LDA-based model for semi-supervised part-ofspeech tagging", "author": ["K. Toutanova", "M. Johnson"], "venue": "In Advances in NIPS,", "citeRegEx": "Toutanova and Johnson,? \\Q2008\\E", "shortCiteRegEx": "Toutanova and Johnson", "year": 2008}, {"title": "Multi-document summarization via information extraction", "author": ["M. White", "T. Korelsky", "C. Cardie", "V. Ng", "D. Pierce", "K. Wagstaff"], "venue": "In Proceedings of HLT,", "citeRegEx": "White et al\\.,? \\Q2001\\E", "shortCiteRegEx": "White et al\\.", "year": 2001}, {"title": "More accurate tests for the statistical significance of result differences", "author": ["A. Yeh"], "venue": "In Proceedings of COLING,", "citeRegEx": "Yeh,? \\Q2000\\E", "shortCiteRegEx": "Yeh", "year": 2000}, {"title": "Mark-up barking up the wrong tree", "author": ["A. Zaenen"], "venue": "Computational Linguistics,", "citeRegEx": "Zaenen,? \\Q2006\\E", "shortCiteRegEx": "Zaenen", "year": 2006}], "referenceMentions": [{"referenceID": 40, "context": "Learning-based approaches have dramatically increased the scope and robustness of such semantic processing, but they are typically dependent on large expert-annotated datasets, which are costly to produce (Zaenen, 2006).", "startOffset": 205, "endOffset": 219}, {"referenceID": 4, "context": "Combining topics induced by LDA with external supervision was first considered by Blei and McAuliffe (2008) in their supervised Latent Dirichlet Allocation (sLDA) model.", "startOffset": 82, "endOffset": 108}, {"referenceID": 16, "context": "For example, Hu and Liu (2004) employ association mining to identify noun phrases that express key portions of product reviews.", "startOffset": 13, "endOffset": 31}, {"referenceID": 27, "context": "Property extraction was further refined in OPINE (Popescu et al., 2005), another system for review analysis.", "startOffset": 49, "endOffset": 71}, {"referenceID": 16, "context": "Empirical results demonstrate that OPINE outperforms Hu and Liu\u2019s system in both opinion extraction and in identifying the polarity of opinion words. These two feature extraction methods are informed by human knowledge about the way opinions are typically expressed in reviews: for Hu and Liu (2004), human knowledge is encoded using WordNet and the seed adjectives; for Popescu et al.", "startOffset": 53, "endOffset": 300}, {"referenceID": 16, "context": "Empirical results demonstrate that OPINE outperforms Hu and Liu\u2019s system in both opinion extraction and in identifying the polarity of opinion words. These two feature extraction methods are informed by human knowledge about the way opinions are typically expressed in reviews: for Hu and Liu (2004), human knowledge is encoded using WordNet and the seed adjectives; for Popescu et al. (2005), opinion phrases are extracted via handcrafted rules.", "startOffset": 53, "endOffset": 393}, {"referenceID": 16, "context": "Empirical results demonstrate that OPINE outperforms Hu and Liu\u2019s system in both opinion extraction and in identifying the polarity of opinion words. These two feature extraction methods are informed by human knowledge about the way opinions are typically expressed in reviews: for Hu and Liu (2004), human knowledge is encoded using WordNet and the seed adjectives; for Popescu et al. (2005), opinion phrases are extracted via handcrafted rules. An alternative approach is to learn the rules for feature extraction from annotated data. To this end, property identification can be modeled in a classification framework (Kim & Hovy, 2006). A classifier is trained using a corpus in which free-text pro and con keyphrases are specified by the review authors. These keyphrases are compared against sentences in the review text; sentences that exhibit high word overlap with previously identified phrases are marked as pros or cons according to the phrase polarity. The rest of the sentences are marked as negative examples. Clearly, the accuracy of the resulting classifier depends on the quality of the automatically induced annotations. Our analysis of free-text annotations in several domains shows that automatically mapping from even manually-extracted annotation keyphrases to a document text is a difficult task, due to variability in keyphrase surface realizations (see Section 3). As we argue in the rest of this paper, it is beneficial to explicitly address the difficulties inherent in free-text annotations. To this end, our work is distinguished in two significant ways from the property extraction methods described above. First, we are able to predict properties beyond those that appear verbatim in the text. Second, our approach also learns the semantic relationships between different keyphrases, allowing us to draw direct comparisons between reviews even when the semantic ideas are expressed using different surface forms. Working in the related domain of web opinion mining, Lu and Zhai (2008) describe a system that generates integrated opinion summaries, which incorporate expert-written articles (e.", "startOffset": 53, "endOffset": 2013}, {"referenceID": 35, "context": "The work closest in methodology to our approach is a review summarizer developed by Titov and McDonald (2008a). Their method summarizes a review by selecting a list of phrases that express writers\u2019 opinions in a set of predefined properties (e.", "startOffset": 84, "endOffset": 111}, {"referenceID": 0, "context": "In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999; Radev, Jing, & Budzikowska, 2000; Nenkova, Vanderwende, & McKeown, 2006).", "startOffset": 119, "endOffset": 215}, {"referenceID": 0, "context": "Alternatively, some methods compare sentences via alignment of their syntactic trees (Barzilay et al., 1999; Marsi & Krahmer, 2005).", "startOffset": 85, "endOffset": 131}, {"referenceID": 0, "context": "In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999; Radev, Jing, & Budzikowska, 2000; Nenkova, Vanderwende, & McKeown, 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters. Identification of repeated information is equally central in our approach \u2014 our multi-document summarization method only selects properties that are stated by a plurality of users, thereby eliminating rare and/or erroneous opinions. The key difference between our algorithm and existing summarization systems is the method for identifying repeated expressions of a single semantic property. Since most of the existing work on multi-document summarization focuses on topic-independent newspaper articles, redundancy is identified via sentence comparison. For instance, Radev et al. (2000) compare sentences using cosine similarity between corresponding word vectors.", "startOffset": 120, "endOffset": 936}, {"referenceID": 10, "context": "78 on this joint set, indicating high agreement (Cohen, 1960).", "startOffset": 48, "endOffset": 61}, {"referenceID": 34, "context": "The generation of numerical ratings is based on the algorithm described in Snyder and Barzilay (2007).", "startOffset": 75, "endOffset": 102}, {"referenceID": 39, "context": "Approximate randomization (Yeh, 2000; Noreen, 1989) is used for statistical significance testing.", "startOffset": 26, "endOffset": 51}, {"referenceID": 26, "context": "Approximate randomization (Yeh, 2000; Noreen, 1989) is used for statistical significance testing.", "startOffset": 26, "endOffset": 51}, {"referenceID": 8, "context": "Previous work that used this test include evaluations at the Message Understanding Conference (Chinchor, Lewis, & Hirschman, 1993; Chinchor, 1995); more recently, Riezler and Maxwell (2005) advocated for its use in evaluating machine translation systems.", "startOffset": 94, "endOffset": 146}, {"referenceID": 8, "context": "Previous work that used this test include evaluations at the Message Understanding Conference (Chinchor, Lewis, & Hirschman, 1993; Chinchor, 1995); more recently, Riezler and Maxwell (2005) advocated for its use in evaluating machine translation systems.", "startOffset": 95, "endOffset": 190}, {"referenceID": 17, "context": "We use support vector machines, built using SVMlight (Joachims, 1999) with the same features as our model, i.", "startOffset": 53, "endOffset": 69}, {"referenceID": 24, "context": "Discriminative training is often considered to be more powerful than equivalent generative approaches (McCallum et al., 2005), leading us to expect a high level of performance from this system.", "startOffset": 102, "endOffset": 125}, {"referenceID": 31, "context": "For this purpose we use the Rand Index (Rand, 1971), a measure of cluster similarity.", "startOffset": 39, "endOffset": 51}, {"referenceID": 3, "context": "This suggests possible connections to the correlated topic model of Blei and Lafferty (2006).", "startOffset": 68, "endOffset": 93}, {"referenceID": 6, "context": "Portions of this work were previously presented in a conference publication (Branavan et al., 2008).", "startOffset": 76, "endOffset": 99}], "year": 2009, "abstractText": "This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations. Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as \u201ca real bargain\u201d or \u201cgood value.\u201d These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with joint inference. We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.", "creator": "TeX"}}}