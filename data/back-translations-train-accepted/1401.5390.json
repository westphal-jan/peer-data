{"id": "1401.5390", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "abstract": "Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.", "histories": [["v1", "Sat, 18 Jan 2014 21:10:57 GMT  (1694kb)", "http://arxiv.org/abs/1401.5390v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["s r k branavan", "david silver", "regina barzilay"], "accepted": true, "id": "1401.5390"}, "pdf": {"name": "1401.5390.pdf", "metadata": {"source": "CRF", "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "authors": ["S.R.K. Branavan", "David Silver", "Regina Barzilay"], "emails": ["branavan@csail.mit.edu", "d.silver@cs.ucl.ac.uk", "regina@csail.mit.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, we will be able to move to another world, where we are able to change the world, where we are able to change the world, where we are able to change the world, where we are able to change the world we are in."}, {"heading": "1.1 Summary of the Approach", "text": "We address the above challenges within a unified framework based on Markov decision-making processes (MDP), a formulation commonly used for game algorithms. This setup consists of a game in a stochastic environment in which the goal of the player is to maximize a predetermined usage function R (s) at the states s. Player behavior is determined by an action-value function Q (s, a), which evaluates the quality of the action a at the states s, based on the attributes of s and a.In order to include linguistic information in the MDP formulation, we extend the action-value function to include linguistic characteristics. While state and action characteristics are known at each point of the calculation, relevant words and their semantic roles are not considered. Therefore, we model text relevance as a hidden value in the MDP formulation. Likewise, we use hidden variables to use the words that describe the action, the rest of the action, to describe the rest of the function, to differentiate the rest of the state characteristics in our MDP formulation."}, {"heading": "1.2 Evaluation", "text": "We are testing our method on the strategy game Civilization II, a notoriously challenging game with an immense scope for action. 2 As a source of knowledge for our model, we are using the official game manual. As a starting point, we are using a similar player from Monte-Carlo who does not have access to text information. We are showing that the linguistically informed player significantly outperforms the baseline in terms of the number of games won. Furthermore, we are showing that modeling the deeper linguistic structure of sentences further improves performance. In full-length games, our algorithm achieves 34% -2. Civilization II was ranked third in the IGN list of the best video games of all time in 2007. (http: / / top100.ign.com / 2007 / ign top game 3.html) Beyond a linguistically unconscious baseline, our algorithm wins over 65% of games against the built-in, handmade AI of Civilization II. A video of our method playing the game is available at http: / / game 3.ign / top."}, {"heading": "1.3 Roadmap", "text": "In Section 2, we provide intuition about the benefits of integrating textual information into learning algorithms for controlling. Section 3 describes previous work on language learning and highlights the unique challenges and opportunities of our setup. This section also positions our work in a large research corpus on Monte-Carlo players. Section 4 presents background information on Monte-Carlo search in its application to the game. In Section 5, we present a multi-layered formulation of neural networks for the action value function that combines information from the text and control application. Next, we present a Monte-Carlo method for assessing the parameters of this non-linear function. In Sections 6 and 7, we focus on applying our algorithm to the game Civilization II. In Section 8, we compare our method with a number of competitive game bases and empirically analyze the properties of the algorithm. Finally, in Section 9, we discuss the implications of this research and conclude."}, {"heading": "2. Learning Game Play from Text", "text": "This year, it will be able to purify the aforementioned cerebral consecrated cerebral consecrated cerebral teaseSe."}, {"heading": "3. Related Work", "text": "In this section, we will first look at previous work in the field of grounded language acquisition. We will then look at two areas specific to our field of application - the analysis of natural language in the context of games and the Monte Carlo Search applied to playing games."}, {"heading": "3.1 Grounded Language Acquisition", "text": "Our work fits into the broad field of research on the acquisition of grounded language, where the goal is to learn linguistic analysis from a non-linguistic context (Oates, 2001; Barnard & Forsyth, 2001; Siskind, 2001; Roy & Pentland, 2002; Yu & Ballard, 2004; Chen & Mooney, 2008; Zettlemoyer & Collins, 2009; Liang, Jordan, & Klein, 2009; Branavan, Chen, Zettlemoyer, & Barzilay, 2009; Branavan, Zettlemoyer, & Barzilay, 2010; Vogel & Jurafsky, 2010; Clarke, Goldwasser, Chang, & Roth, 2010; Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, & Roy, 2011; Chen & Mooney, 2011; Liang, Jordan, & Klein, 2011; Goldwasser, Reichart, & Roth, 2011)."}, {"heading": "3.1.1 Learning Grounding from Parallel Data", "text": "In many applications, linguistic content is closely linked to perceptual observations that provide a rich source of information for grounding language. Examples of such parallel data include images with captions (Barnard & Forsyth, 2001), robocup gaming events paired with text commentary (Chen & Mooney, 2008), and sequences of robotic actions in natural language (Tellex et al., 2011). The great diversity in the properties of such parallel data has led to the development of algorithms tailored to specific grounding contexts rather than application-independent grounding approaches. Nevertheless, existing grounding approaches can be characterized along multiple dimensions that illuminate the connection between these algorithms."}, {"heading": "3.1.2 Learning Grounding from Control Feedback", "text": "Recent work has moved away from dependence on parallel corpora and uses control feedback as the primary source of monitoring. The assumption behind this setup is that when textual information is used to control a control application, the performance of the application correlates with the quality of speech analysis. It is also assumed that performance measurement can be achieved automatically. This setup is conducive to learning approaches that can estimate model parameters from the feedback signal, even if it is loud and delayed. A line of previous work has focused on the task of translating textual instructions into an executable GUI database, provided that text fully specifies all actions to be performed in the environment. In our previous work (Branavan et al., 2009, 2010), this approach was applied to the task of translating instructions from a computer manual into an executable GUI database, with the interaction between individual lines incorrect. Vogel and Jurafsky (2010) show that this framework can form the appropriate directions in a land map."}, {"heading": "3.2 Language Analysis and Games", "text": "Although games can provide a rich domain for situated text analysis, there have been few previous attempts to take advantage of this opportunity (Gorniak & Roy, 2005; Eisenstein, Clarke, Goldwasser, & Roth, 2009).Eisenstein et al. (2009) aim to automatically extract information from a collection of documents in order to identify the rules of a game. This information, presented as predicate formulas, is appreciated in an unsupervised manner by a generative model.The extracted formulas, along with observed traces of the game, are then fed into an inductive logic program that attempts to reconstruct the rules of the game."}, {"heading": "3.3 Monte-Carlo Search for Game AI", "text": "Monte-Carlo Search (MCS) is a state-of-the-art framework that has been successfully applied to complex games such as Go, Poker, Scrabble and Real-Time Strategy Games (Gelly, Wang, Munos, & Teytaud, 2006; Tesauro & Galperin, 1996; Billings, Castillo, Schaeffer, & Szafron, 1999; Sheppard, 2002; Schaser, 2008; Sturtevant, 2008; Balla & Fern, 2009) using simulation games to estimate the quality or value of different candidate actions."}, {"heading": "3.3.1 Leveraging Domain Knowledge", "text": "In previous work, this has been achieved by manually encoding relevant domain knowledge into the game algorithm - for example, through manually specified heuristics for action selection (Billings et al., 1999; Gelly et al., 2006), craft characteristics (Tesauro & Galperin, 1996), and value functions for encoding expert knowledge (Sturtevant, 2008). In contrast to such approaches, our goal is to automatically extract and use domain knowledge from relevant documents of natural language, bypassing the need for manual specification. Our method learns both text interpretation and game action selection based on the results of simulated games in MCS, which allows to identify and use textual domain knowledge relevant to the observed game context."}, {"heading": "3.3.2 Estimating the Value of Untried Actions", "text": "Previous approaches to estimating the value of untested actions relied on two techniques: The first, Upper Confidence bounds for Tree (UCT), is a heuristic used in conjunction with the Monte Carlo Tree Search variant of MCS. It increases the value of an action by an exploration bonus for rarely visited state shareholder pairs, resulting in better action selection and better overall performance of the game (Gelly et al., 2006; Sturtevant, 2008; Balla & Fern, 2009); and the second technique is to learn a linear functional approximation of the action values for the current state based on game feedback (Tesauro & Galperin, 1996; Silver, Sutton, & Mu \u00bc ller, 2008). Although our method follows the latter approach, we model the action value Q (s, a) using a nonlinear functional approximation for the current state. Given the complexity of our application domain, this non-linear approach does not produce results that are more meaningful than our approximation to the actual state."}, {"heading": "4. Monte-Carlo Search", "text": "Our task is to use textual information to win a turn-based strategy game against a particular opponent. In this section, we first describe the framework of the Monte Carlo search within which our method works. Details of our linguistically informed Monte Carlo search algorithm are given in Section 5."}, {"heading": "4.1 Game Representation", "text": "Formally, we represent the given turn-based stochastic game as the Markov Decision Process (MDP). This MDP is defined by the quadruple reward < S, A, T, R >, where \u2022 State Space, S, is the set of all possible states. Each state s, S represents a complete configuration of the game between moves. \u2022 Action Space A, is the totality of all possible actions. In a turn-based strategy game, a player controls several game units on each turn. Thus, each action a, which represents a common mapping of all units executed by the current player during the turn. \u2022 Transition Distribution, T (s, a), is the likelihood that the execution of an action in a state s, in the next round of play, will result in state s. This distribution encodes the way in which the game state changes, both the rules of play and the actions of the opposing player."}, {"heading": "4.2 Monte-Carlo Framework for Computer Games", "text": "The Monte Carlo search algorithm shown in Figure 3 is a simulation-based search paradigm for the dynamic estimation of the action values Q\u03c0 (s, a) for a certain state st (see algorithm 1 for pseudo-code), based on the rewards observed during multiple roll-outs, each of which is a simulated game starting from a certain state st. (s, a) Specifically, in each roll-out, the algorithm starts at a state st, and actions are selected and executed repeatedly according to a simulation policy. (s, a), sampling state transitions from T (s, a), action outs of the game completion at time, the final reward R (s) is measured, and the action value function is updated accordingly. 4 As in Monte Carlo control (Sutton & Barto, 1998), the updated action value3. Monte Carlo search assumes that it is possible to play simulated games."}, {"heading": "5. Adding Linguistic Knowledge to the Monte-Carlo Framework", "text": "The aim of our work is to improve the performance of the Monte Carlo Search framework described above by automatically using information extracted from text. In this section, we describe how we achieve this in terms of model structure and parameter estimation."}, {"heading": "5.1 Model Structure", "text": "In order to achieve our goal of using textual information to improve game play, our method must fulfill three tasks: (1) identify sentences that are relevant to the current game status, (2) label sentences with a predicate structure, and (3) predict good game actions by combining game functions with text functions extracted via the language analysis steps. We first describe how each of these tasks can be modeled separately, before showing how to integrate them into a single coherent model.Procedure PlayGame () Start the game status into a fixed initial state s1 \u2190 s0for t = 1... T doRun N simulated game actions for i = 1... N do (su, su) \u2190 SimulateGame (st) endCompute averagely observed utility program for each action: arg max a1Na. i: ai = a riExecute selected action in gamest + 1 \u2190 T (s) endprocedure SimulateGame Simulatefor Game (st procedure) (st) (st)"}, {"heading": "5.1.1 Modeling Sentence Relevance", "text": "As discussed in Section 1, only a small fraction of a strategy document is likely to provide guidance for the current game context. To effectively use information from a given document d, we first need to identify the sentence yi that is most relevant to the current game state and story.6 We model this decision as a loglinear distribution and define the probability that yi is the relevant sentence as follows: p (y = yi | s, a, d).e ~ u \u00b7??? (yi, s, a, d). (1) Here is ~? (yi, s, a, d).Rn a feature function, and ~ u are the parameters we need to appreciate. ~?? (\u00b7) encodes features that combine the attributes of the set yi with the attributes of the game state and plot. These features allow the model to learn correlations between game attributes and the attributes of relevant sentences."}, {"heading": "5.1.2 Modeling Predicate Structure", "text": "When we use text to guide the action selection, in addition to the use of word correspondences, we also want to use information encoded in the structure of the sentence. Verbs in a sentence, for example, could be more likely to describe proposed actions. We aim to access this information by triggering a task-related predicate structure on the sentences, that is, we label the words of a sentence either as action description, state description or background. In view of the sentence y and its predicated dependence parse q, we model the word-by-word markup decision in a loglinear manner - i.e. the distribution over the predicate z of the sentence y is given by: p (z | y, q) = p (~ e | y, q) = \u0394j p (ej | j, y, q), (2) p (ej | j, y, q), (2) p (ej | j, y, q)."}, {"heading": "5.1.3 Modeling the Action-Value Function", "text": "Once the relevant sentence has been identified and labeled with a predicate structure, our algorithm must use this information, along with the attributes of the current state of the game, to select the best possible game action a. To this end, we redefine the action value function Q (s, a) as a weighted linear combination of features of the game and text information: Q (s, \"a\") = ~ w \u00b7 ~ f (s, a, yi, zi). (3) 6. We use the approximation of the selection of the single most relevant sentence as an alternative to combining the features of all sentences in the text, weighted by their likelihood of relevance p (y = yi | s, a, d)."}, {"heading": "5.1.4 Complete Joint Model", "text": "The two text models, and the action value function described above, form the three primary components of our text-based game. We construct a single principal model from these components and represent it in various layers of the multi-layered neural network. Essentially, the text analyses are identified as latent variables from the second, hidden layer of the network, while the final initial layer models the action function."}, {"heading": "5.2 Parameter Estimation", "text": "As shown in Figure 3, these three steps are repeated a fixed number of times in each current game state, and the information from these times is then used to select the actual game action. The algorithm redefines all the parameters of the Action Value function for each new game state, specializing the Action Value function in the partial game that comes from st. Learning a specialized Q (st, at) for each game state is common and useful in games with complex state spaces and dynamics, where learning a single global function can be particularly difficult to approach to the respective function (Sutton et al., 2007). One consequence of this functional specialization is the need for online learning - as we cannot predict which games will be played."}, {"heading": "6. Applying the Model", "text": "The game on which we are testing our model, Civilization II, is a multi-player strategy game that is played either on Earth or on a randomly generated world. Each player acts as the ruler of a civilization and begins with a few game units - i.e., two settlers, two workers and one explorer. The goal is to expand their civilization by developing new technologies, building cities and new units, and win the game by either controlling the entire world or successfully sending a spaceship to another world. The game world is divided into a grid of typically 4000 squares, with each grid location representing a tile of land or sea. Figure 6 shows a portion of this world map from a particular instance of the game, along with a player's game units. In our experiments, we are looking at a game for two players of Civilization II on a map of 1000 squares - the smallest map allowed on Freeciv. This map size is used by beginners looking for a more advanced level of aggression, as well as by those using a shorter level of difficulty."}, {"heading": "6.1 Game States and Actions", "text": "We define the game state for the Monte Carlo search to be the map of the game world, along with the attributes of each tile and the location and attributes of each player's cities and units. Some examples of these attributes are in Figure 7. The space of possible actions for each city and unit is defined by the game rules underlying the current game state. Cities, for example, can build buildings such as ports and banks or create new units of different types; while individual units move on the grid and can perform unit-specific actions such as irrigation for settlers and military defense for archers. As a player controls several cities and units, the player's action space on each turn is defined by combining all possible actions for those cities and units. In our experiments, a player controls an average of about 18 units, with each unit having 15 possible actions. The resulting action space for a player is very large - i.e. the player's action space on each turn is defined by combining all possible actions for those cities and units."}, {"heading": "6.2 Utility Function", "text": "Crucial to the Monte Carlo search algorithm is the availability of a utility function that can evaluate the results of simulated game transfers. In the typical application of the algorithm, the end result in the form of win or loss is used as a utility function (Tesauro & Galperin, 1996). Unfortunately, the complexity of Civilization II and the length of a typical game preclude the possibility of performing simulation transfers until the end of the game. However, the game provides each player with a real score of the game, which is a loud indicator of the strength of their civilization. As we play a game with two players, the score of our player relative to the opponent can be used as a utility function. Specifically, we use the ratio of the scores of the two players. [11]"}, {"heading": "6.3 Features", "text": "All components of our method work on features calculated using a basic set of text and game attributes. ~ The text properties include the words of each sentence along with their speech and dependency parts. To determine the sentence most relevant to the current state and candidate action, the sentence relevance component calculates features available to human players through the graphical user interface of the game. Some examples of these features are shown in Figure 7.The first calculates the Cartesian product between the game state attributes and the candidate set attributes. The second type consists of binary features that overlap between the words from the candidate set and each sentence from the text, and the text properties of the current game state and candidate action are of two types - the first type calculates the cartesian product between the attributes of the game and the candidate set attributes. The second type consists of text features that overlap between the current game state and candidate action."}, {"heading": "7. Experimental Setup", "text": "In this section, we describe the data sets, evaluation metrics, and experimental framework used to test the performance of our method and the various baselines."}, {"heading": "7.1 Datasets", "text": "The text of this manual uses a vocabulary of 3638 word types and consists of 2083 sentences, each averaging 16.9 words.This manual contains information about the game rules, the game interface, and basic strategy tips on various aspects of the game.We use the Stanford parser (de Marneffe, MacCartney, & Manning, 2006) to generate the dependency information for sentences in the manual under the default settings."}, {"heading": "7.2 Experimental Framework", "text": "In order to apply our method to Civilization II games, we use the open source reimplementation Freeciv.13 We got FreeCiv to program our methodology. Remove www.civfanatics.com / content / civ2 / reference / Civ2manual.zip, control the game, i.e. measure the current game status, perform game actions, save and stop the current game status. 14Across all the experiments we start and perform in the same place."}, {"heading": "7.3 Evaluation Metrics", "text": "We would like to evaluate two aspects of our method: how well it improves the game by using text information, and how accurately it analyzes text by learning from game feedback. We evaluate the first aspect by comparing our method in terms of the percentage of games won with Freeciv's built-in AI. This AI is a fixed heuristic algorithm developed with extensive knowledge of the game, with the intention of challenging human players. [15] Therefore, it provides a good open reference base. We evaluate our method by measuring the percentage of games won, averaged over 100 independent runs. However, complete games can sometimes take several days, making it difficult to perform a comprehensive analysis of model performance and contributing factors. Therefore, our primary rating measures the percentage of games won within the first 100 game steps, averaging over 200 independent runs. This rating is an underestimation of model performance - any game in which the player did not win by considering the control over the entire game card to be equaled by the number of games won within 100 runs, as if each of these two runs is averaged."}, {"heading": "8. Results", "text": "In order to adequately characterize the performance of our method, we evaluate it taking into account various aspects. In this section, we first describe its performance and analyze the effects of textual information. We then examine the quality of the text analysis that our model produces in terms of both sentence relevance and predicate naming."}, {"heading": "8.1 Game Performance", "text": "In this scenario, our language-aware Monte Carlo algorithm wins an average of 53.7% of games, significantly outperforming all baselines, while the best non-language-aware method has a win rate of only 26.1%. The dismal performance of the random baseline and the game's built-in AI, which plays against itself, are indicative of the difficulty of winning games within the first 100 steps. As shown in Table 2, our method has a win rate of 65.4% on a full-game rating, compared to 31.5% for the best text-ignorant baseline.1615 While this AI is limited to following the rules of the game, it has access to information that is not normally available to human players, such as information about the technology, cities and units of their opponents. Our methods, on the other hand, are limited to the actions and information available to human players. 16. Note that the performance of all methods is not available on the sporadic game as listed in previous games."}, {"heading": "8.1.1 Textual Advice and Game Performance", "text": "The simplest of these methods, Game only, models the action value function Q (s, a) as a linear approach to the state and action attributes of the game. This non-text-conscious method wins only 17.3% of games (see Table 1). To confirm that the improved performance of our method is not simply due to its inherently richer nonlinear approach, we also evaluate two ablative non-linear baselines. The first of these latent variables expands the linear action value function of the game only with a set of latent variables. It is essentially a four-layer neural network, similar to our complete model, in which the units of the second layer are activated only on the basis of game information. This baseline wins 26.1% of games (Table 1), which significantly improves over the linear game base, but still lags behind our text."}, {"heading": "8.1.2 Impact of Seed Vocabulary on Performance", "text": "The Sentence Relevance component of our model uses features that calculate the similarity between words in a sentence and the text names of the game state and plot. This requires the availability of a seed vocabulary that names the game attributes. In our section, of the 256 unique text names in the game manual 135 are found in the vocabulary, resulting in a sparse seed vocabulary of 135 words that covers only 3.7% of the word types and 3.2% of the word marks in the manual. Despite this scarcity, the seed vocabulary can potentially have a big impact on the performance of the model as it provides a first set of word bases. To assess the importance of this initial generation, we test our method with a blank seed vocabulary. In this setup, our complete model wins 49.0% of games, showing that the seed words are important, but our method can also function effectively in their absence."}, {"heading": "8.1.3 Linguistic Representation and Game Performance", "text": "To characterize the contribution of language to the performance of the game, we perform a series of evaluations that vary the type and complexity of the linguistic analysis performed by our methodology, the results of which are shown in Table 3. The first of these evaluations, Sentence Relevance, highlights the contributions of the two language components of our model. This algorithm, which is identical to our complete model but lacks the predicate marking component, wins 46.7% of games and shows that while it is essential to identify the textual advice relevant to the current state of the game, a deeper syntactical analysis of the extracted text significantly improves performance.To assess the importance of predicate marking in our language analysis, we vary the type of features available to the predicate marking component of our model."}, {"heading": "8.1.4 Model Complexity vs Computation Time Trade-off", "text": "An inherent disadvantage of non-linear models compared to simpler linear models is that they increase the calculation time required for parameter estimation. In our Monte Carlo Search Setup, the model parameters are reestimated after each simulated roll-out. Therefore, more roll-outs can be made for a simpler and faster model over a set period of time. Naturally, this scenario is particularly relevant in games where players have a limited time available for each round. To explore this trade-off, we vary the number of simulation roll-outs allowed for each method in each game step by recording the win rate and average calculation time per game. Figure 12 shows the results of this assessment for 100, 200 and 500 roll-outs."}, {"heading": "8.1.5 Learned Game Strategy", "text": "Essentially, they are trying to develop basic technologies, build an army, and capture enemy cities as quickly as possible, and the difference in performance between the different models is largely due to how well they learn this strategy. There are two fundamental reasons why our algorithms learn the Quick Strategy: First, because we try to maximize the game score, the methods implicitly tend to find the quickest route to victory - which happens to be the Quick Strategy when playing against Civilization 2 \"s built-in AI. Second, more complex strategies typically require multi-unit coordination. Since our models assume that game units are independent, they cannot explicitly learn such coordination - pushing many complex strategies beyond the capabilities of our algorithms."}, {"heading": "8.2 Accuracy of Linguistic Analysis", "text": "As described in Section 5, text analysis in our method is closely linked to playing - both in terms of modelling and learning from game feedback. We have seen from the results so far that this text analysis actually helps the game. In this section, we focus on game-driven text analysis itself and examine to what extent it conforms to more common notions of linguistic correctness. We do this by comparing model predictions of sentence relevance and predicate markup with manual annotations."}, {"heading": "8.2.1 Sentence Relevance", "text": "Figure 13 shows examples of the set of relevance decisions produced by our method. Ideally, to assess the accuracy of these decisions, we would use a bottom-truth-relevance remark in the user's manual. However, this is impractical, since the relevance decision depends on the game context and is therefore specific to each and every step of the game instance. Therefore, we evaluate the relevance of the sentence using a synthetic document. We create this document by combining the original game manual with an equal number of sentences that are irrelevant to the game. These sentences are collected by random sampling from the Wall Street Journal Corpus (Marcus, Santorini, & Marcinkiewicz)."}, {"heading": "8.2.2 Predicate Labeling", "text": "Figure 13 shows examples of the results of the predicate structure of our model. We evaluate the accuracy of this label by comparing it to a gold standard annotation in the Game Manual. 19 Table 4 shows the performance of our method in terms of how accurately it identifies words as state, action, or background, and also how accurately it distinguishes between state and action words. In addition to improving performance over the random baseline, these results show a clear trend: In both evaluations, the label is higher in the early stages of the game. This is to be expected since the model is heavily based on textual characteristics at the beginning of the game (see Figure 15). To verify the usefulness of our method's predicate labeling, we are conducting a final series of experiments in which predicate labels are consistently randomly selected within our complete model. This random labeling of words results in a 44% win rate - a performance similar to the predicate label model that does not use predicate information."}, {"heading": "9. Conclusions", "text": "In this paper, we have presented a novel approach to improving the performance of control applications by using information that is automatically extracted from text documents while learning language analysis based on control feedback. To effectively learn this grounding, the model distorts the strategy learned by enriching the political function with text functions, thereby modelling the mapping between words in a manual and state-specific action selection. To effectively learn this grounding, the model identifies text that is relevant to the current game state and induces a predicate structure on that text. These linguistic decisions are jointly modelled using a non-linear political function trained in the Monte Carlo search framework. Empirical results show that our model is capable of significantly improving the win rate of games by using text information compared to strong language agnostic baselines. We also show that despite the increased complexity of our model, the knowledge it acquires is reduced by good performance even when the number of simulations is enabled."}, {"heading": "Acknowledgments", "text": "The authors thank the NSF (CAREER grant IIS-0448168, grant IIS0835652), the DARPA BOLT Program (HR0011-11-2-0008), the DARPA Machine Reading Program (FA8750-09-C-0172, PO # 4910018860), Batelle (PO # 300662), and the Microsoft Research New Faculty Fellowship. Thanks to the anonymous reviewers Michael Collins, Tommi Jaakkola, Leslie Kaelbling, Nate Kushman, Sasha Rush, Luke Zettlemoyer, and the MIT NLP Group for their suggestions and comments. All opinions, results, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding organizations."}, {"heading": "Appendix A. Parameter Estimation", "text": "To derive the parameter updates, consider the slightly simplified neural network shown below. This network is identical to our model, but for clarity's sake it only has a single second layer ~ y instead of the two parallel layers ~ y and ~ z. The parameter updates for these parallel layers ~ y and ~ z are similar, so we will only show the derivative for ~ y in addition to the updates for the final layer. As in our model, the nodes yi in the network above are activated via a softmax function; the third layer, ~ f, is calculated deterrently from the active nodes of the second layer via the function ~ g (yi, ~ x); and the output Q is a linear combination of ~ f, weighted to ~ w: p (yi = 1 | ~ ui) = e ~ ui ~ ~ ~ ~ x ~ k ~ i ~ k ~."}, {"heading": "Appendix B. Example of Sentence Relevance Predictions", "text": "The following is a part of the Civilization II Strategic Guide. Sentences identified as relevant by our text-conscious model are highlighted in green color. When a new city is built, it is carefully planned where to place it. Citizens can work the terrain around the city square in an x-shaped pattern (see city radius for a diagram showing the exact dimensions), an area known as the city radius (the terrain on which the settlers stand becomes the city square). Available natural resources affect their ability to produce food and goods. Cities built on or near water sources can increase their crop yields, and cities that are close to mineral deposits become raw materials. On the other hand, cities that are surrounded by drought are hampered by the dryness of their terrain."}, {"heading": "Appendix C. Examples of Predicate Labeling Predictions", "text": "Below are the predicate names calculated by our text-conscious method using sample sentences from the game manual. Predicted labels are given under the letters A, S and B for action description, state description and background. Wrong labels are marked with a red check mark, along with the correct lettering in brackets. When the settlers become active, select the road construction. A AAS SSAfter the road is built, use the settlers to start improving the terrain. AA AA AA AS AS SSA A AA Use settlers or engineers to improve a place within the city radius. SSSS (A) (S) AB Bronze allows you to build phalanx units. S AAS (S) To expand your civilization, you must build other cities. AS B (A) AS B (A) B (B) B) to explore your city. (S) To protect the city, the city must remain within the Phalanx."}, {"heading": "Appendix D. Examples of Learned Text to Game Attribute Mappings", "text": "Below are examples of some of the word associations to game attributes that our model learned. The ten playful attributes with the strongest association by property weight are listed in three of the example words - \"Attack,\" \"Build,\" and \"Grassland.\" In the fourth word, \"Settler,\" only seven attributes in experiments used to collect these statistics had an unequal weighting as zero. Phalanx (Unit) Warrior (Unit) Colossus (Miracle) City Walls (City Improvement) Archer (Unit) Catapult (Unit) Palace (Urban Improvement) Coinage (Urban Production) city _ build _ warriors (Action) city _ build _ phalanx (Action) worker _ goto (Action) worker _ autosettle (Action) worker _ autosettle (Action) Pheasant (Terrain Attribut) settler (Action) Settler (Terrain Attribut) _ Settler _ Settle (Action) City _ Build (Action) City _ Build (Action) City _ City (Action) (Action _ Build (Action) Settler (Action) (Action)."}, {"heading": "Appendix E. Features Used by the Model", "text": "In fact, most of them are able to navigate the city."}], "references": [{"title": "UCT for tactical assault planning in real-time strategy games", "author": ["R. Balla", "A. Fern"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Balla and Fern,? \\Q2009\\E", "shortCiteRegEx": "Balla and Fern", "year": 2009}, {"title": "Learning the semantics of words and pictures", "author": ["K. Barnard", "D.A. Forsyth"], "venue": "In Proceedings of ICCV,", "citeRegEx": "Barnard and Forsyth,? \\Q2001\\E", "shortCiteRegEx": "Barnard and Forsyth", "year": 2001}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["A.G. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Barto and Mahadevan,? \\Q2003\\E", "shortCiteRegEx": "Barto and Mahadevan", "year": 2003}, {"title": "Using probabilistic knowledge and simulation to play poker", "author": ["D. Billings", "L.P. Castillo", "J. Schaeffer", "D. Szafron"], "venue": "In Proceedings of AAAI/IAAI,", "citeRegEx": "Billings et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Billings et al\\.", "year": 1999}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S. Branavan", "H. Chen", "L. Zettlemoyer", "R. Barzilay"], "venue": "In Proceedings of ACL,", "citeRegEx": "Branavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Learning to win by reading manuals in a monte-carlo framework", "author": ["S. Branavan", "D. Silver", "R. Barzilay"], "venue": "In Proceedings of ACL,", "citeRegEx": "Branavan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2011}, {"title": "Non-linear monte-carlo search in civilization II", "author": ["S. Branavan", "D. Silver", "R. Barzilay"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Branavan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2011}, {"title": "Reading between the lines: Learning to map high-level instructions to commands", "author": ["S. Branavan", "L. Zettlemoyer", "R. Barzilay"], "venue": "In Proceedings of ACL,", "citeRegEx": "Branavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2010}, {"title": "Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters", "author": ["J.S. Bridle"], "venue": "In Advances in NIPS,", "citeRegEx": "Bridle,? \\Q1990\\E", "shortCiteRegEx": "Bridle", "year": 1990}, {"title": "Applied optimal control: optimization, estimation, and control", "author": ["A.E. Bryson", "Ho", "Y.-C"], "venue": null, "citeRegEx": "Bryson et al\\.,? \\Q1969\\E", "shortCiteRegEx": "Bryson et al\\.", "year": 1969}, {"title": "Learning to sportscast: a test of grounded language acquisition", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "In Proceedings of ICML,", "citeRegEx": "Chen and Mooney,? \\Q2008\\E", "shortCiteRegEx": "Chen and Mooney", "year": 2008}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Chen and Mooney,? \\Q2011\\E", "shortCiteRegEx": "Chen and Mooney", "year": 2011}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "Chang", "M.-W", "D. Roth"], "venue": "In Proceedings of CoNNL,", "citeRegEx": "Clarke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["de Marneffe", "M.-C", "B. MacCartney", "C.D. Manning"], "venue": "In Proceedings of LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Reading to learn: Constructing features from semantic abstracts", "author": ["J. Eisenstein", "J. Clarke", "D. Goldwasser", "D. Roth"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Eisenstein et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2009}, {"title": "Intentional context in situated natural language learning", "author": ["M. Fleischman", "D. Roy"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Fleischman and Roy,? \\Q2005\\E", "shortCiteRegEx": "Fleischman and Roy", "year": 2005}, {"title": "Modification of UCT with patterns in Monte-Carlo Go", "author": ["S. Gelly", "Y. Wang", "R. Munos", "O. Teytaud"], "venue": "Tech. rep. 6062,", "citeRegEx": "Gelly et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gelly et al\\.", "year": 2006}, {"title": "Confidence driven unsupervised semantic parsing", "author": ["D. Goldwasser", "R. Reichart", "J. Clarke", "D. Roth"], "venue": "In Proceedings of ACL,", "citeRegEx": "Goldwasser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwasser et al\\.", "year": 2011}, {"title": "Speaking with your sidekick: Understanding situated speech in computer role playing games", "author": ["P. Gorniak", "D. Roy"], "venue": "In Proceedings of AIIDE,", "citeRegEx": "Gorniak and Roy,? \\Q2005\\E", "shortCiteRegEx": "Gorniak and Roy", "year": 2005}, {"title": "Learning semantic correspondences with less supervision", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Proceedings of ACL,", "citeRegEx": "Liang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Proceedings of ACL,", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Grounding knowledge in sensors: Unsupervised learning for language and planning", "author": ["J.T. Oates"], "venue": "Ph.D. thesis,", "citeRegEx": "Oates,? \\Q2001\\E", "shortCiteRegEx": "Oates", "year": 2001}, {"title": "Learning words from sights and sounds: a computational model", "author": ["D.K. Roy", "A.P. Pentland"], "venue": "Cognitive Science", "citeRegEx": "Roy and Pentland,? \\Q2002\\E", "shortCiteRegEx": "Roy and Pentland", "year": 2002}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "The UCT algorithm applied to games with imperfect information", "author": ["J. Sch\u00e4fer"], "venue": "Diploma Thesis. Otto-von-Guericke-Universita\u0308t Magdeburg", "citeRegEx": "Sch\u00e4fer,? \\Q2008\\E", "shortCiteRegEx": "Sch\u00e4fer", "year": 2008}, {"title": "World-championship-caliber Scrabble", "author": ["B. Sheppard"], "venue": "Artificial Intelligence,", "citeRegEx": "Sheppard,? \\Q2002\\E", "shortCiteRegEx": "Sheppard", "year": 2002}, {"title": "Sample-based learning and search with permanent and transient memories", "author": ["D. Silver", "R. Sutton", "M. M\u00fcller"], "venue": "In Proceedings of ICML,", "citeRegEx": "Silver et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2008}, {"title": "Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic", "author": ["J.M. Siskind"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Siskind,? \\Q2001\\E", "shortCiteRegEx": "Siskind", "year": 2001}, {"title": "An analysis of UCT in multi-player games", "author": ["N. Sturtevant"], "venue": "In Proceedings of ICCG,", "citeRegEx": "Sturtevant,? \\Q2008\\E", "shortCiteRegEx": "Sturtevant", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "On the role of tracking in stationary environments", "author": ["R.S. Sutton", "A. Koop", "D. Silver"], "venue": "In Proceedings of ICML,", "citeRegEx": "Sutton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2007}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S. Teller", "N. Roy"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Tellex et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "On-line policy improvement using Monte-Carlo search", "author": ["G. Tesauro", "G. Galperin"], "venue": "In Advances in NIPS,", "citeRegEx": "Tesauro and Galperin,? \\Q1996\\E", "shortCiteRegEx": "Tesauro and Galperin", "year": 1996}, {"title": "Learning to follow navigational directions", "author": ["A. Vogel", "D. Jurafsky"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Vogel and Jurafsky,? \\Q2010\\E", "shortCiteRegEx": "Vogel and Jurafsky", "year": 2010}, {"title": "On the integration of grounding language and learning objects", "author": ["C. Yu", "D.H. Ballard"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Yu and Ballard,? \\Q2004\\E", "shortCiteRegEx": "Yu and Ballard", "year": 2004}, {"title": "Learning context-dependent mappings from sentences to logical form", "author": ["L. Zettlemoyer", "M. Collins"], "venue": "In Proceedings of ACL,", "citeRegEx": "Zettlemoyer and Collins,? \\Q2009\\E", "shortCiteRegEx": "Zettlemoyer and Collins", "year": 2009}], "referenceMentions": [{"referenceID": 22, "context": "Our work fits into the broad area of research on grounded language acquisition where the goal is to learn linguistic analysis from a non-linguistic situated context (Oates, 2001; Barnard & Forsyth, 2001; Siskind, 2001; Roy & Pentland, 2002; Yu & Ballard, 2004; Chen & Mooney, 2008; Zettlemoyer & Collins, 2009; Liang, Jordan, & Klein, 2009; Branavan, Chen, Zettlemoyer, & Barzilay, 2009; Branavan, Zettlemoyer, & Barzilay, 2010; Vogel & Jurafsky, 2010; Clarke, Goldwasser, Chang, & Roth, 2010; Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, & Roy, 2011; Chen & Mooney, 2011; Liang, Jordan, & Klein, 2011; Goldwasser, Reichart, Clarke, & Roth, 2011).", "startOffset": 165, "endOffset": 654}, {"referenceID": 28, "context": "Our work fits into the broad area of research on grounded language acquisition where the goal is to learn linguistic analysis from a non-linguistic situated context (Oates, 2001; Barnard & Forsyth, 2001; Siskind, 2001; Roy & Pentland, 2002; Yu & Ballard, 2004; Chen & Mooney, 2008; Zettlemoyer & Collins, 2009; Liang, Jordan, & Klein, 2009; Branavan, Chen, Zettlemoyer, & Barzilay, 2009; Branavan, Zettlemoyer, & Barzilay, 2010; Vogel & Jurafsky, 2010; Clarke, Goldwasser, Chang, & Roth, 2010; Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, & Roy, 2011; Chen & Mooney, 2011; Liang, Jordan, & Klein, 2011; Goldwasser, Reichart, Clarke, & Roth, 2011).", "startOffset": 165, "endOffset": 654}, {"referenceID": 32, "context": "scribed in natural language (Tellex et al., 2011).", "startOffset": 28, "endOffset": 49}, {"referenceID": 1, "context": "For instance, Barnard and Forsyth (2001) segment images into regions that are subsequently mapped to words.", "startOffset": 14, "endOffset": 41}, {"referenceID": 13, "context": "For instance, Fleischman and Roy (2005) parse action sequences using a context-free grammar which is subsequently mapped into semantic frames.", "startOffset": 14, "endOffset": 40}, {"referenceID": 10, "context": "Chen and Mooney (2008) represent action sequences using first order logic.", "startOffset": 0, "endOffset": 23}, {"referenceID": 32, "context": "More recent methods have relied on a richer representation of linguistic data, such as syntactic trees (Chen & Mooney, 2008) and semantic templates (Tellex et al., 2011).", "startOffset": 148, "endOffset": 169}, {"referenceID": 32, "context": "For this reason, some models assume that alignment is provided as part of the training data (Fleischman & Roy, 2005; Tellex et al., 2011).", "startOffset": 92, "endOffset": 137}, {"referenceID": 1, "context": "Examples of such approaches are the methods of Barnard and Forsyth (2001), and Liang et al.", "startOffset": 47, "endOffset": 74}, {"referenceID": 1, "context": "Examples of such approaches are the methods of Barnard and Forsyth (2001), and Liang et al. (2009). Both of these models jointly generate the text and attributes of the grounding context, treating alignment as an unobserved variable.", "startOffset": 47, "endOffset": 99}, {"referenceID": 12, "context": "A second line of prior work has focused on full semantic parsing \u2013 converting a given text into a formal meaning representation such as first order logic (Clarke et al., 2010).", "startOffset": 154, "endOffset": 175}, {"referenceID": 4, "context": "For example, in our previous work (Branavan et al., 2009, 2010), this approach was applied to the task of translating instructions from a computer manual to executable GUI actions. Vogel and Jurafsky (2010) demonstrate that this grounding framework can effectively map navigational directions to the corresponding path in a map.", "startOffset": 35, "endOffset": 207}, {"referenceID": 14, "context": "Eisenstein et al. (2009) aim to automatically extract information from a collection of documents to help identify the rules of a game.", "startOffset": 0, "endOffset": 25}, {"referenceID": 14, "context": "Eisenstein et al. (2009) aim to automatically extract information from a collection of documents to help identify the rules of a game. This information, represented as predicate logic formulae, is estimated in an unsupervised fashion via a generative model. The extracted formulae, along with observed traces of game play are subsequently fed to an Inductive Logic Program, which attempts to reconstruct the rules of the game. While at the high-level, our goal is similar, i.e., to extract information from text useful for an external task, there are several key differences. Firstly, while Eisenstein et al. (2009) analyze the text and the game as two disjoint steps, we model both tasks in an integrated fashion.", "startOffset": 0, "endOffset": 616}, {"referenceID": 18, "context": "Gorniak and Roy (2005) develop a machine controlled game character which responds to spoken natural language commands.", "startOffset": 0, "endOffset": 23}, {"referenceID": 18, "context": "Gorniak and Roy (2005) develop a machine controlled game character which responds to spoken natural language commands. Given traces of game actions manually annotated with transcribed speech, their method learns a structured representation of the text and aligned action sequences. This learned model is then used to interpret spoken instructions by grounding them in the actions of a human player and the current game state. While the method itself does not learn to play the game, it enables human control of an additional game character via speech. In contrast to Gorniak and Roy (2005), we aim to develop algorithms to fully and autonomously control all actions of one player in the game.", "startOffset": 0, "endOffset": 590}, {"referenceID": 26, "context": "Monte-Carlo Search (MCS) is a state-of-the-art framework that has been very successfully applied, in prior work, to playing complex games such as Go, Poker, Scrabble, and real-time strategy games (Gelly, Wang, Munos, & Teytaud, 2006; Tesauro & Galperin, 1996; Billings, Castillo, Schaeffer, & Szafron, 1999; Sheppard, 2002; Sch\u00e4fer, 2008; Sturtevant, 2008; Balla & Fern, 2009).", "startOffset": 196, "endOffset": 376}, {"referenceID": 25, "context": "Monte-Carlo Search (MCS) is a state-of-the-art framework that has been very successfully applied, in prior work, to playing complex games such as Go, Poker, Scrabble, and real-time strategy games (Gelly, Wang, Munos, & Teytaud, 2006; Tesauro & Galperin, 1996; Billings, Castillo, Schaeffer, & Szafron, 1999; Sheppard, 2002; Sch\u00e4fer, 2008; Sturtevant, 2008; Balla & Fern, 2009).", "startOffset": 196, "endOffset": 376}, {"referenceID": 29, "context": "Monte-Carlo Search (MCS) is a state-of-the-art framework that has been very successfully applied, in prior work, to playing complex games such as Go, Poker, Scrabble, and real-time strategy games (Gelly, Wang, Munos, & Teytaud, 2006; Tesauro & Galperin, 1996; Billings, Castillo, Schaeffer, & Szafron, 1999; Sheppard, 2002; Sch\u00e4fer, 2008; Sturtevant, 2008; Balla & Fern, 2009).", "startOffset": 196, "endOffset": 376}, {"referenceID": 3, "context": "In prior work this has been achieved by manually encoding relevant domain knowledge into the game playing algorithm \u2013 for example, via manually specified heuristics for action selection (Billings et al., 1999; Gelly et al., 2006), hand crafted features (Tesauro & Galperin, 1996), and value functions encoding expert knowledge (Sturtevant, 2008).", "startOffset": 186, "endOffset": 229}, {"referenceID": 16, "context": "In prior work this has been achieved by manually encoding relevant domain knowledge into the game playing algorithm \u2013 for example, via manually specified heuristics for action selection (Billings et al., 1999; Gelly et al., 2006), hand crafted features (Tesauro & Galperin, 1996), and value functions encoding expert knowledge (Sturtevant, 2008).", "startOffset": 186, "endOffset": 229}, {"referenceID": 29, "context": ", 2006), hand crafted features (Tesauro & Galperin, 1996), and value functions encoding expert knowledge (Sturtevant, 2008).", "startOffset": 105, "endOffset": 123}, {"referenceID": 16, "context": "It augments an action\u2019s value with an exploration bonus for rarely visited state-action pairs, resulting in better action selection and better overall game performance (Gelly et al., 2006; Sturtevant, 2008; Balla & Fern, 2009).", "startOffset": 168, "endOffset": 226}, {"referenceID": 29, "context": "It augments an action\u2019s value with an exploration bonus for rarely visited state-action pairs, resulting in better action selection and better overall game performance (Gelly et al., 2006; Sturtevant, 2008; Balla & Fern, 2009).", "startOffset": 168, "endOffset": 226}, {"referenceID": 27, "context": "Prior work has shown such linear value function approximations to be effective in the Monte-Carlo Search framework (Silver et al., 2008).", "startOffset": 115, "endOffset": 136}, {"referenceID": 31, "context": "Second, due to this simpler representation, it can be learned from fewer observations than a global actionvalue function (Sutton et al., 2007).", "startOffset": 121, "endOffset": 142}, {"referenceID": 8, "context": "The second layer consists of two disjoint sets of hidden units ~y and ~z, where each set operates as a stochastic 1-of-n softmax selection layer (Bridle, 1990).", "startOffset": 145, "endOffset": 159}, {"referenceID": 31, "context": "Learning a specialized Q(st, at) for each game state is common and useful in games with complex state spaces and dynamics, where learning a single global function approximation can be particularly difficult (Sutton et al., 2007).", "startOffset": 207, "endOffset": 228}, {"referenceID": 24, "context": "The parameter of our model are estimated via standard error backpropagation (Bryson & Ho, 1969; Rumelhart et al., 1986).", "startOffset": 76, "endOffset": 119}], "year": 2012, "abstractText": "Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.", "creator": "TeX"}}}