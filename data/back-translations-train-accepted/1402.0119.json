{"id": "1402.0119", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2014", "title": "Randomized Nonlinear Component Analysis", "abstract": "Classical techniques such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are ubiquitous in statistics. However, these techniques only reveal linear relationships in data. Although nonlinear variants of PCA and CCA have been proposed, they are computationally prohibitive in the large scale.", "histories": [["v1", "Sat, 1 Feb 2014 19:54:06 GMT  (312kb,D)", "http://arxiv.org/abs/1402.0119v1", null], ["v2", "Tue, 13 May 2014 16:41:11 GMT  (530kb,D)", "http://arxiv.org/abs/1402.0119v2", "Appearing in ICML 2014"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["david lopez-paz", "suvrit sra", "alexander j smola", "zoubin ghahramani", "bernhard sch\u00f6lkopf"], "accepted": true, "id": "1402.0119"}, "pdf": {"name": "1402.0119.pdf", "metadata": {"source": "META", "title": "Randomized Nonlinear Component Analysis", "authors": ["David Lopez-Paz", "Suvrit Sra", "Alexander J. Smola", "Zoubin Ghahramani", "Bernhard Sch\u00f6lkopf"], "emails": ["DAVID@LOPEZPAZ.ORG", "SUVRIT@TUE.MPG.DE", "ALEX@SMOLA.ORG", "ZOUBIN@ENG.CAM.AC.UK", "BS@TUE.MPG.DE"], "sections": [{"heading": null, "text": "In a separate strand of research, randomized methods have been proposed to construct traits that help uncover nonlinear patterns in data. For basic tasks such as regression or classification, random traits show little or no performance loss, while achieving dramatic savings in computing requirements. In this paper, we use randomness to design scalable new variants of nonlinear PCA and CCA; our ideas also extend to important multivariate analysis tools such as spectral clustering or LDA. We demonstrate our algorithms by experimenting with real data, comparing them with the state of the art. Code in R for implementing our methods is included in the appendix."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to move to another world, in which they are able to integrate, and in which they are able to change the world."}, {"heading": "1.1. Related Work", "text": "Since the groundbreaking work of Rahimi & Recht (2008), there have been a number of new research approaches to kernel approximations using randomised character maps (Kar & Karnick, 2012) and polynomial nuclei (Hamid et al., 2013); the development of advanced methods for sample analysis using quasi-Monte Carlo methods (Yang et al., 2014) or their accelerated calculation using rapid Walsh-Hadamard transformations (Le et al., 2013); and the use of randomised techniques for core analysis (Achlioptas et al., 2002), where three sub-sampling strategies for accelerating KPCA have been proposed; on the other hand (Avron et al., 2013), randomised methods for analysing components have been used to adapt linear CCA to large datasets."}, {"heading": "2. Random Nonlinear Features", "text": "Before we present our new methods, let us first recall a few key aspects of nonlinear random characteristics."}, {"heading": "2.1. Random Features and Kernel Matrices", "text": "Bochner's theorem helps to connect shift-invariant nuclei (Schoolhead & Smola, 2002) and random nonlinear characteristics to each other. Let k (x, y) be a real value, normalized (k (x, y) \u2264 1), shift-invariant kernels on Rd \u00b7 Rd. Then k (x, y) = conservation p (w) e \u2212 jw T (x \u2212 y) dw. Here, z (x) is a random characteristic map as in (4) with p (w) set to be the inverse Fourier transformation of k (w T i x) cos (w T i y) = < 1 \u221a m z (x), 1 \u221a m z (y) >, where z (x) is a random characteristic sketch as in (4), where p (w) is the reverse Fourier transformation of k (w T i x)."}, {"heading": "3. Principal Component Analysis (PCA)", "text": "Principal Component Analysis or PCA (Pearson, 1901; Jolliffe, 2002) is the orthogonal transformation of a set of n observations d of correlated variablesX, Rn \u00b7 d into a set of n observations d of uncorrelated major components. For a centered data matrix (zero middle columns) X, PCA requires the calculation of the (complete) singular value decomposiX = U\u0121F T, which is a diagonal matrix containing the singular values of X in decreasing order. The major components are calculated using the linear transformationXF. Nonlinear variants of the PCA are also known; in particular \u2022 Kernel PCA (Scho \u00f6lkopf et al., 1999) uses the kernel trick to embed data into a high-dimensional rendering of the Hilbert Space kernel in which regular PCA is performed. The calculation of the main components of the net component is reduced to a eigenvalue problem requiring 3."}, {"heading": "3.1. Randomized Nonlinear PCA (RPCA)", "text": "3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3 3-3 3 3 3 3 3 3 3 3-3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3"}, {"heading": "4. Canonical Correlation Analysis (CCA)", "text": "Canonical Correlation Analysis or CCA (Hotelling, 1936) measures the correlation between two multidimensional random variables. Specifically, taking into account two samples X-Rn \u00b7 p and Y-Rn \u00b7 q, CCA calculates a pair of canonical bases F-Rp \u00b7 r and G-Rq \u00b7 r, so that the correlations between the canonical variables XF and Y-G are minimized, corr (XF, XF) = I, corr (Y G, Y) = I, where I stand for the identity matrix. Correlations between the canonical variables XF and Y G are called canonical correlations, and up to r = max (rank (Y), rank (Y))) of which can be computed."}, {"heading": "4.1. Randomized Nonlinear CCA (RCCA)", "text": "It is shown that RCCA is a low rate of KCCA when KCCA has a pair of differently invariant nuclei. RCCA can be understood as linear CCA running on a pair of randomized nonlinear maps (see 4): zx: Rn \u00b7 p \u2192 Rn \u00b7 mx, zy (Y). CCA requires O (p2 + q2) n operations, while RCCA costs (mx, my) 2n. Both are linear in the example size n; if max, q), RCCA costs (mx, my)."}, {"heading": "5. Experiments", "text": "Interestingly, in Section 5.3, we provide a novel algorithm based on RCCA to perform learning with privileged information (Vapnik & Vashist, 2009). We define our random features in such a way that they exhibit nonlinearity \u03c6 (x) = cos (x) and sample parameters according to wi-N (0, 2sI). This setup approaches the widely used spherical Gaussian kernel k (x, x \u2032) = exp (\u2212 s-x-x \u2032 22) as m \u2192. The parameter s is set for each data set tested with Jaakkola's heuristics. RCCA does not require regulation."}, {"heading": "5.1. Empirical Validation of Bernstein Inequalities", "text": "We first turn to the question of empirical validation of the limits obtained in theorems 3 and 4. To this end, we run simulations in which we vary the values of the sample size n, the number of random projections m and the regularization parameter \u03b3 separately. We use the synthetic data matrices X-Rn \u00b7 10 and Y-Rn \u00b7 10 formed by i.i.d. normal entries. If the parameters do not vary, they are to n = 1000, k = 1000 and \u03b3 = 10 \u2212 3.Figure 1 represents the value of the norms from equations (7, 12), since the parameters {n, m, \u03b3} vary when averaged over a total of 1000 random samples {Xi, Yi} 1000i = 1. The correspondence with the theoretical analysis is remarkable: the sample size n and the regularization parameters \u03b3 exhibit a linear effect, while the increase in the number of random features of the comparison is a reduction (O / 1) / m (1)."}, {"heading": "5.2. Canonical Correlation Analysis", "text": "We are comparing the performance of three variants of CCA on the basis of two modalities: Linear CCA, Deep CCA, 2013 and the proposed RCCA. We have not been able to establish exact relationships between the proposed data sets due to a certain number of data. Participating countries are able to measure the results, both in terms of the quality and quality of the data as well as in terms of the quality of the data."}, {"heading": "5.3. Learning Using Privileged Information", "text": "The challenge is to develop algorithms that are able to extract information from this privileged information in the training time to build a better classification for the test time. We propose to use RCCA to construct a highly dependent subspace between the regular characteristics of X and the privileged characteristics of X. We experiment with the animals with attribute datasets. In this dataset, the regular set of characteristics X is a set of 30,000 images of 35 different animals. The set of privileged characteristics of FeaturesX? is a set of 85 high-level binary attributes associated with each image (e.g. eaten fish or can fly)."}, {"heading": "6. Conclusions", "text": "We presented a theoretical and empirical treatment of randomized, nonlinear variants of the classical multivariate analysis methods of Principal Component Analysis and Canonical Correlation Analysis. Using nonlinear random features significantly reduces the computational complexity of these algorithms without losing their strong predictive power, and the proposed algorithms do not contain any parameters that can be adjusted except for the number of random features to be used, but this can be understood as a handy button to establish a direct trade-off between speed and accuracy. Several questions that are of interest for further investigation remain open, for example: (i) How can we use knowledge from certain data sets to better shape the random feature sampling distribution p (w)? (ii) How can we incorporate thrift assumptions about the random features? (iii) Can we build a hierarchy of random feature layers to develop more powerful methods for component analysis?"}, {"heading": "A. R Source Code", "text": "rcca _ fit < - Function (x, y, mx, my, sx, sy) {w < - Matrix (sx * rnorm (ncol (x) * mx), ncol (x)) m < - Matrix (sy * rnorm (ncol (y) * my), ncol (y) C < - Delete (cos (x% *% w), cos (y% *% rcca $w)) List (a = C $xcoef, b = C $ycoef, w = w, m = m)} rcca _ eval < - Function (rcca, x, y) List (x = cos (x% *% rcca $w)% *% rcca $w)% *% rcca $b), rcca _ fit < - Function (x, p), p (p, p, p, p, p, p, p, p () () (l * l * l) (l * l () (l * l) (p, p, p, p, p, p, p, p) (l) (l) (l, p, p (l, p, p, p, p, p, p, p, p, p, p, p, p) (p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p () (p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p (), p () (p (p, p, p, p, p, p (, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p), p (, p (, p (, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p), p ("}, {"heading": "B. Acknowledgments", "text": "The DLP is grateful for the fruitful discussions with Yarin Gal and Maxim Rabinovich. The DLP is supported by Obra Social \"la Caixa.\""}], "references": [{"title": "Sampling techniques for kernel methods", "author": ["Achlioptas", "Dimitris", "McSherry", "Frank", "Sch\u00f6lkopf", "Bernhard"], "venue": "In NIPS 14,", "citeRegEx": "Achlioptas et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Achlioptas et al\\.", "year": 2002}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "In ICML,", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias"], "venue": "In ICML,", "citeRegEx": "Avron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2013}, {"title": "Restarted block lanczos bidiagonalization methods", "author": ["J. Baglama", "L. Reichel"], "venue": "Numerical Algorithms,", "citeRegEx": "Baglama and Reichel,? \\Q2006\\E", "shortCiteRegEx": "Baglama and Reichel", "year": 2006}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks,", "citeRegEx": "Baldi and Hornik,? \\Q1989\\E", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Eigenproblems in pattern recognition", "author": ["Bie", "T. De", "N. Cristianini", "R. Rosipal"], "venue": "Handbook of Geometric Computing,", "citeRegEx": "Bie et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bie et al\\.", "year": 2005}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "In ICML,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Compact random feature maps", "author": ["Hamid", "Raffay", "Xiao", "Ying", "Gittens", "Alex", "DeCoste", "Dennis"], "venue": null, "citeRegEx": "Hamid et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hamid et al\\.", "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science,", "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["S.M. Kakade", "D.P. Foster"], "venue": "In COLT,", "citeRegEx": "Kakade and Foster,? \\Q2007\\E", "shortCiteRegEx": "Kakade and Foster", "year": 2007}, {"title": "Random feature maps for dot product kernels", "author": ["Kar", "Purushottam", "Karnick", "Harish"], "venue": null, "citeRegEx": "Kar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2012}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P. Lai", "C. Fyfe"], "venue": "International Journal of Neural Systems,", "citeRegEx": "Lai and Fyfe,? \\Q2000\\E", "shortCiteRegEx": "Lai and Fyfe", "year": 2000}, {"title": "Fastfood \u2013 Approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarlos", "A. Smola"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes", "year": 1998}, {"title": "The randomized dependence coefficient", "author": ["D. Lopez-Paz", "P. Hennig", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2013}, {"title": "A tutorial on spectral clustering", "author": ["U. Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "Luxburg,? \\Q2007\\E", "shortCiteRegEx": "Luxburg", "year": 2007}, {"title": "Matrix Concentration Inequalities via the Method of Exchangeable Pairs", "author": ["L. Mackey", "M.I. Jordan", "R.Y. Chen", "B. Farrell", "J.A. Tropp"], "venue": "ArXiv e-prints,", "citeRegEx": "Mackey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2012}, {"title": "Randomized algorithms for matrices and data", "author": ["M.W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney,? \\Q2011\\E", "shortCiteRegEx": "Mahoney", "year": 2011}, {"title": "Correlated random features for fast semi-supervised learning", "author": ["B. McWilliams", "D. Balduzzi", "J. Buhmann"], "venue": "In NIPS,", "citeRegEx": "McWilliams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McWilliams et al\\.", "year": 2013}, {"title": "Kernel canonical correlation analysis", "author": ["T. Melzer", "M. Reiter", "H. Bischof"], "venue": "Proceedings of the International Conference on Artificial Neural Networks,", "citeRegEx": "Melzer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Melzer et al\\.", "year": 2001}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine,", "citeRegEx": "Pearson,? \\Q1901\\E", "shortCiteRegEx": "Pearson", "year": 1901}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A. Rahimi", "B. Recht"], "venue": null, "citeRegEx": "Rahimi and Recht,? \\Q2008\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2008}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Kernel principal component analysis. In Advances in kernel methods Support vector learning, pp. 327\u2013352", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.R. M\u00fcller"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "User-Friendly Tools for Random Matrices: An Introduction", "author": ["J.A. Tropp"], "venue": "NIPS Tutorials,", "citeRegEx": "Tropp,? \\Q2012\\E", "shortCiteRegEx": "Tropp", "year": 2012}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["V. Vapnik", "A. Vashist"], "venue": "Neural Networks,", "citeRegEx": "Vapnik and Vashist,? \\Q2009\\E", "shortCiteRegEx": "Vapnik and Vashist", "year": 2009}, {"title": "X-Ray microbeam speech production database user\u2019s handbook version 1.0", "author": ["J.R. Westbury"], "venue": null, "citeRegEx": "Westbury,? \\Q1994\\E", "shortCiteRegEx": "Westbury", "year": 1994}, {"title": "Quasi-Monte Carlo Feature Maps for ShiftInvariant Kernels", "author": ["Yang", "Jiyan", "Sindhwani", "Vikas", "Avron", "Haim", "Mahoney", "Michael W"], "venue": "In ICML,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "Principal Component Analysis (Pearson, 1901) and Canonical Correlation Analysis (Hotelling, 1936) are two of the most popular multivariate analysis methods.", "startOffset": 29, "endOffset": 44}, {"referenceID": 6, "context": "CCA is widely used to learn from multiple modalities of data (Kakade & Foster, 2007), an ability particularly useful when some of the modalities are only available at training time, but keeping information about them at testing time is beneficial (Chaudhuri et al., 2009; Vapnik & Vashist, 2009).", "startOffset": 247, "endOffset": 295}, {"referenceID": 23, "context": "For PCA, these include Kernel Principal Component Analysis or KPCA (Sch\u00f6lkopf et al., 1999) and Autoencoder Neural Networks (Baldi & Hornik, 1989; Hinton & Salakhutdinov, 2006).", "startOffset": 67, "endOffset": 91}, {"referenceID": 19, "context": "For CCA, common extensions are Kernel Canonical Correlation Analysis or KCCA (Lai & Fyfe, 2000; Melzer et al., 2001; Bach & Jordan, 2002) and Deep Canonical Correlation Analysis (Andrew et al.", "startOffset": 77, "endOffset": 137}, {"referenceID": 1, "context": ", 2001; Bach & Jordan, 2002) and Deep Canonical Correlation Analysis (Andrew et al., 2013).", "startOffset": 69, "endOffset": 90}, {"referenceID": 12, "context": "In a separate strand of recent research, randomized strategies have been introduced for constructing features that can help reveal nonlinear patterns in data when used in conjunction with linear algorithms (Rahimi & Recht, 2008; Le et al., 2013).", "startOffset": 206, "endOffset": 245}, {"referenceID": 16, "context": "Our analysis is powered by the recently developed matrix Bernstein inequality (Mackey et al., 2012).", "startOffset": 78, "endOffset": 99}, {"referenceID": 1, "context": "We demonstrate the effectiveness of the proposed randomized methods by experimenting with several real-world data and comparing against the state-of-the-art on CCA: Deep Canonical Correlation Analysis (Andrew et al., 2013).", "startOffset": 201, "endOffset": 222}, {"referenceID": 7, "context": "For instance, their extensions to dot-product kernels (Kar & Karnick, 2012) and polynomial kernels (Hamid et al., 2013); the development of advanced sampling techniques using Quasi-Monte-Carlo methods (Yang et al.", "startOffset": 99, "endOffset": 119}, {"referenceID": 27, "context": ", 2013); the development of advanced sampling techniques using Quasi-Monte-Carlo methods (Yang et al., 2014) or their accelerated computation via fast Walsh-Hadamard transforms (Le et al.", "startOffset": 89, "endOffset": 108}, {"referenceID": 12, "context": ", 2014) or their accelerated computation via fast Walsh-Hadamard transforms (Le et al., 2013).", "startOffset": 76, "endOffset": 93}, {"referenceID": 0, "context": "The use of randomized techniques for kernelized component analysis methods dates back to (Achlioptas et al., 2002), where three kernel sub-sampling strategies were suggested to speed up KPCA.", "startOffset": 89, "endOffset": 114}, {"referenceID": 2, "context": "On the other hand, (Avron et al., 2013) made use of randomized Walsh-Hadamard transforms to adapt linear CCA to large-scale datasets.", "startOffset": 19, "endOffset": 39}, {"referenceID": 0, "context": "The use of randomized techniques for kernelized component analysis methods dates back to (Achlioptas et al., 2002), where three kernel sub-sampling strategies were suggested to speed up KPCA. On the other hand, (Avron et al., 2013) made use of randomized Walsh-Hadamard transforms to adapt linear CCA to large-scale datasets. The use of nonlinear random features is more scarce and has only appeared twice in previous literature. First, McWilliams et al. (2013) use the Nystr\u00f6m method to define a randomized feature map and perform CCA to achieve fast state-of-the-art semisupervised learning.", "startOffset": 90, "endOffset": 462}, {"referenceID": 0, "context": "The use of randomized techniques for kernelized component analysis methods dates back to (Achlioptas et al., 2002), where three kernel sub-sampling strategies were suggested to speed up KPCA. On the other hand, (Avron et al., 2013) made use of randomized Walsh-Hadamard transforms to adapt linear CCA to large-scale datasets. The use of nonlinear random features is more scarce and has only appeared twice in previous literature. First, McWilliams et al. (2013) use the Nystr\u00f6m method to define a randomized feature map and perform CCA to achieve fast state-of-the-art semisupervised learning. Second, Lopez-Paz et al. (2013) define the dependence statistic RDC as the largest canonical correlation between two sets of copula random projections.", "startOffset": 90, "endOffset": 626}, {"referenceID": 12, "context": "Recent techniques that use subsampled Hadamard randomized transforms (Le et al., 2013) allow computation of the random features even faster, yieldingO(n log(d)m+mn) operations to solve (5) and O(t log(d)m) to test t new points.", "startOffset": 69, "endOffset": 86}, {"referenceID": 17, "context": "It is of special interest that randomized algorithms are in many cases more robust than their deterministic analogues (Mahoney, 2011).", "startOffset": 118, "endOffset": 133}, {"referenceID": 20, "context": "Principal Component Analysis or PCA (Pearson, 1901; Jolliffe, 2002) is the orthogonal transformation of a set of n observations of d correlated variablesX \u2208 Rn\u00d7d into a set of n observations of d uncorrelated principal components.", "startOffset": 36, "endOffset": 67}, {"referenceID": 23, "context": "\u2022 Kernel PCA (Sch\u00f6lkopf et al., 1999) uses the kernel trick to embed data into a high-dimension Reproducing Kernel Hilbert Space, where regular PCA is performed.", "startOffset": 13, "endOffset": 37}, {"referenceID": 16, "context": "Theorem 2 (Matrix Bernstein, (Mackey et al., 2012)).", "startOffset": 29, "endOffset": 50}, {"referenceID": 24, "context": "We follow a derivation similar to (Tropp, 2012).", "startOffset": 34, "endOffset": 47}, {"referenceID": 15, "context": "Spectral clustering (Luxburg, 2007) uses the spectrum of K to perform dimensionality reduction before applying k-means.", "startOffset": 20, "endOffset": 35}, {"referenceID": 5, "context": ", gr \u2208 R form the eigensystem of the generalized eigenvalue problem (Bie et al., 2005): ( 0 CXY CY X 0 )( f g ) =", "startOffset": 68, "endOffset": 86}, {"referenceID": 6, "context": "This is particularly useful when the two views are available at training time, but only one of them is available at test time (Kakade & Foster, 2007; Chaudhuri et al., 2009; Vapnik & Vashist, 2009).", "startOffset": 126, "endOffset": 197}, {"referenceID": 19, "context": "Several nonlinear extensions of CCA have been proposed: \u2022 Kernel Canonical Correlation Analysis or KCCA (Lai & Fyfe, 2000; Melzer et al., 2001; Bach & Jordan, 2002) uses the kernel trick to derive a nonparametric and nonlinear CCA algorithm.", "startOffset": 104, "endOffset": 164}, {"referenceID": 1, "context": "\u2022 Deep Canonical Correlation Analysis or DCCA (Andrew et al., 2013) feeds the pair of input variables through a deep neural network.", "startOffset": 46, "endOffset": 67}, {"referenceID": 5, "context": "LDA can be solved by CCA(X,T ), where Tij = I{yi = j} (Bie et al., 2005).", "startOffset": 54, "endOffset": 72}, {"referenceID": 14, "context": "The Randomized Dependence Coefficient or RDC (Lopez-Paz et al., 2013) is defined as the largest canonical correlation of RCCA when performed on the copula transformation of the data matrices ofX and Y .", "startOffset": 45, "endOffset": 69}, {"referenceID": 1, "context": "data: linear CCA, Deep CCA (Andrew et al., 2013) and the proposed RCCA.", "startOffset": 27, "endOffset": 48}, {"referenceID": 1, "context": "data: linear CCA, Deep CCA (Andrew et al., 2013) and the proposed RCCA. We were unable to run exact KCCA on the proposed datasets due to its cubic computational complexity. We replicate the two experiments presented in Galen et al. (2013). The task is to measure performance as the accumulated correlation between the canonical variables associated with the top canonical correlations on some unseen test data.", "startOffset": 28, "endOffset": 239}, {"referenceID": 26, "context": "Learn correlated representations of simultaneous acoustic and articulatory speech measurements (Westbury, 1994).", "startOffset": 95, "endOffset": 111}], "year": 2017, "abstractText": "Classical techniques such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are ubiquitous in statistics. However, these techniques only reveal linear relationships in data. Although nonlinear variants of PCA and CCA have been proposed, they are computationally prohibitive in the large scale. In a separate strand of recent research, randomized methods have been proposed to construct features that help reveal nonlinear patterns in data. For basic tasks such as regression or classification, random features exhibit little or no loss in performance, while achieving dramatic savings in computational requirements. In this paper we leverage randomness to design scalable new variants of nonlinear PCA and CCA; our ideas also extend to key multivariate analysis tools such as spectral clustering or LDA. We demonstrate our algorithms through experiments on real-world data, on which we compare against the state-of-the-art. Code in R implementing our methods is provided in the Appendix.", "creator": "LaTeX with hyperref package"}}}