{"id": "1402.0288", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2014", "title": "Transductive Learning with Multi-class Volume Approximation", "abstract": "Given a hypothesis space, the large volume principle by Vladimir Vapnik prioritizes equivalence classes according to their volume in the hypothesis space. The volume approximation has hitherto been successfully applied to binary learning problems. In this paper, we extend it naturally to a more general definition which can be applied to several transductive problem settings, such as multi-class, multi-label and serendipitous learning. Even though the resultant learning method involves a non-convex optimization problem, the globally optimal solution is almost surely unique and can be obtained in O(n^3) time. We theoretically provide stability and error analyses for the proposed method, and then experimentally show that it is promising.", "histories": [["v1", "Mon, 3 Feb 2014 06:09:52 GMT  (261kb,D)", "http://arxiv.org/abs/1402.0288v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["gang niu", "bo dai", "marthinus christoffel du plessis", "masashi sugiyama"], "accepted": true, "id": "1402.0288"}, "pdf": {"name": "1402.0288.pdf", "metadata": {"source": "CRF", "title": "Transductive Learning with Multi-class Volume Approximation", "authors": ["Gang Niu", "Masashi Sugiyama"], "emails": ["gang@sg.cs.titech.ac.jp", "bohr.dai@gmail.com", "christo@sg.cs.titech.ac.jp", "sugi@cs.titech.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "The history of the Great Volume Principle (LVP) dates back to the early age of statistical learning theory, when Vapnik (1982) introduced it in the case of hyperplanes, but it did not gain much attention until a creative approach in El-Yaniv et al. (2008) to implement LVP in the case of soft response vectors was successfully applied to various binary learning problems, such as binary transductive Xiv: 140 2.02 88learning (El-Yaniv et al., 2008), binary clustering (Niu et al., 2013a), and outlier recognition (Li and Ng, 2013). LVP is a learning theory principle that selects learning as a hypothesis from a particular hypotheses space H. Despite the form of the hypothesis, H can always be divided into a finite number of equivalence classes, where an equivalence class generates a set of hypotheses that generate the data being observed."}, {"heading": "2 Binary Volume Approximation", "text": "The binary approach in El-Yaniv et al. (2008) includes a few key concepts: The soft response vector, the space hypothesis and the equivalence class, and the power and volume of equivalence classes. We review the concepts in this section for later use in the next section. Suppose that X is the domain of the input data, and most often, but not necessarily, X Rd, where d is a natural number. (1) Given a set of n data Xn = {x1,.), where xi X, a soft response vector is an n-dimensional vectorh: (h1,.)., hn) > Rn, that hi stands for a soft or trusted label of xi. For binary transductive learning problems, a soft response vector suggests that xi is from the positive class, if hi > 0, xi is from the negative class."}, {"heading": "3 Multi-class Volume Approximation", "text": "In this section we propose a more general multi-class approximation of the volume, which is suitable for several problems."}, {"heading": "3.1 Problem settings", "text": "Remember the definition of binary transductive problems (Vapnik, 1998, p. 341). A fixed sentence Xn = {x1,.., xn} of n dots of X is observed, and the labels y1,..., yn = {\u2212 1, + 1} of these dots are also fixed, but unknown. A subset Xl # Xn of size l is uniformly selected, and then yi is revealed when xi # Xl. We call Sl = (xi, yi) | xi # Xl} the labeled data, and Xu = Xn\\ Xl the unlabeled data. Using # Xu labels, the goal is pursued, yi by xi # Xu (while ignored x\\ Xn is currently ignored). A minor modification is sufficient to expand the setting. Instead of y1,.."}, {"heading": "3.2 Definitions", "text": "In order to extend the binary definitions, we must only extend the hypothesis and the hypothesis. (H1, H2, H2, H2, H3, H3, H3, H3, H3, H3, H3, H3, H3, H3, H3, H3, H3, H3, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H6, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H4, H6, H6, H6, H6, H6, H6, H6, H6, H6, 6, H6, 6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6, H6."}, {"heading": "4 Multi-class Approximate Volume Regularization", "text": "The proposed volume adjustment motivates a family of new transductive methods, which they regard as regularization. In this section, we develop and analyze an instance whose optimization problem is not convex, but can be solved accurately and efficiently."}, {"heading": "4.1 Model", "text": "It is not only a question of time, but also of the time in which we have to deal with the losses. (H, H) The difference between Y and H. The difference between Y and H. The multiclass volume approximation motivates the following family of transductive methods: min H, Q and Y. (H, H) The multiclass volume approximation: min H, Q and Y. (H, H) The multiclass approximation: min H, H and Y. (H, H) The multiclass approximation: min H, Q and Y. (H, H) The multiclass approximation: n. (H) The multiclass approximation: min H, Q and Y. (H) The multiclass approximation: min H, H (H) + The multiclass approximation: n. (H, H. (H). (H)."}, {"heading": "4.2 Algorithm", "text": "The optimization (9) is non-convex, but we can use it with the Q = > Q = > Q = > Q = > Q = > Q = = vec (H) asmin h (H) asmin h \u2212 h (P'Q) hs.t. (Q = > Q = > Q = Q = > Q = Q = Q (H) is the vectorization of Y (H). In this representation, the goal is a secondary polynomia and the constraint is an origin-centered sphere, and fortunately we could solve it accurately and efficiently according to Forsythe and Golub (1965). To this end, a fundamental property of the Kronecker product is necessary (see, Theorems 13.10 and 13.12 of Laub, 2005): Theorem 1. Let'Q, 1 \u2264 QQ, n be the eigenvalues and vQ, 1,."}, {"heading": "4.3 Theoretical analyses", "text": "We provide two theoretical results. Under certain assumptions, the stability analysis is upper limits of the difference between two optimal H and H \"with two different metrics Y and Y.\" The error analysis limits the difference between H and H. \"Theorem 2 guarantees that\" Q \"and\" Q, \"k0.\" In fact, with high probability about the choice of Y. \"It is true that\" H, \"\" H \"and\" H \"in our experiments are only k0.\" For this reason, we make the following assumptions: Fix P and Q, \"and allow Y to change according to the division of Xn into different Xl and Xu.\" There are \"H,\" CCP \"> 0,\" which depends only on \"and\" T, \"so that for all optimal Y.\" We assume that we have \"QQ\" and \"VR.\""}, {"heading": "5 Experiments", "text": "In this section we evaluate MAVR numerically."}, {"heading": "5.1 Serendipitous learning", "text": "We show how random problems can be dealt with directly by MAVR without clustering (Hartigan and Wong, 1979; Ng et al., 2001; Sugiyama et al., 2014) or estimating the previous class change (du Plessis and Sugiyama, 2012). Experimental results are presented in Figure 2. There are 5 datasets, the latter 3 datasets originating from ZelnikManor and Perona (2004). Matrix Q was used as a normalized graph Laplacian (see e.g. from Luxburg, 2007) 1 Lnor = In \u2212 D \u2212 1 / 2WD \u2212 1 / 2, where W \u2012 Rn is a similarity matrix and D \u2012 Rn \u00b7 n is the degree matrix of W. The matrix P was used by P1 = 1 0 0 0 0 0 0 0 0 0 0 0 0 / 3 = 0."}, {"heading": "5.2 Multi-class learning", "text": "We have three classes with the class ratio 1 / 6, 1 / 2. Let us show the nuance of the restricted MAVR and LGC, the unrestricted MAVR is exactly reduced to LGC. Although LGC is motivated by the label propagation, it can be described as follows: The artificial data set is generated as follows: The artificial data set of 3circles is generated as follows: We have three classes with the class ratio 1 / 6, 1 / 2. Let us use the basic truth marking of xi, then xi byxi = (6yi cos) is generated (ai) + i, 1, 5yi sin (ai) + i, 2), where an angel is drawn i.e. we are generated by the uniform distribution of universality and universality (6yi cos) (ai) + i, 5yi sin (ai)."}, {"heading": "6 Conclusions", "text": "We proposed a multi-level volume approach, which can be applied to several transductive problems, such as multi-class, multi-class and random learning. The ultra-convex learning method is not convex, but can be solved precisely and efficiently. It is theoretically justified by our stability and error analyses and experimentally proven to be promising."}, {"heading": "A Proofs", "text": "The proof of the theory 2The proof of the theory 2The proof of the g (r) isg (r) isg (r) isg (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i) i (r) i i) i i (r) i i i (r) i i i (r) i i (r) i i i i i (r) i i i i i i i i (r) i i i i (r) i) i i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i (r) i) i (r) i i i (r) i (r) i (r) i (r) i (r) i i (r) i i (r) i i (r) i i i i (r) i (r) i i) i i i i i i (r) i (r) i i i (r) i i) i i i i i i i i i (r) i (r) i (r) i) i i i i i i (r) i (r) i (r) i i) i i i) i i i i (r) i (r) i (r) i"}], "references": [{"title": "Manifold regularization: a geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "In ICML,", "citeRegEx": "Blum and Chawla.,? \\Q2001\\E", "shortCiteRegEx": "Blum and Chawla.", "year": 2001}, {"title": "Semi-supervised learning of class balance under class-prior change by distribution matching", "author": ["M.C. du Plessis", "M. Sugiyama"], "venue": "In ICML,", "citeRegEx": "Plessis and Sugiyama.,? \\Q2012\\E", "shortCiteRegEx": "Plessis and Sugiyama.", "year": 2012}, {"title": "Large margin vs. large volume in transductive learning", "author": ["R. El-Yaniv", "D. Pechyony", "V. Vapnik"], "venue": "Machine Learning,", "citeRegEx": "El.Yaniv et al\\.,? \\Q2008\\E", "shortCiteRegEx": "El.Yaniv et al\\.", "year": 2008}, {"title": "On the stationary values of a second-degree polynomial on the unit sphere", "author": ["G. Forsythe", "G. Golub"], "venue": "Journal of the Society for Industrial and Applied Mathematics,", "citeRegEx": "Forsythe and Golub.,? \\Q1965\\E", "shortCiteRegEx": "Forsythe and Golub.", "year": 1965}, {"title": "A k-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Applied Statistics,", "citeRegEx": "Hartigan and Wong.,? \\Q1979\\E", "shortCiteRegEx": "Hartigan and Wong.", "year": 1979}, {"title": "Transductive multi-label learning via label set propagation", "author": ["X. Kong", "M. Ng", "Z.-H. Zhou"], "venue": "IEEE Transaction on Knowledge and Data Engineering,", "citeRegEx": "Kong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2013}, {"title": "Matrix Analysis for Scientists and Engineers", "author": ["A.J. Laub"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Laub.,? \\Q2005\\E", "shortCiteRegEx": "Laub.", "year": 2005}, {"title": "Maximum volume outlier detection and its applications in credit risk analysis", "author": ["S. Li", "W. Ng"], "venue": "International Journal on Artificial Intelligence Tools,", "citeRegEx": "Li and Ng.,? \\Q2013\\E", "shortCiteRegEx": "Li and Ng.", "year": 2013}, {"title": "Squared-loss mutual", "author": ["2013a. G. Niu", "W. Jitkrittum", "B. Dai", "H. Hachiya", "M. Sugiyama"], "venue": null, "citeRegEx": "Niu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2013}, {"title": "Algorithms for Linear-Quadratic Optimization", "author": ["V. Sima"], "venue": null, "citeRegEx": "Sima.,? \\Q1996\\E", "shortCiteRegEx": "Sima.", "year": 1996}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing,", "citeRegEx": "1982", "shortCiteRegEx": "1982", "year": 1998}, {"title": "Self-tuning spectral clustering", "author": ["Zelnik-Manor", "P. Perona"], "venue": "In NIPS,", "citeRegEx": "Zelnik.Manor and Perona.,? \\Q2007\\E", "shortCiteRegEx": "Zelnik.Manor and Perona.", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "But it did not gain much attention until a creative approximation was proposed in El-Yaniv et al. (2008) to implement LVP for the case of soft response vectors.", "startOffset": 82, "endOffset": 105}, {"referenceID": 3, "context": "learning (El-Yaniv et al., 2008), binary clustering (Niu et al.", "startOffset": 9, "endOffset": 32}, {"referenceID": 8, "context": ", 2013a), and outlier detection (Li and Ng, 2013).", "startOffset": 32, "endOffset": 49}, {"referenceID": 3, "context": "learning (El-Yaniv et al., 2008), binary clustering (Niu et al., 2013a), and outlier detection (Li and Ng, 2013). LVP is a learning-theoretic principle which views learning as hypothesis selecting from a certain hypothesis space H. Despite the form of the hypothesis, H can always be partitioned into a finite number of equivalence classes after we observe certain data, where an equivalence class is a set of hypotheses that generate the same labeling of the observed data. LVP, as one of the learning-theoretic principles from the statistical learning theory, prioritizes those equivalence classes according to the volume they occupy in H. See the illustration in Figure 1: The blue ellipse represents H, and it is partitioned into C1, . . . , C4 each occupying a quadrant of the Cartesian coordinate system R intersected with H; LVP claims that C1 and C3 are more preferable than C2 and C4, since C1 and C3 have larger volume than C2 and C4. In practice, the hypothesis space H cannot be as simple as in Figure 1. It frequently locates in very high-dimensional spaces where exact or even quantifiable volume estimation is challenging. Therefore, El-Yaniv et al. (2008) proposed a volume approximation to bypass the volume estimation.", "startOffset": 10, "endOffset": 1172}, {"referenceID": 6, "context": ", 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages.", "startOffset": 42, "endOffset": 65}, {"referenceID": 2, "context": "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn\u00d7c and linked to an ellipsoid in R, such that the equivalence classes and the volume approximation can be defined accordingly.", "startOffset": 42, "endOffset": 486}, {"referenceID": 2, "context": "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn\u00d7c and linked to an ellipsoid in R, such that the equivalence classes and the volume approximation can be defined accordingly. Similarly to the binary volume approximation, our approach is also distribution free, that is, the labeled and unlabeled data do not necessarily share the same marginal distribution. This advantage of transductive learning over (semi-supervised) inductive learning is especially useful for serendipitous problems where the labeled and unlabeled data must not be identically distributed. We name the learning method which realizes the proposed multi-class volume approximation multi-class approximate volume regularization (MAVR). It involves a non-convex optimization problem, but the globally optimal solution is almost surely unique and accessible in O(n) time following Forsythe and Golub (1965). Moreover, we theoretically provide stability and error analyses for MAVR, as well as experimentally compare it to two state-of-the-art methods in Zhou et al.", "startOffset": 42, "endOffset": 1373}, {"referenceID": 2, "context": "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn\u00d7c and linked to an ellipsoid in R, such that the equivalence classes and the volume approximation can be defined accordingly. Similarly to the binary volume approximation, our approach is also distribution free, that is, the labeled and unlabeled data do not necessarily share the same marginal distribution. This advantage of transductive learning over (semi-supervised) inductive learning is especially useful for serendipitous problems where the labeled and unlabeled data must not be identically distributed. We name the learning method which realizes the proposed multi-class volume approximation multi-class approximate volume regularization (MAVR). It involves a non-convex optimization problem, but the globally optimal solution is almost surely unique and accessible in O(n) time following Forsythe and Golub (1965). Moreover, we theoretically provide stability and error analyses for MAVR, as well as experimentally compare it to two state-of-the-art methods in Zhou et al. (2003) and Belkin et al.", "startOffset": 42, "endOffset": 1539}, {"referenceID": 0, "context": "(2003) and Belkin et al. (2006) using USPS, MNIST, 20Newsgroups and Isolet.", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": "The binary volume approximation in El-Yaniv et al. (2008) involves a few key concepts: The soft response vector, the hypothesis space and the equivalence class, and the power and volume of equivalence classes.", "startOffset": 35, "endOffset": 58}, {"referenceID": 3, "context": "However, it is very hard to accurately compute the geometric volume of even a single convex body in R, let alone all 2 convex bodies, so El-Yaniv et al. (2008) introduced an efficient approximation.", "startOffset": 137, "endOffset": 160}, {"referenceID": 6, "context": "A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2 possible label sets and 3 possible label vectors.", "startOffset": 109, "endOffset": 128}, {"referenceID": 6, "context": "To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet.", "startOffset": 74, "endOffset": 93}, {"referenceID": 6, "context": "To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet. The latter setting is more general, since the former one requires labeled data to be fully labeled, while the latter one allows labeled data to be partially labeled. A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2 possible label sets and 3 possible label vectors. A more challenging serendipitous setting which is a multi-class setting but some labels have no labeled data has been studied in Zhang et al. (2011). Let Yl = {yi | xi \u2208 Xl} and Yu = {yi | xi \u2208 Xu, yi 6\u2208 Yl}, then we have #Yu \u2265 1 where # measures the cardinality.", "startOffset": 74, "endOffset": 655}, {"referenceID": 6, "context": "To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet. The latter setting is more general, since the former one requires labeled data to be fully labeled, while the latter one allows labeled data to be partially labeled. A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2 possible label sets and 3 possible label vectors. A more challenging serendipitous setting which is a multi-class setting but some labels have no labeled data has been studied in Zhang et al. (2011). Let Yl = {yi | xi \u2208 Xl} and Yu = {yi | xi \u2208 Xu, yi 6\u2208 Yl}, then we have #Yu \u2265 1 where # measures the cardinality. It is still solvable when #Yu = 1 if a special label of outliers is allowed and when #Yu > 1 as a combination of classification and clustering problems. Zhang et al. (2011) is the unique previous work which successfully dealt with #Yu = 2 and #Yu = 3.", "startOffset": 74, "endOffset": 943}, {"referenceID": 6, "context": "For multi-label problems, we need a threshold Th that is either preset or learned since usually positive and negative labels are imbalanced, and yi can be predicted by \u0177i = {j | hi,j \u2265 Th}; or we can use the label set prediction methods proposed in Kong et al. (2013). Then, a soft response matrix as our transductive hypothesis is an n-by-c matrix defined by H = (h1, .", "startOffset": 249, "endOffset": 268}, {"referenceID": 3, "context": "The denominator \u2016H\u2016Fro is quite difficult to tackle, so we would like to eliminate it as El-Yaniv et al. (2008) and Niu et al.", "startOffset": 89, "endOffset": 112}, {"referenceID": 3, "context": "The denominator \u2016H\u2016Fro is quite difficult to tackle, so we would like to eliminate it as El-Yaniv et al. (2008) and Niu et al. (2013a).", "startOffset": 89, "endOffset": 135}, {"referenceID": 3, "context": ", 2003) and the binary counterparts of the fourth and third loss functions have been used for binary transductive learning (El-Yaniv et al., 2008) and clustering (Niu et al.", "startOffset": 123, "endOffset": 146}, {"referenceID": 3, "context": "Although the optimization in (8) is done in Rn\u00d7c, the regularization is carried out relative to HP,Q, since under the constraint \u2016H\u2016Fro = \u03c4 , the regularization tr(H>QHP ) is a weighted sum of the squares of cosines between vec(H) and the principal axes of EP,Q like El-Yaniv et al. (2008). Subsequently, we denote by y1, .", "startOffset": 267, "endOffset": 290}, {"referenceID": 4, "context": "In this representation, the objective is a second-degree polynomial and the constraint is an origin-centered sphere, and fortunately we could solve it exactly and efficiently following Forsythe and Golub (1965). To this end, a fundamental property of the Kronecker product is necessary (see, e.", "startOffset": 185, "endOffset": 211}, {"referenceID": 4, "context": "1 of Forsythe and Golub (1965), the smallest root of g(\u03c1) determines a unique h so that (h, \u03c1) is the globally optimal solution to \u03a6(h, \u03c1), i.", "startOffset": 5, "endOffset": 31}, {"referenceID": 10, "context": "(12) is a discrete Sylvester equation which consumes O(n) for solving it (Sima, 1996).", "startOffset": 73, "endOffset": 85}, {"referenceID": 5, "context": "1 Serendipitous learning We show how to handle serendipitous problems by MAVR directly without performing clustering (Hartigan and Wong, 1979; Ng et al., 2001; Sugiyama et al., 2014) or estimating the class-prior change (du Plessis and Sugiyama, 2012).", "startOffset": 117, "endOffset": 182}, {"referenceID": 1, "context": "Over the past decades, a huge number of transductive learning and semi-supervised learning methods have been proposed based on various motivations as graph cut (Blum and Chawla, 2001), random walk (Zhu et al.", "startOffset": 160, "endOffset": 183}, {"referenceID": 0, "context": ", 2003), manifold regularization (Belkin et al., 2006), and information maximization (Niu et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 0, "context": "A state-of-the-art semi-supervised learning method called Laplacian regularized least squares (LapRLS) (Belkin et al., 2006) is included to be compared with MAVR besides LGC.", "startOffset": 103, "endOffset": 124}], "year": 2014, "abstractText": "Given a hypothesis space, the large volume principle by Vladimir Vapnik prioritizes equivalence classes according to their volume in the hypothesis space. The volume approximation has hitherto been successfully applied to binary learning problems. In this paper, we extend it naturally to a more general definition which can be applied to several transductive problem settings, such as multi-class, multi-label and serendipitous learning. Even though the resultant learning method involves a non-convex optimization problem, the globally optimal solution is almost surely unique and can be obtained in O(n3) time. We theoretically provide stability and error analyses for the proposed method, and then experimentally show that it is promising.", "creator": "LaTeX with hyperref package"}}}