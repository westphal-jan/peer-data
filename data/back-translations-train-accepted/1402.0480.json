{"id": "1402.0480", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2014", "title": "Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets", "abstract": "Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations (CP vs DNCP) of the latent variables. We derive rules for deciding when such parameterizations are beneficial for gradient-based inference in terms of decreased posterior correlations, and show that in the DNCP, a Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are validated in experiments.", "histories": [["v1", "Mon, 3 Feb 2014 19:39:20 GMT  (2904kb,D)", "https://arxiv.org/abs/1402.0480v1", null], ["v2", "Mon, 3 Mar 2014 13:56:26 GMT  (3932kb,D)", "http://arxiv.org/abs/1402.0480v2", null], ["v3", "Tue, 13 May 2014 11:17:41 GMT  (5926kb,D)", "http://arxiv.org/abs/1402.0480v3", null], ["v4", "Mon, 16 Jun 2014 09:04:26 GMT  (6071kb,D)", "http://arxiv.org/abs/1402.0480v4", null], ["v5", "Thu, 22 Jan 2015 11:05:53 GMT  (6530kb,D)", "http://arxiv.org/abs/1402.0480v5", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["diederik p kingma", "max welling"], "accepted": true, "id": "1402.0480"}, "pdf": {"name": "1402.0480.pdf", "metadata": {"source": "META", "title": "Efficient Gradient-Based Inference through  Transformations between Bayes Nets and Neural Nets", "authors": ["Diederik P. Kingma", "Max Welling"], "emails": ["D.P.KINGMA@UVA.NL", "M.WELLING@UVA.NL"], "sections": [{"heading": "1. Introduction", "text": "Bavarian networks (also called faith networks) are probabilistic graphical models in which conditional dependencies within a set of random variables are described by a directed acyclic graph (DAG). Many monitored and unsupervised models can be considered special cases of Bayesian networks (e.g. conditional dependencies between variables are non-linear), but the common distribution is distinguishable. Algorithms for approximate conclusions in Bayesian networks can be roughly divided into two categories: sampling approaches and parametric approaches. Parametric approaches include Belief Propagation (Pearl, 1982) or the Proceedings of the 31st International Conference on Machine Learning, Beijing, China, 2014."}, {"heading": "1.1. Outline of the paper", "text": "After checking the background material in Section 2, we introduce in Section 3 a generally applicable differentiable repair parameterization of continuous latent variables into a differentiable non-centered form. In Section 4, we analyze the posterior dependencies in this repaired form. Experimental results are shown in Section 6."}, {"heading": "2. Background", "text": "Notation. We use lower case letters in bold (e.g. x or y) for random variables and instances (values) of random variables. We write p\u03b8 (x | y) and p\u03b8 (x) to denote (conditional) probability density (PDF) or mass functions (PMF) of variables. \u03b8 is the vector that contains all parameters; each distribution in the network uses a subset of elements. Variable sets are written in uppercase and bold, matrices are written in bold and bold, and vectors are written in bold and lowercase."}, {"heading": "2.1. Bayesian networks", "text": "A Bayesian network models a series of random variables V and their conditional dependencies as directed acyclic graphs, each variable corresponding to a vertex and each edge to a conditional dependency. Let the distribu-ar Xiv: 140 2.04 80v5 [cs.LG] 2 2Ja n20 be 15tion of each variable vj p\u03b8 (vj | paj), where we condition on vj's (possibly empty) theorem of parents paj. Given the factoring property of bayesian networks, the common distribution across all variables is simple: p\u03b8 (v1,.., vN) = N-j = 1 p\u03b8 (vj | paj) (1) Let the graph be one or more (discrete or continuous) observed variables xj and continuous latent variables zj, where the latent variable zj, with the corresponding conditional distributions p\u03b8 (xj | paj) (1), will be most effective both in the partite and the general case (we will be most effective in both the inez and the inez)."}, {"heading": "2.2. Conditionally deterministic variables", "text": "A conditional deterministic variable vj with parent paj is a variable whose value is a (possibly non-linear) deterministic function gj (.) of the parents and the parameters: vj = gj (paj, \u03b8). The PDFof a conditional deterministic variable is a Dirac delta function that we call a Gaussian PDFN (.; \u00b5, \u03c3) with infinitesimal Infinitesimal Infinitesimal Infinitesimal Infinitesimal Infinitesimal Infinitesimal Infinitesimal Infinitesimal Infinitel Infinitesimal Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitesimal Infinitesimal Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel Infinitel In"}, {"heading": "2.3. Inference problem under consideration", "text": "We are often interested in performing a posterior conclusion, which most often consists of either optimization (searching for a mode argmaxz p\u03b8 (z | x)) or sampling from the posterior p\u03b8 (z | x). Gradients of the log posterior w.r.t. The latent variables can easily be determined on the basis of equality: log posterior w.r.t. The latent variables are simply the sum of the gradients of individual factors w.r.t. The latent variables can then be converted into a mode if one is interested in finding a MAP solution. If one is interested in finding an optimal solution of the log posterior w.r.t., the latent variables are simply the sum of the gradients of individual factors w.r.t. These gradients can then be converted into a mode if one is interested in finding an MAP solution."}, {"heading": "3. The differentiable non-centered parameterization (DNCP)", "text": "In this section, we present a generally applicable transformation between continuous latent random variables and deterministic units with auxiliary variables. In the rest of the paper, we analyze their impact on gradient-based conclusions."}, {"heading": "3.1. Parameterizations of latent variables", "text": "In the literature of statistics, this is also known as the centered parameterization (CP) of the latent variable zj be: zj = gj (paj, j, \u03b8), which is a differentiable, non-centered function (DNCP). Note that in DNCP, the value of the zj variable is both as a paj and as a newly introduced auxiliary variable j distributed as p (j). See Figure 1 for an illustration of the two parameterization variables zamej, the variables zamej. Changing the variable changes the relationship between the original PDF variable z, the function gj (j, j) and the J variable zamej, for example."}, {"heading": "3.2. Approaches to DNCPs", "text": "There are a few basic approaches to transforming CP from a latent variable zj to a DNCP: 1. Tractable and differentiable inverse CDF. In this case, leave j \u0445 U (0, 1), and let gj (zj, paj, \u03b8) = F \u2212 1 (zj | paj; \u03b8) be the inverse CDF of the conditional distribution. Examples: Exponentials, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel, and Erlang distributions. 2. For each \"spatially scaled\" family of distributions (with differentiable log PDF) we can choose the standard distribution (with location = 0, scale = 1) as an auxiliary variable j, and let gj (.) = Layer + Scale \u00b7 j. Examples: Gaussian, Uniform, Laplace, Elliptical, Student's Variations, Logistic, and Triangular distributions. 3. Composition: It is possible to distribute CDG with variables frequently."}, {"heading": "3.3. DNCP and neural networks", "text": "It is instructive to interpret the DNCP form of latent variables as \"hidden units\" of a neural network. Together, the network of hidden units forms a neural network with inserted noise, which we can efficiently differentiate using the backpropagation algorithm (Rumelhart et al., 1986). However, there has recently been an increase in the popularity of deep neural networks with inserted noise (e.g. (Krizhevsky et al., 2012; Goodfellow et al., 2013; Bengio et al., 2013). Frequently, the parameters of such neural networks are optimized towards the maximum probability of targets. In this case, the neural network can be interpreted as a probabilistic model protocol p.x,) the calculation of a conditional distribution over some target variables t (e.g. classes) that give some input variables x. In (Bengio & Thibodeau-Laufer, 2013), chaotic hidden units are used for learning."}, {"heading": "3.4. A differentiable MC likelihood estimator", "text": "We have shown that many hierarchical, continuous latent variable models can be converted into a DNCP model (x,) in which all the latent variables (the auxiliary variables introduced) are root nodes (see eq. (8)). This is important for learning, since DNCP (as opposed to a CP) can be used to form a differentiated Monte Carlo estimator of the borderline probability: log nodes (x) 'log 1L L L-p (). This MC estimator can use the MC estimation to obtain an MC estimate of the log probability gradient as well as log profile nodes (x) whose values are sampled from their limits: (l) \u0445 p ()."}, {"heading": "4. Effects of parameterizations on posterior dependencies", "text": "What is the effect of the proposed repair parameterization on the efficiency of the inference? If the latent variables subsequently have linear Gaussian conditional distributions (AB =), we can use the metric of the quadratic correlation between the latent variable and each of its children in its posterior distribution. If, after the repair terization, the quadratic correlation is reduced, this will generally lead to more efficient inference.For non-linear Gaussian conditional distributions, logpdf can be approached locally as a linear Gaussian variable with a second-order Taylor expansion. Therefore, the results derived for the linear case can also be applied to the nonlinear case; the correlation calculated on the basis of this approximation is a local dependence between the two variables. Denote by z a scalar latent variable that we are repairing, and by y their parents, where one is the parent."}, {"heading": "H = \u2207z\u2207Tz log p\u03b8(z|x) = \u2207z\u2207Tz log p\u03b8(x, z)", "text": "In this section the following abbreviation is used: L = log p\u03b8 (x, z) (sum of all factors) z = the variable y = z's parents to be repaired L (z) = log p\u03b8 (z | y) (z's factor) L (\\ z) = L \u2212 L (z) (all factors minus z's factor) L (z \u2192) = the factors of z's parents\u03b1 = \u2202 2L (\\ z) \u2202 yi \u2202 yi\u03b2 = \u2202 2L (z \u2192) \u2202 z \u2202 z"}, {"heading": "4.1. Squared Correlations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1. CENTERED CASE", "text": "In the CP case, the relevant Hessian elements are as follows: Hyiyi = \u2202 2L \u2202 yi = \u03b1 + \u2202 2L (z) \u2202 yi = \u03b1 \u2212 w2i / \u03c32Hzz = \u2202 2L \u2202 z = \u03b2 + \u0445 2L (z) \u2202 z = \u03b2 \u2212 1 / \u03c32Hyiz = \u0445 2L \u2202 yi \u2202 z = \u2202 2L (z) \u2202 yi \u2202 z = wi / \u03c3 2 (11) Therefore, the square correlation between yi and z using equivalent (10) is: \u03c12yi, z = (Hyiz) 2HyiyiHzz = w2i / \u03c3 4 (\u03b1 \u2212 w2i / \u03c32) (\u03b2 \u2212 1 / \u03c32) (12)"}, {"heading": "4.1.2. NON-CENTERED CASE", "text": "In the DNCP case, the Hessian elements are: Hyiyi = \u2202 2L \u2202 yi = \u03b1 + \u2202 yi \u2202 L (z \u2192) \u2202 yi = \u03b1 + \u2202 yi (wi \u2202 L (z \u2192) \u2202 z) = \u03b1 + w2i \u03b2H = \u2202 2L \u2202 = \u2202 2L (z \u2192) \u2202 + \u2202 2 log p () \u2202 = \u03c32\u03b2 \u2212 1Hyi = \u2202 2L \u2202 yi \u2202 = \u03c3wi\u03b2 (13) The square correlation between yi and is therefore: \u03c12yi, = (Hyi) 2HyiyiH = \u03c32w2i \u03b2 2 (\u03b1 + w2i \u03b2) (\u03c3 2\u03b2 \u2212 1) (14)"}, {"heading": "4.2. Correlation inequality", "text": "Assuming that \u03b1 < 0 and \u03b2 < 0 (i.e. L (\\ z) and L (z \u2192) are concave, e.g. exponential families): \u03c12yi, z > \u03c1 2 yi, w2i / \u03c3 4 (\u03b1 \u2212 w2i / \u03c32) (\u03b2 \u2212 1 / \u03c32) > \u03c32w2i \u03b2 2 (\u03b1 \u2212 w2i \u2212 1) w2i / \u03c32) (\u03b2 \u2212 w2i \u03b2 2) (\u03b2 \u2212 1 / \u03c32) 1 / \u03c34 (\u03b1 \u2212 w2i / \u03c32) > \u03b22 (\u03b1 \u2212 w2i / \u03c32) > \u03b22 (\u03b2 \u2212 w2i / \u03c32) > \u03b22 (\u03b2 \u2212 w2i \u03b2) 1 / \u03b22 (\u03b1 \u2212 w2i \u03b2) 1 / \u03c34 (\u03b1 \u2212 w2i / \u03c32) > \u03b22 (\u03b1 \u2212 w2i \u03b2), we have thus shown the surprising fact that the correlation of inequality assumes an extremely simple form, in which parental values are then dependent on the relative characteristics of the children and then are not dependent on inequality."}, {"heading": "4.3. A beauty-and-beast pair", "text": "Further insights into the properties of CP and DNCP can be gained by extending the limits of the quadratic correlations (12) and (14). The limiting behavior of these correlations can be seen in Table 1. As becomes clear in these limits, CP and DNCP often form a beauty and beast pair: if the posterior correlations are high in one parameterization, they are low in the other. This is especially true for the limits of \u03c3 \u2192 0 and \u03b2 \u2192 \u2212 \u221e, where the quadratic correlations converge to either 0 or 1, so that the posterior inference is extremely inefficient in either CP or DNCP, but efficient in the other. This difference in the form of the log posterior is illustrated in Figure 3."}, {"heading": "4.4. Example: Simple Linear Dynamical System", "text": "Let's take a simple model with scalar latent variables z1 and z2 and scalar observed variables x1 and x2. The common PDF is defined as p (x1, x2, z1, z2) = p (z1) p (x1 | z1) p (z2 | z1) p (x2 | z2), where p (z1) = N (0, 1), p (x1 | z1) = N (z1, \u03c32x), p (z2 | z1) and p (x2 | z2) = N (z2, \u03c32x). Note that the parameter \u03c3z determines the dependence between the latent variables and \u03c3x determines the dependence between latent and observed variables. Let's repair z2 so that it is conditionally deterministic when there is a new auxiliary variable."}, {"heading": "5. Related work", "text": "This is, to the best of our knowledge, the first paper to investigate the effects of different differentiable non-centered parameters on the efficiency of grade-based inference. However, the topic of centered vs. non-centered parameterization for efficient (non-gradient-based) Gibbs sampling was discussed in the work of Papaspiliopoulos et al. (2003; 2007), which also discussed some strategies for constructing parameterization for these cases. There were some publications for parameterization of specific models; (Gelfand et al., 1995), discussed parameterization of mixed models, and (Meng & Van Dyk, 1998) examined several rules for selecting suitable parameterization for mixed effect models for faster EM. In the specific case where Gibbs sam-pling is tractable, efficient sampling is possible by interacting between centered and non-centered parameterizations."}, {"heading": "6. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Nonlinear DBN", "text": "From the derived posterior correlations in the previous sections, we can conclude that, depending on the parameters of the model, the posterior sampling can be extremely inefficient in one parameterization, while it is efficient in the other. If the parameters are known, the best parameters (i.e. posterior correlations) can be selected based on correlation inequality (15). In practice, model parameters are often subject to changes, e.g. when optimizing the parameters with Monte Carlo EM; in these situations where there is uncertainty about the value of the model parameters, it is impossible to select the best parameter risks in advance. The beauty-beast duality in Section 4.3 indicates a solution in the form of a very simple sampling strategy: let us mix the two parameters. Let QCP (z \u2032 | z) base the distribution of the MCMC / HMC suggestions on z (z | x)."}, {"heading": "6.2. Generative multilayer neural net", "text": "As explained in Section 3.4, a hierarchical model in DNCP form can be learned with the help of an MC probability estimator, which can be differentiated and optimized. We compare this Maximum Monte Carlo Likelihood (MMCL) method with the MCEM method for learning the parameters of a 4-layer hierarchical model of the MNIST dataset, where x | z3 \u0445 Bernoulli (sigmoid (Wxz3 + bx) and zt | zt \u2212 1 \u0445 N (tanh (Wizt \u2212 1 + bi), \u03c32ztI) are used. For MCEM, we used HMC with 10 lepfrog steps followed by a weight update using adagrad (Duchi et al., 2010). For MMCL, we used L {10, 100, 500}."}, {"heading": "7. Conclusion", "text": "We have shown how Bayesian networks are related to continuous latent variables and generative neural networks by two different parameterizations of the latent variables: CP and DNCP. A key finding is that differentiable, non-centered parameterization (DNCP) is preferred to a latent variable with regard to its effect on decreased posterior correlations when the variable is more strongly linked to its parents than to its children. Furthermore, we have shown through theoretical analysis that the two parameterizations complement each other: if the posterior correlations are large in one form, they are small in the other."}, {"heading": "Acknowledgments", "text": "The authors thank the reviewers for their excellent feedback and Joris Mooij, Ted Meeds and Taco Cohen for valuable discussions and suggestions."}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1305.2982,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "\u00c9ric"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Nonlinear deterministic relationships in Bayesian networks. In Symbolic and Quantitative Approaches to Reasoning with Uncertainty", "author": ["Cobb", "Barry R", "Shenoy", "Prakash P"], "venue": null, "citeRegEx": "Cobb et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cobb et al\\.", "year": 2005}, {"title": "Sample-based non-uniform random variate generation", "author": ["Devroye", "Luc"], "venue": "In Proceedings of the 18th conference on Winter simulation,", "citeRegEx": "Devroye and Luc.,? \\Q1986\\E", "shortCiteRegEx": "Devroye and Luc.", "year": 1986}, {"title": "Hybrid Monte Carlo", "author": ["Duane", "Simon", "Kennedy", "Anthony D", "Pendleton", "Brian J", "Roweth", "Duncan"], "venue": "Physics letters B,", "citeRegEx": "Duane et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Duane et al\\.", "year": 1987}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Variational learning in nonlinear Gaussian belief networks", "author": ["Frey", "Brendan J", "Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Frey et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Frey et al\\.", "year": 1999}, {"title": "Efficient parameterisations for normal linear mixed models", "author": ["AE Gelfand", "SK Sahu", "Carlin", "BP"], "venue": null, "citeRegEx": "Gelfand et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Gelfand et al\\.", "year": 1995}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo", "author": ["Hoffman", "Matthew D", "Gelman", "Andrew"], "venue": "arXiv preprint arXiv:1111.4246,", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Markov chain Monte Carlo in practice: A roundtable discussion", "author": ["Kass", "Robert E", "Carlin", "Bradley P", "Gelman", "Andrew", "Neal", "Radford M"], "venue": "The American Statistician,", "citeRegEx": "Kass et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kass et al\\.", "year": 1998}, {"title": "Fast gradient-based inference with continuous latent variable models in auxiliary form", "author": ["Kingma", "Diederik P"], "venue": "arXiv preprint arXiv:1306.0733,", "citeRegEx": "Kingma and P.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and P.", "year": 2013}, {"title": "Auto-Encoding Variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoff"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning with marginalized corrupted features", "author": ["Maaten", "Laurens", "Chen", "Minmin", "Tyree", "Stephen", "Weinberger", "Kilian Q"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Maaten et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2013}, {"title": "Fast EM-type implementations for mixed effects models", "author": ["Meng", "X-L", "Van Dyk", "David"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Meng et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Meng et al\\.", "year": 1998}, {"title": "Expectation propagation for approximate bayesian inference", "author": ["Minka", "Thomas P"], "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Minka and P.,? \\Q2001\\E", "shortCiteRegEx": "Minka and P.", "year": 2001}, {"title": "Probabilistic inference using Markov Chain Monte Carlo methods", "author": ["Neal", "Radford M"], "venue": null, "citeRegEx": "Neal and M.,? \\Q1993\\E", "shortCiteRegEx": "Neal and M.", "year": 1993}, {"title": "Non-centered parameterisations for hierarchical models and data augmentation", "author": ["Papaspiliopoulos", "Omiros", "Roberts", "Gareth O", "Sk\u00f6ld", "Martin"], "venue": "In Bayesian Statistics 7: Proceedings of the Seventh Valencia International Meeting,", "citeRegEx": "Papaspiliopoulos et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Papaspiliopoulos et al\\.", "year": 2003}, {"title": "A general framework for the parametrization of hierarchical models", "author": ["Papaspiliopoulos", "Omiros", "Roberts", "Gareth O", "Sk\u00f6ld", "Martin"], "venue": "Statistical Science,", "citeRegEx": "Papaspiliopoulos et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Papaspiliopoulos et al\\.", "year": 2007}, {"title": "Reverend Bayes on inference engines: A distributed hierarchical approach", "author": ["Pearl", "Judea"], "venue": "Cognitive Systems Laboratory, School of Engineering and Applied Science,", "citeRegEx": "Pearl and Judea.,? \\Q1982\\E", "shortCiteRegEx": "Pearl and Judea.", "year": 1982}, {"title": "Causality: models, reasoning and inference, volume 29", "author": ["Pearl", "Judea"], "venue": null, "citeRegEx": "Pearl and Judea.,? \\Q2000\\E", "shortCiteRegEx": "Pearl and Judea.", "year": 2000}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["Raiko", "Tapani", "Valpola", "Harri", "LeCun", "Yann"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Raiko et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "Stochastic back-propagation and variational inference in deep latent gaussian models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Learning stochastic feedforward neural networks", "author": ["Tang", "Yichuan", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "The art of data augmentation", "author": ["Van Dyk", "David A", "Meng", "Xiao-Li"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Dyk et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dyk et al\\.", "year": 2001}, {"title": "A Monte Carlo implementation of the EM algorithm and the poor man\u2019s data augmentation algorithms", "author": ["Wei", "Greg CG", "Tanner", "Martin A"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Wei et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Wei et al\\.", "year": 1990}, {"title": "To Center or Not to Center: That Is Not the Question\u2013An Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency", "author": ["Yu", "Yaming", "Meng", "Xiao-Li"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "In high-dimensional spaces, gradient-based samplers such as Hybrid Monte Carlo (Duane et al., 1987) and the recently proposed noU-turn sampler (Hoffman & Gelman, 2011) are known for their relatively fast mixing properties.", "startOffset": 79, "endOffset": 99}, {"referenceID": 4, "context": "If one is interested in sampling from the posterior then the gradients can be plugged into a gradient-based sampler such as Hybrid Monte Carlo (Duane et al., 1987); if also interested in learning parameters, the resulting samples can be used for the E-step in Monte Carlo EM (Wei & Tanner, 1990) (MCEM).", "startOffset": 143, "endOffset": 163}, {"referenceID": 13, "context": "(Krizhevsky et al., 2012; Goodfellow et al., 2013; Bengio, 2013)).", "startOffset": 0, "endOffset": 64}, {"referenceID": 8, "context": "For example, in (Hinton et al., 2012) a \u2019dropout\u2019 regularization method is introduced where (in its basic ver-", "startOffset": 16, "endOffset": 37}, {"referenceID": 14, "context": "5), and where the parameters are learned by following the gradient of the log-likelihood lower bound: \u2207\u03b8E [ log p\u03b8(t (i)|x(i), ) ] ; this gradient can sometimes be computed exactly (Maaten et al., 2013) and can otherwise be approximated with a Monte Carlo estimate (Hinton et al.", "startOffset": 181, "endOffset": 202}, {"referenceID": 8, "context": ", 2013) and can otherwise be approximated with a Monte Carlo estimate (Hinton et al., 2012).", "startOffset": 70, "endOffset": 91}, {"referenceID": 23, "context": "(Frey & Hinton, 1999; Rezende et al., 2014; Tang & Salakhutdinov, 2013) applying (partially) MCMC or (partically) factorized variational approaches to modelling the posterior.", "startOffset": 0, "endOffset": 71}, {"referenceID": 7, "context": "There have been some publications for parameterizations of specific models; (Gelfand et al., 1995), for example, discusses parameterizations of mixed models, and (Meng & Van Dyk, 1998) investigate several rules for choosing an appropriate parameterization for mixed-effects models for faster EM.", "startOffset": 76, "endOffset": 98}, {"referenceID": 22, "context": "Recently, (Raiko et al., 2012) analyzed the elements of the Hessian w.", "startOffset": 10, "endOffset": 30}, {"referenceID": 10, "context": "(Kass et al., 1998) for a discussion on ESS.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "For MCEM, we used HMC with 10 leapfrog steps followed by a weight update using Adagrad (Duchi et al., 2010).", "startOffset": 87, "endOffset": 107}], "year": 2015, "abstractText": "Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the non-centered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.", "creator": "LaTeX with hyperref package"}}}