{"id": "1402.0555", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits", "abstract": "We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes an action in response to the observed context, observing the reward only for that action. Our method assumes access to an oracle for solving cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only $\\tilde{O}(\\sqrt{T})$ oracle calls across all $T$ rounds. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines.", "histories": [["v1", "Tue, 4 Feb 2014 00:48:29 GMT  (37kb)", "http://arxiv.org/abs/1402.0555v1", null], ["v2", "Tue, 14 Oct 2014 01:41:47 GMT  (39kb)", "http://arxiv.org/abs/1402.0555v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alekh agarwal", "daniel j hsu", "satyen kale", "john langford", "lihong li", "robert e schapire"], "accepted": true, "id": "1402.0555"}, "pdf": {"name": "1402.0555.pdf", "metadata": {"source": "CRF", "title": "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits", "authors": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 2.05 55v1 [cs.LG] \u221a T) oracle calls in all T rounds. In this way, we obtain the most practical context-dependent bandit learning algorithm using approaches suitable for general political classes. Furthermore, we conduct a proof-of-concept experiment demonstrating the excellent computing and prediction performance (an online variant) of our algorithm compared to several baselines."}, {"heading": "1 Introduction", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "2 Preliminaries", "text": "In this section, we recall the i.i.d. contextual bandit setting and some basic techniques used in previous works (Auer et al., 2002; Beygelzimer et al., 2011; Dud \u0301 \u0131k et al., 2011a)."}, {"heading": "2.1 Learning Setting", "text": "Let A be a finite number of K-actions, X a room of possible contexts (e.g. a feature room), and D-AX a finite set of measures that map the contexts x-X to actions a-A.3 Let RA +: = Q-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-"}, {"heading": "2.2 Inverse Propensity Scoring", "text": "An unbiased estimate of a policy's reward can be derived from a history of interaction records Ht = reward marker (IPS; also referred to as inverse probability weighting). (1) The expected reward of a policy is estimated using standard arguments. (4) We have defined empirical cumulative regret as relative to \u03c0 (ai) rather than empirical reward maximizerargmax\u03c0 (ai). (4) We have defined empirical cumulative regret as relative to \u03c0 rather than empirical reward maximizerargmax\u03c0 (ai). (4) This technique can be regarded as mapping Ht = 1 rt (xt). (4) In the i.i.d. setting, however, the two do not differ by more than O (V ln), but by a biometric reward (N / 3), with the probability of at least 1 \u2212 pyrical reward max.This technique can be considered a mapping."}, {"heading": "2.3 Optimization Oracle", "text": "In this work, we only access it via an optimization oracle corresponding to a cost-sensitive learner. In the following (Dud'\u0131k et al., 2011a), we refer to this oracle as AMO5.Definition 1. For a number of strategies, the Argmax oracle (AMO) is an algorithm that applies to each sequence of context and reward vectors (x1, r1), (x2, r2),..., (xt, rt), (X), (RA +, return-sargmax."}, {"heading": "2.4 Projections and Smoothing", "text": "In each round, our algorithm selects an action by randomly drawing a policy \u03c0 from a distribution over \u0445 and then selecting the action \u03c0 (x) recommended by \u03c0 from the current context x. This corresponds to drawing an action according to Q (a | x): = \u2211 \u03c0: \u03c0 (x) = aQ (\u03c0), \u0395a \u27e9 A. To keep the variance of the reward estimates from IPS in check, it is desirable to prevent the probability of an action being too low. Therefore, as in previous work, we also use a smoothed projection Q\u00b5 (\u00b7 x) for \u00b5 [0, 1 / K], Q\u00b5 (a | x): = (1 \u2212 K\u00b5)."}, {"heading": "3 Algorithm and Main Results", "text": "Our algorithm (ILOVETOCONBANDITS) is an epoch-based variant of the RandomizedUCB algorithm by Dud \u0301 \u0131k et al. (2011a) and is specified in algorithm 1, i.e., only in certain pre-specified rounds \u03c41, \u03c42,... The only requirement of the epochal plan is that the length of the epochal possibility is limited by (step 7), but does so on an epochal schedule, i.e., only on certain pre-specified rounds \u03c41, \u03c42,... The only requirement of the epochal plan is that the length of the epochal possibility be limited by (step 7), namely on an epochal plan, i.e., only on certain pre-specified rounds \u03c41, \u04211, \u04212,."}, {"heading": "3.1 Solving (OP) via Coordinate Descent", "text": "The pseudo code is given in algorithm 2. Our analysis, as well as the algorithm itself, is based on a potential function that we use to measure progress. The algorithm can be regarded as a form of coordinate origin that is applied to the same potential function. The main idea of our analysis is to show that this function decreases significantly with each iteration of this algorithm; since the function is not negative, there is an upper limit to the total number of iterations as expressed in the following theorem.Theorem 3. Algorithm 2 (with Qinit: = 0) stops in \u2264 4 ln (1 / (K\u00b5)) \u00b5 iterations and gives a solution to Q (OP)."}, {"heading": "3.2 Using an Optimization Oracle", "text": "We show how to implement algorithm 2 via AMO (c.f. section 2,3 = Q > Q (Q). (Lemma 1. Algorithm 2 can be implemented with a call to AMO before the loop is started, and a call for each iteration of the loop thereafter. (Algorithm 2: Coordinated descent Algorithm Require: History Ht, Minimum probability \u00b5, Initial weights Qinit. (sentence Q: = Qinit.) 2: loop3: Define, for all risk points. (Q) = E-Commerce Algorithm Require: History Ht, Minimum probability \u00b5, Initial weights Qinit, Initial weights Qinit. (sentence 3: Qinit.) 2: Define, for all risk programs. (Q) = E-Proceptions. (Q) = E-Proceptors ceptors (1 / Q)."}, {"heading": "3.3 Epoch Schedule", "text": "Theorem 3 shows that algorithm 2 (OP) with O-calls (\u221a t) to AMO in round t. Thus, if we use the epoch plan \u03c4m = m (i.e., execute algorithm 2 in each round), we receive a total of O-calls (T 3 / 2) to AMO over all T-rounds. This number can be drastically reduced by a carefully selected epoch schedule. Lemma 2. For the epoch plan \u03c4m: = 2 m \u2212 1, the total number of calls to AMO O O O-calls O-calls (\u221a KT) is proof. The epoch plan fulfils the requirement \u03c4m + 1 \u2264 2\u03c4m. With this epoch plan, algorithm 2 is only executed O-calls (log T) over T-calls, which leads to O-calls (\u221a KT) over the entire period."}, {"heading": "3.4 Warm Start", "text": "We now introduce another technique to reduce the number of calls to AMO. This is based on the observation that, practically speaking, it seems terribly wasteful to throw out the results of all previous calculations at the beginning of a new epoch and start again from scratch. Instead, we intuitively expect the calculations to be more moderate when we start again from where we left off last, namely, with a \"warm start\" approach. Here, when Algorithm 2 is called at the end of epoch m, we use Qinit: = Qm \u2212 1 (the previously calculated weights) instead of 0.We can combine warm start with another epoch plan to guarantee O-KT total calls to AMO that are distributed among O-T calls (assuming T) to algorithm 2.Lemma 3. Define the epoch plan (previously calculated weights): = (3, 5) and icm: = m-2 for calls to AMO that are pre-recorded (assuming T calls to algorithm 2.Lemma suffices)."}, {"heading": "3.5 A Lower Bound on the Support Size", "text": "So far, we have seen several ways to solve the optimization problem (OP), with corresponding limits on the number of calls to Q (Q). An attractive feature of the coordinated parentage algorithm, algorithm 2, is that the number of oracle calls is directly related to the number of calls to support Qm. Specifically, the total number of oracle calls for the warm-start approach in Section 3.4, Theorem 3, means that the total number of calls that ever have a non-zero weight across all T rounds never deviates from 0. The size of support for the distributions Qm in algorithm 1 is critical to the computational complexity of sampling a measure (step 4 of algorithm 1). & ltm shows that it is not possible to construct a significant problem Qm in algorithm 1 that also low defines the quantity."}, {"heading": "4 Regret Analysis", "text": "In this section, we outline the regret analysis for our ILOVETOCONBANDITS algorithm, deferring details in Appendix B and Appendix C. Deviations in the political reward are controlled by the variance of the individual terms in Equation (1): essentially the left side of Equation (3) of (OP), with the exception of E-x-x-Ht [\u00b7], which is replaced by Ex-x-Ht [\u00b7]. Solving this discrepancy is done using deviation limits, so that Equation (3) applies to Ex-X [\u00b7], with worse right constants. The rest of the analysis, which deviates from the RandomizedUCB [\u00b7], compares the expected regret Reg (\u03c0) of any policy measure with the estimated regret R-egt (\u03c0) using the deviation limitations in Equation (3). \u2212 With high probability."}, {"heading": "5 Analysis of the Optimization Algorithm", "text": "In this section we give a sketch of the analysis of our most important optimization algorithms for calculating Qm at each epoch as in Algorithm 2. As mentioned in Section 3.1, this analysis is based on a possible mode of operation. Since our attention is currently based on a single epoch, here and in what follows when it is clear that we are dropping m out of our notation and simply writing what we need to do. Let UA be put the even distribution over the action. \"(RE).\" (RE). \"The function in Eq.\" (6) is defined for all vectors Q. \"(RE). (RE) denotes the unnormalized relative entropy between two non-vectors and QQ.\""}, {"heading": "5.1 Epoching and Warm Start", "text": "In order to reduce the number of oracle calls, one approach is the \"double trick\" of section 3.3, which allows us to bind the total number of iterations of algorithm 2 in the first T rounds, which means that the oracle is called far less than once per round, at a vanishingly low rate. We now turn to the warm-start approach of section 3.4, where in each epoch m + 1 we initialize the coordinate algorithms with Qinit = Qm, i.e. the weights compressed in the previous epoch."}, {"heading": "6 Experimental Evaluation", "text": "In this section, a variant of algorithm 1 is evaluated against several baselines. While algorithm 1 is significantly more efficient than many other approaches, the totality of the computational complexity is still at least as complex as the way in which it is about the distribution of money. (5) It seems as if it is about the complexity of an ordinary superordinate learning process, in which it is about the way in which it is about the way in which it is about the distribution of money. (5) It seems as if it is about the complexity of an ordinary superordinate learning process, in which it is typically possible to obtain a fresh example of a fresh example. (5) A natural solution is to be used to use an \"online\" oracle \"oracle,\" which is an example of an example that is, of an ordinary superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinately, superordinately, superordinarily, superordinately, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, superordinarily, oracle, superordinarily, oracle, superordinarily, \""}, {"heading": "7 Conclusions", "text": "In this paper, we have presented the first practical algorithm of our knowledge that achieves the statistically optimal repentance guarantee and is mathematically efficient in defining general policy classes. A notable feature of the algorithm is that the total number of oracle calls across all T-rounds is sublinear - a remarkable improvement over previous work in this field. We believe that the online variant of the approach we implemented in our experiments has the right practical touch for a scalable solution to the contextual bandit problem. In the future, it would be interesting to directly analyze the online cover algorithm."}, {"heading": "A Omitted Algorithm Details", "text": "Algorithm 3 and Algorithm 4 specify the details of the inverse propensity scoring transformation IPS and the action sample method."}, {"heading": "B Deviation Inequalities", "text": "B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B."}, {"heading": "C Regret Analysis", "text": "Throughout this section, we fix the allowable probability of failure. (0, 1) This is to be understood as input to the algorithm, as is the epoch table. (12) and remember that if this is not the case. (12) Note that dt / t is not the case. (16) This is not the case. (16) Note that dt / t is not the case. (16) Note that dt / t is not the case. (16) Note that dt / t is not the case. (16) Note that dt / t is not the case. (16) Note that dt / t is the case. (16)"}, {"heading": "D Details of Optimization Analysis", "text": "(1) Proof of Lemma 5After executing step 4, we must Q = Q = Q (Q) (2K + b\u03c0) \u2264 2K (24) This is because if the condition in step 7 is not correct, then Eq (24) already applies. Otherwise, Q (2) is replaced by Q (= cQ), and for this set of weights, Eq. (24) actually applies with equality. Note that since all quantities are not negative, Eq. (24) immediately applies both Eq. (2), and that Qi. (3) Qi. (1) Furthermore, at the point at which the algorithm holds in step 10, it must be so that for all policies, D\u03c0 (Q) \u2264 0. Definitions we can see that this is exactly equivalent to Eq. (3) Qi. (2) Qi."}, {"heading": "E Proof of Theorem 4", "text": "Remember the earlier definition of the low variance distribution setQm = Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "F Online Cover algorithm", "text": "This section describes the pseudo-code of the precise use of the algorithm in our experiments. The minimum exploration probability \u00b5 was set for our evaluation as 0.05 min (1 / K, 1 / \u221a tK). Algorithm 5 Online Coverinput Cover size n, minimum scanning probability \u00b5. 1: Initialize cost-sensitive online minimization oracles O1, O2,..., On, each of which controls a policy\u03c0 (1), \u03c0 (2),.., \u03c0 (n); U: = uniform probability distribution across these guidelines. 2: for round t = 1, 2,... do 3: Note the context xt x. 4: (at, pt (at): = Sample (xt, U, \u2205, \u00b5). 5: Select action and observe reward rt (at) rt (at) [0, 1]. 6: for each i = 1, 2,."}], "references": [{"title": "The nonstochastic multiarmed", "author": ["Research", "2002. Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": null, "citeRegEx": "Research et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Research et al\\.", "year": 2002}, {"title": "An empirical evaluation of Thompson sampling", "author": ["cross-validation. In COLT", "1999. Olivier Chapelle", "Lihong Li"], "venue": null, "citeRegEx": "COLT et al\\.,? \\Q2011\\E", "shortCiteRegEx": "COLT et al\\.", "year": 2011}, {"title": "Predicting nearly as well as the best pruning of a decision", "author": ["2011b. David P. Helmbold", "Robert E. Schapire"], "venue": null, "citeRegEx": "Helmbold and Schapire.,? \\Q2011\\E", "shortCiteRegEx": "Helmbold and Schapire.", "year": 2011}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "Langford and Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2007}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Generalized Thompson sampling for contextual bandits", "author": ["Lihong Li"], "venue": "CoRR, abs/1310.7163,", "citeRegEx": "Li.,? \\Q2013\\E", "shortCiteRegEx": "Li.", "year": 2013}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E. Schapire"], "venue": "In WWW,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Tighter bounds for multi-armed bandits with expert advice", "author": ["H. Brendan McMahan", "Matthew Streeter"], "venue": "In COLT,", "citeRegEx": "McMahan and Streeter.,? \\Q2009\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2009}, {"title": "Reinforcement learning, an introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R. Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}], "referenceMentions": [{"referenceID": 7, "context": "The strongest known results (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) provide an algorithm that carefully controls the exploration distribution to achieve an optimal regret after T rounds of", "startOffset": 28, "endOffset": 101}, {"referenceID": 8, "context": "1), it is possible to create \u01eb-greedy (Sutton and Barto, 1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |\u03a0|) with only a single call to the oracle per round.", "startOffset": 38, "endOffset": 62}, {"referenceID": 3, "context": "1), it is possible to create \u01eb-greedy (Sutton and Barto, 1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |\u03a0|) with only a single call to the oracle per round.", "startOffset": 79, "endOffset": 105}, {"referenceID": 3, "context": "1), it is possible to create \u01eb-greedy (Sutton and Barto, 1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |\u03a0|) with only a single call to the oracle per round. However, these algorithms have suboptimal regret bounds of O((K log |\u03a0|)1/3T ) because the algorithms randomize uniformly over actions when they choose to explore. The RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) achieves the optimal regret bound (up to logarithmic factors) in the i.", "startOffset": 80, "endOffset": 411}, {"referenceID": 7, "context": "The EXP4-family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) solve the contextual bandit problem with optimal regret by updating weights (multiplicatively) over all policies in every round.", "startOffset": 30, "endOffset": 103}, {"referenceID": 8, "context": "This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007).", "startOffset": 108, "endOffset": 132}, {"referenceID": 3, "context": "This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007).", "startOffset": 150, "endOffset": 176}, {"referenceID": 9, "context": "Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011).", "startOffset": 76, "endOffset": 102}, {"referenceID": 5, "context": "Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011).", "startOffset": 76, "endOffset": 102}, {"referenceID": 2, "context": "Except for a few special cases (Helmbold and Schapire, 1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in the number of policies. In contrast, the RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) is based on a natural abstraction from supervised learning\u2014the ability to efficiently find a function in a rich function class that minimizes the loss on a training set.", "startOffset": 32, "endOffset": 251}, {"referenceID": 2, "context": "Except for a few special cases (Helmbold and Schapire, 1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in the number of policies. In contrast, the RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) is based on a natural abstraction from supervised learning\u2014the ability to efficiently find a function in a rich function class that minimizes the loss on a training set. This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007). However, these algorithms have only suboptimal regret bounds. Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011). Such algorithms, as well as the closely related upper-confidence bound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the posterior distribution over policies can be efficiently maintained or approximated. In our experiments, we compare to a strong baseline algorithm that uses this approach (Chu et al., 2011). To circumvent the \u03a9(|\u03a0|) running time barrier, we restrict attention to algorithms that only access the policy class via the optimization oracle. Specifically, we use a cost-sensitive classification oracle, and a key challenge is to design good supervised learning problems for querying this oracle. The RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) uses a similar oracle to construct a distribution over policies that solves a certain convex program.", "startOffset": 32, "endOffset": 1606}, {"referenceID": 2, "context": "Except for a few special cases (Helmbold and Schapire, 1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in the number of policies. In contrast, the RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) is based on a natural abstraction from supervised learning\u2014the ability to efficiently find a function in a rich function class that minimizes the loss on a training set. This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007). However, these algorithms have only suboptimal regret bounds. Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011). Such algorithms, as well as the closely related upper-confidence bound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the posterior distribution over policies can be efficiently maintained or approximated. In our experiments, we compare to a strong baseline algorithm that uses this approach (Chu et al., 2011). To circumvent the \u03a9(|\u03a0|) running time barrier, we restrict attention to algorithms that only access the policy class via the optimization oracle. Specifically, we use a cost-sensitive classification oracle, and a key challenge is to design good supervised learning problems for querying this oracle. The RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) uses a similar oracle to construct a distribution over policies that solves a certain convex program. However, the number of oracle calls in their work is prohibitively large, and the statistical analysis is also rather complex. Throughout this paper, we use the \u00d5 notation to suppress dependence on logarithmic factors. The paper of Dud\u0301\u0131k et al. (2011a) is colloquially referred to, by its authors, as the \u201cmonster paper\u201d (Langford, 2014).", "startOffset": 32, "endOffset": 1962}, {"referenceID": 4, "context": ", 2011b) on the CCAT document classification problem in RCV1 (Lewis et al., 2004).", "startOffset": 61, "endOffset": 81}, {"referenceID": 8, "context": "\u01eb-greedy (Sutton and Barto, 1998) explores randomly with probability \u01eb and otherwise exploits.", "startOffset": 9, "endOffset": 33}, {"referenceID": 5, "context": "Another reasonable baseline is based on Thompson sampling (Chapelle and Li, 2011; Li, 2013) or linear UCB (Auer, 2002; Li et al.", "startOffset": 58, "endOffset": 91}, {"referenceID": 6, "context": "Another reasonable baseline is based on Thompson sampling (Chapelle and Li, 2011; Li, 2013) or linear UCB (Auer, 2002; Li et al., 2010), the latter being quite effective in past evaluations (Li et al.", "startOffset": 106, "endOffset": 135}, {"referenceID": 6, "context": ", 2010), the latter being quite effective in past evaluations (Li et al., 2010; Chapelle and Li, 2011).", "startOffset": 62, "endOffset": 102}], "year": 2014, "abstractText": "We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes an action in response to the observed context, observing the reward only for that action. Our method assumes access to an oracle for solving cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only \u00d5( \u221a T ) oracle calls across all T rounds. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines.", "creator": "LaTeX with hyperref package"}}}