{"id": "1402.0562", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Online Stochastic Optimization under Correlated Bandit Feedback", "abstract": "In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high confidence tree (HCT) algorithm, a novel any-time $\\mathcal X$-armed bandit algorithm, and derive regret bounds matching the performance of existing state-of-the-art in terms of dependency on number of steps $n$ and near-optimality dimensions $d$. The main advantage of HCT is that it handles the challenging case of correlated arms, whereas existing methods require that rewards to be conditionally independent of each others. HCT also improves on the state-of-the-art in terms of space complexity as well as requiring a weaker smoothness assumption on the mean-reward function in compare to the previous any time algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.", "histories": [["v1", "Tue, 4 Feb 2014 01:34:50 GMT  (527kb)", "https://arxiv.org/abs/1402.0562v1", null], ["v2", "Thu, 13 Feb 2014 20:50:52 GMT  (527kb)", "http://arxiv.org/abs/1402.0562v2", null], ["v3", "Mon, 19 May 2014 17:30:53 GMT  (527kb)", "http://arxiv.org/abs/1402.0562v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.SY", "authors": ["mohammad gheshlaghi azar", "alessandro lazaric", "emma brunskill"], "accepted": true, "id": "1402.0562"}, "pdf": {"name": "1402.0562.pdf", "metadata": {"source": "CRF", "title": "Online Stochastic Optimization under Correlated Bandit Feedback", "authors": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 2.05 62v3 [st at.M L] 19 M"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before we get a result like that."}, {"heading": "2 Preliminaries", "text": "The optimization problem is a measurable space of the arms. We formalize the optimization problem as an interaction between the learner and the environment."}, {"heading": "3 The High Confidence Tree algorithm", "text": "In fact, one can show that, under a certain mild premise, a near-optimal function is achieved, regardless of the size of the room. (http: / / www.hCT) Online stochastic optimization algorithm under bandit feedback. (1) During this discussion, a function evaluation is equivalent to the reward you get when you pull an arm (because an arm corresponds to the selection of a correlated feedback, where the reward from pulling an arm can depend on all previous arms and depends on the resulting results. (Alg.) 1, the structure of the algorithm for HCT-iid and HCT-i.2Note shows that in many cases it can be much smaller than D, the actual dimension of the arm space X in the ongoing case."}, {"heading": "4 Theoretical Analysis", "text": "In this section, we analyze the regret and complexity of HCT. All the evidence is included in the supplement."}, {"heading": "4.1 Regret Analysis", "text": "We start with reporting the maximum depth of the trees generated by HCT (0, 1)."}, {"heading": "4.2 Complexity", "text": "rE \"s tis rf\u00fc eid rf\u00fc ide f\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die for the R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die for the R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R"}, {"heading": "5 Application to Policy Search in MDPs", "text": "As we discussed in Section 2, HCT is designed to handle the very general case of optimization in problems where there is a strong correlation between the rewards, arm and contexts, in different time steps. An important subset of this general term is represented by the problem of political search in infinite horizons Markov decision-making processes. It is noteworthy that extending to the case of partially observable MDPs is easy as long as the POMDP meets some ergodicity premises. An MDP M is defined as a tuple < S, P > where S is the set of states in which the measures are determined, P: S \u00d7 A \u2192 M (S \u00d7 M) \u2192 M (S) is the transition phase of each state-action pair to a distribution over states and rewards. A (stochastic) policy: S \u2192 M (A) is an assignment of states for distribution over measures."}, {"heading": "6 Numerical Results", "text": "In fact, it is in such a way that it is about a way in which one sees oneself in a position to outwit and outdo oneself. (...) In fact, it is in such a way that it is about a way in which one puts oneself and others in the centre. (...) In fact, it is in such a way that it is about a way in which one sees oneself in a position to outdo oneself. (...) It is in such a way that it is about a way in which one sees oneself and oneself in a position. (...) \"It is in such a way as if one sees oneself in a way in which one is about oneself.\" (...) It is in such a way that it is about a way in which it is about. (...) \"It is in such a way that it is about. (...) It is in such a way that it is about, in such a way that it is about. (...) It is in such a way that it is about. (...) It is in such a way that it is about, in such a way that it is about. (...)."}, {"heading": "7 Discussion and Future Work", "text": "In the current version of the HCT, we assume that the learner has access to information about the smoothness of the f (x) function and the mixing time. In many cases, this information is not available to the learner. In the future, it would be interesting to build on the previous work, which deals with unknown smoothness in iid settings and extends it to correlated feedback. For example, Bubeck et al. (2011b) requires a stronger global Lipschitz assumption and suggests an algorithm to estimate the Lipschitz constant. Other work on the iid setting includes Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but use only the mild local smoothing assumption that we define in Asm. 4, and do not require knowledge of the dissimilarity measurement. On the other hand, Slivkins et al. (2011) and Bull (2013) research the cumulative regret, but have introduced a different zooming concept to the smoothness of the small mountain."}, {"heading": "A Proof of Thm. 1", "text": "We begin with the introduction of an additional notation required to analyze both algorithms. - We designate the indicator function of an event E by IE. - For all 1 \u2264 h \u2264 H (t) and t > 0, which comprise only the internal nodes (i.e., nodes that are not sheets), which correspond to the nodes at depth h that were expanded before time t. - We denote the subset of Ih (t), which are only the internal nodes (i.e., nodes that are not sheets), which correspond to the nodes at depth h that were expanded before time t. - At each time we denote by (ht, it) the subset of Ih (t), i) we define the set of time steps when (h, i) which are called Ch, i: = {t = {t = 1,."}, {"heading": "B Correlated Bandit feedback", "text": "We begin by analyzing the HCT results by demonstrating some useful concentration imbalances for non-id random variables that we have selected when we re-select the number of selected episodes. (We begin by analyzing the HCT results for non-id random variables among the mix assumptions of Sect. (2.B.1) We derive from this a concentration imbalance for non-id random variables grouped into episodes. (In this section, the rewards from an arm x are not necessarily consecutive, but they are obtained over several episodes. This result is of independent interest, so we report on it first in its general form and apply it later to HCT results. In HCT results, once an arm is selected, it is pulled for a number of consecutive steps before it is re-selected."}], "references": [{"title": "Online learning in markov decision processes with adversarially chosen transition probability distributions", "author": ["Abbasi", "Yasin", "Bartlett", "Peter", "Kanade", "Varun", "Seldin", "Yevgeny", "Szepesvari", "Csaba"], "venue": "K.Q. (eds.), Advances in Neural Information Processing Systems", "citeRegEx": "Abbasi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abbasi et al\\.", "year": 2013}, {"title": "Improved rates for the stochastic continuum-armed bandit problem", "author": ["Auer", "Peter", "Ortner", "Ronald", "Szepesv\u00e1ri", "Csaba"], "venue": "In COLT, pp", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Regret bounds for reinforcement learning with policy advice", "author": ["Azar", "Mohammad Gheshlaghi", "Lazaric", "Alessandro", "Brunskill", "Emma"], "venue": "In ECML/PKDD,", "citeRegEx": "Azar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2013}, {"title": "Reinforcement learning in pomdp\u2019s via direct gradient ascent", "author": ["Baxter", "Jonathan", "Bartlett", "Peter L"], "venue": "In ICML, pp", "citeRegEx": "Baxter et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Baxter et al\\.", "year": 2000}, {"title": "Lipschitz bandits without the lipschitz constant", "author": ["Bubeck", "S\u00e9bastien", "Stoltz", "Gilles", "Yu", "Jia Yuan"], "venue": "In ALT,", "citeRegEx": "Bubeck et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2011}, {"title": "Adaptive-tree bandits", "author": ["Bull", "Adam"], "venue": "arXiv preprint arXiv:1302.2489,", "citeRegEx": "Bull and Adam.,? \\Q2013\\E", "shortCiteRegEx": "Bull and Adam.", "year": 2013}, {"title": "Regret and convergence bounds for immediate-reward reinforcement learning with continuous action spaces", "author": ["Cope", "Eric"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Cope and Eric.,? \\Q2009\\E", "shortCiteRegEx": "Cope and Eric.", "year": 2009}, {"title": "High dimensional gaussian process bandits", "author": ["Djolonga", "Josip", "Krause", "Andreas", "Cevher", "Volkan"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Djolonga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Djolonga et al\\.", "year": 2013}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Multi-armed bandits in metric spaces", "author": ["Kleinberg", "Robert", "Slivkins", "Aleksandrs", "Upfal", "Eli"], "venue": "In STOC,", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Policy search for motor primitives in robotics", "author": ["Kober", "Jens", "Peters", "Jan"], "venue": "Machine Learning,", "citeRegEx": "Kober et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2011}, {"title": "The sample-complexity of general reinforcement learning", "author": ["Lattimore", "Tor", "Hutter", "Marcus", "Sunehag", "Peter"], "venue": "In Proceedings of Thirtieth International Conference on Machine Learning (ICML),", "citeRegEx": "Lattimore et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lattimore et al\\.", "year": 2013}, {"title": "Empirical bernstein bounds and sample variance penalization", "author": ["Maurer", "Andreas", "Pontil", "Massimiliano"], "venue": "arXiv preprint arXiv:0907.3740,", "citeRegEx": "Maurer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maurer et al\\.", "year": 2009}, {"title": "Optimistic optimization of a deterministic function without the knowledge of its smoothness", "author": ["Munos", "R\u00e9mi"], "venue": "In NIPS, pp", "citeRegEx": "Munos and R\u00e9mi.,? \\Q2011\\E", "shortCiteRegEx": "Munos and R\u00e9mi.", "year": 2011}, {"title": "From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning", "author": ["Munos", "R\u00e9mi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Munos and R\u00e9mi.,? \\Q2013\\E", "shortCiteRegEx": "Munos and R\u00e9mi.", "year": 2013}, {"title": "Online regret bounds for undiscounted continuous reinforcement learning", "author": ["Ortner", "Ronald", "Ryabko", "Daniil"], "venue": "K.q. (eds.), Advances in Neural Information Processing Systems", "citeRegEx": "Ortner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ortner et al\\.", "year": 2012}, {"title": "Policy search: Any local optimum enjoys a global performance guarantee", "author": ["Scherrer", "Bruno", "Geist", "Matthieu"], "venue": "arXiv preprint arXiv:1306.1520,", "citeRegEx": "Scherrer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Scherrer et al\\.", "year": 2013}, {"title": "Contextual bandits with similarity information", "author": ["Slivkins", "Aleksandrs"], "venue": "CoRR, abs/0907.3986,", "citeRegEx": "Slivkins and Aleksandrs.,? \\Q2009\\E", "shortCiteRegEx": "Slivkins and Aleksandrs.", "year": 2009}, {"title": "Multi-armed bandits on implicit metric spaces", "author": ["Slivkins", "Aleksandrs"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Slivkins and Aleksandrs.,? \\Q2011\\E", "shortCiteRegEx": "Slivkins and Aleksandrs.", "year": 2011}, {"title": "Gaussian process bandits without regret: An experimental design approach", "author": ["Srinivas", "Niranjan", "Krause", "Andreas", "Kakade", "Sham M", "Seeger", "Matthias"], "venue": "CoRR, abs/0912.3995,", "citeRegEx": "Srinivas et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2009}, {"title": "PAC-bayes-empirical-bernstein inequality", "author": ["Tolstikhin", "Ilya O", "Seldin", "Yevgeny"], "venue": "In Advances in Neural Information Processing Systems, pp", "citeRegEx": "Tolstikhin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tolstikhin et al\\.", "year": 2013}, {"title": "Stochastic simultaneous optimistic optimization", "author": ["Valko", "Michal", "Carpentier", "Alexandra", "Munos", "R\u00e9mi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Valko et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Valko et al\\.", "year": 2013}, {"title": "Model-free reinforcement learning as mixture learning", "author": ["Vlassis", "Nikos", "Toussaint", "Marc"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Vlassis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Vlassis et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": "This immediately implies that the reward, conditioned on its corresponding arm pull, is not an independent and identically distributed (iid) random variable, in contrast to the prior work on X -armed bandits (Bull, 2013; Djolonga et al., 2013; Bubeck et al., 2011a; Srinivas et al., 2009; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).", "startOffset": 208, "endOffset": 343}, {"referenceID": 19, "context": "This immediately implies that the reward, conditioned on its corresponding arm pull, is not an independent and identically distributed (iid) random variable, in contrast to the prior work on X -armed bandits (Bull, 2013; Djolonga et al., 2013; Bubeck et al., 2011a; Srinivas et al., 2009; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).", "startOffset": 208, "endOffset": 343}, {"referenceID": 9, "context": "This immediately implies that the reward, conditioned on its corresponding arm pull, is not an independent and identically distributed (iid) random variable, in contrast to the prior work on X -armed bandits (Bull, 2013; Djolonga et al., 2013; Bubeck et al., 2011a; Srinivas et al., 2009; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).", "startOffset": 208, "endOffset": 343}, {"referenceID": 1, "context": "This immediately implies that the reward, conditioned on its corresponding arm pull, is not an independent and identically distributed (iid) random variable, in contrast to the prior work on X -armed bandits (Bull, 2013; Djolonga et al., 2013; Bubeck et al., 2011a; Srinivas et al., 2009; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).", "startOffset": 208, "endOffset": 343}, {"referenceID": 9, "context": "Our approach builds on recent advances in X -armed bandits for iid settings (Bubeck et al., 2011a; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).", "startOffset": 76, "endOffset": 153}, {"referenceID": 1, "context": "Our approach builds on recent advances in X -armed bandits for iid settings (Bubeck et al., 2011a; Cope, 2009; Kleinberg et al., 2008; Auer et al., 2007).", "startOffset": 76, "endOffset": 153}, {"referenceID": 9, "context": ", 2011a) and zooming algorithm (Kleinberg et al., 2008), both of which only apply to iid setting, in terms of dependency on the number of steps n and the near-optimality dimension d (to be defined later).", "startOffset": 31, "endOffset": 55}, {"referenceID": 4, "context": "Similar to the HOO algorithm of Bubeck et al. (2011a), HCT makes use of a covering binary tree for exploring the arm space.", "startOffset": 32, "endOffset": 54}, {"referenceID": 10, "context": "Among prior work in RL our setting is most similar to the general reinforcement learning model of Lattimore et al. (2013) which also considers an arbitrary temporal dependence between the rewards and observations.", "startOffset": 98, "endOffset": 122}, {"referenceID": 10, "context": "Among prior work in RL our setting is most similar to the general reinforcement learning model of Lattimore et al. (2013) which also considers an arbitrary temporal dependence between the rewards and observations. Our setting differs from that of Lattimore et al. (2013), since we consider the regret in undiscounted reward scenario, whereas Lattimore et al.", "startOffset": 98, "endOffset": 271}, {"referenceID": 10, "context": "Among prior work in RL our setting is most similar to the general reinforcement learning model of Lattimore et al. (2013) which also considers an arbitrary temporal dependence between the rewards and observations. Our setting differs from that of Lattimore et al. (2013), since we consider the regret in undiscounted reward scenario, whereas Lattimore et al. (2013) focus on proving PAC-bounds in discounted reward case.", "startOffset": 98, "endOffset": 366}, {"referenceID": 4, "context": ", Bubeck et al., 2011a), our approach seeks to minimize regret by smartly building an estimate of f using an infinite binary covering tree T , in which each node covers a subset of X .1 We denote by (h, i) the node at depth h and index i among the nodes at the same depth (e.g., the root node which covers X is indexed by (0, 1)). By convention (h + 1, 2i \u2212 1) and (h + 1, 2i) refer to the two children of the node (h, i). The area corresponding to each node (h, i) is denoted by Ph,i \u2282 X . These regions must be measurable and, at each depth, they partition X with no overlap: The reader is referred to Bubeck et al. (2011a) for a more detailed description of the covering tree.", "startOffset": 2, "endOffset": 626}, {"referenceID": 4, "context": "These assumptions coincide with those in (Bubeck et al., 2011a), except for the local smoothness (Assumption 4.d), which is weaker than that of Bubeck et al. (2011a), where the function is assumed to be Lipschitz between any two arms x, x\u2032 close to the maximum x\u2217 (i.", "startOffset": 42, "endOffset": 166}, {"referenceID": 4, "context": "These assumptions coincide with those in (Bubeck et al., 2011a), except for the local smoothness (Assumption 4.d), which is weaker than that of Bubeck et al. (2011a), where the function is assumed to be Lipschitz between any two arms x, x\u2032 close to the maximum x\u2217 (i.e., |f(x) \u2212 f(x\u2032)| \u2264 l(x, x\u2032)), while here we only require the function to be Lipschitz w.r.t. the maximum. Finally, we characterize the complexity of the problem using the near-optimality dimension, which defines how large is the set of \u01eb-optimal arms in X . For the sake of clarity, we consider a slightly simplified definition of near-optimality dimension w.r.t. Bubeck et al. (2011a). Assumption 5 (Near-optimality dimension).", "startOffset": 42, "endOffset": 655}, {"referenceID": 21, "context": "As it has been shown in (Valko et al., 2013) the case of d = 0 covers a rather large class of functions, since every function which satisfies some mild local smoothness assumption, around its", "startOffset": 24, "endOffset": 44}, {"referenceID": 2, "context": "Policy search algorithms (Scherrer & Geist, 2013; Azar et al., 2013; Kober & Peters, 2011) aim at finding the policy in a given policy set which maximizes the long-term performance.", "startOffset": 25, "endOffset": 90}, {"referenceID": 8, "context": "A related work to HCT-\u0393 is the UCCRL algorithm by Ortner & Ryabko (2012), which extends the original UCRL algorithm (Jaksch et al., 2010) to continuous state spaces.", "startOffset": 116, "endOffset": 137}, {"referenceID": 0, "context": "Another relevant work is the OMDP algorithm of Abbasi et al. (2013) which deals with the problem of RL in continuous state-action MDPs with adversarial rewards.", "startOffset": 47, "endOffset": 68}, {"referenceID": 4, "context": "For our first experiment we compare HCT-iid to the truncated hierarchical optimistic optimization (T-HOO) algorithm Bubeck et al. (2011a). T-HOO is a state-of-the-art X -armed bandit algorithm, developed as a computationally-efficient alternative of HOO.", "startOffset": 116, "endOffset": 138}, {"referenceID": 4, "context": "For our first experiment we compare HCT-iid to the truncated hierarchical optimistic optimization (T-HOO) algorithm Bubeck et al. (2011a). T-HOO is a state-of-the-art X -armed bandit algorithm, developed as a computationally-efficient alternative of HOO. In Fig. 1(a) we show the per-step regret, the runtime, and the space requirements of each approach. As predicted by the theoretical bounds, the per-step regret R\u0303n of both HCT-iid and truncated HOO decrease rapidly with number of steps. Though the big O theoretical bounds are identical for both approaches, empirically we observe in this Refer to Bubeck et al. (2011a); Munos (2013) for how to transform bounds on accumulated regret to simple regret bounds.", "startOffset": 116, "endOffset": 625}, {"referenceID": 4, "context": "For our first experiment we compare HCT-iid to the truncated hierarchical optimistic optimization (T-HOO) algorithm Bubeck et al. (2011a). T-HOO is a state-of-the-art X -armed bandit algorithm, developed as a computationally-efficient alternative of HOO. In Fig. 1(a) we show the per-step regret, the runtime, and the space requirements of each approach. As predicted by the theoretical bounds, the per-step regret R\u0303n of both HCT-iid and truncated HOO decrease rapidly with number of steps. Though the big O theoretical bounds are identical for both approaches, empirically we observe in this Refer to Bubeck et al. (2011a); Munos (2013) for how to transform bounds on accumulated regret to simple regret bounds.", "startOffset": 116, "endOffset": 639}, {"referenceID": 4, "context": "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant.", "startOffset": 13, "endOffset": 35}, {"referenceID": 4, "context": "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm.", "startOffset": 13, "endOffset": 201}, {"referenceID": 4, "context": "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm.", "startOffset": 13, "endOffset": 218}, {"referenceID": 4, "context": "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm. 4, and do not require knowledge of the dissimilarity measure l. On the other hand, Slivkins (2011) and Bull (2013) study the cumulative regret but consider a different definition of smoothness related to the zooming concept introduced by Kleinberg et al.", "startOffset": 13, "endOffset": 439}, {"referenceID": 4, "context": "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm. 4, and do not require knowledge of the dissimilarity measure l. On the other hand, Slivkins (2011) and Bull (2013) study the cumulative regret but consider a different definition of smoothness related to the zooming concept introduced by Kleinberg et al.", "startOffset": 13, "endOffset": 455}, {"referenceID": 4, "context": "For example, Bubeck et al. (2011b) require a stronger global Lipschitz assumption and propose an algorithm to estimate the Lipschitz constant. Other work on the iid setting include Valko et al. (2013) and Munos (2011), which are limited to the simple regret scenario, but who only use the mild local smoothness assumption we define in Asm. 4, and do not require knowledge of the dissimilarity measure l. On the other hand, Slivkins (2011) and Bull (2013) study the cumulative regret but consider a different definition of smoothness related to the zooming concept introduced by Kleinberg et al. (2008). Finally, we notice that to deal with unknown mixing time, one may rely on data-dependent tail\u2019s inequalities, such as empirical Bernstein inequality (Tolstikhin & Seldin, 2013; Maurer & Pontil, 2009), replacing the mixing time with the empirical variance of the rewards.", "startOffset": 13, "endOffset": 602}, {"referenceID": 2, "context": "1 Concentration Inequality for non-iid Episodic Random Variables In this section we extend the result in (Azar et al., 2013) and we derive a concentration inequality for averages of non-iid random variables grouped in episodes.", "startOffset": 105, "endOffset": 124}, {"referenceID": 8, "context": "Step (4) is an immediate application of Lemma 19 in Jaksch et al. (2010). Constants apart the terms (a) and (b) coincides with the terms defined in Eq.", "startOffset": 52, "endOffset": 73}], "year": 2014, "abstractText": "In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel any-time X -armed bandit algorithm, and derive regret bounds matching the performance of existing state-of-the-art in terms of dependency on number of steps and smoothness factor. The main advantage of HCT is that it handles the challenging case of correlated rewards, whereas existing methods require that the reward-generating process of each arm is an identically and independent distributed (iid) random process. HCT also improves on the state-of-the-art in terms of its memory requirement as well as requiring a weaker smoothness assumption on the mean-reward function in compare to the previous anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.", "creator": "LaTeX with hyperref package"}}}