{"id": "1402.0635", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Generalization and Exploration via Randomized Value Functions", "abstract": "We consider the problem of reinforcement learning with an orientation toward contexts in which an agent must generalize from past experience and explore to reduce uncertainty. We propose an approach to exploration based on randomized value functions and an algorithm -- randomized least-squares value iteration (RLSVI) -- that embodies this approach. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient and present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Our experiments focus on learning over episodes of a finite-horizon Markov decision process and use a version of RLSVI designed for that task, but we also propose a version of RLSVI that addresses continual learning in an infinite-horizon discounted Markov decision process.", "histories": [["v1", "Tue, 4 Feb 2014 06:41:59 GMT  (47kb,D)", "https://arxiv.org/abs/1402.0635v1", "arXiv admin note: text overlap witharXiv:1307.4847"], ["v2", "Tue, 7 Jul 2015 23:11:02 GMT  (176kb,D)", "http://arxiv.org/abs/1402.0635v2", "arXiv admin note: text overlap witharXiv:1307.4847"], ["v3", "Mon, 15 Feb 2016 10:20:11 GMT  (3057kb,D)", "http://arxiv.org/abs/1402.0635v3", "arXiv admin note: text overlap witharXiv:1307.4847"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1307.4847", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG cs.SY", "authors": ["ian osband", "benjamin van roy", "zheng wen"], "accepted": true, "id": "1402.0635"}, "pdf": {"name": "1402.0635.pdf", "metadata": {"source": "META", "title": "Generalization and Exploration via Randomized Value Functions", "authors": ["Ian Osband", "Benjamin Van Roy", "Zheng Wen"], "emails": ["IOSBAND@STANFORD.EDU", "BVR@STANFORD.EDU", "ZHENGWEN207@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is as if it is a matter of a way in which people are able to survive themselves and themselves. (...) It is not as if people are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "2. Episodic reinforcement learning", "text": "A finite horizon MDPM = (S, A, H, P, R, \u03c0), where S is a finite state space, A a finite action space, H the number of periods, P the transition probabilities, R the reward distributions, and \u03c0 a state distribution. In each episode, the initial state s0 is scanned from \u03c0 and in the period h = 0,1, \u00b7 \u00b7, H \u2212 1, if the state sh is and an action ah is selected, then a next state sh + 1 from Ph (\u00b7 sh, ah) and a reward rh from Rh (\u00b7 sh, sh, sh + 1) is scanned. The episode ends when the state sH is reached and a final reward from RH (\u00b7 sH) is selected. To represent the history of actions and observations over several episodes, we will often index variables by episodes and period."}, {"heading": "A policy \u00b5\u2217 is optimal\u21d0\u21d2 \u00b5\u2217h(s)\u2208argmax\u03b1\u2208AQ\u2217h(s,\u03b1), \u2200s,h.", "text": "A reinforcement learning algorithm generates each action on the basis of observations made up to the period h of episode l. Above each episode, the algorithm realizes the reward \u2211 H h = 0rlh. One way to quantify the performance of a reinforcement learning algorithm is in terms of the expected cumulative regret over L episodes or the time T = LH, defined by remorse (T, M) = \u2211 T / H \u2212 1 l = 0 EM [V \u0445 0 (sl0) \u2212 \u2211 H h = 0 rlh]. Imagine a scenario in which the agent models that for each h Q \u0445 h span [\u03a6h] for a certain \u03a6h-RSA \u00d7 K. With some notation abuse, we use S and A to denote the cardinalities of the state and the scope of action. We refer to this matrix \u03a6h as a generalization matrix and use \u03a6h (s) to denote the action-agstic pair."}, {"heading": "3. The problem with dithering for exploration", "text": "To form an RL algorithm based on LSVI, we need to specify how the agent chooses measures. \u2212 The most common schema is to selectively take random measures, we call this approach hesitancy. Appendix A presents RL algorithms resulting from the combination of LSVI with the most common schemes of -greedy or Boltzmann exploration. \u2212 The literature on efficient RL shows that these dithering schemes exponentially grow into H and / or S schemata schemes (Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Kakade, 2003) leads to regret that these dithering schemes exponentially grow into H and / or S exponentially (Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Kakade & Tennenholtz, 2003) Schemata-Schemata-Schemata-Schemata exploration measures in RL result in exploration schemes growing exponentially into H and / or S exponentially across state-oriented action schemata."}, {"heading": "4. Randomized value functions", "text": "We are now considering an alternative approach to exploration that includes random sampling value functions rather than actions. < As a specific scheme of this kind, we propose Randomized Least Squares Value Iteration (RLSVI), which we present as Algorithm 1.1, to obtain an RL algorithm, we simply select greedy actions in each episode, as in Algorithm 2. The manner in which RLSVI sampling (Thompson, 1933) is inspired extends efficiently across a very general class of online optimization problems (Russo & Van Roy, 2013; 2014). In Thompson sampling, agent samples are selected from a posterior distribution over models and the action that optimizes the sampling model is optimized. RLSVI similar to sampling samples from a distribution over plausible value functions and selected actions that optimize resulting samples. This distribution can be considered an approximation to a posterior distribution over value functions."}, {"heading": "5. Provably efficient tabular learning", "text": "RLSVI is an algorithm for efficiently researching large MDPs with linear value function generalization. So far, there are no algorithms in this context with analytical limits of regret. In fact, most common methods are demonstrably inefficient, as shown in Example 1, regardless of the choice of basic function. In this section, we define an expected limit of regret for RLSVI in a tabular context without generalization, in which the basis h = I. We assume that the limit in terms of probability space (B, F, P) is deterministic, and that R and P are drawn from a previous distribution. We assume that the rewards R (S, H, P, E) and all other random variables we will consider in relation to this probability space are deterministic, and that R and P are drawn from a previous distribution."}, {"heading": "5.1. Preliminaries", "text": "Central to our analysis is the notion of stochastic optimism, which leads to a partial order among random variables.Definition 1. For all real X and Y random variables, we say that X is stochastically optimistic for Y, if and only if for any u: R \u2192 R convex and increasing E [u (X)] [u (Y)].We will use the notation X < so Y to express this relationship. It is worth noting that stochastic optimism is closely related to stochastic second-order dominance: X < i.e. Y, if and only if \u2212 Y second-order stochastically dominates \u2212 X (Hadar & Russell, 1969).We repeat the following result, which establishes such a relationship between Gaussian and Dirichlet random variables in Appendix G.Lemma 1: For all V [0, 1] N and skins [0, T1) N with a supply of 2 if X (\u03b1 > \u03b1 / Y = 1 and Dixi)."}, {"heading": "5.2. Proof sketch", "text": "s look stoically optimistic for the true Q values. (< LSVI for episode l and leave V-L (s) = maxa Q-L (s, a). We can use Pro-Episode-R (sl0) -L (sl0) -V-L (sl) -L (sl0) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) (sl) -L (sl) -L (sl) (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl) -L (sl (sl) -L (sl) -L (sl) -L (sl (sl) -L (sl) -L (sl) -L (sl (sl) -L (sl) -L (sl (sl) -L (sl)."}, {"heading": "6. Experiments", "text": "Our analysis in Section 5 shows that RLSVI with basic tabular functions acts as an effective Gaussian approach to the PSFD. This shows a clear distinction between exploration using randomized value functions and hesitation strategies such as Example 1. However, the motivation for RLSVI does not lie in tabular environments where several demonstrably efficient RLSVI algorithms already exist, but in large systems that require generalization. We believe that under certain conditions it may be possible to set polynomic limits of regret for RLSVI with a generalization of value functions. To stimulate thinking on this topic, we present a conjecture of the result, which may be possible in Appendix B. First, we will present a series of experiments designed to test the applicability and scalability of RLSVI for exploration with generalization. Our experiments are divided into three sections."}, {"heading": "6.1. Testing for efficient exploration", "text": "We are now looking at a number of environments modelled on Example 1 where dithering exploration strategies are demonstrably inefficient. Importantly, unlike the tabular setting in Section 5, our algorithm interacts only with the MDP, but through a series of basic functions \u03a6 that generalize across states. We are examining the empirical performance of the RLSVI and find that it efficiently balances exploration and generalization in this didactic example."}, {"heading": "6.1.1. COHERENT LEARNING", "text": "In our first experiments we generate a random series of K-base functions. This basis is coherent, but the individual basic functions are not otherwise informative. We form a linear subspace VhK, that of (1, Q-1, w-1,.., w-2). Here are wi and w-i IID Gaussian, N (0, I), RSA. We then form ourselves by projecting (1, w1,.., wk-1) on VhK and renormalize each component to have equal 2-normal2. Figure 2 presents the empirical regret for RLSVI withK = 10, N = 50, h = 1 and a -greedy agent over 5 seeds3.Figure 1 shows that RLSVI consistently learns the optimal policy in about 500 episodes."}, {"heading": "6.1.2. AGNOSTIC LEARNING", "text": "To examine RLSVI in this setting, we create base functions by adding Gaussian noise to the function of the true value \u03c6hk \u0445 N (Q \u0445 h, \u03c1I). The parameter \u03c1 determines the order of magnitude of this noise. For \u03c1 = 0, this problem is coherent, but for \u03c1 > 0 this will not typically be the case. We put N = 20, K = 20, \u03c3 = 0.1 and \u03bb = 1.For i = 0,.. 1000, we perform RLSVI for 10,000 episodes with \u03c1 = i / 1000 and a random seed. Figure 6 shows the number of episodes up to 10 rewards for each value of \u03c1. For large values and an extremely incorrect basis, RLSVI is not effective. However, there is a region 0 < \u043c < \u043c where learning remains remarkably stable. This simple example gives us some hope that RLSVI for 4- and an extremely incorrect base, RSVI for the chain = = > = = 5Q = = practical = = = = points in the chain = > = = = > N = = = = 5Q."}, {"heading": "6.2. Tetris", "text": "We are now turning our attention to learning to play the iconic video game Tetris. In this game, random blocks fall sequentially onto a 2D grid with 20 rows and 10 columns. At each step, the agent can move and rotate the object subject to the constraints of the grid. The game begins with an empty grid and ends when a square in the top row becomes full. However, when a row becomes full, it is removed and all bricks above it move down. Our goal is to maximize the results obtained (total number of rows removed) before the end of the game. Tetris is something of a benchmark problem for RL and approximate dynamic programming, with several papers on the subject (Gabillon et al., 2013). Our focus is not so much to learn a player with high scoring Tetris, but instead we offer benefits over other forms of exploration with LSVSVI."}, {"heading": "6.3. A recommendation engine", "text": "\"We will show that efficient exploration and generalization in a simple model of customer interaction can be helpful.\" Consider an agent who J \u2264 N products from Z = 1, 2,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,...,................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "7. Closing remarks", "text": "However, the real promise of RLSVI lies in its potential as an efficient method of exploration in large-scale generalized environments. RLSVI is simple, practical, and explores efficiently in multiple environments where state-of-the-art approaches are ineffective. We believe that this approach to exploration using randomized value functions is an important concept that goes beyond our specific implementation of RLSVI. RLSVI is designed for generalization with linear value functions, but many of the great successes in RL are associated with highly nonlinear \"deep\" neural networks from backgammon (Tesauro, 1995) to Atari8 (Mnih, 2015). Knowledge and approaches derived from RLSVI may still be useful in this nonlinear environment."}, {"heading": "APPENDICES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. LSVI with Boltzmann exploration/ -greedy exploration", "text": "The LSVI algorithm iterates backwards over time periods in the planning horizon, in each iteration that adapts a value function to the sum of the immediate rewards and valuations of the next period. (3) Note that in algorithm 3, if l = 0, matrix A and vector b are empty. In this case, we simply specify the procedure l0 = procedure l1 = \u00b7 \u00b7 procedure l1 = procedure l1 = procedure, H \u2212 1 = procedure l0 \u2212 procedure l0 \u2212 procedure l0 \u2212 procedure l0 \u2212 procedure ll0 \u2212 procedure, H \u2212 l1 = procedure l1 = procedure l1 = procedure l0 \u2212 procedure lu \u2212 input: data (si0, ai0), ri0,.. ln, procedure (siH \u2212 1), procedure ln: procedure 0 \u2212 lu \u2212 0,... n: 0 \u2212 n:..."}, {"heading": "B. Efficient exploration with generalization", "text": "Our computational results indicate that the RLSVI, in conjunction with a generalization, exhibits a level of efficiency far beyond what can be achieved by Boltzmann or greedy research. We leave open the possibility of establishing efficiency guarantees in such contexts as an open problem. To stimulate thinking on this topic, we present a conjecture.Conjecture 1. For all M = (S, A, H, P, R, \u03c0), \u03a60,.., \u03a6H \u2212 1, \u03c3, and, if reward distributions have support for R, there is a unique (phenomena 0,.., \u03b8H \u2212 1): RK \u00b7 H satisfactory Q \u0445 h = \u03a6h\u03b8h for h = 0,.., H \u2212 1, and \u0445H \u2212 1 h = 0, if reward distributions have support for R, then there is a clear (phenomena 0,...,.): RK \u00b7 H \u00b7 1) satisfactory Q \u03a6h = h, h = h, h = h."}, {"heading": "C. Chain experiments", "text": "C.1. Generating a Random Coherent Base We present full details for algorithm 6, which contains the random coherent base functions \u03a6h-RSA-K for h = 1,.., H. In this algorithm, we use a standard notation for indexing vector elements. For each A-Rm-n element, we write A [i, j] for the element in the ith series and jth column. We use the placeholder \u00b7 to replicate the entire axis, so that for example A [\u00b7, 1] Rn is the first column of the A.algorithm 6. Input: S, A, H, K-N, Q-H-RSA for h = 1,.., H-Output: \u0445h-RSA-K for h = 1,.., H1: Sample RSA-N (0, I)."}, {"heading": "C.2. Robustness to \u03bb, \u03c3", "text": "In Figures 10 and 11, we present the cumulative regret for N = 50, K = 10 over the first 10,000 episodes for several orders of magnitude for \u03c3 and \u03bb. For most parameter combinations, learning remains remarkably stable. We find that large values of \u03c3 lead to slower learning because the Bayesian posterior is very slow to focus on new data. However, in stochastic areas, we have found that the choice of a too small frame can cause the posterior of the RLSVI to concentrate too quickly and thus not sufficiently investigate. This is a similar finding to previous analyses of Thomson's samples (Agrawal & Goyal, 2012) and corresponds to the taste of Theorem 1.C.3. Scaling with the number of bases K In Figure 4, we have shown that RLSVI gracefully matches the number of basic features on a chain of N = 50. In Figure 13, we reproduce these results for chains of different lengths. To highlight the general trend, a local polyresonance increase appears for almost every single episode."}, {"heading": "D. Tetris experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1. Algorithm specification", "text": "In algorithm 7 we present a natural fit to RLSVI without known episode length, but still a regular episodic structure. This is the algorithm we use for our experiments in Tetris. LSVI algorithms are built in the same way. Algorithm 7 Stationary RLSVI input: data \u03a6 (s1, a1), r1,.., \u03a6 (sT, aT). Previous estimate. \u2212 l Previous estimate. \u2212 l (sT, aT) bi. \u2212 1. Parameter. > 0, \u03c3 > 0, \u03b3 [0,1] Output: Results. \u2212 l1: Generate regression problem. \u2212 K, b. \u2212 R: A value. \u2212 l (sT, aT) bi."}, {"heading": "D.2. Effective improvements", "text": "We present the results for the fixed \u03c3 = 1 and \u03bb = 1 RLSVI. This corresponds to a Bayesian linear regression with known noise variance in the algorithm 7. We actually found a slightly better performance when using a Bayesian linear regression with inverse gamma before an unknown variance. This is the conjugate before the Gaussian regression with known variance. Since the improvements were minor and it slightly complicated the algorithm, we omit these results. However, we believe that the use of a wider predecessor variance over the variance will be more robust in the application rather than selecting a specific \u03c3 and \u03bb."}, {"heading": "D.3. Mini-tetris", "text": "In Figure 7, we show that RLSVI outperforms LSVI even with a highly tuned incandescent scheme. However, these results are much more extreme with a didactic version of Mini-Tetris. We produce a Tetris board with only 4 rows and only S, Z parts. This problem is much more difficult and highlights the need for efficient exploration in a more extreme manner. In Figure 14, we present the results for this mini-Tetris environment. As expected, this example highlights the advantages of RLSVI over LSVI by procrastination. RLSVI clearly outperforms LSVI even with a coordinated schedule. RLSVI learns faster and achieves a higher converged policy."}, {"heading": "E. Recommendation system experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E.1. Experiment Setup", "text": "For the recommendation system experiments, the experimental setup is specified in algorithm 9. We set N = 10, J = H = 5, c = 2 and L = 1200.Algorithm 9 recommendation system experiments: Experiment Setup Input: N = Z + +, J = H = Z + +, c > 0, L = Z + + Output: 0,.. 100 times Run the linear context-dependent bandit algorithm for i = 1,., 100 times for i = 1,.,., 10 \u2212 2, 10 \u2212 1, 1, 10} doRun LSVI-Boltzmann with \u03bb = 0.2 and 10 times end for Run RLSVI bandit algorithm."}, {"heading": "Input: N \u2208 N, J \u2208 N, L \u2208 N", "text": "The initialization: Set \u03b1n = \u03b2n = 1, \u0432 n = 1,. >., N for l = 0,.., L \u2212 1 doRandomly sample p \u2012 ln \u0445 beta (\u03b1n, \u03b2n), \u0432 n = 1,. \u00b7 \u00b7 \u00b7, N for l = 0,. \u00b7 \u00b7 ln's in descending order, and recommend the first J products to the customer in order n = 1,. \u2212 \u2212 \u2212 n In this subsection we describe the linear contextual bandit algorithm \u2212 l if the customer likes the product, n + 1else \u03b2n + 1end ifend for end forE.3. Linear contextual bandit algorithms \u2212 H \u2212 \u2212 \u2212 2 in sequence. The linear contextual bandit algorithm is similar to RLSVI, but without reverse value propagation, a key feature of the RLSVI. It is easy to see that the linear contextual bandit algorithm aims to learn the optional functions of this policy."}, {"heading": "F. Extensions", "text": "We will now briefly discuss a few possible enhancements to the version of RLSVI proposed in Algorithms 1 and 8, one being an incremental version that is more computationally efficient, the other addressing continuous learning in an infinite horizon without taking into account the Markov decision-making process. In the same sense that RLSVI shares much with LSVI but is distinguished by its new approach to exploration, these enhancements share much with Q-Learning with the fewest squares (Lagoudakis et al., 2002).F.1. Incremental Learning Note that Algorithm 1 is a batch learning algorithm in the sense that in each episode l, although its architecture can be computed incrementally, all past observations are needed to calculate the computing of Lh's. Therefore, its time waxes calculated per episode are with l, which is undesirable if the algorithm is applied via many algorithms."}, {"heading": "F.2. Continual learning", "text": "Finally, we propose a version of RLSVI for RL in the infinite horizon. (> Sample-1) A discounted MDP is identified by a sextupleM = (S, A, \u0432, P, R, \u03c0) in which a discount factor (0, 1) is the discount factor. S, A, P, R, \u03c0 are similarly defined by the finite horizon case. (Specifically, at any time t = 0, 1,.. If the state is xt and an action at is selected, then a subsequent state xt + 1 of P (\u00b7 xt, at) and a reward rt is sampled by R (\u00b7 xt, at, xt + 1,.). We also use V to indicate the optimal state value function, and Q to indicate the optimal state value."}, {"heading": "G. Gaussian vs Dirichlet optimism", "text": "The aim of this sub-section is to prove Lemma 1, which is reproduced below: For all v [0, 1] N and \u03b1 [1, \u221e) N with \u03b1T1 \u2265 2, if x-N (\u03b1 > v / \u03b1 > 1, 1 / \u03b1 > 1) and y = pT v for p-Dirichlet (\u03b1) then x < so y.We start with a problem showing some basic equivalents stochastic optimism. Lemma 3 (Optimism Equivalence): The following are equivalent to X < so Y: 1. For any random variable Z independent of X and Y, E [max (X, Z)] \u2265 E [max (Y, Z)] 2. For each \u03b1-R integration and each \u03b1-R equivalence."}, {"heading": "G.1. Beta vs. Dirichlet", "text": "To prove lemmas 1, we will first prove an intermediate result showing a specific beta distribution y = a = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c c = c c = c c = c = c = c c = c = c c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c"}, {"heading": "G.2. Gaussian vs Beta", "text": "In the previous section, we showed that a concerted beta distribution y would be optimistic for the dirichlet y. We will now show that the normal random variable x is optimistic for y, thus providing evidence for Lemma 1, x < so y < so y. Unfortunately, unlike the Beta vs. Dirichlet case, it is quite difficult to directly show this random relationship between Gauss x and Beta y. Instead, we point to the stronger dominance relationship of Single Crossing CDFs. Definition 2 (Single Crossing Dominance). Let X and Y be real random variables with CDFX and FY, respectively. We say that X-Crossing Y dominates when E [X] \u2265 E [Y] and there is an intersection, so that: FX (s), FY (s) and Y are real random variables with CDFX and FY, respectively. (5) Note that a Single Crossing Dominance Y implies a cross Y (the rest of this paragraph is dedicated to Y)."}, {"heading": "G.3. Double crossing PDFs", "text": "Repeatedly applying the mean theorem (1 +) > fB (1 +) > fB (1 +) > fB (1) > fB (1) > fB (1) > fB (1) > fB (1) > fB (1) = \u03b2 \u03b2 = \u03b2 \u03b2 (1) = \u03b2 \u03b2 \u03b2 = \u03b2 (1) then it is sufficient to prove that the PDFs cross can be used at most twice in the same interval (0, 1). We deplore that the proof in its present form is so laborious, but our attempts to find a more elegant solution have so far been unsuccessful; the rest of this appendix is devoted to proving this \"double crossing\" property by manipulating the PDFs (0, 1) x x x x x x (1) for different values of \u03b1, \u03b2.We write fN for the density of beta y and fB for the density of beta y. > f. We know that at the limit of fN (0) > f1 > (N) > (1) and N (1) where the boundaries are different in each case."}, {"heading": "G.4. Recap", "text": "On the basis of the results of the preceding sections, we complete the proof for Lemma 6 for Gaussian versus beta dominance for all possible \u03b1, \u03b2 > 0, so that \u03b1 + \u03b2 \u2265 1. Putting Lemma 5 with Lemma 6 completes our proof for Lemma 1. We imagine that a much more elegant and general method of proof is available for future work."}], "references": [{"title": "Regret bounds for the adaptive control of linear quadratic systems", "author": ["Abbasi-Yadkori", "Yasin", "Szepesv\u00e1ri", "Csaba"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Further optimal regret bounds for Thompson sampling", "author": ["Agrawal", "Shipra", "Goyal", "Navin"], "venue": "arXiv preprint arXiv:1209.3353,", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Temporal differences-based policy iteration and applications in neuro-dynamic programming", "author": ["Bertsekas", "Dimitri P", "Ioffe", "Sergey"], "venue": "Lab. for Info. and Decision Systems Report LIDS-P-2349,", "citeRegEx": "Bertsekas et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas et al\\.", "year": 1996}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2002}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Dann", "Christoph", "Brunskill", "Emma"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dann et al\\.", "year": 2015}, {"title": "Approximate dynamic programming finally performs well in the game of tetris", "author": ["Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Scherrer", "Bruno"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2013}, {"title": "Rules for ordering uncertain prospects", "author": ["Hadar", "Josef", "Russell", "William R"], "venue": "The American Economic Review, pp", "citeRegEx": "Hadar et al\\.,? \\Q1969\\E", "shortCiteRegEx": "Hadar et al\\.", "year": 1969}, {"title": "Nearoptimal regret bounds for reinforcement learning", "author": ["Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Kakade", "Sham"], "venue": "PhD thesis,", "citeRegEx": "Kakade and Sham.,? \\Q2003\\E", "shortCiteRegEx": "Kakade and Sham.", "year": 2003}, {"title": "Efficient reinforcement learning in factored MDPs", "author": ["Kearns", "Michael J", "Koller", "Daphne"], "venue": "In IJCAI, pp", "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael J", "Singh", "Satinder P"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Least-squares methods in reinforcement learning for control", "author": ["Lagoudakis", "Michail", "Parr", "Ronald", "Littman", "Michael L"], "venue": "In Second Hellenic Conference on Artificial Intelligence (SETN-02),", "citeRegEx": "Lagoudakis et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lagoudakis et al\\.", "year": 2002}, {"title": "The sample-complexity of general reinforcement learning", "author": ["Lattimore", "Tor", "Hutter", "Marcus", "Sunehag", "Peter"], "venue": "In ICML,", "citeRegEx": "Lattimore et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lattimore et al\\.", "year": 2013}, {"title": "Stochastic dominance and expected utility: survey and analysis", "author": ["Levy", "Haim"], "venue": "Management Science,", "citeRegEx": "Levy and Haim.,? \\Q1992\\E", "shortCiteRegEx": "Levy and Haim.", "year": 1992}, {"title": "Reducing reinforcement learning to KWIK online regression", "author": ["Li", "Lihong", "Littman", "Michael"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Li", "Lihong", "Littman", "Michael L", "Walsh", "Thomas J"], "venue": "In ICML, pp", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "State of the art control of atari games using shallow reinforcement learning", "author": ["Liang", "Yitao", "Machado", "Marlos C", "Talvitie", "Erik", "Bowling", "Michael H"], "venue": "CoRR, abs/1512.01563,", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih and Volodymyr,? \\Q2015\\E", "shortCiteRegEx": "Mnih and Volodymyr", "year": 2015}, {"title": "Online regret bounds for undiscounted continuous reinforcement learning", "author": ["Ortner", "Ronald", "Ryabko", "Daniil"], "venue": "In NIPS,", "citeRegEx": "Ortner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ortner et al\\.", "year": 2012}, {"title": "Model-based reinforcement learning and the eluder dimension", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Near-optimal reinforcement learning in factored MDPs", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Bootstrapped thompson sampling and deep exploration", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "arXiv preprint arXiv:1507.00300,", "citeRegEx": "Osband et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2015}, {"title": "More) efficient reinforcement learning via posterior sampling", "author": ["Osband", "Ian", "Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "PAC optimal exploration in continuous space Markov decision processes", "author": ["Pazis", "Jason", "Parr", "Ronald"], "venue": "In AAAI. Citeseer,", "citeRegEx": "Pazis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pazis et al\\.", "year": 2013}, {"title": "Eluder dimension and the sample complexity of optimistic exploration", "author": ["Russo", "Dan", "Van Roy", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Russo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2013}, {"title": "Learning to optimize via posterior sampling", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2014}, {"title": "PAC model-free reinforcement learning", "author": ["Strehl", "Alexander L", "Li", "Lihong", "Wiewiora", "Eric", "Langford", "John", "Littman", "Michael L"], "venue": "In ICML, pp", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard", "Barto", "Andrew"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["Szepesv\u00e1ri", "Csaba"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "Szepesv\u00e1ri and Csaba.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Csaba.", "year": 2010}, {"title": "Temporal difference learning and tdgammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Efficient exploration and value function generalization in deterministic systems", "author": ["Wen", "Zheng", "Van Roy", "Benjamin"], "venue": "In NIPS, pp", "citeRegEx": "Wen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "This matches the worst case lower bound for this problem up to logarithmic factors (Jaksch et al., 2010).", "startOffset": 83, "endOffset": 104}, {"referenceID": 7, "context": ", UCRL2 (Jaksch et al., 2010)) adapted to this context.", "startOffset": 8, "endOffset": 29}, {"referenceID": 12, "context": "There is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006).", "startOffset": 99, "endOffset": 254}, {"referenceID": 22, "context": "There is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006).", "startOffset": 99, "endOffset": 254}, {"referenceID": 26, "context": "There is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006).", "startOffset": 99, "endOffset": 254}, {"referenceID": 15, "context": "regression (Li & Littman, 2010; Li et al., 2008).", "startOffset": 11, "endOffset": 48}, {"referenceID": 30, "context": "The manner in which RLSVI explores is inspired by Thompson sampling (Thompson, 1933), which has been shown to explore efficiently across a very general class of online optimization problems (Russo & Van Roy, 2013; 2014).", "startOffset": 68, "endOffset": 84}, {"referenceID": 22, "context": "RLSVI bears a close connection to PSRL (Osband et al., 2013), which maintains and samples from a posterior distribution over MDPs and is a direct application of Thompson sampling to RL.", "startOffset": 39, "endOffset": 60}, {"referenceID": 7, "context": "Surprisingly, these scalings better state of the art optimistic algorithms specifically designed for efficient analysis which would admit \u00d5( \u221a H3S2AT ) regret (Jaksch et al., 2010).", "startOffset": 159, "endOffset": 180}, {"referenceID": 5, "context": "Tetris has been something of a benchmark problem for RL and approximate dynamic programming, with several papers on this topic (Gabillon et al., 2013).", "startOffset": 127, "endOffset": 150}, {"referenceID": 16, "context": "Interestingly, recent work has been able to reproduce similar performance using linear value functions (Liang et al., 2015).", "startOffset": 103, "endOffset": 123}], "year": 2016, "abstractText": "We propose randomized least-squares value iteration (RLSVI) \u2013 a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or -greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates nearoptimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.", "creator": "LaTeX with hyperref package"}}}