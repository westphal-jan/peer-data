{"id": "1402.0915", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2014", "title": "Learning Ordered Representations with Nested Dropout", "abstract": "In this paper, we study ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results in the simple case of a semi-linear autoencoder. We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA. We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications. Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows codes that are hundreds of times longer than currently feasible for retrieval. We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods. We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction.", "histories": [["v1", "Wed, 5 Feb 2014 00:41:58 GMT  (1153kb)", "http://arxiv.org/abs/1402.0915v1", "11 pages, 5 figures. Submitted for publication"]], "COMMENTS": "11 pages, 5 figures. Submitted for publication", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["oren rippel", "michael a gelbart", "ryan p adams"], "accepted": true, "id": "1402.0915"}, "pdf": {"name": "1402.0915.pdf", "metadata": {"source": "META", "title": "Learning Ordered Representations with Nested Dropout", "authors": ["Oren Rippel", "Michael A. Gelbart", "Ryan P. Adams"], "emails": ["RIPPEL@MATH.MIT.EDU", "MGELBART@SEAS.HARVARD.EDU", "RPA@SEAS.HARVARD.EDU"], "sections": [{"heading": null, "text": "ar Xiv: 140 2.09 15v1 [st at.M L] 5F eb"}, {"heading": "1. Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2. Ordering with nested dropout", "text": "Dropout (Hinton et al., 2012) is a neural network regulation technology that adds stochasticity to the architecture during training. At each iteration, unbiased coins are flipped independently for each unit in the network to determine whether it is \"dropped\" or not. Second, each falling unit is deleted from the network for this iteration, and an optimization step is taken with respect to the resulting networking."}, {"heading": "2.1. Interpretation", "text": "It was shown by Vincent et al. (2010) that the formation of an autoencoder corresponds to the maximization of a lower limit between the mutual information I (y; x) between the input data and their representations. (Specifically: the objective of the b-trunktionproblem can be amplified in the formC \u2193 b (ig; x) \u2248 Ey [\u2212 log pY | X) \u2193 b (y; f) \u2193 b (y; b)] (4), where we assume that our data will be amplified from the true distribution py (\u00b7). The choice py | x (y; x) = N (y; p), p (y), p (y), p (y), p (y), p) b)] (4), whereby we assume that our data will be removed from the true distribution py (x). The choice py | x (py | x) = N (x), p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p...p.p....p.p...p.p.....p.p....p..p......p......p.....p.p......choice."}, {"heading": "3. Exact recovery of PCA", "text": "In this section, we apply nested dropouts to a semi-linear autoencoder. This model has a linear or sigmoidal encoder and a linear decoder. The relative simplicity of this case allows us to rigorously study the ordering characteristics implied by nested dropouts. First, we show that the class of optimal solutions of the nested dropouts is a subset of the class of optimal solutions of a standard autoencoder. This means that the introduction of nested dropouts does not sacrifice the quality of the autoencoder solution. Second, we show that equipping an auto encoder with nested dropouts significantly limits its class of optimal solutions. We characterize these limitations. Finally, we show that the model has a single, unique solution under an additional orthonormality restriction that covers exactly the amount of K eigenvectors with the largest orders of magnitude, which arise from the codices of the characteristic matrix to the A exactly."}, {"heading": "3.1. Problem definitions and prior results", "text": "The standard encoder problem In view of our inputs, we apply the linear encoder data (y =): \"We must apply the results of the encoder results (y =).\" \"We,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we.\" We, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we. \""}, {"heading": "3.2. The nested dropout problem recovers PCA exactly", "text": "Theorem 1. Any optimal solution to the nested dropout problem is necessarily an optimal solution to the standard autoencoder problem. Definition. we define the matrix T-RK-K as commutative in its shortening and inversion when each of its leading minors JK-bTJ T-K-b-b-b-b-b-1,.. K is invertable, and the inversion of each of its leading minors is equal to the leading minor of the inverse minors T-1, namelyJK-1JTK-b-b-b-b-b-b-b-b-b-b. (12) The lower theorem, combined with Lemma-1, imposes narrow limitations on the class of dropout problems."}, {"heading": "4. Training deep models with nested dropout", "text": "In this section we discuss our extension of the nested dropout approach to deep architectures. Specifically, we applied this to deep autoencoders with tens of millions of parameters, which we trained on the data set of 80 million tiny images (80MTI) (Torralba et al., 2008) on a cluster of GPUs. Training models with nested dropout present a series of unconventional technical challenges. In the following sections, we describe these challenges and discuss strategies for overcoming them. First, we describe our overall architecture and optimization. The 80MTI are 79,302,017 color images of size 32 x 32. We pre-processed the data by subtracting their mean value from each pixel and normalizing them by their variance within the data set. We optimize our models using the nonlinear gradient method and select step sizes using a strong wolf-condition line search."}, {"heading": "4.1. Unit sweeping for decaying gradients", "text": "Due to the decreasing distribution pB (\u00b7), it is increasingly unlikely that we will scan higher indices of representation during the training. To combat this effect, we are developing a technique we call unit sweep. The idea stems from the observation that the covariance of two latent units decreases sharply depending on the difference in their indices. If pB (\u00b7) is a geometric distribution, then the probability of observing both units i and j is given that one of them is observed, P [b \u2265 max (i, j) | b \u2265 min (i, j)] = P [b \u0430 | i \u2212 j |] = by the unremembered property of distribution. In other words, a latent unit is ultimately linked to a sensitive unit of convergence 1 by comparing this unit."}, {"heading": "4.2. Adaptive regularization coefficients", "text": "The gradient drop as a function of the representation index represents a difficulty for regulation. In particular, the ratio of the orders of magnitude of the reconstruction and regularization gradients disappears as a function of the index. Therefore, a single regularization term, such as \"K k = 1-k-L1,\" would not be suitable for nested dropouts, since the regularization gradient would dominate the high-index gradients. Therefore, regularization must be decoupled as a function of the representation index. For the weight drop, for example, this would be in the form of \"Kk = 1-k-k-k-k-L1.\" Selecting the coefficients that we perform manually is a challenge, and for this purpose we assign them adaptively. We do this by determining the ratio between the order of magnitude of the reconstruction gradient and the regularization gradient in advance, and selecting the \"k\" to meet this relative requirement of each term in the definition of the optimization."}, {"heading": "4.3. Code binarization", "text": "We have achieved good empirical performance by binding the weights of the encoder and decoder and the threshold to the representation layer. Although the gradient itself cannot exceed this threshold, a signal can do this: the encoder can be trained because it is connected to the decoder, and its modifications are then reflected in the target. To achieve fixed boundary distributions across the binarized representation units, i.e. xk \u0445 Bern (\u03b2) for k = 1,..., we calculate the \u03b2 quantum for each unit and use this value for the threshold."}, {"heading": "4.4. Code invariance", "text": "Inspired by Rifai et al. (2011b; a) we carry out this regularization stochastically by disrupting each input in our minibatch with a small \u03b5n \u0445 N (0, \u03b5 \u0442 ID) and introduce the penalty terminalI = 1NN \u0445 n = 1-f\u0443 (yn + \u03b5n) \u2212 f\u0443 (yn) \u0445 2-\u03b5n-2. (14) By means of Taylor expansion it can be shown that this corresponds to a stochastic formulation of the regularization of the Frobenius norm of the Jacobic f\u0442 (\u00b7)."}, {"heading": "5. Retrieval with ordered binary codes", "text": "In this section we discuss how orderly representations can be used to construct data structures that allow fast retrieval and at the same time allow very long codes."}, {"heading": "5.1. Binary tree on the representation space", "text": "The ordering property, coupled with the ability to control the information capacity across representation units, motivates the construction of a binary tree over large datasets. Each node in this tree contains pointers to the set of examples that share the same path along the tree up to that point. Guarantee that this tree is balanced is not feasible, as this is equivalent to the complete characterization of the common distribution across the representation space. However, due to the characteristics of the training algorithm we are able to fix the marginal distributions of all representation bits as the xk database Bern (\u03b2) for some hyperparameters \u03b2 (0, 1).In accordance with the training process, we code our database as X = {xn} Nn {0, 1} K, xn = fB database. We then construct a binary tree on the resulting codes.Given a query y y that we encode it as x."}, {"heading": "5.2. Empirical results", "text": "We empirically examined the characteristics of the resulting codes and data structures in several respects. First, we applied ordered retrieval to a toy problem in which we trained a tiny 2-16-32-16-2 auto encoder on 2D synthetic pinwheel data. Here, we can visualize the nesting of neighborhood families for various queries. Note that, as expected, the nested neighborhood boundaries are orthogonal in the direction of local variation of the data, resulting from the model's reconstruction loss function. We then trained on 80MTI a binarized nested dropout auto encoder with layer widths 3072-2048-524-3072 with weight shifting and inventory regulation (see Section 4.4). We selected pB (\u00b7), Geom (0.97) and the binarization quantity \u03b2 = 0.2 Empirical repetition speeds for different neighborhood models to be performed."}, {"heading": "6. Adaptive compression", "text": "Another application of the arranged representations is the continuous degradation of lossy compression systems. By \"continuous degradation,\" we mean that the message can be deciphered for each number, b, of the bits received, and that the reconstruction error L (y, y, b) decreases monotonously with b. Such representations result in a continuous (up to a single bit) range of bit rate quality combinations, with each additional bit corresponding to a small incremental increase in the quality of the receiver. The property of continuous degradation is attractive in many situations. First, we consider a digital video signal transmitted to receivers with different bandwidths. Further, suppose that the probability distribution across bandwidths is known or can be estimated for the population, pB (\u00b7), and that a receiver with bandwidth b receives only the first bits of transmission."}, {"heading": "6.1. Empirical results", "text": "In Figure 5 (a) we qualitatively evaluate the continuous degradation of lossy compression with orderly representations. We trained a single-layer auto encoder 3072-1024-3072 with nested dropout to CIFAR-10 and produced reconstructions for different code lengths. Each column represents a different image and each line represents a different code length. As the code length increases (in the figure below), the quality of reconstruction increases. In addition to the above images, we also have a standard auto encoder with the same architecture but without nested dropouts trained in the lower line (24576 bits each). Figure 5 (b) shows orderly reconstruction rates depending on the code length for different approaches to the problem. In addition, we trained a standard auto encoder with the same architecture but without nested dropouts."}, {"heading": "7. Discussion and future work", "text": "We have presented a novel technique for learning representations in which dimensions have a known order, which is exactly the same as PCA for flat autoencoders, but can also be generalized to deep networks, allowing learned representations of data that are adaptive in the sense that they can be truncated, with the certainty that the shorter codes contain as much information as possible. Such codes are of interest for applications such as retrieving and compressing. Orderly retrieval of data can also be used for efficient supervised learning, namely, it allows performing k-nearest neighbors on very long codes in their cardinality in logarithmic time. This idea can be combined with various existing approaches to metric learning of kNN and binarized representations (Norouzi et al., 2012; Salakhutdinov & Hinton, 2007; Weinberger & Saul, 2009)."}, {"heading": "Appendix A. Proofs for Section 3.2", "text": "This means that any optimal solution to the nested dropout problem must be minimized exactly."}], "references": [{"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Manuscript M217,", "citeRegEx": "Bourlard and Kamp,? \\Q1987\\E", "shortCiteRegEx": "Bourlard and Kamp", "year": 1987}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In Proc. of AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Datar", "Mayur", "Immorlica", "Nicole", "Indyk", "Piotr", "Mirrokni", "Vahab S"], "venue": "In Proceedings of the Twentieth Annual Symposium on Computational Geometry,", "citeRegEx": "Datar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Datar et al\\.", "year": 2004}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Proc. of AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Learning binary hash codes for large-scale image search. In Machine Learning for Computer Vision, volume 411 of Studies in Computational Intelligence, pp. 49\u201387", "author": ["Grauman", "Kristen", "Fergus", "Rob"], "venue": null, "citeRegEx": "Grauman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grauman et al\\.", "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G E Hinton", "Salakhutdinov", "R R"], "venue": "Science,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey E"], "venue": "In ESANN,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2011}, {"title": "Convolutional Networks for Images, Speech and Time Series, pp. 255\u2013258", "author": ["Lecun", "Yann", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Lecun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1995}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Hamming distance metric learning", "author": ["Norouzi", "Mohammad", "Fleet", "David", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Higher order contractive auto-encoder", "author": ["Rifai", "Salah", "Mesnil", "Gr\u00e9goire", "Vincent", "Pascal", "Muller", "Xavier", "Bengio", "Yoshua", "Dauphin", "Yann", "Glorot"], "venue": "In ECML/PKDD,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proc. of ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "Sam T", "Saul", "Lawrence K"], "venue": null, "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Learning a nonlinear embedding by preserving class neighborhood structure", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey"], "venue": "In Proc. of AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Fast exact inference for recursive cardinality models", "author": ["Tarlow", "Daniel", "Swersky", "Kevin", "Zemel", "Richard S", "Adams", "Ryan P", "Frey", "Brendan"], "venue": "In 28th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Tarlow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2012}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Tenenbaum", "Joshua B", "de Silva", "Vin", "Langford", "John C"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Robert", "Freeman", "William T"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}, {"title": "Visualizing data using t-SNE", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Weinberger", "Kilian Q", "Saul", "Lawrence K"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Spectral hashing", "author": ["Weiss", "Yair", "Torralba", "Antonio", "Fergus", "Robert"], "venue": "In NIPS, pp. 1753\u20131760,", "citeRegEx": "Weiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 19, "context": "Representation learning enables one to avoid explicit feature engineering; indeed, approaches to deep feature learning have often found representations that outperform their hand-crafted counterparts (e.g., Lecun & Bengio, 1995; Hinton & Salakhutdinov, 2006; Vincent et al., 2010; Coates et al., 2011).", "startOffset": 200, "endOffset": 301}, {"referenceID": 1, "context": "Representation learning enables one to avoid explicit feature engineering; indeed, approaches to deep feature learning have often found representations that outperform their hand-crafted counterparts (e.g., Lecun & Bengio, 1995; Hinton & Salakhutdinov, 2006; Vincent et al., 2010; Coates et al., 2011).", "startOffset": 200, "endOffset": 301}, {"referenceID": 6, "context": "As with the original dropout formulation (Hinton et al., 2012), nested dropout applies a stochastic mask over models.", "startOffset": 41, "endOffset": 62}, {"referenceID": 2, "context": "Locality sensitive hashing (Datar et al., 2004) seeks to preserve distance information by means of random projections; however, this can lead to very inefficient codes for high input dimensionality.", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Dropout (Hinton et al., 2012) is a regularization technique for neural networks that adds stochasticity to the architecture during training.", "startOffset": 8, "endOffset": 29}, {"referenceID": 19, "context": "It was shown by Vincent et al. (2010) that training an autoencoder corresponds to maximizing a lower bound on the mutual information I(y;x) between the input data and their representations.", "startOffset": 16, "endOffset": 38}, {"referenceID": 19, "context": "We inject noise to promote robustness, as in (Vincent et al., 2010); namely, with probability 0.", "startOffset": 45, "endOffset": 67}, {"referenceID": 3, "context": "Glorot et al. (2011) features an in-depth discussion and motivation of rectified linear units.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "The second is Optimal Brain Damage truncation (LeCun et al., 1990), which removes units in decreasing order of their influence on the reconstruction objective, measured in terms of the first and second order terms in its Taylor expansion.", "startOffset": 46, "endOffset": 66}, {"referenceID": 10, "context": "This idea can be combined with various existing approaches to metric learning of kNN and binarized representations (Norouzi et al., 2012; Salakhutdinov & Hinton, 2007; Weinberger & Saul, 2009).", "startOffset": 115, "endOffset": 192}, {"referenceID": 15, "context": "Finally, the nesting idea can be generalized to more complicated dependency structures, such as those described in Tarlow et al. (2012).", "startOffset": 115, "endOffset": 136}], "year": 2014, "abstractText": "In this paper, we study ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results in the simple case of a semi-linear autoencoder. We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA. We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications. Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows codes that are hundreds of times longer than currently feasible for retrieval. We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods. We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction.", "creator": "LaTeX with hyperref package"}}}