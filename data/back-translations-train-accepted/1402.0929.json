{"id": "1402.0929", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2014", "title": "Input Warping for Bayesian Optimization of Non-Stationary Functions", "abstract": "Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions which can be queried efficiently, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in \"log-space,\" to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.", "histories": [["v1", "Wed, 5 Feb 2014 03:55:39 GMT  (955kb,D)", "https://arxiv.org/abs/1402.0929v1", null], ["v2", "Thu, 20 Feb 2014 22:00:38 GMT  (955kb,D)", "http://arxiv.org/abs/1402.0929v2", null], ["v3", "Wed, 11 Jun 2014 20:32:11 GMT  (956kb,D)", "http://arxiv.org/abs/1402.0929v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jasper snoek", "kevin swersky", "richard s zemel", "ryan p adams"], "accepted": true, "id": "1402.0929"}, "pdf": {"name": "1402.0929.pdf", "metadata": {"source": "CRF", "title": "INPUT WARPING FOR BAYESIAN OPTIMIZATION OF NON-STATIONARY FUNCTIONS BY JASPER SNOEK", "authors": ["RICHARD S. ZEMEL", "RYAN P. ADAMS"], "emails": [], "sections": [{"heading": null, "text": "INPUT WARNING FOR BAYESIAN OPTIMATION OF NON-STATIONARY FUNCTIONS"}, {"heading": "BY JASPER SNOEK , KEVIN SWERSKY , RICHARD S. ZEMEL AND RYAN P. ADAMS", "text": "Harvard University and the University of Toronto Bayesian Optimization has proven to be a highly effective method for the global optimization of unknown, expensive, and multimodal functions. Accurately modeling distributions across functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes have flexible precedence over functions, there are several classes of functions that are difficult to model, one of the most common of which is the class of non-stationary functions. Optimizing the hyperparameters of machine learning algorithms is an area of concern where parameters are often a priori transformed manually, for example by optimizing in logspace to mitigate the effects of spatially different length scales. We are developing a methodology for automatically learning a large family of bijective transformations or input space distortions using the beta-cumulative distribution function. We are further expanding the warp framework to include multifunctional Bayesian optimization, so that the results of a common set of tasks can be significantly distorted."}, {"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Background and Related Work", "text": "An attractive feature of the Gaussian process in the context of Bayesian optimization is that, due to a series of observations, the expected output value and the corresponding uncertainty of an unobserved input is easily computed.The properties of the Gaussian process are specified by an average function m: X \u2192 R and a positive definitive covariance, or a series of observations, the expected output value and the corresponding uncertainty of unobserved input points is easily computed.The properties of the Gaussian process are defined by an average function m: X \u2192 R and a positive definition of tasks, FunctionK: X \u00b7 X \u2192 R. Given a finite number of training points IN = {xn, yn} Nn = 1, where xn \u00b2 -X, yn \u00b2 -R, the predictive mean and covariance under a GP can be expressed appropriately as: \u00b5 (x; IN) = m (X) + K (X) > K (X) \u2212 1 (y (X) \u2212 X)."}, {"heading": "3. Input Warping", "text": "We assume that we have a positive, defined covariance function K (x, x) and the prior adjustment of the functions to the respective function (\u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2, 1] D due to the projection of a limited input range onto the hypercube unit. In practice, when we set the hyperparameters of an algorithm, e.g. the regularization parameters of a support vector engine, researchers often first transform the input space with a monotonic function such as the natural logarithm and then perform a grid search in that transformed space. Such optimization in the \"log space\" uses a primary knowledge of the non-stationary inherent in the input space. Often, however, the non-stationary properties of the input space are not known and such a transformation is generally a rough approximation to the ideal (unknown) transformation. Our approach instead is to take into account a class of biological warping functions in evaluating these earlier and objective functions."}, {"heading": "4. Empirical Analyses", "text": "In the second half of the last decade, we have to do it with a series of problems, which we have presented to ourselves in the way we have presented it to us, how we have presented it to ourselves, how we have presented it to ourselves, how we have presented it to ourselves, how we have done it, how we have done it, how we have done it, how we have done it, how we have done it, how we have done it, how it was done, how it was done, how it was done, how it was done, how it was done, how it was done, how it was done, how it was done, how we have done it, how we have done it, how we have done it, how it was done, how it was done, how it was done, how it was done, how it was done, how it was done, how it was done, how it was not done, it is not done, it is not done, it is not done, it is not done, is not done it is not done, it is not done, it is not done, it is not done, it is not done, it is not done, it is not done, is not done it is not done, is not done it, is not done it is not done, is not done it, is not done it is not done it, is not done it, is not done it is not done it, is not done it, is not done it is not done it, is not done it is not done it, is not done it is not done it is not done it, is not done it is not done it is not done it, is not done it is not done it is not done it, is not done it is not it is not done it is not done it, is not done it is not it is not done it is not it is not done it is not it is not done it is not done it, is not it is not done it is not it is not it is not done it is not done it is not it is not it is not done it is not it is not done it is not it is not done, is not done it is not it is not done it is not it is not done it is not it is not done it is not it is not done it is not it is not done it is not it is not done, is not it is not done it is not"}, {"heading": "5. Conclusion", "text": "In this paper, we develop a novel formulation to elegantly model non-stationary functions using Gaussian processes, which are particularly well suited to Bayesian optimization. Our approach uses the cumulative distribution function of the beta distribution to shift the entry space to remove the effects of mild, input-dependent length variants, an approach that allows us to automatically infer a variety of warpings in a computationally efficient manner. In our empirical analysis, we see that the inability to model non-stationary functions is a major weakness when using stationary cores within the GP Bayesian. Our simple approach to learning the form of non-standard performance outperforms the standard optimization routine of Snoek et al. [2012] both in the number of ratings needed to achieve convergence and the method of finding good solutions."}, {"heading": "Acknowledgements", "text": "The authors thank Nitish Srivastava for her help with the Deepnet package. Jasper Snoek is a fellow at the Harvard Center for Research on Computation and Society. During his time at the University of Toronto, Jasper Snoek was supported by a grant from Google. This work was funded by the DARPA Young Faculty Award N66001-12-1-4219, an Amazon AWS in Research Grant, the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Canadian Institute for Advanced Research (CIFAR)."}], "references": [{"title": "A Bayesian interactive optimization approach to procedural animation design", "author": ["Eric Brochu", "Tyson Brochu", "Nando de Freitas"], "venue": "Intelligent Optimization,", "citeRegEx": "Brochu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2009}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams"], "venue": null, "citeRegEx": "Snoek et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2011}, {"title": "Gaussian Processes for Machine Learning", "author": ["Carl E. Rasmussen", "Christopher Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Modeling nonstationary processes through dimension expansion", "author": ["Luke Bornn", "Gavin Shaddick", "James V. Zidek"], "venue": null, "citeRegEx": "Bornn et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bornn et al\\.", "year": 2008}, {"title": "The application of Bayesian methods for seeking the extremum", "author": ["Jonas Mockus", "Vytautas Tiesis", "Antanas Zilinskas"], "venue": null, "citeRegEx": "Mockus et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Mockus et al\\.", "year": 1996}, {"title": "Collaborative hyperparameter tuning", "author": ["R\u00e9mi Bardenet", "M\u00e1ty\u00e1s Brendel", "Bal\u00e1zs K\u00e9gl", "Mich\u00e8le Sebag"], "venue": "Processing Systems,", "citeRegEx": "Bardenet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bardenet et al\\.", "year": 2011}, {"title": "Alex Krizhevsky", "author": ["Information Processing Systems."], "venue": "Learning multiple layers of features from tiny images. Technical report, Department of Computer Science,", "citeRegEx": "Systems.,? 2010", "shortCiteRegEx": "Systems.", "year": 2010}, {"title": "Improving neural networks", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "We further observe that on four different challenging machine learning optimization tasks our method outperforms that of Snoek et al. [2012], consistently converging to a better result in fewer function evaluations.", "startOffset": 121, "endOffset": 141}, {"referenceID": 1, "context": "or the ARD Mat\u00e9rn 5/2 kernel advocated for hyperparameter tuning with Bayesian optimization by Snoek et al. [2012]:", "startOffset": 95, "endOffset": 115}, {"referenceID": 2, "context": ", 1998, Rasmussen and Williams, 2006]. Previously, Sampson and Guttorp [1992] proposed projecting the inputs into a stationary latent space using a combination of metric multidimensional scaling and thin plate splines.", "startOffset": 8, "endOffset": 78}, {"referenceID": 2, "context": ", 1998, Rasmussen and Williams, 2006]. Previously, Sampson and Guttorp [1992] proposed projecting the inputs into a stationary latent space using a combination of metric multidimensional scaling and thin plate splines. Schmidt and O\u2019Hagan [2003] extended this warping approach for general GP regression problems using a flexible GP mapping.", "startOffset": 8, "endOffset": 246}, {"referenceID": 2, "context": ", 1998, Rasmussen and Williams, 2006]. Previously, Sampson and Guttorp [1992] proposed projecting the inputs into a stationary latent space using a combination of metric multidimensional scaling and thin plate splines. Schmidt and O\u2019Hagan [2003] extended this warping approach for general GP regression problems using a flexible GP mapping. Spatial deformations of two dimensional inputs have been studied extensively in the spatial statistics literature [Anderes and Stein, 2008]. Bornn et al. [2012] project the inputs into a higher dimensional stationary latent representation.", "startOffset": 8, "endOffset": 502}, {"referenceID": 2, "context": ", 1998, Rasmussen and Williams, 2006]. Previously, Sampson and Guttorp [1992] proposed projecting the inputs into a stationary latent space using a combination of metric multidimensional scaling and thin plate splines. Schmidt and O\u2019Hagan [2003] extended this warping approach for general GP regression problems using a flexible GP mapping. Spatial deformations of two dimensional inputs have been studied extensively in the spatial statistics literature [Anderes and Stein, 2008]. Bornn et al. [2012] project the inputs into a higher dimensional stationary latent representation. Snelson et al. [2003] apply a warping to the output space, y, while Adams and Stegle [2008] perform input-dependent output scaling with a second Gaussian process.", "startOffset": 8, "endOffset": 603}, {"referenceID": 2, "context": ", 1998, Rasmussen and Williams, 2006]. Previously, Sampson and Guttorp [1992] proposed projecting the inputs into a stationary latent space using a combination of metric multidimensional scaling and thin plate splines. Schmidt and O\u2019Hagan [2003] extended this warping approach for general GP regression problems using a flexible GP mapping. Spatial deformations of two dimensional inputs have been studied extensively in the spatial statistics literature [Anderes and Stein, 2008]. Bornn et al. [2012] project the inputs into a higher dimensional stationary latent representation. Snelson et al. [2003] apply a warping to the output space, y, while Adams and Stegle [2008] perform input-dependent output scaling with a second Gaussian process.", "startOffset": 8, "endOffset": 673}, {"referenceID": 0, "context": ", 1978], see Brochu et al. [2010] or Lizotte [2008] for an in-depth explanation and review.", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": ", 1978], see Brochu et al. [2010] or Lizotte [2008] for an in-depth explanation and review.", "startOffset": 13, "endOffset": 52}, {"referenceID": 0, "context": ", 1978], see Brochu et al. [2010] or Lizotte [2008] for an in-depth explanation and review. The strategy relies on the use of a relatively cheap probabilistic model that can be queried liberally as a surrogate in order to more effectively evaluate an expensive function of interest. Bayes\u2019 rule is used to derive the posterior estimate of the true function, given observations, and the surrogate is then used to determine, via a proxy optimization over an acquisition function, the next most promising point to query. Using the posterior mean and variance of the probabilistic model, the acquisition function generally expresses a tradeoff between exploitation and exploration. Numerous acquisition functions and combinations thereof have been proposed [e.g., Kushner, 1964, Srinivas et al., 2010, Hoffman et al., 2011]. In this work, we follow the common approach, which is to use a GP to define a distribution over objective functions from the input space to a loss that one wishes to minimize. Our approach is based on that of Jones [2001]. Specifically, we use a GP surrogate, and the expected improvement acquisition function [Mockus et al.", "startOffset": 13, "endOffset": 1043}, {"referenceID": 5, "context": "Other approaches include Bardenet et al. [2013], which finds a joint latent function over tasks explicitly using a ranking model, and Hutter et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 5, "context": "Other approaches include Bardenet et al. [2013], which finds a joint latent function over tasks explicitly using a ranking model, and Hutter et al. [2011] which uses a set of auxiliary task features to improve prediction.", "startOffset": 25, "endOffset": 155}, {"referenceID": 1, "context": "We treat the collection {\u03b1d, \u03b2d}d=1 as hyperparameters of the covariance function and use Markov chain Monte Carlo via slice sampling, following the treatment of covariance hyperparameters from Snoek et al. [2012]. We use a log-normal distribution, i.", "startOffset": 194, "endOffset": 214}, {"referenceID": 1, "context": ", 2011] and Spearmint [Snoek et al., 2012]. The results for SMAC, Spearmint and TPE are reproduced from Eggensperger et al. [2013]. Following the standard protocol for these benchmarks, each algorithm was run ten times for the given number of evaluations, and the average validation loss and standard deviation are reported.", "startOffset": 23, "endOffset": 131}, {"referenceID": 1, "context": "In the first experiment, we compare to the method of Snoek et al. [2012] in order to demonstrate the effectiveness of input warping.", "startOffset": 53, "endOffset": 73}, {"referenceID": 1, "context": "In the first experiment, we compare to the method of Snoek et al. [2012] in order to demonstrate the effectiveness of input warping. In the second experiment, we compare to other hyperparameter optimization methods using a subset of the benchmark suite found in Eggensperger et al. [2013]. Finally, we show how our multi-task extension can further benefit this important setting.", "startOffset": 53, "endOffset": 289}, {"referenceID": 1, "context": "Experimental setup We evaluate the standard Gaussian process expected improvement algorithm (GP EI MCMC) as implemented by Snoek et al. [2012], with and without warping.", "startOffset": 123, "endOffset": 143}, {"referenceID": 1, "context": "Experimental setup We evaluate the standard Gaussian process expected improvement algorithm (GP EI MCMC) as implemented by Snoek et al. [2012], with and without warping. Following their treatment, we use the Mat\u00e9rn 5/2 kernel and we marginalize over kernel parameters \u03b8 using slice sampling [Murray and Adams, 2010]. We repeat three of the experiments1 from Snoek et al. [2012], and perform an experiment involving the tuning of a deep convolutional neural network2 on a subset of the popular CIFAR-10 data set [Krizhevsky, 2009].", "startOffset": 123, "endOffset": 378}, {"referenceID": 1, "context": "See Snoek et al. [2012] for details of these experiments.", "startOffset": 4, "endOffset": 24}, {"referenceID": 1, "context": "Our simple approach to learn the form of the non-stationarity significantly outperforms the standard Bayesian optimization routine of Snoek et al. [2012] both in the number of evaluations it takes to converge and the value reached.", "startOffset": 134, "endOffset": 154}], "year": 2014, "abstractText": "Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in \u201clog-space,\u201d to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.", "creator": "LaTeX with hyperref package"}}}