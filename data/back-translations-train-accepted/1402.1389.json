{"id": "1402.1389", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2014", "title": "Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models", "abstract": "The recently developed Bayesian Gaussian process latent variable model (GPLVM) is a powerful generative model for discovering low dimensional embeddings in linear time complexity. However, modern datasets are so large that even linear-time models find them difficult to cope with. We introduce a novel re-parametrisation of variational inference for the GPLVM and sparse GP model that allows for an efficient distributed inference algorithm.", "histories": [["v1", "Thu, 6 Feb 2014 16:08:40 GMT  (210kb,D)", "http://arxiv.org/abs/1402.1389v1", "9 pages, 8 figures"], ["v2", "Mon, 29 Sep 2014 21:16:47 GMT  (172kb,D)", "http://arxiv.org/abs/1402.1389v2", "9 pages, 8 figures"]], "COMMENTS": "9 pages, 8 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yarin gal", "mark van der wilk", "carl e rasmussen"], "accepted": true, "id": "1402.1389"}, "pdf": {"name": "1402.1389.pdf", "metadata": {"source": "META", "title": "Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models", "authors": ["Yarin Gal", "Mark van der Wilk", "Carl E. Rasmussen"], "emails": ["YG279@CAM.AC.UK", "MV310@CAM.AC.UK", "CER54@CAM.AC.UK"], "sections": [{"heading": null, "text": "We present a uniform derivative for both models and analytically derive the optimal variation distribution across the inducing points. We then evaluate the proposed conclusion on data sets of different sizes and show that it can be easily scaled with both data and computing resources. Furthermore, we demonstrate its practicality in the real world using data sets of up to 100,000 points, compare the conclusion with sequential implementations, evaluate the load distribution between the different nodes and test its robustness against network failures."}, {"heading": "1. Introduction", "text": "It's only a matter of time before it's as far as it's ever been."}, {"heading": "2. The Gaussian Process Latent Variable Model and Sparse GP Regression", "text": "Next, we will review the sparse Gaussian process regression model and the Gaussian process latent variable model (GPLVM) and review the model structure and the approximations developed (Titsias & Lawrence, 2010; Titsias, 2009) to make the conclusion efficient."}, {"heading": "2.1. Sparse Gaussian Process Regression", "text": "Considering a training data set consisting of n inputs {X1,., Xn} and their respective outputs {F1,.., Fn}, we would like to find the rear distribution of the functions that represent the inputs to the outputs. Functions are assumed to be d dimensional, while inputs q are dimensional. These data are often written in matrix form to ensure convenience: X-Rn \u00b7 qF-Rn \u00b7 dFi = g (Xi) Here we will adopt the convention that capitals denote the matrices of the data, while the subscribed vectors of the same letter have a series of clues, i.e. a single data point. For example, Fi2see github.com / markvdw denote the i'th function values, while F denotes the matrix of all the given function values. In regression setting, we represent a Gaussian process before the space of the functions."}, {"heading": "2.2. Gaussian Process Latent Variable Models", "text": "The GPLVM model is the unattended equivalent of the above regression problem, which can be regarded as a non-linear generalization of PCA (Lawrence, 2005).The model structure is identical to the regression problem, except that we start from a predecessor model using the now latent variable X and try to derive both the mapping from X to Y and the distribution over X. Xi \u0445 N (Xi; 0, I) F (Xi) \u0445 GP (0, k (X, X))) Yi \u0445 N (Fi, \u03b2 \u2212 1I) A variational Bayes approximation for this model was developed by Titsias & Lawrence (2010), which uses techniques similar to those used for low-variation GPs. In fact, the sparse GP can be regarded as a special case of the GPLVM, in which the inputs are given as zero variance (Fi, \u03b2 \u2212 1I).The main task in deriving an approximate inference revolves around finding a variation X (detail) (Y = (p)."}, {"heading": "3. Parallel inference", "text": "We now use the conditional independence of the data that results from the inductive points to derive a parallel q-schema for the sparse GP model and the GPLVM, which enables us to easily scale these models to large datasets. The key equations are given below, with a detailed explanation given in sections q (sections 3 and 4 of the supplementary material). We present a unifying derivative of the inference procedure for the case of regression and the case of the latent variables (LVM), in which we determine that the explicit inputs in the case of regression are identical to the latent inputs in the case of LVM if their mean p is set to the observed inputs and X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-"}, {"heading": "3.1. Decoupling the data conditioned on the inducing points", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.2. The parallel inference", "text": "Using the Map-Reduce q framework (Dean & Ghemawat, 2008), we can maintain various subsets of input factors and corresponding outputs on each node in a parallel implementation, and distribute the global parameters (such as the kernel hyperparameters and the places of the inducing points) among the nodes. We designate by G the global parameters over which we need to perform optimizations, including Z (the places of the inducing points), \u03b2 (the observation noise), and k (the set of kernel hyperparameters), and we designate by Lk the set of local parameters over which we need to perform optimizations, including Z (the places of the inducing points), \u03b2 (the observation noise), and K (the set of kernel hyperparameters)."}, {"heading": "4. Experimental Evaluation", "text": "We investigated the scalability of the conclusion using a variety of experiments with computational power and data and examined the numerical stability of the conclusion; we also investigated the distribution of the load across the different nodes and compared the conclusion with sequential implementations such as GPy (Titsias & Lawrence, 2010); finally, we tested the robustness of our parallel inference method by accidental omission of nodes and measured the resulting log marginal probability; we tested the performance of the GPLVM on large datasets that are not commonly treated in the GP community; and performed dimensionality reductions and density estimates on the entire USPS dataset (4K dataset) and datasets with hundreds of thousands of points."}, {"heading": "4.1. Implementation & Setup", "text": "In the following experiments, we used an SE-ARD kernel over latent space to automatically determine the intrinsic dimensionality of latent space, as in (Titsias & Lawrence, 2010).We initialize our latent points with PCA and our inducing points with k-means and additional noise. We optimize our latent points using a scaled conjugate gradient (M\u00f8ller, 1993) after the original implementation of (Titsias & Lawrence, 2010).Our experiments were conducted on a machine with 4 processors, 64-core Opteron 6276. One caveat regarding this processor is that there is only one floating point unit (FPU) per two cores. Often, a single thread does not fully utilize the FPU, so that a smart statement allows two threads to use an FPU without much performance deterioration."}, {"heading": "4.2. Scaling with Computation Power", "text": "We are investigating how much inferences to a given dataset can be accelerated with our parallel implementation, as more computing resources are available. Ideally, we would see a halving of the time, as twice as many resources are available, but due to the overheads involved in distributing the calculation, it is usually only possible to get close. We are assessing the improvement in the runtime of the algorithm using a simple synthetic dataset, from which large amounts of data could be easily generated, the dataset was achieved by simulating 1D latent space and converting these into 3D observations using superimposed linear functions (see Figure 1). 100k points were generated and the algorithm was executed using an increasing number of cores and a 2-dimensional latent space. We measured both the total runtime of the algorithm, including initialization and threading of overheads, as well as the amount of time spent only in the two cores (see Figure 1)."}, {"heading": "4.3. Scaling with Data", "text": "With the same setup, we evaluated runtime scaling by increasing both the size of the dataset and the computing resources equally, which answers the question of how large a dataset can be in the face of an increasing amount of resources. If the data is doubled, we double the number of available CPUs. Ideally, without constant costs and without threading overheads, the computing time should be constant. Again, we measure the total runtime of the algorithm and the time spent only on map reduction functions. Figure 3 shows that we are able to use the additional computing resources effectively."}, {"heading": "4.4. Comparison to GPy", "text": "We also compare the calculation time of our inference procedure with the individual but highly optimized GPy implementation (see Figure 3). We show that we significantly outperform GPy with more computing resources. However, our parallel conclusion allows us to run the GPLVM on data sets that would simply take too long to run with a single thread implementation. However, GPy is significantly faster with small data sets, partly due to the high overheads discussed in the previous experiment, and partly due to optimizations in GPy. Just as in the original paper (Titsias & Lawrence, 2010), we use the oil flow data sets in native C + +, while our implementation is written entirely in Python. In addition to scaling experiments, we compare our latent with GPy, which we use as a reference implementation, and, just like in the original paper (Titsias & Lawrence, 2010) we use the oil flow data sets in native C + + as a latent, while our implementation is not represented in both until the python is fully implemented."}, {"heading": "4.5. USPS Data", "text": "As a practical test, we performed a GPLVM over the entire set of 4649 examples with all digits from 0-9 using 150 inducing points. Training was done overnight and completed the following day. We were able to reconstruct digits where 34% of their pixels were missing (see Figure 6). We performed the same experiment with only 1000 digits to assess the benefit of using more data in the GPLVM. We compared the average reconstruction error and found that the larger dataset has a 5.9% improvement. As this dataset can now be executed overnight, a greater number of development cycles can be used to fine-tune the model."}, {"heading": "5. Practicality of the Inference", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Distribution of the Load", "text": "One of our stated requirements for a practical parallel inference algorithm is an approximately equal load distribution to the nodes. This is particularly relevant in a MapReduce framework, where the reduction step can only occur after completion of all map calculations, so the maximum execution time of a thread is the speed limiting step. Figure 5 shows the minimum, maximum and average execution time of all threads for a series of iterations of a pass. On average, there is a difference of 3.7% between the average and maximum runtime of a thread, indicating an even load distribution."}, {"heading": "5.2. Robustness to Node Failure", "text": "Another desirable feature of a parallel inference scheme is robustness up to the failure of the nodes. One way to deal with this would be to load the data to another node and restart the calculation. However, since the speed of an iteration is limited by the slowest calculation of failure on one of the nodes, this could slow the algorithm down to the time it takes to load the intermediate data on the new node. An alternative strategy would be to drop the partial term from the calculation and use a slightly noisy gradient calculation in the optimization for one iteration. Here, we examine the robustness of our inference on this method. We have our parallel inferences on the oil flow data using the same setting as over 500 iterations that accumulate the marginal probability as a function of iteration."}, {"heading": "6. Related Work", "text": "Recent research conducted by Hensman et al. (2013) suggested a stochastic variation conclusion (SVI, Hoffman et al. (2013) for the problem of extending the sparse Gaussian process regression. In their research, the variation distribution via the inducing points was explicitly presented and the optimization was performed via the variation distribution itself, rather than using the optimal analytical solution, a necessity of SVI adjustment that cannot be prevented. Hensman et al. (2013) proposed future research guidelines that include deriving SVI for GPLVMs and suggested that the proposed SVI for the sparse GP regression could be performed in parallel. However, in order to execute a weaker limit probability of the protocol than that proposed by Titsias (2009), a derivation of SVI for GPLVMs must be used, and many parameters must be optimized in addition to the kernel hyperparameters."}, {"heading": "7. Conclusions", "text": "We have scaled the GPLVM model and the sparse GPs model, which presents the first unifying parallel inference algorithm capable of processing datasets with hundreds of thousands of data points. An extensive series of experiments has been presented that examine the properties of the proposed conclusion, which has been implemented for a multi-core architecture and is available as an open source package that includes a fully documented implementation of the derivatives with references to the equations set out in the supplementary explanatory material."}], "references": [{"title": "The theory of linear models and multivariate analysis. Wiley series in probability and mathematical statistics: Probability and mathematical statistics", "author": ["S.F. Arnold"], "venue": null, "citeRegEx": "Arnold,? \\Q1981\\E", "shortCiteRegEx": "Arnold", "year": 1981}, {"title": "Asynchronous distributed learning of topic models", "author": ["Asuncion", "Arthur U", "Smyth", "Padhraic", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Asuncion et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2008}, {"title": "Parallel Markov Chain Monte Carlo simulation by Pre-Fetching", "author": ["A.E. Brockwell"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Brockwell,? \\Q2006\\E", "shortCiteRegEx": "Brockwell", "year": 2006}, {"title": "MapReduce: Simplified data processing on large clusters", "author": ["Dean", "Jeffrey", "Ghemawat", "Sanjay"], "venue": "Commun. ACM,", "citeRegEx": "Dean et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2008}, {"title": "A systematic Bayesian treatment of the IBM alignment models", "author": ["Gal", "Yarin", "Blunsom", "Phil"], "venue": "In Proceedings of NAACL-HLT, pp", "citeRegEx": "Gal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2013}, {"title": "Gaussian processes for big data", "author": ["Hensman", "James", "Fusi", "Nicolo", "Lawrence", "Neil D"], "venue": null, "citeRegEx": "Hensman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2013}, {"title": "Stochastic Variational Inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John"], "venue": "JOURNAL OF MACHINE LEARNING RESEARCH,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Probabilistic non-linear principal component analysis with gaussian process latent variable models", "author": ["Lawrence", "Neil"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lawrence and Neil.,? \\Q2005\\E", "shortCiteRegEx": "Lawrence and Neil.", "year": 2005}, {"title": "A scaled conjugate gradient algorithm for fast supervised learning", "author": ["M\u00f8ller", "Martin Fodslette"], "venue": "Neural networks,", "citeRegEx": "M\u00f8ller and Fodslette.,? \\Q1993\\E", "shortCiteRegEx": "M\u00f8ller and Fodslette.", "year": 1993}, {"title": "A unifying view of sparse approximate gaussian process regression", "author": ["Qui\u00f1onero-Candela", "Joaquin", "Rasmussen", "Carl Edward"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Qui\u00f1onero.Candela et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Qui\u00f1onero.Candela et al\\.", "year": 2005}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Rasmussen", "Carl Edward", "Williams", "Christopher K. I"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Sparse gaussian processes using pseudo-inputs", "author": ["Snelson", "Edward", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Snelson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snelson et al\\.", "year": 2006}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["M.K. Titsias"], "venue": "Technical report, Technical Report,", "citeRegEx": "Titsias,? \\Q2009\\E", "shortCiteRegEx": "Titsias", "year": 2009}, {"title": "Bayesian gaussian process latent variable model", "author": ["Titsias", "Michalis", "Lawrence", "Neil"], "venue": null, "citeRegEx": "Titsias et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2010}, {"title": "Parallel Bayesian computation", "author": ["Wilkinson", "Darren J"], "venue": "Handbook of Parallel Computing and Statistics,", "citeRegEx": "Wilkinson and J.,? \\Q2005\\E", "shortCiteRegEx": "Wilkinson and J.", "year": 2005}], "referenceMentions": [{"referenceID": 12, "context": "Originating as an extension of sparse Gaussian process regression (Titsias, 2009), it can be used to perform non-linear dimensionality reductions in linear time complexity.", "startOffset": 66, "endOffset": 81}, {"referenceID": 12, "context": "The Bayesian Gaussian process latent variable model (GPLVM, Titsias & Lawrence (2010)) forms an important component in the Bayesian non-parametric arsenal.", "startOffset": 60, "endOffset": 86}, {"referenceID": 2, "context": "Many have reasoned about the requirements such distributed inference procedures should satisfy (Brockwell, 2006; Wilkinson, 2005; Asuncion et al., 2008).", "startOffset": 95, "endOffset": 152}, {"referenceID": 1, "context": "Many have reasoned about the requirements such distributed inference procedures should satisfy (Brockwell, 2006; Wilkinson, 2005; Asuncion et al., 2008).", "startOffset": 95, "endOffset": 152}, {"referenceID": 12, "context": "We derive an exact unifying reparametrisation of the bounds derived by Titsias (2009) and Titsias & Lawrence (2010) which allows us to perform inference using the original guarantees without the need for weaker lower bounds, and using the optimal variational distribution over the inducing points.", "startOffset": 71, "endOffset": 86}, {"referenceID": 12, "context": "We derive an exact unifying reparametrisation of the bounds derived by Titsias (2009) and Titsias & Lawrence (2010) which allows us to perform inference using the original guarantees without the need for weaker lower bounds, and using the optimal variational distribution over the inducing points.", "startOffset": 71, "endOffset": 116}, {"referenceID": 12, "context": "We will review the model structure and the approximations developed (Titsias & Lawrence, 2010; Titsias, 2009) to make the inference efficient.", "startOffset": 68, "endOffset": 109}, {"referenceID": 0, "context": "Qui\u00f1onero-Candela & Rasmussen We follow the definition of matrix normal distribution (Arnold, 1981).", "startOffset": 85, "endOffset": 99}, {"referenceID": 0, "context": "Qui\u00f1onero-Candela & Rasmussen We follow the definition of matrix normal distribution (Arnold, 1981). For a full treatment of Gaussian Processes, see Rasmussen & Williams (2006).", "startOffset": 86, "endOffset": 177}, {"referenceID": 12, "context": "On the other hand, Titsias (2009) relates this approximation to a variational approximation, with Z as variational parameters.", "startOffset": 19, "endOffset": 34}, {"referenceID": 12, "context": "A Variational Bayes approximation for this model has been developed by Titsias & Lawrence (2010) using similar techniques as for variational sparse GPs.", "startOffset": 71, "endOffset": 97}, {"referenceID": 12, "context": "In the next section we derive a parallel inference scheme for both models following a reparametrisation of the derivations of Titsias (2009) which allows us to decouple the distribution over data points.", "startOffset": 126, "endOffset": 141}, {"referenceID": 12, "context": "Up to here the derivation is identical to the two derivations given in (Titsias & Lawrence, 2010; Titsias, 2009).", "startOffset": 71, "endOffset": 112}, {"referenceID": 12, "context": "Notice that the obtained unifying bound is identical to the ones derived in (Titsias, 2009) for the regression case and (Titsias & Lawrence, 2010) for the LVM case since \u3008Kmi\u3009q(Xi) = Kmi for q(Xi) with variance 0 and mean Xi.", "startOffset": 76, "endOffset": 91}, {"referenceID": 5, "context": "Recent research carried out by Hensman et al. (2013) proposed stochastic variational inference (SVI, Hoffman et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 5, "context": "Recent research carried out by Hensman et al. (2013) proposed stochastic variational inference (SVI, Hoffman et al. (2013)) for the problem of scaling up sparse Gaussian process regression.", "startOffset": 31, "endOffset": 123}, {"referenceID": 5, "context": "Recent research carried out by Hensman et al. (2013) proposed stochastic variational inference (SVI, Hoffman et al. (2013)) for the problem of scaling up sparse Gaussian process regression. In their research, the variational distribution over the inducing points was explicitly represented and optimisation was performed over the variational distribution itself instead of using the optimal analytical solution, a necessity of the SVI setting that cannot be averted. Hensman et al. (2013) proposed future directions for research which include the derivation of SVI for GPLVMs, and suggested that the proposed SVI for sparse GP regression could be carried out in parallel.", "startOffset": 31, "endOffset": 489}, {"referenceID": 5, "context": "Furthermore, in the SVI setting many additional optimiser parameters have to be introduced and fine-tuned by hand (Hensman et al., 2013) to control the step-length of different quantities such as the gradient.", "startOffset": 114, "endOffset": 136}, {"referenceID": 11, "context": "However, in order to perform SVI a weaker lower bound on the log marginal likelihood than the one proposed by Titsias (2009) has to be used, and many parameters have to be optimised in addition to the kernel hyper-parameters.", "startOffset": 110, "endOffset": 125}], "year": 2017, "abstractText": "The recently developed Bayesian Gaussian process latent variable model (GPLVM) is a powerful generative model for discovering low dimensional embeddings in linear time complexity. However, modern datasets are so large that even linear-time models find them difficult to cope with. We introduce a novel re-parametrisation of variational inference for the GPLVM and sparse GP model that allows for an efficient distributed inference algorithm. We present a unifying derivation for both models, analytically deriving the optimal variational distribution over the inducing points. We then assess the suggested inference on datasets of different sizes, showing that it scales well with both data and computational resources. We furthermore demonstrate its practicality in real-world settings using datasets with up to 100 thousand points, comparing the inference to sequential implementations, assessing the distribution of the load among the different nodes, and testing its robustness to network failures.", "creator": "LaTeX with hyperref package"}}}