{"id": "1402.1454", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2014", "title": "An Autoencoder Approach to Learning Bilingual Word Representations", "abstract": "Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. Since training autoencoders on word observations presents certain computational issues, we propose and compare different variations adapted to this setting. We also propose an explicit correlation maximizing regularizer that leads to significant improvement in the performance. We empirically investigate the success of our approach on the problem of cross-language test classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). These experiments demonstrate that our approaches are competitive with the state-of-the-art, achieving up to 10-14 percentage point improvements over the best reported results on this task.", "histories": [["v1", "Thu, 6 Feb 2014 18:53:30 GMT  (778kb,D)", "http://arxiv.org/abs/1402.1454v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["a p sarath chandar", "stanislas lauly", "hugo larochelle", "mitesh m khapra", "balaraman ravindran", "vikas c raykar", "amrita saha"], "accepted": true, "id": "1402.1454"}, "pdf": {"name": "1402.1454.pdf", "metadata": {"source": "META", "title": "An Autoencoder Approach to Learning Bilingual Word Representations", "authors": ["Sarath Chandar", "Stanislas Lauly", "Hugo Larochelle", "Mitesh M. Khapra", "Balaraman Ravindran"], "emails": ["APSARATHCHANDAR@GMAIL.COM", "STANISLAS.LAULY@USHERBROOKE.CA", "HUGO.LAROCHELLE@USHERBROOKE.CA", "MIKHAPRA@IN.IBM.COM", "RAVI@CSE.IITM.AC.IN", "VIRAYKAR@IN.IBM.COM", "AMRSAHA4@IN.IBM.COM"], "sections": [{"heading": null, "text": "on the problem of cross-language test classification, where a classifier trained in a particular language (e.g. English) must learn to generalize to another language (e.g. German). These experiments show that our approaches compete with the state of the art and achieve up to 10-14 percentage points improvements over the best reported results in this task."}, {"heading": "1. Introduction", "text": "Languages such as English, which have a lot of annotated resources at their disposal, have better capabilities of multiple language processing (NLP) 2001 capabilities than other languages that are not so fortunate in terms of annotated resources. For example, high-quality POS taggers (Toutanova et al., 2003), parsers (Socher et al., 2013), sensation analyzers (Liu, 2012) are already available for English, but this is not the case for many other languages such as Hindi, Marathi, Bodo, Farsi, Urdu, etc. This situation was acceptable in the past when only a few languages dominated the available digital content online and elsewhere. However, the ever-increasing number of languages on the web today is important to accurately process natural language data in such less-favoured languages. An obvious solution for Xiv: 140 2.14 54v1 [to this problem is improving the annotated inventory of these languages, but the associated cost, time and effort as a natural deterrent."}, {"heading": "2. Autoencoder for Bags-of-Words", "text": "Let x be the wordbag representation of a sentence. Specifically, each xi is a word index from a fixed vocabulary of V words. Since it is a wordbag representation, the order of words within x does not correspond to the order of words in the original sentence. We would like to learn a D-dimensional vector representation of our words from a set of wordbags {x (t)} Tt = 1. We propose to achieve this by using an encoder model that encodes an input bag representation x with a sum of the representations (embeddings) of the words contained in x, followed by a nonlinearity. Specifically, let the matrix W be the D \u00b7 V matrix, the columns of which are the vector representations for each word. The composition of the encoder includes the summation of the columns of W for each word in the wordbag representation. We will consider this encoder function."}, {"heading": "2.1. Binary bag-of-words reconstruction training with merged mini-batches", "text": "In the aforementioned way, in which we see ourselves able to change the world, we have not succeeded in changing the world, but in changing the world."}, {"heading": "2.2. Tree-based decoder training", "text": "We assume that we can achieve a probability distribution over any word. (...) We assume that we can achieve a probability distribution over any word. (...) We assume that we can achieve a probability distribution over any word. (...) We assume that we start from another word. (...) We assume that we start from another word. (...) We assume that we start from another word. (...) We assume that we start from another word. (...) We assume that we have a probability distribution over any word. (...) We assume that we speak from another word. (...) We assume that we start from another word. (...) We assume that we start from another word. (...) We assume that we have a probability distribution over any word. (...) We assume that we work from another word. (...) We assume that we work from another word. (...) We assume that we work from another word. (...) We work from another word."}, {"heading": "3. Bilingual autoencoders", "text": "Suppose that for each set of bag-of-words x in any source language X, we have an associated bag-of-words y for the same sentence that is translated into any target language Y by a human expert.Suppose we have a set of such (x, y) pairs that we want to use to learn representations in both languages that are aligned so that pairs of translated words have similar representations. To achieve this, we propose to extend the regular autoencoder proposed in Section 2 so that the sentence representation in a particular language can be used to attempt a reconstruction of the original sentence in the other language (so that pairs of translated words have similar representations).Specifically, we now define language-specific word representations that represent Wx and Wy, according to the languages of the words in x and y. Let us also let V X and V Y be the number of words in the vocabulary of both languages that may be different."}, {"heading": "3.1. Cross-lingual correlation regularization", "text": "The bilingual encoder suggested above can be further enriched by ensuring that the embeddings learned for a particular pair (x, y) are highly correlated. We achieve this by adding a correlation term to the objective function. Specifically, we could optimize \"(x, y) +\" (y, x) \u2212 \u03bb \u00b7 cor (\u03c6 (x), \u03c6 (y) (8), where cor (\u03c6 (x), \u03c6 (y)) is the correlation between the encoder representations learned for x and y and \u03bb is a scaling factor that ensures that the three terms in the loss function have the same range. Note that this approach could be used for either the binary wordbook or the tree-based reconstruction autoencoder."}, {"heading": "3.2. Document representations", "text": "Once we have learned the language-specific word representation matrices Wx and Wy as described above, we can use them to construct document representations by using their columns as vector representations for words in both languages. Now that a document d is written in the language Z-X-Y and contains m words, z1, z2,..., zm, we present it as the tf-idf weighted sum of its word representations: \u0432 (d) = m \u2211 i = 1 tf-idf (zi) \u00b7 WZ., zi (9) We use the document representations obtained in this way to train our document classifiers in the task of lingual document classification described in Section 5."}, {"heading": "4. Related Work", "text": "Klementiev et al. (2012) propose to simultaneously learn two models of neural network languages, along with a regularization concept that encourages pairs of frequently oriented words to have similar word embeddings. Zou et al. (2013) use a similar approach, with a different form for regularization and neural network language models as in (Collobert et al., 2011). In our paper, we mention in particular the work of Gao et al. (2013), which showed that a useful linear mapping between separately trained monolingual language models learning multilingual representation of words or phrases can be comparably useful."}, {"heading": "5. Experiments", "text": "The techniques proposed in this thesis allow us to learn bilingual embeddings that capture cross-lingual similarities between words. We propose to evaluate the quality of these embeddings by using them for the task of cross-lingual document classification, following the same structure as Klementiev et al. (2012) and comparing it to their methodology. The structure is as follows: A labeled record of documents in any language X is available to train a classifier, but we are interested in classifying documents in another language Y at the test date. To achieve this, we use some bilingual corporations that are not labeled with any categories at the document level. This bilingual corpora is instead used to learn document representations in both languages X and Y, which are encouraged to be invariant in translations from one language to another. Therefore, the hope is that we can successfully apply the document representations for language X directly to the document classifiers Y for the language."}, {"heading": "5.1. Data", "text": "For learning the bilingual embeddings we used the English section of the Europarlcorpus (Koehn, 2005), which contains about 2 million parallel sentences. As already mentioned, we do not use word alignments between these parallel sentences. We use the same procedure as Kle-mentiev et al. (2012). Specifically, we use the sentences with NLTK (Bird Steven and Klein), remove punctuations and Lowercase all words. We do not remove stopwords (similar to Klementiev et al. (2012)."}, {"heading": "5.2. Cross language classification", "text": "Our general procedure for cross-language classification can be summarised as follows: \u2022 Train bilingual word representations Wx and Wy on sentence pairs extracted from Europarl v7 for languages X and Y. \u2022 Train document classifier on the Reuters training set for language X, where documents are represented using the word representations Wx. \u2022 Use the classifier trained in the previous step on the Reuters test set for language Y, using the word representations Wy to represent the documents.As in Klementiev et al. (2012) we used an average perceptron to train a multi-class classifier for 10 epochs, for all the experiments (Klementiev et al. (2012) report that the results were not sensitive to the number of epochs)."}, {"heading": "5.3. Different models for learning embeddings", "text": "Using the different autoencoder architectures and the previously proposed optional correlation-based regularization term, we trained 3 different models for learning bilingual embedding. Each of these models is described below. \u2022 BAE-tr: uses tree-based decoder training (see Section 2.2). \u2022 BAE-cr: uses the correlation-based regularization term (see Section 3.1). \u2022 BAE-cr / corr: uses reconstruction error-based decoder training (see Section 2.1), but unlike BAE-cr uses the correlation-based regularization term (see Section 3.1). As we will see, BAE-cr is our worst performing model, so in this experiment we can determine whether correlation regularization can play an important role in improving display quality. All of the above models were selected up to 20 epochs using the same data as the previous BAE training year (all results relate to this word ratio = D in the set)."}, {"heading": "6. Results and Discussions", "text": "Before discussing the results of the cross-language classification, we first want to give a qualitative sense of the embeddings learned with our method by conducting a small experiment in which we select a few English words and list the ten English and German words that are most similar to these words (in terms of the Euclidean distance between their embeddings, as learned by BAE-cr / corr).Table 1 shows the result of this experiment. For example, Table 1 shows that in all cases the German word that comes closest to a given English word is actually the translation of that English word. Also, note that the model is able to capture semantic similarities between words by embedding semantically similar words (such as, (January, March), (said, said), (market, commercial), etc.) close together."}, {"heading": "6.1. Comparison of the performance of different models", "text": "We now present the results of the cross-language classification obtained by using the embedding of the three models described above. We also compare our models with the following approaches reported in Klementiev et al. (2012): \u2022 Klementiev et al.: This model uses word embedding learned through a multitask neural network language model with a regularization term that encourages pairs of frequently aligned words to use similar word embedding, from which document representations are calculated as described in Section 3.2. \u2022 MT: Here test documents are translated into the language of the training documents using a machine translation system. \u2022 MOSES1, an MT system based on standard phrases, uses standard parameters, and a 5 gram language model was in fact constructed on the Europarl v7 corpus (the same as the one used to obtain our bilingual test documents)."}, {"heading": "6.2. Effect of varying training size", "text": "Next, we evaluate the effect of varying the amount of monitored training data for training the classifier with either BAE-tr, BAE-cr / corr or Klementiev et al. (2012). We are experimenting with training sizes of 100, 200, 500, 1000, 5000 and 10000. These results for EN-DE and DEEN are summarised in Figure 3 and Figure 4 respectively. We observe that BAE-cr / corr significantly outperforms the other models on almost all data sizes. More importantly, it performs remarkably well on very small data sizes (t = 100), suggesting that it actually learns very meaningful embeddings that can be generalised well even with small data sizes."}, {"heading": "6.3. Effect of coarser alignments", "text": "The excellent performance of BAE-cr / corr suggests that merging minibatches into single word embedding does not significantly affect the quality of word embedding. In other words, not only do we not need to rely on word alignments, but exact sentence alignment is not strictly necessary to achieve good performance, so it is natural to ask for the effect of even coarser layer alignments. We verify this by varying the size of the merged minibatches from 5, 25 to 50 for both BAE-cr / corr and BAE-tr. Cross-language classification results obtained by using these coarser alignments are summarised in Table 3.Surprisingly, BAE-tr's performance does not significantly decrease by using merged minibatches of size 5 (in fact, the performance of the EN-cr / corr task actually improves)."}, {"heading": "7. Conclusion and Future Work", "text": "We presented evidence that meaningful bilingual word representation can be learned without relying on word-level alignment, and that it can succeed even on fairly rough sentence levels. In particular, we demonstrated that although our model does not use word-level alignment, it is capable of surpassing a state-of-the-art method of word-level alignment that uses word-level alignment. In addition, it also exceeds a strong machine translation-based baseline. We observed that the use of a correlation-based regularization term leads to better bilingual embeddings, which are highly correlated, and therefore perform better in cross-language classification tasks. As a future work, we would like to explore extensions of our bilingual autoencoder from words to bag-of-of-grammes, where the model would also need to learn short-phrase representations, which should be particularly useful in the context of a machine translation system.We would also like to explore the possibility of translating multiple languages into a multi-lingual model in parallel."}], "references": [{"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Sentiment Analysis and Opinion Mining", "author": ["Bing Liu"], "venue": "Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers,", "citeRegEx": "Liu.,? \\Q2012\\E", "shortCiteRegEx": "Liu.", "year": 2012}, {"title": "Word representations: A simple and general method for semisupervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Inducing Crosslingual Distributed Representations of Words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai"], "venue": "In Proceedings of the International Conference on Computational Linguistics (COLING),", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Tomas Mikolov", "Quoc Le", "Ilya Sutskever"], "venue": "Technical report,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning multilingual subjective language via cross-lingual projections", "author": ["Rada Mihalcea", "Carmen Banea", "Janyce Wiebe"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Mihalcea et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2007}, {"title": "Cross-lingual annotation projection for semantic roles", "author": ["Sebastian Pad\u00f3", "Mirella Lapata"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Pad\u00f3 and Lapata.,? \\Q2009\\E", "shortCiteRegEx": "Pad\u00f3 and Lapata.", "year": 2009}, {"title": "LargeScale Learning of Embeddings with Reconstruction Sampling", "author": ["Yann Dauphin", "Xavier Glorot", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML", "citeRegEx": "Dauphin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2011}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Morin and Bengio.,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "A Scalable Hierarchical Distributed Language Model", "author": ["Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Mnih and Hinton.", "year": 2008}, {"title": "A Neural Autoregressive Topic Model", "author": ["Hugo Larochelle", "Stanislas Lauly"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Larochelle and Lauly.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Lauly.", "year": 2012}, {"title": "Learning Semantic Representations for the Phrase Translation Model", "author": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT Summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Visualizing Data using t-SNE", "author": ["Laurens van der Maaten", "Geoffrey E Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "For example, high quality POS taggers (Toutanova et al., 2003), parsers (Socher et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 1, "context": ", 2013), sentiment analyzers (Liu, 2012) are already available for English but this is not the case for many other languages such as Hindi, Marathi, Bodo, Farsi, Urdu, etc.", "startOffset": 29, "endOffset": 40}, {"referenceID": 2, "context": "In this context, vectorial text representations have proven useful for multiple NLP tasks (Turian et al., 2010; Collobert et al., 2011).", "startOffset": 90, "endOffset": 135}, {"referenceID": 3, "context": "While the majority of previous work on vectorial text representations has concentrated on the monolingual case, recent work has started looking at learning word and document representations that are aligned across languages (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013).", "startOffset": 224, "endOffset": 289}, {"referenceID": 4, "context": "While the majority of previous work on vectorial text representations has concentrated on the monolingual case, recent work has started looking at learning word and document representations that are aligned across languages (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013).", "startOffset": 224, "endOffset": 289}, {"referenceID": 5, "context": "While the majority of previous work on vectorial text representations has concentrated on the monolingual case, recent work has started looking at learning word and document representations that are aligned across languages (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013).", "startOffset": 224, "endOffset": 289}, {"referenceID": 6, "context": "Such aligned representations can potentially allow the use of resources from a resource fortunate language to develop NLP capabilities in a resource deprived language (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Mihalcea et al., 2007; Wan, 2009; Pad\u00f3 and Lapata, 2009).", "startOffset": 167, "endOffset": 271}, {"referenceID": 7, "context": "Such aligned representations can potentially allow the use of resources from a resource fortunate language to develop NLP capabilities in a resource deprived language (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Mihalcea et al., 2007; Wan, 2009; Pad\u00f3 and Lapata, 2009).", "startOffset": 167, "endOffset": 271}, {"referenceID": 6, "context": "Such reuse of resources across languages has been tried in the past by projecting parameters learned from the annotated data of one language to another language (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Mihalcea et al., 2007; Wan, 2009; Pad\u00f3 and Lapata, 2009) These projections are enabled by a bilingual resource such as a Machine Translation (MT) system.", "startOffset": 161, "endOffset": 265}, {"referenceID": 7, "context": "Such reuse of resources across languages has been tried in the past by projecting parameters learned from the annotated data of one language to another language (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Mihalcea et al., 2007; Wan, 2009; Pad\u00f3 and Lapata, 2009) These projections are enabled by a bilingual resource such as a Machine Translation (MT) system.", "startOffset": 161, "endOffset": 265}, {"referenceID": 3, "context": "Recent attempts at learning common bilingual representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013) aim to eliminate the need of such a MT system.", "startOffset": 61, "endOffset": 126}, {"referenceID": 4, "context": "Recent attempts at learning common bilingual representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013) aim to eliminate the need of such a MT system.", "startOffset": 61, "endOffset": 126}, {"referenceID": 5, "context": "Recent attempts at learning common bilingual representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013) aim to eliminate the need of such a MT system.", "startOffset": 61, "endOffset": 126}, {"referenceID": 3, "context": "Such bilingual representations have been applied to a variety of problems, including crosslanguage document classification (Klementiev et al., 2012) and phrase-based machine translation (Zou et al.", "startOffset": 123, "endOffset": 148}, {"referenceID": 4, "context": ", 2012) and phrase-based machine translation (Zou et al., 2013).", "startOffset": 45, "endOffset": 63}, {"referenceID": 3, "context": ", to derive a regularization term relating word embeddings across languages (Klementiev et al., 2012; Zou et al., 2013).", "startOffset": 76, "endOffset": 119}, {"referenceID": 4, "context": ", to derive a regularization term relating word embeddings across languages (Klementiev et al., 2012; Zou et al., 2013).", "startOffset": 76, "endOffset": 119}, {"referenceID": 3, "context": "Unlike previous approaches (Klementiev et al., 2012), we only require aligned sentences and do not rely on word-level alignments (e.", "startOffset": 27, "endOffset": 52}, {"referenceID": 8, "context": "We note that, additionally, we could have used the stochastic approach proposed by Dauphin et al. (2011) for reconstructing binary bag-of-words representations of documents, to further improve the efficiency of training.", "startOffset": 83, "endOffset": 105}, {"referenceID": 9, "context": "We instead opt for an approach borrowed from the work on neural network language models (Morin and Bengio, 2005; Mnih and Hinton, 2009).", "startOffset": 88, "endOffset": 135}, {"referenceID": 11, "context": "As for organizing words into a tree, as in Larochelle and Lauly (2012) we used a random assignment of words to the leaves of the full binary tree, which we have found to work well in practice.", "startOffset": 43, "endOffset": 71}, {"referenceID": 3, "context": "Klementiev et al. (2012) propose to train simultaneously two neural network languages models, along with a regularization term that encourages pairs of frequently aligned words to have similar word embeddings.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Klementiev et al. (2012) propose to train simultaneously two neural network languages models, along with a regularization term that encourages pairs of frequently aligned words to have similar word embeddings. Zou et al. (2013) use a similar approach, with a different form for the regularizer and neural network language models as in (Collobert et al.", "startOffset": 0, "endOffset": 228}, {"referenceID": 11, "context": "Looking more generally at neural networks that learn multilingual representations of words or phrases, we mention the work of Gao et al. (2013) which showed that a useful linear mapping between separately trained monolingual skip-gram language models could be learned.", "startOffset": 126, "endOffset": 144}, {"referenceID": 5, "context": "Mikolov et al. (2013) also propose a method for training a neural network to learn useful representations of phrases (i.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "We follow the same setup as used by Klementiev et al. (2012) and compare with their method.", "startOffset": 36, "endOffset": 61}, {"referenceID": 13, "context": "For learning the bilingual embeddings, we used the English German section of the Europarl corpus (Koehn, 2005) which contains roughly 2 million parallel sentences.", "startOffset": 97, "endOffset": 110}, {"referenceID": 3, "context": "As mentioned earlier, unlike Klementiev et al. (2012), we do not use any word alignments between these parallel sentences.", "startOffset": 29, "endOffset": 54}, {"referenceID": 3, "context": "We do not remove stopwords (similar to Klementiev et al. (2012)).", "startOffset": 39, "endOffset": 64}, {"referenceID": 3, "context": "Note that Klementiev et al. (2012) use the word-aligned Europarl corpus to first learn an interaction matrix between the words in the two languages.", "startOffset": 10, "endOffset": 35}, {"referenceID": 3, "context": "Note that Klementiev et al. (2012) use the word-aligned Europarl corpus to first learn an interaction matrix between the words in the two languages. This interaction matrix is then used in a multitask learning setup to induce bilingual embeddings from English and German sections of the Reuters RCV1/RCV2 corpora. Note that these documents are not parallel. Each document is assigned one or more categories from a pre-defined hierarchy of topics. Following Klementiev et al. (2012), we consider only those documents which were assigned exactly one of the 4 top level categories in the topic hierarchy.", "startOffset": 10, "endOffset": 482}, {"referenceID": 3, "context": "Note that Klementiev et al. (2012) use the word-aligned Europarl corpus to first learn an interaction matrix between the words in the two languages. This interaction matrix is then used in a multitask learning setup to induce bilingual embeddings from English and German sections of the Reuters RCV1/RCV2 corpora. Note that these documents are not parallel. Each document is assigned one or more categories from a pre-defined hierarchy of topics. Following Klementiev et al. (2012), we consider only those documents which were assigned exactly one of the 4 top level categories in the topic hierarchy. These topics are CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). The number of such documents sampled by Klementiev et al. (2012) for English and German is 34,000 and 42,753 respectively.", "startOffset": 10, "endOffset": 776}, {"referenceID": 3, "context": "Note that Klementiev et al. (2012) use the word-aligned Europarl corpus to first learn an interaction matrix between the words in the two languages. This interaction matrix is then used in a multitask learning setup to induce bilingual embeddings from English and German sections of the Reuters RCV1/RCV2 corpora. Note that these documents are not parallel. Each document is assigned one or more categories from a pre-defined hierarchy of topics. Following Klementiev et al. (2012), we consider only those documents which were assigned exactly one of the 4 top level categories in the topic hierarchy. These topics are CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). The number of such documents sampled by Klementiev et al. (2012) for English and German is 34,000 and 42,753 respectively. In contrast to Klementiev et al. (2012), we do not require a two stage approach (of learning an interaction matrix and then inducing bilingual embeddings).", "startOffset": 10, "endOffset": 874}, {"referenceID": 3, "context": "Note that Klementiev et al. (2012) use the word-aligned Europarl corpus to first learn an interaction matrix between the words in the two languages. This interaction matrix is then used in a multitask learning setup to induce bilingual embeddings from English and German sections of the Reuters RCV1/RCV2 corpora. Note that these documents are not parallel. Each document is assigned one or more categories from a pre-defined hierarchy of topics. Following Klementiev et al. (2012), we consider only those documents which were assigned exactly one of the 4 top level categories in the topic hierarchy. These topics are CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). The number of such documents sampled by Klementiev et al. (2012) for English and German is 34,000 and 42,753 respectively. In contrast to Klementiev et al. (2012), we do not require a two stage approach (of learning an interaction matrix and then inducing bilingual embeddings). We directly learn the embeddings from the Europarl corpus which is parallel. Further, in addition to the Europarl corpus, we also considered feeding the same RCV1/RCV2 documents (34000 EN and 42,753 DE) to the autoencoders. These non-parallel documents are used only to reinforce the monolingual embeddings (by reconstructing x from x or y from y). So, in effect, we use the same amount of data as that used by Klementiev et al. (2012) but our model/training procedure is completely different.", "startOffset": 10, "endOffset": 1426}, {"referenceID": 3, "context": "Next for the cross language classification experiments, we again follow the same setup as used by Klementiev et al. (2012). Specifically, we use 10,000 single-topic documents for training and 5000 single-topic documents for testing in each language.", "startOffset": 98, "endOffset": 123}, {"referenceID": 3, "context": "As in Klementiev et al. (2012) we used an averaged perceptron to train a multi-class classifier for 10 epochs, for all the experiments (Klementiev et al.", "startOffset": 6, "endOffset": 31}, {"referenceID": 3, "context": "As in Klementiev et al. (2012) we used an averaged perceptron to train a multi-class classifier for 10 epochs, for all the experiments (Klementiev et al. (2012) report that the results were not sensitive to the number of epochs).", "startOffset": 6, "endOffset": 161}, {"referenceID": 3, "context": "All results are for word embeddings of size D = 40, as in Klementiev et al. (2012). Further, to speed up the training for BAEcr and BAE-cr/corr we merged mini-batches of 5 adjacent sentence pairs into a single training instance, as described in Section 2.", "startOffset": 58, "endOffset": 83}, {"referenceID": 3, "context": "We also compare our models with the following approaches, for which the results are reported in Klementiev et al. (2012):", "startOffset": 96, "endOffset": 121}, {"referenceID": 3, "context": "Between BAE-tr and BAEcr, we observe that BAE-tr provides better performance and is comparable to the embeddings learned by the neural network language model of Klementiev et al. (2012) which, unlike BAE-tr, relies on word-level alignments.", "startOffset": 161, "endOffset": 186}, {"referenceID": 3, "context": "Next, we evaluate the effect of varying the amount of supervised training data for training the classifier, with either BAE-tr, BAE-cr/corr or Klementiev et al. (2012) embeddigns.", "startOffset": 143, "endOffset": 168}], "year": 2014, "abstractText": "Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. Since training autoencoders on word observations presents certain computational issues, we propose and compare different variations adapted to this setting. We also propose an explicit correlation maximizing regularizer that leads to significant improvement in the performance. We empirically investigate the success of our approach on the problem of cross-language test classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). These experiments demonstrate that our approaches are competitive with the state-of-the-art, achieving up to 10-14 percentage point improvements over the best reported results on this task.", "creator": "LaTeX with hyperref package"}}}