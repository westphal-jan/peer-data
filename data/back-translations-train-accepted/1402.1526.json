{"id": "1402.1526", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2014", "title": "Dual Query: Practical Private Query Release for High Dimensional Data", "abstract": "We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude.", "histories": [["v1", "Thu, 6 Feb 2014 23:20:43 GMT  (229kb,D)", "http://arxiv.org/abs/1402.1526v1", null], ["v2", "Thu, 19 Nov 2015 04:36:00 GMT  (338kb,D)", "http://arxiv.org/abs/1402.1526v2", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CR cs.DB cs.LG", "authors": ["marco gaboardi", "emilio jes\u00fas gallego arias", "justin hsu", "aaron roth", "zhiwei steven wu"], "accepted": true, "id": "1402.1526"}, "pdf": {"name": "1402.1526.pdf", "metadata": {"source": "CRF", "title": "Dual Query: Practical Private Query Release for High Dimensional Data", "authors": ["Marco Gaboardi", "Emilio Jes\u00fas Gallego Arias", "Justin Hsu", "Aaron Roth", "Zhiwei Steven Wu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "1.1 Related work", "text": "In fact, most people who are able to move, to move and to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move."}, {"heading": "2 Differential privacy background", "text": "Differential privacy has become a standard algorithmic term for protecting the privacy of individual records in a statistical database. It formalizes the requirement that the addition or removal of a record does not alter the probability of a result of the mechanism by many different factors.We consider databases to be multisets of elements from an abstract domain X representing the set of all possible datasets. We often consider elements in X to be bit strings of length. Two databases D, D'X are adjacent if they differ in a single data element: i.e., their symmetrical difference is no more than 1.Definition 2.1 (Dwork et al. [15]). A mechanism M: X n \u2192 R satisfies differential privacy if for each S'R and for all adjacent databases D, D'x."}, {"heading": "3 The query release game", "text": "The analysis of our algorithm is based on the interpretation of the query release as a zero-sum game for two players [20]. In this section we will discuss this idea and related tools."}, {"heading": "3.1 Game definition", "text": "Suppose we want to answer all queries in Q. For each query q, Q we can form the negated query q, which takes values q (D) = 1 \u2212 q (D) for each database D. Similarly, for a linear query defined by a predicate \u0442, we can define the negated query by negating euros of the predicate. For the rest, we assume that Q is closed under negation; otherwise, we can add negated copies of each query to Q. Let us be two players we call data players and query players: = q (D) \u2212 q (x), (1) where D is the true database. As a zero-sum game, the data player tries to minimize the payout while the query player tries to maximize the payout."}, {"heading": "3.2 Equilibrium of the game", "text": "We look at how good each player can be when deciding their actions randomly, i.e. when playing on the basis of a probability distribution of their actions. \u2212 Following Neumann's Minimax theorem, according to which A (u, w) maximum w (Q) A (u, w) maximum w (u, w) is the expected payout. The usual value is called the value of the game we call vA. \u2212 Intuitively, von Neumann's theorem states that there is no advantage in a player going first: the minimizing player can always force the payout to vA, while the maximizing player always forces the payout to vA. This indicates that each player can play an optimal strategy, assuming that the strategies come first: the minimizing player can always force the payout to vA, while the maximizing player always forces the payout to vA."}, {"heading": "3.3 Solving the game", "text": "To construct the approximate equilibrium, we use the Multiplicative Weight Actuation Algorithm (MW).2 This algorithm maintains a distribution of actions (initially uniform) over a series of steps. At each step, the MW algorithm receives a (possibly opposing) loss for each action. Then, the distribution is reweighted to favor actions with less loss. \u2212 The algorithm is represented in Algorithm 1.Algorithm 1. \u2212 The Multiplicative Weight Actuation Algorithm Let all answers > 0 be given, leave the action space A-1 uniform distribution to A-1, 2,."}, {"heading": "4 Dual query release", "text": "At a high level, our algorithm solves the query publishing problem with the help of an algorithm dual-linked to the MWEM algorithm Q = > Q = = Q = = Q = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4.1 Privacy", "text": "The privacy proofs are largely routine, based on the composition theories. Instead of defining privacy costs \u03b5 as a function of parameters T, s, p, we will adjust these parameters for our experimental assessment. We will apply the privacy of the following mechanism (based on McSherry and Talwar [26]) as part of our privacy forecast. \u2212 Definition 4.1 (McSherry and Talwar [26]) Given some arbitrary output ranges R, the exponential mechanism with the score function S is selected and gives an element r-R with the probability of being proportional to exp (\u03b5S, r-2), where the sensitivity of S is defined, defined as max. D, D exponential mechanisms with the score function S (D, r exponentials).The exponential mechanism is the sensitivity of S, defined as max."}, {"heading": "4.2 Accuracy", "text": "Proving the accuracy is done in two steps. First, we show that the \"average query\" formed from the samples is at least as high as the actual weighted distribution Qt. < < < < < 2 exp (\u2212 NT 2 / 3) for each T.Lemma 4.5. Let's \u03b2 (0, 1), and let p be a distribution over the queries. Let's say we draw = 48 log (X \u2212 \u00b5 | > T] < 2 exp (\u2212 NT 2 / 3) for each T.Lemma 4.5. Let \u03b2 (0, 1), and let p be a distribution over the queries."}, {"heading": "5 Case study: 3-way marginals", "text": "In our algorithm, the mathematically difficult step is to find the approximate best response of the data player to the distribution of the query player. As mentioned above, the form of this problem depends on the respective query class Q. In this section, we will first discuss the optimization problem in general, and then specifically for the well-studied class of marginal queries."}, {"heading": "5.1 The best-response problem", "text": "Remember that the query game has the payout A (x, q) defined by Equation (1); the data player tries to minimize the payout while the query player tries to maximize it. If the query player has the distribution of Qt over queries, the best response from the data player minimizes the expected loss: argmin x X E q \u2190 Qt [q (D) \u2212 q (x)].For privacy reasons, the data player is actually playing against the distribution of queries q-1...., q-s. Therefore, the problem with the best answer is isargmin x-X1s: i = 1 q-i (D) \u2212 q-i (x) = argmax x-X s-i = 1 q-i (x), sinceD is fixed and q-i are linear queries. With Theorem 4.6 it is even enough to find an approximate maximizer to guarantee accuracy."}, {"heading": "5.2 3-way marginal queries", "text": "To look at the exact shape of the best-response problem, we look at 3-way marginal questions. We think of data sets as if we have d binary attributes, so that the data universe is all bit strings of length d. We write for x-way marginalities to mean the bit bits of the data set. We think of data sets as if we each have one bit of the dataset x-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits-bits"}, {"heading": "6.1 Accuracy", "text": "We evaluate the accuracy of the algorithm on 500,000 3-way margins on Adult, KDD99 and Netflix. We report maximum errors in Figure 2, averaged over 5 runs. (Marginal queries have a range [0, 1], so error 1 is trivial.) The runs are (\u03b5, 0.001) -differentiated private, with \u03b5 in the range of 0.25 to 5.4 for the Adult and KDD99 datasets setting the step size \u03b7 = 2.0, sample size s = 1000, while we vary the number of steps T according to the privacy budget \u03b5, using the formula from Theorem 4.3. For the Netflix dataset we use the same heuristics, unless we set s to 5000. Accuracy markedly improves when \u03b5 increases from 0.25 to 1 over 3 datasets, and the improvement gradually decreases with larger size. For larger datasets, both KDD99 and Netflix datasets allow to get significantly more dualery with better steps and better qualery."}, {"heading": "6.2 Scaling to More Queries", "text": "Next, we evaluate accuracy and runtime when we vary the number of queries. We use a set of 40,000 to 2 million randomly generated margins Q on the KDD99 dataset and execute DualQuery with (1, 0.001) - Privacy. For all experiments, we use the same set of parameters: \u03b7 = 1.2, T = 170, and s = 1750. According to Theorem 4.3, each run of the experiment fulfills (1, 0.001) differential privacy. These parameters provide stable performance when the query class Q grows. As shown in Figure 3, both the average and maximum error remain largely stable and show improved errors compared to simpler error approaches. For example: 4 Because our privacy analysis follows from Lemma 2.4, our algorithm actually fulfills privacy requirements for smaller values of category Q. For example, our algorithm is also (2, 4) -private for the query = 10 6. \u2212 Similarly, we could choose a small growth value for only a small part of the query."}, {"heading": "6.3 Scaling to Higher Dimensional Data", "text": "Finally, we evaluate accuracy and runtime behavior for data dimensions in the range of 50 to 512,000. We evaluate DualQuery under (1, 0.001) privacy for 100,000 3-way marginals on synthetically generated data sets. We report runtime, maximum and average errors over 3 runs in Figure 4; note the logarithmic scale for attribute axes. We do not include query evaluation in our time measurements - this overhead is common to all approaches that answer a series of queries. When generating synthetic data, one way is to randomly set each attribute to 0 or 1. However, this generates very uniform synthetic data: a record fills each 3-way marginality with a probability of 1 / 8, so that most marginals have a value close to 1 / 8. To generate more sophisticated and realistic data, we choose a separate bias dimension [0, 1] uniformly randomly for each attribute."}, {"heading": "6.4 Methodology", "text": "In this section we will discuss our experimental setup in more detail.Implementation details. The implementation is written in OCaml, using the CPLEX constraint solver. We have the experiments on a mid-range desktop machine with a 4-core Intel Xeon processor and 12Gb of RAM. Heuristically, we advocate a timeout for each CPLEX request to find the best current solution when we have shown the timeout that timeout was rarely achieved. We discredit KDDD99 and Adult datasets in binary attributes that have mapped every possible attribute."}, {"heading": "7 Discussion and conclusion", "text": "We have developed a new mechanism for releasing private queries to process datasets with dimensions several orders of magnitude larger than previously possible. In fact, we do not seem to have reached the limits of our approach yet - even for synthetic data with more than 500,000 attributes, DualQuery continues to provide useful answers with about 30 minutes of overhead in addition to query evaluation (which is already on the hour scale alone). We believe that DualQuery makes private analysis of high-dimensional data practicable for the first time. However, this remarkable runtime improvement is not free: our theoretical accuracy limits are worse than those of previous approaches [18, 19]. For low-dimensional datasets where it is possible to maintain distribution over datasets, the MWEM algorithm by Hardt et al. [19] probably remains state-of-the-art. Our work complements MWEM with the possibility of private data analysis on higher-dimensional datasets."}, {"heading": "A Case study: Parity queries", "text": "In this section, we show how to apply DualQuery to another well-studied class of queries: Parities. Each of these queries is specified by a subset S of characters, and these queries measure the number of rows with an even number of bits in S compared to the number of rows with an odd number of bits in S. Definition A.1. Allow X = {\u2212 1, + 1}. A k-wise query is a linear query specified by a subset of characters S [d] with | S | = k, with the value qS (x) = {+ 1: even number of xi = + 1 for i \u00b2 S \u2212 1: otherwise.As before, we can define a negated k-wise query: qS (x) = 1: odd number of xi = + 1 for i \u00b2: otherwise.Barak et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al"}], "references": [{"title": "The multiplicative weights update method: a metaalgorithm and applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Privacy, accuracy, and consistency too: a holistic solution to contingency table release", "author": ["Boaz Barak", "Kamalika Chaudhuri", "Cynthia Dwork", "Satyen Kale", "Frank McSherry", "Kunal Talwar"], "venue": "In ACM SIGACT\u2013SIGMOD\u2013SIGART Symposium on Principles of Database Systems (PODS), Beijing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Characterizing the sample complexity of private learners", "author": ["Amos Beimel", "Kobbi Nissim", "Uri Stemmer"], "venue": "In ACM SIGACT Innovations in Theoretical Computer Science (ITCS), Berkeley,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A learning theory approach to noninteractive database privacy", "author": ["A. Blum", "K. Ligett", "A. Roth"], "venue": "Journal of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Practical privacy: the sulq framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In ACM SIGACT\u2013SIGMOD\u2013SIGART Symposium on Principles of Database Systems (PODS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Sample complexity bounds for differentially private learning", "author": ["Kamalika Chaudhuri", "Daniel Hsu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Privacy-preserving logistic regression", "author": ["Kamalika Chaudhuri", "Claire Monteleoni"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A stability-based validation procedure for differentially private machine learning", "author": ["Kamalika Chaudhuri", "Staal A. Vinterbo"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Differentially private empirical risk minimization", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Near-optimal differentially private principal components", "author": ["Kamalika Chaudhuri", "Anand Sarwate", "Kaushik Sinha"], "venue": "In Conference on Neural Information Processing Systems (NIPS), Lake Tahoe,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Local privacy and statistical minimax rates", "author": ["J.C. Duchi", "M.I. Jordan", "M.J. Wainwright"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS), Berkeley, California,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "On the complexity of differentially private data release: efficient algorithms and hardness results", "author": ["C. Dwork", "M. Naor", "O. Reingold", "G.N. Rothblum", "S.P. Vadhan"], "venue": "In ACM SIGACT Symposium on Theory of Computing (STOC),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Boosting and differential privacy", "author": ["C. Dwork", "G.N. Rothblum", "S. Vadhan"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In IACR Theory of Cryptography Conference (TCC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Game theory, on-line prediction and boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In Conference on Computational Learning Theory (CoLT), Desenzano sul Garda, Italy,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Iterative constructions and private data release", "author": ["A. Gupta", "A. Roth", "J. Ullman"], "venue": "In IACR Theory of Cryptography Conference (TCC),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A multiplicative weights mechanism for privacy-preserving data analysis", "author": ["Moritz Hardt", "Guy N. Rothblum"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "A simple and practical algorithm for differentially private data release", "author": ["Moritz Hardt", "Katrina Ligett", "Frank McSherry"], "venue": "In Conference on Neural Information Processing Systems (NIPS), Lake Tahoe,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Differential privacy for the analyst via private equilibrium computation", "author": ["Justin Hsu", "Aaron Roth", "Jonathan Ullman"], "venue": "In ACM SIGACT Symposium on Theory of Computing (STOC),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "What can we learn privately", "author": ["Shiva Prasad Kasiviswanathan", "Homin K. Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "SIAM Journal on Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael J. Kearns"], "venue": "Journal of the ACM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Private convex empirical risk minimization and high-dimensional regression", "author": ["Daniel Kifer", "Adam Smith", "Abhradeep Thakurta"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "An adaptive mechanism for accurate query answering under differential privacy", "author": ["Chao Li", "Gerome Miklau"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Optimizing linear counting queries under differential privacy", "author": ["Chao Li", "Michael Hay", "Vibhor Rastogi", "Gerome Miklau", "Andrew McGregor"], "venue": "In ACM SIGACT\u2013SIGMOD\u2013SIGART Symposium on Principles of Database Systems (PODS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Mechanism design via differential privacy", "author": ["F. McSherry", "K. Talwar"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Robust de-anonymization of large sparse datasets", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "In IEEE Symposium on Security and Privacy (S&P),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for SVM learning", "author": ["Benjamin I.P. Rubinstein", "Peter L. Bartlett", "Ling Huang", "Nina Taft"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Nearly) optimal algorithms for private online learning in full-information and bandit settings", "author": ["Abhradeep G. Thakurta", "Adam Smith"], "venue": "In Conference on Neural Information Processing Systems (NIPS), Lake Tahoe,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Answering n2+o(1) counting queries with differential privacy is hard", "author": ["J. Ullman"], "venue": "In ACM SIGACT Symposium on Theory of Computing (STOC), Palo Alto, California,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "PCPs and the hardness of generating private synthetic data", "author": ["J. Ullman", "S.P. Vadhan"], "venue": "In IACR Theory of Cryptography Conference (TCC),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}], "referenceMentions": [{"referenceID": 25, "context": "The competition was a great success (the winning team improved on the existing recommendation system by more than 10%), but the ad hoc anonymization was not as successful: Narayanan and Shmatikov [27] were later able to re-identify individuals in the dataset.", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "[6], performing private query release is sufficient to simulate any learning algorithm in the \u201cstatistical query model\u201d of Kearns [22].", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[6], performing private query release is sufficient to simulate any learning algorithm in the \u201cstatistical query model\u201d of Kearns [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 11, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 12, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 16, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 15, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 17, "context": "While simple perturbation can be used to privately answer a small number of queries [15], more sophisticated approaches can accurately answer nearly exponentially many queries in the size of the private database [5, 13, 14, 29, 18, 17, 19].", "startOffset": 212, "endOffset": 239}, {"referenceID": 16, "context": "Unfortunately, even the most efficient approaches have a per-query running time linear in the size of the data universe, which is exponential in the dimension of the data [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 28, "context": "Moreover, this running time is necessary in the worst case [32], especially if the algorithm produces synthetic data [33].", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "Moreover, this running time is necessary in the worst case [32], especially if the algorithm produces synthetic data [33].", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "[19], who perform a thorough experimental evaluation of one such algorithm, which they called MWEM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] showed how to convert learning algorithms in the SQ model of Kearns [22] into differentially private learning algorithms with similar accuracy guarantees.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[6] showed how to convert learning algorithms in the SQ model of Kearns [22] into differentially private learning algorithms with similar accuracy guarantees.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 117, "endOffset": 131}, {"referenceID": 5, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 117, "endOffset": 131}, {"referenceID": 2, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 117, "endOffset": 131}, {"referenceID": 10, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 117, "endOffset": 131}, {"referenceID": 6, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 8, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 26, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 21, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 9, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 27, "context": "Since then, private machine learning has become a very active field with both foundational sample complexity results [21, 7, 4, 12] and numerous efficient algorithms for particular learning problems [8, 10, 30, 23, 11, 31].", "startOffset": 199, "endOffset": 222}, {"referenceID": 3, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 11, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 12, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 16, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 15, "context": "In parallel, there has been a significant amount of work on privately releasing synthetic data based on a true dataset while preserving the answers to large numbers of statistical queries [5, 13, 29, 14, 18, 17].", "startOffset": 188, "endOffset": 211}, {"referenceID": 16, "context": "But, all of these algorithms (including the notable multiplicative weights algorithm of Hardt and Rothblum [18], which achieves the theoretically optimal accuracy and runtime) have running time exponential in the dimension of the data.", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "With standard cryptographic assumptions, this is necessary in the worst case for mechanisms that answer many arbitrary statistical queries [32].", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "[19], which is based on the private multiplicative weights mechanism [18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19], which is based on the private multiplicative weights mechanism [18].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "[19] were able to implement the multiplicative weights algorithm on several real datasets with up to 77 attributes (and even more when the queries are restricted to take positive values only 1 Hardt et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] are able to scale up to 1000 features on synthetic data when the features are partitioned into a number of small buckets, and the queries are chosen to never depend on features in more than one bucket.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Another family of query release algorithms are based on the Matrix Mechanism [25, 24].", "startOffset": 77, "endOffset": 85}, {"referenceID": 22, "context": "Another family of query release algorithms are based on the Matrix Mechanism [25, 24].", "startOffset": 77, "endOffset": 85}, {"referenceID": 18, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] solves the game by having a data player use a no-regret learning algorithm, while the query player repeatedly best responds by optimizing over queries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14]; the main difference is that our optimization problem is over single records rather than sets of records.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For any predicate \u03c6 : X \u2192 {0, 1}, the linear query Q\u03c6 : X n \u2192 [0, 1] is defined by", "startOffset": 62, "endOffset": 68}, {"referenceID": 12, "context": "[14]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The analysis of our algorithm relies on the interpretation of query release as a two player, zero-sum game [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "Freund and Schapire [16] showed that if one player maintains a distribution over actions using MW, while the other player selects a best-response action versus the current MW distribution (i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Interpreted this way, the private multiplicative weights algorithm of Hardt and Rothblum [18] (and the MWEM algorithm of Hardt et al.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "[19]) uses MW for the data player, while the query player plays best responses.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "For privacy, their algorithm selects the query best-responses privately via the exponential mechanism of McSherry and Talwar [26].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Given known hardness results for the query release problem [32], our algorithm must have worst-case runtime polynomial in the universe size |X |, so is not theoretically more efficient than prior approaches.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": ", Hardt and Rothblum [18]), our algorithm has a worse accuracy guarantee.", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "We will use the privacy of the following mechanism (due to McSherry and Talwar [26]) as an ingredient in our privacy proof.", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "1 (McSherry and Talwar [26]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": ", XN be IID random variables with mean \u03bc, taking values in [0, 1].", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "Since q takes values in [0, 1], the payoffs are all in [\u22121, 1].", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": "(Marginal queries have range [0, 1], so error 1 is trivial.", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "To generate more challenging and realistic data, we pick a separate bias pi \u2208 [0, 1] uniformly at random for each attribute i.", "startOffset": 78, "endOffset": 84}, {"referenceID": 7, "context": ", Chaudhuri and Vinterbo [9]).", "startOffset": 25, "endOffset": 28}, {"referenceID": 16, "context": "However, this remarkable improvement in running time is not free: our theoretical accuracy bounds are worse than those of previous approaches [18, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 17, "context": "However, this remarkable improvement in running time is not free: our theoretical accuracy bounds are worse than those of previous approaches [18, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 17, "context": "[19] likely remains the state of the art.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude.", "creator": "LaTeX with hyperref package"}}}