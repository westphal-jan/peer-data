{"id": "1402.1869", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2014", "title": "On the Number of Linear Regions of Deep Neural Networks", "abstract": "We study the complexity of functions computable by deep feedforward neural networks with piece-wise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer's input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.", "histories": [["v1", "Sat, 8 Feb 2014 17:16:27 GMT  (406kb,D)", "http://arxiv.org/abs/1402.1869v1", "12 pages"], ["v2", "Sat, 7 Jun 2014 19:56:14 GMT  (3672kb,D)", "http://arxiv.org/abs/1402.1869v2", null]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["guido f mont\u00fafar", "razvan pascanu", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1402.1869"}, "pdf": {"name": "1402.1869.pdf", "metadata": {"source": "CRF", "title": "On the Number of Linear Regions of Deep Neural Networks", "authors": ["Guido Mont\u00fafar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio", "PASCANU CHO BENGIO"], "emails": ["MONTUFAR@MIS.MPG.DE", "PASCANUR@IRO.UMONTREA.CA", "KYUNGHYUN.CHO@AALTO.FI", "YOSHUA.BENGIO@UMONTREA.CA"], "sections": [{"heading": "1. Introduction", "text": "We are interested in extending this type of analysis to more popular neural networks, which are composed of different levels of non-linear computational units. A well-known result is the Hornomial network 1989. (This is an unprecedented success in a variety of individual functions). (See, e.g., Krizhevsky et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012) In light of this empirical evidence, deep neural networks are becoming increasingly popular via flat networks and are often implemented with more than five levels. However, at present only a limited amount of publications has been examined from a theoretical perspective. Recently, Delalleau and Bengio (2011) showed that a flat network (i.e., with a single hidden layer) requires exponentially more sum product hidden units than a deep sum product network to calculate certain families of polynomials. We are interested in extending this kind of analysis to neural networks."}, {"heading": "2. Complexity of Feedforward Networks and Their Compositional Properties", "text": "The main observation of our analysis is that each layer of a deep model is able to identify regions of its input that should be mapped to a common output, resulting in a compositional structure in which calculations on higher layers in the identified regions in the input space are effectively duplicated. The ability to replicate calculations across the input space grows exponentially in the number of layers of the network. Before expanding these ideas, we introduce basic definitions needed in the rest of the article. At the end of this section, we will give an intuitive perspective for reflecting on the replication capacity of deep models."}, {"heading": "2.1. Definitions", "text": "A forward-facing neural network is a composition of computational units that define a function of the formu (x | \u03b8) = \u03c6 (U > \u03c6L (W > L \u00b7 \u00b7 \u03c61 (W > 1 x) \u00b7 \u00b7 \u00b7), with Wl being the weights and \u03c6l the nonlinear activation function of the l-th hidden layer, and W1 and U the weights associated with the input and output, respectively. The bias term on each layer has been dropped for clarity purposes. Faced with an input x from the previous layer, each hidden unit computes a function (v > x + b), where v and b form the input and distortion weights of the layer."}, {"heading": "2.2. Shallow Neural Networks", "text": "Assuming that the activation function g has a distinguishable point at the origin of its domain, the combinatorics of the function computed by a layer of units can be described with the hyperplanes Wi: x + bi = 0.A region of a hyperplane arrangement is a contiguous component of the complement of the hyperplanes. The number of regions generated by a hyperplane arrangement can be specified in the form of a characteristic function, which is a well-known result of Zaslawsky (1975). We will only use the fact that an arrangement of n1 hyperplanes in Rn0 to \u2211 n0 j = 0 (n1 j) regions is achieved when the hyperplanes are in general position. This number is also the maximum number of regions of linearity of functions that can be computed by a flat rectifier network with n0 inputs and a single hidden layer of latitude n1 (see Procanal, 2013, 5)."}, {"heading": "2.3. Deep Neural Networks", "text": "We build our analysis of the complex relationships that are present in the entirety of the identified input regions. Each individual layer of the network is able to identify the individual regions. In this case, we also say that S and T are identified by the definition of the individual regions. (1) By identifying parts of the input space of a neural network, each subsequent layer of the calculation can be focused on a single output region, which effectively acts on many input regions defined by the previous layer. (1) One can define the subsequent layers of the network in such a way that each of their calculations focuses on that particular output region, thus creating a complex output function.Each layer of the network in such a way that each of its output regions focuses on that particular output region."}, {"heading": "2.4. Identification of Inputs as Space Foldings", "text": "In this section, we will discuss the intuition behind Theorem 2 in relation to space folding. A map g, which identifies two subsets S and S of a room, can be considered a folding operator, which folds space so that the subsets S and S \u00b2 match. For example, a coordinate absolute function g: R2 \u2192 R2 in equivalent (1) can fold the input space twice along both coordinates. In this case, this folding is equivalent to stating that the four quadrants of 2-D euclidean space are identified by the map. See Fig. 3 (a) for an illustration. The same map can be used again to fold the resulting image of the original input space.One can easily see that each spatial folding corresponds to a single hidden layer of a deep neural network. Each hidden layer folds the space defined by the layer immediately below in relation to a specific map. A neural network effectively begins its first folding cursively."}, {"heading": "3. Deep Rectifier Networks", "text": "In this section, we offer an analysis of deep neural networks with rectifier units, based on the general result of paragraph 2. We obtain a result that is better than that of Pascanu et al. (2013), whereby the maximum number of linear regions is limited by functions that can be calculated by deep rectifier networks."}, {"heading": "3.1. Illustration", "text": "Consider a single hidden layer of n rectifiers with n0 input variables, where n \u2265 n0 (effective) acts. We divide the group of rectifiers into n0 (non-overlapping) subsets of cardinality p = n n0. Consider one of the n0 subsets. We construct rectifiers such as f1 (x) = max {0, w > x}, f2 (x) = max {0, 2w > x \u2212 1}, f3 (x) = max {0, 2w > x \u2212 2},... fp (x) = max {0, 2w > x \u2212 (p \u2212 1)}, where w = [0, \u00b7, 0, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, 0] is a vector that selects a single coordinate from the input space (fp). We can summarize these p rectifiers into a single scalar value: g (x) = [1 \u2212 1 \u2212 1 \u2212 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 x] (x)."}, {"heading": "3.2. Formal Result", "text": "We can generalize the procedure described above to the case where there are n0 input variables and L hidden layers of width ni \u2265 n0 for all i (L). In this case, the maximum number of regions in the i-th layer is lower than the maximum number of regions of linearity of the functions calculated by a neural network with n0 input units and L hidden layers. (L) The maximum number of regions in the i-th layer is lower than the number of regions in the i-th layer is lower than (L) n0 (nL j).Proof The proof is provided by counting the number of regions for a suitable choice of network parameters. We organize the units so that in each layer j the set of units is divided into n0 subsets of cardinality. Each subset responds to a coordinate of a non-singular affine space."}, {"heading": "3.3. Stability to Perturbation", "text": "Our lower limits in terms of complexity, which can be achieved by deep models, are based on appropriate decisions of the network weights. This does not mean that the limits are only valid in individual cases. The parameterization of the functions calculated by a given network is continuous, i.e., the map is: RN \u2192 C (Rn0; RnL); \u03b8 = {W, b} 7 \u2192 f\u03b8 is continuous. We considered the number of regions of linearity of functions f\u03b8. By definition, each region of linearity contains an open neighborhood of domain Rn0. In the face of a functional disturbance, there is a > 0 so that for each - disturbance of the parameter level the resulting function f\u03b8 + has at least as many regions as f\u03b8, provided there is only a limited number of regions. The regions of linearity of the f\u0456 are maintained with small disturbances of the parameters, because they have a limited volume. However, it may occur that the function of the perturbed region is a function of equal probability (where a perturbed function is the least likely to have a disturbed area of space)."}, {"heading": "4. Deep Maxout Networks", "text": "A maxout network is a support hyperplane of a convex polytope in particular space. Definition 5 Consider a linear map f: Rn \u2192 Rk \u00b7 m; f (x) = Wx + b with n, k \u00b2 N and W \u00b2 Rk \u00b7 n, b \u00b2 Rk \u00b7 m. If g: Rk \u00b7 m \u2192 Rm is a function with coordinates maxout units of rank k. The maximum of two convex functions is also a convex function. Linear functions are convex, and so a maxout layer g: Rn \u2192 Rm is also convex. The maximum of a collection of functions is its upper shell. We can consider the graph of each linear function as supporting hypervexes."}, {"heading": "5. Conclusions and Outlook", "text": "We investigated the complexity of functions that can be computed by deep, forward-facing neural networks in terms of their number of regions of linearity. We discussed the idea that each layer of a deep model is able to identify portions of its input in such a way that the concatenation of layers identifies an exponential number of input regions, and that this leads to an exponential replication of the complexity of functions computed in the higher layers of the model. Functions computed in this way by deep models are complicated, with a rigidity caused by the replications that can help deep models to generalize better than flat models. This framework is applicable to any neural network that has a piecemeal linear activation function. For example, if we look at an evolutionary network with rectifier units, such as the one used in (Krizhevsky et al., 2012), we can see that the confusion has maximum concentration followed by bundling of all regions within an input region."}], "references": [{"title": "Multi column deep neural network for traffic sign classification", "author": ["Dan Ciresan", "Ueli Meier", "Jonathan Masci", "Juergen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Delalleau and Bengio.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau and Bengio.", "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Geoffrey Hinton", "Li Deng", "George E. Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural Networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Approximation properties of DBNs with binary hidden units and real-valued visible units", "author": ["Oswin Krause", "Asja Fischer", "Tobias Glasmachers", "Christian Igel"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "Krause et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deep belief networks are compact universal approximators", "author": ["Nicolas Le Roux", "Yoshua Bengio"], "venue": "Neural Computation,", "citeRegEx": "Roux and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Roux and Bengio.", "year": 2010}, {"title": "Refinements of universal approximation results for deep belief networks and restricted Boltzmann machines", "author": ["Guido Mont\u00fafar", "Nihat Ay"], "venue": "Neural Computation,", "citeRegEx": "Mont\u00fafar and Ay.,? \\Q2011\\E", "shortCiteRegEx": "Mont\u00fafar and Ay.", "year": 2011}, {"title": "Universal approximation depth and errors of narrow belief networks with discrete units", "author": ["Guido F Mont\u00fafar"], "venue": "arXiv preprint arXiv:1303.7461,", "citeRegEx": "Mont\u00fafar.,? \\Q2013\\E", "shortCiteRegEx": "Mont\u00fafar.", "year": 2013}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "Proceedings of the Twenty-seventh International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Mont\u00fafar", "Yoshua Bengio"], "venue": "[cs.LG],", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "An introduction to hyperplane arrangements", "author": ["Richard Stanley"], "venue": "In Lect. notes, IAS/Park City Math. Inst.,", "citeRegEx": "Stanley.,? \\Q2004\\E", "shortCiteRegEx": "Stanley.", "year": 2004}, {"title": "Facing Up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes", "author": ["Thomas Zaslavsky"], "venue": "Number no. 154 in Memoirs of the American Mathematical Society. American Mathematical Society,", "citeRegEx": "Zaslavsky.,? \\Q1975\\E", "shortCiteRegEx": "Zaslavsky.", "year": 1975}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Artificial neural networks with several hidden layers, called deep neural networks, have become popular due to their unprecedented success in a variety of machine learning tasks (see, e.g., Krizhevsky et al., 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012).", "startOffset": 191, "endOffset": 295}, {"referenceID": 3, "context": "Introduction Artificial neural networks with several hidden layers, called deep neural networks, have become popular due to their unprecedented success in a variety of machine learning tasks (see, e.g., Krizhevsky et al., 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012).", "startOffset": 191, "endOffset": 295}, {"referenceID": 8, "context": "Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Mont\u00fafar and Ay, 2011), as well as their approximation properties (Mont\u00fafar, 2013; Krause et al.", "startOffset": 107, "endOffset": 156}, {"referenceID": 9, "context": "Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Mont\u00fafar and Ay, 2011), as well as their approximation properties (Mont\u00fafar, 2013; Krause et al., 2013).", "startOffset": 200, "endOffset": 237}, {"referenceID": 5, "context": "Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Mont\u00fafar and Ay, 2011), as well as their approximation properties (Mont\u00fafar, 2013; Krause et al., 2013).", "startOffset": 200, "endOffset": 237}, {"referenceID": 2, "context": "(2013) reported a theoretical result on the complexity of functions computable by deep feedforward networks with piecewise linear units called rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), which have an activation function of the form g(x) = max{0,x}.", "startOffset": 159, "endOffset": 203}, {"referenceID": 10, "context": "(2013) reported a theoretical result on the complexity of functions computable by deep feedforward networks with piecewise linear units called rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), which have an activation function of the form g(x) = max{0,x}.", "startOffset": 159, "endOffset": 203}, {"referenceID": 0, "context": ", 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012). In view of this empirical evidence, deep neural networks are becoming increasingly favoured over shallow networks, and are often implemented with more than five layers. At the time being, however, only a limited amount of publications have investigated deep networks from a theoretical perspective. Recently, Delalleau and Bengio (2011) showed that a shallow network (i.", "startOffset": 8, "endOffset": 414}, {"referenceID": 0, "context": ", 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012). In view of this empirical evidence, deep neural networks are becoming increasingly favoured over shallow networks, and are often implemented with more than five layers. At the time being, however, only a limited amount of publications have investigated deep networks from a theoretical perspective. Recently, Delalleau and Bengio (2011) showed that a shallow network (i.e., with a single hidden layer) requires exponentially many more sum-product hidden units than a deep sum-product network in order to compute certain families of polynomials. We are interested in extending this kind of analysis to more popular neural networks composed of layers of nonlinear computational units. A well-known result due to Hornik et al. (1989) states that a feedforward neural network with a single hidden layer is a universal approximator (of Borel measurable functions).", "startOffset": 8, "endOffset": 808}, {"referenceID": 0, "context": ", 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012). In view of this empirical evidence, deep neural networks are becoming increasingly favoured over shallow networks, and are often implemented with more than five layers. At the time being, however, only a limited amount of publications have investigated deep networks from a theoretical perspective. Recently, Delalleau and Bengio (2011) showed that a shallow network (i.e., with a single hidden layer) requires exponentially many more sum-product hidden units than a deep sum-product network in order to compute certain families of polynomials. We are interested in extending this kind of analysis to more popular neural networks composed of layers of nonlinear computational units. A well-known result due to Hornik et al. (1989) states that a feedforward neural network with a single hidden layer is a universal approximator (of Borel measurable functions). Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Mont\u00fafar and Ay, 2011), as well as their approximation properties (Mont\u00fafar, 2013; Krause et al., 2013). More recently, Pascanu et al. (2013) reported a theoretical result on the complexity of functions computable by deep feedforward networks with piecewise linear units called rectifier units (Glorot et al.", "startOffset": 8, "endOffset": 1213}, {"referenceID": 11, "context": "In this paper, we follow the ideas of Pascanu et al. (2013) to analyze deep models, but we take a broader perspective.", "startOffset": 38, "endOffset": 60}, {"referenceID": 11, "context": "In this paper, we follow the ideas of Pascanu et al. (2013) to analyze deep models, but we take a broader perspective. We show that deep models are able to sequentially map pieces of their inputs into the same output and in that way, the layer-wise composition of functions that they compute re-uses low-level computations exponentially often as the number of layers increases. Our analysis and results apply to a variety of deep networks, including deep rectifier networks. We estimate the number of regions of linearity of functions computable by deep neural networks for two important cases: with rectifier units and maxout units (Goodfellow et al., 2013). Our bound for the complexity of deep rectifier networks improves the previous bound obtained by Pascanu et al. (2013). The number of linear regions of functions computed by a deep model is a measure of flexibility for the model.", "startOffset": 38, "endOffset": 778}, {"referenceID": 12, "context": "The number of regions generated by a hyperplane arrangement can be given in terms of a characteristic function, which is a well known result by Zaslavsky (1975). We will only use the fact that an arrangement of n1 hyperplanes in Rn0 produces up to \u2211n0 j=0 ( n1 j ) regions.", "startOffset": 144, "endOffset": 161}, {"referenceID": 11, "context": "We obtain a result that improves upon that by Pascanu et al. (2013), with a tighter", "startOffset": 46, "endOffset": 68}], "year": 2014, "abstractText": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer\u2019s input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.", "creator": "LaTeX with hyperref package"}}}