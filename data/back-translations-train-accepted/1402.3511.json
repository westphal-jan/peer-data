{"id": "1402.3511", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2014", "title": "A Clockwork RNN", "abstract": "Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.", "histories": [["v1", "Fri, 14 Feb 2014 16:05:12 GMT  (392kb,D)", "http://arxiv.org/abs/1402.3511v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["jan koutn\u00edk", "klaus greff", "faustino j gomez", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1402.3511"}, "pdf": {"name": "1402.3511.pdf", "metadata": {"source": "META", "title": "A Clockwork RNN", "authors": ["Jan Koutn\u0131\u0301k", "Klaus Greff", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "emails": ["HKOU@IDSIA.CH", "KLAUS@IDSIA.CH", "TINO@IDSIA.CH", "JUERGEN@IDSIA.CH"], "sections": [{"heading": "1. Introduction", "text": "Recursive neural networks (RNNs; Robinson & Fallside, 1987; Werbos, 1988; Williams, 1989) are a class of connecting models that exhibit an internal state or short-term memory due to recurring feed-back connections that make them suitable for dealing with sequential problems, such as language classification, prediction, and generations.Standard RNNs with stochastic gradient lineage have difficulty learning long-term dependencies (i.e., they encrypt more than 10 time steps) encrypted by the disappearance of gradients (Hochreiter, 1991; Hochreiter et al., 2001) The problem has been addressed by using a specialized neuron structure, or cells encoded in the input sequence sequences of sequences (LSTM) networks (Hochreiter & Schmidhuber) that develop constant gradients (Hochreiter et) that are neural algorithms located in the location of the neural algorithms."}, {"heading": "2. Related Work", "text": "In fact, most of them will be able to play by the rules they have adopted in recent years."}, {"heading": "3. A Clockwork Recurrent Neural Network", "text": "The question is whether the two modules are in fact the two main modules in which the two main modules each have a duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the duration of the"}, {"heading": "4. Experiments", "text": "CW-RNNs were compared to the simple RNN (SRN) and LSTM networks. All networks have a hidden layer with the Tanh activation function, and the number of nodes in the hidden layer was selected to obtain (approximately) the same number of parameters for all three methods (in the case of CW-RNN, time intervals were included in the parameter counting). Initial values for all weights were drawn from a Gaussian distribution with a mean value of zero and a standard deviation of 0.1. Initial values of all internal state variables were set to 0. Each setup was executed 100 times with different random initializations of parameters. All networks were trained using stochastic gradient deviation (SGD) with Nesterov-like impulses (Sutskever et al., 2013)."}, {"heading": "4.1. Sequence Generation", "text": "The goal of this task is to train a recursive neural network that does not receive any input to generate a target sequence as accurately as possible. The weights of the network can be considered as (lossy) encoding of the entire sequence that could be used for compression. In the following experiments, we compare the performance of these five target sequences by scanning a piece of music 2 at 44.1Hz for 7ms. The resulting sequences of 320 data points were scaled to the interval [\u2212 1, 1]. In the following experiments, we compare the performance on these five sequences. All networks used the same architecture: no inputs, a hidden layer, and a single linear output neuron. Each network type was executed with 4 different sizes: 100, 250, 500, and 1000 parameters, see Table 1 for summarizing the number of hidden nodes. The networks were calibrated over 2000 epochs to minimize the quadratic error."}, {"heading": "4.2. Spoken Word Classification", "text": "The second task is the sequence classification instead of the generation. Each sequence contains an audio signal of a spoken word from the TIMIT Speech Recognition Benchmark (Garofolo et al., 1993). The data set contains 25 different words (classes) arranged in 5 clusters based on their suffix. Due to the suffix similarity, the network must learn long-term dependencies to clarify the words. Words are: Cluster 1: Manufacture, Go, Cook, Search, WorkCluster 2: Biblical, Cyclical, Technical, Classical, Critical Cluster 3: Tradition, Addition, Audition, Recognition, CompetitionCluster 4: Musicians, Discussions, Regulations, Allegations, Conditions Cluster 5: Subway, Leeway, Highway, Motorway, Corridor For each word there are 7 examples from different speakers divided into 5 for training and 2 for testing."}, {"heading": "4.1 NMSE 0.46\u00b10.08 0.04\u00b10.01 0.007\u00b10.004", "text": "For the CWRNN, the neurons were divided evenly into 7 groups with exponentially increasing periods: {1, 2, 4, 8, 16, 32, 64}. Figure 4 shows the classification error of the different networks in the task of word classification. Here, too, RNNNs perform worst, followed by LSTMs, which give substantially better results, especially with more parameters. CW RNNNs beat both RNN and LSTM networks on average by a considerable distance of 8-20%, regardless of the number of parameters. The error of the largest networks is summarized in Table 3 (row 2)."}, {"heading": "5. Discussion", "text": "The experimental results show that the simple mechanism of executing subsets of neurons at different speeds cannot be applied to all possible velocities to efficiently learn the different dynamic time scales in complex signals. Other functions could be used to determine the module periods: linear, fibonacci, logarithmic series, or even fixed random periods. These were not considered in this paper because the intuitive constellation of using an exponential series works well in these preliminary experiments. Another option would be to learn the periods period as well, which would require a differentiated modulo function to trigger the clocks. Alternatively, one could form the clocks (along with the weights) using evolutionary algorithms that do not have a closed form for the gradients.Note that the lowest period in the network may be larger than 1. Such a network would not be able to modify its output in any module, which may be useful in any time step as well."}, {"heading": "Acknowledgments", "text": "This research was supported by the Swiss National Science Foundation with funding number 138219: \"Theory and Practice of Reinforcement Learning 2\" and the EU FP7 project \"NanoBioTouch\" with funding number 228844."}], "references": [{"title": "Finding structure in time. CRL Technical Report 8801", "author": ["J.L. Elman"], "venue": "Center for Research in Language,", "citeRegEx": "Elman,? \\Q1988\\E", "shortCiteRegEx": "Elman", "year": 1988}, {"title": "Sequence labelling in structured domains with hierarchical recurrent neural networks", "author": ["Fernandez", "Santiago", "Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Fernandez et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fernandez et al\\.", "year": 2007}, {"title": "DARPA TIMIT acoustic phonetic continuous speech corpus CD-ROM", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren"], "venue": null, "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Rapid retraining on speech data with LSTM recurrent networks", "author": ["A. Graves", "N. Beringer", "J. Schmidhuber"], "venue": "Technical Report IDSIA-09-05,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural nets", "author": ["A. Graves", "S. Fernandez", "F.J. Gomez", "J. Schmidhuber"], "venue": "In ICML\u201906: Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "A novel connectionist system for improved unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "rahman Mohamed", "Abdel", "Hinton", "Geoffrey E"], "venue": "In ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S. Hochreiter"], "venue": "Diploma thesis, Institut fu\u0308r Informatik, Lehrstuhl Prof. Brauer, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter", "year": 1991}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Short term memory in echo state networks. GMD-Report 152, GMD - German", "author": ["H. Jaeger"], "venue": "National Research Institute for Computer Science,", "citeRegEx": "Jaeger,? \\Q2002\\E", "shortCiteRegEx": "Jaeger", "year": 2002}, {"title": "Learning longterm dependencies in NARX recurrent neural networks", "author": ["T. Lin", "B.G. Horne", "P. Tino", "C.L. Giles"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Lin et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lin et al\\.", "year": 1996}, {"title": "Distance measures for speech recognition: Psychological and instrumental", "author": ["P. Mermelstein"], "venue": "Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Mermelstein,? \\Q1976\\E", "shortCiteRegEx": "Mermelstein", "year": 1976}, {"title": "Induction of multiscale temporal structure", "author": ["M.C. Mozer"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mozer,? \\Q1992\\E", "shortCiteRegEx": "Mozer", "year": 1992}, {"title": "Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multi-scale processing", "author": ["M.C. Mozer"], "venue": "Connection Science,", "citeRegEx": "Mozer,? \\Q1994\\E", "shortCiteRegEx": "Mozer", "year": 1994}, {"title": "Learning sequential tasks by incrementally adding higher orders", "author": ["Ring", "Mark"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Ring and Mark.,? \\Q1993\\E", "shortCiteRegEx": "Ring and Mark.", "year": 1993}, {"title": "Recurrent transition hierarchies for continual learning: A general overview", "author": ["Ring", "Mark"], "venue": "In Lifelong Learning, volume WS-11-15 of AAAI Workshops. AAAI,", "citeRegEx": "Ring and Mark.,? \\Q2011\\E", "shortCiteRegEx": "Ring and Mark.", "year": 2011}, {"title": "The utility driven dynamic error propagation network", "author": ["A.J. Robinson", "F. Fallside"], "venue": "Technical Report CUED/FINFENG/TR.1,", "citeRegEx": "Robinson and Fallside,? \\Q1987\\E", "shortCiteRegEx": "Robinson and Fallside", "year": 1987}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Parallel Distributed Processing,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Long Short-Term Memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Sak", "Ha\u015fim", "Senior", "Andrew", "Beaufays", "Fran\u00e7oise"], "venue": "Technical report,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Neural sequence chunkers", "author": ["J. Schmidhuber"], "venue": "Technical Report FKI-148-91,", "citeRegEx": "Schmidhuber,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber", "year": 1992}, {"title": "Evolino: Hybrid neuroevolution / optimal linear search for sequence prediction", "author": ["J. Schmidhuber", "D. Wierstra", "F.J. Gomez"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Schmidhuber et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 2005}, {"title": "Training recurrent networks by Evolino", "author": ["J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F.J. Gomez"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 2007}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Temporal-kernel recurrent neural networks", "author": ["Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "Neural Networks,", "citeRegEx": "Sutskever et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2010}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks,", "citeRegEx": "Werbos,? \\Q1988\\E", "shortCiteRegEx": "Werbos", "year": 1988}, {"title": "Complexity of exact gradient computation algorithms for recurrent neural networks", "author": ["R.J. Williams"], "venue": "Technical Report NU-CCS-89-27,", "citeRegEx": "Williams,? \\Q1989\\E", "shortCiteRegEx": "Williams", "year": 1989}, {"title": "Training recurrent networks using the extended kalman filter", "author": ["Williams", "Ronald J"], "venue": "In Neural Networks,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 27, "context": "Recurrent Neural Networks (RNNs; Robinson & Fallside, 1987; Werbos, 1988; Williams, 1989) are a class of connectionist models that possess internal state or short term memory due to recurrent feed-back connections, that make them suitable for dealing with sequential problems, such as speech classification, prediction and generation.", "startOffset": 26, "endOffset": 89}, {"referenceID": 28, "context": "Recurrent Neural Networks (RNNs; Robinson & Fallside, 1987; Werbos, 1988; Williams, 1989) are a class of connectionist models that possess internal state or short term memory due to recurrent feed-back connections, that make them suitable for dealing with sequential problems, such as speech classification, prediction and generation.", "startOffset": 26, "endOffset": 89}, {"referenceID": 8, "context": "spanning more that 10 time-steps) encoded in the input sequences due to the vanishing gradient (Hochreiter, 1991; Hochreiter et al., 2001).", "startOffset": 95, "endOffset": 138}, {"referenceID": 10, "context": "spanning more that 10 time-steps) encoded in the input sequences due to the vanishing gradient (Hochreiter, 1991; Hochreiter et al., 2001).", "startOffset": 95, "endOffset": 138}, {"referenceID": 0, "context": "This paper presents a novel modification to the simple RNN (SRN; Elman, 1988) architecture and, mutatis mutandis, an associated error back-propagation through time (Rumelhart et al.", "startOffset": 59, "endOffset": 77}, {"referenceID": 19, "context": "This paper presents a novel modification to the simple RNN (SRN; Elman, 1988) architecture and, mutatis mutandis, an associated error back-propagation through time (Rumelhart et al., 1986; Werbos, 1988; Williams, 1989) training algorithm, that show superior performance in the generation and classification of sequences that contain long-term dependencies.", "startOffset": 164, "endOffset": 218}, {"referenceID": 27, "context": "This paper presents a novel modification to the simple RNN (SRN; Elman, 1988) architecture and, mutatis mutandis, an associated error back-propagation through time (Rumelhart et al., 1986; Werbos, 1988; Williams, 1989) training algorithm, that show superior performance in the generation and classification of sequences that contain long-term dependencies.", "startOffset": 164, "endOffset": 218}, {"referenceID": 28, "context": "This paper presents a novel modification to the simple RNN (SRN; Elman, 1988) architecture and, mutatis mutandis, an associated error back-propagation through time (Rumelhart et al., 1986; Werbos, 1988; Williams, 1989) training algorithm, that show superior performance in the generation and classification of sequences that contain long-term dependencies.", "startOffset": 164, "endOffset": 218}, {"referenceID": 12, "context": "One model that is similar in spirit to our approach is the NARX RNN1 (Lin et al., 1996).", "startOffset": 69, "endOffset": 87}, {"referenceID": 3, "context": "These networks have been very successful recently in speech and handwriting recognition (Graves et al., 2005; 2009; Sak et al., 2014).", "startOffset": 88, "endOffset": 133}, {"referenceID": 20, "context": "These networks have been very successful recently in speech and handwriting recognition (Graves et al., 2005; 2009; Sak et al., 2014).", "startOffset": 88, "endOffset": 133}, {"referenceID": 1, "context": "Stacking LSTMs into several layers (Fernandez et al., 2007; Graves & Schmidhuber, 2009) aims for hierarchical sequence processing.", "startOffset": 35, "endOffset": 87}, {"referenceID": 4, "context": "NARX stands for Non-linear Auto-Regressive model with eXogeneous inputs tionist Temporal Classification (CTC; Graves et al., 2006), performs simultaneous segmentation and recognition of sequences.", "startOffset": 104, "endOffset": 130}, {"referenceID": 7, "context": "Its deep variant currently holds the state-of-theart result in phoneme recognition on the TIMIT database (Graves et al., 2013).", "startOffset": 105, "endOffset": 126}, {"referenceID": 14, "context": "One of the earliest attempts to enable RNNs to handle long-term dependencies is the Reduced Description Network (Mozer, 1992; 1994).", "startOffset": 112, "endOffset": 131}, {"referenceID": 11, "context": "This technique was recently picked up by Echo State Networks (ESN; Jaeger, 2002).", "startOffset": 61, "endOffset": 80}, {"referenceID": 23, "context": "Evolino (Schmidhuber et al., 2005; 2007) feeds the input to an RNN (which can be e.", "startOffset": 8, "endOffset": 40}, {"referenceID": 25, "context": "HF optimization allowed for training of Multiplicative RNN (MRNN; Sutskever et al., 2011) that port the concept of multiplicative gating units to SRNs.", "startOffset": 59, "endOffset": 89}, {"referenceID": 21, "context": "The Sequence Chunker, Neural History Compressor or Hierarchical Temporal Memory (Schmidhuber, 1991; 1992) consists of a hierarchy or stack of RNN that may run at different time scales, but, unlike the simpler CW-RNN, it requires unsupervised event predictors: a higher-level RNN receives an input only when the lower-level RNN below is unable to predict it.", "startOffset": 80, "endOffset": 105}, {"referenceID": 2, "context": "Each sequence contains an audio signal of one spoken word from the TIMIT Speech Recognition Benchmark (Garofolo et al., 1993).", "startOffset": 102, "endOffset": 125}, {"referenceID": 13, "context": "Each sequence element consists of 12-dimensional MFCC vector (Mermelstein, 1976) plus energy, sampled every 10ms over a 25ms window with a pre-emphasis coefficient of 0.", "startOffset": 61, "endOffset": 80}], "year": 2014, "abstractText": "Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.", "creator": "LaTeX with hyperref package"}}}