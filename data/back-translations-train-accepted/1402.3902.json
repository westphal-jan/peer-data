{"id": "1402.3902", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2014", "title": "Sparse Polynomial Learning and Graph Sketching", "abstract": "Let f be a real valued polynomial evaluated over the boolean hypercube with at most s non-zero real coefficients in the fourier domain. We give an algorithm for exactly reconstructing f given random examples only from the uniform distribution on the boolean hypercube that runs in time polynomial in n and 2s and succeeds if each coefficient of f has been perturbed by a small Gaussian (or any other reasonable distribution on the reals). Learning sparse polynomials over the Boolean domain in time polynomial in n and 2s is considered a notoriously hard problem in the worst-case. Our result shows that the problem is tractable in the smoothed- analysis setting. Our proof combines a method for identifying unique sign patterns induced by the underlying monomials of f with recent work in compressed sensing. We identify other natural conditions on f for which our techniques will succeed.", "histories": [["v1", "Mon, 17 Feb 2014 06:00:16 GMT  (27kb)", "http://arxiv.org/abs/1402.3902v1", "15 pages"], ["v2", "Tue, 18 Feb 2014 06:56:27 GMT  (17kb)", "http://arxiv.org/abs/1402.3902v2", "11 pages"], ["v3", "Wed, 5 Nov 2014 22:35:40 GMT  (76kb)", "http://arxiv.org/abs/1402.3902v3", "14 pages; to appear in NIPS 2014l Updated proof of Theorem 5 and some other minor changes during revision"], ["v4", "Fri, 7 Nov 2014 03:00:28 GMT  (65kb)", "http://arxiv.org/abs/1402.3902v4", "14 pages; to appear in NIPS 2014l Updated proof of Theorem 5 and some other minor changes during revision"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["murat kocaoglu", "karthikeyan shanmugam", "alexandros g dimakis", "adam r klivans"], "accepted": true, "id": "1402.3902"}, "pdf": {"name": "1402.3902.pdf", "metadata": {"source": "CRF", "title": "A Smoothed Analysis for Learning Sparse Polynomials", "authors": ["Alexandros G. Dimakis", "Murat Kocaoglu", "Karthikeyan Shanmugam", "Csaba Szepesv\u00e1ri", "K. Shanmugam", "G. DIMAKIS", "KLIVANS KOCAOGLU SHANMUGAM"], "emails": ["DIMAKIS@AUSTIN.UTEXAS.EDU", "KLIVANS@CS.UTEXAS.EDU", "MKOCAOGLU@UTEXAS.EDU", "KARTHIKSH@UTEXAS.EDU"], "sections": [{"heading": null, "text": "ar Xiv: 140 2.39 02v1 [cs.LG] 1 7Our evidence combines a method for identifying unique character patterns induced by the underlying monomials of f with more recent work in compressed perception. We identify other natural conditions at f for which our techniques will be successful. Keywords: smoothed analysis, learning sparse polynomials, compressed perception."}, {"heading": "1. Introduction", "text": "It is one of the fundamental problems dependent on mathematical learning theory and has been extensively studied over the last twenty-five years. (1) In almost all cases, known algorithms for learning or interpolating polynomials are required to gain access to the unknown polynomials. (1) One outstanding open problem is to find an algorithm for learning that s-sparse polynomials in relation to the uniform distribution of {\u2212 1} n running polynomially in n and g (s) in time (where g each fixed function is independent of n) and requires only randomly selected examples to succeed. Specifically, such an algorithm would imply a groundbreaking result for learning k-juntas (functions that depend on n); < < < < <;"}, {"heading": "1.1. Approach and Related Work", "text": "The question that arises is to what extent it is actually a matter of a way in which people see themselves as being in a position to put themselves in the world, in which they are able to understand the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live"}, {"heading": "2. Definitions", "text": "Consider a function over the boolean hypercuity f: {\u2212 1} n > q = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 ="}, {"heading": "3. Problem Setting", "text": "Suppose that m (x) cI (x) cI (x) cI (x) cI (s), where we (n) s (n) s (n) s (n) s (n) s (n) s (n) s (S) s (n) s (n) s (n) s (n) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S (S) s (S) s (S) s (S) s (S (S) s (S) s (S) s (S (S) s) s (S (S) s (S (S) s) s (S (S) s) s (S (S (S) s) s (S (S (S (S) s) s) s (S (S (S (S (S (S) s) s) s) s (S (S (S (S (S (S (S (S (S (S (S) s) s) s) s) s (S (S (S (S (S (S (S (S (S (S (S (S) s) s) s) s) s (S (S (S (S (S (S (S (S (S (S (s) s) s) s) s (S (S (S (S (S (S (S (S (S (S (s) s) s) s) s (S (S (S (S (S (S (S (S (s) s) s) s) s) s (S (S (S (S (S (S (S (S (S (s) s) s) s"}, {"heading": "4. Algorithm and Guarantees: Noiseless case", "text": "Let me use the family of s subsets (n, 2s) with the probability 1 (n, 2s). (c) All the coefficients of s parity functions are linear independent of each other. (c) All coefficients of s functions are independent of each other. (c) All coefficients of s parity functions are independent of each other. (c) All coefficients of s parity functions are independent of each other. (c) All coefficients of s functions are independent of each other. (c) All coefficients are positive of each other. (c) All coefficients are positive of each other. (c) All coefficients are positive of each other."}, {"heading": "5. Algorithms and Guarantees: Noisy Case", "text": "In this section we provide our algorithm for learning an approximate (s, \u03bd) -sparse function under the aforementioned conditions and prove guarantees regarding the error between the learned function and the actual function. If m random samples are observed, the output model for an approximate (s, \u03bd) -sparse function f is given by: y = Ac + \u03b5 (12), where A is the m of 2n matrix in which each row corresponds to a sample x and each column, and c is the set of Fourier coefficients for f and noise. We remember that S Ic is for an approximate sparse f. We assume that the observations are noisy, the set of inputs for the sparse representation of the function f, i.e., fI (this only depends on the Fourier coefficients in I)."}, {"heading": "Appendix A. Proof of Lemma 14", "text": "Proof Let E1 be the event that the maximum value observed under m1 samples in algorithm 1 is the maximum value reached by f. Note that the probability that the function reaches the maximum value is at least 12s. To see this, the probability that the parity functions have rank r, then the set of linear independent parity functions takes values uniformly in the hypercube and if E1 is true, it is easy to see that the actual party functions pi in the specified P are in the algorithm. Consider algorithm 1. Let E3 be the event that the matrix Ymax has at least rank n. E3 implies that | P | S | \u2264 2s + 1 that the hyperactivity is in the P series."}, {"heading": "Appendix B. Proof of Lemma 19", "text": "Proof Let rank (P) = r. Suffice it to show that rank (f \u2212 1 (1)) = rank (Z1) with probability 1 \u2212 O (1 n). Let E1 be the event that H1 is tried at least 2n times. Let Egg be the event that Hi is tried at least once for every 2 \u2264 i \u2264 m. Let E be the event that the row space of Z1 includes the vector space chip (N (P), u1). Let z1, z2. Let 2n + q (q \u2265 0) be the row entries from H1. Then zj = u1 + vj, where vj \u30fb N (P) is odd. Let E be the event that {vj} 2nj = 1 rank n \u2212 r. Consider the event E \u00b2 conditionally on E1. After 2n random drawings it is clear that the probability that {vj} 2nj = 1 rank is equal to the probability."}], "references": [{"title": "Deterministic sparse fourier approximation via fooling arithmetic progressions", "author": ["Adi Akavia"], "venue": "In COLT, pages 381\u2013393,", "citeRegEx": "Akavia.,? \\Q2010\\E", "shortCiteRegEx": "Akavia.", "year": 2010}, {"title": "Learning sparse polynomial functions", "author": ["Alexandr Andoni", "Rina Panigrahy", "Gregory Valiant", "Li Zhang"], "venue": "In SODA,", "citeRegEx": "Andoni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Andoni et al\\.", "year": 2014}, {"title": "Decoding by linear programming", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2005\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2005}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["Emmanuel J Cand\u00e8s", "Justin Romberg", "Terence Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Compressed sensing", "author": ["David L Donoho"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Near-optimal sparse fourier representations via sampling", "author": ["Anna C. Gilbert", "Sudipto Guha", "Piotr Indyk", "S. Muthukrishnan", "Martin Strauss"], "venue": "In STOC,", "citeRegEx": "Gilbert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gilbert et al\\.", "year": 2002}, {"title": "Agnostically learning decision trees", "author": ["P. Gopalan", "A. Kalai", "A. Klivans"], "venue": "In Proceedings of STOC,", "citeRegEx": "Gopalan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2008}, {"title": "Learning and smoothed analysis", "author": ["Adam Tauman Kalai", "Alex Samorodnitsky", "Shang-Hua Teng"], "venue": "In FOCS,", "citeRegEx": "Kalai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2009}, {"title": "Learning decision trees using the fourier spectrum", "author": ["Eyal Kushilevitz", "Yishay Mansour"], "venue": null, "citeRegEx": "Kushilevitz and Mansour.,? \\Q1993\\E", "shortCiteRegEx": "Kushilevitz and Mansour.", "year": 1993}, {"title": "Randomized interpolation and approximation of sparse polynomials", "author": ["Yishay Mansour"], "venue": null, "citeRegEx": "Mansour.,? \\Q1995\\E", "shortCiteRegEx": "Mansour.", "year": 1995}, {"title": "Learning juntas", "author": ["Elchanan Mossel", "Ryan O\u2019Donnell", "Rocco P Servedio"], "venue": "In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Mossel et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Mossel et al\\.", "year": 2003}, {"title": "Learning sparse boolean polynomials", "author": ["Sahand Negahban", "Devavrat Shah"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "Negahban and Shah.,? \\Q2012\\E", "shortCiteRegEx": "Negahban and Shah.", "year": 2012}, {"title": "Learning sparse multivariate polynomials over a field with queries and counterexamples", "author": ["R. Schapire", "R. Sellie"], "venue": null, "citeRegEx": "Schapire and Sellie.,? \\Q1996\\E", "shortCiteRegEx": "Schapire and Sellie.", "year": 1996}, {"title": "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time", "author": ["D. Spielman", "S. Teng"], "venue": null, "citeRegEx": "Spielman and Teng.,? \\Q2004\\E", "shortCiteRegEx": "Spielman and Teng.", "year": 2004}, {"title": "Learning fourier sparse set functions", "author": ["Peter Stobbe", "Andreas Krause"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Stobbe and Krause.,? \\Q2012\\E", "shortCiteRegEx": "Stobbe and Krause.", "year": 2012}, {"title": "Finding correlations in subquadratic time, with applications to learning parities and juntas", "author": ["Gregory Valiant"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Valiant.,? \\Q2012\\E", "shortCiteRegEx": "Valiant.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al.", "startOffset": 200, "endOffset": 231}, {"referenceID": 5, "context": "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al.", "startOffset": 200, "endOffset": 247}, {"referenceID": 5, "context": "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al.", "startOffset": 200, "endOffset": 275}, {"referenceID": 4, "context": "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al. (2002); Gopalan et al.", "startOffset": 276, "endOffset": 298}, {"referenceID": 4, "context": "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al. (2002); Gopalan et al. (2008); Akavia (2010).", "startOffset": 276, "endOffset": 321}, {"referenceID": 0, "context": "(2008); Akavia (2010). In almost all cases, known algorithms for learning or interpolating sparse polynomials require query access to the unknown polynomial.", "startOffset": 8, "endOffset": 22}, {"referenceID": 13, "context": "Smoothed-analysis, pioneered in Spielman and Teng (2004), has now become a common alternative for problems that seem intractable in the worst-case.", "startOffset": 32, "endOffset": 57}, {"referenceID": 2, "context": "Approach and Related Work The problem of recovering the sparsest solution of a set of underdetermined linear equations has received significant recent attention in the context of compressed sensing Cand\u00e8s et al. (2006); Cand\u00e8s and Tao (2005); Donoho (2006).", "startOffset": 198, "endOffset": 219}, {"referenceID": 2, "context": "(2006); Cand\u00e8s and Tao (2005); Donoho (2006).", "startOffset": 8, "endOffset": 30}, {"referenceID": 2, "context": "(2006); Cand\u00e8s and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise.", "startOffset": 8, "endOffset": 45}, {"referenceID": 2, "context": "(2006); Cand\u00e8s and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise. A large body of recent results has established conditions under which convex relaxations can exactly recover the unknown vector or its support. The recent papers Stobbe and Krause (2012); Negahban and Shah (2012) are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing.", "startOffset": 8, "endOffset": 383}, {"referenceID": 2, "context": "(2006); Cand\u00e8s and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise. A large body of recent results has established conditions under which convex relaxations can exactly recover the unknown vector or its support. The recent papers Stobbe and Krause (2012); Negahban and Shah (2012) are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing.", "startOffset": 8, "endOffset": 409}, {"referenceID": 2, "context": "(2006); Cand\u00e8s and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise. A large body of recent results has established conditions under which convex relaxations can exactly recover the unknown vector or its support. The recent papers Stobbe and Krause (2012); Negahban and Shah (2012) are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing. The authors show that the problem of learning a sparse polynomial is equivalent to recovering the unknown sparse coefficient vector using linear measurements. The key difference from standard compressed sensing is that the unknown vector has exponential length, having one coordinate for each Fourier coefficient. By applying techniques from compressed sensing theory, namely Restricted Isometry Property (see Stobbe and Krause (2012)) and incoherence (see Negahban and Shah (2012)), the authors independently established results for reconstructing sparse polynomials using convex optimization.", "startOffset": 8, "endOffset": 972}, {"referenceID": 2, "context": "(2006); Cand\u00e8s and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise. A large body of recent results has established conditions under which convex relaxations can exactly recover the unknown vector or its support. The recent papers Stobbe and Krause (2012); Negahban and Shah (2012) are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing. The authors show that the problem of learning a sparse polynomial is equivalent to recovering the unknown sparse coefficient vector using linear measurements. The key difference from standard compressed sensing is that the unknown vector has exponential length, having one coordinate for each Fourier coefficient. By applying techniques from compressed sensing theory, namely Restricted Isometry Property (see Stobbe and Krause (2012)) and incoherence (see Negahban and Shah (2012)), the authors independently established results for reconstructing sparse polynomials using convex optimization.", "startOffset": 8, "endOffset": 1019}, {"referenceID": 1, "context": "We remark here on the interesting recent work of Andoni et al. (2014) that approximately learns sparse polynomial functions when the underlying domain is Gaussian.", "startOffset": 49, "endOffset": 70}, {"referenceID": 1, "context": "We remark here on the interesting recent work of Andoni et al. (2014) that approximately learns sparse polynomial functions when the underlying domain is Gaussian. Their results do not seem to translate to the Boolean domain. We also note the work of Kalai et al. (2009) that gives an algorithm for learning sparse Boolean functions with respect to a randomly chosen product distribution on {\u22121, 1}n.", "startOffset": 49, "endOffset": 271}, {"referenceID": 11, "context": "We use the compressed sensing framework used in Negahban and Shah (2012) and Stobbe and Krause (2012).", "startOffset": 48, "endOffset": 73}, {"referenceID": 11, "context": "We use the compressed sensing framework used in Negahban and Shah (2012) and Stobbe and Krause (2012). Specifically, for the remainder of this paper, we rely on Negahban and Shah (2012) as a point of reference.", "startOffset": 48, "endOffset": 102}, {"referenceID": 11, "context": "We use the compressed sensing framework used in Negahban and Shah (2012) and Stobbe and Krause (2012). Specifically, for the remainder of this paper, we rely on Negahban and Shah (2012) as a point of reference.", "startOffset": 48, "endOffset": 186}, {"referenceID": 11, "context": "We use the compressed sensing framework used in Negahban and Shah (2012) and Stobbe and Krause (2012). Specifically, for the remainder of this paper, we rely on Negahban and Shah (2012) as a point of reference. We review their framework and explain how we use it to obtain our results. Theorem 10 (Negahban and Shah (2012)) Any two columns of A satisfy the incoherence property, (AS1) T AS2 \u2264 4 \u221a n m with probability at least 1\u2212 2 256n for S1 6= S2.", "startOffset": 48, "endOffset": 323}, {"referenceID": 11, "context": "The compressive sensing framework in Negahban and Shah (2012) along with the incoherence property has the following intuition: In a problem where an approximately sparse (\u03b2S ) feasible solution (coefficients in a sparse set I \u2286 S are \u2019big\u2019 and others \u2019small\u2019) is known to exist, the optimum (\u03b2 S ) approximates the sparse solution well and the error is bounded by the small coefficients and noise in the observation.", "startOffset": 37, "endOffset": 62}, {"referenceID": 11, "context": "The compressive sensing framework in Negahban and Shah (2012) along with the incoherence property has the following intuition: In a problem where an approximately sparse (\u03b2S ) feasible solution (coefficients in a sparse set I \u2286 S are \u2019big\u2019 and others \u2019small\u2019) is known to exist, the optimum (\u03b2 S ) approximates the sparse solution well and the error is bounded by the small coefficients and noise in the observation. We state a slightly general result, that is implied by the results in Negahban and Shah (2012). Theorem 11 (Negahban and Shah (2012)) For any family of subsets I \u2208 2[n] such that |I| = s, m = 4096ns2 and c1 = 4, c2 = 8, for any feasible solution \u03b2S of program 6, we have: \u2016\u03b2S \u2212 \u03b2 opt S \u20162 \u2264 c1\u01eb+ c2 ( n m )1/4 \u2016\u03b2Ic \u22c2 S\u20161 (7) with probability at least 1\u2212O ( 1 4n )", "startOffset": 37, "endOffset": 512}, {"referenceID": 11, "context": "The compressive sensing framework in Negahban and Shah (2012) along with the incoherence property has the following intuition: In a problem where an approximately sparse (\u03b2S ) feasible solution (coefficients in a sparse set I \u2286 S are \u2019big\u2019 and others \u2019small\u2019) is known to exist, the optimum (\u03b2 S ) approximates the sparse solution well and the error is bounded by the small coefficients and noise in the observation. We state a slightly general result, that is implied by the results in Negahban and Shah (2012). Theorem 11 (Negahban and Shah (2012)) For any family of subsets I \u2208 2[n] such that |I| = s, m = 4096ns2 and c1 = 4, c2 = 8, for any feasible solution \u03b2S of program 6, we have: \u2016\u03b2S \u2212 \u03b2 opt S \u20162 \u2264 c1\u01eb+ c2 ( n m )1/4 \u2016\u03b2Ic \u22c2 S\u20161 (7) with probability at least 1\u2212O ( 1 4n )", "startOffset": 37, "endOffset": 550}, {"referenceID": 10, "context": "slowdown (see Mossel et al. (2003)).", "startOffset": 14, "endOffset": 35}, {"referenceID": 10, "context": "slowdown (see Mossel et al. (2003)). As far as we are aware of, the best running time for learning a k-junta is n0.6kpoly(n) due to Valiant (2012). In Section 6, using algebraic insights from the previous cases, we show that if rank ( f\u22121(1) )", "startOffset": 14, "endOffset": 147}], "year": 2017, "abstractText": "Let f : {\u22121, 1}n \u2192 R be a polynomial with at most s non-zero real coefficients. We give an algorithm for exactly reconstructing f given random examples only from the uniform distribution on {\u22121, 1}n that runs in time polynomial in n and 2s and succeeds if each coefficient of f has been perturbed by a small Gaussian (or any other reasonable distribution on the reals). Learning sparse polynomials over the Boolean domain in time polynomial in n and 2s is considered a notoriously hard problem in the worst-case. Our result shows that the problem is tractable in the smoothedanalysis setting. Our proof combines a method for identifying unique sign patterns induced by the underlying monomials of f with recent work in compressed sensing. We identify other natural conditions on f for which our techniques will succeed.", "creator": "LaTeX with hyperref package"}}}