{"id": "1402.4102", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2014", "title": "Stochastic Gradient Hamiltonian Monte Carlo", "abstract": "Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system---such a computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.", "histories": [["v1", "Mon, 17 Feb 2014 19:57:59 GMT  (147kb,D)", "https://arxiv.org/abs/1402.4102v1", null], ["v2", "Mon, 12 May 2014 06:38:21 GMT  (110kb,D)", "http://arxiv.org/abs/1402.4102v2", "ICML 2014 version"]], "reviews": [], "SUBJECTS": "stat.ME cs.LG stat.ML", "authors": ["tianqi chen", "emily b fox", "carlos guestrin"], "accepted": true, "id": "1402.4102"}, "pdf": {"name": "1402.4102.pdf", "metadata": {"source": "META", "title": "Stochastic Gradient Hamiltonian Monte Carlo", "authors": ["Tianqi Chen", "Emily B. Fox", "Carlos Guestrin"], "emails": ["TQCHEN@CS.WASHINGTON.EDU", "EBFOX@STAT.WASHINGTON.EDU", "GUESTRIN@CS.WASHINGTON.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Hamiltonian Monte Carlo", "text": "Suppose we want a set of independent observations from the posterior distribution of the imbalances. (...) Suppose we want a set of independent observations from the posterior distribution of the imbalances. (...) Suppose we have a common distribution of the energy. (...) Suppose we want a common distribution of the energy. (...) Suppose we want a common distribution of the energy. (...) Suppose we want a common distribution of the energy. (...) Suppose we have a common distribution of the energy. (...) Suppose we have a common distribution of the energy. (...) Suppose we want a common distribution of the energy. (...) Suppose we have a common distribution of the energy. (...) Suppose we want a common distribution of the energy. (...) Suppose we want a common distribution of the energy. (...) Suppose we consider the common distribution of the energy. (...) Suppose we consider the common distribution of the energy. (...) Suppose we want a common distribution of the energy. (...) Suppose we want a common distribution of the energy."}, {"heading": "3. Stochastic Gradient HMC", "text": "In this section, we examine the effects of implementing HMC using a stochastic gradient and propose variants of Hamiltonian dynamics that are more resistant to the noise caused by the stochastic gradient estimates. In all scenarios, instead of calculating the costly gradient directly using Eq. (2), which requires an examination of the entire dataset D, we consider a noisy estimate based on a minibatch D that is sampled evenly at random by D. (5) We assume that our observations are x independent and, relying on the central limit theorem, an approximate of these noisy gradients as Eq. (5) We want a significant reduction in the miniature parameters. (6) Here V is the covariance of stochastic gradient noise that depends on the current size of that sample."}, {"heading": "3.1. Na\u0131\u0308ve Stochastic Gradient HMC", "text": "The most straightforward approach to this approach is that we can only imagine the surface if we replace it with the surface. (...) This is only a matter of time. (...) The resulting discrete time system can be regarded as a -discretization system. (...) Here, B (...) is the diffusion matrix provided by gradient noise. (...) As with the original HMC formulation, it is useful to return to a continuous time system in order to derive characteristics of the procedure. (...) To gain some intuition about this procedure, we consider the analogy of c. (...) Here we can imagine that we derive the surface with a continuous time system in relation to the procedure. (...) It is useful to derive the characteristics of the procedure of the procedure. (...) To gain some intuitions about this setting, we can imagine c. (Here) (Here) the analogy."}, {"heading": "3.2. Stochastic Gradient HMC with Friction", "text": "In Sec. 3.1 we showed that HMC has stochastic gradients 2. This is a frequently costly MH correction stage, or alternatively, long simulation runs with low acceptance probabilities. (7) To this end, we consider a modification of Eq. (9) Here, and throughout the rest of the paper, we omit the dependence on B based on notation. Let us again make an analogy to hockey. Imagine we are now playing street hockey instead of hockey, which introduces friction from the asphalt."}, {"heading": "3.3. Stochastic Gradient HMC in Practice", "text": "In all these areas, we are able to deal with the question of whether we are going to be able to deal with what areas we need to deal with. (...) We need to deal with the question of to what extent we are able to deal with what areas we are going to deal with. (...) We need to deal with the question of to what extent we are able to deal with what areas we are going to deal with. (...) We need to deal with the question of to what extent we are able to deal with what areas we are going to deal with. (...) We need to deal with the question of what areas we are going to deal with. (...) We need to deal with the question of what areas we are going to deal with. (...) We need to deal with the question of what areas we are going to deal with. (...) We need to deal with the question. (...) We need to deal with the question. (...) We need to deal with the question. \"(...) We need to deal with the question. (...) We need to deal with the question. (...) We need to deal with the question."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Simulated Scenarios", "text": "Too empirically, we study the behavior of HMC with exact gradients relative to stochastic gradients, although we do not conduct experiments on a simulated basis. As a basis, we consider the standard HMC implementation of Alg. 1, both with and without the MH correction. We then compare with HMC with stochastic gradients generated by the various sampling algorithms, and consider this proposal with and without an MH correction. Finally, we compare our proposed SGHMC correction, which does not use MH correction.1 shows the empirical distributions generated by the various sampling algorithms. We see that even without an MH correction, the HMC and SGHMC algorithms yield results close to the true distribution, implying that errors from the consideration are not zero. On the other hand, the results of na\u00efve stochastic distributions are significant."}, {"heading": "4.2. Bayesian Neural Networks for Classification", "text": "We also test our method using a handwritten number classification task using the MNIST dataset 3. The dataset consists of 60,000 training instances and 10,000 test instances. We randomly divide a validation set of 10,000 instances from the training data to select training parameters, and use the remaining 50,000 instances for training. For classification, we consider a two-layer Bavarian neural network with 100 hidden variables using a sigmoid unit and an output layer using Softmax. We tested four methods: SGD, SGD with Dynamics, SGLD and SGHMC. For optimization-based methods, we use the validation set to select the optimal regulator of network weights 4. For sample-based methods, we take a fully Bavarian approach and place a weakly informative gamma run on the weight controllers of each layer."}, {"heading": "4.3. Online Bayesian Probabilistic Matrix Factorization for Movie Recommendations", "text": "The task is to predict a user's preferences over a number of elements (e.g. movies, music) and formulate recommendations. The probabilistic matrix factorization (PMF) (Salakhutdinov & Mnih, 2008b) has proven effective for this task. Due to the rarity of the rating matrix (users versus items) in recommendation systems, overmatching is a serious problem with Bayean approaches that offer a natural solution (Salakhutdinov & Mnih, 2008a).We are conducting an experiment in Bayean online PMF methods on the Movielens dataset ml-1M5. The dataset contains about 1 million reviews of 3,952 films from 6,040 users. The number of latent dimensions is set to 20. When comparing our stochastic gradient-based approaches, we are using minibatches of 4,000 reviews to dramatically generate the user's choice and the larger matrix (i.e., we are using the minibatches of 4,000 reviews rather than the larger ones)."}, {"heading": "5. Conclusion", "text": "To address this problem on a large scale or online, we proposed SGHMC, an efficient method of generating high-quality, \"remote\" steps in such sampling methods. Our approach builds on the basic framework of HMC, but uses stochastic estimates of the gradient to avoid the costly calculation of the full gradient. Surprisingly, we discovered that the natural way to include stochastic gradient estimates in HMC can lead to divergence and poor behavior both in theory and practice. To meet this challenge, we introduced second-order Langevin dynamics with a friction term that counteracts the effects of the loud gradient and maintains the desired target distribution as an invariant distribution of the continuous system. Our empirical results, both in a simulated experiment and on real data, confirm our theory and demonstrate the practical value of introducing this simple modification. A natural next step is to combine adaptation techniques with the HMC unit described here as efficient."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the TerraSwarm Research Center sponsored by MARCO and DARPA, ONR Grant N0001410-1-0746, DARPA Grant FA9550-12-1-0406 negotiated by AFOSR, NSF IIS-1258741 and Intel ISTC Big Data. We also appreciate the discussions with Mark Girolami, Nick Foti, Ping Ao and Hong Qian."}, {"heading": "A. Background on Fokker-Planck Equation", "text": "The Fokker-Planck Equation (FPE), which is linked to a given stochastic differential equation (SDE), describes the temporal evolution of the distribution of the random variables within the given stochastic dynamics. Consider, for example, the SDE: dz = g (z) dt + N (0, 2D (z) dt), (16), where z-Rn, g (z) x-Rn, D (z) x-Rn \u00b7 n. The distribution of z is determined by Equation (16) (denoted by pt (z))), develops according to the following equation: tpt (z) = \u2212 n \u00b2 i = 1 x zi (z) + n \u00b2 i = 1 x zi = 1 x zj [Dij (z).Here gi (z) is the i-th input of the vector g (z) and Dij (z) is the i-th input of the matrix D."}, {"heading": "B. Proof of Theorem 3.1", "text": "Leave G = [0 \u2212 I] and D = [0 \u2212 I] (0 \u2212 I) (0 \u2212 I) (0 \u2212 I) (0 \u2212 I) (0 \u2212 I) (0 \u2212 I) (0 \u2212 I) (0 \u2212 I) (0 \u2212 H) (0, 2Ddt) (0, 2Ddt) (0, 2Ddt) (0, 2Ddt) (0, 2Ddt) (0, 2Ddt) (2Ddt) (0, 2Ddt) (19) We use z (z) (z) to name the common variable of position and weight (z). Entropy is defined by h (pt, r) (z) (z). (19) We use z = (pt, r) to name the common variable of position and weight. (z) Entropy is defined by h (pt, r) (z). (z) Entropy is defined by h (pt, r) (z)."}, {"heading": "C. Proof of Corollary 3.1", "text": "Let us assume that \u03c0 (\u03b8, r) = exp (\u03b8, r) / Z under equation (7) is invariant and is a well-behaved distribution, such as thatH (\u03b8, r) \u221e as a relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay))))))) relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay relay race"}, {"heading": "D. FPE for Second-Order Langevin Dynamics", "text": "The dynamics of second-order Langevin can be described with the following equation: \"F + G\" q (0 \u2212 I B), \"U\" M \u2212 1r) dt + N (0, 2\u03c4Ddt) = \u2212 \"F + G\" q (0, 2\u03c4Ddt), (20), \"D + G\" is a temperature (normally set to 1). In this paper, we use the following compact form of FPE to calculate the distribution evolution under Eq (20): \"D + G\" [D + G] [pt [D + G] [pt, r), \"H.\" (21) To derive this FPE, we apply the equation (18) to the equation (20) that defines g (z) = \u2212 \"H,\" which results in equal distribution. \"(D + G),\" H. \""}, {"heading": "E. Reversibility of SGHMC Dynamics", "text": "The dynamics of SGHMC are not reversible in the conventional definition of reversibility, but the dynamics have the following property: Theorem E.1. Assuming that the dynamics P (\u03b8t, rt | \u03b80, r0) is the distribution determined by the dynamics in Eq. (20), i.e. P (\u03b8t, rt | \u03b80, r0) follows Eq. (21), then the dynamics (\u03b8, r). (22) Assumption of dynamics is the stationary distribution and P is the reversal time Markov process associated with P: \u03c0, r0, r0) P (\u03b8t, \u2212 rt) P (\u03b8t, \u2212 havid, \u2212 rt). (22) Assumption of dynamics is the stationary distribution and P is the reversal time Markov process associated with P."}, {"heading": "F. Convergence Analysis", "text": "In the thesis we have discussed that the efficiency of SGHMC decreases when the step size decreases. In practice, we usually want to act a small amount of error effectiveness. In the case of SGHMC, we are interested in a small, not zero and fast approximation of B given by B. In this case, even under the continuous dynamics, the scanning procedure contains errors related to the inaccurate estimation of B with B. In this section, we will examine how the choice of B can be related to the error in the final stationary distribution. The scanning procedure with the inaccurate estimation of B can be described with the following dynamics: D - 1r dt dr = \u2212 U, dt \u2212 CM \u2212 1rdt + N (0, 2 (C + S) dt."}, {"heading": "G. Setting SGHMC Parameters", "text": "How we discussed in section 3.3, we can SGHMC with SGD with dynamics in connection, whereby we connect the dynamics as (see simile. (15)) {???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "H. Experimental Setup", "text": "(27) Here are the results for the first layer (1, 2, \u00b7 \u00b7 \u00b7, 10) but also the results for the second layer (1, 2, \u00b7 \u00b7, 10). We are also presenting a model for the first layer. We are also presenting a model for the second layer. We are presenting a model for the third layer. (1, 3) We are presenting a model for the third layer. (1, 4) We are presenting a model for the third layer. (1, 4) We are presenting a model for the third layer. (1, 4) We are presenting a model for the third layer. (1, 4) We are presenting a model for the third layer. (1, 5) We are presenting a model for the third layer. (A) exp. (A) exp. (B) exp. (B) exp. (1) exp. (a) exp. (a) exp. (a) exp. (a) exp. (a) exp."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Towards scaling up Markov chain Monte Carlo: An adaptive subsampling approach", "author": ["R. Bardenet", "A. Doucet", "C. Holmes"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML\u201914),", "citeRegEx": "Bardenet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bardenet et al\\.", "year": 2014}, {"title": "Hybrid Monte Carlo", "author": ["S. Duane", "A.D. Kennedy", "B.J. Pendleton", "D. Roweth"], "venue": "Physics Letters B,", "citeRegEx": "Duane et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Duane et al\\.", "year": 1987}, {"title": "Riemann manifold Langevin and Hamiltonian Monte Carlo methods", "author": ["M. Girolami", "B. Calderhead"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Girolami and Calderhead,? \\Q2011\\E", "shortCiteRegEx": "Girolami and Calderhead", "year": 2011}, {"title": "The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian", "author": ["M.D. Hoffman", "A. Gelman"], "venue": "Monte Carlo. arXiv,", "citeRegEx": "Hoffman and Gelman,? \\Q2011\\E", "shortCiteRegEx": "Hoffman and Gelman", "year": 2011}, {"title": "Stochastic variational inference", "author": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"], "venue": "Journal of Maching Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "A generalized guided Monte Carlo algorithm", "author": ["A.M. Horowitz"], "venue": "Physics Letters B,", "citeRegEx": "Horowitz,? \\Q1991\\E", "shortCiteRegEx": "Horowitz", "year": 1991}, {"title": "Austerity in MCMC land: Cutting the Metropolis-Hastings budget", "author": ["A. Korattikara", "Y. Chen", "M. Welling"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML\u201914),", "citeRegEx": "Korattikara et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Korattikara et al\\.", "year": 2014}, {"title": "MCMC using Hamiltonian dynamics", "author": ["R.M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "Neal,? \\Q2010\\E", "shortCiteRegEx": "Neal", "year": 2010}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Robbins and Monro,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In Proceedings of the 25th International Conference on Machine Learning", "citeRegEx": "Salakhutdinov and Mnih,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Mnih", "year": 2008}, {"title": "Relation of a new interpretation of stochastic differential equations to Ito process", "author": ["J. Shi", "T. Chen", "R. Yuan", "B. Yuan", "P. Ao"], "venue": "Journal of Statistical Physics,", "citeRegEx": "Shi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML\u201913),", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "On the Theory of the Brownian Motion II", "author": ["M.C. Wang", "G.E. Uhlenbeck"], "venue": "Reviews of Modern Physics,", "citeRegEx": "Wang and Uhlenbeck,? \\Q1945\\E", "shortCiteRegEx": "Wang and Uhlenbeck", "year": 1945}, {"title": "Adaptive Hamiltonian and Riemann manifold Monte Carlo", "author": ["Z. Wang", "S. Mohamed", "D. Nando"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML\u201913),", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Welling and Teh,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Existence and construction of dynamical potential in nonequilibrium processes without detailed balance", "author": ["L. Yin", "P. Ao"], "venue": "Journal of Physics A: Mathematical and General,", "citeRegEx": "Yin and Ao,? \\Q2006\\E", "shortCiteRegEx": "Yin and Ao", "year": 2006}], "referenceMentions": [{"referenceID": 2, "context": "Hamiltonian Monte Carlo (HMC) (Duane et al., 1987; Neal, 2010) sampling methods provide a powerful Markov chain Monte Carlo (MCMC) sampling algorithm.", "startOffset": 30, "endOffset": 62}, {"referenceID": 8, "context": "Hamiltonian Monte Carlo (HMC) (Duane et al., 1987; Neal, 2010) sampling methods provide a powerful Markov chain Monte Carlo (MCMC) sampling algorithm.", "startOffset": 30, "endOffset": 62}, {"referenceID": 8, "context": "Based on the attractive properties of HMC in terms of rapid exploration of the state space, HMC methods have grown in popularity recently (Neal, 2010; Hoffman & Gelman, 2011; Wang et al., 2013).", "startOffset": 138, "endOffset": 193}, {"referenceID": 15, "context": "Based on the attractive properties of HMC in terms of rapid exploration of the state space, HMC methods have grown in popularity recently (Neal, 2010; Hoffman & Gelman, 2011; Wang et al., 2013).", "startOffset": 138, "endOffset": 193}, {"referenceID": 5, "context": "Recently, in a variety of machine learning algorithms, we have witnessed the many successes of utilizing a noisy estimate of the gradient based on a minibatch of data to scale the algorithms (Robbins & Monro, 1951; Hoffman et al., 2013; Welling & Teh, 2011).", "startOffset": 191, "endOffset": 257}, {"referenceID": 9, "context": "A majority of these developments have been in optimization-based algorithms (Robbins & Monro, 1951; Nemirovski et al., 2009), and a question is whether similar efficiencies can be garnered by sampling-based algorithms that maintain many desirable theoretical properties for Bayesian inference.", "startOffset": 76, "endOffset": 124}, {"referenceID": 0, "context": "One attempt at applying such methods in a sampling context is the recently proposed stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011; Ahn et al., 2012; Patterson & Teh, 2013).", "startOffset": 129, "endOffset": 191}, {"referenceID": 7, "context": "The efficiency of this MH step could potentially be improved using the recent results of (Korattikara et al., 2014; Bardenet et al., 2014).", "startOffset": 89, "endOffset": 138}, {"referenceID": 1, "context": "The efficiency of this MH step could potentially be improved using the recent results of (Korattikara et al., 2014; Bardenet et al., 2014).", "startOffset": 89, "endOffset": 138}, {"referenceID": 2, "context": "(2) Hamiltonian (Hybrid) Monte Carlo (HMC) (Duane et al., 1987; Neal, 2010) provides a method for proposing samples of \u03b8 in a Metropolis-Hastings (MH) framework that efficiently explores the state space as compared to standard random-walk proposals.", "startOffset": 43, "endOffset": 75}, {"referenceID": 8, "context": "(2) Hamiltonian (Hybrid) Monte Carlo (HMC) (Duane et al., 1987; Neal, 2010) provides a method for proposing samples of \u03b8 in a Metropolis-Hastings (MH) framework that efficiently explores the state space as compared to standard random-walk proposals.", "startOffset": 43, "endOffset": 75}, {"referenceID": 8, "context": "(4) concrete, a common analogy in 2D is as follows (Neal, 2010).", "startOffset": 51, "endOffset": 63}, {"referenceID": 15, "context": "The \u201cNo U-Turn\u201d sampler (Hoffman & Gelman, 2011) and the methods proposed by Wang et al. (2013) allow automatic tuning of the step size, , and number of simulation steps,m.", "startOffset": 77, "endOffset": 96}, {"referenceID": 0, "context": "Empirically, in a wide range of settings, simply considering a minibatch size on the order of hundreds of data points is sufficient for the central limit theorem approximation to be accurate (Ahn et al., 2012).", "startOffset": 191, "endOffset": 209}, {"referenceID": 8, "context": "For the correctness of an MH step (based on the entire dataset), we appeal to the same arguments made for the HMC data-splitting technique of Neal (2010). This approach likewise considers minibatches of data and simulating the (continuous) Hamiltonian dynamics on each batch sequentially.", "startOffset": 142, "endOffset": 154}, {"referenceID": 8, "context": "For the correctness of an MH step (based on the entire dataset), we appeal to the same arguments made for the HMC data-splitting technique of Neal (2010). This approach likewise considers minibatches of data and simulating the (continuous) Hamiltonian dynamics on each batch sequentially. Importantly, Neal (2010) alludes to the fact that the resulting H from the split-data scenario may be far from that of the full-data scenario after simulation, which leads to lower acceptance rates and thereby reduces the apparent computational gains in simulation.", "startOffset": 142, "endOffset": 314}, {"referenceID": 8, "context": "For the correctness of an MH step (based on the entire dataset), we appeal to the same arguments made for the HMC data-splitting technique of Neal (2010). This approach likewise considers minibatches of data and simulating the (continuous) Hamiltonian dynamics on each batch sequentially. Importantly, Neal (2010) alludes to the fact that the resulting H from the split-data scenario may be far from that of the full-data scenario after simulation, which leads to lower acceptance rates and thereby reduces the apparent computational gains in simulation. Empirically, as we demonstrate in Fig. 2, we see that even finite-length simulations from the noisy system can diverge quite substantially from those of the noise-free system. Although the minibatch-based HMC technique considered herein is slightly different from that of Neal (2010), the theory we have developed in Theorem 3.", "startOffset": 142, "endOffset": 839}, {"referenceID": 8, "context": "For the correctness of an MH step (based on the entire dataset), we appeal to the same arguments made for the HMC data-splitting technique of Neal (2010). This approach likewise considers minibatches of data and simulating the (continuous) Hamiltonian dynamics on each batch sequentially. Importantly, Neal (2010) alludes to the fact that the resulting H from the split-data scenario may be far from that of the full-data scenario after simulation, which leads to lower acceptance rates and thereby reduces the apparent computational gains in simulation. Empirically, as we demonstrate in Fig. 2, we see that even finite-length simulations from the noisy system can diverge quite substantially from those of the noise-free system. Although the minibatch-based HMC technique considered herein is slightly different from that of Neal (2010), the theory we have developed in Theorem 3.1 surrounding the high-entropy properties of the resulting invariant distribution of Eq. (7) provides some intuition for the observed deviations in H both in our experiments and those of Neal (2010).", "startOffset": 142, "endOffset": 1081}, {"referenceID": 6, "context": "One possible direction of future research is to consider using the recent results of Korattikara et al. (2014) and Bardenet et al.", "startOffset": 85, "endOffset": 111}, {"referenceID": 1, "context": "(2014) and Bardenet et al. (2014) that show that it is possible to do MH using a subset of data.", "startOffset": 11, "endOffset": 34}, {"referenceID": 12, "context": "(9) can be written in the following decomposed form (Yin & Ao, 2006; Shi et al., 2012)", "startOffset": 52, "endOffset": 86}, {"referenceID": 6, "context": "Our revised momentum update can also be viewed as akin to partial momentum refreshment (Horowitz, 1991; Neal, 1993), which also corresponds to second-order Langevin dynamics.", "startOffset": 87, "endOffset": 115}, {"referenceID": 8, "context": "Such partial momentum refreshment was shown to not greatly improve HMC in the case of noise-free gradients (Neal, 2010).", "startOffset": 107, "endOffset": 119}, {"referenceID": 0, "context": "It is also possible to set B\u0302 = 12 V\u0302 , where V\u0302 is estimated using empirical Fisher information as in (Ahn et al., 2012) for SGLD.", "startOffset": 103, "endOffset": 121}, {"referenceID": 0, "context": "Importantly, we note that our SGHMC time complexity is the same as that of SGLD (Welling & Teh, 2011; Ahn et al., 2012) in both parameter settings.", "startOffset": 80, "endOffset": 119}, {"referenceID": 0, "context": "For a decaying series of step sizes t, an MH step is not required (Welling & Teh, 2011; Ahn et al., 2012)1.", "startOffset": 66, "endOffset": 105}, {"referenceID": 0, "context": "As in (Welling & Teh, 2011; Ahn et al., 2012) for SGLD, we consider using a small, non-zero leading to some bias.", "startOffset": 6, "endOffset": 45}, {"referenceID": 13, "context": "A more sophisticated strategy involves using momentum scheduling (Sutskever et al., 2013).", "startOffset": 65, "endOffset": 89}, {"referenceID": 8, "context": "It is known that a benefit of HMC over many other MCMC algorithms is the efficiency in sampling from correlated distributions (Neal, 2010)\u2014this is where the introduction of the momentum variable shines.", "startOffset": 126, "endOffset": 138}], "year": 2014, "abstractText": "Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system\u2014such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.", "creator": "LaTeX with hyperref package"}}}