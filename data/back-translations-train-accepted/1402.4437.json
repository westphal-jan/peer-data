{"id": "1402.4437", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2014", "title": "Learning the Irreducible Representations of Commutative Lie Groups", "abstract": "We present a new probabilistic model of commutative Lie groups, that produces invariant-equivariant and disentangled representations of data. We borrow a fundamental principle from physics, used to define the elementary particles of a physical system, and use it to give a mathematically precise definition of the popular but heretofore rather vague notion of a \"disentangled representation\". Our model is based on a newfound Bayesian conjugacy relation that enables us to perform fully tractable probabilistic inference over so-called Toroidal Lie groups -- a class that includes practically relevant groups such as rotations and translations of images. We train the model on pairs of transformed image patches, and show that it produces a completely invariant representation which is highly effective for classification.", "histories": [["v1", "Tue, 18 Feb 2014 18:47:41 GMT  (129kb)", "http://arxiv.org/abs/1402.4437v1", null], ["v2", "Sun, 25 May 2014 12:21:34 GMT  (311kb)", "http://arxiv.org/abs/1402.4437v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["taco cohen", "max welling"], "accepted": true, "id": "1402.4437"}, "pdf": {"name": "1402.4437.pdf", "metadata": {"source": "META", "title": "Learning the Irreducible Representations of Commutative Lie Groups", "authors": [], "emails": ["T.S.COHEN@UVA.NL", "M.WELLING@UVA.NL"], "sections": [{"heading": null, "text": "ar Xiv: 140 2.44 37v1 [cs.LG] 1 8Fe b20 14"}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "1.1. Related work", "text": "The first to propose an algorithm for learning Lie groups from data was Rao & Ruderman (1999), later expanded by Miao and Rao (2007).The first model deals only with one-parameter Lie groups, while the later work included several types of transformation, which left open the problem of efficient derivation of transformation parameters for non-infinitesimal transformations.This problem was solved by Sohl-Dickstein et al. (2010) with an elegant adaptive smoothing technique that allows learning from large transformations.Other, non-group theoretical approaches to learning transformations and invariant representations exist that do something similar to the irreducible reduction of a toroidal group, but this was not recognized as a general principle for untangling that is applicable to other groups as well."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Equivalence, Invariance and Reducibility", "text": "In this section, we will discuss three basic concepts on which the analysis in the rest of this paper is based: Q = general Q function: equivalence, invariance, and reducibility. Consider a function \u03a6: RD \u2192 X that assigns a class name to every possible data point x-RD (X = {1,.., L} as a discrete set of labels in this case) or any distributed representation (e.g. X = RL). Such a function induces an equivalence relationship to the input space RD: we say that two vectors x, y, RD, are an equivalent when mapped to the same representation. Symbolically, the x-RD group is considered as a kind of representation space (y). Each equivalence relationship to the input space completely determines a symmetry group that acts on space. This group, we call G, contains all transformations: RD \u2192 RD that prove to be immutable."}, {"heading": "2.2. Maximal Tori in the Orthogonal Group", "text": "In fact it is so that it is a way in which the real world of the real world and the real world of the real world is turned away. (...) The real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world, of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of"}, {"heading": "3. Toroidal Subgroup Analysis", "text": "A data pair (x, y) is related to a transformation Q = WR (3) WT from the group representation: y = WR (2) WTx + WTx, 2). We will find it useful to introduce some notations for indexing invariant subspaces. As before, Wj = (W (:, 2j \u2212 1), W (:, 2j \u2212 1)). Let us use uj = WTj x and vj = W T y, if we want to access one of the coordinates of u or v, we write uj1 = W T (:, 2j \u2212 1) x or uj2 = W T (:, 2j = W T)."}, {"heading": "3.1. Invariant Representation and Metric", "text": "One way to do this is to use an invariant metric known as the manifold distance, which is defined as the minimum distance between orbits x (x, y) = 0 x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.2. Relation to the Discrete Fourier Transform", "text": "We show that the DFT is a special case of TSA. DFT of a discrete 1D signal x = (x1,.., xD) T is defined: Xj = D \u2212 1 \u2211 n = 0xn\u03c1 \u2212 jn (14), where \u03c1 = e2\u03c0i / D is the D-th primitive root of the unit. If we choose a base of sinusoids for the filters in W, W (:, 2j \u2212 1) = R (\u03c1 \u2212 j,..., \u03c1 \u2212 j (D \u2212 1))) T = (cos (2\u03c0j / D),..., cos (2\u03c0j (D \u2212 1) / D))))) TW (:, 2j) = I (\u03c1 \u2212 j (D \u2212 1)))) T = (sin (\u2212 2\u03c0j / D) e), sin (\u2212 2\u03c0j (D \u2212 1) / D) T, (15) then the change of the posteristic form of jerij."}, {"heading": "3.3. Modeling a Lie subalgebra", "text": "Typically one is interested in study groups with less than J degrees of freedom. Since the stabilizing representation is invariant for all transformations from the maximum torus containing the symmetry group of interest, it is certainly also invariant against the subgroup of the maximum torus which is the true symmetry group. However, this representation will identify vectors that are actually considered from the standpoint of the true symmetry group. We model this using a coupled rotation matrix as follows: Q (s) = W R (1 s).. R (Js) WT (16) Where s [0, 2] is the scalar parameter of this subgroup. The probability then becomes y (y) N (s)."}, {"heading": "3.4. Maximum Marginal Likelihood Learning", "text": "We train the model on the basis of the gradient parentage on the marginal probability. Perhaps surprisingly, given the nonlinearity in the model, the integrations required to evaluate the marginal probability can be obtained in a closed form for both the coupled and the decoupled models. For the decoupled model, we obtain: p (y | x) = area value (y | TJ N (y | WR (n) WTx). (19) If we consider that I0 (n) / I0 (n) column (n) is the ratio of the normalization constants of the regular von-Mises distributions, the analog expression for the coupled model is easy to recognize."}, {"heading": "4. Experiments", "text": "We trained a TSA model with 100 filters on a current of 250,000 16 x 16 fields x (t), y (t).The fields x (t) were drawn from a normal standard distribution, and y (t) was obtained by rotating x (t) by an angle randomly drawn from [0, 2\u03c0].The learning rate \u03b1 was initialized at \u03b10 = 0.25 and decayed as \u03b1 = \u03b10 / 270 T, with T increased by one with each pass through the data.Each minibatch consisted of 100 data pairs. After learning W, we estimate the weights \u03c9j and sort the filter pairs by increasing the absolute value for visualization. As shown in Fig. 3, the filters are very clean and the weights are correctly estimated, with the exception of some filters on rows 1 and 2, which are assigned the weight 0 if they actually have a higher frequency."}, {"heading": "5. Conclusions and outlook", "text": "We have presented a novel principle for learning unbundled representations and calculated its consequences for a simple type of symmetry group, resulting in a fully tractable and very elegant model with potential applications for invariant classification and Bayesian estimation of motion. The model reproduces the pooling operations of winding networks from a probabilistic and lie-group perspective and provides a probabilistic interpretation of DFT and its generalizations. The nature of the unbundling achieved in this paper depends on the rather minimalist assumption that everything that can be said about images is equivalent (rotated copies) or unequal. However, the universal nature of the Weyl principle bodes well for future applications in deep, non-linear and non-commutative unbundling."}], "references": [{"title": "Representation Learning: A Review and New Perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Unsupervised learning of a steerable basis for invariant image representations", "author": ["Bethge", "Matthias", "Gerwinn", "Sebastian", "Macke", "Jakob H"], "venue": "Proceedings of SPIE Human Vision and Electronic Imaging XII (EI105),", "citeRegEx": "Bethge et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bethge et al\\.", "year": 2007}, {"title": "Learning intermediate-level representations of form and motion from natural movies", "author": ["Cadieu", "Charles F", "Olshausen", "Bruno A"], "venue": "Neural computation,", "citeRegEx": "Cadieu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cadieu et al\\.", "year": 2012}, {"title": "A Note on the Theory of n-Variable Generalized Bessel Functions", "author": ["G. Dattoli", "C. Chiccoli", "S. Lorenzutta", "G. Maino", "M. Richetta", "A. Torre"], "venue": "Il Nuovo Cimento B,", "citeRegEx": "Dattoli et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Dattoli et al\\.", "year": 1991}, {"title": "The generalized von Mises distribution", "author": ["Gatto", "Riccardo", "Jammalamadaka", "SR"], "venue": "Statistical Methodology,", "citeRegEx": "Gatto et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gatto et al\\.", "year": 2007}, {"title": "Measuring invariances in deep networks", "author": ["I Goodfellow", "Q Le", "A. Saxe"], "venue": "Advances in Neural Information Processing Systems, pp", "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "Group Theoretical Methods in Image Understanding", "author": ["Kanatani", "Kenichi"], "venue": null, "citeRegEx": "Kanatani and Kenichi.,? \\Q1990\\E", "shortCiteRegEx": "Kanatani and Kenichi.", "year": 1990}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L. Bottou"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun and Bottou,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Bottou", "year": 1998}, {"title": "On multi-view feature learning", "author": ["Memisevic", "Roland"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Memisevic and Roland.,? \\Q2012\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2012}, {"title": "Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines", "author": ["Memisevic", "Roland", "Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Memisevic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2010}, {"title": "Learning the Lie groups of visual invariance", "author": ["Miao", "Xu", "Rao", "Rajesh P N"], "venue": "Neural computation,", "citeRegEx": "Miao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2007}, {"title": "Learning Lie groups for invariant visual perception", "author": ["RPN Rao", "Ruderman", "DL"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Rao et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Rao et al\\.", "year": 1999}, {"title": "Transformation invariance in pattern recognition: Tangent distance and propagation", "author": ["Simard", "Patrice Y", "Le Cun", "Yann a", "Denker", "John S", "Victorri", "Bernard"], "venue": "International Journal of Imaging Systems and Technology,", "citeRegEx": "Simard et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2000}, {"title": "Actionable information in vision", "author": ["Soatto", "Stefano"], "venue": "IEEE 12th International Conference on Computer Vision, pp. 2138\u20132145", "citeRegEx": "Soatto and Stefano.,? \\Q2009\\E", "shortCiteRegEx": "Soatto and Stefano.", "year": 2009}, {"title": "An unsupervised algorithm for learning lie group transformations", "author": ["J Sohl-Dickstein", "JC Wang", "Olshausen", "BA"], "venue": "arXiv preprint,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Exponential Family Harmoniums with an Application to Information Retrieval", "author": ["Welling", "Max", "Rosen-zvi", "Michal", "Hinton", "Geoffrey"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Welling et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 16, "context": "The hallmark of the deep learning approach is to learn multiple layers of representation of data, and much work has gone into the development of representation learning modules such as RBMs and their generalizations (Welling et al., 2005), and autoencoders (Vincent et al.", "startOffset": 216, "endOffset": 238}, {"referenceID": 15, "context": ", 2005), and autoencoders (Vincent et al., 2008).", "startOffset": 26, "endOffset": 48}, {"referenceID": 5, "context": "Various desiderata for learned representations have been expressed, including meaningful (Bengio & Lecun, 2014), invariant (Goodfellow et al., 2009), abstract and disentangled (Bengio et al.", "startOffset": 123, "endOffset": 148}, {"referenceID": 0, "context": ", 2009), abstract and disentangled (Bengio et al., 2013) representations, but so far most of these notions have not been defined in a mathematically precise way.", "startOffset": 35, "endOffset": 56}, {"referenceID": 14, "context": "This problem was solved by Sohl-Dickstein et al. (2010) using an elegant adaptive smoothing technique, making it possible to learn from large transformations.", "startOffset": 27, "endOffset": 56}, {"referenceID": 1, "context": "Therefore, orthogonal matrices cannot express transformations such as contrast scaling, but they can still model the interesting structural changes in images (Bethge et al., 2007).", "startOffset": 158, "endOffset": 179}, {"referenceID": 14, "context": "As is well known, commuting matrices can be simultaneously diagonalized, so one could represent a toroidal group in terms of a basis of eigenvectors shared by every element in the group, and one diagonal matrix of eigenvalues for each element of the group, as was done in (Sohl-Dickstein et al., 2010) for 1-parameter Lie groups.", "startOffset": 272, "endOffset": 301}, {"referenceID": 14, "context": "Previous approaches to Lie group learning only provide point estimates of the transformation parameters, which have to be obtained using an expensive iterative optimization procedure (Sohl-Dickstein et al., 2010).", "startOffset": 183, "endOffset": 212}, {"referenceID": 14, "context": "Previous approaches to Lie group learning only provide point estimates of the transformation parameters, which have to be obtained using an expensive iterative optimization procedure (Sohl-Dickstein et al., 2010). In contrast, TSA provides a full tractable posterior distribution which is obtained using a simple feed-forward computation. Compared to the work of Cadieu & Olshausen (2012), our model deals well with low-energy subspaces, by simply describing the uncertainty in the estimate instead of providing inaccurate estimates that have to be discarded.", "startOffset": 184, "endOffset": 389}, {"referenceID": 12, "context": "In practice, it has proven difficult to compute this distance exactly, so approximations such as tangent distance have been invented (Simard et al., 2000).", "startOffset": 133, "endOffset": 154}, {"referenceID": 3, "context": ", exp (\u2212i\u03bc+K) (Dattoli et al., 1991): Z(\u03ba, \u03bc) = 2\u03c0I0(\u03ba ; e + ).", "startOffset": 14, "endOffset": 36}, {"referenceID": 14, "context": "If required, accurate MAP inference can be performed using the algorithm of Sohl-Dickstein et al. (2010), as described in the supplementary material.", "startOffset": 76, "endOffset": 105}, {"referenceID": 3, "context": "The gradient of the coupled model is easily computed using the differential recurrence relations that hold for the GBF (Dattoli et al., 1991).", "startOffset": 119, "endOffset": 141}, {"referenceID": 12, "context": "We compared the Euclidean distance (ED) in pixel space, tangent distance (TD) (Simard et al., 2000), Euclidean distance on the space of \u221a \u03ba\u0302 (equivalent to the exact manifold distance for the maximal torus, see section 3.", "startOffset": 78, "endOffset": 99}], "year": 2017, "abstractText": "We present a new probabilistic model of commutative Lie groups, that produces invariantequivariant and disentangled representations of data. We borrow a fundamental principle from physics, used to define the elementary particles of a physical system, and use it to give a mathematically precise definition of the popular but heretofore rather vague notion of a \u201cdisentangled representation\u201d. Our model is based on a newfound Bayesian conjugacy relation that enables us to perform fully tractable probabilistic inference over so-called Toroidal Lie groups \u2013 a class that includes practically relevant groups such as rotations and translations of images. We train the model on pairs of transformed image patches, and show that it produces a completely invariant representation which is highly effective for classification.", "creator": "LaTeX with hyperref package"}}}