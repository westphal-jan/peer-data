{"id": "1402.4746", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2014", "title": "Near-Optimal-Sample Estimators for Spherical Gaussian Mixtures", "abstract": "Statistical and machine-learning algorithms are frequently applied to high-dimensional data. In many of these applications data is scarce, and often much more costly than computation time. We provide the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures.", "histories": [["v1", "Wed, 19 Feb 2014 17:59:55 GMT  (61kb)", "http://arxiv.org/abs/1402.4746v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IT math.IT stat.ML", "authors": ["ananda theertha suresh", "alon orlitsky", "jayadev acharya", "ashkan jafarpour"], "accepted": true, "id": "1402.4746"}, "pdf": {"name": "1402.4746.pdf", "metadata": {"source": "CRF", "title": "Near-optimal-sample estimators for spherical Gaussian mixtures", "authors": ["Jayadev Acharya", "Ashkan Jafarpour"], "emails": ["jacharya@ucsd.edu", "ashkan@ucsd.edu", "alon@ucsd.edu", "asuresh@ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 2.47 46v1 [cs.LG] 1 9Fe bFor mixtures of all k d-dimensional spherical Gaussian Gaussians, we derive an intuitive spectral estimator that uses Ok (d log2 d\u04454) samples and runs in time Ok (d3 log5 d), both of which are significantly lower than previously known. Constant factor Ok is polynomial for sample complexity and exponential for time complexity, again much smaller than previously known. We also show that for each algorithm samples of k (d\u041a2) are required. Therefore, sample complexity is nearly optimal in terms of the number of dimensions. We also derive a simple estimator for one-dimensional k component mixtures that uses O (k log k\u041a2) samples and runs in time O ((k\u0442) 3k + 1)."}, {"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Background", "text": "Significant information is often located in high-dimensional spaces: voice signals are expressed in many > frequency bands, credit ratings are influenced by several parameters, and document issues manifest themselves in the prevalence of numerous words. Some applications, such as subject modeling and genomic analysis, consider data in more than 1000 dimensions, [17, 44].Typically, information can be generated from various types of sources: voice is spoken by men or women, credit parameters correspond to rich or poor individuals, and documents address issues such as sports or politics. In such cases, the aggregate data follows a mix distribution [26, 36, 38].Mixtures of high-dimensional distributions are therefore central to understanding and processing many natural phenomena. Methods for recovering the mixing components from the data have therefore been extensively studied by statisticians, engineers, and computer scientists. Initially, heuristic methods such as expectation maximization (EM) have been developed."}, {"heading": "1.2 Sample complexity", "text": "Reducing the number of samples is of great practical importance. For example, in subject modeling each sample is an entire document, in credit analysis each sample is a person's credit history, and in genetics each sample is a human DNA. Therefore, samples can be very scarce and their collection can be very expensive. In contrast, current CPUs run at several gigahertz, so samples are typically much scarcer of a resource than time. Note that for one-dimensional statistical problems the need for sample-efficient algorithms has been widely recognized. Sample complexity of many problems is fairly well known, often within a constant factor. For example, for discrete distributions over {1,.., s}, an approach proposed in [32] and its changes have been used in [40, 41] to assess the probability complexity of these problems using (s / log s) samples.Learning one-dimensional m-modal distributions over {1, 41]."}, {"heading": "1.3 Previous and new results", "text": "Our main contribution is PAC learning d-dimensional Gaussian mixtures with almost linear samples. We show few auxiliary results for one-dimensional Gaussians. 1.3.1 d-dimensional Gaussian mixtures. They showed that this class requires PAC learning of discrete and Gaussian product mixtures. [20] Mixtures of two-dimensional Bernoulli products where all probabilities are delimited from 0 are taken into account. [18] Taking into account the probability constraints and generalizing the results of binary to arbitrary discrete alphabets and from 2 to k mixture components are taken into account. They showed that mixtures of discrete products are PAC learnable in O-notation logical factors. [18] Taking into account the probability constraints and generalizing the results of causable discrete alphabets and from 2 to k mixture components."}, {"heading": "1.3.2 One-dimensional Gaussian mixtures", "text": "Independently, and at approximately the same time as this paper [15], we showed that mixtures of two one-dimensional Gaussian samples can be learned from O samples and in time O samples. We provide a natural estimator to learn mixtures of k one-dimensional Gaussian samples, using some basic properties of Gaussian distributions, and show that mixtures of any k-one-dimensional Gaussian samples with O samples and in time O samples (kB samples) 3k + 1 can be learned."}, {"heading": "1.4 The approach and technical contributions", "text": "rE \"s tis rf\u00fc ide r\u00fc the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f"}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "For arbitrary product distributions p1,..., pk over a d-dimensional space be pj, I am the distribution of pj over the coordinate i, and let \u00b5j, i and \u03c3j be the mean and the variance of pj, i. Let f = (w1,.., wk, p1,..., pk) be the mixture of these distributions with the mixed weights w1,..., wk. We denote estimates of a quantity x by x. It can be an empirical mean or a more complex estimate."}, {"heading": "2.2 Selection from a pool of distributions", "text": "Many algorithms for learning mixtures within domain X first obtain a small collection of mixtures of the distribution F and then use the samples to test the maximum probability to produce a distribution [14,18,20]. Our algorithm also obtains a series of distributions containing at least one that is close to the underlying value at distance 1. The estimation problem is now reduced to the following: For a class F of distributions and samples of an unknown distribution f, we find a distribution in F that is close to f. Leave D (f, F) def = minfi - F D (f, fi).The well-known Scheffe method [16] uses O (2 log - F) samples from the underlying distribution f \u2212 and in time O (2 - F) samples from the underlying distribution f."}, {"heading": "2.3 Lower bound", "text": "Using the inequality of Fano, we show an information-theoretical lower limit of the samples to learn d-dimensional mixtures of spherical Gaussian components for any algorithm. More precisely, theorem 2 (appendix C). Any algorithm that learns all d-dimensional d-dimensional Gaussian mixtures with k components up to a distance of 0 \u00b0 1 with a probability of 1 / 2 requires at least 2 samples."}, {"heading": "3 Mixtures in d dimensions", "text": "To gain clarity, we assume that all components have the same variance, i.e. that the simple part of the algorithm has the same number of samples for 1 \u2264 i \u2264 k. Modifying this algorithm works for components with different deviations. The simple component of the algorithm is the estimation of 2 \u00b0 C. If X (1) and X (2) are two samples from the same component, then X (1) \u2212 X (2) is distributed and we include it in the final version of the paper. The simple part of the algorithm is the estimation of 2 \u00b0. If X (1) and X (2) are two samples from the same component, then X (1) \u2212 X."}, {"heading": "3.2 Sketch of correctness", "text": "To simplify the boundaries and expressions, let us assume that d > 1000 + + + + + + + 3 > min (2n2e \u2212 d / 10.1 / 3) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 +"}, {"heading": "4 Mixtures in one dimension", "text": "In the last ten years, the estimation of one-dimensional distributions has received significant attention [1, 13-15, 30, 31, 33, 41]. We now provide a simple estimator to calculate the one-dimensional distributions using the MODIFIED SCHEFFE estimator. The d-dimensional estimator uses spectral forecasts to find the range of averages, whereas we use a simple observation of the properties of samples from Gaussian to estimate. Formally, one looks at samples from f, a mixture of Gaussian distributions pi def = N (\u00b5i, \u03c32i) with weights w1, w2, wk, our goal is to find a mixture f = (w)."}, {"heading": "5 Acknowledgements", "text": "We thank Sanjoy Dasgupta, Todd Kemp and Krishnamurthy Vishwanathan for helpful discussions."}, {"heading": "A Useful tools", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Bounds on \u21131 distance", "text": "For two-dimensional product distributions p1 and p2 = two-dimensional product distributions p1 and p2, if we bind the distances on each coordinate by (previous), then by triangular unevenness D (p1, p2) \u2264 d2. However, this boundary is often weak. One way to achieve a stronger boundary is to relate the distance to Bhattacharyya as follows: Bhattacharyya parameters B (p1, p2) \u2264 d2. * x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "A.2 Concentration inequalities", "text": "We use the following concentration inequalities for Gaussian, chi-square and the sum of Bernoulli random variables in the rest of the paper. Lemma 15. For a Gaussian random variable X with average \u00b5 and variance \u03c32, Pr (EX \u2212 EX) \u2264 e \u2212 t2 / 2. Lemma 16 ([25]) If Y1, Y2,.. Yn n n n i.e. Gaussian variables with average \u00b5 and variance \u0445 2, thenPr (EX \u2212 EY = 1 Y 2i \u2212 n2 (EY + T) \u04452) \u2264 e \u2212 t, and Pr (EX \u2212 EY 2i \u2212 n\u03c32 \u2264 \u2212 EY) \u2264 e \u2212 t. Furthermore, for a fixed vector a, Pr (EX \u2212 EY + EY) \u04322 ai (Y 2i \u2212 EX \u2212 EY \u2212 EY 2i \u2212 EY \u2212 n \u2264 \u2212 EY 2i \u2212 EY \u2212 EY \u2212 EY \u2212 EY)."}, {"heading": "A.3 Matrix eigenvalues", "text": "Let us now give some simple numbers to the eigenvalues of the disturbed matrices.Lemma 19: Let us consider the eigenvalues of two symmetrical matrices A and B. (Let us consider the eigenvalues of A and B.) Let us consider the eigenvalues of A and B. (Let us consider the eigenvalues of A and B.) Let us consider the eigenvalues of A and B. (Let us consider the eigenvalues of A and B. (Let us consider the eigenvalues of A and B.) Let us consider the eigenvalues of B. (Let us consider the eigenvalues of A and B.) Let us consider the eigenvalues of B."}, {"heading": "B Selection from a set of candidate distributions", "text": "Given an unclear distribution f, the goal is to achieve a distribution of a known collection of distributions. (F, F). (F, F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F. (F). (F). (F). (F. (F). (F). (F). (F. (F). (F). (F. (F). (F). (F. (F). (F. (F). (F. (F). (F). (F). (F.). (F. (F.). (F. (F). (F. (F). (F). (F. (F). (F). (F.). (F. (F. (F). (F). (F.). (F. (F. (F.). (F). (F. (F.). (F.). (F. (F.). (F). (F. (F"}, {"heading": "C Lower bound", "text": "We first show a lower limit for a single Gaussian distribution and generalize it to mixtures."}, {"heading": "C.1 Single Gaussian distribution", "text": "The proof is an application of the following version of Fano's inequality [9, 45]. It states that we cannot simultaneously estimate all distributions in a class from n samples if they meet certain conditions.Lemma 22. (Fano's inequality) Let f1,... + 1 be a collection of distributions so that for each i-j, D (fi, fj) \u2265 \u03b1, and KL (fi, fj) \u2264 \u03b2. Let f be an estimate of the underlying distribution from n i.i.d. samples from one of the fi's. Then sup i E [fi, f)] \u2265 2 (1 \u2212 n\u03b2 + log 2 log r).We consider d \u2212 dimensional spherical Gaussians with identity covariance matrix, with averages along each coordinate limited to \u00b1 c\u0430 \u00b2 s minimum distance. The KL divergence between two spherical Gaussians with identity covariants."}, {"heading": "C.2 Mixtures of k Gaussians", "text": "We extend the construction for a single spherical Gaussian system to prove the lower limit for the lower limit of the spherical Gaussians, and show a lower limit for the lower limit of the spherical Gaussians, we designed a class of 2d / 8 distributions around the origin. Let P def = {P1,.., PT}, where T = 2d / 8, be this class. Recall that each Pi is a spherical Gaussians with unit variance. For a distribution P def = {P1,.,., PT}, where T = 2d / 8, be this class. Recall that each Pi is a spherical Gaussian with unit variance. For a distribution P over Rd, let P + \u00b5 be the distribution P +."}, {"heading": "D Proofs for k spherical Gaussians", "text": "Lemma 23. Samples are given from a series of Gaussian distributions, with the probability \u2265 1 \u2212 2\u043c, for each pair of samples X \u00b2 N (\u00b51, \u03c32Id) and Y \u00b2 N (\u00b52, \u03c32Id), as well as for each pair of samples X \u00b2 2d\u03c32 + 4\u03c32 \u00b0 d log n2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (2). We prove that the proof for the upper boundary is similar and omitted. Since X and Y \u2212 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (2) is the lower boundary (2), the proof for the upper boundary is \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (2) and the proof for the boundary \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (2)."}, {"heading": "D.1 Proof of Lemma 4", "text": "We show that if the equations (1) and (2) are fulfilled, the problem exists. The error probability is that of Lemma 23 and is \u2264 2\u03b4. Since the minimum is above k + 1 indices, at least two samples originate from the same component. Application of equations (1) and (2) for these two samples 2d\u03c3 2 \u2264 2d2 + 4\u03c32 \u221a d log n2\u03b4 + 4\u03c32 log n2 \u03b4. Likewise, by equations (1) and (2) for any two samples X (a), X (b) in [k + 1], and between X (a) \u2212 X (b)."}, {"heading": "D.2 Proof of Lemma 5", "text": "We show that if the equations (1) and (2) are fulfilled, the problem exists. The error probability is that of Lemma 23 and is \u2264 2\u043c. Since the equations (1) and (2) are fulfilled by the detection of Lemma 4, the error probability is higher than that of Lemma 4 \u2212 X (b). If two samples X (a) and X (b) originate from the same component, the above-mentioned quantity according to Lemma is 23 \u2212 X (a) \u2212 X (b). If two samples X (a) \u2212 X (a) \u2212 X \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}, {"heading": "D.3 Proof of Lemma 6", "text": "The detection is involved and we show it in steps. We first show some few concentration limits, which we use later to argue that the samples are clusterable if the sample covariance matrix has a large eigenvalue. First, let's show an empirical average of the samples from pi. Let's show the empirical average of the samples in cluster C. If C is the entire set of samples we use instead (C). We first show a concentration inequality, which we use in the remaining calculations. Lemma 24. Give n samples from a k component Gaussian mixture with a probability of 1 to 2p, for each component i."}, {"heading": "D.4 Proof of Lemma 7", "text": "(5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) (5) (5 (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) (5) (5 (5) (5) (5 (5) (5) (5) (5) (5) (5 (5) (5) (5 (5) (5) (5) (5) (5) (5) (5) (5 (5) (5 (5) (5) (5) ("}, {"heading": "D.5 Proof of Theorem 8", "text": "We show that the theorem is valid if the conclusions in Lemmas 7 and 26 are most likely correct."}], "references": [{"title": "Optimal probability estimation with applications to prediction and classification", "author": ["Jayadev Acharya", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "On spectral learning of mixtures of distributions", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory (COLT),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Strong converse for identification via quantum channels", "author": ["Rudolf Ahlswede", "Andreas Winter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures", "author": ["Joseph Anderson", "Mikhail Belkin", "Navin Goyal", "Luis Rademacher", "James R. Voss"], "venue": "CoRR, abs/1311.2891,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Minimax theory for high-dimensional gaussian mixtures with sparse mean separation", "author": ["Martin Azizyan", "Aarti Singh", "Larry A. Wasserman"], "venue": "CoRR, abs/1306.2035,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In Proceedings of the 51st Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Learning mixtures of gaussians using the k-means algorithm", "author": ["Kamalika Chaudhuri", "Sanjoy Dasgupta", "Andrea Vattani"], "venue": "CoRR, abs/0912.0086,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Image segmentation by clustering", "author": ["G.B. Coleman", "Harry C. Andrews"], "venue": "Proceedings of the IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1979}, {"title": "Elements of information theory (2", "author": ["Thomas M. Cover", "Joy A. Thomas"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In Proceedings of the 40th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "A two-round variant of EM for gaussian mixtures", "author": ["Sanjoy Dasgupta", "Leonard J. Schulman"], "venue": "In Proceedings of the 16th Annual Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard J. Schulman"], "venue": "Journal on Machine Learning Research (JMLR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Learning k-modal distributions via testing", "author": ["Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A. Servedio"], "venue": "In SODA,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Learning poisson binomial distributions", "author": ["Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A. Servedio"], "venue": "In Proceedings of the 44th Annual Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Faster and sample near-optimal algorithms for proper learning mixtures of gaussians", "author": ["Constantinos Daskalakis", "Gautam Kamath"], "venue": "CoRR, abs/1312.1054,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Combinatorial methods in density estimation", "author": ["Luc Devroye", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Iterative clustering of high dimensional text data augmented by local search", "author": ["Inderjit S. Dhillon", "Yuqiang Guan", "Jacob Kogan"], "venue": "In Proceedings of the 2nd Industrial Conference on Data Mining (ICDM),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Learning mixtures of product distributions over discrete domains", "author": ["Jon Feldman", "Ryan O\u2019Donnell", "Rocco A. Servedio"], "venue": "In Proceedings of the 46th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "PAC learning axis-aligned mixtures of gaussians with no separation assumption", "author": ["Jon Feldman", "Rocco A. Servedio", "Ryan O\u2019Donnell"], "venue": "In Proceedings of the 19th Annual Conference on Learning Theory (COLT),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Estimating a mixture of two product distributions", "author": ["Yoav Freund", "Yishay Mansour"], "venue": "In Proceedings of the 13th Annual Conference on Learning Theory (COLT),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M. Kakade"], "venue": "In Proceedings of the 4th Innovations in Theoretical Computer Science Conference (ITCS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "In Proceedings of the 42nd Annual Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "SIAM Journal on Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "On the learnability of discrete distributions", "author": ["Michael J. Kearns", "Yishay Mansour", "Dana Ron", "Ronitt Rubinfeld", "Robert E. Schapire", "Linda Sellie"], "venue": "In Proceedings of the 26th Annual Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["B. Laurent", "Pascal Massart"], "venue": "The Annals of Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Mixture Models: Theory, Geometry and Applications. NSF-CBMS Conference series in Probability and Statistics, Penn", "author": ["Bruce G. Lindsay"], "venue": "State University,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "Asymptotic convergence rate of the em algorithm for gaussian mixtures", "author": ["Jinwen Ma", "Lei Xu", "Michael I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In Proceedings of the 51st Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Efficient density estimation via piecewise polynomial approximation", "author": ["Siu on Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "venue": "CoRR, abs/1305.3207,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Learning mixtures of structured distributions over discrete domains", "author": ["Siu on Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "venue": "In Proceedings of the 24th Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "On modeling profiles instead of values", "author": ["Alon Orlitsky", "Narayana P. Santhanam", "Krishnamurthy Viswanathan", "Junan Zhang"], "venue": "In Proceedings of the 20th Annual Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Variational minimax estimation of discrete distributions under kl loss", "author": ["Liam Paninski"], "venue": "In Proceedings of the 18th Annual Conference on Neural Information Processing (NIPS),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Mixture densities, maximum likelihood and the em algorithm", "author": ["Richard A. Redner", "Homer F. Walker"], "venue": "SIAM Review,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1984}, {"title": "Robust text-independent speaker identification using gaussian mixture speaker models", "author": ["Douglas A. Reynolds", "Richard C. Rose"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1995}, {"title": "Slink: An optimally efficient algorithm for the single-link cluster method", "author": ["Robin Sibson"], "venue": "The Computer Journal,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1973}, {"title": "Statistical analysis of finite mixture distributions, volume 7", "author": ["D Michael Titterington", "Adrian FM Smith", "Udi E Makov"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1985}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts", "author": ["G. Valiant", "P. Valiant"], "venue": "Proceedings of the 43rd Annual Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Estimating the unseen: A sublinear-sample canonical estimator of distributions", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "A spectral algorithm for learning mixtures of distributions", "author": ["Santosh Vempala", "Grant Wang"], "venue": "In Proceedings of the 43rd Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "CoRR, abs/1011.3027,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Feature selection for high-dimensional genomic microarray data", "author": ["Eric P. Xing", "Michael I. Jordan", "Richard M. Karp"], "venue": "In Proceedings of the 18th Annual International Conference on Machine Learning (ICML),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2001}], "referenceMentions": [{"referenceID": 16, "context": "Some applications, such as topic modeling and genomic analysis consider data in over 1000 dimensions, [17, 44].", "startOffset": 102, "endOffset": 110}, {"referenceID": 41, "context": "Some applications, such as topic modeling and genomic analysis consider data in over 1000 dimensions, [17, 44].", "startOffset": 102, "endOffset": 110}, {"referenceID": 25, "context": "In such cases the overall data follow a mixture distribution [26, 36, 38].", "startOffset": 61, "endOffset": 73}, {"referenceID": 33, "context": "In such cases the overall data follow a mixture distribution [26, 36, 38].", "startOffset": 61, "endOffset": 73}, {"referenceID": 35, "context": "In such cases the overall data follow a mixture distribution [26, 36, 38].", "startOffset": 61, "endOffset": 73}, {"referenceID": 26, "context": "Initially, heuristic methods such as expectation-maximization (EM) were developed [27, 35].", "startOffset": 82, "endOffset": 90}, {"referenceID": 32, "context": "Initially, heuristic methods such as expectation-maximization (EM) were developed [27, 35].", "startOffset": 82, "endOffset": 90}, {"referenceID": 4, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 6, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 10, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 11, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 20, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 39, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 117, "endOffset": 139}, {"referenceID": 1, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 3, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 5, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 9, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 21, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 27, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 159, "endOffset": 180}, {"referenceID": 22, "context": "Over the past decade, more rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaussians [5, 7, 11, 12, 21, 42], general Gaussians [2, 4, 6, 10, 22, 29], and other log-concave distributions [23].", "startOffset": 218, "endOffset": 222}, {"referenceID": 21, "context": "Recently, [22, 29] showed that any d-dimensional Gaussian mixture can be recovered in polynomial time.", "startOffset": 10, "endOffset": 18}, {"referenceID": 27, "context": "Recently, [22, 29] showed that any d-dimensional Gaussian mixture can be recovered in polynomial time.", "startOffset": 10, "endOffset": 18}, {"referenceID": 23, "context": "PAC learning [24] does not approximate each mixture component, but instead derives a mixture distribution that is close to the original one.", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 6, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 10, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 11, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 20, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 39, "context": "An important and extensively studied special case of mixture distributions are spherical-Gaussians [5, 7, 11, 12, 21, 42], where different coordinates have the same variance, though potentially different means.", "startOffset": 99, "endOffset": 121}, {"referenceID": 6, "context": "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].", "startOffset": 175, "endOffset": 190}, {"referenceID": 10, "context": "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].", "startOffset": 175, "endOffset": 190}, {"referenceID": 11, "context": "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].", "startOffset": 175, "endOffset": 190}, {"referenceID": 39, "context": "Due to their simple structure, they are easier to analyze and under a minimum-separation assumption have provably-practical algorithms for clustering and parameter estimation [7, 11, 12, 42].", "startOffset": 175, "endOffset": 190}, {"referenceID": 30, "context": ",s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using \u0398(s/ log s) samples.", "startOffset": 29, "endOffset": 33}, {"referenceID": 37, "context": ",s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using \u0398(s/ log s) samples.", "startOffset": 69, "endOffset": 77}, {"referenceID": 38, "context": ",s}, an approach proposed in [32] and its modifications were used in [40, 41] to estimate the probability multiset using \u0398(s/ log s) samples.", "startOffset": 69, "endOffset": 77}, {"referenceID": 13, "context": ",s} requires \u0398(m log(s/m)/\u01eb3) samples [14].", "startOffset": 38, "endOffset": 42}, {"referenceID": 29, "context": ",s} can be learned with O(k/\u01eb4), O(k log(s/\u01eb)/\u01eb4), and O(k log(s)/\u01eb4) samples, respectively, and these bounds are tight up to a factor of \u01eb [31].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "For example, for learning spherical Gaussian mixtures, the number of samples required by previous algorithms is O(d12) for k = 2 components, and increased exponentially with k [19].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "[20] considered mixtures of two d-dimensional Bernoulli products where all probabilities are bounded away from 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] eliminated the probability constraints and generalized the results from binary to arbitrary discrete alphabets, and from 2 to k mixture components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] generalized these results to Gaussian products, showing in particular that mixtures of k Gaussians, where the difference between the means normalized by the ratio of standard deviations is bounded by B, are PAC learnable in \u00d5((dB/\u01eb)2k(k+1)) time, and can be shown to use \u00d5((dB/\u01eb)4(k+1)) samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Observe that recent algorithms typically construct the covariance matrix [19,42], hence require \u2265 nd time.", "startOffset": 73, "endOffset": 80}, {"referenceID": 39, "context": "Observe that recent algorithms typically construct the covariance matrix [19,42], hence require \u2265 nd time.", "startOffset": 73, "endOffset": 80}, {"referenceID": 14, "context": "2 One-dimensional Gaussian mixtures Independently and around the same time as this work [15] showed that mixtures of two one-dimensional Gaussians can be learnt with \u00d5(\u01eb\u22122) samples and in time O(\u01eb\u22127.", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "4 The approach and technical contributions The popular SCHEFFE estimator takes a collection F of distributions and uses O(log \u2223F\u2223) independent samples from an underlying distribution f to find a distribution in F whose distance from f is at most a constant factor larger than that of the distribution in F that is closet to f [16].", "startOffset": 326, "endOffset": 330}, {"referenceID": 17, "context": "[18, 19] constructs the sample correlation matrix and uses k of its columns to approximate the span of mean vectors.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[18, 19] constructs the sample correlation matrix and uses k of its columns to approximate the span of mean vectors.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "Using recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in \u00d5(d) samples.", "startOffset": 60, "endOffset": 71}, {"referenceID": 36, "context": "Using recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in \u00d5(d) samples.", "startOffset": 60, "endOffset": 71}, {"referenceID": 40, "context": "Using recent tools from non-asymptotic random matrix theory [3, 39, 43], we show that the approximation of the span of the means converges in \u00d5(d) samples.", "startOffset": 60, "endOffset": 71}, {"referenceID": 13, "context": "2 Selection from a pool of distributions Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20].", "startOffset": 241, "endOffset": 251}, {"referenceID": 17, "context": "2 Selection from a pool of distributions Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20].", "startOffset": 241, "endOffset": 251}, {"referenceID": 19, "context": "2 Selection from a pool of distributions Many algorithms for learning mixtures over the domain X first obtain a small collection of mixtures distributions F and then perform Maximum Likelihood test using the samples to output a distribution [14,18,20].", "startOffset": 241, "endOffset": 251}, {"referenceID": 15, "context": "The well-known Scheffe\u2019s method [16] uses O(\u01eb\u22122 log \u2223F\u2223) samples from the underlying distribution f , and in time O(\u01eb\u22122\u2223F\u22232T log \u2223F\u2223) outputs a distribution in F with l1 distance of at most 9.", "startOffset": 32, "endOffset": 36}, {"referenceID": 39, "context": "One of the natural and well-used methods to estimate the span of mean vectors is using the correlation matrix [42].", "startOffset": 110, "endOffset": 114}, {"referenceID": 1, "context": "Even though spectral clustering algorithms are studied in [2,42], they assume that the weights are strictly bounded away from 0, which does not hold here.", "startOffset": 58, "endOffset": 64}, {"referenceID": 39, "context": "Even though spectral clustering algorithms are studied in [2,42], they assume that the weights are strictly bounded away from 0, which does not hold here.", "startOffset": 58, "endOffset": 64}, {"referenceID": 34, "context": "Note that the run time is calculated based on the efficient implementation of single-linkage [37] and the exponential term is not optimized.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 12, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 13, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 14, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 28, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 29, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 31, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 38, "context": "Over the past decade estimating one dimensional distributions has gained significant attention [1, 13\u201315, 30, 31, 33, 41].", "startOffset": 95, "endOffset": 121}, {"referenceID": 14, "context": "The above bound matches the independent and contemporary result by [15] for k = 2.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "[1] Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Dimitris Achlioptas and Frank McSherry.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Rudolf Ahlswede and Andreas Winter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Martin Azizyan, Aarti Singh, and Larry A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Mikhail Belkin and Kaushik Sinha.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Kamalika Chaudhuri, Sanjoy Dasgupta, and Andrea Vattani.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Thomas M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Sanjoy Dasgupta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Sanjoy Dasgupta and Leonard J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Sanjoy Dasgupta and Leonard J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Constantinos Daskalakis and Gautam Kamath.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Luc Devroye and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Inderjit S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Jon Feldman, Ryan O\u2019Donnell, and Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Jon Feldman, Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Yoav Freund and Yishay Mansour.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Daniel Hsu and Sham M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Ravindran Kannan, Hadi Salmasian, and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Michael J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Bruce G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Jinwen Ma, Lei Xu, and Michael I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Ankur Moitra and Gregory Valiant.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Siu on Chan, Ilias Diakonikolas, Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Siu on Chan, Ilias Diakonikolas, Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] Alon Orlitsky, Narayana P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] Liam Paninski.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] Richard A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[36] Douglas A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[37] Robin Sibson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[38] D Michael Titterington, Adrian FM Smith, and Udi E Makov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[39] Joel A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[40] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[41] Gregory Valiant and Paul Valiant.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[42] Santosh Vempala and Grant Wang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[43] Roman Vershynin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[44] Eric P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "For Gaussian distributions the Bhattacharyya parameter is (see [8]), B(p1, p2) = ye, where x = (\u03bc1\u2212\u03bc2)) 4(\u03c32 1 +\u03c3 2 ) and y = \u221a 2\u03c31\u03c32 \u03c3 1 +\u03c3 2 .", "startOffset": 63, "endOffset": 66}, {"referenceID": 24, "context": "Lemma 16 ( [25]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 40, "context": "Lemma 18 ( [43] Remark 5.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "Scheffe estimate [16] outputs a distribution from F whose l1 distance from f is at most 9.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "Together with an observation in Scheffe estimation in [16] one can show that if the number of samples n = O ( log \u2223F\u2223 \u03b4 \u01eb ), then SCHEFFE* has a guarantee 10max(\u01eb,D(f,F)) with probability \u2265 1 \u2212 \u03b4.", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "1 Single Gaussian distribution The proof is an application of the following version of Fano\u2019s inequality [9, 45].", "startOffset": 105, "endOffset": 112}], "year": 2014, "abstractText": "Statistical and machine-learning algorithms are frequently applied to high-dimensional data. In many of these applications data is scarce, and often much more costly than computation time. We provide the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. For mixtures of any k d-dimensional spherical Gaussians, we derive an intuitive spectral-estimator that uses Ok(d log d \u01eb ) samples and runs in time Ok,\u01eb(d3 log d), both significantly lower than previously known. The constant factor Ok is polynomial for sample complexity and is exponential for the time complexity, again much smaller than what was previously known. We also show that \u03a9k( d \u01eb ) samples are needed for any algorithm. Hence the sample complexity is near-optimal in the number of dimensions. We also derive a simple estimator for k-component one-dimensional mixtures that uses O(k log k\u01eb \u01eb ) samples and runs in time \u00d5 ((k \u01eb )). Our other technical contributions include a faster algorithm for choosing a density estimate from a set of distributions, that minimizes the l1 distance to an unknown underlying distribution. jacharya@ucsd.edu ashkan@ucsd.edu alon@ucsd.edu asuresh@ucsd.edu", "creator": "LaTeX with hyperref package"}}}