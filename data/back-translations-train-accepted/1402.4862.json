{"id": "1402.4862", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2014", "title": "Learning the Parameters of Determinantal Point Process Kernels", "abstract": "Determinantal point processes (DPPs) are well-suited for modeling repulsion and have proven useful in many applications where diversity is desired. While DPPs have many appealing properties, such as efficient sampling, learning the parameters of a DPP is still considered a difficult problem due to the non-convex nature of the likelihood function. In this paper, we propose using Bayesian methods to learn the DPP kernel parameters. These methods are applicable in large-scale and continuous DPP settings even when the exact form of the eigendecomposition is unknown. We demonstrate the utility of our DPP learning methods in studying the progression of diabetic neuropathy based on spatial distribution of nerve fibers, and in studying human perception of diversity in images.", "histories": [["v1", "Thu, 20 Feb 2014 01:54:37 GMT  (2333kb,D)", "http://arxiv.org/abs/1402.4862v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["raja hafiz affandi", "emily b fox", "ryan p adams", "benjamin taskar"], "accepted": true, "id": "1402.4862"}, "pdf": {"name": "1402.4862.pdf", "metadata": {"source": "META", "title": "Learning the Parameters of Determinantal Point Process Kernels", "authors": ["Raja Hafiz Affandi"], "emails": ["rajara@wharton.upenn.edu", "ebfox@stat.washington.edu", "rpa@seas.harvard.edu", "taskar@cs.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "3. Learning Parametric DPPs", "text": "Suppose that we obtain a training set consisting of samples A1, A2,..., AT, and that we model this data using a DPP / k-DPP with parametric kernel L (x, y; \u044b) = q (x; \u044b) k (x, y; \u044b) q (y; \u044b), (7) with parameters \u044b. We designate the associated kernel matrix for a set of At (\u0445) and the complete kernel matrix / operator of L (\u044b). Likewise, we designate the kernel eigenvalues by \u03bbi (\u044b). In this section, we will examine various methods for DPP / k-DPP learning."}, {"heading": "3.1. Learning using Optimization Methods", "text": "To learn the parameters of a discrete DPP model, we can use the log likelihoodL (N) = 1 log det (N) \u2212 T log det (N) + I).This method is based on the evaluation of the objective function of a simplex, then iteratively the Nelder-Mead algorithms shrink towards an optimal point. While this method is convenient, it does not require explicit knowledge of the derivatives of L (N), it is considered a heuristic search method and is known for its failure at a stationary point."}, {"heading": "3.2. Bayesian Learning for Discrete DPPs", "text": "In fact, it is such that it is a matter of a way in which people in the various countries of the world move in a way in which they are able, in which they are able, in which they are able, in which they are able, in which they are able to assert themselves that they are able, in which they are able, in which they are able, in which they are able, in which they are able to show themselves in the world of the world, in which they are able, in which they are able, in which they are able to assert themselves that they are able, in which they are able, in which they are able, in which they are able, in which they are able, in the world of the world of the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, they live in which they live, in which they live in the world, in which they live, they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in the world, in which they live, in which they live, in which they live, in which they live, in the world, in which they live, in which they live, in the world, in which they live,"}, {"heading": "3.3. Bayesian Learning for Large-Scale Discrete and Continuous DPPs", "text": "In fact, it is so that it is a way in which people are able to determine themselves how they want to behave. (...) In fact, it is so that people are able to determine themselves. (...) It is so that they do not. (...) It is so. (...) It is so. (...) It is so. (...) \"\" It is as if \"(...).\" (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (((()) \"(((()) ((()) (())\" (() () \"() () (()\" () () \"() () (() ()\" () () (() () () (() () () () () ())) (()) () ()) () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () ()) () () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () (() () () (() () () (() () () (() () () () () () () (() () () () ((() () (() () () (() () () () ((() () () (() () () (("}, {"heading": "4. Method of Moments", "text": "Although generic techniques such as Gelman-Rubin diagnostics (Gelman & Rubin, 1992) are applicable, we also provide a number of tools that are more directly tailored to the DPP by deriving a series of theoretical moments. In the discrete case, we must first calculate the marginal probabilities. Borodin (2009) shows that the moments of our data correspond to the theoretical moments of the posterior samples, which can happen in cases where the natural structure is fully known. In the discrete case, we must first calculate the marginal probabilities."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Simulations", "text": "We offer an explicit example of Bavarian learning for a continuous DPP with the kernel defined byq = (x) = \u221a \u03b1 D = 1 \u221a \u03c0\u03c1d exp {\u2212 x 2 d 2\u03c1d} (25) k (x, y) = D = 1 exp {\u2212 (xd \u2212 yd) 2 2\u03c3d}, x, y \u0445 RD. (26) Here the results and eigenvalues of the operator L (3) are given by (Fassshauer & McCourt, 2012), \u03bbm (\u03b1) = \u03b1 D = 1 \u221a d = 1 \u03b22d + 1 2 + 12\u03b3d (1) + 1) md \u2212 1, where the eigenvalues of the operator L (27) are given."}, {"heading": "5.2. Applications", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1. Diabetic Neuropathy", "text": "Recent breakthroughs in skin tissue imaging have aroused interest in investigating the spatial patterns of nerve fibers in diabetics = 6. These nerve fibers appear to accumulate as diabetes progresses. Waller et al. (2011) previously analyzed these phenomena on the basis of 6 samples of thigh nerve fibers. These samples were collected from 5 diabetics at different stages of diabetic neuropathy and a healthy subject. On average, there are 79 points in each sample (see Fig. 4). Waller et al. (2011) analyzed the ripley K function and concluded that the difference between healthy and severely diabetic samples is highly significant. Instead, we investigate the differences between these samples by learning the core parameters of a DPP and quantifying the extent of the point rejection process. Due to the small sample size, we are conducting two-stage studies of normal / mildly diabetic versus severe diabetic analyses."}, {"heading": "5.2.2. Diversity in Images", "text": "This is useful in applications such as image search, where it is desirable to confront users with a set of images that are not only relevant to the query, but also diverse. Building on the work of Kulesza & Taskar (2011a), three image categories - cars, dogs and cities - were examined; within each category, 8-12 sub-categories (such as Ford for cars, London for cities and poodle for dogs) derived from Google Image Search and the Top 64 results were examined; for one sub-category, these images form our sub-categories. To assess human perceptions of diversity, size six human annotated sentences were generated from these basic sentences. However, it is difficult to ask a human to choose six different images from a set of 64 images contiguously; instead, Kulesza & Taskar (2011a) have a partial result of five images from a 5-DPP on each cat with a cat."}, {"heading": "6. Conclusion", "text": "Although many important DPP calculations are efficient, learning the parameters of a DPP kernel is difficult due to the non-convexity of probability. We suggested Bayesian approaches, especially using MCMC, to derive these parameters. These algorithms are not only more robust and provide a characterization of posterior uncertainty, but can also be modified to deal with large-scale and continuous DPPs. We also demonstrated how our posterior samples can be evaluated using motion matching as a method of model verification. Finally, we demonstrated the utility of learning DPP parameters in investigating diabetic neuropathy and assessing human perception of diversity in images."}, {"heading": "A. Gradient for Discrete DPP", "text": "Gradient and stochastic gradient supply attractive approaches in learning parameters. (...) Gradient and stochastic gradient supply attractive approaches in learning parameters. (...) Gradient and stochastic gradient supply attractive approaches in learning parameters. (...) In the discrete DPP environment this gradient (...) can be directly calculated and we supply examples of discrete Gaussian and polynomiale kernels here.L (...) = T-Log (...)). (T-Log (L (...) + I). (30) dL (...). (...) dL-Log (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.... (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (...). (...).). (...). (...).). (...). (...).). (...).). (...).). (...). (...). (...).). (...). (...). (...). (...). (...). (...).).). (...).). (...). (...). (...). (...).). (...). (...). (...). (...).). (...). (...). (...). (...). (...).).). (...).). (...). (...).). (...). (...).)."}, {"heading": "B. Bayesian Learning", "text": "The main work highlights two techniques: random walk = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan."}, {"heading": "D. Moments for Continuous DPP with Gaussian Quality and Similarity", "text": "In the ongoing case, the self-decomposition of the kernel operator, L (x, y) = holistic view of the individual phases (x) + holistic view of the individual phases (x) = holistic view of the individual phases (x, y) = holistic view of the individual phases (x) = holistic view of the individual phases (x) = holistic view of the individual phases (x) = holistic view of the individual phases (x)."}, {"heading": "E. Details on Simulation", "text": "In the main work, we use our Bayesian learning algorithms to learn parameters from (i) simulated data generated from a two-dimensional isotropic discrete nucleus (\u03c3d = \u03c3, \u03c1d = \u03c1 for d = 1, 2), (ii) nerve fiber data generated from a two-dimensional isotropic continuous nucleus (\u03c3d = \u03c3, \u03c1d = \u03c1 for d = 1, 2) and (ii) image inversion data using 3600-dimensional discrete nuclei with Gaussian similarity. In all these experiments, we use weakly informative inverse gamma priors on \u03c3, \u03c1 and \u03b1. Specifically, for all three parameters, we used the same priors for all three parameters P (\u03b1) = P (\u03c1) = P (\u03c3) = Inv-gamma (0.001, 0.001) (45)."}, {"heading": "F. Details on Image Diversity", "text": "In fact, it is as if most people are able to recognize themselves and understand how they have behaved. (...) In fact, it is as if most of us are not able to recognize ourselves. (...) It is as if they were able to identify themselves. (...) It is as if they were able to identify themselves. (...) It is as if they were able to identify themselves. (...) It is as if they were able to identify themselves. (...) It is as if they were able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves."}], "references": [{"title": "Markov determinantal point processes", "author": ["R.H. Affandi", "A. Kulesza", "E.B. Fox"], "venue": "In Proc. UAI,", "citeRegEx": "Affandi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2012}, {"title": "Approximate inference in continuous determinantal processes", "author": ["R.H. Affandi", "E.B. Fox", "B. Taskar"], "venue": "In Proc. NIPS,", "citeRegEx": "Affandi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2013}, {"title": "Nystr\u00f6m approximation for large-scale determinantal processes", "author": ["R.H. Affandi", "A. Kulesza", "E.B. Fox", "B. Taskar"], "venue": "In Proc. AISTATS,", "citeRegEx": "Affandi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2013}, {"title": "Determinantal point processes", "author": ["A. Borodin"], "venue": "arXiv preprint arXiv:0911.1153,", "citeRegEx": "Borodin,? \\Q2009\\E", "shortCiteRegEx": "Borodin", "year": 2009}, {"title": "Eynard-Mehta theorem, Schur process, and their Pfaffian analogs", "author": ["A. Borodin", "E.M. Rains"], "venue": "Journal of statistical physics,", "citeRegEx": "Borodin and Rains,? \\Q2005\\E", "shortCiteRegEx": "Borodin and Rains", "year": 2005}, {"title": "Stable evaluation of Gaussian radial basis function interpolants", "author": ["G.E. Fasshauer", "M.J. McCourt"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Fasshauer and McCourt,? \\Q2012\\E", "shortCiteRegEx": "Fasshauer and McCourt", "year": 2012}, {"title": "Inference from iterative simulation using multiple sequences", "author": ["A. Gelman", "D.B. Rubin"], "venue": "Statistical science,", "citeRegEx": "Gelman and Rubin,? \\Q1992\\E", "shortCiteRegEx": "Gelman and Rubin", "year": 1992}, {"title": "Discovering diverse and salient threads in document collections", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "In Proc. EMNLP,", "citeRegEx": "Gillenwater et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gillenwater et al\\.", "year": 2012}, {"title": "Schur-convexity of the complete elementary symmetric function", "author": ["K. Guan"], "venue": "Journal of Inequalities and Applications,", "citeRegEx": "Guan,? \\Q2006\\E", "shortCiteRegEx": "Guan", "year": 2006}, {"title": "Determinantal processes and independence", "author": ["J.B. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability Surveys,", "citeRegEx": "Hough et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hough et al\\.", "year": 2006}, {"title": "Structured determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In Proc. NIPS,", "citeRegEx": "Kulesza and Taskar,? \\Q2010\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2010}, {"title": "k-DPPs: Fixed-size determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In ICML,", "citeRegEx": "Kulesza and Taskar,? \\Q2011\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2011}, {"title": "Learning determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In In Proc. UAI,", "citeRegEx": "Kulesza and Taskar,? \\Q2011\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2011}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulesza and Taskar,? \\Q2012\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2012}, {"title": "Statistical aspects of determinantal point processes", "author": ["F. Lavancier", "J. M\u00f8ller", "E. Rubak"], "venue": "arXiv preprint arXiv:1205.4818,", "citeRegEx": "Lavancier et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lavancier et al\\.", "year": 2012}, {"title": "Object recognition from local scaleinvariant features", "author": ["D.G. Lowe"], "venue": "In Computer vision,", "citeRegEx": "Lowe,? \\Q1999\\E", "shortCiteRegEx": "Lowe", "year": 1999}, {"title": "Convergence of the Nelder\u2013Mead simplex method to a nonstationary point", "author": ["McKinnon", "K.I.M"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "McKinnon and I.M.,? \\Q1998\\E", "shortCiteRegEx": "McKinnon and I.M.", "year": 1998}, {"title": "A simplex method for function minimization", "author": ["J. Nelder", "R. Mead"], "venue": "Computer Journal,", "citeRegEx": "Nelder and Mead,? \\Q1965\\E", "shortCiteRegEx": "Nelder and Mead", "year": 1965}, {"title": "Building the gist of a scene: The role of global image features in recognition", "author": ["A. Oliva", "A. Torralba"], "venue": "Progress in brain research,", "citeRegEx": "Oliva and Torralba,? \\Q2006\\E", "shortCiteRegEx": "Oliva and Torralba", "year": 2006}, {"title": "Asymptotics of integrals of hermite polynomials", "author": ["R.B. Paris"], "venue": "Appl. Math. Sci", "citeRegEx": "Paris,? \\Q2010\\E", "shortCiteRegEx": "Paris", "year": 2010}, {"title": "Modelling spatial patterns", "author": ["B.D. Ripley"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Ripley,? \\Q1977\\E", "shortCiteRegEx": "Ripley", "year": 1977}, {"title": "A determinantal point process latent variable model for inhibition in neural spiking data", "author": ["J. Snoek", "R. Zemel", "R.P. Adams"], "venue": "In Proc. NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2013}, {"title": "Vlfeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "In Proceedings of the international conference on Multimedia,", "citeRegEx": "Vedaldi and Fulkerson,? \\Q2010\\E", "shortCiteRegEx": "Vedaldi and Fulkerson", "year": 2010}, {"title": "Second-order spatial analysis of epidermal nerve fibers", "author": ["L.A. Waller", "A. S\u00e4rkk\u00e4", "V. Olsbo", "M. Myllym\u00e4ki", "I.G. Panoutsopoulou", "W.R. Kennedy", "G. Wendelschafer-Crabb"], "venue": "Statistics in Medicine,", "citeRegEx": "Waller et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Waller et al\\.", "year": 2011}, {"title": "Priors for diversity in generative latent variable models", "author": ["J. Zou", "R.P. Adams"], "venue": "In Proc. NIPS,", "citeRegEx": "Zou and Adams,? \\Q2012\\E", "shortCiteRegEx": "Zou and Adams", "year": 2012}, {"title": "In this case, the eigenvalues and eigenvectors of the operator L are given by Fasshauer", "author": ["y \u2208 R"], "venue": null, "citeRegEx": "x and R.,? \\Q2012\\E", "shortCiteRegEx": "x and R.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recently, DPPs have played an increasingly important role in machine learning and statistics with applications both in the discrete setting\u2014where they are used as a diverse subset selection method (Affandi et al., 2012; 2013b; Gillenwater et al., 2012; Kulesza & Taskar, 2010; 2011a; Snoek et al., 2013)\u2014 and in the continuous setting for generating point configurations that tend to be spread out(Affandi et al.", "startOffset": 197, "endOffset": 303}, {"referenceID": 7, "context": "Recently, DPPs have played an increasingly important role in machine learning and statistics with applications both in the discrete setting\u2014where they are used as a diverse subset selection method (Affandi et al., 2012; 2013b; Gillenwater et al., 2012; Kulesza & Taskar, 2010; 2011a; Snoek et al., 2013)\u2014 and in the continuous setting for generating point configurations that tend to be spread out(Affandi et al.", "startOffset": 197, "endOffset": 303}, {"referenceID": 21, "context": "Recently, DPPs have played an increasingly important role in machine learning and statistics with applications both in the discrete setting\u2014where they are used as a diverse subset selection method (Affandi et al., 2012; 2013b; Gillenwater et al., 2012; Kulesza & Taskar, 2010; 2011a; Snoek et al., 2013)\u2014 and in the continuous setting for generating point configurations that tend to be spread out(Affandi et al.", "startOffset": 197, "endOffset": 303}, {"referenceID": 9, "context": "One of the remarkable aspects of DPPs is that they offer efficient algorithms for inference, including computing the marginal and conditional probabilities (Kulesza & Taskar, 2012), sampling (Affandi et al., 2013a;b; Hough et al., 2006; Kulesza & Taskar, 2010), and restricting to fixed-sized point configurations (k-DPPs)(Kulesza & Taskar, 2011a).", "startOffset": 191, "endOffset": 260}, {"referenceID": 14, "context": "So far, the only attempt to learn the parameters of the similarity kernel k(x,y) has used Nelder-Mead optimization (Lavancier et al., 2012), which lacks theoretical guarantees about convergence to a stationary point.", "startOffset": 115, "endOffset": 139}, {"referenceID": 0, "context": "However, Affandi et al. (2013a) showed that a low-rank approximation to L can be used to recover an approximation to a finite truncation of the eigenvalues representing an important part of the eigenspectrum.", "startOffset": 9, "endOffset": 32}, {"referenceID": 14, "context": "(8) Lavancier et al. (2012) suggests that the Nelder-Mead simplex algorithm (Nelder & Mead, 1965) can be used to maximize L(\u0398).", "startOffset": 4, "endOffset": 28}, {"referenceID": 3, "context": "Borodin (2009) shows that the marginal kernel, K, can be computed directly from L: K = L(I + L)\u22121 .", "startOffset": 0, "endOffset": 15}, {"referenceID": 20, "context": "In spatial statistics, one standard quantity used to measure dispersion is the Ripley Kfunction (Ripley, 1977).", "startOffset": 96, "endOffset": 110}, {"referenceID": 22, "context": "Waller et al. (2011) previously analyzed this phenomena based on 6 thigh nerve fiber samples.", "startOffset": 0, "endOffset": 21}, {"referenceID": 22, "context": "Waller et al. (2011) previously analyzed this phenomena based on 6 thigh nerve fiber samples. These samples were collected from 5 diabetic patients at different stages of diabetic neuropathy and one healthy subject. On average, there are 79 points in each sample (see Fig. 4). Waller et al. (2011) analyzed the Ripley K-function and concluded that the", "startOffset": 0, "endOffset": 298}, {"referenceID": 23, "context": "5 indicate that our \u03b3 measure clearly separates the two classes, concurring with the results of Waller et al. (2011). Furthermore, we are able to correctly classify all six samples.", "startOffset": 96, "endOffset": 117}, {"referenceID": 15, "context": "As in Kulesza & Taskar (2011a), we extracted three types of features from the images\u2014color features, SIFT descriptors (Lowe, 1999; Vedaldi & Fulkerson, 2010) and GIST descriptors (Oliva & Torralba, 2006) described in the Supplementary Material.", "startOffset": 118, "endOffset": 157}], "year": 2014, "abstractText": "Determinantal point processes (DPPs) are well-suited for modeling repulsion and have proven useful in many applications where diversity is desired. While DPPs have many appealing properties, such as efficient sampling, learning the parameters of a DPP is still considered a difficult problem due to the non-convex nature of the likelihood function. In this paper, we propose using Bayesian methods to learn the DPP kernel parameters. These methods are applicable in large-scale and continuous DPP settings even when the exact form of the eigendecomposition is unknown. We demonstrate the utility of our DPP learning methods in studying the progression of diabetic neuropathy based on spatial distribution of nerve fibers, and in studying human perception of diversity in images.", "creator": "LaTeX with hyperref package"}}}