{"id": "1402.5596", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2014", "title": "Exact Post Model Selection Inference for Marginal Screening", "abstract": "We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response $y$, conditional on the model being selected (``condition on selection\" framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in high-dimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix $X$. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit, non-negative least squares, and marginal screening+Lasso.", "histories": [["v1", "Sun, 23 Feb 2014 10:30:21 GMT  (99kb,D)", "https://arxiv.org/abs/1402.5596v1", null], ["v2", "Fri, 28 Feb 2014 00:28:21 GMT  (99kb,D)", "http://arxiv.org/abs/1402.5596v2", null]], "reviews": [], "SUBJECTS": "stat.ME cs.LG math.ST stat.ML stat.TH", "authors": ["jason d lee", "jonathan e taylor"], "accepted": true, "id": "1402.5596"}, "pdf": {"name": "1402.5596.pdf", "metadata": {"source": "CRF", "title": "Exact Post Model Selection Inference for Marginal Screening", "authors": ["Jason D. Lee", "Jonathan E. Taylor"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Consider the hypothesis H0 j = 0 and form confidence intervals for \u03b20j using the high-dimensional problem (0, \u03c32I), (1) where \u00b5 (x) is an arbitrary function, and xi \u00b2 Rp. Our goal is to draw conclusions about (XTX) \u2212 1XT\u00b5, which is the best linear predictor of \u00b5. In the classic constellation of n > p, screening screening is a commonly used estimate for (XTX) \u2212 1XT\u00b5, the least square estimation methods used. Under the linear model assumption \u00b5 = X\u03b20, the exact distribution of \u03b2 is\u03b2 2 (XTX) \u2212 1). (3) Using the normal distribution, we can hypothesize the hypothesis H0: \u03b2 0 j = 0 and form confidence intervals for \u03b20j."}, {"heading": "2 Related Work", "text": "Most of the theoretical work on high-dimensional linear models focuses on consistency. Such results justify, under restrictive assumptions about X, the lasso method is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15]. We refer the reader for a comprehensive discussion of the theoretical properties of the lasso. There is also recent work on recording consistency intervals and the importance of testing for penalized M-estimators such as the lasso. A method class uses sample splitting or subsampling to obtain consistency intervals and p-values [31, 23]. In the post-model-model selection literature, the most recent work of the POSI method proposes a correction to the usual t-test consistency intervals by controlling the familywise error rate for all possible parameters in any submodel. The POSI approach will be valid selection interval for all possible model selection intervals, but it will be extremely consistent."}, {"heading": "3 Marginal Screening", "text": "Let X, Rn, p be the design matrix, y, Rn be the response variable, and assume that X is in general position and has standard columns; the algorithm estimates \u03b2 to be algorithm 1. The marginal screening algorithm 1 Marginal screening algorithm1: Input: Design matrix X, response matrix y and model size k. 2: Compute | XT y |. 3: Let S index the k-largest inputs of | XT y |. 4: Compute \u03b2-S = (X T, XS) - 1XT S-yrithm selects the k variables with the highest absolute point product with y and then matches a linear model with these k variables. Let's assume that k \u2212 j, min (n, p). < < < < < < < Z, z, the distribution of \u03b2-S, S-Sch and S-ST (S1) is not defined."}, {"heading": "3.1 Failure of z-test confidence intervals", "text": "We will empirically demonstrate that the z-test intervals do not cover 1 \u2212 \u03b1 when selected in the marginal screening algorithm 1 S \u0445. For this experiment, we generated X from a standard standard standard with n = 20 and p = 200. The signal vector is 2 sparse with \u03b201, \u03b2 0 2 = SNR, y = X\u03b20 + and \u0445 N (0, 1). The confidence intervals were constructed for the k = 2 variables selected by the marginal screening algorithm. The z-test intervals were constructed over (6) with \u03b1 = 1, and the adjusted intervals were constructed according to algorithm 3. The results are described in Figure 1. The y-axis plots the coverage ratio or fraction of the actual parameter value in the confidence interval. Each point represents 500 independent studies. The x-axis varies the SNR parameters over the values 0.1, 0.1,.2,.5,.1,.2,.5,.5,.5,.5,.5."}, {"heading": "4 Representing the selection event", "text": "Since Equation (5) does not apply to a selected S, if the selection procedure depends on y, the z test intervals are invalid. Our strategy will be to understand the conditional distribution of y and contrasts (linear functions of y), and then draw conclusions that depend on the selection event E. We will use E (y) to represent a random variable, and E to represent an element of the range of E (y). In the case of the selection event E (y), the selection event corresponds to the selected variable S and character s: E (y) = {y: character (xTi y) x y > \u00b1 xTj y y for all i-S and j-S selection events."}, {"heading": "5 Truncated Gaussian test", "text": "This section summarizes the most recent tools developed in [16] to test contrast2 throusts2 throusts2 \u03b7j y (cumbersome Gaussian y = cumbersome Gaussian y), the results are given without proof and the evidence can be found in [16]. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 - The distribution of a restricted Gaussian y \u0445 N (cumbersome, comprehensive) due to affine constraints {Ay \u2264 b} has the density 1Pr (Ay \u2264 b) f (y; cumbersome, cumbersome) 1 (cumbersome), includes the intractable normalization constant Pr (Ay \u2264 b). In this section we derive a one-dimensional speed for cumbersome V (cumbersome). This direction of rotation is based on the characterization of the distribution of \u03b7T y y y y y y y y y y as trunctional normality. The key step to derive this pivot is the following problem: {5.1} Conditiony y y y y: The following is:"}, {"heading": "6 Inference for marginal screening", "text": "In this section, we will apply the theory summarised in sections 4 and 5 to marginal screening. In particular, we will construct confidence intervals for the selected variables. To summarize the developments to date, remember that our model (1) states that y-N is (\u00b5, \u03c32I). The distribution of interest is y | {E-Y (y) = E}, and according to Theorem 4.1 this corresponds to y | {A (S, s) z \u2264 b (S, s)}, where y-N (\u00b5, \u03c32I). Applying theorem 5.3, we obtain the rotational speed F [V \u2212, V +] \u03b7T\u00b5, \u03c32 | | | 22 (\u03b7T y) \u03c0\u0430\u0432\u0430\u0432\u0430\u043d\u043d\u043d\u00fcn\u00fcn\u00fcn\u00fcn\u00fcn\u00fcn\u00fcn\u00fcn\u0438\u0435 (y) = E-Unif (0, 1) (15) for each case in which V \u2212 and V + are defined in (10) and (11)."}, {"heading": "6.1 Hypothesis tests for selected variables", "text": "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXII-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"}, {"heading": "6.2 Confidence intervals for selected variables", "text": "Next, we will discuss how to get confidence intervals for \u03b2? j \u00b2 S \u00b2. The standard way to get a confidence interval is to reverse a crucial quantity [4]. In other words, one can get a (1 \u2212 \u03b1) (conditional) confidence interval for \u03b2, 2 | | | 2 (\u03b7 T j y) \u2264 1 \u2212 \u00b2 (E \u00b2) = \u00b2 (1 \u2212 \u03b1) (conditional) confidence interval for \u03b2? j, E \u00b2 as [x: \u03b1 2 \u2264 F [V \u2212, V +] x, \u03c32 | | 2 (E \u00b2). (18) In fact, F is a monotonous reduction in x, in order to find its endpoints, one only has to find a solution for the root of a smooth one-dimensional function. Monotonic function is a consequence of the fact that the intoxicated Gaussian distribution is a natural, horizontal family."}, {"heading": "6.3 Experiments on Diabetes dataset", "text": "In Figure 1, we have already seen that the confidence intervals constructed with the help of algorithm 3 have exactly 1 \u2212 \u03b1 coverage ratio (in this section, we are conducting an experiment with real data where the linear model does not hold, the noise is not Gaussian, and the deviation from the noise behavior is unknown).The diabetes dataset contains n = 442 diabetes patients measured one year after the baseline. [6] The basic variables are age, sex, body mass index, average blood pressure, and six blood serum measurements, and the answer y is a quantitative measurement of disease progression measured one year after the baseline. The goal is to use the basic variables to predict disease progression one year after the baseline and to determine which baseline variables are needed to predict y.Since the noise variance is unknown one year after the baseline, we estimate it using non-smoking intervals measured one year after the baseline."}, {"heading": "7 Extensions", "text": "This framework was first proposed in [16] to form valid hypotheses tests and confidence intervals after model selection via the lasso. However, the framework is not limited to the lasso, and we have shown how to apply it to marginal screening. For expository purposes, we focused the essay on marginal screening, where the framework is particularly easy to understand. In the rest of this section, we will show how to apply the framework to marginal screening + lasso, orthogonal matching struts, and non-negative smallest squares. This is a non-exhaustive list of selection procedures where the condition applies to the selection frame, but we hope that this incomplete list emphasizes the ease of constructing tests and confidence intervals after selection by conditioning."}, {"heading": "7.1 Marginal screening + Lasso", "text": "The marginal screening + lasso procedure was introduced in [9] as a variable selection method for the ultra-high-dimensional setting of p = O (en k). Fan et al. [9] recommend applying the marginal screening algorithm with k = n \u2212 1, followed by the lasso on the selected variables. This is a two-stage procedure, so to properly consider the selection, we must encode the selection event of the marginal screening followed by the lasso. This can be done by presenting the two-stage selection as a single event. Let (S-m, s-m) be the variables and characters selected by the marginal screening, and that (S-L, z-L) be the variables and characters selected by the lasso [16]. In Proposition 2.2 of [16] it is shown how to encode the lasso selection event (S-L, s-L) as a set of {by-L} constraints."}, {"heading": "7.2 Orthogonal Matching Pursuit", "text": "For each iteration, OMP selects the variable that correlates most strongly with the residual criterion r, and then computes the residual criterion using the selected variables. The description of the OMP algorithm is given in Algorithm 4.4The lasso selection event with respect to the lasso optimization problem is given according to the marginal criterion screening. Algorithm 4: Orthogonal Matching Method (OMP) 1: Input: Design matrix X, Response and model size k. 2: for: i = 1 to k 3: pi = arg maxj = 1,..., p | rTi xj | 4: S = 1 {pi}. 5: ri + 1 = (I \u2212 XS \u00b2 iX \u00b2)."}, {"heading": "7.3 Nonnegative Least Squares", "text": "Non-negative smallest squares (NNLS) are a simple modification of the linear regression estimator with non-negative constraints for \u03b2: \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "8 Conclusion", "text": "Due to the increasing size of the data sets, marginal screening has become an important method for rapid variable selection. However, the standard hypotheses tests and confidence intervals used in linear regression become invalid after the use of marginal screening to select important variables. We have described a method for making hypotheses and building confidence intervals after marginal screening. Conditional selection is not limited to marginal screening and also applies to OMP, marginal screening + lasso and NNLS."}, {"heading": "Acknowledgements", "text": "Jonathan Taylor was partially supported by the NSF scholarship DMS 1208857 and the AFOSR scholarship 113039. Jason Lee was supported by an NSF scholarship and a Stanford Graduate Fellowship."}], "references": [{"title": "False discovery rate\u2013adjusted multiple confidence intervals for selected parameters", "author": ["Yoav Benjamini", "Daniel Yekutieli"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Valid post-selection inference", "author": ["Richard Berk", "Lawrence Brown", "Andreas Buja", "Kai Zhang", "Linda Zhao"], "venue": "Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Statistics for Highdimensional Data", "author": ["Peter Lukas B\u00fchlmann", "Sara A van de Geer"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Statistical inference, volume 70", "author": ["George Casella", "Roger L Berger"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Nonnegativity constraints in numerical analysis", "author": ["Donghui Chen", "Robert J Plemmons"], "venue": "In Symposium on the Birth of Numerical Analysis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "The Annals of statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "An introduction to the bootstrap, volume 57", "author": ["Bradley Efron", "Robert Tibshirani"], "venue": "CRC press,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Variance estimation using refitted cross-validation in ultrahigh dimensional regression", "author": ["Jianqing Fan", "Shaojun Guo", "Ning Hao"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Sure independence screening for ultrahigh dimensional feature space", "author": ["Jianqing Fan", "Jinchi Lv"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Ultrahigh dimensional feature selection: beyond the linear model", "author": ["Jianqing Fan", "Richard Samworth", "Yichao Wu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Sure independence screening in generalized linear models with np-dimensionality", "author": ["Jianqing Fan", "Rui Song"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "A comparison of the lasso and marginal regression", "author": ["Christopher R Genovese", "Jiashun Jin", "Larry Wasserman", "Zhigang Yao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Confidence intervals and hypothesis testing for high-dimensional regression", "author": ["Adel Javanmard", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1306.3171,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "On model selection consistency of penalized m-estimators: a geometric theory", "author": ["Jason Lee", "Yuekai Sun", "Jonathan E Taylor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Exact inference after model selection via the lasso", "author": ["Jason D Lee", "Dennis L Sun", "Yuekai Sun", "Jonathan E Taylor"], "venue": "arXiv preprint arXiv:1311.6238,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "The finite-sample distribution of post-model-selection estimators and uniform versus nonuniform approximations", "author": ["Hannes Leeb", "Benedikt M P\u00f6tscher"], "venue": "Econometric Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "P\u00f6tscher. Model selection and inference: Facts and fiction", "author": ["Hannes Leeb", "Benedikt M"], "venue": "Econometric Theory,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Can one estimate the conditional distribution of post-model-selection estimators", "author": ["Hannes Leeb", "Benedikt M P\u00f6tscher"], "venue": "The Annals of Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Sign-constrained least squares estimation for high-dimensional regression", "author": ["Nicolai Meinshausen"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "P-values for high-dimensional regression", "author": ["Nicolai Meinshausen", "Lukas Meier", "Peter B\u00fchlmann"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "A study of error variance estimation in lasso regression", "author": ["Stephen Reid", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Non-negative least squares for high-dimensional linear models: Consistency and sparse recovery without regularization", "author": ["Martin Slawski", "Matthias Hein"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Scaled sparse linear regression", "author": ["Tingni Sun", "Cun-Hui Zhang"], "venue": "Biometrika, 99(4):879\u2013898,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Significance analysis of microarrays applied to the ionizing radiation response", "author": ["Virginia Goss Tusher", "Robert Tibshirani", "Gilbert Chu"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "On asymptotically optimal confidence regions and tests for high-dimensional models", "author": ["Sara van de Geer", "Peter B\u00fchlmann", "Ya\u2019acov Ritov"], "venue": "arXiv preprint arXiv:1303.0518,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming (lasso)", "author": ["M.J. Wainwright"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "High dimensional variable selection", "author": ["Larry Wasserman", "Kathryn Roeder"], "venue": "Annals of statistics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Confidence intervals for lowdimensional parameters with high-dimensional data", "author": ["Cun-Hui Zhang", "S Zhang"], "venue": "arXiv preprint arXiv:1110.2563,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "On model selection consistency of lasso", "author": ["P. Zhao", "B. Yu"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}], "referenceMentions": [{"referenceID": 2, "context": "However in the high-dimensional p > n setting, the least squares estimator is an underdetermined problem, and the predominant approach is to perform variable selection or model selection [3].", "startOffset": 187, "endOffset": 190}, {"referenceID": 12, "context": "Marginal screening is the simplest and most commonly used of the variable selection procedures [13, 28, 20].", "startOffset": 95, "endOffset": 107}, {"referenceID": 25, "context": "Marginal screening is the simplest and most commonly used of the variable selection procedures [13, 28, 20].", "startOffset": 95, "endOffset": 107}, {"referenceID": 11, "context": "Furthermore, the selection properties are comparable to the Lasso [12].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S \u2282 \u015c), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10].", "startOffset": 205, "endOffset": 216}, {"referenceID": 10, "context": "In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S \u2282 \u015c), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10].", "startOffset": 205, "endOffset": 216}, {"referenceID": 9, "context": "In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S \u2282 \u015c), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10].", "startOffset": 205, "endOffset": 216}, {"referenceID": 16, "context": "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].", "startOffset": 76, "endOffset": 91}, {"referenceID": 17, "context": "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].", "startOffset": 76, "endOffset": 91}, {"referenceID": 18, "context": "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].", "startOffset": 76, "endOffset": 91}, {"referenceID": 1, "context": "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].", "startOffset": 76, "endOffset": 91}, {"referenceID": 16, "context": "A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.", "startOffset": 21, "endOffset": 33}, {"referenceID": 17, "context": "A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.", "startOffset": 21, "endOffset": 33}, {"referenceID": 18, "context": "A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.", "startOffset": 21, "endOffset": 33}, {"referenceID": 15, "context": "Although the focus of this paper is on marginal screening, the \u201ccondition on selection\u201d framework, first proposed for the Lasso in [16], is much more general; we use marginal screening as a simple and clean illustration of the applicability of this framework.", "startOffset": 131, "endOffset": 135}, {"referenceID": 21, "context": "Such results establish, under restrictive assumptions on X, the Lasso \u03b2\u0302 is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "Such results establish, under restrictive assumptions on X, the Lasso \u03b2\u0302 is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15].", "startOffset": 135, "endOffset": 147}, {"referenceID": 27, "context": "Such results establish, under restrictive assumptions on X, the Lasso \u03b2\u0302 is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15].", "startOffset": 135, "endOffset": 147}, {"referenceID": 14, "context": "Such results establish, under restrictive assumptions on X, the Lasso \u03b2\u0302 is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15].", "startOffset": 135, "endOffset": 147}, {"referenceID": 2, "context": "We refer to the reader to [3] for a comprehensive discussion about the theoretical properties of the Lasso.", "startOffset": 26, "endOffset": 29}, {"referenceID": 28, "context": "One class of methods uses sample splitting or subsampling to obtain confidence intervals and p-values [31, 23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 20, "context": "One class of methods uses sample splitting or subsampling to obtain confidence intervals and p-values [31, 23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 1, "context": "In the post model selection literature, the recent work of [2] proposed the POSI approach, a correction to the usual t-test confidence intervals by controlling the familywise error rate for all parameters in any possible submodel.", "startOffset": 59, "endOffset": 62}, {"referenceID": 26, "context": "A separate line of work establishes the asymptotic normality of a corrected estimator obtained by \u201cinverting\u201d the KKT conditions [29, 32, 14].", "startOffset": 129, "endOffset": 141}, {"referenceID": 29, "context": "A separate line of work establishes the asymptotic normality of a corrected estimator obtained by \u201cinverting\u201d the KKT conditions [29, 32, 14].", "startOffset": 129, "endOffset": 141}, {"referenceID": 13, "context": "A separate line of work establishes the asymptotic normality of a corrected estimator obtained by \u201cinverting\u201d the KKT conditions [29, 32, 14].", "startOffset": 129, "endOffset": 141}, {"referenceID": 15, "context": "Most closely related to our work is the \u201ccondition on selection\u201d framework laid out in [16] for the Lasso.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "By extension, we do not make any assumptions on n and p, which is unusual in high-dimensional statistics [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "A data splitting technique is used in [8], while [27] proposes a method that computes the regression estimate and an estimate of the variance simultaneously.", "startOffset": 38, "endOffset": 41}, {"referenceID": 24, "context": "A data splitting technique is used in [8], while [27] proposes a method that computes the regression estimate and an estimate of the variance simultaneously.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "We refer the reader to [25] for a survey and comparison of the various methods,", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "This section summarizes the recent tools developed in [16] for testing contrasts2 \u03b7T y of a constrained Gaussian y.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "The results are stated without proof and the proofs can be found in [16].", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "In [9], the screening property S0 \u2282 \u015c for the marginal screening algorithm is", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "The standard way to obtain an interval is to invert a pivotal quantity [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "In relation to the literature on False Coverage Rate (FCR) [1], our procedure also controls the FCR.", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "The diabetes dataset contains n = 442 diabetes patients measured on p = 10 baseline variables [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "This is known as the residual bootstrap, and is a standard method for assessing statistical procedures when the underlying model is unknown [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 15, "context": "This framework was first proposed in [16]", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "1 Marginal screening + Lasso The marginal screening+Lasso procedure was introduced in [9] as a variable selection method for the ultra-high dimensional setting of p = O(en k ).", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "[9] recommend applying the marginal screening algorithm with k = n \u2212 1, followed by the Lasso on the selected variables.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Let (\u015cm, \u015dm) be the variables and signs selected by marginal screening, and the (\u015cL, \u1e91L) be the variables and signs selected by Lasso [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "2 of [16], it is shown how to encode the Lasso selection event (\u015cL, \u1e91L) as a set of constraints {ALy \u2264 bL} 4, and in Section 4 we showed how to encode the marginal screening selection event (\u015cm, \u015dm) as a set of constraints {Amy \u2264 bm}.", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "Under a positive eigenvalue conditions on X, several authors [26, 22] have shown that NNLS is comprable to the Lasso in terms of prediction and", "startOffset": 61, "endOffset": 69}, {"referenceID": 19, "context": "Under a positive eigenvalue conditions on X, several authors [26, 22] have shown that NNLS is comprable to the Lasso in terms of prediction and", "startOffset": 61, "endOffset": 69}, {"referenceID": 4, "context": "Non-negativity constraints arise naturally in non-negative matrix factorization, signal deconvolution, spectral analysis, and network tomography; we refer to [5] for a comprehensive survey of the applications of NNLS.", "startOffset": 158, "endOffset": 161}], "year": 2014, "abstractText": "We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response y, conditional on the model being selected (\u201ccondition on selection\u201d framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in highdimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix X. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit, non-negative least squares, and marginal screening+Lasso.", "creator": "LaTeX with hyperref package"}}}