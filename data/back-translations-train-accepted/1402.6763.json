{"id": "1402.6763", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2014", "title": "Linear Programming for Large-Scale Markov Decision Problems", "abstract": "We consider the problem of controlling a Markov decision process (MDP) with a large state space, so as to minimize average cost. Since it is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing with a low-dimensional family of policies. We use the dual linear programming formulation of the MDP average cost problem, in which the variable is a stationary distribution over state-action pairs, and we consider a neighborhood of a low-dimensional subset of the set of stationary distributions (defined in terms of state-action features) as the comparison class. We propose two techniques, one based on stochastic convex optimization, and one based on constraint sampling. In both cases, we give bounds that show that the performance of our algorithms approaches the best achievable by any policy in the comparison class. Most importantly, these results depend on the size of the comparison class, but not on the size of the state space. Preliminary experiments show the effectiveness of the proposed algorithms in a queuing application.", "histories": [["v1", "Thu, 27 Feb 2014 01:43:38 GMT  (64kb)", "http://arxiv.org/abs/1402.6763v1", "27 pages, 3 figures"]], "COMMENTS": "27 pages, 3 figures", "reviews": [], "SUBJECTS": "math.OC cs.AI cs.NA", "authors": ["alan malek", "yasin abbasi-yadkori", "peter l bartlett"], "accepted": true, "id": "1402.6763"}, "pdf": {"name": "1402.6763.pdf", "metadata": {"source": "CRF", "title": "Linear Programming for Large-Scale Markov Decision Problems", "authors": ["Yasin Abbasi-Yadkori"], "emails": ["yasin.abbasiyadkori@qut.edu.au", "bartlett@eecs.berkeley.edu", "malek@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 2.67 63v1 [m ath. OWe look at the problem of controlling a Markov decision-making process (MDP) with a large government space to minimize average costs. As it is impossible to compete with the optimal policy for large problems, we pursue the more modest goal of competing with a low-dimensional family of measures. We use the dual linear programming formulation of the MDP average problem, where the variable is a stationary distribution over state pairs of shareholders, and consider a neighborhood of a low-dimensional subset of the set of stationary distributions (defined in terms of state characteristics) as a comparison class. We propose two techniques, one based on stochastic convex optimization and one based on forced sampling. In both cases, we set limits that show that the performance of our algorithms approaches the best possible policy in the comparison class."}, {"heading": "1 Introduction", "text": "The problem is well understood when the state and the margins of action are small (Bertsekas, 2007). Dynamic programming algorithms (DP), such as value estimation (Bellman, 1957) and policy iteration (Howard, 1960), are standard techniques for calculating the optimal policy. In large state spatial problems, exact DP is not practicable, since computational complexity is scaled square with the number of states. A popular approach to large-scale problems is to limit the search to the linear span of a small number of characteristics. The goal is to compete with the best solution within this comparison class. Two popular methods are Approximate Dynamic Programming (ADP) and Approximate Linear Programming (ALP). This paper focuses on the linear span small number of characteristics."}, {"heading": "1.1 Notation", "text": "Let X and A be positive integers. Let X = {1, 2,.., X} and A = {1, 2,..., A} be positive integers. Let \u2206 S be probability distributions via sentence S. A policy \u03c0 is a map from the state space to \u2206 A: \u03c0: X \u2192 \u0445 A. We use \u03c0 (a | x) to denote the probability of selecting an action in state x within the framework of politics \u03c0. A transition forecast kernel (or transition core) P: X \u00d7 A \u2192 X represents the direct product of the state and spheres of action. Let P \u03c0 denote the probability transition core within the framework of politics \u03c0. A loss function is a limited real evaluation function over states and spheres of action,: X \u00d7 A \u2192 [0, 1]. Let Mi,::, and M:, j denote the row and jst column of the matrix M. Let us, v = 1, c = I ci vi and vice versa, maxi = all with one vector each and one vci = each for each and one vector."}, {"heading": "1.2 Linear Programming Approach to Markov Decision Problems", "text": "Under certain assumptions, there is one skalare solution and one vector h-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "1.3 Approximations for Large State Spaces", "text": "The LP formulations (1) and (2) are impractical for major problems, since the number of variables and constraints increases linearly with the number of states. Schweitzer and Seidmann (1985) propose approximate linear programming formulas (ALP), methods that were later improved by de Farias and Van Roy (2003a, b), Hauskrecht and Kveton (2003), Guestrin et al. (2004), Petrik and Zilberstein (2009), Desai et al. (2012). As Desai et al. (2012) have pointed out, previous work on ALP either requires access to samples from a distribution that depends on optimal policies, or assumes that an LP with as many constraints as states can be solved. (See section 2 for a more detailed discussion.) Our goal is to design algorithms for very large MDPs that do not require knowledge of optimal policies."}, {"heading": "1.4 Problem definition", "text": "With the above notation, we can now explicitly address the problem that we solve. Definition 1 (Efficient Large-Scale Dual ALP) (Efficient Large-Scale Dual ALP) (Efficient Large-Scale Dual ALP) (Efficient Large-Scale Dual ALP) (Efficient Large-Scale Dual ALP) (Efficient Large-Scale Dual ALP) (Efficient Large-Scale Dual ALP) (Efficient Large-Scale Dual-Scale Dual ALP) (Efficient Large-Scale Dual-Scale Dual ALP) (Efficient-Scale Dual-Dual-Scale Dual-Dual ALP) (Efficient-Scale Dual-Dual-Efficient-Efficient-Dual ALP) (Efficient-Scale Dual-Dual-DALP) (Efficient-Scale Dual-Dual-DALP) (Efficient-Scale Dual-Dual-Scale Dual-DALP) (Efficient-Scale Dual-Scale Dual-Dual-DALP) (Efficient-Scale Dual-Dual-Scale Dual-Dual-DALP) (Efficient-Scale Dual-Dual-Scale Dual-DALP) (Efficient-Scale Dual-Dual-Scale Dual-Dual-DALP) (Efficient-Dual-Scale Dual-DALP) (Efficient-Dual-Scale Dual-DALP) (Efficient-Scale Dual-DALP) (Efficient-Scale Dual-Dual-Scale Dual-DALP) (Efficient-DALP) (Efficient-Scale Dual-DALP) (Efficient-Scale Dual-Scale DALP) (Efficient-Dual-DALP) (Efficient-Scale Dual-DALP) (Efficient-Scale Dual-Scale DALP) (Efficient-DALP) (Efficient-DALP) (Efficient-DALP) (Efficient-Scale D"}, {"heading": "1.5 Our Contributions", "text": "In this essay we introduce an algorithm that solves the extended efficient large dual ALP problem under the (standard) assumption that each policy quickly approaches its stationary distribution. Our algorithm accepts a constant S and a fault tolerance as input and has access to the various products listed in Definition 1. We define this using stochastic convex optimization. We prove that for all available measurement variables (0, 1), according to O (1 / 4) steps of the gradient decrease of the algorithm, a vector problem occurs with a probability of at least 1 \u2212 3% and a probability of at least 1 \u2212 4% and the measurement variables exceeding the measurement variables. The algorithm finds a vector problem that finds with a probability of at least 1 \u2212 4% and the measurement variables, the measurement variables, the measurement variables, the measurement variables, the measurement variables, the measurement variables and the measurement variables, the measurement variables, the measurement variables and the measurement variables, the measurement variables, the measurement variables, the measurement variables, the parameters, the measurement variables, the parameters, the parameters, the"}, {"heading": "2 Related Work", "text": "de Farias and Van Roy (x, a), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "3 A Reduction to Stochastic Convex Optimization", "text": "In this section, we describe our algorithm as a reduction of Markov decision problems to stochastic convex optimization. The main idea is to convert the ALP (3) into an unrestricted convex cost function by adding a function of constraint violations to the target and then performing stochastic gradient formation with an unbiased estimate of the course. (3) For a positive constant, we form the following convex cost function by adding a multiple of constraint violations to the target of the LP (3): c (3) = subject deviation with an unbiased estimate. (3) For a positive constant, we form the following convex cost function by adding a multiple of constraint violations. (3): c (3) = subject deviation. (3): (0 + subject deviation. (4) + subject deviation. (4) + subject deviation (4) + (positive) (H)."}, {"heading": "3.1 Analysis", "text": "We start with a discussion of the assumptions, which we then follow with the main theorem. We break down the evidence into two main ingredients. First, we show that a good approximation of the replacement loss yields a characteristic vector that is almost a stationary distribution; this is Lemma 2. Second, we justify the use of unbiased gradients in Theorem 3 and Lemma 5. The section concludes with the assumption that a hybrid assumption exists on the MDP, so that any policy converts quickly to its stationary distribution. Assumption A1 (Fast Mixing) For each policy there is a constant answer to the question of whether there is a constant answer to all distributions d and d."}, {"heading": "4 Sampling Constraints", "text": "In this section, we describe our second algorithm for the average cost of MDP problems. Using the results of the polytopic constraints sample (de Farias and Van Roy, 2004, Calafiore and Campi, 2005, Campi and Garatti, 2008), we reduce the solution for the dual ALP with the solution for a smaller, sampled LP. Basically, de Farias and Van Roy (2004) claim that a set of affinity constraints in Rd and some measures q about these constraints are also satisfactory if we consider k = O (d log (1 / 4) / 4 constraints, then with a probability of at least 1 \u2212 3 points that meet all these sampled constraints in Rd and some measures q about these constraints. This result is independent of the number of original constraints."}, {"heading": "4.1 Analysis", "text": "There is a vector that meets all constraints (L1 and L2.Validity of this assumption depends on the choice of functions v1 and v2. Larger functions ensure that this assumption is fulfilled, but as we show, this leads to greater errors (). If we decide, this leads to greater errors. () The next two lemmas apply Theorem 6 to the constraints L1 and L2. () Lemma 7. Let this assumption (0, 1) and 1 (1) (1). () If we opt for k1 (1 + log 2), then with probability at least 1 \u2212 v1, (x, a)."}, {"heading": "5 Experiments", "text": "In this section, we apply both algorithms to the four-dimensional discrete queue network shown in Figure 5. This network has a relatively long history; see, for example, Rybko and Stolyar (1992) and more recently de Farias and Van Roy (2003a) (c.f. Section 6.2). There are four queues, \u00b51,.., \u00b54, each with state 0,.., B. Since the cardinality of the state space is X = (1 + B) 4, even a modest B result in huge state spaces. For the time t, let Xt, X, the state and si, t, {0, 1}, i = 1, 3 denote whether queue i is served. Server 1 serves only queue 1 or 4, server 2 serves only queue or 3, and none can be idle."}, {"heading": "5.1 Stochastic Gradient Descent", "text": "We have executed our stochastic gradient descend algorithm with I = 1000 constraints and constraints sampled and the average gain H = 2. Our learning rate started at 10 \u2212 4 and halved every 2000 iterations. The results of our algorithm are plotted in Figure 5.1, where \"0 +\" indicates the current average of \"t.\" The left figure corresponds to the LP target, (0 +). The middle figure is the sum of the constraint violations, where \"0 +\" indicates the ongoing average of \"t.\" The middle figure is the sum of the first two diagrams. Finally, the right figure is the average losses, and the two horizontal lines correspond to the loss of the two heuristics, LONGER and LBFS. The right figure shows that, as predicted by our theory, minimizing the loss of \"Van\" and the difference from the \"Van circulation systems\" (all 200 distributions) does not work."}, {"heading": "5.2 Constraint Sampling", "text": "For the limitation sampling algorithm, we have sampled the simplex constraints uniformly with 10 different sample sizes: 508, 792, 1235, 1926, 3003, 4684, 7305, 11393, 17768 and 27712. Since XA = 4.1 \u043c 106, these sample sizes correspond to less than 1%. The stationary constraints were sampled in the same ratio (i.e. A times fewer samples). Let a1,., aAN and b1,. bN are the indices of the sampled simplex and stationary constraints, respectively. Explicit, which is sampledLP: Minimum constraints of the samplex size (26) s.t. (Non-samplex size constraints are the indices of the sampled simplex and stationary constraints of the stationary constraints."}, {"heading": "6 Conclusions", "text": "We have shown that, under certain assumptions about dynamics, the stochastic subgradient method produces a policy with average losses that is competitive for all \u03b8, not just all \u03b8, and generates a stationary distribution. We have demonstrated this algorithm on the Rybko Stoylar Network for Four-Dimensional Queues and recovered a policy that is better than two common heuristics and comparable to previous results on ALPs de Farias and Van Roy (2003a). A future direction is to find other interesting regularity conditions under which we can deal with major MDP problems. We also plan to conduct further experiments with challenging large-scale problems."}, {"heading": "7 Acknowledgements", "text": "We would like to thank NSF for its support by granting CCF-1115788 and ARC an Australian Research Council Australian Laureate Fellowship (FL110100281)."}], "references": [{"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Athena Scientific,", "citeRegEx": "Bellman.,? \\Q2012\\E", "shortCiteRegEx": "Bellman.", "year": 2012}, {"title": "Uncertain convex programs: randomized solutions and confidence levels", "author": ["G. Calafiore", "M.C. Campi"], "venue": "Mathematical Programming,", "citeRegEx": "Calafiore and Campi.,? \\Q2005\\E", "shortCiteRegEx": "Calafiore and Campi.", "year": 2005}, {"title": "The exact feasibility of randomized solutions of uncertain convex programs", "author": ["M.C. Campi", "S. Garatti"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Campi and Garatti.,? \\Q2008\\E", "shortCiteRegEx": "Campi and Garatti.", "year": 2008}, {"title": "The linear programming approach to approximate dynamic programming", "author": ["D.P. de Farias", "B. Van Roy"], "venue": "Operations Research,", "citeRegEx": "Farias and Roy.,? \\Q2003\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2003}, {"title": "Approximate linear programming for average-cost dynamic programming", "author": ["D.P. de Farias", "B. Van Roy"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Farias and Roy.,? \\Q2003\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2003}, {"title": "On constraint sampling in the linear programming approach to approximate dynamic programming", "author": ["D.P. de Farias", "B. Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Farias and Roy.,? \\Q2004\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2004}, {"title": "A cost-shaping linear program for average-cost approximate dynamic programming with performance guarantees", "author": ["D.P. de Farias", "B. Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Farias and Roy.,? \\Q2006\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2006}, {"title": "Self-normalized processes: Limit theory and Statistical Applications", "author": ["V.H. de la Pe\u00f1a", "T.L. Lai", "Q-M. Shao"], "venue": null, "citeRegEx": "Pe\u00f1a et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pe\u00f1a et al\\.", "year": 2009}, {"title": "Approximate dynamic programming via a smoothed linear program", "author": ["V.V. Desai", "V.F. Farias", "C.C. Moallemi"], "venue": "Operations Research,", "citeRegEx": "Desai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Desai et al\\.", "year": 2012}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A.D. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Solving factored mdps with continuous and discrete variables", "author": ["C. Guestrin", "M. Hauskrecht", "B. Kveton"], "venue": "In Twentieth Conf. Uncertainty in Artificial Intelligence,", "citeRegEx": "Guestrin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2004}, {"title": "Linear program approximations to factored continuous-state markov decision processes", "author": ["M. Hauskrecht", "B. Kveton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hauskrecht and Kveton.,? \\Q2003\\E", "shortCiteRegEx": "Hauskrecht and Kveton.", "year": 2003}, {"title": "Convergent temporal-difference learning with arbitrary smooth function approximation", "author": ["H.R. Maei", "Cs. Szepesv\u00e1ri", "S. Bhatnagar", "D. Precup", "D. Silver", "R.S. Sutton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Maei et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maei et al\\.", "year": 2009}, {"title": "Toward off-policy learning control with function approximation", "author": ["H.R. Maei", "Cs. Szepesv\u00e1ri", "S. Bhatnagar", "R.S. Sutton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Maei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maei et al\\.", "year": 2010}, {"title": "Linear programming and sequential decisions", "author": ["A.S. Manne"], "venue": "Management Science,", "citeRegEx": "Manne.,? \\Q1960\\E", "shortCiteRegEx": "Manne.", "year": 1960}, {"title": "Constraint relaxation in approximate linear programs", "author": ["M. Petrik", "S. Zilberstein"], "venue": "In Proc. 26th Internat. Conf. Machine Learning (ICML),", "citeRegEx": "Petrik and Zilberstein.,? \\Q2009\\E", "shortCiteRegEx": "Petrik and Zilberstein.", "year": 2009}, {"title": "Ergodicity of stochastic processes describing the operation of open queueing networks", "author": ["A.N. Rybko", "A.L. Stolyar"], "venue": "Problemy Peredachi Informatsii,", "citeRegEx": "Rybko and Stolyar.,? \\Q1992\\E", "shortCiteRegEx": "Rybko and Stolyar.", "year": 1992}, {"title": "Generalized polynomial approximations in Markovian decision processes", "author": ["P. Schweitzer", "A. Seidmann"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "Schweitzer and Seidmann.,? \\Q1985\\E", "shortCiteRegEx": "Schweitzer and Seidmann.", "year": 1985}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Cs. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation", "author": ["R.S. Sutton", "Cs. Szepesv\u00e1ri", "H.R. Maei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Approximate linear programming for average cost mdps", "author": ["M.H. Veatch"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Veatch.,? \\Q2013\\E", "shortCiteRegEx": "Veatch.", "year": 2013}, {"title": "Dual representations for dynamic programming", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 14, "context": "The MDP problem can also be stated in the LP formulation (Manne, 1960),", "startOffset": 57, "endOffset": 70}, {"referenceID": 13, "context": "Schweitzer and Seidmann (1985) propose approximate linear programming (ALP) formulations.", "startOffset": 0, "endOffset": 31}, {"referenceID": 9, "context": "These methods were later improved by de Farias and Van Roy (2003a,b), Hauskrecht and Kveton (2003), Guestrin et al.", "startOffset": 70, "endOffset": 99}, {"referenceID": 9, "context": "These methods were later improved by de Farias and Van Roy (2003a,b), Hauskrecht and Kveton (2003), Guestrin et al. (2004), Petrik and Zilberstein (2009), Desai et al.", "startOffset": 100, "endOffset": 123}, {"referenceID": 9, "context": "These methods were later improved by de Farias and Van Roy (2003a,b), Hauskrecht and Kveton (2003), Guestrin et al. (2004), Petrik and Zilberstein (2009), Desai et al.", "startOffset": 100, "endOffset": 154}, {"referenceID": 8, "context": "(2004), Petrik and Zilberstein (2009), Desai et al. (2012). As noted by Desai et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 8, "context": "(2004), Petrik and Zilberstein (2009), Desai et al. (2012). As noted by Desai et al. (2012), the prior work on ALP either requires access to samples from a distribution that depends on optimal policy or assumes the ability to solve an LP with as many constraints as states.", "startOffset": 39, "endOffset": 92}, {"referenceID": 8, "context": "Desai et al. (2012) study a smoothed version of ALP, in which slack variables are introduced that allow for some violation of the constraints.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Desai et al. (2012) prove that if w\u2217 is a solution to above problem, then \u2016J\u2217 \u2212\u03a8w\u2217\u20161,c \u2264 inf w,u\u2208U \u2016J\u2217 \u2212\u03a8w\u2016\u221e,1/u ( cu+ 2(\u03bc\u03c0\u2217,cu)(1 + \u03b2u) 1\u2212 \u03b3 ) .", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "Similar methods are also proposed by Petrik and Zilberstein (2009). One problem with this result is that c is defined in terms of w\u2217, which itself depends on c.", "startOffset": 37, "endOffset": 67}, {"referenceID": 8, "context": "Desai et al. (2012) also propose a computationally efficient algorithm.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Desai et al. (2012) prove high probability bounds on the approximation error \u2016J\u2217 \u2212\u03a8\u0175\u20161,c.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Desai et al. (2012) prove high probability bounds on the approximation error \u2016J\u2217 \u2212\u03a8\u0175\u20161,c. However, it is no longer clear if a performance bound on \u2016J\u2217 \u2212 J\u03c0\u03a8\u0175\u20161,c can be obtained from this approximation bound. Next, we turn our attention to average cost ALP, which is a more challenging problem and is also the focus of this paper. Let \u03bd be a distribution over states, u : X \u2192 [1,\u221e), \u03b7 > 0, \u03b3 \u2208 [0, 1], P \u03c0 \u03b3 = \u03b3P \u03c0 + (1 \u2212 \u03b3)1\u03bd\u22a4, and L\u03b3h = min\u03c0(l\u03c0 + P \u03c0 \u03b3 h). de Farias and Van Roy (2006) propose the following optimization problem:", "startOffset": 0, "endOffset": 488}, {"referenceID": 21, "context": "Similar results are obtained more recently by Veatch (2013). An appropriate choice for vector \u03bd is \u03bd = \u03bc\u03b3,w\u2217.", "startOffset": 46, "endOffset": 60}, {"referenceID": 21, "context": "Similar results are obtained more recently by Veatch (2013). An appropriate choice for vector \u03bd is \u03bd = \u03bc\u03b3,w\u2217. Unfortunately, w\u2217 depends on \u03bd. We should also note that solving (10) can be computationally expensive. de Farias and Van Roy (2006) propose constraint sampling techniques similar to (de Farias and Van Roy, 2004), but no performance bounds are provided.", "startOffset": 46, "endOffset": 243}, {"referenceID": 21, "context": "Similar results are obtained more recently by Veatch (2013). An appropriate choice for vector \u03bd is \u03bd = \u03bc\u03b3,w\u2217. Unfortunately, w\u2217 depends on \u03bd. We should also note that solving (10) can be computationally expensive. de Farias and Van Roy (2006) propose constraint sampling techniques similar to (de Farias and Van Roy, 2004), but no performance bounds are provided. Wang et al. (2008) study ALP (3) and show that there is a dual form for standard value function based algorithms, including on-policy and off-policy updating and policy improvement.", "startOffset": 46, "endOffset": 383}, {"referenceID": 9, "context": "1 of Flaxman et al. (2005) for stochastic convex optimization, is sufficient.", "startOffset": 5, "endOffset": 27}, {"referenceID": 23, "context": "By Theorem 1 of Zinkevich (2003),", "startOffset": 16, "endOffset": 33}, {"referenceID": 7, "context": "Let St = \u2211t\u22121 s=1(z\u2217\u2212zs)\u03b7s, which is a self-normalized sum (de la Pe\u00f1a et al., 2009). By Corollary 3.8 and Lemma E.3 of Abbasi-Yadkori (2012), we get that for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, |St| \u2264 \u221a\u221a\u221a\u221a ( 1 + t\u22121 \u2211", "startOffset": 66, "endOffset": 142}, {"referenceID": 1, "context": "Using the results on polytope constraint sampling (de Farias and Van Roy, 2004, Calafiore and Campi, 2005, Campi and Garatti, 2008), we reduce approximate the solution to the dual ALP with the solution to a smaller, sampled LP. Basically, de Farias and Van Roy (2004) claim that given a set of affine constraints in Rd and some measure q over these constraints, if we sample k = O(d log(1/\u03b4)/\u01eb) constraints, then with probability at least 1 \u2212 \u03b4, any point that satisfies all of these k sampled constraints also satisfies 1 \u2212 \u01eb of the original set of constraints under measure q.", "startOffset": 80, "endOffset": 268}, {"referenceID": 1, "context": "Using the results on polytope constraint sampling (de Farias and Van Roy, 2004, Calafiore and Campi, 2005, Campi and Garatti, 2008), we reduce approximate the solution to the dual ALP with the solution to a smaller, sampled LP. Basically, de Farias and Van Roy (2004) claim that given a set of affine constraints in Rd and some measure q over these constraints, if we sample k = O(d log(1/\u03b4)/\u01eb) constraints, then with probability at least 1 \u2212 \u03b4, any point that satisfies all of these k sampled constraints also satisfies 1 \u2212 \u01eb of the original set of constraints under measure q. This result is independent of the number of original constraints. Let L be a family of affine constraints indexed by i: constraint i is satisfied at point w \u2208 Rd if ai w + bi \u2265 0. Let I be the family of constraints by selecting k random constraints in L with respect to measure q. Theorem 6 (de Farias and Van Roy (2004)).", "startOffset": 80, "endOffset": 900}, {"referenceID": 16, "context": "Rybko and Stolyar (1992) and more recently de Farias and Van Roy (2003a) (c.", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "Rybko and Stolyar (1992) and more recently de Farias and Van Roy (2003a) (c.", "startOffset": 0, "endOffset": 73}, {"referenceID": 16, "context": "Rybko and Stolyar (1992) and more recently de Farias and Van Roy (2003a) (c.f. section 6.2). There are four queues, \u03bc1, . . . , \u03bc4, each with state 0, . . . , B. Since the cardinality of the state space is X = (1 +B) 4, even a modest B results in huge state-spaces. For time t, let Xt \u2208 X be the state and si,t \u2208 {0, 1}, i = 1, 2, 3, 3 denote whether queue i is being served. Server 1 only serves queue 1 or 4, server 2 only serves queue 2 or 3, and neither server can idle. Thus, s1,t + s4,t = 1 and s2,t + s3,t = 1. The dynamics are as follows. At each time t, the following random variables are sampled independently: A1,t \u223c Bernoulli(a1), A3,t \u223c Bernoulli(a3), and Di,t \u223c Bernoulli(di \u2217 si,t) for i = 1, 2, 3, 4. Using e1, . . . , e4 to denote the standard basis vectors, the dynamics are: X \u2032 t+1 =Xt +A1,te1 +A3,te3 +D1,t(e2 \u2212 e1)\u2212D2,te2 +D3,t(e4 \u2212 e3)\u2212D4,te4, and Xt+1 = max(0,min((B),X \u2032 t+1)) (i.e. all four states are thresholded from below by 0 and above by B). The loss function is the total queue size: l(Xt) = ||Xt||1. We compared our method against two common heuristics. In the first, denoted LONGER, each server operates on the queue that is longer with ties broken uniformly at random (e.g. if queue 1 and 4 had the same size, they are equally likely to be served). In the second, denoted LBFS (last buffer first served), the downstream queues always have priority (server 1 will serve queue 4 unless it has length 0, and server 2 will serve queue 2 unless it has length 0). These heuristics are common and have been used an benchmarks for queueing networks (e.g. de Farias and Van Roy (2003a)).", "startOffset": 0, "endOffset": 1612}], "year": 2014, "abstractText": "We consider the problem of controlling a Markov decision process (MDP) with a large state space, so as to minimize average cost. Since it is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing with a lowdimensional family of policies. We use the dual linear programming formulation of the MDP average cost problem, in which the variable is a stationary distribution over state-action pairs, and we consider a neighborhood of a low-dimensional subset of the set of stationary distributions (defined in terms of state-action features) as the comparison class. We propose two techniques, one based on stochastic convex optimization, and one based on constraint sampling. In both cases, we give bounds that show that the performance of our algorithms approaches the best achievable by any policy in the comparison class. Most importantly, these results depend on the size of the comparison class, but not on the size of the state space. Preliminary experiments show the effectiveness of the proposed algorithms in a queuing application.", "creator": "LaTeX with hyperref package"}}}