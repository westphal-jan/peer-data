{"id": "1402.6964", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2014", "title": "Scalable Methods for Nonnegative Matrix Factorizations of Near-separable Tall-and-skinny Matrices", "abstract": "Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms efficient for data matrices that have many more rows than columns, so-called \"tall-and-skinny matrices\". One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need a single pass over the data matrix and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized synthetic matrices and real-world matrices from scientific computing and bioinformatics.", "histories": [["v1", "Thu, 27 Feb 2014 16:41:26 GMT  (224kb,D)", "http://arxiv.org/abs/1402.6964v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC cs.NA stat.ML", "authors": ["austin r benson", "jason d lee", "bartek rajwa", "david f gleich"], "accepted": true, "id": "1402.6964"}, "pdf": {"name": "1402.6964.pdf", "metadata": {"source": "CRF", "title": "Scalable methods for nonnegative matrix factorizations of near-separable tall-and-skinny matrices", "authors": ["Austin R. Benson", "Jason D. Lee", "Bartek Rajwa", "David F. Gleich"], "emails": ["arbenson@stanford.edu", "jdl17@stanford.edu", "brajwa@purdue.edu", "dgleich@purdue.edu"], "sections": [{"heading": null, "text": "Categories and Subject Descriptions G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.1.6 [Numerical Analysis]: OptimizationGeneral Terms AlgorithmKeywords Nonnegative Matrix Factorization, Separable, QR, SVD, MapReduce, Heat Transfer, Flow Cytometry"}, {"heading": "1. NONNEGATIVE MATRIX FACTORIZATIONS AT SCALE", "text": "This year, we will be able to establish ourselves in the region, \"he said in an interview with the Deutsche Presse-Agentur.\" We will be able to solve the problems, \"he said,\" but we will not be able to solve them, \"he told the Deutsche Presse-Agentur."}, {"heading": "1.1 Separable NMF", "text": "Let us first turn to the question of how to calculate the factorization in Equation (1) in practice. To make the problem comprehensible, we make assumptions about Xiv: 140 2.69 64v1 [cs.LG] 2 7Fe b20 14the data. In particular, we need a separability condition on the matrix. A nonnegative matrix X is r-separable ifX = X (:, K) H, where K is an index with | K | = r and X (:, K) is matlab notation for the matrix X limited to columns indexed by K. Since the coefficients of H are not negative, all columns of X live in the conical shell of the \"extreme\" columns indexed by K."}, {"heading": "1.2 Alternative NMF algorithms", "text": "There are several approaches to solving the equation (1) that do not start from the separability condition. These algorithms typically work with block coordinate descent and optimize over W and H while fixing a factor at the same time. Examples are the groundbreaking work of Lee and Seung [27], alternately smallest squares [11, 22] and fast projection-based smallest squares [21]. Some of these methods are used in MapReduce architectures on a scale [26]. Alternating methods require updating the whole factor W or H after each optimization step. If one of the factors is large, repeated updates can be prohibitively expensive. In contrast, we show in sections 2 and 3 that the matrices do not require large separation algorithms."}, {"heading": "2. ALGORITHMS AND DIMENSION REDUCTION FOR NEAR-SEPARABLE", "text": "NMF There are several popular algorithms for nearly separable NMF that are motivated by convex geometry. The aim of this section is to show that when X is large and thin, we can apply dimension reduction techniques so that established algorithms can execute on n \u00b7 n matrices instead of the original m \u00b7 n. Our new dimension reduction method in Section 2.2 is also motivated by convex geometry. In Section 3, we use dimension reduction in scalable algorithms."}, {"heading": "2.1 Geometric algorithms", "text": "There are two geometric strategies that are typically applied to nearly separable NMF. The first deals with conical hulls. A cone C-Rm is a non-empty convex with C-R +, xi-Rm, which xi generate vectors. In separable NMF, X-X (:, K) implies that all columns of X are in the cone generated by columns indexed by K-columns. For each k column R-K, {\u03b1X (:, k) | \u03b1 > 0} is an extreme beam of this cone. The goal of the XRAY algorithm [25] is to find these extreme beams. \u2212 In particular, the greedy variant of XRAY chooses the maximum column R-J-RTX (:, j)."}, {"heading": "2.2 Orthogonal transformations", "text": "It is easy to show that x is an extreme beam of C when and only when Mx is an extreme point of MS. We use these facts by applying specific orthogonal transformations as the reversible column M. Let X = QR technique and X = UD factorization and singular value substitution (SVD) of X so that Q and U apply m-orthogonal transformations as the immutable column M. Let X = QR-V T be the full QR factorization and singular value substitution (SVD) of X, so that Q and U are m-m orthogonal matrices (and therefore non-singular). ThenQTX = (R-0), UTX = (V-T 0), where R-R and T-N blocks of R are."}, {"heading": "2.3 Gaussian projection", "text": "An alternative method of dimension reduction is random Gaussian projection, and the idea has also been applied to hyper-spectral separation problems [6]. In the literature on hyper-spectral separation, separability is referred to as a pure pixel assumption, and the random projections are also motivated by convex geometry [8]. In particular, for a matrix G-Rm-k with Gaussian i.i.d. entries, the extreme columns of X are taken as the extreme columns of GTX, which the dimension k-n. The algorithm assumes that the columns of X are normalized to one. In other words: X (:, i) x (:, i) / | | X (:, i) | 1, i = 1,... In Section 3.3 we show how to run the algorithm in one pass over the data matrix, even if the columns are not standardized."}, {"heading": "2.4 Computing H", "text": "The selection of extreme columns indexed by K completes half of the NMF factorization in Equation (1). How do we calculate H? We wantH = arg min Y (:, i) = arg min y (:, K) y (:, i) y-X (:, i = 1,.. Leave X = QR with the upper n block of R (:, i) = arg min y (:, K) y-X (:, i) e-X (:, i), i = 1,."}, {"heading": "3. IMPLEMENTATION", "text": "Remarkably, for a high-resolution matrix, only one pass over the data matrix and an implementation of MapReduce is sufficient to achieve optimal communication.While all algorithms use a complex calculation, these routines are invoked only with matrices of size n \u00b7 n.This gives us extremely scalable implementations.Algorithm 1 MapReduce Gaussian projection for NMFfunction map (key k, matrix series xTi) sample column vector gi \u0445 N (0, Ik).For each line rk of gix T i doemit (k, rk) terminal function Reduce (key k, matrix series < rk >) emit (k, sum (< rk >)) terminal function"}, {"heading": "3.1 TSQR and R-SVD", "text": "The thin QR factorization of an m \u00b7 n real-weighted matrix A with m > n isA = QRwhere Q is an m \u00b7 n orthogonal matrix and R is an n \u00b7 n upper triangular matrix. This is exactly the factorization we use in Section 2. For our purposes, QT is implicitly applied, and we only need to calculate R. when m n communication-optimized algorithms for calculating the factorization are called TSQR [15]. TSQR, however, is implemented in several environments, including MapReduce [5, 12], distributed memory MPI [15], GPUs [2], and grid computing [1]. All of these methods avoid the calculation of ATA and are therefore numerically stable. The dimension reduction techniques in Section 2 are independent of the platform. However, as explained in Section 3.4, we use MapReduce to calculate the target data."}, {"heading": "3.2 Gaussian projection", "text": "For the implementation of the Gaussian projection in Section 2.3, we assume that we can quickly scan Gaussian Gaussians in parallel, assuming that the transformation is easy to implement in MapReduce. For each line of the data matrix xTi, the card function calculates the outer product gix T i, where gi consists of n sampled Gaussians. The card function returns a key-value pair for each line of this outer product, with the key being the line number. The reduction function simply summarizes all lines with the same key. Algorithm 1 contains the functions. In theory and practice, all outer products are added to a single card process before key-value pairs are emitted. The function that performs the aggregation is called a combinator.In practice, we can only create Gaussian Gaussians on a single processor."}, {"heading": "3.3 Column normalization", "text": "The convex hull algorithms in Section 2.1 and the Gaussian projection algorithm in Section 2.3 require normalization of the columns of data matrix X. Naively implementing the column normalization for the convex hull algorithms in a MapReduce- or streaming environment would result in: 1. Read X and calculate the column norms. 2. Read X, normalize the columns and write the normalized data to disk.3. Use TSQR on the normalized matrix. To do this, read the data matrix twice and write O (mn) data to the disk once to normalize the columns only. A better approach is a single step: 1. Use TSQR on the unnormalized data X and simultaneously calculate the column norms of X.Let D be the diagonal matrix of the column norms. Note that X = QR 1 \u2212 Q (RD = GTD-D-1) is transferred to the matrix-1."}, {"heading": "3.4 MapReduce motivation", "text": "For the experiments in this paper, we use a MapReduce implementation for the NMF algorithms introduced in Section 2.1 using the dimension reduction techniques in Section 2.2. Our core computational core is high-resolution QR factorization (TSQR), which has been optimized for multiple architectures (see the references in Section 3.1). Therefore, our ideas in this paper are not limited to MapReduce architectures. Nevertheless, MapReduce remains a popular framework for data-intensive computing for several reasons. Firstly, many large data sets are already stored in MapReduce clusters. Since the cluster runs algorithms, the data does not need to be transferred to another computer. However, the algorithms in this paper need only one pass over the data. Since runtime is dominated by the cost of loading data from disk to main memory, the time for data transfer may be as long as the simple execution of the algorithm directly on the cluster."}, {"heading": "3.5 Communication costs for NMF on MapReduce", "text": "There are two communication costs that we analyze for MapReduce: the first is the time it takes to read the input data. In Hadoop, the data is stored on hard disk, and loading the data is often the predominant cost factor in numerical algorithms; the second is the time spent using the MapReduce method, roughly measured by the number and size of key-value pairs sorted in the shuffle step. Current implementations of TSQR and R-SVD in MapReduce can calculate R or VT in a single MapReduce iteration [5]. For dimension reduction, the data matrix needs to be read only once. Although algorithms like Hott Topixx, SPA, and Gaussian projection require normalized columns, we have shown that the column norms can be calculated at the same time as TSQR (see Section 3.3). For Gaussian preproduction, we cannot calculate the factor in the same H projected space."}, {"heading": "4. TESTING ON SYNTHETIC MATRICES", "text": "In this section, we test our dimension reduction techniques on high and thin matrices that are synthetically generated to be separable or near-separable. All experiments were conducted on a 10-node, 40-core MapReduce cluster at Stanford's Institute for Computational and Mathematical Engineering (ICME). Each node has 6 2TB hard drives, 24 GB of RAM, and a single Intel Core i7-960 3.2 GHz processor, connected via Gigabit ethernet. We test the following three algorithms: 1. Dimensional reduction with SVD followed by SPA.2. Dimensional reduction with SVD followed by the greedy variant of the XRAY algorithm. The greedy method is not exactly separable, but works well in practice. 3. Gaussian projection as described in Section 2.3.0."}, {"heading": "5. APPLICATIONS", "text": "We are now testing our algorithms and implementations on the basis of scientific data sets, using the same algorithms and calculation system configurations as in Section 4. The data is not negative, but we do not know from the outset that the data is separable."}, {"heading": "5.1 Heat transfer simulation data", "text": "The heat transfer data contains the simulated heat in a high-conducting stainless steel block with a low-conducting bubble inserted into the block [13]. Each column of the matrix corresponds to the simulation results for a bubble in a different radius. Multiple simulations for random bubbles are included in one column. Each row corresponds to a three-dimensional spatial coordinate, a time step and a bubble position. An entry of the matrix is the temperature of the block in a single column, a time step, a bubble position and a bubble radius. The matrix is constructed so that the columns are close to 64 color variability in the data - this is responsible for additional \"shunting\" structures. Therefore, we intuitively expect that the NMF algorithms select closer to the end of the matrix."}, {"heading": "5.2 Flow cytometry", "text": "The phenotype and function of individual cells can be identified by decoding these tag combinations. The analyzed dataset contains measurements of 40,000 single cells. Measurements of the fluorescence intensity, which conveys abundance information, were collected on five different bands corresponding to FITC, PE, ECD, PC5 and PC7. Fluorescence labels indicate antibodies to CD4, CD8, CD45 and CD3 epicenters. Results are presented as data matrix A of size 40, 000 x 5. Our interest in the presented analysis was to investigate paired interactions in the data (cell versus cell and marker versus marker)."}, {"heading": "6. DISCUSSION", "text": "We have demonstrated how to efficiently calculate nonnegative matrix factorizations for nearly separable large and thin matrices. Our most important tool was TSQR, and our algorithms had to read the data matrix only once. By reducing the dimension of the problem, we can easily calculate the effectiveness of factorizations for multiple values of the separation range r. With these tools, we have calculated the largest divisible nonnegative matrix factorizations to date. Furthermore, our algorithms provide new insights into massive scientific datasets. The coefficient matrix H revealed structure in the results of heat transfer simulations. Extreme column selection in flow cytometry showed that one of the labels used in measurements could be redundant. In future work, we would be happy to analyze these additional large scientific datasets. We also plan to test additional NMF algorithms. The practical limitations of our algorithm are dictated by the high and thin requirement for these datasets, where we assume that 5000 is too large to manipulate."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "Austin R. Benson is supported by an Office of Technology Licensing Stanford Graduate Fellowship. Jason D. Lee is supported by an Office of Technology Licensing Stanford Graduate Fellowship and a National Science Foundation Graduate Research Fellowship. We thank Anil Damle and Yuekai Sun for helpful conversations."}, {"heading": "8. REFERENCES", "text": "[1] E. Agullo, C. Coti, J. Dongarra, T. Herault, andJ. Long QR factorization of tall and skinny matrices in a grid computing environment. In Parallel & Distributed Processing (IPDPS), 2010 IEEE International Symposium on, pp. 1-11. IEEE International, 2010. [2] M. Anderson, G. Ballard, J. Demmel, and K. Keutzer L. Communication-avoiding QR decomposition for gpus J. J. S. In Parallel & Distributed Processing Symposium on, pp. 1-11. IEEE International, pp. 48-58. IEEE tall, 2011. [3] M. C. U. Arau. Xijo, T. C. Saldanha, R. K. H. Galva, T. Yoneyama, H. C. Chame, and V. Visani. The successive projections algorithm for variable selection in spectroscopic multicomponent analysis."}], "references": [{"title": "QR factorization of tall and skinny matrices in a grid computing environment", "author": ["E. Agullo", "C. Coti", "J. Dongarra", "T. Herault", "J. Langem"], "venue": "In Parallel & Distributed Processing (IPDPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Communication-avoiding QR decomposition for gpus", "author": ["M. Anderson", "G. Ballard", "J. Demmel", "K. Keutzer"], "venue": "In Parallel & Distributed Processing Symposium (IPDPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "The successive projections algorithm for variable selection in spectroscopic multicomponent analysis", "author": ["M.C.U. Ara\u00fajo", "T.C.B. Saldanha", "R.K.H. Galv\u00e3o", "T. Yoneyama", "H.C. Chame", "V. Visani"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Computing a nonnegative matrix factorization\u2013provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "In Proceedings of the 44th symposium on Theory of Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Direct QR factorizations for tall-and-skinny matrices in MapReduce architectures", "author": ["A.R. Benson", "D.F. Gleich", "J. Demmel"], "venue": "IEEE International Conference on Big Data,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "An overview on hyperspectral unmixing: geometrical, statistical, and sparse regression based approaches", "author": ["J.M. Bioucas-Dias", "A. Plaza"], "venue": "In Geoscience and Remote Sensing Symposium (IGARSS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Factoring nonnegative matrices with linear programs", "author": ["V. Bittorf", "B. Recht", "C. Re", "J.A. Tropp"], "venue": "arXiv preprint arXiv:1206.1270,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Automating spectral unmixing of aviris data using convex geometry concepts", "author": ["J.W. Boardman"], "venue": "In Summaries 4th Annu. JPL Airborne Geoscience Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "An improved algorithm for computing the singular value decomposition", "author": ["T.F. Chan"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "On the compression of low rank matrices", "author": ["H. Cheng", "Z. Gimbutas", "P. Martinsson", "V. Rokhlin"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Regularized alternating least squares algorithms for non-negative matrix/tensor factorization", "author": ["A. Cichocki", "R. Zdunek"], "venue": "In Advances in Neural Networks\u2013ISNN", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Tall and skinny QR factorizations in MapReduce architectures", "author": ["P.G. Constantine", "D.F. Gleich"], "venue": "In Proceedings of the second international workshop on MapReduce and its applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Model reduction with mapreduce-enabled tall and skinny singular value decomposition", "author": ["P.G. Constantine", "D.F. Gleich", "Y. Hou", "J. Templeton"], "venue": "arXiv preprint arXiv:1306.4690,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "A randomized algorithm for separable non-negative matrix factorization", "author": ["A. Damle", "Y. Sun"], "venue": "Technical report, Stanford University,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Communication-optimal parallel and sequential QR and LU factorizations", "author": ["J. Demmel", "L. Grigori", "M. Hoemmen", "J. Langou"], "venue": "SIAM J. Sci. Comp.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts? In Advances in neural information processing", "author": ["D. Donoho", "V. Stodden"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization", "author": ["N. Gillis", "R. Luce"], "venue": "ArXiv e-prints,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Fast and robust recursive algorithms for separable nonnegative matrix factorization", "author": ["N. Gillis", "S. Vavasis"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Non-negative matrix factorization for face recognition", "author": ["D. Guillamet", "J. Vitri\u00e0"], "venue": "In Topics in Artificial  Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Constrained nonnegative matrix factorization for hyperspectral unmixing", "author": ["S. Jia", "Y. Qian"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Fast projection-based methods for the least squares nonnegative matrix approximation problem", "author": ["D. Kim", "S. Sra", "I.S. Dhillon"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework", "author": ["J. Kim", "Y. He", "H. Park"], "venue": "Journal of Global Optimization,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Sparse nonnegative matrix factorization for protein sequence motif discovery", "author": ["W. Kim", "B. Chen", "J. Kim", "Y. Pan", "H. Park"], "venue": "Expert Systems with Applications,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Symmetric nonnegative matrix factorization for graph clustering", "author": ["D. Kuang", "H. Park", "C.H. Ding"], "venue": "In SDM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Fast conical hull algorithms for near-separable non-negative matrix factorization", "author": ["A. Kumar", "V. Sindhwani", "P. Kambadur"], "venue": "arXiv preprint arXiv:1210.1190,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Distributed nonnegative matrix factorization for web-scale dyadic data analysis on mapreduce", "author": ["C. Liu", "H.-c. Yang", "J. Fan", "L.-W. He", "Y.-M. Wang"], "venue": "In Proceedings of the 19th international conference on World wide web,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D. Seung", "L. Lee"], "venue": "Advances in neural information processing systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "On the complexity of nonnegative matrix factorization", "author": ["S. Vavasis"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M. Franklin", "S. Shenker", "I. Stoica"], "venue": "In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "the singular value decomposition and the interpolative decomposition [10], the advantage of NMF is in interpretability of the data.", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "A common example is facial image decomposition [19].", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "For this reason, among a host of other reasons, NMF is used in a broad range of applications including graph clustering [24], protein sequence motif discovery [23], and hyperspectral unmixing [20].", "startOffset": 120, "endOffset": 124}, {"referenceID": 22, "context": "For this reason, among a host of other reasons, NMF is used in a broad range of applications including graph clustering [24], protein sequence motif discovery [23], and hyperspectral unmixing [20].", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "For this reason, among a host of other reasons, NMF is used in a broad range of applications including graph clustering [24], protein sequence motif discovery [23], and hyperspectral unmixing [20].", "startOffset": 192, "endOffset": 196}, {"referenceID": 5, "context": "We compare this method with a Gaussian transformation technique from the hyperspectral unmixing community [6, 8].", "startOffset": 106, "endOffset": 112}, {"referenceID": 7, "context": "We compare this method with a Gaussian transformation technique from the hyperspectral unmixing community [6, 8].", "startOffset": 106, "endOffset": 112}, {"referenceID": 4, "context": "Since tall-and-skinny matrices are amenable to streaming computations [5, 12], we present a MapReduce implementation in Section 3.", "startOffset": 70, "endOffset": 77}, {"referenceID": 11, "context": "Since tall-and-skinny matrices are amenable to streaming computations [5, 12], we present a MapReduce implementation in Section 3.", "startOffset": 70, "endOffset": 77}, {"referenceID": 27, "context": "Unfortunately, for a fixed non-negative rank r, finding the factors W and H for which the residual \u2016X \u2212WH\u2016 is minimized is NP-complete [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "The idea of separability was developed by Donoho and Stodden [16], and recent work has produced tractable NMF algorithms by assuming that X almost satisfies a separability condition [4, 7, 17].", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "The idea of separability was developed by Donoho and Stodden [16], and recent work has produced tractable NMF algorithms by assuming that X almost satisfies a separability condition [4, 7, 17].", "startOffset": 182, "endOffset": 192}, {"referenceID": 6, "context": "The idea of separability was developed by Donoho and Stodden [16], and recent work has produced tractable NMF algorithms by assuming that X almost satisfies a separability condition [4, 7, 17].", "startOffset": 182, "endOffset": 192}, {"referenceID": 16, "context": "The idea of separability was developed by Donoho and Stodden [16], and recent work has produced tractable NMF algorithms by assuming that X almost satisfies a separability condition [4, 7, 17].", "startOffset": 182, "endOffset": 192}, {"referenceID": 26, "context": "Examples include the seminal work by Lee and Seung [27], alternating least squares [11, 22], and fast projection-based least squares [21].", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "Examples include the seminal work by Lee and Seung [27], alternating least squares [11, 22], and fast projection-based least squares [21].", "startOffset": 83, "endOffset": 91}, {"referenceID": 21, "context": "Examples include the seminal work by Lee and Seung [27], alternating least squares [11, 22], and fast projection-based least squares [21].", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "Examples include the seminal work by Lee and Seung [27], alternating least squares [11, 22], and fast projection-based least squares [21].", "startOffset": 133, "endOffset": 137}, {"referenceID": 25, "context": "Some of these methods are used in MapReduce architectures at scale [26].", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "The goal of the XRAY algorithm [25] is to find these extreme rays.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "Popular approaches in the context of NMF include the Successive Projection Algorithm (SPA, [3]) and its generalization [18].", "startOffset": 91, "endOffset": 94}, {"referenceID": 17, "context": "Popular approaches in the context of NMF include the Successive Projection Algorithm (SPA, [3]) and its generalization [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 6, "context": "Another alternative, based on linear programming, is Hott Topixx [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "An alternative dimension reduction technique is random Gaussian projections, and the idea has been used in hyperspectral unmixing problems [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 7, "context": "In the hyperspectral unmixing literature, the separability is referred to as the pure-pixel assumption, and the random projections are also motivated by convex geometry [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 13, "context": "Recent work shows that when X is r-separable and k = O(r log r), then all of the extreme columns are found with high probability [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "When m n, communication-optimal algorithms for computing the factorization are referred to as TSQR [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "TSQR is implemented in several environments, including MapReduce [5, 12], distributed memory MPI [15], GPUs [2], and grid computing [1].", "startOffset": 65, "endOffset": 72}, {"referenceID": 11, "context": "TSQR is implemented in several environments, including MapReduce [5, 12], distributed memory MPI [15], GPUs [2], and grid computing [1].", "startOffset": 65, "endOffset": 72}, {"referenceID": 14, "context": "TSQR is implemented in several environments, including MapReduce [5, 12], distributed memory MPI [15], GPUs [2], and grid computing [1].", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "TSQR is implemented in several environments, including MapReduce [5, 12], distributed memory MPI [15], GPUs [2], and grid computing [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "TSQR is implemented in several environments, including MapReduce [5, 12], distributed memory MPI [15], GPUs [2], and grid computing [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "When m n, this method for computing the SVD is called the R-SVD [9].", "startOffset": 64, "endOffset": 67}, {"referenceID": 28, "context": "Second, systems like Hadoop [28] and Spark [30] typically manage the distributed file input-output routines and communication collectives.", "startOffset": 43, "endOffset": 47}, {"referenceID": 4, "context": "Current implementations of TSQR and R-SVD in MapReduce can compute R or \u03a3V T in a a single MapReduce iteration [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 24, "context": "The greedy method is not exact in the separable case but works well in practice [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "X := W ( Ir H \u2032)\u03a0, where H \u2032 is a r\u00d7(n\u2212r) and W is a m\u00d7r matrix with entries generated from a Uniform [0, 1] distribution.", "startOffset": 102, "endOffset": 108}, {"referenceID": 12, "context": "The heat transfer simulation data contains the simulated heat in a high-conductivity stainless steel block with a lowconductivity foam bubble inserted in the block [13].", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "The examples we explored used up to 200 columns and we have explored regimes up to 5000 columns in prior work [12].", "startOffset": 110, "endOffset": 114}], "year": 2014, "abstractText": "Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms efficient for data matrices that have many more rows than columns, so-called \u201ctall-and-skinny matrices\u201d. One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need a single pass over the data matrix and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized synthetic matrices and real-world matrices from scientific computing and bioinformatics.", "creator": "LaTeX with hyperref package"}}}