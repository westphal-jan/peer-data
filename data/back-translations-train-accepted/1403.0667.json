{"id": "1403.0667", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2014", "title": "The Hidden Convexity of Spectral Clustering", "abstract": "In recent years, spectral clustering has become a standard method for data analysis used in a broad range of applications. In this paper we propose a new class of algorithms for multiway spectral clustering based on optimization of a certain \"contrast function\" over a sphere. These algorithms are simple to implement, efficient and, unlike most of the existing algorithms for multiclass spectral clustering, are not initialization-dependent. Moreover, they are applicable without modification for normalized and un-normalized clustering, which are two common variants of spectral clustering.", "histories": [["v1", "Tue, 4 Mar 2014 02:48:20 GMT  (854kb,D)", "https://arxiv.org/abs/1403.0667v1", "26 pages, 3 figures"], ["v2", "Thu, 1 Oct 2015 21:59:05 GMT  (1057kb,D)", "http://arxiv.org/abs/1403.0667v2", "37 pages"], ["v3", "Wed, 4 May 2016 18:10:13 GMT  (2769kb,D)", "http://arxiv.org/abs/1403.0667v3", "22 pages"]], "COMMENTS": "26 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["james r voss", "mikhail belkin", "luis rademacher"], "accepted": true, "id": "1403.0667"}, "pdf": {"name": "1403.0667.pdf", "metadata": {"source": "CRF", "title": "The Hidden Convexity of Spectral Clustering\u2217", "authors": ["James Voss", "Mikhail Belkin", "Luis Rademacher"], "emails": ["vossj@cse.ohio-state.edu", "mbelkin@cse.ohio-state.edu", "lrademac@cse.ohio-state.edu"], "sections": [{"heading": null, "text": "Geometrically, the proposed algorithms can be interpreted as a hidden restoration of the base by means of function optimization. We give a complete characterization of the contrast functions permitted for detectable restoration of the base. We show how these conditions can be interpreted as the \"hidden convexity\" of our optimization problem on the sphere; interestingly, we use efficient convex maximization instead of the more frequent convex minimization. We also show encouraging experimental results on real and simulated data. Keywords: spectral clustering, convex maximization, basic recovery."}, {"heading": "1 Introduction", "text": "It is one of the most basic and practical methods of data analysis and machine learning. It has an enormous number of applications from speech recognition to image analysis to bioinformatics and data compression. There is an extensive literature on the subject, including a number of different methods and their various practical and theoretical aspects. [11] This is due to the simplicity of the algorithms, a number of desirable properties it has and their amenable nature to theoretical analysis. In its simplest form, spectral analysis is an attractive tool for attractive algorithm analysis."}, {"heading": "2 Basis Recovery and Spectral Clustering", "text": "In this section, we present our most important technical results on a hidden basis. Afterwards, we briefly discuss how our results are applied to the setting of spectral clustering. Notation. In the following, we use the following notations: For a matrix B, bij denotes the element in its ith series and jth column. The ith row vector of B is denoted as bi \u2022, and the jth column vector of B as b \u2022 j. For a vector v, we denote the indicator vector for the set S, i.e. the vector that is 1 for indices in S and v, u \u2022 v otherwise its point product. We denote the set {1, 2,...., k} by [k]. We denote with 1S the indicator vector for the set S, i.e. the vector that is 1 for indices in S and v."}, {"heading": "2.1 Basis Recovery via Convex Maximization", "text": "The main technical results of this section deal with the reconstruction of a hidden base using simple optimization techniques."}, {"heading": "2.2 Spectral Clustering as Basis Recovery", "text": "It turns out that the recovery of the orthogonal base has a direct effect on the formation of spectral clusters. In particular, if an n-vertex similarity graph G k has associated components corresponding to the desired clusters, in Section 4 it is seen that the spectral embedding in Rk points each vertex vi in the jth connected component to a beam protruding from the origin towards zj. It happens that the directions z1,.. zk are orthogonal. To see that Fg is actually an orthogonal BEF, we consider the following theoretical constructions: = 1n n \u00b2 i = 1 g (| u \u2022 xi |), from the embedded data and the contrast function g: R. To see that Fg is actually an orthogonal BEF, we consider the following theoretical construction: Let us leave S1,.. Sk are the vertex index sets corresponding to the different components of the diagram BEj \u00b2 and let us define the functions BEj \u2192 gj."}, {"heading": "3 Spectral Clustering Problem Statement", "text": "Let G = (V, A) denote a similarity graph where V is a set of n vertices and A is an adjacence matrix with non-negative weights. Two vertices i, j, V are incidental when aij > 0, and the value of aij is interpreted as a measure of similarity between vertices. In spectral clusters, the goal is to divide the vertices of a graph into sentences S1,..., Sk so that these vertices form natural clusters in the graph. In the simplest case, G consists of k-connected components, and the natural clusters should be the components themselves. In this case, if i, \"Si and j,\" Sj then ai \u00b7 \u00b7 \u00b7 \u00b7 j \"= 0, whenever i 6 = j. For convenience, we can look at the vertices of V so that they are indexed so that all indices in Si, if i < j.\" is the matrix A."}, {"heading": "4 The Spectral Embedding", "text": "The following well-known property of the graph Laplacian (see [19] for a review) helps to shed light on its meaning (see [19] for a review). [19] The graph Laplacian L is symmetrically positive semi-defined as equation (2) cannot be negative. [19] The graph Laplacian L is symmetrically positive semi-defined as equation (2). Furthermore, the graph is a 0 eigenvector of L (or equivalent, u-V aij) if and only if uTLu = 0. If G from k connects components with indices in the sets S1,.., Sk, Inspection of the equation (2) indicates that u eigenvector of L (L) is exactly constant."}, {"heading": "5 Basis Recovery for Spectral Clustering", "text": "We now focus on the second step of spectral clustering i = 1 g (2), which summarizes the individual points. (2) We agree that this is a functional optimization problem, where the maximum structure of the function can be used to regenerate the desired clusters. (3) We see the functioning of the functioning of the functioning of the functioning of the functioning. (3) We construct the functioning of the functioning of the functioning of the functioning. (3) We construct the functioning of the functioning. (4) We construct the functioning of the functioning. (3) We construct the functioning of the functioning. (4) We construct the functioning of the functioning. (7) We construct Fg asFg (u)."}, {"heading": "5.1 Proposed Algorithms", "text": "In fact, it is so that it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which"}, {"heading": "5.2 Choosing a Contrast Function", "text": "There are many possible contrast g selections that are permissible for spectral clustering under > Theorem 9, including the following: gsig (t) = \u2212 11 + exp (\u2212 | t |) gp (t) = | t | p, where p (2, \u221e) ggau = e \u2212 t2 gabs (t) = \u2212 | t | ght (t) = log cosh (t). When selecting contrasts, it is instructive to first consider the function g2 (y) = y 2 (which loosens the criterion that t 7 \u2192 g (\u221a | t |) should be strictly convex to simple convexity and therefore not permissible)."}, {"heading": "6 Clustering Experiments", "text": "We are now discussing our test results on our proposed spectral cluster algorithms on a variety of real and simulated data. Implementations for our spectral cluster algorithms are available on github: https: / / github.com / vossj / HBR Spectral Clustering."}, {"heading": "6.1 An Illustrating Example", "text": "Figure 1 illustrates our framework for functional optimization for spectral clustering. In this example, random points pi were created from 3 concentric circles: 200 points were drawn evenly randomly from a radius 1 circle, 350 points from a radius 3 circle, and 700 points from a radius 5 circle. The points were then radially disturbed. The points created are shown in Figure 1 (a).The similarity matrix A was constructed as aij = exp (\u2212 14% pi \u2212 pj \u00b2 2), and the laplac embedding was performed with Lrw. Figure 1 (b) shows the cluster process with contrast contrast on the resulting embedded points. In this figure, the embedded data sufficiently encode the desired orthogonal base structure so that all local maxima of Fgsig correspond to the desired clusters. The value of Fgsig is represented by the color scale heat map on the unity sphere in Figure 1 (BRb), where the color atings shown here match."}, {"heading": "6.2 Image Segmentation Examples", "text": "The goal of image segmentation is to divide an image into regions that represent different objects or characteristics of the image. Figure 2 and Figure 3 show several segments produced by HBRopt-gabs and spherical k-means on several sample images from the BSDS300 test set [13]. For this example application, we used a relatively simple notion of similarity based only on the color and proximity of the image pixels. Let pi characterize the ith-pixel-like image cutout. Each pi has a location xi and an RGB color ci = (ri, gi, bi) T. We used the following similarity between two different pixels pi and pj: aij = [e \u2212 1] BRP-2 \"xi \u2212 xj \u2212 1 \u03b22\" ci \u2212 cj [c] < RP-2 \"i\" i \"i\" i \"i\" i \"i\" i \"i\" \"\" i."}, {"heading": "6.3 Stochastic Block Model with Imbalanced Clusters", "text": "We construct a similarity diagram A = diag (A1, A2, A3) + E, where each Ai is a symmetrical matrix corresponding to a cluster, and E is a minor disturbance. We set A1 = A2 to 10 x 10 matrices with entries 0.1. We set A3 to a 1000 x 1000 matrix which is symmetrical, approximately 95% sparse with randomly selected non-zero locations set to 0.001. By performing this experiment 50 times, HBRopt-gsig achieved an average accuracy of 99.9%. In contrast, spherical k averages with randomly chosen starting points achieved an average accuracy of only 42.1%. It turned out that the division of the large cluster in terms of the spherical k-mean objective function is indeed optimal, but results in poor classification performance. Our method does not suffer from this deficiency."}, {"heading": "6.4 Performance Evaluation on UCI Datasets", "text": "We compare spectral cluster performance on a number of datasets with unbalanced cluster sizes. \u2022 Specifically, we use the E. coli, flags, glass, iris, thyroid diseases and auto-evaluation datasets that are part of the UCI Machine Learning Repository. \u2022 We also use the standardized gene expression datasets [24, 23], which are also called the cell cycle. \u2022 For the flag datasets, we use religion as a basic truth laboratory, and for thyroid diseases, we use the new thyroid data. For all datasets, we use only fields for which there were no missing values, we normalize the data so that each field had a standard deviation, and we constructed the similarity matrix A using a Gaussian core k (yi, yj) = exgorithmic (\u2212 \u03b1, yi \u2212 yy \u2212 yy).The parameters were chosen separately for each dataset to create a good insertion."}, {"heading": "7 Basis Recovery With Each Laplacian Embedding", "text": "We have already argued that the Laplacian L and Lrw graphs can be used for spectral clustering within our BEF framework, and we have argued that Lsym can also be used. We will now discuss how orthogonal BEF recovery can be used for spectral clustering in the environment in which G consists of k-connected components, using each of the Laplacian graphs. First, in Section 7.1, we will show how laplac embedding differs for the symmetric standardized Laplacian Lsym and generalize to the embedding structure that arises for L and Lrw (cf. Proposition 7). Then, in Section 7.2, we will show that spectral embedding caused by one of the Laplacian Lsym graphs discussed leads to an optimization problem on Sk \u2212 1, in which the local maxima will list the desired clusters for spectral clustering (cf. below)."}, {"heading": "7.1 Null Space Structure of the Normalized Laplacians", "text": "We will first describe the zero space structures Lsym and Lrw for a diagram consisting of k components. (Then, we will show how the zero space structures of Lsym, Lrw, and L can be viewed within a single, generalized notion of a diagram. (F, A) be an n-vertex diagram containing k components so that the ithComponent has indentations with indices in the set Si. For each fixed C-V we will define D (C): = 1 sythos i. (7) where D = diag (d11, d22, dnn) is a diagonal matrix with strictly positive entries. For now, we will use D to be the diagonal degree matrix D, which is dii = dii = 1 aij."}, {"heading": "7.2 Maxima Structure of the Resulting BEFs", "text": "In this section we show that we have a continuous contrast function in which it is only a limited amount of data resulting from any embedding. (...) We assume that D = diag (d11,.) is a positive definitive matrix. We assume that x1,. xn is an (G, D) -orthogonal embedding of G in such a way that xi (Sj) \u2212 1zj for each i in Sj. Parallel to the text of section 5, we construct a function Fg: Sk \u2212 1. R from a continuous contrast function."}, {"heading": "A Facts About Convex Functions", "text": "In this section, intervals can be open, semi-open or closed. There is a large literature examining the properties of convex functions. Since strict convexity is considered more special than convexity, the results are typically given in the form of convex functions. The following characterization of strict convexity is a version of Proposition 1.1.4 of [8] for strictly convex functions and can be proven in a similar way. Lemma 19. For an interval I, let f: I \u2192 R be a strictly convex function. Then, specifying any x0-I, the slope function defined by m (x): = f (x) \u2212 f (x0) x \u2212 x0 increases strictly to I\\ {x0}. The following result is largely a sequence of Lemma 19.Lemma 20. Let me be an interval and let f: I \u2192 R be a convex function."}, {"heading": "Acknowledgements", "text": "This work was supported by NSF grants IIS 1117707, CCF 1350870 and CCF 1422830."}], "references": [{"title": "Efficient learning of simplices", "author": ["J. Anderson", "N. Goyal", "L. Rademacher"], "venue": "COLT, pages 1020\u20131045,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning spectral clustering, with application to speech separation", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, 7:1963\u20132001,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput., 15(6):1373\u20131396,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "The rotation of eigenvectors by a perturbation", "author": ["C. Davis", "W.M. Kahan"], "venue": "iii. SIAM Journal on Numerical Analysis, 7(1):1\u201346,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1970}, {"title": "Identification of almost invariant aggregates in reversible nearly uncoupled markov chains", "author": ["P. Deuflhard", "W. Huisinga", "A. Fischer", "C. Sch\u00fctte"], "venue": "Linear Algebra and its Applications, 315(1):39\u201359,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "On the 0\u20131-maximization of positive definite quadratic forms", "author": ["P. Gritzmann", "V. Klee"], "venue": "Operations Research Proceedings 1988, pages 222\u2013227. Springer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "Convex Analysis and Minimization Algorithms: Part 1: Fundamentals, volume 1", "author": ["J.-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions", "author": ["D. Hsu", "S.M. Kakade"], "venue": "Proceedings of the 4th conference on Innovations in Theoretical Computer Science (ITCS), pages 11\u201320. ACM,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Independent component analysis, volume 46", "author": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "venue": "John Wiley & Sons,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Algorithms for clustering data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": "Prentice-Hall, Inc., Upper Saddle River, NJ, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Spectral clustering as mapping to a simplex", "author": ["P. Kumar", "N. Narasimhan", "B. Ravindran"], "venue": "2013 ICML workshop on Spectral Learning,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D.R. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "ICCV, pages 416\u2013425,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A random walks view of spectral segmentation", "author": ["M. Meil\u0103", "J. Shi"], "venue": "AI and Statistics (AISTATS),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Advances in neural information processing systems, 2:849\u2013856,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Convex analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton Landmarks in Mathematics. Princeton University Press, Princeton, NJ,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888\u2013905,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparison of spectral clustering algorithms", "author": ["D. Verma", "M. Meil\u0103"], "venue": "Technical report, University of Washington CSE Department, Seattle, WA 98195-2350, 2003. doi=10.1.1.57.6424, Accessed online via CiteSeerx 5 Mar", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and computing, 17(4):395\u2013416,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "The hidden convexity of spectral clustering", "author": ["J. Voss", "M. Belkin", "L. Rademacher"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence, pages 2108\u20132114,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Perron cluster analysis and its connection to graph partitioning for noisy data", "author": ["M. Weber", "W. Rungsarityotin", "A. Schliep"], "venue": "Konrad-Zuse-Zentrum f\u00fcr Informationstechnik Berlin,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "A study of the fixed points and spurious solutions of the deflation-based fastica algorithm", "author": ["T. Wei"], "venue": "Neural Computing and Applications, pages 1\u201312,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Model-based clustering and data transformations for gene expression data supplementary web site", "author": ["K.Y. Yeung", "C. Fraley", "A. Murua", "A.E. Raftery", "W.L. Ruzzo"], "venue": "http://faculty. washington.edu/kayee/model/, 2001. Accessed: 20 Jan", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Model-based clustering and data transformations for gene expression data", "author": ["K.Y. Yeung", "C. Fraley", "A. Murua", "A.E. Raftery", "W.L. Ruzzo"], "venue": "Bioinformatics, 17(10):977\u2013987,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Multiclass spectral clustering", "author": ["S.X. Yu", "J. Shi"], "venue": "Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on (ICCV), pages 313\u2013319,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "There is an extensive literature on the subject, including a number of different methodologies as well as their various practical and theoretical aspects [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 15, "context": "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].", "startOffset": 204, "endOffset": 219}, {"referenceID": 13, "context": "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].", "startOffset": 204, "endOffset": 219}, {"referenceID": 1, "context": "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].", "startOffset": 204, "endOffset": 219}, {"referenceID": 23, "context": "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].", "startOffset": 204, "endOffset": 219}, {"referenceID": 18, "context": "Typical algorithms for multiway spectral clustering follow a two-step process: \u2217A short version of this paper previously appeared in the proceedings of the Thirtieth AAAI Conference on Artificial Intelligence [20].", "startOffset": 209, "endOffset": 213}, {"referenceID": 17, "context": "The meaning can be explained by spectral graph theory as relaxations of multiway cut problems [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "There are also connections to other areas of machine learning and mathematics, in particular to the geometry of the underlying space from which the data is sampled [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 19, "context": "This view of spectral clustering as basis recovery is related to previous observations that the spectral embedding generates a discrete weighted simplex (see [21, 12] for some applications).", "startOffset": 158, "endOffset": 166}, {"referenceID": 10, "context": "This view of spectral clustering as basis recovery is related to previous observations that the spectral embedding generates a discrete weighted simplex (see [21, 12] for some applications).", "startOffset": 158, "endOffset": 166}, {"referenceID": 8, "context": "The proposed approach relies on an optimization problem resembling certain Independent Component Analysis techniques, such as FastICA (see [10] for a broad overview).", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "Interestingly, while maximizing a convex function over a convex domain is generally difficult (even maximizing a positive definite quadratic form over the continuous cube [0, 1]n is NP-hard2), our setting allows for efficient optimization.", "startOffset": 171, "endOffset": 177}, {"referenceID": 0, "context": "[1] use the method of moments to recover a continuous simplex given samples from the uniform probability distribution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Also, one of the results of Hsu and Kakade [9] shows recovery of parameters in a Gaussian Mixture Model using the moments of order three, and this result can be thought of as a case of the basis recovery problem.", "startOffset": 43, "endOffset": 46}, {"referenceID": 20, "context": "In particular, typical versions of FastICA are known to have spurious maxima [22].", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "This follows from [7] together with Fact 6 below.", "startOffset": 18, "endOffset": 21}, {"referenceID": 17, "context": "The following well known property of the graph Laplacian (see [19] for a review) helps shed light on its importance: Given u \u2208 Rn, uLu = 1 2 \u2211", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "We formulate this result as follows (see [21], [18, Proposition 5], and [15, Proposition 1] for related statements).", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "Using the perturbation theory of symmetric matrices, it can be shown that when the perturbation is not too large, the structure of X is approximately maintained (see [5, 19]).", "startOffset": 166, "endOffset": 173}, {"referenceID": 17, "context": "Using the perturbation theory of symmetric matrices, it can be shown that when the perturbation is not too large, the structure of X is approximately maintained (see [5, 19]).", "startOffset": 166, "endOffset": 173}, {"referenceID": 23, "context": "Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].", "startOffset": 207, "endOffset": 214}, {"referenceID": 12, "context": "Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].", "startOffset": 207, "endOffset": 214}, {"referenceID": 15, "context": "Spectral clustering was first applied to image segmentation by Shi and Malik [17], and it has remained a popular application of spectral clustering.", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Figure 2 and Figure 3 show several segmentations produced by HBRopt-gabs and spherical k-means on several example images from the BSDS300 test set [13].", "startOffset": 147, "endOffset": 151}, {"referenceID": 22, "context": "We also use the standardized gene expression data set [24, 23], which is also referred to as cell cycle.", "startOffset": 54, "endOffset": 62}, {"referenceID": 21, "context": "We also use the standardized gene expression data set [24, 23], which is also referred to as cell cycle.", "startOffset": 54, "endOffset": 62}, {"referenceID": 13, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "4 of [8] for strictly convex functions, and can be proven in a similar fashion.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "2 of Hiriart-Urruty and Lemar\u00e9chal [8].", "startOffset": 35, "endOffset": 38}], "year": 2016, "abstractText": "In recent years, spectral clustering has become a standard method for data analysis used in a broad range of applications. In this paper we propose a new class of algorithms for multiway spectral clustering based on optimization of a certain \u201ccontrast function\u201d over the unit sphere. These algorithms, partly inspired by certain Independent Component Analysis techniques, are simple, easy to implement and efficient. Geometrically, the proposed algorithms can be interpreted as hidden basis recovery by means of function optimization. We give a complete characterization of the contrast functions admissible for provable basis recovery. We show how these conditions can be interpreted as a \u201chidden convexity\u201d of our optimization problem on the sphere; interestingly, we use efficient convex maximization rather than the more common convex minimization. We also show encouraging experimental results on real and simulated data. keywords: spectral clustering, convex maximization, basis recovery", "creator": "LaTeX with hyperref package"}}}