{"id": "1403.1347", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction", "abstract": "Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio &amp; Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of amino acids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9% (Wang et al., 2011) for this challenging secondary structure prediction problem.", "histories": [["v1", "Thu, 6 Mar 2014 05:18:26 GMT  (264kb,D)", "http://arxiv.org/abs/1403.1347v1", "Accepted by ICML 2014"]], "COMMENTS": "Accepted by ICML 2014", "reviews": [], "SUBJECTS": "q-bio.QM cs.CE cs.LG", "authors": ["jian zhou", "olga g troyanskaya"], "accepted": true, "id": "1403.1347"}, "pdf": {"name": "1403.1347.pdf", "metadata": {"source": "META", "title": "Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction", "authors": ["Jian Zhou", "Olga G. Troyanskaya"], "emails": ["JZTHREE@PRINCETON.EDU", "OGT@CS.PRINCETON.EDU"], "sections": [{"heading": "1. Introduction", "text": "The fact is that we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to put ourselves in."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Generative Stochastic Networks", "text": "The generative stochastic network (GSN) is a recently proposed model that uses a new unconventional approach to learn a generative model of data distribution without explicitly specifying a probabilistic graphical model, and allows learning a deep generative model through global training through back-propagation. Generative stochastic network learns a Markov chain to sample from the data distribution P (X). The difference with GSN is that it allows the computer-aided diagram to have a latent state, and the noise is transferred not only to the input, but also to the intermediate compilations. Compared to other deep generative models such as deep Boltzmann machines, a great advantage of GSN is that it avoids intractable inferences and explicit marginalization via latent variables, as it is unable to learn explicit parameters."}, {"heading": "2.2. Supervised Generative Stochastic Network", "text": "We formulate the monitored stochastic network as follows: We use X to mark input variable and Y to mark output variable. The monitored GSN aims to capture the conditional dependency structure of Y \u2212 \u2212 Similar to GSN, it learns a Markov chain that samples from P (Y | X) can be a corruption process that transforms Y into a random variable Y structure. Let PTB (Y | Y, X) be a denoising auto encoder that calculates the probability of Y (Y | X). Then we have the following consequences. If PTB (Y | Y, X) is a consistent estimator of the true conditional distribution P (Y | Y, X), and Tn defines an irreducible and ergodic Markov chain, then as n \u2192 undirected distribution of the samples produced."}, {"heading": "3. Algorithms", "text": "This means that we do not have to engage with the spatial structure of the data, so each feature is connected to data for each location, making it difficult to scale. \"The original DBM-like GSN architecture is applied in (Bengio et al., 2013a) not to the spatial structure of the data, but to the spatial connectivity of the data. (Bengio et al., 2013a) not to the spatial structure of the data, but to the spatial structure of the data. (h2) Each connectivity layer can contain a single connectivity layer (h1) or a connectivity layer (h2), and a connectivity layer (h2), which acts as intermediate computations for the computation of evolutionary layers. The original DBM-like GSN architecture is not applied to the spatial structure of the data in (Bengio et al., 2013a), so each feature is connected to each location that it is so scalable."}, {"heading": "4. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Features and dataset", "text": "We focus on the secondary structure prediction as this is a more difficult problem than the 3-D prediction and shows more structural information. We have the ability to predict both the secondary structures and the amino acid accessibility because we have different structural properties than the 3-D predictions."}, {"heading": "4.2. Training setup", "text": "To give the labels noise, the input markers were corrupted by randomly setting half of the values to zero. Gaussian noise with standard deviation 2 was added on all but the first revolutionary GSN layer. Each yt was obtained by sampling from the reconstructed distribution. We consider the reconstructed distribution to be a multinomial distribution for the secondary structure and binomial distribution for the accessibility of solvents. The return number is set to 12 to ensure the flow of information across the network. Motivated to better learn the reconstruction distribution when the chain is initiated by an arbitrary label Y 'far away from where the probability density of the true P (Y | X) is located, which is important for supervised prediction tasks, during the training for half of the training samples that we have Y activated by the same arbitrary initiation of the Y' model, rather than by its true value. We call this trick \"let's all of the start function be used for activating the global dynamics except for the W."}, {"heading": "4.3. Performance", "text": "We achieved 72.1 \u00b1 0.6% Q8 accuracy on our Cull PDB Test Set Secondary Structures (Table 2). The best single model we tested has a 3-layer convolutional structure. We refer to our structure as {80 \u00d7 Convolutional or Pooling Layer, and the number following the layer type indicates the size of the filter or pooling window. Mean Pooling is used for all pooling layers in this architecture. We used 80 channels in all the Convolutionary Layers. We used 80 channels in all the Convolutionary Layers."}, {"heading": "4.4. Analysis of architecture", "text": "To discover key factors in the success of our architecture, we experimented with alternative model architectures with different number of layers and tried to remove the \"kick-start\" training or Gaussian noise injection (Table 2, Figure 4). For model architectures with different number of layers, we removed the top 1 and 2 curved GSN layers from the original best 3-layer architecture. The 2-layer model dramatically exceeds the 1-layer model, and the 3-layer model performs slightly better than the 2-layer model. Abstaining from \"kick-start\" during training dramatically reduces the rate of convergence to optimal sampling performance and predictive accuracy, probably because the \"kick-start\" model has learned better reconstruction distribution when the chain is activated from an arbitrary state."}, {"heading": "5. Conclusions", "text": "In order to apply profound representations to the prediction of protein sequences and structures, we implemented a supervised generative stochastic network (GSN) and introduced a revolutionary architecture that enables it to learn hierarchical representation based on complete data. While we have demonstrated its success in predicting secondary structures, such architecture can potentially be applied to other tasks for predicting protein structures. Therefore, our experiments suggest that a supervised generative stochastic network is an effective algorithm for structured predictions that is sensitive at the local level, while extending the success of generative stochastic networks in detecting complex dependencies in the data. Therefore, such an architecture can also be applicable to a wide range of structured prediction problems outside of bioinformatics, such as scene parsing and image segmentation. To further develop the protein sequence and structural modeling, an optimal coherent structure can be modified in such a way that the present architecture can be altered in a structured way."}, {"heading": "Acknowledgments", "text": "We thank Robert E. Schapire for the helpful discussions and the TIGRESS High Performance Computing Center at Princeton University for supporting computing resources. This work was supported by the CAREER Prize of the National Science Foundation (NSF) (DBI-0546275), the National Institutes of Health (NIH) (R01 GM071966, R01 HG005998 and T32 HG003284) and the National Institute of General Medical Sciences (NIGMS) Center of Excellence (P50 GM071508). O.G.T. is a Senior Fellow of the Canadian Institute for Advanced Research."}], "references": [{"title": "Exploiting the past and the future in protein secondary structure", "author": ["Baldi", "Pierre", "Brunak", "Sren", "Frasconi", "Paolo", "Soda", "Giovanni", "Pollastri", "Gianluca"], "venue": "prediction. Bioinformatics,", "citeRegEx": "Baldi et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 1999}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "ric"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Better mixing via deep representations", "author": ["Bengio", "Yoshua", "Mesnil", "Gregoire", "Dauphin", "Yann", "Rifai", "Salah"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1305.6663,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Machine learning methods for protein structure prediction", "author": ["Cheng", "Jianlin", "Tegge", "Allison N", "Baldi", "Pierre"], "venue": "Biomedical Engineering, IEEE Reviews in,", "citeRegEx": "Cheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2008}, {"title": "Protein secondary structure prediction based on position-specific scoring matrices", "author": ["Jones", "David T"], "venue": "Journal of molecular biology,", "citeRegEx": "Jones and T.,? \\Q1999\\E", "shortCiteRegEx": "Jones and T.", "year": 1999}, {"title": "Dictionary of protein secondary structure: pattern recognition of hydrogenbonded and geometrical features", "author": ["Kabsch", "Wolfgang", "Sander", "Christian"], "venue": "Biopolymers, 22(12):2577\u20132637,", "citeRegEx": "Kabsch et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kabsch et al\\.", "year": 1983}, {"title": "Conditional neural fields", "author": ["Peng", "Jian", "Bo", "Liefeng", "Xu", "Jinbo"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Peng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "A unified multitask architecture for predicting local protein properties", "author": ["Qi", "Yanjun", "Oja", "Merja", "Weston", "Jason", "Noble", "William Stafford"], "venue": "PLoS One,", "citeRegEx": "Qi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2012}, {"title": "Predicting the secondary structure of globular proteins using neural network models", "author": ["Qian", "Ning", "Sejnowski", "Terrence J"], "venue": "Journal of molecular biology,", "citeRegEx": "Qian et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Qian et al\\.", "year": 1988}, {"title": "Prediction of protein secondary structure at better than 70accuracy", "author": ["Rost", "Burkhard", "Sander", "Chris"], "venue": "Journal of molecular biology,", "citeRegEx": "Rost et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Rost et al\\.", "year": 1993}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Bayesian segmentation of protein secondary structure", "author": ["Schmidler", "Scott C", "Liu", "Jun S", "Brutlag", "Douglas L"], "venue": "Journal of computational biology,", "citeRegEx": "Schmidler et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Schmidler et al\\.", "year": 2000}, {"title": "Predicting protein secondary and supersecondary structure", "author": ["Singh", "Mona"], "venue": "Handbook of Computational Molecular Biology. Chapman & Hall CRC Computer and Information Science Series,", "citeRegEx": "Singh and Mona.,? \\Q2005\\E", "shortCiteRegEx": "Singh and Mona.", "year": 2005}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher", "Richard", "Huang", "Eric H", "Pennin", "Jeffrey", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Hidden-unit conditional random fields", "author": ["van der Maaten", "Laurens", "Welling", "Max", "Saul", "Lawrence K"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "Maaten et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2011}, {"title": "Pisces: a protein sequence culling", "author": ["G. Wang", "R.L. Dunbrack", "Jr."], "venue": "server. Bioinformatics,", "citeRegEx": "Wang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Protein 8class secondary structure prediction using conditional neural fields. Proteomics", "author": ["Wang", "Zhiyong", "Zhao", "Feng", "Peng", "Jian", "Xu", "Jinbo"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "9% (Wang et al., 2011) for this challenging secondary structure prediction problem.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "Understanding complex dependency between protein sequence and structure is one of the greatest challenges in computational biology (Cheng et al., 2008).", "startOffset": 131, "endOffset": 151}, {"referenceID": 0, "context": "Since the early work by (Qian & Sejnowski, 1988), neural networks have been widely applied and are core components of many most successful approaches (Rost & Sander, 1993; Jones, 1999; Baldi et al., 1999).", "startOffset": 150, "endOffset": 204}, {"referenceID": 0, "context": "Other developments include better capturing spatial dependencies using bidirectional recurrent neural networks (BRNN) (Baldi et al., 1999; Pollastri et al., 2002), probabilistic graphical models (Schmidler et al.", "startOffset": 118, "endOffset": 162}, {"referenceID": 12, "context": ", 2002), probabilistic graphical models (Schmidler et al., 2000; Chu et al., 2004; van der Maaten et al., 2011), or combination of neural network and graphical models, like conditional neural fields (CNF) which integrate a windows-based neural network with conditional random field (CRF) (Peng et al.", "startOffset": 40, "endOffset": 111}, {"referenceID": 7, "context": ", 2011), or combination of neural network and graphical models, like conditional neural fields (CNF) which integrate a windows-based neural network with conditional random field (CRF) (Peng et al., 2009; Wang et al., 2011).", "startOffset": 184, "endOffset": 222}, {"referenceID": 17, "context": ", 2011), or combination of neural network and graphical models, like conditional neural fields (CNF) which integrate a windows-based neural network with conditional random field (CRF) (Peng et al., 2009; Wang et al., 2011).", "startOffset": 184, "endOffset": 222}, {"referenceID": 17, "context": "lenging and less-addressed problem (Pollastri et al., 2002; Wang et al., 2011).", "startOffset": 35, "endOffset": 78}, {"referenceID": 17, "context": "5\u00c5 resolution while sharing less than 30% identity, which is the same set up as used in (Wang et al., 2011).", "startOffset": 88, "endOffset": 107}, {"referenceID": 8, "context": "We discretized solvent accessibility scores to absolute solvent accessibility and relative solvent accessibility following (Qi et al., 2012).", "startOffset": 123, "endOffset": 140}, {"referenceID": 17, "context": "005 Previous state-of-the-art (Wang et al., 2011) 0.", "startOffset": 30, "endOffset": 49}, {"referenceID": 17, "context": "649) (Wang et al., 2011).", "startOffset": 5, "endOffset": 24}, {"referenceID": 14, "context": "To better model the long-range interactions in a protein, adaptive and dynamic architecture that changes the connectivity adaptively based on input, analogous to unfolding recursive auto-encoder (Socher et al., 2011), may further improve the quality of representation in future.", "startOffset": 195, "endOffset": 216}], "year": 2014, "abstractText": "Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of aminoacids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9% (Wang et al., 2011) for this challenging secondary structure prediction problem.", "creator": "LaTeX with hyperref package"}}}