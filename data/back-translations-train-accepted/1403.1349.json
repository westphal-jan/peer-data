{"id": "1403.1349", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Learning Soft Linear Constraints with Application to Citation Field Extraction", "abstract": "Accurately segmenting a citation string into fields for authors, titles, etc. is a challenging task because the output typically obeys various global constraints. Previous work has shown that modeling soft constraints, where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation performance. On the other hand, for imposing hard constraints, dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference. We extend the technique to perform prediction subject to soft constraints. Moreover, with a technique for performing inference given soft constraints, it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training. This allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset.", "histories": [["v1", "Thu, 6 Mar 2014 05:24:02 GMT  (26kb)", "https://arxiv.org/abs/1403.1349v1", null], ["v2", "Fri, 17 Oct 2014 13:27:02 GMT  (26kb)", "http://arxiv.org/abs/1403.1349v2", "appears in Proc. the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014)"]], "reviews": [], "SUBJECTS": "cs.CL cs.DL cs.IR", "authors": ["sam anzaroot", "alexandre passos", "david belanger", "andrew mccallum"], "accepted": true, "id": "1403.1349"}, "pdf": {"name": "1403.1349.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mccallum}@cs.umass.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 3.13 49v2 [cs.CL] 1 7O ct2 014"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Structured Linear Models", "text": "For this underlying model, we use a chain-structured conditional random field (CRF), as CRFs have been shown to perform better than other simple unrestricted models, such as hidden Markov models of citation extraction (Peng and McCallum, 2004).We make a prediction by performing MAP inferences (Koller and Friedman, 2009).The MAP inference task in a CRF can be expressed as an optimization problem with a linear target (Sontag, 2010; Sontag et al., 2011).Here, we define a binary indicator variable for each candidate set in the graphical model. Each of these indicator variables is associated with the score that the factor takes when the indicator variable has the corresponding value. As the protocol probability of some predictors in the CRF is proportional to the sum of the results of all problem factors, we can associate the variables > U as variables."}, {"heading": "2.2 Dual Decomposition for Global Constraints", "text": "In order to make predictions subject to various global constraints, we may need to add additional constraints to problem (1). (1) Dual decomposition is a popular method of performing MAP conclusions in this scenario, since it uses known algorithms for MAP in the base problem where these additional constraints were not added (Komodakis et al., 2007; Sontag et al., 2011; Rush and Collins, 2012). In this case, the MAP problem can be formulated as a structured linear model similar to equation (1), for which we have an MAP algorithm, but in which we have imposed additional constraints that no longer allow us to use the algorithm. InAlgorithm 1 DD: projected subgradient for dual decomposition with hard constraints (1): while we have not converged."}, {"heading": "3 Soft Constraints in Dual Decomposition", "text": "We are introducing an extension of algorithm 1 to handle soft constraints. < < < < < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212"}, {"heading": "3.1 Learning Penalties", "text": "One consideration when using soft v.s. hard constraints is that soft constraints are a new training problem, since we have to choose vector c, the penalties for violating the constraints. An important feature of the problem (5) in the previous section is that it corresponds to a structured linear model over y and z. Therefore, we can use well-known training algorithms to choose the parameters of structured linear models. All we need to use the structured perceptron algorithm (Collins, 2002) or the structured SVM algorithm (Tsochantaridis et al., 2004) is a black box method for performing MAP inferences within the framework of an articulated cost vector."}, {"heading": "4 Citation Extraction Data", "text": "We look at the UMass citation dataset, first presented in Anzaroot and McCallum (2013). It contains over 1800 citations from many academic disciplines extracted from arXiv. This dataset contains both coarse-grained and fine-grained labels; for example, it contains labels for the segment of all authors, segments for each author, and for the first and last name of each author. There are 660 citations in the development set and 367 citations in the testset.The labels in the UMass dataset are a concatenation of labels from a hierarchically defined schema. For example, a first name of an author is labeled with: Author / Person / First. In addition, individual tokens are labeled with an BIO labeling scheme for each level in the hierarchy. BIO is a commonly used labeling scheme for information extraction tasks. BIO labeling schemes are individual labels for this segment, and the labels for this segment are the first of the segment to be labeled by the O."}, {"heading": "5 Global Constraints for Citation Extraction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Constraint Templates", "text": "We now describe the families of global constraints that we consider for citation extraction. Note that these constraints are all linear, as they depend only on the number of any possible conditional random field. Moreover, since our labels are BIO-encoded, it is possible to count by counting B tags how often each quotation tag itself appears in a sentence. The first two families of constraints that we describe are generic for each sequence labeling task, while the last one is specific to hierarchical labeling as available in the UMass dataset. Our sequence output is called y and one element of that sequence is yk.We designate [yk = i] as the function that prints 1 when yk has a 1 at index i and 0 otherwise. Here, yk is an output tag of the CRF, so if [yk = i]] = 1, then we have yk output an index with ik."}, {"heading": "5.2 Singleton Constraints", "text": "Singleton constraints ensure that each label can appear at most once in a quote, the same global constraints used to extract citation fields in Chang et al. (2012). We define s (i) as the number of times the label with index i is predicted in a quote, formally: s (i) = \u2211 yk-y [[[yk = i]] The constraint that each label can appear at most once takes the form of: s (i) < = 1"}, {"heading": "5.3 Pairwise Constraints", "text": "Paired constraints are constraints on the number of two terms in a citation. We define z1 (i, j) to bez1 (i, j) = \u2211 yk-y [[yk = i]] + \u2211 yk-y [[[yk = j]] and z2 (i, j) to bez2 (i, j) = \u2211 yk-y [[[yk = i] - \u2211 yk-y [[yk = j]] We consider all constraints of forms: z (i, j) \u2264 0, 1, 2, 3 and z (i, j) \u2265 0, 1, 2, 3. Note that some of these constraints are redundant or logically incompatible, but we use them as soft constraints, so these constraints are not necessarily satisfied by the output of the model, which removes concerns about enforcing logically impossible results."}, {"heading": "5.4 Hierarchical Equality Constraints", "text": "The labels in the citation data set are hierarchical labels. This means that the labels are the concatenation of all hierarchical levels. We can create constraints that depend on only one or more elements in the hierarchy. We define C (x, i) as the function that returns 1 if output x contains the label i in the hierarchy and 0 otherwise. We define e (i, j) to bee (i, j) = \u2211 yk-y [[[C (yk, i)]] \u2212 \u2211 yk-y [[C (yk, j)] Hierarchical equality constraints take the form of: e (i, j) \u2265 0 (8) e (i, j) \u2264 0 (9)"}, {"heading": "5.5 Local constraints", "text": "We limit the output labeling of the chainstructured CRF to a valid BIO encoding, which improves both the performance of the underlying model when used without global restrictions, and the validity of the global restrictions we impose, since they are applied only to B labels. The condition that the labeling is a valid BIO can be expressed as a collection of pairwise restrictions of adjacent labels in the sequence. Instead of enforcing these restrictions by dual decomposition, they can be enforced directly when performing MAP inferences in the CRF by modifying the dynamic program of the Viterbi algorithm to allow only valid pairs of adjacent labels."}, {"heading": "5.6 Constraint Pruning", "text": "While the techniques in Section 3.1 can easily cope with a large number of limitations in training time, this can be arithmetically costly, especially when very large limitations are taken into account. This is problematic because the size of some limitations grows families that we are looking at square with the number of candidate labels, and there are about 100 in the UMass dataset. Such a family consists of limitations that must be limited to the sum of the number of limitations of two different types of labels (a useful example is that there can be no more than one of \"doctoral thesis\" and \"journal\"). Therefore, quickly printing bad limitations can save a considerable amount of training time and lead to better generalization by calculating a score that estimates how useful each limitation should be. Our score compares how often the limitation is anchored in the basic truth limitation and how much it affects our predictions."}, {"heading": "6 Related Work", "text": "There are several previous examples of extending the chain of structured sequence models with terms that capture global relations by extending the chain to a more complex graphical model with non-local dependencies between results. Conclusions in these models can be made, for example, with loopy faith propagation (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or Gibbs sampling (Finkel et al., 2005). Belonging propagation is prohibitively expensive in our model due to the high cardinalities of output variables and global factors that include all output variables at the same time. There are various methods to exploit the combinatorial structure of these factors, but the performance would still be higher than our method. While Gibbs sampling works well, as the so-called entity recognition (Finkel et al al al al al, 2005), our previous experiments show that it does not work well for extraction."}, {"heading": "7 Experimental Results", "text": "It is only a matter of time before that happens, that it happens."}, {"heading": "7.1 Examples of learned constraints", "text": "The number of captured persons who are able to move is so high that they are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to fight, to move, to fight, to fight, to fight, to move, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "8 Conclusion", "text": "We are introducing a novel modification of the standard projected dual subgradient degradation algorithm to perform MAP conclusions subject to severe constraints, right up to a procedure for performing MAP in the presence of soft constraints. In addition, we offer an easy-to-implement method for learning the penalties for soft constraints. This method leads many penalties to zero, allowing users to automatically detect discriminatory constraints from large families of applicants. By experimenting with a current large data set, we demonstrate that the use of soft constraints and the selection of which constraints we apply in our criminal proceedings can lead to significant increases in accuracy. We achieve a 17% gain in accuracy over a chain-structured CRF model, whereas we only have to run MAP on average less than 2 times per example in the CRF. These minor incremental costs over Viterbi, as well as the fact that we obtain certificates for the validity of our 100% algorithm for the practice of our optimum."}, {"heading": "Acknowledgments", "text": "This work has been supported in part by the Center for Intelligent Information Retrieval, in part by DARPA under contract number FA8750-132-0020, in part by NSF grant # CNS-0958392, and in part by IARPA under DoI / NBC contract # D11PC20152. The U.S. government is authorized to reproduce and distribute the reprint for government purposes, regardless of the copyright comments contained therein. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}], "references": [{"title": "A new dataset for finegrained citation field extraction", "author": ["Anzaroot", "McCallum2013] Sam Anzaroot", "Andrew McCallum"], "venue": "In ICML Workshop on Peer Reviewing and Publishing Models", "citeRegEx": "Anzaroot et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anzaroot et al\\.", "year": 2013}, {"title": "Collective information", "author": ["Bunescu", "Mooney2004] Razvan Bunescu", "Raymond J Mooney"], "venue": null, "citeRegEx": "Bunescu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2004}, {"title": "Structured learning with constrained conditional models", "author": ["Chang et al.2012] M. Chang", "L. Ratinov", "D. Roth"], "venue": "Machine Learning,", "citeRegEx": "Chang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2012}, {"title": "Combining local and non-local information with dual decomposition for named entity recognition from text", "author": ["Chieu", "Teow2012] Hai Leong Chieu", "Loo-Nin Teow"], "venue": "In Information Fusion (FUSION),", "citeRegEx": "Chieu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2012}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["Trond Grenager", "Christopher Manning"], "venue": "In Proceedings of the 43rd Annual Meeting on Association", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "Friedman2009] Daphne Koller", "Nir Friedman"], "venue": null, "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Mrf optimization via dual decomposition: Message-passing revisited", "author": ["Nikos Paragios", "Georgios Tziritas"], "venue": "In Computer Vision,", "citeRegEx": "Komodakis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Komodakis et al\\.", "year": 2007}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["Koo et al.2010] Terry Koo", "Alexander M Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Implicitly intersecting weighted automata using dual decomposition", "author": ["Paul", "Eisner2012] Michael J Paul", "Jason Eisner"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Paul et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2012}, {"title": "Accurate information extraction from research papers using conditional random fields", "author": ["Peng", "McCallum2004] Fuchun Peng", "Andrew McCallum"], "venue": null, "citeRegEx": "Peng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2004}, {"title": "A linear programming formulation for global inference in natural language", "author": ["Roth", "Yih2004] Dan Roth", "Wen-tau Yih"], "venue": null, "citeRegEx": "Roth et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2004}, {"title": "A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing", "author": ["Rush", "Collins2012] Alexander M. Rush", "Michael Collins"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Rush et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2012}, {"title": "On dual decomposition and linear programming relaxations for natural language processing", "author": ["David Sontag", "Michael Collins", "Tommi Jaakkola"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods", "citeRegEx": "Rush et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2010}, {"title": "Improved parsing and pos tagging using inter-sentence consistency constraints", "author": ["Roi Reichart", "Michael Collins", "Amir Globerson"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Rush et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2012}, {"title": "Learning hidden markov model structure for information extraction", "author": ["Andrew McCallum", "Roni Rosenfeld"], "venue": "Workshop on Machine Learning for Information Extraction,", "citeRegEx": "Seymore et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Seymore et al\\.", "year": 1999}, {"title": "Introduction to dual decomposition for inference", "author": ["Sontag et al.2011] David Sontag", "Amir Globerson", "Tommi Jaakkola"], "venue": null, "citeRegEx": "Sontag et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2011}, {"title": "Approximate Inference in Graphical Models using LP Relaxations", "author": ["David Sontag"], "venue": "Ph.D. thesis,", "citeRegEx": "Sontag.,? \\Q2010\\E", "shortCiteRegEx": "Sontag.", "year": 2010}, {"title": "Collective segmentation and labeling of distant entities in information extraction", "author": ["Sutton", "McCallum2004] Charles Sutton", "Andrew McCallum"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2004}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Thomas Hofmann", "Thorsten Joachims", "Yasemin Altun"], "venue": null, "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 13, "context": "Alternatively, one can employ dual decomposition (Rush et al., 2010).", "startOffset": 49, "endOffset": 68}, {"referenceID": 2, "context": "strated improvements in citation field extraction by imposing soft constraints (Chang et al., 2012).", "startOffset": 79, "endOffset": 99}, {"referenceID": 2, "context": "Using our new method, we are able to incorporate not only all the soft global constraints of Chang et al. (2012), but also far more complex data-driven constraints, while also providing stronger optimality certificates than their beam search technique.", "startOffset": 93, "endOffset": 113}, {"referenceID": 2, "context": "Using our new method, we are able to incorporate not only all the soft global constraints of Chang et al. (2012), but also far more complex data-driven constraints, while also providing stronger optimality certificates than their beam search technique. On a new, more broadly representative, and challenging citation field extraction data set, we show that our methods achieve a 17.9% reduction in error versus a linear-chain conditional random field. Furthermore, we demonstrate that our inference technique can use and benefit from the constraints of Chang et al. (2012), but that including our data-driven constraints on top of these is beneficial.", "startOffset": 93, "endOffset": 573}, {"referenceID": 17, "context": "The MAP inference task in a CRF be can expressed as an optimization problem with a linear objective (Sontag, 2010; Sontag et al., 2011).", "startOffset": 100, "endOffset": 135}, {"referenceID": 16, "context": "The MAP inference task in a CRF be can expressed as an optimization problem with a linear objective (Sontag, 2010; Sontag et al., 2011).", "startOffset": 100, "endOffset": 135}, {"referenceID": 7, "context": "Dual Decomposition is a popular method for performing MAP inference in this scenario, since it leverages known algorithms for MAP in the base problem where these extra constraints have not been added (Komodakis et al., 2007; Sontag et al., 2011; Rush and Collins, 2012).", "startOffset": 200, "endOffset": 269}, {"referenceID": 16, "context": "Dual Decomposition is a popular method for performing MAP inference in this scenario, since it leverages known algorithms for MAP in the base problem where these extra constraints have not been added (Komodakis et al., 2007; Sontag et al., 2011; Rush and Collins, 2012).", "startOffset": 200, "endOffset": 269}, {"referenceID": 4, "context": "All we need to employ the structured perceptron algorithm (Collins, 2002) or the structured SVM algorithm (Tsochantaridis et al.", "startOffset": 58, "endOffset": 73}, {"referenceID": 19, "context": "All we need to employ the structured perceptron algorithm (Collins, 2002) or the structured SVM algorithm (Tsochantaridis et al., 2004) is a blackbox procedure for performing MAP inference in the structured linear model given an arbitrary cost vector.", "startOffset": 106, "endOffset": 135}, {"referenceID": 2, "context": "These are same global constraints that were used for citation field extraction in Chang et al. (2012). We define s(i) to be the number of times the label with index i is predicted in a citation, formally:", "startOffset": 82, "endOffset": 102}, {"referenceID": 5, "context": "Inference in these models can be performed, for example, with loopy belief propagation (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or Gibbs sampling (Finkel et al., 2005).", "startOffset": 159, "endOffset": 180}, {"referenceID": 5, "context": "While Gibbs sampling has been shown to work well tasks such as named entity recognition (Finkel et al., 2005), our previous experiments show that it does not work well for citation extraction, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF.", "startOffset": 88, "endOffset": 109}, {"referenceID": 8, "context": "Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP (Koo et al., 2010; Rush et al., 2010; Rush and Collins, 2012; Paul and Eisner, 2012; Chieu and Teow, 2012).", "startOffset": 115, "endOffset": 221}, {"referenceID": 13, "context": "Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP (Koo et al., 2010; Rush et al., 2010; Rush and Collins, 2012; Paul and Eisner, 2012; Chieu and Teow, 2012).", "startOffset": 115, "endOffset": 221}, {"referenceID": 4, "context": ", 2010; Rush and Collins, 2012; Paul and Eisner, 2012; Chieu and Teow, 2012). Soft constraints can be implemented inefficiently using hard constraints and dual decomposition\u2014 by introducing copies of output variables and an auxiliary graphical model, as in Rush et al. (2012). However, at every iteration of dual decomposition, MAP must be run in this auxiliary model.", "startOffset": 17, "endOffset": 276}, {"referenceID": 15, "context": "Hidden Markov models (HMMs), were originally employed for automatically extracting information from research papers on the CORA dataset (Seymore et al., 1999; Hetzner, 2008).", "startOffset": 136, "endOffset": 173}, {"referenceID": 2, "context": "Recent work on globally-constrained inference in citation extraction used an HMM , which is an HMM with the addition of global features that are restricted to have positive weights (Chang et al., 2012).", "startOffset": 181, "endOffset": 201}, {"referenceID": 2, "context": "Recent work on globally-constrained inference in citation extraction used an HMM , which is an HMM with the addition of global features that are restricted to have positive weights (Chang et al., 2012). Approximate inference is performed using beam search. This method increased the HMM token-level accuracy from 86.69 to 93.92 on a test set of 100 citations from the CORA dataset. The global constraints added into the model are simply that each label only occurs once per citation. This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA (Peng and McCallum, 2004). In our experiments, we demonstrate that the specific global constraints used by Chang et al. (2012) help on the UMass dataset as well.", "startOffset": 182, "endOffset": 787}, {"referenceID": 2, "context": "line CRF than the HMM results presented in Chang et al. (2012), which include soft constraints.", "startOffset": 43, "endOffset": 63}, {"referenceID": 2, "context": "line CRF than the HMM results presented in Chang et al. (2012), which include soft constraints. Given this high performance of our base model on CORA, we did not apply our Soft-DD algorithm to the dataset. Furthermore, since the dataset is so small, learning the penalties for our large collection of constraints is difficult, and test set results are unreliable. Rather than compare our work to Chang et al. (2012) via results on CORA, we apply their constraints on the UMass data using Soft-DD and demonstrate accuracy gains, as discussed above.", "startOffset": 43, "endOffset": 416}], "year": 2014, "abstractText": "Accurately segmenting a citation string into fields for authors, titles, etc. is a challenging task because the output typically obeys various global constraints. Previous work has shown that modeling soft constraints, where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation performance. On the other hand, for imposing hard constraints, dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference. We extend dual decomposition to perform prediction subject to soft constraints. Moreover, with a technique for performing inference given soft constraints, it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training. This allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset.", "creator": "LaTeX with hyperref package"}}}