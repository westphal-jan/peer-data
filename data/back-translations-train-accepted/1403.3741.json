{"id": "1403.3741", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2014", "title": "Near-optimal Reinforcement Learning in Factored MDPs", "abstract": "Any learning algorithm over Markov decision processes (MDPs) will have worst-case regret $\\Omega(\\sqrt{SAT})$ where $T$ is the elapsed time and $S$ and $A$ are the cardinalities of the state and action spaces. In many settings of interest $S$ and $A$ may be so huge that it is impossible to guarantee good performance for an arbitrary MDP on any practical timeframe $T$. We show that, if we know the true system can be represented as a \\emph{factored} MDP, we can obtain regret bounds which scale polynomially in the number of \\emph{parameters} of the MDP, which may be exponentially smaller than $S$ or $A$. Assuming an algorithm for approximate planning and knowledge of the graphical structure of the underlying MDP, we demonstrate that posterior sampling reinforcement learning (PSRL) and an algorithm based upon optimism in the face of uncertainty (UCRL-Factored) both satisfy near-optimal regret bounds.", "histories": [["v1", "Sat, 15 Mar 2014 01:56:02 GMT  (18kb)", "http://arxiv.org/abs/1403.3741v1", null], ["v2", "Fri, 6 Jun 2014 23:17:54 GMT  (18kb)", "http://arxiv.org/abs/1403.3741v2", null], ["v3", "Fri, 31 Oct 2014 23:34:32 GMT  (21kb)", "http://arxiv.org/abs/1403.3741v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ian osband", "benjamin van roy"], "accepted": true, "id": "1403.3741"}, "pdf": {"name": "1403.3741.pdf", "metadata": {"source": "CRF", "title": "Near-optimal Regret Bounds for Reinforcement Learning in Factored MDPs", "authors": ["Ian Osband", "Benjamin Van Roy"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 3.37 41v1 [st at.M L] 15 Mar 2Any learning algorithm on Markov decision-making processes (MDPs) will show remorse at worst if T is the expired time and S and A are the cardinals of the state and the scope for action. In many areas of interest, S and A can be so great that it is impossible to guarantee good performance for an arbitrary MDP in any practical timeframe T. We show that if we know that the true system can be presented as factored MDP, we can reach limits of regret, which can be polynomically scaled in the number of parameters of the MDP, which can be exponentially smaller than S or A. Assuming an algorithm for approximate planning and knowledge of the graphical structure of the underlying MDP, we show that posterioral sampling reinforcement learning (PSRL) and an algorithm based on optimism in the face of optimum factored uncertainty (both) will satisfy both."}, {"heading": "1 Introduction", "text": "The classic amplification problem looks at an agent who has to make sequential decisions within his environment while trying to maximize the rewards accumulated over time. [1, 2] The environment is modeled as a Markov decision-making process (MDP), but the agent is uncertain about the true dynamics of the MDP. The agent must plan actions to maximize rewards based on his imperfect knowledge, but he also learns about his environment through experience. Effective amplification learning manages this tradeoff between exploration and exploitation in such a way that the deviation from the optimal policy of perfect information is controlled. Factored MDPs [3] allow us to present large structured MDPs compactly. A state is described by a selection of state variables whose transitions can be represented by a dynamic Bayesian network (DBN). [4] This is particularly beneficial when the transition of a state variable depends on only a small subset of other variables."}, {"heading": "2 Problem formulation", "text": "We look at the problem of learning to optimize a random finite horizon MDP M = (S, A, RM, PM, \u03c4, \u03c1) in repeated finite interaction episodes. This is the same formula as previous work [13], which we reproduce here for completeness. S is the state space, A is the action space, RM (s, a) is a probability distribution over R in selecting an action for a while in the state s, PM (s, a) is the probability of transition to the state s \"if the action a is selected, while in the state s is the time horizon and the initial state distribution. We define the MDP and all other random variables that we will look at in relation to a probability space."}, {"heading": "3 Factored MDPs", "text": "In order to formalize our definition of a factored MDP, we introduce some notations commonly used in literature: [9] Reward and reward. (Definition 1 (Scope Operation for factored sets X = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D = 4D"}, {"heading": "4 Results", "text": "We present two algorithms, PSRL and UCRL-Factored, with efficient repentance limits via factored MDPs (PSRL). Full details of these algorithms are in Section 6.Our first result shows that we can bind the expected repentance of PSRL theorem 1 (Expected repentance for PSRL in factored MDPs). Let M-Factored with graph structure G = ({Si} mi = 1; {ZRi} li = 1; {ZPi} mi = 1; If the distribution of UCRL components is the range of optimal value function, then we can bind the regret of the PSRL."}, {"heading": "4.1 Interpretting regret bounds", "text": "The boundaries for PSRL's and UCRL's factored are qualitatively similar and largely share the same analysis. For any algorithm, the regret is O-PSRL and the scaled diameter CD for UCRL-Factored is. The span of an MDP is defined as the measure of MDP's connectivity, however, whenever the regret is E-PSRL's and the scaled diameter CD for UCRL-Factored is. The diameter of an MDP's is defined as the measure of MDP's connectivity, but whenever the regret is E-PSRL's and the scaled diameter CD for UCRL-Factored is. The diameter of an MDP's \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\""}, {"heading": "5 Confidence sets", "text": "Our analysis will be based on the construction of trust sets based on empirical estimates of the underlying reward and transitional functions. (These trust sets are chosen so that at the beginning of each episode k the true and sampled functions are included within the high probability confidence set. (We will then determine the deviation between the true function and the elements in the confidence set by the maximum deviation within the confidence.) This technique is common in the literature and follows the same arguments as numerous previous work on the subject [12, 26, 13]. Consider a family of functions F MX, (Y, Y) which leads x-x to a probability distribution over (Y, Y) measurable space. We will write this as MX, Y unless we want to emphasize the dependence on a particular algebra that is unavoidable. Definition 4 (Set widths): X is a finite set, and we let it be Y (measurable space)."}, {"heading": "6 Algorithms", "text": "Both algorithms require prior knowledge of G = ({Si} mi = 1; {Xi} ni = 1; {ZRi} li = 1; {ZPi} mi = 1; \u03c4), the graphical structure of the FMDP. They also assume access to a \"black box\" that performs approximate dynamic programming for FMDPs. (PSRL) requires a protocol that requires a single MDP M and approximate optimal policy for M. UCRLFactored requires that a family of MDPs consider M and output approximate optimization for the most optimized M. Generally, it will be much more difficult to obtain an approximate solver for M than the previous treatment. PSRL remains identical to previous treatments [27, 13] if G is encoded in the previous treatment. UCRL-Factored is essentially UCRL2 [12] modified to yield G in graph and episodic structure."}, {"heading": "7 Analysis", "text": "We will now compile the necessary analysis for our main results: First, we will review the analysis of PSRL and UCRL2, which allow us to regret the error. Next, we will show that for factored MDPs, it is possible to bind this estimation error to the error in each factored component separately. From here, we will apply concentration inequalities to the individual factors to show that the true MDP M * is highly likely to be within Mk for all k, and the final results will be obtained by applying Theorem 3."}, {"heading": "7.1 From regret to Bellman error", "text": "For many amplification learning algorithms, there is no clean way to relate the unknown optimal policy to the states and actions observed by the agent. Using the OFU principle, we can guarantee with high probability that the optimal rewards of the true MDP (posterior sampling) are higher than the optimal rewards of the optimistic MDP [11]. In the case of posterior sampling, we use the posterior sampling problem [17] Lemma 1 (posterior sampling). If the distribution of the MDP is then the optimal distribution of the MDP (Htk) measurable function g, E [g]."}, {"heading": "7.2 Factorization decomposition", "text": "We will now show how we can further bind the equation (11) by the sums of errors in each factor of the reward and transition functions. It is quite clear that we can limit the deviations of R \u00b2, R \u00b2 k upwards by the sum of the deviations of their factors using the triangular inequality. In fact, as we show in Lemma 3, we can also do this for the transition functions P \u00b2 and P \u2212 k. This result is really the key finding for our results in this paper. Lemma 3 (Limitation of factored deviations). Let us leave the transition function class P \u00b2 PX, S after X = X1 \u00d7 Xn and S \u2212 k. \u00b7 Sm with the scopes Z1,.. Zm. Then, for each P \u00b2 P \u00b2 P we can limit their deviations by the sum of the differences of their factorings to 1 \u00b2 p."}, {"heading": "7.3 Concentration guarantees for Mk", "text": "We now want to show that the true MDP is within Mk with a high probability. Note that the posterior sampling method also allows us to say that the sampled Mk is within Mk with a high probability. To show this, we first present a concentration result for the L1 deviation of empirical probabilities. Lemma 4 (L1 limits for the empirical transition function). For all finite sentences X, finite sentences Y, function classes P PX, Y then for each x deviation X, 0: P (x) - P limits (x) - 1) - finite sentences X - 22) Proof. This is a relativization of the result proven by Weissman [28]. Lemma 4 ensures that for each x deviation X, j = 1, m P (x) - finite sentences P (x) - finite sentences X - 22) - proof."}, {"heading": "7.4 Regret bounds", "text": "We begin with the analysis of the PSRL. Using Equation (12) and the posterior sampling lexicon, we can say that P (M), Mk, Mk, Mk, k, k, N), [1], [1] and Theorem 3, to say that the contributions of near-optimal regret in the planning function, [1], are limited by [2], [4], [3], [4], [3], [3], [4, [4], [4,], [4,] [4,], [4], [4], [4], [4], [5] and [5], [5], [5], [5], [5], [6] and [6], [6] and [7], [7] and [8], [8]."}, {"heading": "A Bounding the widths of confidence sets", "text": "We present elementary arguments culminating in a proof of the Theorem 3rd Lemma 6 (concentration results for each single run (x). (For all finite sentences X and each run X). (for all finite sentences X and each run X). (for all finite sentences X and each run X (x). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (x. (sK). (xsK). (x. (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (xsK). (x. (xsK). (xsK). (x. (xsK). (xsK). (xsK). (x. (xsK). (xsK). (xsK). (xsK). (xsK). (x. (xsK). (xsK). (xsK). (sK). (sK). (sK). (sK). (sK). (x. (sK). (sK)."}, {"heading": "B Clean bounds for the symmetric problem", "text": "We now give concrete upper limits for theorems 1 and 2 in the simple symmetrical case l + 1 = m, C = \u03c3 = 1, | Si | = | Xi | = K and | ZRi | = | ZPi | = \u043c for all suitable i and write J = K\u0445. For a non-trivial problem we assume that K \u2265 2, m = 2, \u03c4 \u2265 2.From section 7.4 we have that E [Regret (T, \u03c0PSligence, M)] \u2264 4 + 2 \u221a T + m {4 (\u03c4J + 1) + 4 \u0432 (4mJD + 1) + 8 log (4mJD + 2) JD} + E [pending] (1 + 4T \u2212 4T \u2212 4) m {4 (UCJ + 1) + 4mJD 2 / 4mJD} By looking at the constant term we know that the limits are trivially satisfied."}], "references": [{"title": "Optimal adaptive policies for Markov decision processes", "author": ["A.N. Burnetas", "M.N. Katehakis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Stochastic systems: estimation, identification and adaptive control", "author": ["P.R. Kumar", "P. Varaiya"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Stochastic dynamic programming with factored representations", "author": ["Craig Boutilier", "Richard Dearden", "Mois\u00e9s Goldszmidt"], "venue": "Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Learning dynamic bayesian networks. In Adaptive processing of sequences and data structures, pages 168\u2013197", "author": ["Zoubin Ghahramani"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Efficient reinforcement learning in factored MDPs", "author": ["Michael Kearns", "Daphne Koller"], "venue": "In IJCAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Model-based reinforcement learning in factored-state MDPs", "author": ["Alexander Strehl"], "venue": "In Approximate Dynamic Programming and Reinforcement Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Optimistic initialization and greediness lead to polynomial time learning in factored MDPs", "author": ["Istv\u00e1n Szita", "Andr\u00e1s L\u0151rincz"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Efficient structure learning in factored-state MDPs", "author": ["Alexander Strehl", "Carlos Diuk", "Michael Littman"], "venue": "In AAAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "arXiv preprint cs/9605103,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "More) Efficient Reinforcement Learning via Posterior Sampling", "author": ["Ian Osband", "Daniel Russo", "Benjamin Van Roy"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1933}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Thompson sampling: an asymptotically optimal finite time analysis", "author": ["E. Kauffmann", "N. Korda", "R. Munos"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "CoRR, abs/1301.2609,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Efficient solution algorithms for factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venkataraman"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Policy iteration for factored MDPs", "author": ["Daphne Koller", "Ronald Parr"], "venue": "In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Max-norm projections for factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr"], "venue": "In IJCAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Efficient solutions to factored MDPs with imprecise transition probabilities", "author": ["Karina Valdivia Delgado", "Scott Sanner", "Leliane Nunes De Barros"], "venue": "Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Approximate linear programming for first-order MDPs", "author": ["Scott Sanner", "Craig Boutilier"], "venue": "arXiv preprint arXiv:1207.1415,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Online regret bounds for undiscounted continuous reinforcement learning", "author": ["Ronald Ortner", "Daniil Ryabko"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Regret bounds for the adaptive control of linear quadratic systems", "author": ["Yasin Abbasi-Yadkori", "Csaba Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Generalization and exploration via randomized value functions", "author": ["Benjamin Van Roy", "Zheng Wen"], "venue": "arXiv preprint arXiv:1402.0635,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Regal: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["P.L. Bartlett", "A. Tewari"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction The classic reinforcement learning problem considers an agent who must make sequential decisions within its environment while trying to maximize total reward accumulated over time [1, 2].", "startOffset": 195, "endOffset": 201}, {"referenceID": 1, "context": "1 Introduction The classic reinforcement learning problem considers an agent who must make sequential decisions within its environment while trying to maximize total reward accumulated over time [1, 2].", "startOffset": 195, "endOffset": 201}, {"referenceID": 2, "context": "Factored MDPs [3] allow us to represent large structured MDPs compactly.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "A state is described by a selection of state variables, whose transitions can be represented by a dynamic Bayesian network (DBN) [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "Kearns and Koller extend the E algorithm [5, 6] to exploit the DBN structure and obtain probably approximately correct (PAC) bounds with polynomial sample complexity.", "startOffset": 41, "endOffset": 47}, {"referenceID": 5, "context": "Kearns and Koller extend the E algorithm [5, 6] to exploit the DBN structure and obtain probably approximately correct (PAC) bounds with polynomial sample complexity.", "startOffset": 41, "endOffset": 47}, {"referenceID": 6, "context": "There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9].", "startOffset": 59, "endOffset": 65}, {"referenceID": 7, "context": "There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9].", "startOffset": 59, "endOffset": 65}, {"referenceID": 8, "context": "There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 9, "context": "Some algorithms do seek to learn this structure from experience [10], but we will assume this structure is known.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "These bound the difference in accumulated rewards of a learning algorithm and the optimal policy over T steps [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "[12] present UCRL2, which attains near-optimal regret of \u00d5(S \u221a AT ) with high probability.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] analyze PSRL, which also provides bounds on the expected regret of \u00d5(S \u221a AT ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "UCRL-Factored is guided by the OFU principle whereas PSRL is guided by posterior (also known as Thompson) sampling [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].", "startOffset": 79, "endOffset": 91}, {"referenceID": 15, "context": "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].", "startOffset": 79, "endOffset": 91}, {"referenceID": 16, "context": "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].", "startOffset": 79, "endOffset": 91}, {"referenceID": 12, "context": "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "However, even where an FMDP is able to represented concisely, solving for the optimal policy may still be intractable in the most general case [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 4, "context": "Our focus in this paper is upon the statistical aspect of the learning problem and like earlier discussions [5] we do not specify which computational methods are used.", "startOffset": 108, "endOffset": 111}, {"referenceID": 18, "context": "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].", "startOffset": 77, "endOffset": 93}, {"referenceID": 19, "context": "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].", "startOffset": 77, "endOffset": 93}, {"referenceID": 20, "context": "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].", "startOffset": 77, "endOffset": 93}, {"referenceID": 21, "context": "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].", "startOffset": 77, "endOffset": 93}, {"referenceID": 2, "context": "Factored MDPs are an approach with successful applications in many fields [3] but they are not the only one.", "startOffset": 74, "endOffset": 77}, {"referenceID": 22, "context": "There are regret bounds for continuous state spaces where the underlying reward and transition functions are known to belong to some function class, such as H\u00f6der continuous [23] or linear quadratic control [24].", "startOffset": 174, "endOffset": 178}, {"referenceID": 23, "context": "There are regret bounds for continuous state spaces where the underlying reward and transition functions are known to belong to some function class, such as H\u00f6der continuous [23] or linear quadratic control [24].", "startOffset": 207, "endOffset": 211}, {"referenceID": 24, "context": "Value-based approaches typically struggle to plan efficient exploration and so cannot obtain efficient learning guarantees, although there has been interesting progress in this field as well [25].", "startOffset": 191, "endOffset": 195}, {"referenceID": 12, "context": "This is the same formulation as earlier work [13], which we reproduce here for completeness.", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "3 Factored MDPs To formalize our definition of a factored MDP we introduce some notation common to the literature [9].", "startOffset": 114, "endOffset": 117}, {"referenceID": 25, "context": "There is an optimistic algorithm REGAL [26] which formally 4", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "This is accomplished by imposing artificial episodes which end whenever the number of visits to a state-action pair is doubled [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "Nevertheless, there has been good empirical performance using this method for non-factored MDPs without episodic reset in simulation [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].", "startOffset": 117, "endOffset": 129}, {"referenceID": 25, "context": "This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].", "startOffset": 117, "endOffset": 129}, {"referenceID": 12, "context": "This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].", "startOffset": 117, "endOffset": 129}, {"referenceID": 26, "context": "PSRL remains identical to earlier treatment [27, 13] provided G is encoded in the prior \u03c6.", "startOffset": 44, "endOffset": 52}, {"referenceID": 12, "context": "PSRL remains identical to earlier treatment [27, 13] provided G is encoded in the prior \u03c6.", "startOffset": 44, "endOffset": 52}, {"referenceID": 11, "context": "UCRL-Factored is essentially UCRL2 [12] modified to exploit G in graph and episodic structure.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "Using the OFU principle, we can guarantee with high probability that the optimal rewards of the true MDP are upper bounded by the optimal rewards of the optimistic MDP [11].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "In the case of posterior sampling, we make use of the posterior sampling lemma [17] Lemma 1 (Posterior Sampling).", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "By the OFU principle we know that \u03a8k \u2264 CD [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "\u2264 \u03b12 |\u03b11 \u2212 \u03b21|+ \u03b21 |\u03b12 \u2212 \u03b22| This result also holds for any \u03b11, \u03b12, \u03b21, \u03b22 \u2208 [0, 1], where including 0 can be verified case by case.", "startOffset": 77, "endOffset": 83}], "year": 2017, "abstractText": "Any learning algorithm over Markov decision processes (MDPs) will have worst-case regret \u03a9( \u221a SAT ) where T is the elapsed time and S and A are the cardinalities of the state and action spaces. In many settings of interest S and A may be so huge that it is impossible to guarantee good performance for an arbitrary MDP on any practical timeframe T . We show that, if we know the true system can be represented as a factored MDP, we can obtain regret bounds which scale polynomially in the number of parameters of the MDP, which may be exponentially smaller than S or A. Assuming an algorithm for approximate planning and knowledge of the graphical structure of the underlying MDP, we demonstrate that posterior sampling reinforcement learning (PSRL) and an algorithm based upon optimism in the face of uncertainty (UCRL-Factored) both satisfy near-optimal regret bounds.", "creator": "LaTeX with hyperref package"}}}