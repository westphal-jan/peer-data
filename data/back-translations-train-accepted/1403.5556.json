{"id": "1403.5556", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2014", "title": "Learning to Optimize via Information-Directed Sampling", "abstract": "This paper proposes information directed sampling--a new algorithm for balancing between exploration and exploitation in online optimization problems in which a decision-maker must learn from partial feedback. The algorithm quantifies the amount learned by selecting an action through an information theoretic measure: the mutual information between the true optimal action and the algorithm's next observation. Actions are then selected by optimizing a myopic objective that balances earning high immediate reward and acquiring information. We show this algorithm is provably efficient and is empirically efficient in simulation trials. We provide novel and general regret bounds that scale with the entropy of the optimal action distribution. Furthermore, as we highlight through several examples, information directed sampling sometimes dramatically outperforms popular approaches like UCB algorithms and Thompson sampling which don't quantify the information provided by different actions.", "histories": [["v1", "Fri, 21 Mar 2014 02:02:25 GMT  (17kb)", "https://arxiv.org/abs/1403.5556v1", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v2", "Sun, 8 Jun 2014 20:40:38 GMT  (172kb,D)", "http://arxiv.org/abs/1403.5556v2", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v3", "Thu, 3 Jul 2014 01:09:22 GMT  (174kb,D)", "http://arxiv.org/abs/1403.5556v3", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v4", "Tue, 22 Jul 2014 17:59:30 GMT  (175kb,D)", "http://arxiv.org/abs/1403.5556v4", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v5", "Fri, 12 Aug 2016 06:53:32 GMT  (257kb,D)", "http://arxiv.org/abs/1403.5556v5", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v6", "Wed, 24 May 2017 00:06:24 GMT  (257kb,D)", "http://arxiv.org/abs/1403.5556v6", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v7", "Fri, 7 Jul 2017 05:51:15 GMT  (259kb,D)", "http://arxiv.org/abs/1403.5556v7", "arXiv admin note: substantial text overlap witharXiv:1403.5341"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1403.5341", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel russo 0001", "benjamin van roy"], "accepted": true, "id": "1403.5556"}, "pdf": {"name": "1403.5556.pdf", "metadata": {"source": "CRF", "title": "Learning to Optimize Via Information-Directed Sampling", "authors": ["Daniel Russo", "Benjamin Van Roy"], "emails": ["daniel.russo@kellogg.northwestern.edu", "bvr@stanford.edu"], "sections": [{"heading": null, "text": "Each action is sampled in a way that minimizes the relationship between the expected quadratic repentance of a single period and a measure of information gain: the mutual information between the optimal action and the next observation. We note an expected repentance that is limited to information-oriented samples that are applied across a very general class of models and scales with the entropy of optimal action distribution. We illustrate, using simple analytical examples, how information-oriented sampling takes into account types of information that do not adequately square alternative approaches and that this can lead to dramatic performance gains. For the widely studied Bernoulli, Gaussian and linear bandit problems, we show state-of-the-art simulation performance."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most people are able to survive themselves, and that they are able to survive themselves, and that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () () () () () (() ((() () () () ((() (() (() () () (() () (() ((() () ((() (() (() (() () () (() () () (() () (((() (() () (() (() ((()) (()) ((((((()))) (((((("}, {"heading": "2 Literature review", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "3 Problem formulation", "text": "We consider a general probability calculation, or Bayesian, formulation, in which uncertain quantities are modeled as random variables. (The decision maker selects actions sequentially (At) t-N from a finite action set A and observes the corresponding results (Yt, At) t-N. There is a random result Yt, a-Y, which is associated with each action. (Yt) t-N is an id sequence. This can be regarded as a Bayesian formulation in which randomness is thought of in the preceding uncertainty of the decision maker about the true nature of the system. (Yt) t-N is an optimal action. The agent associates a reward R (y) with each result."}, {"heading": "4 Algorithm design principles", "text": "The primary contribution of this paper is information-oriented sampling (IDS), a general principle for designing action selection algorithms. We will define IDS in this section after discussing the underlying motivations behind its structure. We will also use a number of examples to illustrate how alternative design principles do not take certain types of information into account and can therefore be dramatically surpassed by IDS."}, {"heading": "4.1 Motivation", "text": "This is achieved through a Bayes-optimal policy, which can in principle be calculated using dynamic programming. Unfortunately, we are motivated by contexts in which the time horizon T is \"large.\" For large T and moderate T, there is considerable interest in the development of computationally efficient heuristics. The depiction of the state of belief to action prescribed by the Bayes-optimal policy does not vary significantly from one time horizon to the next. As such, it makes sense to limit attention to stationary heuristic measures. IDS falls into this category. IDS is largely motivated by the desire to overcome inadequacies of the currently popular design principles. In particular, it takes into account types of information that do not adequately address alternatives. IDS can select an action to receive useful feedback."}, {"heading": "4.2 Information-directed sampling", "text": "In fact, it is the case that politicians will be able to play by the rules. (...) It is very likely that they will play by the rules. (...) It is very likely that they will play by the rules. (...) It is very likely that they will play by the rules. (...) It is very likely that they will play by the rules. (...) It is very likely that they will play by the rules. (...) It is very likely that they will play by the rules. (...) It is very likely that they will play by the rules. \"(...) It is very likely that they will play by the rules.\""}, {"heading": "4.3 Alternative design principles", "text": "Several alternative design principles have played a prominent role in the literature. However, each of these principles does not adequately address one or more of the information categories listed in Section 4.1, which was the motivation for the development of IDS. In this section we will illustrate, using a number of examples, how the IDS takes such information into account while alternatives fail."}, {"heading": "4.3.1 Upper confidence bounds and Thompson sampling", "text": "The Upper Trust Limit (UCB) and Thompson Sampling (TS) are typically constructed to be optimistic (DB), two of the most popular principles for the balance between exploration and exploitation. As data is collected, both approaches will not only assess the benefits generated by various actions, but will also carefully monitor the uncertainty in their estimates, continuing to experiment with any actions that might be plausibly optimal given the data observed, guaranteeing actions that are not hastily discarded but that, unlike more na\u00efve approaches, the -greedy algorithms like, also ensure that the samples are not wasted on clearly sub-optimal actions. A UCB algorithm selects actions by two steps. First, for each action an upper trust limit Bt (a) is constructed, and then the algorithm selects an action from maxa (a) with maximum certainty. The Upper Trust Limit Bt (a) represents the greatest plausible statistical value of all."}, {"heading": "4.3.2 Other information-directed approaches", "text": "Another natural, information-oriented algorithm aims to maximize the acquired information about the uncertain model parameter and allows for an associated system. In particular, one should consider an algorithm that selects the action at a specific time to maximize the weighted combination of the expected reward - and the information it gathers about the uncertain model parameter depends on: Et [Rt, a] + \u03bbEs (Yt, a; \u03b8). In this section, we will refer to this algorithm to address the problem of repeatedly recommending products to a customer with unknown preferences. The next example emphasizes that it can invest in acquiring information about products that are irrelevant to the decision. Example 5. (unrestricted assortment optimization) Let's look again at the problem of repeatedly recommending products to a customer with unknown preferences. The referral system can select any subset of products that a product offers."}, {"heading": "4.3.3 Expected improvement and the knowledge gradient", "text": "We look at two algorithms that measure the quality of the best decision based on current information, and encourage you to collect observations that are expected to immediately increase this metric: the first is the expected improvement algorithm, which is one of the most widely used techniques in the active field of Bayesian optimization (see [13]); define the expected reward generated by a downstream action; and Vt = maxa \u2032 ul, which represents the best objective value achievable in light of current information; and the expected improvement of the action is defined as E [max {f3] to be the expected reward generated by a downstream action. (a) Vt} | Ft, where the Frankish action is generated the expected reward generated by an action, is the expected reward generated by an action; the expected improvement of the action is defined to be a high-performance action."}, {"heading": "5 Regret bounds", "text": "In this section, regret limits for information-driven sampling are set for some of the most frequently studied classes of online optimization problems, resulting from our recent information theory analysis of Thompson-Sampling [61]. In the next subsection, we specify that there are limits to any policy regarding its information relationship. Since the information relationship of IDS is always smaller than that of TS, the limits of TS's information relationship provided in Russo and Van Roy [61] immediately result in regret limits for IDS for a number of important problem classes."}, {"heading": "5.1 General bound", "text": "We start with a general result that limits the regrets of each policy in terms of its information relationship and the entropy of the optimal distribution of action. Remember, we have defined the information relationship of an action sample distribution to be carried out. Entropy of the optimal distribution of action H (\u03b11) captures the magnitude of the initial uncertainty of the decision-maker as to which action is optimal. Then, the next result can be interpreted as a limit of regret, depending on the cost of obtaining new information and the total amount of information to be acquired. Proposal 1. For each policy, the following applies: E = (E = 1, E = 2, E = 3,.) and the time T = N, E [E] [Regret (T, E)."}, {"heading": "5.2 Specialized bounds on the minimal information ratio", "text": "These limits show that the expected regrets of the algorithm can only be large in each period if it is expected to receive a lot of information about which action is optimal. In this sense, it effectively balances between exploration and exploitation in each period of time. For each problem, we will compare our upper limits in terms of expected regrets with known lower limits. The limits of the information relationship also help clarify the role it plays in our results: it roughly captures the extent to which the scanning of some actions allows the decision maker to draw conclusions about other actions. In the worst case, the ratio depends on the number of actions, which reflects the fact that actions could not provide information about others. For problems with complete information, the information relationship is limited by a numerical constant that reflects that the scanning of an action perfectly reveals the reward that would have been earned by the selection of other actions."}, {"heading": "5.2.1 Worst case bound", "text": "This means that there is always a random distribution of actions, which can be much lower within certain information structures between repentance and information gain. Proposal 2. For each t-N, the ratio between repentance and information gain can be almost certain. If you combine sentence 2 with conclusion 1, it shows that E [Regret (T, \u03c0IDS)] \u2264 1 2 | A | H (\u03b11) T."}, {"heading": "5.2.2 Full information", "text": "In this paper, we focus on problems with partial feedback. In such problems, what the decision-maker observes depends on the actions chosen, which leads to a tension between exploration and exploitation. Problems with complete information arise as an extreme point of our formulation, where the result Yt, a is perfectly revealed by observing Yt, a. Suppose for each t-N there is a random variable Zt: Z such that for each a-A, Yt, a = (a, Zt) the minimum information ratio is 1 / 2. Sentence 3. Suppose that for each t-N there is a random variable Zt: A, Yt, a = (a, Zt). Then for all t-N there is an almost certain connection of this result with sequence 1, that E [Regret (T, 2-IDS)] \u2264 1 (1-1) T."}, {"heading": "5.2.3 Linear optimization under bandit feedback", "text": "The stochastic linear bandit problem has been extensively investigated (e.g. [1, 24, 56]) and is one of the most important examples of a multi-armed bandit problem with \"correlated arms.\" In this context, each action is associated with a finite dimensional characteristic vector, and the mean reward generated by an action is the inner product between its known characteristic vector and an unknown parameter vector. Due to this structure, observations from the taking of an action allow the decision maker to draw conclusions about other actions. The next sentence limits the minimum information ratio for such problems. Proposal 4. If A [Regret], E [Regret], E [Rt], E [Rt, a] = aT for each action a \u00b2 A, then the decision maker can almost certainly draw conclusions about all t \u00b2 N. This result shows that E [Regret], E [Regret (T, \u03c0IDS) and [Rt = improvised distribution]."}, {"heading": "5.2.4 Combinatorial action sets and \u201csemi-bandit\u201d feedback", "text": "To motivate the information structure examined here, we consider a simple resource allocation problem. There are d possible projects, but the decision maker can allocate resources at most m \u2264 d of them at a time. At a time when t, project i,.., d, a random reward Xt, i, and the reward from the selection of a subset of projects a, a,. (0,.), the agent in selecting a subset of projects would only receive the general reward m \u2212 1, i,. It may be natural to assume instead that the result of each selected project (Xt, i, a) is observed. This kind of observation structure is sometimes referred to as \"semi-bandit.\""}, {"heading": "6 Computational methods", "text": "IDS provides an abstract design principle that captures some important qualitative properties of the Bayesian optimal solution while allowing for traceable calculations for many relevant problem classes. However, additional work is required to design efficient calculation methods that implement IDS for certain problem classes. In this section, we provide guidance and examples. In this section, we will focus on the problem of generating an action given the posterior distribution in due course. This eliminates the problem of calculating and displaying a posterior distribution that may pose its own challenges. Although IDS could be combined with approximate Bayesian inference methods, we will focus here on the simpler context in which posterior distributions can be efficiently calculated and stored, as is the case when working with traceable finite uncertainty theories or appropriately chosen conjugatories. However, it is worth noting that two of our IDS algorithms can not be calculated from samples obtained from the post-Carlo distributions, as is possible in Monte Carlo distributions or in Monte Carlo distributions."}, {"heading": "6.1 Evaluating the information ratio", "text": "Given a finite plot A = {1,.., K}, we can consider a plot \u03c0 as a K-dimensional vector of probabilities, and then write the information ratio so that it is t (\u03c0) = (\u03c0 > \u00b2) 2 \u03c0 > ~ g, where ~ \u0445 and ~ g are K-dimensional vectors with components. There is no general efficient method for calculating ~ \u0445 (k) and ~ g in the face of a posterior distribution, because this would require the calculation of integrals over potentially high-dimensional spaces. Such a calculation can often be carried out efficiently, using the functional form of specific posterior distribution and often performing numerical integration. To illustrate the design of problematic computing methods, we will give two simple examples in this subdivision. We will start with a conceptual form of specific posterior distribution and require numerical integration."}, {"heading": "R : Y 7\u2192 R is arbitrary. Let p1 be the prior probability mass function of \u03b8 and let q\u03b8,a(y) be the probability, conditioned on \u03b8, of observing y when action a is selected.", "text": "Note that the posterior probability mass function pt, due to observations made prior to the period t = \u03b22, can be calculated recursively via Bayes' rule (p + 1 (p)). \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "6.2 Optimizing the information ratio", "text": "Let us now discuss how to create a given action by selecting an action by solving an action resulting from the resulting distribution, and the following result states that (8) is a convex optimization problem and surprisingly has an optimal solution with a maximum of two non-zero components. Therefore, while IDS is a randomized policy, it is sufficient to randomize an action over two actions at random. Proposition 6. For all ~ g-RK +, a convex optimization problem resulting in a random action, ~ g-RK + is such an optimization problem that ~ g-RK + is such that ~ g-RK + is a random action resulting in a random action."}, {"heading": "6.3 Approximating the information ratio", "text": "Although relatively efficient algorithms can be developed to implement IDS for different problem classes, some applications, such as those arising from high-frequency web services, are very effective. (In this section we will discuss some useful approaches to acquiring information.) The dominant source of complexity in calculating computational operations and routines for evaluating integrals that may require integration over high-dimensional spaces is to replace integrals with sample-based estimates. Algorithm 4 does this in addition to the number of actions that K and routines perform for evaluating q and R, the algorithm takes representative examples of liabilities as input M. In the simplest application scenario, these independent examples would be drawn from posterior distribution. The steps correspond to those of the algorithms, but with the possible models approximated by the number of representative examples."}, {"heading": "7 Computational results", "text": "In Section 4.3, we have shown that alternative approaches such as UCB algorithms, Thompson sampling, and the Knowledge Gradient Algorithm can perform very poorly in the face of complicated information structures, and can therefore be dramatically outperformed by IDS. Instead, in this section, we will focus on simpler approaches where current approaches are most effective. We find that even with these simple and widely studied settings, information-oriented scanning does not take Horizon T as input and is instead designed to work well for all long enough horizons. Other algorithms we are simulating have been optimized for the specific horizon of the simulation study."}, {"heading": "7.1 Beta-Bernoulli bandit", "text": "It is the first experiment that outlines a multi-armed bandit problem with independent weapons and binary rewards. The mean reward of each individual arm is determined by beta (1, 1), which classifies the uniform distribution and means of each arm as insufficient to explore the results of 1000 independent attempts and a time horizon of 1000 years. We compared the performance of IDS with that of six other algorithms, and found that it has the lowest average regret of 18.0.The UCB1 algorithms of Auer et al, which maximizes the supreme trust in the action that binds the upper trust in the upper trust in the upper trust in the upper affiliation to the other algorithms that it puts in the lower dissemination of 18.0.The UCB1 algorithms of Auer et al al al al al, which maximizes the upper trust, which puts the upper trust in the upper trust in the upper trust, the upper trust in the trust in the upper trust in the upper it supervises the trust in the upper trust in the upper, the trust in the upper trust in the upper the trust in the upper"}, {"heading": "7.2 Independent Gaussian bandit", "text": "The second experiment deals with a different, multi-armed bandit problem with independent weapons. The reward value for each action follows a Gaussian distribution N (\u03b8a, 1). The results are shown in Figure 1b and Table 2.For this problem, we compare variance-based IDS against Thompson-Sampling, Bayes UCB and KG. We use the variance-based variant of IDS because it allows us to make computational advances.We also simulate the GPUCB of Srinivas et al. [67] This algorithm maximizes the upper limit of confidence \u00b5t (a) and \u03b2t\u0432\u0430\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043dknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknkn"}, {"heading": "7.3 Asymptotic optimality", "text": "The previous subsections present numerical examples in which IDS outperforms Bayes UCB and Thompson on some problems with independent arms. This is surprising, since each of these algorithms is known, in a sense that we will soon formalize, as asymptotically optimal for these problems. This section presents simulation results over a much longer time horizon that suggest IDS scales in the same asymptotically optimal way. We again consider a problem with binary rewards and independent actions. The action ai [a1,.., aK} yields a reward in each time horizon that is 1 with probability \u03b8i and 0 otherwise. The groundbreaking work of Lai and Robbins [49] provides the following asymptotic lower limit for regretting any policy action: lim inf T \u2192 \u221e E [Regret (T, \u03c0) | Celsius] yields a reward in each time horizon that 1 with probability \u0445i and 0 is otherwise groundbreaking for any policy action."}, {"heading": "7.4 Linear bandit problems", "text": "Our final numerical experiment deals with a linear bandit problem. Each action a-R5 is defined by a 5-dimensional feature vector. The reward of the action a in due time t is aT \u03b8 + t, where \u03b8-N (0, 10I) is drawn from a multivariate Gaussian predistribution, and t-N (0, 1) is independent Gaussian noise. In each period only the reward of the selected action is observed. In our experiment, the action table A contains 30 actions, each of which is drawn randomly from [\u2212 1 / \u221a 5, 1 / \u221a 5]. The results shown in Figure 3 and Table 5 compare regret with 2,000 independent trials.We simulate variance-based IDS using the implementation presented in Algorithm 6. We compare their regret with six competing algorithms. Like IDS, GP-UCB and Thompson sampling strong regret in relation to this problem."}, {"heading": "10 0.00298 0.000008 0.00002 0.00001 0.000146 0.001188", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "30 0.012597 0.000005 0.000009 0.000005 0.000097 0.003157", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "50 0.023084 0.000006 0.000009 0.000005 0.000094 0.005146", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "70 0.03913 0.000006 0.000009 0.000005 0.000098 0.006364", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.5 Runtime Comparison", "text": "In our experiments, Thompson sampling and UCB algorithms are extremely fast and sometimes require only a few microseconds to make a decision. As expected, our implementation of IDS requires significantly more computing time. In independent Gaussian models, certain integrals can often be calculated in a fraction of a second, which can be tolerated in many areas of application. In addition, IDS can be significantly accelerated by parallel processing or optimized implementation. In independent Gaussian models, certain integrals can be calculated using closed form expressions, allowing KG to be executed quickly. There is also a specialized numerical method for implementing KG for correlated (or linear) Gaussian models, but the computation is an order of magnitude slower than in the independent case. In correlated Gaussian models x, the policy of KG * is significantly slower than in both KG and IDS. In beta Bernoulli problem, however, a KG problem can be calculated very slowly, but very slowly."}, {"heading": "8 Conclusion", "text": "This paper proposes information-driven sampling - a new algorithm for online optimization problems where a decision maker must learn from partial feedback. We establish a general regret tied to the algorithm and specialize in several widely studied problem classes. We show that it sometimes far outperforms other popular approaches that are not careful enough."}, {"heading": "15 3 0.004305 0.000178 0.000139 0.000048 0.002709 0.311935", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "30 5 0.008635 0.000064 0.000048 0.000038 0.004789 0.589998", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "50 20 0.026222 0.000077 0.000083 0.000068 0.008356 1.051552", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "100 30 0.079659 0.000115 0.000148 0.00013 0.017034 2.067123", "text": "Finally, for some simple and widely studied classes of multi-arm bandit problems, we show simulation performance that exceeds conventional approaches, but many important unanswered questions remain. IDS solves a single-period optimization problem as a substitute for a persistent multi-period problem. Solving this one-period problem can be even arithmetically challenging, especially in cases where the number of actions is enormous or mutual information is difficult to evaluate. An important direction for future research concerns the development of mathematically elegant procedures for implementing IDS in important cases. Even if the algorithm cannot be implemented directly, one can hope to develop simple algorithms that capture its main advantages. Proposal 1 shows that any algorithm with a small information ratio satisfies strong regret limits. Thompson's sampling is a simple algorithm that, we suspect, sometimes has an almost minimal information ratio. Perhaps, simple schemes with a low-line information ratio can be developed for other important problem classes, such as bandits."}, {"heading": "9 Extensions", "text": "This section presents a number of ways in which the results and ideas discussed in this article can be extended. We will consider the use of algorithms such as information-oriented sampling for purely exploratory problems, a form of information-oriented sampling that aims to capture information about \u03b8 instead of A *, and a version of information-oriented sampling that uses a tuning parameter to control how aggressively the algorithm investigates. In any case, new theoretical guarantees can easily be established by using our analysis of information-oriented sampling."}, {"heading": "9.1 Pure exploration problems", "text": "Consider the problem of adaptive recording of observations (A1, Y1, A1,..) by conditional collection (A1, YT-1, AT-1) to minimize the expected loss of the best decision at a given time. (9) Let us remember that we have defined \"t\" (a): = E [Rt, A-Rt, a-Ft] to be the expected regret of the act a at a given time. This is a \"pure exploratory problem,\" in the sense that one is only interested in the finite regret (9) and not in the cumulative regret of the algorithm. However, the next proposal shows that the limits of cumulative expected regret on E [mina]."}, {"heading": "9.2 Using information gain about \u03b8", "text": "Information-driven scanning optimizes a goal achieved in a single period of time that balances a high immediate reward and obtains information. Information is quantified by using the mutual information between the true optimal action A * and the next observation by the algorithm. In this section, we will consider an algorithm that instead quantifies the amount that is learned by selecting an action a using mutual information. However, in some cases, such an algorithm can be computationally simple and at the same time provide reasonable statistical efficiency.We are introducing a modified form of information rate: = non-collection of information that is irrelevant to the decision problem. However, in some cases such an algorithm is simple while providing reasonable statistical efficiency.We are introducing a modified form of information rate."}, {"heading": "9.3 A tunable version of information-directed sampling", "text": "In this section, we present an alternative form of information-oriented scanning that depends on a tuning parameter \u03bb-R. Since \u03bb varies, the algorithm finds a different balance between exploration and exploration. The following suggestion provides remorse limits for this algorithm, provided \u03bb is sufficiently large. Suggestion 11. Repair all \u03bb-R so that each t-value (\u03c0IDSt) is almost certainly respected (1,.., T). If \u03c0 = (\u03c01, \u03c02,..) is defined so that it is not too long, but too long, that it is too long (A).D (A).B).D (\u03c0): = \u0445 t (zipiert) 2 \u2212 \u03bbgt (zipiert)}, (14) then E [Regret (T, zipiert).H (\u03b1) T.Evidence."}, {"heading": "Acknowledgements", "text": "We thank the anonymous referees for feedback and stimulating exchanges, Junyang Qian for correcting misjudgments in an earlier draft on Example 3 and 4, and Eli Gutin and Yashodan Kanoria for helpful suggestions. This work has been generously supported by a research grant from Boeing, a Marketing Research Award from Adobe, and the Burt and Deedee McMurty Stanford Graduate Fellowship."}, {"heading": "A Proof of Proposition 6", "text": "Proposition 6: For all others applies that Proposition 6: Proposition 6: Proposition 1: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 6: Proposition 7: Proposition 7: Proposition 7: Proposition 7: Proposition 7: Proposition 7: Proposition 7: Proposition 7: Proposition 7: 7: 7: 7 7: 7 7: 7 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7: 7:"}, {"heading": "B Proof of Proposition 1", "text": "The following fact expresses the mutual information between A \u221a and Yt, a as an expected decrease in entropy of A \u0445 = 1 \u0445 (N \u00b2) due to observance of Yt, a.Fact 1. (Lemma 5,5,6 by Gray [35]) It (A \u00b2; Yt, a) = E [H (\u03b1t) \u2212 H (\u03b1t + 1) | At = a, Ft] Proposition 1. For each policy \u03c0 = (\u03c01, \u03c02, \u03c03,.) and time T \u00b2 N, E [Regret (T, \u03c0)] \u2264 T (\u03c0) T (\u03b11) T. Where T (\u03c0) \u0445 T = 1 E\u03c0 [\u03c0t)] is the average expected information ratio under Exp.Proof. Since the policy \u03c0 is fixed throughout, we will simplify the notation and write off the final T (T \u00b2 E), with T \u00b2 t \u00b2 t (\u03b1T) and T \u00b2 t \u00b2 t (t \u00b2 E) following."}, {"heading": "C Proof of Proposition 7", "text": "Proposition 7: p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p (p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p = p = p = p p p = p = p = p p p p p = p = p p p"}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Y. Abbasi-Yadkori", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems, 24,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Online-to-confidence-set conversions and application to sparse stochastic bandits", "author": ["Y. Abbasi-Yadkori", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Asymptotically efficient adaptive allocation schemes for controlled iid processes: Finite parameter space", "author": ["R. Agrawal", "D. Teneketzis", "V. Anantharam"], "venue": "IEEE Transactions on Automatic Control, 34(3):258\u2013267,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Asymptotically efficient adaptive allocation schemes for controlled Markov chains: Finite parameter space", "author": ["R. Agrawal", "D. Teneketzis", "V. Anantharam"], "venue": "IEEE Transactions on Automatic Control, 34(12):1249\u20131259,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Further optimal regret bounds for Thompson sampling", "author": ["S. Agrawal", "N. Goyal"], "venue": "Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pages 99\u2013107,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 127\u2013135,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["J.-Y. Audibert", "S. Bubeck"], "venue": "Proceedings of the 22nd Annual Conference on Learning Theory (COLT), pages 217\u2013226,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Regret in online combinatorial optimization", "author": ["J.-Y. Audibert", "S. Bubeck", "G. Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, 47(2):235\u2013256,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Bayesian mixture modelling and inference based Thompson sampling in Monte-Carlo tree search", "author": ["A. Bai", "F. Wu", "X. Chen"], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc.,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Partial monitoringclassification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D.P. Foster", "D. P\u00e1l", "A. Rakhlin", "C. Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research, 39(4): 967\u2013997,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "venue": "Technical Report TR-2009-23,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Dynamic pricing under a general parametric choice model", "author": ["J. Broder", "P. Rusmevichientong"], "venue": "Operations Research, 60(4):965\u2013980,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and trends in machine learning, 5(1):1\u2013122,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-scale exploration of convex functions and bandit convex optimization", "author": ["S. Bubeck", "R. Eldan"], "venue": "Proceedings of the 29th Annual Conference on Learning Theory (COLT), pages 583\u2013589,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "X-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research, 12:1655\u20131695, June", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Bandit convex optimization:  \u221a T regret in one dimension", "author": ["S. Bubeck", "O. Dekel", "T. Koren", "Y. Peres"], "venue": "Proceedings of the 28st Annual Conference on Learning Theory (COLT),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Kullback-Leibler upper confidence bounds for optimal sequential allocation", "author": ["O. Capp\u00e9", "A. Garivier", "O.-A. Maillard", "R. Munos", "G. Stoltz"], "venue": "Annals of Statistics, 41(3):1516\u20131541,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian experimental design: A review", "author": ["K. Chaloner", "I. Verdinelli"], "venue": "Statistical Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian process optimization with mutual information", "author": ["E. Contal", "V. Perchet", "N. Vayatis"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "The price of bandit information for online optimization", "author": ["V. Dani", "S.M. Kakade", "T.P. Hayes"], "venue": "Advances in Neural Information Processing Systems, pages 345\u2013352,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["V. Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "Proceedings of the 21st Annual Conference on Learning Theory (COLT), pages 355\u2013366,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Parametric bandits: The generalized linear case", "author": ["S. Filippi", "O. Capp\u00e9", "A. Garivier", "C. Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems, 23:1\u20139,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Choosing a good toolkit, I: Formulation, heuristics, and asymptotic properties", "author": ["A. Francetich", "D.M. Kreps"], "venue": "preprint,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Choosing a good toolkit, II: Simulations and conclusions", "author": ["A. Francetich", "D.M. Kreps"], "venue": "preprint,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Paradoxes in learning and the marginal value of information", "author": ["P.I. Frazier", "W.B. Powell"], "venue": "Decision Analysis, 7(4):378\u2013403,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "A knowledge-gradient policy for sequential information collection", "author": ["P.I. Frazier", "W.B. Powell", "S. Dayanik"], "venue": "SIAM Journal on Control and Optimization, 47(5):2410\u20132439,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-Armed Bandit Allocation Indices", "author": ["J. Gittins", "K. Glazebrook", "R. Weber"], "venue": "John Wiley & Sons, Ltd,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "Journal of Artificial Intelligence Research, 42(1):427\u2013486,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "Advances in Neural Information Processing Systems, pages 766\u2013774,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Thompson sampling for complex online problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 100\u2013108,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled Markov chains", "author": ["T.L. Graves", "T.L. Lai"], "venue": "SIAM journal on control and optimization, 35(3):715\u2013743,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Entropy and information theory", "author": ["R.M. Gray"], "venue": "Springer,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Entropy search for information-efficient global optimization", "author": ["P. Hennig", "C.J. Schuler"], "venue": "The Journal of Machine Learning Research, 98888(1):1809\u20131837,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Predictive entropy search for multi-objective Bayesian optimization", "author": ["D. Hern\u00e1ndez-Lobato", "J.M. Hern\u00e1ndez-Lobato", "A. Shah", "R.P. Adams"], "venue": "arXiv preprint arXiv:1511.05467,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Predictive entropy search for efficient global optimization of black-box functions", "author": ["J.M. Hern\u00e1ndez-Lobato", "M.W. Hoffman", "Z. Ghahramani"], "venue": "Advances in neural information processing systems, pages 918\u2013926,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Predictive entropy search for Bayesian optimization with unknown constraints", "author": ["J.M. Hern\u00e1ndez-Lobato", "M.A. Gelbart", "M.W. Hoffman", "R.P. Adams", "Z. Ghahramani"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, Lille, France,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "Journal of Machine Learning Research, 11:1563\u20131600,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Twenty questions with noise: Bayes optimal policies for entropy loss", "author": ["B. Jedynak", "P.I. Frazier", "R. Sznitman"], "venue": "Journal of Applied Probability,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Refined knowledge-gradient policy for learning probabilities", "author": ["B. Kami\u0144ski"], "venue": "Operations Research Letters, 43(2):143\u2013147,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "On Bayesian upper confidence bounds for bandit problems", "author": ["E. kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Thompson sampling: an asymptotically optimal finite time analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "International Conference on Algorithmic Learning Theory,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "Proceedings of the 40th ACM Symposium on Theory of Computing,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "Cs. Szepesv\u00e1ri"], "venue": "In ECML,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise", "author": ["H.J. Kushner"], "venue": "Journal of Basic Engineering, 86(1):97\u2013106,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1964}, {"title": "Adaptive treatment allocation and the multi-armed bandit problem", "author": ["T.L. Lai"], "venue": "The Annals of Statistics, pages 1091\u20131114,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics, 6(1):4\u201322,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1985}, {"title": "On a measure of the information provided by an experiment", "author": ["D.V. Lindley"], "venue": "Annals of Mathematical Statistics, 78(4):986\u20131005,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1956}, {"title": "The application of Bayesian methods for seeking the extremum", "author": ["J. Mockus", "V. Tiesis", "A. Zilinskas"], "venue": "Towards Global Optimization, 2(117-129):2,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1978}, {"title": "Computing a classic index for finite-horizon bandits", "author": ["J. Ni\u00f1o-Mora"], "venue": "INFORMS Journal on Computing, 23(2):254\u2013267,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "More) efficient reinforcement learning via posterior 40  sampling", "author": ["I. Osband", "D. Russo", "B. Van Roy"], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc.,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["A. Piccolboni", "C. Schindelhauer"], "venue": "International Conference on Computational Learning Theory, pages 208\u2013223. Springer,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2001}, {"title": "Optimal learning, volume 841", "author": ["W.B. Powell", "I.O. Ryzhov"], "venue": "John Wiley & Sons,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Linearly parameterized bandits", "author": ["P. Rusmevichientong", "J.N. Tsitsiklis"], "venue": "Mathematics of Operations Research, 35(2):395\u2013411,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic assortment optimization with a multinomial logit choice model and capacity constraint", "author": ["P. Rusmevichientong", "Z.-J.M. Shen", "D.B. Shmoys"], "venue": "Operations research, 58(6): 1666\u20131680,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Eluder dimension and the sample complexity of optimistic exploration", "author": ["D. Russo", "B. Van Roy"], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2256\u20132264. Curran Associates, Inc.,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "Mathematics of Operations Research, 39(4):1221\u20131243,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to optimize via information-directed sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1583\u20131591. Curran Associates, Inc.,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "An information-theoretic analysis of Thompson sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "Journal of Machine Learning Research, 17(68):1\u201330,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2016}, {"title": "On the robustness of a one-period look-ahead policy in multi-armed bandit problems", "author": ["I. Ryzhov", "P. Frazier", "W. Powell"], "venue": "Procedia Computer Science, 1(1):1635\u20131644,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2010}, {"title": "The knowledge gradient algorithm for a general class of online learning problems", "author": ["I.O. Ryzhov", "W.B. Powell", "P.I. Frazier"], "venue": "Operations Research, 60(1):180\u2013195,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimal dynamic assortment planning with demand learning", "author": ["D. Saur\u00e9", "A. Zeevi"], "venue": "Manufacturing & Service Operations Management, 15(3):387\u2013404,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "A modern Bayesian look at the multi-armed bandit", "author": ["S.L. Scott"], "venue": "Applied Stochastic Models in Business and Industry, 26(6):639\u2013658,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2010}, {"title": "Information-theoretic regret bounds for Gaussian process optimization in the bandit setting", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "IEEE Transactions on Information Theory, 58(5):3250 \u20133265, may", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic simultaneous optimistic optimization", "author": ["M. Valko", "A. Carpentier", "R. Munos"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 19\u201327,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}, {"title": "Finite-time analysis of kernelised contextual bandits", "author": ["M. Valko", "N. Korda", "R. Munos", "I. Flaounas", "N. Cristianini"], "venue": "Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2013}, {"title": "An informational approach to the global optimization of expensive-to-evaluate functions", "author": ["J. Villemonteix", "E. Vazquez", "E. Walter"], "venue": "Journal of Global Optimization, 44(4):509\u2013534,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2009}, {"title": "Bisection search with noisy responses", "author": ["R. Waeber", "P.I. Frazier", "S.G. Henderson"], "venue": "SIAM Journal on Control and Optimization, 51(3):2261\u20132279,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 60, "context": "Further, by leveraging the tools of our recent information theoretic analysis of Thompson sampling [61], we establish an expected regret bound for IDS that applies across a very general class of models and scales with the entropy of the optimal action distribution.", "startOffset": 99, "endOffset": 103}, {"referenceID": 48, "context": "This is particularly surprising for Bernoulli bandit problems, where UCB algorithms and Thompson sampling are known to be asymptotically optimal in the sense proposed by Lai and Robbins [49].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": ", [14, 57, 65]), though in each case, developing a computationally efficient version of IDS may require innovation.", "startOffset": 2, "endOffset": 14}, {"referenceID": 56, "context": ", [14, 57, 65]), though in each case, developing a computationally efficient version of IDS may require innovation.", "startOffset": 2, "endOffset": 14}, {"referenceID": 63, "context": ", [14, 57, 65]), though in each case, developing a computationally efficient version of IDS may require innovation.", "startOffset": 2, "endOffset": 14}, {"referenceID": 0, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 106, "endOffset": 117}, {"referenceID": 23, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 106, "endOffset": 117}, {"referenceID": 55, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 106, "endOffset": 117}, {"referenceID": 24, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 138, "endOffset": 142}, {"referenceID": 1, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 161, "endOffset": 164}, {"referenceID": 65, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 207, "endOffset": 211}, {"referenceID": 65, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 268, "endOffset": 276}, {"referenceID": 67, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 268, "endOffset": 276}, {"referenceID": 16, "context": "Lipschitz continuous) model [17, 45, 68].", "startOffset": 28, "endOffset": 40}, {"referenceID": 44, "context": "Lipschitz continuous) model [17, 45, 68].", "startOffset": 28, "endOffset": 40}, {"referenceID": 66, "context": "Lipschitz continuous) model [17, 45, 68].", "startOffset": 28, "endOffset": 40}, {"referenceID": 5, "context": "Agrawal and Goyal [6] provided the first analysis for linear contextual bandit problems.", "startOffset": 18, "endOffset": 21}, {"referenceID": 57, "context": "Russo and Van Roy [58, 59] consider a more general class of models, and show that standard analysis of upper confidence bound algorithms leads to bounds on the expected regret of Thompson sampling.", "startOffset": 18, "endOffset": 26}, {"referenceID": 58, "context": "Russo and Van Roy [58, 59] consider a more general class of models, and show that standard analysis of upper confidence bound algorithms leads to bounds on the expected regret of Thompson sampling.", "startOffset": 18, "endOffset": 26}, {"referenceID": 32, "context": "[33] provides asymptotic frequentist bounds on the growth rate of regret for problems with dependent arms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "sampling have been applied to other types of problems, like reinforcement learning [40, 53] and Monte Carlo tree search [10, 46].", "startOffset": 83, "endOffset": 91}, {"referenceID": 52, "context": "sampling have been applied to other types of problems, like reinforcement learning [40, 53] and Monte Carlo tree search [10, 46].", "startOffset": 83, "endOffset": 91}, {"referenceID": 9, "context": "sampling have been applied to other types of problems, like reinforcement learning [40, 53] and Monte Carlo tree search [10, 46].", "startOffset": 120, "endOffset": 128}, {"referenceID": 45, "context": "sampling have been applied to other types of problems, like reinforcement learning [40, 53] and Monte Carlo tree search [10, 46].", "startOffset": 120, "endOffset": 128}, {"referenceID": 2, "context": "[3] consider a general model in which the reward distribution associated with each action depends on a common unknown parameter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] and Graves and Lai [34] to apply to the adaptive control of Markov chains and to problems with infinite parameter spaces.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[4] and Graves and Lai [34] to apply to the adaptive control of Markov chains and to problems with infinite parameter spaces.", "startOffset": 23, "endOffset": 27}, {"referenceID": 49, "context": "Though the use of mutual information to guide sampling has been the subject of much research, dating back to the work of Lindley [50], to our knowledge, only two other papers [36, 70] have used the mutual information between the optimal action and the next observation to guide action selection.", "startOffset": 129, "endOffset": 133}, {"referenceID": 35, "context": "Though the use of mutual information to guide sampling has been the subject of much research, dating back to the work of Lindley [50], to our knowledge, only two other papers [36, 70] have used the mutual information between the optimal action and the next observation to guide action selection.", "startOffset": 175, "endOffset": 183}, {"referenceID": 68, "context": "Though the use of mutual information to guide sampling has been the subject of much research, dating back to the work of Lindley [50], to our knowledge, only two other papers [36, 70] have used the mutual information between the optimal action and the next observation to guide action selection.", "startOffset": 175, "endOffset": 183}, {"referenceID": 12, "context": "This approach is often called \u201cBayesian optimization\u201d in the machine learning community [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 68, "context": "[70] and Hennig and Schuler [36] propose selecting each sample to maximize the mutual information between the next observation and the optimal solution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[70] and Hennig and Schuler [36] propose selecting each sample to maximize the mutual information between the next observation and the optimal solution.", "startOffset": 28, "endOffset": 32}, {"referenceID": 36, "context": "Several papers [37, 38, 39] have extended this line of work since an initial version of our paper appeared online.", "startOffset": 15, "endOffset": 27}, {"referenceID": 37, "context": "Several papers [37, 38, 39] have extended this line of work since an initial version of our paper appeared online.", "startOffset": 15, "endOffset": 27}, {"referenceID": 38, "context": "Several papers [37, 38, 39] have extended this line of work since an initial version of our paper appeared online.", "startOffset": 15, "endOffset": 27}, {"referenceID": 68, "context": "[70] and Hennig and Schuler [36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[70] and Hennig and Schuler [36].", "startOffset": 28, "endOffset": 32}, {"referenceID": 65, "context": "For such problems, simpler approaches like UCB algorithms [67], probability of improvement [47], and expected improvement [51] are already extremely effective.", "startOffset": 58, "endOffset": 62}, {"referenceID": 46, "context": "For such problems, simpler approaches like UCB algorithms [67], probability of improvement [47], and expected improvement [51] are already extremely effective.", "startOffset": 91, "endOffset": 95}, {"referenceID": 50, "context": "For such problems, simpler approaches like UCB algorithms [67], probability of improvement [47], and expected improvement [51] are already extremely effective.", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "[13], each of these algorithms simply chooses points with \u201cpotentially high values of the objective function: whether because the prediction is high, the uncertainty is great, or both.", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "[70] and Hennig and Schuler [36] propose their algorithms as heuristics without guarantees.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[70] and Hennig and Schuler [36] propose their algorithms as heuristics without guarantees.", "startOffset": 28, "endOffset": 32}, {"referenceID": 50, "context": "[51] and studied further by Frazier et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] and Ryzhov et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[64].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Our work also connects to a much larger literature on Bayesian experimental design (see [20] for a review).", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "[22] study problems with Gaussian process priors and a method that guides exploration using the mutual information between the objective function and the next observation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] and Waeber et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[71] consider problem settings in which this greedy policy is optimal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Another recent line of work [31, 32] shows that measures of information gain sometimes satisfy a decreasing returns property known as adaptive sub-modularity, implying the greedy policy is competitive with the optimal policy.", "startOffset": 28, "endOffset": 36}, {"referenceID": 31, "context": "Another recent line of work [31, 32] shows that measures of information gain sometimes satisfy a decreasing returns property known as adaptive sub-modularity, implying the greedy policy is competitive with the optimal policy.", "startOffset": 28, "endOffset": 36}, {"referenceID": 53, "context": "First introduced by [54] the partial monitoring problem encompasses a broad range of online optimization problems with limited or partial feedback.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "Recent work [11] has focused on classifying the minimax-optimal scaling of regret in the problem\u2019s time horizon as a function of the level of feedback the agent receives.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "Francetich and Kreps [26, 27] discuss a variety of heuristics for the discounted problem.", "startOffset": 21, "endOffset": 29}, {"referenceID": 26, "context": "Francetich and Kreps [26, 27] discuss a variety of heuristics for the discounted problem.", "startOffset": 21, "endOffset": 29}, {"referenceID": 60, "context": "The regret bounds we will present build on our information-theoretic analysis of Thompson sampling [61], which can be used to bound the regret of any policy in terms of its information ratio.", "startOffset": 99, "endOffset": 103}, {"referenceID": 60, "context": "The information ratio of IDS is always smaller than that of TS, and therefore, bounds on the information ratio of TS provided in Russo and Van Roy [61] yield regret bounds for IDS.", "startOffset": 147, "endOffset": 151}, {"referenceID": 59, "context": "This observation and a preliminary version of our results was first presented in a conference paper [60].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "[18] and Bubeck and Eldan [16] build on ideas from [61] in another direction by bounding the information ratio when the reward function is convex and using that bound to study the order of regret in adversarial bandit convex optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] and Bubeck and Eldan [16] build on ideas from [61] in another direction by bounding the information ratio when the reward function is convex and using that bound to study the order of regret in adversarial bandit convex optimization.", "startOffset": 26, "endOffset": 30}, {"referenceID": 60, "context": "[18] and Bubeck and Eldan [16] build on ideas from [61] in another direction by bounding the information ratio when the reward function is convex and using that bound to study the order of regret in adversarial bandit convex optimization.", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "6 of Gray [35], this is equal to the expected reduction in entropy of the posterior distribution of A\u2217 due to observing Yt(a):", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 18, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 43, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 47, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 48, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 16, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 23, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 24, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 32, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 55, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 58, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 65, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 12, "context": "The first is the expected improvement algorithm, which is one of the most widely used techniques in the active field of Bayesian optimization (see [13]).", "startOffset": 147, "endOffset": 151}, {"referenceID": 62, "context": "The knowledge gradient algorithm [64] uses a modified improvement measure.", "startOffset": 33, "endOffset": 37}, {"referenceID": 27, "context": "To address problems like Example 1, Frazier and Powell [28] propose KG* \u2013 a modified form of KG that considers the value of sampling a single action many times.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "\u03b1\u2217 = arg min \u03b1\u2208[0,1] ((1\u2212 \u03b1)/2 + \u03b1(1\u2212 1/d))2", "startOffset": 15, "endOffset": 20}, {"referenceID": 0, "context": "\u03b1 log2(d) = arg min \u03b1\u2208[0,1] 1 2 \u221a \u03b1 + \u221a \u03b1 (1 2 \u2212 1 d ) = 1.", "startOffset": 22, "endOffset": 27}, {"referenceID": 60, "context": "These regret bounds follow from our recent information theoretic-analysis of Thompson sampling [61].", "startOffset": 95, "endOffset": 99}, {"referenceID": 60, "context": "Because the information-ratio of IDS is always smaller than that of TS, the bounds on the information ratio of TS provided in Russo and Van Roy [61] immediately yield regret bounds for IDS for a number of important problem classes.", "startOffset": 144, "endOffset": 148}, {"referenceID": 60, "context": "Several bounds on the information-ratio of TS were provided by Russo and Van Roy [61], and we defer to that paper for the proofs.", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "[18] and Bubeck and Eldan [16] bounds the information ratio when the reward function is convex, and uses this to study the order of regret in adversarial bandit convex optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] and Bubeck and Eldan [16] bounds the information ratio when the reward function is convex, and uses this to study the order of regret in adversarial bandit convex optimization.", "startOffset": 26, "endOffset": 30}, {"referenceID": 60, "context": "This effectively controls the worst-case variance of the reward distribution, and as shown in the appendix of Russo and Van Roy [61], our results can be extended to the case where reward distributions are sub-Gaussian.", "startOffset": 128, "endOffset": 132}, {"referenceID": 22, "context": "[23] show this bound is order optimal, in the sense that for any time horizon T and number of actions |A| there exists a prior distribution over \u03b8 under which inf\u03c0 E [Regret(T, \u03c0)] \u2265 c0 \u221a log(|A|)T where c0 is a numerical constant that does not depend on |A| or T .", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1, 24, 56]) and is one of the most important examples of a multi-armed bandit problem with \u201ccorrelated arms.", "startOffset": 0, "endOffset": 11}, {"referenceID": 23, "context": "[1, 24, 56]) and is one of the most important examples of a multi-armed bandit problem with \u201ccorrelated arms.", "startOffset": 0, "endOffset": 11}, {"referenceID": 55, "context": "[1, 24, 56]) and is one of the most important examples of a multi-armed bandit problem with \u201ccorrelated arms.", "startOffset": 0, "endOffset": 11}, {"referenceID": 22, "context": "[23] again show this bound is order optimal in the sense that, for any time horizon T and dimension d, when the action set is A = {0, 1}d there exists a prior distribution over \u03b8 such that inf\u03c0 E [Regret(T, \u03c0)] \u2265 c0 \u221a log(|A|)dT where c0 is a constant that is independent of d and T .", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "This type of observation structure is sometimes called \u201csemi-bandit\u201d feedback [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "[8], the", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "5 of Bubeck and Cesa-Bianchi [15]), the agent\u2019s regret from each component must be at least \u221a d m T , and hence her overall expected regret is lower bounded by a term of order m \u221a d m T = \u221a mdT .", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": ", xn} \u2282 [0, 1].", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "The compute time of such an implementation scales with K2n where K is the number of actions and n is the number of points used in the discretization of [0, 1].", "startOffset": 152, "endOffset": 158}, {"referenceID": 60, "context": "2Some details related to the derivation of this fact when Yt,a is a general random variable can be found in the appendix of Russo and Van Roy [61].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "1: qa,a\u2032 \u2190 arg minq\u2032\u2208[0,1] [ q\u2032~ \u2206a + (1\u2212 q\u2032)~ \u2206a\u2032 ]2 / [q~ga + (1\u2212 q)~ga\u2032 ] \u2200a < K, a\u2032 > a 2: (a\u2217, a\u2217\u2217)\u2190 arg mina<K,a\u2032>a [ qa,a\u2032 ~ \u2206a + (1\u2212 qa,a\u2032)~ \u2206a\u2032 ]2 / [ qa,a\u2032~ga + (1\u2212 qa,a\u2032)~ga\u2032 ]", "startOffset": 21, "endOffset": 26}, {"referenceID": 0, "context": "R(y) \u2208 [0, 1] for all y, our information measure term is lower-bounded according to", "startOffset": 7, "endOffset": 13}, {"referenceID": 60, "context": "Inequality (a) is a simple corollary of Pinsker\u2019s inequality, and is given as Fact 9 in Russo and Van Roy [61].", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "[9] selects the action a which maximizes the upper confidence bound \u03b8\u0302t(a)+ \u221a 2 log(t)/Nt(a) where \u03b8\u0302t(a) is the empirical average reward from samples of action a and Nt(a) is the number of samples of action a up to time t.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] proposed using an algorithm called UCB-Tuned.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The MOSS algorithm of Audibert and Bubeck [7] is similar to UCB1 and UCB\u2013Tuned, but uses slightly different confidence bounds.", "startOffset": 42, "endOffset": 45}, {"referenceID": 20, "context": "In previous numerical experiments [21, 43, 44, 66], Thompson sampling and Bayes UCB exhibited state-of-the-art performance for this problem.", "startOffset": 34, "endOffset": 50}, {"referenceID": 42, "context": "In previous numerical experiments [21, 43, 44, 66], Thompson sampling and Bayes UCB exhibited state-of-the-art performance for this problem.", "startOffset": 34, "endOffset": 50}, {"referenceID": 43, "context": "In previous numerical experiments [21, 43, 44, 66], Thompson sampling and Bayes UCB exhibited state-of-the-art performance for this problem.", "startOffset": 34, "endOffset": 50}, {"referenceID": 64, "context": "In previous numerical experiments [21, 43, 44, 66], Thompson sampling and Bayes UCB exhibited state-of-the-art performance for this problem.", "startOffset": 34, "endOffset": 50}, {"referenceID": 48, "context": "Each also satisfies strong theoretical guarantees, and is known to be asymptotically optimal in the sense defined by Lai and Robbins [49].", "startOffset": 133, "endOffset": 137}, {"referenceID": 42, "context": "[43], constructs upper confidence bounds based on the quantiles of the posterior distribution: at time step t the upper confidence bound at an action is the 1\u2212 t quantile of the posterior distribution of that action3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "A somewhat different approach is the knowledge gradient (KG) policy of Powell and Ryzhov [55], which uses a one-step lookahead approximation to the value of information to guide experimentation.", "startOffset": 89, "endOffset": 93}, {"referenceID": 61, "context": "[63] offers much better performance in some of these problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "To enable efficient simulation, we use a heuristic approach to computing KG* proposed by Kami\u0144ski [42].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "It is worth pointing out that, although Gittins\u2019 indices characterize the Bayes optimal policy for infinite horizon discounted problems, the finite horizon formulation considered here is computationally intractable [30].", "startOffset": 215, "endOffset": 219}, {"referenceID": 51, "context": "A similar index policy [52] designed for finite horizon problems could be applied as a heuristic in this setting.", "startOffset": 23, "endOffset": 27}, {"referenceID": 65, "context": "[67].", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "The seminal work of Lai and Robbins [49] provides the following asymptotic lower bound on regret of any policy \u03c0:", "startOffset": 36, "endOffset": 40}, {"referenceID": 42, "context": "Nevertheless, when applied with an independent uniform prior over \u03b8, both Bayes UCB and Thompson sampling are known to attain this lower bound [43, 44].", "startOffset": 143, "endOffset": 151}, {"referenceID": 43, "context": "Nevertheless, when applied with an independent uniform prior over \u03b8, both Bayes UCB and Thompson sampling are known to attain this lower bound [43, 44].", "startOffset": 143, "endOffset": 151}, {"referenceID": 42, "context": "We also include Bayes UCB [43] and a version of GP-UCB that was tuned, as in Subsection 7.", "startOffset": 26, "endOffset": 30}, {"referenceID": 65, "context": "5Regret analysis of GP-UCB can be found in [67].", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "Regret bounds for Thompson sampling can be found in [6, 59, 61]", "startOffset": 52, "endOffset": 63}, {"referenceID": 58, "context": "Regret bounds for Thompson sampling can be found in [6, 59, 61]", "startOffset": 52, "endOffset": 63}, {"referenceID": 60, "context": "Regret bounds for Thompson sampling can be found in [6, 59, 61]", "startOffset": 52, "endOffset": 63}, {"referenceID": 41, "context": "A direct implementation of the KG* policy was too slow to simulate, and so we have used a heuristic approach presented in [42], which uses golden section search to maximize a function that is not necessarily unimodal.", "startOffset": 122, "endOffset": 126}, {"referenceID": 48, "context": "One question raised is whether IDS attains the lower bound of Lai and Robbins [49] for some bandit problems with independent arms.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "As shown in Chapter 3 of Boyd and Vandenberghe [12], f : (x, y) 7\u2192 x2/y is convex over {(x, y) \u2208 R2 : y > 0}.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Then we can choose a \u03b2 \u2208 [0, 1] so that m \u2211", "startOffset": 25, "endOffset": 31}, {"referenceID": 34, "context": "6 of Gray [35])", "startOffset": 10, "endOffset": 14}, {"referenceID": 60, "context": "The proof of this proposition essentially reduces to techniques in Russo and Van Roy [61], but some new analysis is required to show the results in that paper apply to variance-based IDS.", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "We propose information-directed sampling \u2013 a new approach to online optimization problems in which a decision-maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between squared expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation. We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. We illustrate through simple analytic examples how information-directed sampling accounts for kinds of information that alternative approaches do not adequately address and that this can lead to dramatic performance gains. For the widely studied Bernoulli, Gaussian, and linear bandit problems, we demonstrate state-of-the-art simulation performance.", "creator": "LaTeX with hyperref package"}}}