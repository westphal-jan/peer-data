{"id": "1404.0736", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2014", "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups by a factor of 2x, while keeping the accuracy within 1% of the original model.", "histories": [["v1", "Wed, 2 Apr 2014 23:31:12 GMT  (907kb,D)", "https://arxiv.org/abs/1404.0736v1", null], ["v2", "Mon, 9 Jun 2014 15:53:55 GMT  (1261kb,D)", "http://arxiv.org/abs/1404.0736v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["emily l denton", "wojciech zaremba", "joan bruna", "yann lecun", "rob fergus"], "accepted": true, "id": "1404.0736"}, "pdf": {"name": "1404.0736.pdf", "metadata": {"source": "CRF", "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "authors": ["Emily Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "emails": ["@cs.nyu.edu"], "sections": [{"heading": null, "text": "These models deliver impressive accuracy, but any image evaluation requires millions of floating-point operations, making their use on smartphones and Internet-scale clusters problematic. Calculation is dominated by the folding operations in the lower layers of the model. We use the redundancy that exists within the folding filters to derive approximate values that significantly reduce the required calculation. Using large modern models, we demonstrate the acceleration of folding layers on both the CPU and the GPU by a factor of 2x, keeping the accuracy within 1% of the original model."}, {"heading": "1 Introduction", "text": "The large neural networks have recently demonstrated impressive performance across a range of speech and visual tasks, but the size of these models can make their use problematic in the test period. For example, mobile computer platforms are limited in CPU speed, memory and battery life. At the other end of the spectrum, deploying thousands of servers to process millions of images per day requires significant electrical and cooling costs for these servers. Formation of large neural networks can take weeks or even months, hampering research and consequently, there is extensive effort to accelerate training processes. However, there is relatively little effort aimed at improving the test time. We consider the Convolutionary Neural Networks (CNNs) as they are large and widely used in commercial applications. These networks require an enormous number of parameters."}, {"heading": "2 Related Work", "text": "They present many solutions for Intel and AMD CPUs, but some of their techniques are generic enough to be used for any type of processor. They describe how they present the weights as 8-bit integers (range 127, 128) instead of increasing the efficiency of matrix multiplication. This approach is similar in spirit to the linear quantification of network weights and inputs. It is about presenting the weights as 8-bit integers (range 127, 128). This approach is in spirit, but it is applied to every weight element."}, {"heading": "3 Convolutional Tensor Compression", "text": "In this section, we describe techniques for compressing 4-dimensional convergence tensors and fully connected weight matrices into a representation that enables efficient calculation and storage; Section 3.1 describes how to construct good approximation criteria; Section 3.2 describes techniques for approximating low-level tensors; Section 3.3 and 3.4 describe how to apply these techniques to approximate the weights of a Convolutionary Neural Network."}, {"heading": "3.1 Approximation Metric", "text": "Our goal is to find an approximation to all the parameters of the S-layer network and the U-layer that allows a more efficient calculation while maintaining the predictive power of the network. A natural choice for an approximation criterion is that all directions in the space of weights affect the predictive power equally. We now present two methods to improve this criterion while maintaining the same efficient approximation to the algorithms. Mahalanobis distance metric: The first distance metric that we propose to produce predictive errors over coordinates whose effect is less harmful to the overall system. Let's get such measurements as follows."}, {"heading": "3.2 Low-rank Tensor Approximations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Matrix Decomposition", "text": "Matrices are 2-tensors that can be compressed linearly using the Singular Value Decomposition. If W-Rm \u00b7 k is a real matrix, the SVD is defined as W = UPS >, where U-Rm \u00b7 m, S-Rm \u00b7 k, V-Rk \u00b7 k. S is a diagonal matrix with the singular values on the diagonal and U, V are orthogonal matrices. If the singular values of W decay quickly, W can be approached well by keeping only the t largest entries of S, which leads to the approximation W-Rm = U-S-V, where U-Rm \u00b7 t, S-Rt \u00b7 t, V-Rt \u00b7 k lead. Then, for I-Rn \u00b7 m, the approximation error IW-IW-F is satisfactory, where IW-IW-F-Rm-T + 1-I is controlled by the decay along the Rm n."}, {"heading": "3.2.2 Higher Order Tensor Approximations", "text": "SVD can be used to approximate a tensor W-Rm \u00b7 n \u00b7 k by first folding all but two dimensions together to convert it into a 2-tensor, and then considering the SVD of the resulting matrix. For example, we can approximate Wm-Rm \u00b7 (nk) as W-m-U-S-S-V-V >. W can be compressed even further by applying SVD to V-K. We call this approximation SVD decomposition and use K1 and K2 to denote the rank used in the first and second application of SVD. Alternatively, we can use a 3-tensor, WS-Rm \u00b7 n \u00b7 k, to use a 1-3 tensor by finding a decomposition that minimizes the rank used in the first and second application of SVD."}, {"heading": "3.3 Monochromatic Convolution Approximation", "text": "We found that the color components of trained CNNs tend to have a low dimensional structure. In particular, the weights can be easily approximated by projecting the color dimension onto a 1D subspace. We illustrate the low-dimensional structure of the weights in Figure 4.1.1. The monochromatic approach takes advantage of this structure and is calculated as follows. First, for each output characteristic, we consider the matrix Wf-RC-RC-RC-RC-RC-RC-RC-RC-RC-RC-RC-RC-RC-RC-RC-RXY-RXY-RXY-RXY-R-R. We then take the rankings 1-RC-RC-RC-RF-S-S-S-RC-RC-R-C-R-R-C-R-R-RC-RC-R-RC-RC-RC-R-RC-RC-RC-R-RC-RC-RC-RC-S-RF-RF-RS-RS-R-C-C-R-C-R-C-R-C-R-C-R-R-C-R-C-R-R-R-C-R-R-RF-C-R-C-R-R-C-R-R-C-R-R-R-R-R-R-R-R-R-R-R-C-R-R-R-C-R-R-C-R-R-C-R-C-C-RC-R-C-RC-RF-RF-R-C-RF-R-R-C-R-R-C-R-R-R-R-R-R-R-R-R-R-R-R-R-C-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-RC-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-"}, {"heading": "3.4 Biclustering Approximations", "text": "We exploit the redundancy within the 4-D weight tensors in the higher revolutionary layers by grouping the filters so that each cluster can be precisely approximated by a low factorization. We start by having the rows of WC-RC-RC-RC-RF-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-RA-R"}, {"heading": "3.5 Fine-tuning", "text": "Many of the approach techniques presented here can efficiently compress the weights of a CNN, provided the approach is not too hard. Alternatively, you can use a harder approach that yields greater acceleration gains but impairs the performance of the network. In this case, the approximate layer and all underlying layers can be repaired and the upper layers fine-tuned until the initial performance is restored."}, {"heading": "4 Experiments", "text": "We use the 15-layer convolutional architecture of [8] that was trained on the ImageNet 2012 dataset. [9] The network consists of 4 convolutional layers, 3 fully connected layers, and a Softmax output layer. We have evaluated the network on both CPU and GPU platforms. All measurements of prediction power are based on the 20K validation images of the ImageNet12 dataset. We present results that show the performance of the approximate values described in Section 3 in terms of prediction accuracy, acceleration of prediction gains, and memory reduction. All of our fine-tuning results were obtained by training with less than 2 passes using the ImageNet12 training dataset. Unless otherwise specified, the classification numbers refer to those of fine-tuning models."}, {"heading": "4.1 Speedup", "text": "For this reason, we limit our attention in our acceleration experiments to the first and second acceleration levels. Both the base and the approximation CPU code are implemented in C + +, using the proprietary library [10] compiled with Intel MKL. We also use Intel's implementation of Openmp and multithreading. The baseline provides comparable performance for highly optimized MATLAB acceleration routines, and all our CPU acceleration results are calculated relative to it. We used Alex Krizhevsky's CUDA acceleration routines 1 as the basis for GPU comparisons. The approximation versions are written in CPU acceleration routines, with the results calculated relative to them."}, {"heading": "4.1.1 First Layer", "text": "The first layer has 3 input channels, 96 output channels and 7x7 filters. We approximated the weights in this layer using the monochromatic approximation described in Section 3.3. Monochromatic approximation works well if the color components comprise a small number of one-dimensional subspaces. Figure 4.1.1 illustrates the effects of monochromatic approximation on the first layer filters. The only parameter in the approximation is C \u2032, the number of color channels used for intermediate representation. As expected, network performance decreases as the number of C \u2032 decreases. 1https: / / code.google.com / p / cuda-convennet / number of flow combinations required to calculate the output of monochromatic folding decreases by a factor of 2 \u2212 3 \u00d7, resulting in the greater gain for the small C \u2032. Figure 3 shows the network performances achieved by Cempirical, PU and the corresponding color combinations we have achieved relative to the Pempirical ones, and the Pempidups."}, {"heading": "4.1.2 Second Layer", "text": "The second convolutionary layer has 96 input channels, 256 output channels and 5x5 filters. We approximated the weights using the techniques described in Section 3.4. We examined different configurations of approximations by varying the number of input channels G, the number of output channels H, and the approximation rank (indicated by K1 and K2 for SVD decomposition and K for outer product decomposition). Figure 4 shows our empirical accelerations on CPU and GPU and the corresponding network performance for different approximation configurations. For CPU implementation, we used biclustering with SVD approximation. For GPU implementation, we use biclustering with approximation of outer product decomposition. We achieved promising results and present accelerations of 2 \u2212 2.5 \u00d7 in relation to baseline with less than 1% power depletion."}, {"heading": "4.2 Combining approximations", "text": "The procedure is as follows: compress the first layer weights and then fine tune all layers on top until performance is restored. Next, compress the second layer weights resulting from fine tune. Fine tune all layers on top until performance is restored and then continue the process.We applied this procedure to the first two layers with contiguous layers. Using the monochrome 6 color approximation for the first layer and bicluster ring with external product replacement approximation for the second layer (G = 48; H = 2; K = 8) and fine tune with a single run through the training set, we are able to maintain accuracy within 1% of the original model. This procedure could be applied to each layer in this sequential manner to achieve total accelerations much greater than each individual layer can provide. A more complete summary of these results can be found in the completion material."}, {"heading": "4.3 Reduction in memory overhead", "text": "In many commercial applications, memory conservation and storage are a central concern, especially for embedded systems (e.g. smartphones), where the amount of memory available is limited and users are reluctant to download large files. In these cases, the ability to compress the neural network is critical to the viability of the product. In addition to fewer operations, our approximations require significantly fewer parameters than the original model. Since most parameters come from the fully connected layers, we include these layers in our analysis of the memory overhead. We compress the fully connected layers with standard SVD as described in 3.2.2 and use K to indicate the rank of approximation.Table 2 shows the number of parameters for different approach methods as a function of hyperparameters for the approximation techniques. The table also shows the empirical reduction of parameters and the corresponding network performance for specific instances of approximation parameters."}, {"heading": "5 Discussion", "text": "In this paper, we have presented techniques that can accelerate bottleneck folding operations in the first layers of a CNN by a factor of 2-3 - with negligible loss of power. We also show that our methods reduce the storage requirements of weights in the first two layers by a factor of 2-3 - and the fully bonded layers by a factor of 5-13. Since the vast majority of weights are in the fully bonded layers, compressing only these layers results in significant savings, which would facilitate the mobile deployment of winding networks. These techniques are orthogonal to other approaches to efficient evaluation, such as quantification or work in the Fourier range, so they can potentially be used together to achieve further results. An interesting possibility of research that could be investigated in further work is the ability of these techniques to assist with regulation either during or after training."}, {"heading": "A Forward propagation time breakdown", "text": "Table 3 shows the temporal distribution of forward propagation for each layer in the CNN architecture we have studied. Nearly 90% of the time is spent on curved layers, and within these layers most of the time is spent on the first two."}, {"heading": "B Theoretical speedups", "text": "It is possible to measure the theoretically achievable accelerations for a given approximation to the number of floating-point operations required for the calculation of the target power. Although it is unlikely that a conversion would achieve accelerations corresponding to the theoretically optimal level, the number of floating-point operations required still represents an informative upper limit for the gains. Table 4 shows the theoretically achievable acceleration of the monochrome approximation. The majority of operations result from the folding part of the calculation. In comparison, the number of operations required for the color transformation is negligible. Therefore, the theoretically achievable acceleration decreases only slightly when the number of color components used is increased. Figure 5 shows the theoretically achievable accelerations against the decrease in classification performance for different configurations of biclustering with the decomposition technique of the outer product."}, {"heading": "C Combined results", "text": "We used the monochromatic approximation with 6 colors for the first layer. Table 5 summarizes the results after fine tuning for 1 pass through the ImageNet12 training data using a variety of approximations for the second layer."}], "references": [{"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N. de Freitas"], "venue": "arXiv preprint arXiv:1306.0543", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "Vedaldi", "Andrea", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3866", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Rank-one approximation to high order tensors", "author": ["T. Zhang", "G.H. Golub"], "venue": "SIAM J. Matrix Anal. Appl", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1311.2901", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "Computer Vision (ICCV),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Tiled convolutional neural networks", "author": ["Q.V. Le", "J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "Computer vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "These networks typically require a huge number of parameters (\u223c 10 in [1]) to produce state-of-the-art results.", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "While these networks tend to be hugely over parameterized [2], this redundancy seems necessary in order to overcome a highly nonconvex optimization [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "While these networks tend to be hugely over parameterized [2], this redundancy seems necessary in order to overcome a highly nonconvex optimization [3].", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "[4] explored the properties of CPUs to speed up execution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] have shown that convolution can be efficiently computed in Fourier domain, where it becomes element-wise multiplication (and there is no cost associated with size of receptive field).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] who demonstrate the redundancies in neural network parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Finally, a recent preprint [6] also exploits low-rank decompositions of convolutional tensors to speed up the evaluation of CNNs, applied to scene text character recognition.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "Another alternative, similar to the one considered in [6], is to replace the isotropic covariance assumption by the empirical covariance of the input of the layer.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "As opposed to [6], this approach adapts to the input distribution without the need to iterate through the data.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "Problem (4) is solved efficiently by performing alternate least squares on \u03b1, \u03b2 and \u03b3 respectively, although more efficient algorithms can also be considered [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 7, "context": "We use the 15 layer convolutional architecture of [8], trained on the ImageNet 2012 dataset [9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "We use the 15 layer convolutional architecture of [8], trained on the ImageNet 2012 dataset [9].", "startOffset": 92, "endOffset": 95}], "year": 2014, "abstractText": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2\u00d7, while keeping the accuracy within 1% of the original model.", "creator": "LaTeX with hyperref package"}}}