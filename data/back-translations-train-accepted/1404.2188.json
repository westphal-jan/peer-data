{"id": "1404.2188", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2014", "title": "A Convolutional Neural Network for Modelling Sentences", "abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.", "histories": [["v1", "Tue, 8 Apr 2014 15:46:44 GMT  (144kb,D)", "http://arxiv.org/abs/1404.2188v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nal kalchbrenner", "edward grefenstette", "phil blunsom"], "accepted": true, "id": "1404.2188"}, "pdf": {"name": "1404.2188.pdf", "metadata": {"source": "CRF", "title": "A Convolutional Neural Network for Modelling Sentences", "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "emails": ["phil.blunsom}@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The goal of a sentence model is to analyze and present the semantic content of a sentence for the purposes of classification or generation.The sentence modeling problem is at the core of many tasks that involve a degree of natural language comprehension, which include sensory analysis, paraphrases, summaries, discourse analysis, machine translation, grounded language learning, and image manipulation. Since individual sentences are rarely or not observed, one must represent a sentence in terms of characteristics that depend on the words and short n-grams of the sentence, which are frequently observed. The core of a sentence model includes a function that defines the procedure in which the characteristics of the sentence are extracted. The characteristics of the sentence are extracted from the characteristics of the words or n-grams. Different types of meaning models have been proposed. Composition methods have been applied to vector representations of the word meaning, which are obtained from co-occurence sentences or n-statistics for some operations."}, {"heading": "2 Background", "text": "The layers of the DCNN are formed by a folding operation, followed by a pooling operation. Let's start with a review of related neural sentence models. Then, we describe the functioning of one-dimensional folding and the classic TimeDelay Neural Network (TDNN) (Hinton, 1989; Waibel et al., 1990). By adding a max pooling1code available at www.nal.colayer, the TDNN can be adopted as a sentence model (Collobert and Weston, 2008)."}, {"heading": "2.1 Related Neural Sentence Models", "text": "A general class of basic sentence models is that of the Neural Bag-of-Words (NBoW) models, which generally consist of a projection layer that maps words, sub-word units, or n-grams to high-dimensional embedding; the latter are then combined component by component with an operation such as summation; the resulting combined vector is classified by one or more fully connected layers; a model that assumes a more general structure provided by an external parse tree is the Recursive Neural Network (RecNN) (Pollack, 1990; Ku \ufffd chler and Goller, 1996; Socher et al., 2011; Hermann and Blunsom, 2013). At each node in the tree, the contexts left and right of the node are combined by a classical layer; the weights of the layer are distributed over all nodes in the tree; the layer calculated at the uppermost node gives a representation of the net 2001, the network is followed by a neural (urcursive) line (where the urinal)."}, {"heading": "2.2 Convolution", "text": "The one-dimensional folding is an operation between a vector of the weights m, m, Rm and a vector of the input, which is considered as a sequence s, s, Rs. The vector m is the filter of the folding. Specifically, we think of s as the input clause and si, R is a single characteristic value associated with the i-th word in the sentence. The idea behind the one-dimensional folding is to take the point product of the vector m with every m-gram in the sentence s, in order to obtain another sequence c: cj = m, sj \u2212 m + 1: j (1) Equation 1 leads to two types of folding depending on the range of the index j. The narrow type of folding requires that s, m and yields a sequence c, Rs \u2212 m + 1, with j ranging from m to s. The wide type of folding has no requirements for s or m and yields a valid sequence c, Rs, where the index is > 1, to i."}, {"heading": "2.3 Time-Delay Neural Networks", "text": "It is a very complex and complex story, in which it is about giving people the opportunity to understand the world and to understand how it is. It is about understanding the world as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it, as it is, as it is, as it is, as it is, as it is, as it is, as it, as it is, as it is, as it is, as it is, as it is, as it is, as it, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is"}, {"heading": "3 Convolutional Neural Networks with", "text": "Dynamic k-Max PoolingWe model sets using a sinuous architecture that alternates broad curved layers with dynamic pool layers provided by dynamic kmax pooling. In the network, the width of a feature map on an intermediate layer varies according to the length of the input set; the resulting architecture is the Dynamic Convolutional Neural Network. Figure 3 represents a DCNN. We go on to describe the network in detail."}, {"heading": "3.1 Wide Convolution", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is not a country in which it is not a country, but a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "3.4 Non-linear Feature Function", "text": "After (dynamic) k-max pooling is applied to the result of a convolution, a preload b-round and a nonlinear function g are applied component by component to the pooled matrix. There is a single bias value for each row of the pooled matrix. If we temporarily ignore the pooled layer, we can specify how to calculate each d-dimensional column a in the matrix, where m are the weights of the d filters of the wide convolution. Let us then define M as the matrix of the diagonals: M = [diag (m:, 1),... diag (m:, m) (5), where m are the weights of the d filters of the wide convolution. Then, after the first pair of a convolutionary and a nonlinear layer, each column a in the matrix a is obtained as follows: a = g M wj... wj + m \u2212 1 + b (6)."}, {"heading": "3.5 Multiple Feature Maps", "text": "So far we have described how to apply a wide folding, a (dynamic) k-max pooling layer and a non-linear function to the input set matrix to obtain a feature map of the first order of F. The three operations can be repeated to obtain feature maps of increasing order and a network of increasing depth. We designate a feature map of the i-th order of F. As in conventional object recognition networks to increase the number of learned feature detectors of a certain order, several feature maps Fi1,..., F i n can be calculated in parallel on the same level. Each feature map Fij is calculated by arranging a unique series of filters in a matrix mij, k with each feature map F i \u2212 1k of the lower order i \u2212 1 and summarizing the results: Fij = n \u00b2 k = 1mij, k \u00b2 Fi \u2212 1k (7), with two types of filters: the mij, the weights of which form a non-dynamic order of 4 and the first order is applied individually."}, {"heading": "3.6 Folding", "text": "In the previous formulation of the network, feature detectors applied to a single line of the sentence matrix can have many orders and generate complex dependencies over the same lines in multiple feature maps. However, feature detectors in different lines are independent of each other up to the topmost completely connected layer. Full dependence between different lines could be achieved by making M in Equivalent 5 a complete matrix instead of a sparse diagonal matrix. At this point, we are examining a simpler method called folding, which does not introduce any additional parameters. After a curved layer and before (dynamic) k-max pooling, one simply summarizes all two lines in a feature map componentwise. In a map of d-lines, folding results in a map of d / 2 rows, thereby halving the size of the representation. In a folding layer, a feature detector of the i-th order now hangs from two line values of the feature maps in the 1st order, thus ending the description of the i."}, {"heading": "4 Properties of the Sentence Model", "text": "We describe some of the properties of the sentence model based on DCNN. We describe the concept of the feature graph, which is induced via a sentence by the sequence of folding and pooling layers. We briefly relate the properties to those of other neural sentence models. 4.1 Word and n-gram order One of the basic properties is sensitivity to the sequence of words in the input sentence. For most applications, and to learn fine-grained feature detectors, it is advantageous for a model to be able to distinguish whether a particular n-gram occurs in the input. Likewise, it is advantageous for a model to determine the relative position of the most relevant n-grams. The network is designed to capture these two aspects. The filters m of the wide folding in the first layer can learn to detect specific n-grams that have less or equal to the filter width m; as we see in the experiments, m is relatively large in the first layer."}, {"heading": "4.2 Induced Feature Graph", "text": "Some sentence models use internal or external structures to calculate the representation for the input set. In a DCNN diagram, folding and pooling layers induce an internal feature graph via input. A node from one layer is connected to a node from the next higher layer when the bottom node is involved in the folding that calculates the value of the higher node. Nodes that are not selected by pooling at one layer are removed from the graph. After the last pooling layer, the remaining nodes connect to a single uppermost root. The induced graph is a connected, directional acyclic graph graph with weighted edges and a root node; two equivalent representations of an induced graph are given in the figure. 1. In a DCNN without folding layers, each of the rows of the sentence matrix can connect a subgraph that connects the other subgraph."}, {"heading": "5 Experiments", "text": "We test the network on four different experiments. First, we specify aspects of the implementation and training of the network. Then we correlate the results of the experiments and inspect the learned feature detectors."}, {"heading": "5.1 Training", "text": "In each of the experiments, the top layer of the network has a fully connected layer, followed by a Softmax nonlinearity, which predicts the probability distribution across classes based on the input set. The network is trained to minimize the transverse entropy of the predicted and true distributions; the goal includes an L2 regularization term for the parameters; the parameter set includes the word embedding, the filter weights, and the weights from the fully connected layers; the network is trained with mini-batches through backpropagation; and the gradient-based optimization is performed using the Adagrad updating rule (Duchi et al., 2011). Using the well-known folding theorem, we can calculate fast one-dimensional linear waves in all rows of an input matrix by using Fast Fourier Transforms. To exploit the parallelism of operations, we train the network on a GPU. One matlab per hour implements the number of millions of inputs in the first layer."}, {"heading": "5.2 Sentiment Prediction in Movie Reviews", "text": "The first two experiments concern the prediction of the sensitivity of film reviews in the Stanford Sentiment Treebank (Socher et al., 2013b). The output variable is binary in one experiment and can have five possible outcomes in the other: negative, negative, neutral, somewhat positive, positive. In the case of doppelgangers, we use the predetermined splits of 6920 trainers, 872 developers and 1821 testers. Likewise, in the case of fine grains, the usual 8544 / 1101 / 2210 splits are used. Significant phrases that occur as substances of the training sets are treated as independent training units. The size of the vocabulary is 15448.Table 1 The results of the neural set models - the MaxTDNN, the NBots and the DCNN vectors - are parameters of the models that are randomly initialized."}, {"heading": "5.3 Question Type Classification", "text": "The TREC questionnaire set includes six different types of questions, such as whether the question is a location, a person or some numerical information (Li and Roth, 2002).The training dataset consists of 5452 labeled questions, while the test dataset consists of 500 questions.The results are presented in Tab. 2. Nonneural approaches use a classifier of a large number of manually edited features and hand-coded resources. For example, Blunsom et al. (2006) represents a maximum entropy model based on 26 sets of syntactic and semantic features, including unigrams, bigrams, trigrams, POS tags, named entity labels, structural relationships from a CCG particle and WordNet synsets. We evaluate the three neural models on this dataset with largely the same hyperparameters as in the binary words, which represent a large difference between a CCG particle and a WordNet synthesis."}, {"heading": "5.4 Twitter Sentiment Prediction with Distant Supervision", "text": "In our last experiment, we train the models using a large dataset of tweets, in which a tweet is automatically marked as positive or negative, depending on the emoticon it contains. The training set consists of 1.6 million tweets with emoticon captions and the test set of about 400 hand-annotated tweets. We process the tweets minimally according to the procedure described in Go et al. (2009); in addition, we reduce all tokens. This results in a vocabulary of 76643 word types. The architecture of the DCNN and the other neural models is the same as in the binary experiment of Sekt. 5,2. The randomly initialized word embeddings are lengthened to a dimension of d = 60. Table 3 reports the results of the experiments. We see a significant increase in the performance of the DCNN in relation to the non-neural n-gram-based classifiers; in the presence of large amounts of training data, these classifiers present particularly strong results, which represent the performance of the automatic CNN emogram advantage."}, {"heading": "5.5 Visualising Feature Detectors", "text": "A filter in the DCNN is associated with a feature detector or neuron that learns during training to be particularly active when presented with a specific sequence of input words. In the first layer, the sequence is a continuous n-gram from the input sentence; in higher layers, sequences can be formed from several separate n-grams. We visualize the feature detectors in the first layer of the network trained on the binary sentiment task (Section 5.2). Since the filters have width 7, we assign for each of the 288 feature detectors all 7-grams that occur in the validation and test kits after activation of the detector. Figure 5.2 presents the five best 7-gram detectors for four feature detectors. In addition to the expected detectors for positive and negative feelings, we find detectors for particles such as \"not,\" which negate the mood, and such as \"to increase the mood.\""}, {"heading": "6 Conclusion", "text": "We have described a dynamic convolutionary neural network that uses the dynamic k-max pooling operator as a nonlinear subsampling function. The function graph induced by the network is capable of capturing word relationships of different sizes, and the network achieves high performance in classifying questions and feelings without the external features provided by parsers or other resources."}, {"heading": "Acknowledgements", "text": "We thank Nando de Freitas and Yee Whye Teh for the great discussions on the paper. This work was supported by an Xerox Foundation Award, EPSRC grant number EP / F042728 / 1 and EPSRC grant number EP / K036580 / 1."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "EMNLP, pages 1183\u20131193. ACL.", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Question classification with log-linear models", "author": ["Phil Blunsom", "Krystle Kocik", "James R. Curran."], "venue": "SIGIR \u201906: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Blunsom et al\\.,? 2006", "shortCiteRegEx": "Blunsom et al\\.", "year": 2006}, {"title": "A context-theoretic framework for compositionality in distributional semantics", "author": ["Daoud Clarke."], "venue": "Computational Linguistics, 38(1):41\u201371.", "citeRegEx": "Clarke.,? 2012", "shortCiteRegEx": "Clarke.", "year": 2012}, {"title": "Mathematical Foundations for a Compositional Distributional Model of Meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "March.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "International Conference on Machine Learning, ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "J. Mach. Learn. Res., 12:2121\u20132159, July.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A structured vector space model for word meaning in context", "author": ["Katrin Erk", "Sebastian Pad\u00f3."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP \u201908, (October):897.", "citeRegEx": "Erk and Pad\u00f3.,? 2008", "shortCiteRegEx": "Erk and Pad\u00f3.", "year": 2008}, {"title": "Vector space models of word meaning and phrase meaning: A survey", "author": ["Katrin Erk."], "venue": "Language and Linguistics Compass, 6(10):635\u2013653.", "citeRegEx": "Erk.,? 2012", "shortCiteRegEx": "Erk.", "year": 2012}, {"title": "Lstm recurrent networks learn simple context-free and context-sensitive languages", "author": ["Felix A. Gers", "Jrgen Schmidhuber."], "venue": "IEEE Transactions on Neural Networks, 12(6):1333\u20131340.", "citeRegEx": "Gers and Schmidhuber.,? 2001", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2001}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang."], "venue": "Processing, pages 1\u20136.", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394\u20131404. Asso-", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Category-theoretic quantitative compositional distributional models of natural language semantics", "author": ["Edward Grefenstette."], "venue": "arXiv preprint arXiv:1311.1539.", "citeRegEx": "Grefenstette.,? 2013", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Modelling Adjective-Noun Compositionality by Regression", "author": ["Emiliano Guevara."], "venue": "ESSLLI\u201910 Workshop on Compositionality and Distributional Semantic Models.", "citeRegEx": "Guevara.,? 2010", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "The Role of Syntax in Vector Space Models of Compositional Semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Sofia, Bulgaria,", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "CoRR, abs/1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Connectionist learning procedures", "author": ["Geoffrey E. Hinton."], "venue": "Artif. Intell., 40(1-3):185\u2013234.", "citeRegEx": "Hinton.,? 1989", "shortCiteRegEx": "Hinton.", "year": 1989}, {"title": "Question classification using head words and their hypernyms", "author": ["Zhiheng Huang", "Marcus Thint", "Zengchang Qin."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201908, pages 927\u2013936, Stroudsburg,", "citeRegEx": "Huang et al\\.,? 2008", "shortCiteRegEx": "Huang et al\\.", "year": 2008}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, October. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013a", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Recurrent Convolutional Neural Networks for Discourse Compositionality", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria, August. Association", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013b", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Prior disambiguation of word tensors for constructing sentence vectors", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA, October.", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2013", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2013}, {"title": "Inductive learning in symbolic domains using structuredriven recurrent neural networks", "author": ["Andreas K\u00fcchler", "Christoph Goller."], "venue": "G\u00fcnther G\u00f6rz and Steffen H\u00f6lldobler, editors, KI, volume 1137 of Lecture Notes in Computer Science, pages 183\u2013197.", "citeRegEx": "K\u00fcchler and Goller.,? 1996", "shortCiteRegEx": "K\u00fcchler and Goller.", "year": 1996}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, November.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth."], "venue": "Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1\u20137. Association for Computational Linguistics.", "citeRegEx": "Li and Roth.,? 2002", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig."], "venue": "SLT, pages 234\u2013239.", "citeRegEx": "Mikolov and Zweig.,? 2012", "shortCiteRegEx": "Mikolov and Zweig.", "year": 2012}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Lukas Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur."], "venue": "ICASSP, pages 5528\u20135531. IEEE.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of ACL, volume 8.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Recursive distributed representations", "author": ["Jordan B. Pollack."], "venue": "Artificial Intelligence, 46:77\u2013105.", "citeRegEx": "Pollack.,? 1990", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Holger Schwenk."], "venue": "COLING (Posters), pages 1071\u20131080.", "citeRegEx": "Schwenk.,? 2012", "shortCiteRegEx": "Schwenk.", "year": 2012}, {"title": "From symbolic to subsymbolic information in question classification", "author": ["Joo Silva", "Lusa Coheur", "AnaCristina Mendes", "Andreas Wichert."], "venue": "Artificial Intelligence Review, 35(2):137\u2013154.", "citeRegEx": "Silva et al\\.,? 2011", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Nat-", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "author": ["Richard Socher", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Transactions of the Association for Computational Linguistics (TACL).", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384\u2013394. Association for", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Domain and function: A dualspace model of semantic relations and compositions", "author": ["Peter Turney."], "venue": "J. Artif. Intell. Res.(JAIR), 44:533\u2013585.", "citeRegEx": "Turney.,? 2012", "shortCiteRegEx": "Turney.", "year": 2012}, {"title": "Readings in speech recognition", "author": ["Alexander Waibel", "Toshiyuki Hanazawa", "Geofrey Hinton", "Kiyohiro Shikano", "Kevin J. Lang."], "venue": "chapter Phoneme Recognition Using Time-delay Neural Networks, pages 393\u2013404. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "Waibel et al\\.,? 1990", "shortCiteRegEx": "Waibel et al\\.", "year": 1990}, {"title": "Estimating linear models for compositional distributional semantics", "author": ["Fabio Massimo Zanzotto", "Ioannis Korkontzelos", "Francesca Fallucchi", "Suresh Manandhar."], "venue": "Proceedings of the 23rd International Conference on Computational Linguis-", "citeRegEx": "Zanzotto et al\\.,? 2010", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "UAI, pages 658\u2013666. AUAI Press.", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}], "referenceMentions": [{"referenceID": 6, "context": "In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors (Erk and Pad\u00f3, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Turney, 2012; Erk, 2012; Clarke, 2012).", "startOffset": 124, "endOffset": 237}, {"referenceID": 25, "context": "In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors (Erk and Pad\u00f3, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Turney, 2012; Erk, 2012; Clarke, 2012).", "startOffset": 124, "endOffset": 237}, {"referenceID": 26, "context": "In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors (Erk and Pad\u00f3, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Turney, 2012; Erk, 2012; Clarke, 2012).", "startOffset": 124, "endOffset": 237}, {"referenceID": 34, "context": "In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors (Erk and Pad\u00f3, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Turney, 2012; Erk, 2012; Clarke, 2012).", "startOffset": 124, "endOffset": 237}, {"referenceID": 7, "context": "In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors (Erk and Pad\u00f3, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Turney, 2012; Erk, 2012; Clarke, 2012).", "startOffset": 124, "endOffset": 237}, {"referenceID": 2, "context": "In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors (Erk and Pad\u00f3, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Turney, 2012; Erk, 2012; Clarke, 2012).", "startOffset": 124, "endOffset": 237}, {"referenceID": 12, "context": "In other cases, a composition function is learned and either tied to particular syntactic relations (Guevara, 2010; Zanzotto et al., 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 100, "endOffset": 138}, {"referenceID": 36, "context": "In other cases, a composition function is learned and either tied to particular syntactic relations (Guevara, 2010; Zanzotto et al., 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 100, "endOffset": 138}, {"referenceID": 0, "context": ", 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2013; Grefenstette, 2013).", "startOffset": 36, "endOffset": 172}, {"referenceID": 3, "context": ", 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2013; Grefenstette, 2013).", "startOffset": 36, "endOffset": 172}, {"referenceID": 10, "context": ", 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2013; Grefenstette, 2013).", "startOffset": 36, "endOffset": 172}, {"referenceID": 19, "context": ", 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2013; Grefenstette, 2013).", "startOffset": 36, "endOffset": 172}, {"referenceID": 11, "context": ", 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2013; Grefenstette, 2013).", "startOffset": 36, "endOffset": 172}, {"referenceID": 37, "context": "Another approach represents the meaning of sentences by way of automatically extracted logical forms (Zettlemoyer and Collins, 2005).", "startOffset": 101, "endOffset": 132}, {"referenceID": 4, "context": "These range from basic neural bag-of-words or bag-of-n-grams models to the more structured recursive neural networks and to time-delay neural networks based on convolutional operations (Collobert and Weston, 2008; Socher et al., 2011; Kalchbrenner and Blunsom, 2013b).", "startOffset": 185, "endOffset": 267}, {"referenceID": 30, "context": "These range from basic neural bag-of-words or bag-of-n-grams models to the more structured recursive neural networks and to time-delay neural networks based on convolutional operations (Collobert and Weston, 2008; Socher et al., 2011; Kalchbrenner and Blunsom, 2013b).", "startOffset": 185, "endOffset": 267}, {"referenceID": 18, "context": "These range from basic neural bag-of-words or bag-of-n-grams models to the more structured recursive neural networks and to time-delay neural networks based on convolutional operations (Collobert and Weston, 2008; Socher et al., 2011; Kalchbrenner and Blunsom, 2013b).", "startOffset": 185, "endOffset": 267}, {"referenceID": 28, "context": "Besides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language model to generate sentences word by word (Schwenk, 2012; Mikolov and Zweig, 2012; Kalchbrenner and Blunsom, 2013a).", "startOffset": 178, "endOffset": 251}, {"referenceID": 23, "context": "Besides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language model to generate sentences word by word (Schwenk, 2012; Mikolov and Zweig, 2012; Kalchbrenner and Blunsom, 2013a).", "startOffset": 178, "endOffset": 251}, {"referenceID": 17, "context": "Besides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language model to generate sentences word by word (Schwenk, 2012; Mikolov and Zweig, 2012; Kalchbrenner and Blunsom, 2013a).", "startOffset": 178, "endOffset": 251}, {"referenceID": 21, "context": "The max pooling operator is a non-linear subsampling function that returns the maximum of a set of values (LeCun et al., 1998).", "startOffset": 106, "endOffset": 126}, {"referenceID": 21, "context": "Like in the convolutional networks for object recognition (LeCun et al., 1998), we enrich the representation in the first layer by computing multiple feature maps with different filters applied to the input sentence.", "startOffset": 58, "endOffset": 78}, {"referenceID": 32, "context": "The first two experiments involve predicting the sentiment of movie reviews (Socher et al., 2013b).", "startOffset": 76, "endOffset": 98}, {"referenceID": 22, "context": "The third experiment involves the categorisation of questions in six question types in the TREC dataset (Li and Roth, 2002).", "startOffset": 104, "endOffset": 123}, {"referenceID": 9, "context": "The fourth experiment involves predicting the sentiment of Twitter posts using distant supervision (Go et al., 2009).", "startOffset": 99, "endOffset": 116}, {"referenceID": 9, "context": "The fourth experiment involves predicting the sentiment of Twitter posts using distant supervision (Go et al., 2009). The network is trained on 1.6 million tweets labelled automatically according to the emoticon that occurs in them. On the hand-labelled test set, the network achieves a greater than 25% reduction in the prediction error with respect to the strongest unigram and bigram baseline reported in Go et al. (2009).", "startOffset": 100, "endOffset": 425}, {"referenceID": 15, "context": "Then we describe the operation of onedimensional convolution and the classical TimeDelay Neural Network (TDNN) (Hinton, 1989; Waibel et al., 1990).", "startOffset": 111, "endOffset": 146}, {"referenceID": 35, "context": "Then we describe the operation of onedimensional convolution and the classical TimeDelay Neural Network (TDNN) (Hinton, 1989; Waibel et al., 1990).", "startOffset": 111, "endOffset": 146}, {"referenceID": 4, "context": "layer to the network, the TDNN can be adopted as a sentence model (Collobert and Weston, 2008).", "startOffset": 66, "endOffset": 94}, {"referenceID": 27, "context": "A model that adopts a more general structure provided by an external parse tree is the Recursive Neural Network (RecNN) (Pollack, 1990; K\u00fcchler and Goller, 1996; Socher et al., 2011; Hermann and Blunsom, 2013).", "startOffset": 120, "endOffset": 209}, {"referenceID": 20, "context": "A model that adopts a more general structure provided by an external parse tree is the Recursive Neural Network (RecNN) (Pollack, 1990; K\u00fcchler and Goller, 1996; Socher et al., 2011; Hermann and Blunsom, 2013).", "startOffset": 120, "endOffset": 209}, {"referenceID": 30, "context": "A model that adopts a more general structure provided by an external parse tree is the Recursive Neural Network (RecNN) (Pollack, 1990; K\u00fcchler and Goller, 1996; Socher et al., 2011; Hermann and Blunsom, 2013).", "startOffset": 120, "endOffset": 209}, {"referenceID": 13, "context": "A model that adopts a more general structure provided by an external parse tree is the Recursive Neural Network (RecNN) (Pollack, 1990; K\u00fcchler and Goller, 1996; Socher et al., 2011; Hermann and Blunsom, 2013).", "startOffset": 120, "endOffset": 209}, {"referenceID": 8, "context": "The Recurrent Neural Network (RNN) is a special case of the recursive network where the structure that is followed is a simple linear chain (Gers and Schmidhuber, 2001; Mikolov et al., 2011).", "startOffset": 140, "endOffset": 190}, {"referenceID": 24, "context": "The Recurrent Neural Network (RNN) is a special case of the recursive network where the structure that is followed is a simple linear chain (Gers and Schmidhuber, 2001; Mikolov et al., 2011).", "startOffset": 140, "endOffset": 190}, {"referenceID": 4, "context": "Finally, a further class of neural sentence models is based on the convolution operation and the TDNN architecture (Collobert and Weston, 2008; Kalchbrenner and Blunsom, 2013b).", "startOffset": 115, "endOffset": 176}, {"referenceID": 18, "context": "Finally, a further class of neural sentence models is based on the convolution operation and the TDNN architecture (Collobert and Weston, 2008; Kalchbrenner and Blunsom, 2013b).", "startOffset": 115, "endOffset": 176}, {"referenceID": 35, "context": "As in the TDNN for phoneme recognition (Waibel et al., 1990), the sequence s is viewed as having a time dimension and the convolution is applied over the time dimension.", "startOffset": 39, "endOffset": 60}, {"referenceID": 4, "context": "The Max-TDNN sentence model is based on the architecture of a TDNN (Collobert and Weston, 2008).", "startOffset": 67, "endOffset": 95}, {"referenceID": 21, "context": "We next describe a pooling operation that is a generalisation of the max pooling over the time dimension used in the Max-TDNN sentence model and different from the local max pooling operations applied in a convolutional network for object recognition (LeCun et al., 1998).", "startOffset": 251, "endOffset": 271}, {"referenceID": 24, "context": "A sentence model based on a recurrent neural network is sensitive to word order, but it has a bias towards the latest words that it takes as input (Mikolov et al., 2011).", "startOffset": 147, "endOffset": 169}, {"referenceID": 31, "context": "Similarly, a recursive neural network is sensitive to word order but has a bias towards the topmost nodes in the tree; shallower trees mitigate this effect to some extent (Socher et al., 2013a).", "startOffset": 171, "endOffset": 193}, {"referenceID": 30, "context": "The first four results are reported from Socher et al. (2013b). The baselines NB and BINB are Naive Bayes classifiers with, respectively, unigram features and unigram and bigram features.", "startOffset": 41, "endOffset": 63}, {"referenceID": 5, "context": "The network is trained with mini-batches by backpropagation and the gradient-based optimisation is performed using the Adagrad update rule (Duchi et al., 2011).", "startOffset": 139, "endOffset": 159}, {"referenceID": 32, "context": "The first two experiments concern the prediction of the sentiment of movie reviews in the Stanford Sentiment Treebank (Socher et al., 2013b).", "startOffset": 118, "endOffset": 140}, {"referenceID": 20, "context": "The first four results are respectively from Li and Roth (2002), Blunsom et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 1, "context": "The first four results are respectively from Li and Roth (2002), Blunsom et al. (2006), Huang et al.", "startOffset": 65, "endOffset": 87}, {"referenceID": 1, "context": "The first four results are respectively from Li and Roth (2002), Blunsom et al. (2006), Huang et al. (2008) and Silva et al.", "startOffset": 65, "endOffset": 108}, {"referenceID": 1, "context": "The first four results are respectively from Li and Roth (2002), Blunsom et al. (2006), Huang et al. (2008) and Silva et al. (2011).", "startOffset": 65, "endOffset": 132}, {"referenceID": 9, "context": "The three non-neural classifiers are based on unigram and bigram features; the results are reported from (Go et al., 2009).", "startOffset": 105, "endOffset": 122}, {"referenceID": 14, "context": "At training time we apply dropout to the penultimate layer after the last tanh non-linearity (Hinton et al., 2012).", "startOffset": 93, "endOffset": 114}, {"referenceID": 22, "context": "whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002).", "startOffset": 91, "endOffset": 110}, {"referenceID": 1, "context": "For instance, Blunsom et al. (2006) present a Maximum Entropy model that relies on 26 sets of syntactic and semantic features including unigrams, bigrams, trigrams, POS tags, named entity tags, structural relations from a CCG parse and WordNet synsets.", "startOffset": 14, "endOffset": 36}, {"referenceID": 33, "context": "As the dataset is rather small, we use lower-dimensional word vectors with d = 32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al., 2010).", "startOffset": 184, "endOffset": 205}, {"referenceID": 9, "context": "We preprocess the tweets minimally following the procedure described in Go et al. (2009); in addition, we also lowercase all the tokens.", "startOffset": 72, "endOffset": 89}], "year": 2014, "abstractText": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.", "creator": "TeX"}}}