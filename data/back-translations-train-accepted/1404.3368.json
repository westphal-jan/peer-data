{"id": "1404.3368", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2014", "title": "Near-optimal sample compression for nearest neighbors", "abstract": "We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.", "histories": [["v1", "Sun, 13 Apr 2014 11:13:02 GMT  (54kb)", "https://arxiv.org/abs/1404.3368v1", null], ["v2", "Thu, 4 Dec 2014 15:23:49 GMT  (96kb)", "http://arxiv.org/abs/1404.3368v2", null], ["v3", "Fri, 5 Dec 2014 10:38:21 GMT  (92kb)", "http://arxiv.org/abs/1404.3368v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CC", "authors": ["lee-ad gottlieb", "aryeh kontorovich", "pinhas nisnevitch"], "accepted": true, "id": "1404.3368"}, "pdf": {"name": "1404.3368.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["leead@ariel.ac.il", "karyeh@cs.bgu.ac.il", "pinhasn@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 140 4.33 68v3 [cs.LG] December 5, 201 4Near-optimal compression of samples for the nearest neighborhoods Lee-Ad Gottlieb Faculty of Informatics and Mathematics, Ariel University Ariel, Israel. leead @ ariel.ac.ilAryeh Kontorovich Computer Science Department, Ben Gurion University Beer Sheva, Israel. karyeh @ cs.bgu.ac.ilPinhas Nisnevitch Faculty of Informatics and Mathematics, Ariel University Ariel, Israel. pinhas @ gmail.com December 8, 2014We present the first compression algorithm for samples for the nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost identical hardness thresholds showing that our boundary is near optimal. Our result provides new insights into margin-based closest neighborhood classifications in metric spaces and allows us to significantly sharpen and simplify existing boundaries. Some encouraging results are also presented."}, {"heading": "1 Introduction", "text": "It is not only the question of whether it is a problem, but also the question of whether it is a problem in which it is a problem that has a limited scope in most countries of the world. (It is the question of whether it is a problem that can exist in most countries of the world.) It is the question of whether it is a problem that is at all a problem in most countries of the world. (It is the question of whether it is a problem that is at all a problem in most countries of the world. (It is the question of whether it is a problem that is at all a problem in most countries of the world.) (It is the question of whether it is a problem that is a problem that is at all a problem in most countries of the world.)"}, {"heading": "1.1 Preliminaries", "text": "The doubling dimension of X is a positive symmetric function that satisfies the triangular inequality d (x, y) \u2264 d (x, z) + d (z, y).The doubling dimension of a set A (X, d) is defined by diam (A) = supx, y).The doubling dimension (X, d) is the smallest value so that any ball in X of radius r (for each r) can be covered.The doubling dimension of X is ddim (X) = log2 (X, d), is the smallest value so that any ball in X of radius r (for each r) can be covered.The doubling dimension of X is ddim (X) = log2).The doubling dimension of X is doubled if the doubling dimension is expanded."}, {"heading": "2 Near-optimal approximation algorithm", "text": "In this section we describe a simple approximation algorithm for the nearest neighbor."}, {"heading": "2.1 Hardness of approximation of NNC", "text": "In this area, we are in a position to take a number of measures that are in a position to put themselves in a position to hide, that are in a position to hide themselves, that is in a position to put themselves in a position to put themselves in a position to put themselves in a position to put themselves in a position to put themselves in a position to put themselves in a position, that is in a position to put themselves in a position, in a position to put themselves in a position, in a position to put themselves in a position, in a position to put themselves in a position, in a position to put themselves in a position, in a position to put themselves in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, a position, in a position, in a position, in a position, a position, in a position, in a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a position, a, a position, a position, a position, a position, a position, a position, a position, a, a position, a, a position, a, a position, a position, a position, a position, a, a, a, a, a, a position, a, a"}, {"heading": "3 Learning", "text": "In this section we apply Theorem 1 in order to obtain improved boundaries of generalization for binary classification (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S. (S). (S). (S). (S). (S). (S). (S.). (S. (S.). (S. (S.). (S.).). (S."}, {"heading": "4 Experiments", "text": "In this section we will discuss experimental results. First, we will describe a simple heuristics based on our algorithm."}, {"heading": "A Fast net construction", "text": "In this section we offer an illustration of the fast net algorithm of Section 2. See q q q in the appendix, where for each p-point q-S we keep lists that represent either the parental or an arbitrary detection point. Although we have assumed that knowledge about children in S2i + 1. Further, we can terminate the algorithm if we encounter a net point p (q, i) in S2i that is either the parental or an arbitrary detection point. (Although we have assumed that knowledge about children in S2i + 1.) In fact, we cannot terminate the algorithm if we encounter a net value p (q, i), where for all p-points S2i and q-points S, if d (p, q) < 2i then p and q-points of the same mark are set."}], "references": [{"title": "Discriminatory analysis. nonparametric discrimination: Consistency properties", "author": ["E. Fix", "J.L. Hodges"], "venue": "International Statistical Review / Revue Internationale de Statistique,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "IEEE Trans. Info. Theo.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1967}, {"title": "A Bayes consistent 1-NN classifier (arXiv:1407.0208)", "author": ["A. Kontorovich", "R. Weiss"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Open problems in geometric methods for instance-based learning", "author": ["G. Toussaint"], "venue": "In Discrete and computational geometry,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Rates of Convergence for Nearest Neighbor Classification", "author": ["K. Chaudhuri", "S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Distance-based classification with Lipschitz functions", "author": ["U. von Luxburg", "O. Bousquet"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Navigating nets: Simple algorithms for proximity search", "author": ["R. Krauthgamer", "J.R. Lee"], "venue": "In SODA,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "An algorithm for approximate closest-point queries", "author": ["K.L. Clarkson"], "venue": "In SCG,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "On the strong universal consistency of nearest neighbor regression function estimates", "author": ["L. Devroye", "L. Gy\u00f6rfi", "A. Krzy\u017cak", "G. Lugosi"], "venue": "Ann. Statist.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Asymptotic expansions of the k nearest neighbor risk", "author": ["R.R. Snapp", "S.S. Venkatesh"], "venue": "Ann. Statist.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Efficient classification for metric data", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "In COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "The condensed nearest neighbor rule", "author": ["P.E. Hart"], "venue": "IEEE Trans. Info. Theo.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1968}, {"title": "Nearest neighbor problems", "author": ["G. Wilfong"], "venue": "In SCG,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}, {"title": "NP-completeness of the problem of prototype selection in the nearest neighbor method", "author": ["A.V. Zukhba"], "venue": "Pattern Recognit. Image Anal.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Fast condensed nearest neighbor rule", "author": ["F. Angiulli"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "The reduced nearest neighbor rule", "author": ["W. Gates"], "venue": "IEEE Trans. Info. Theo.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1972}, {"title": "An algorithm for a selective nearest neighbor decision rule", "author": ["G.L. Ritter", "H.B. Woodruff", "S.R. Lowry", "T.L. Isenhour"], "venue": "IEEE Trans. Info. Theo.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1975}, {"title": "Reduction techniques for instance-based learning algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Mach. Learn.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Commun. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1984}, {"title": "Quantifying inductive bias: AI learning algorithms and valiant\u2019s learning framework", "author": ["D. Haussler"], "venue": "Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1988}, {"title": "Learning the set covering machine by bound minimization and margin-sparsity trade-off", "author": ["F. Laviolette", "M. Marchand", "M. Shah", "S. Shanian"], "venue": "Mach. Learn.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "The set covering machine", "author": ["M. Marchand", "J. Shawe-Taylor"], "venue": "JMLR, 3:723\u2013746,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Proximity algorithms for nearly doubling spaces", "author": ["L. Gottlieb", "R. Krauthgamer"], "venue": "SIAM J. on Discr. Math.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Adaptive metric dimensionality reduction", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "ALT,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Cover trees for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Learnability and the doubling dimension", "author": ["Y. Li", "P.M. Long"], "venue": "In NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Using the doubling dimension to analyze the generalization of learning algorithms", "author": ["N.H. Bshouty", "Y. Li", "P.M. Long"], "venue": "J. Comp. Sys. Sci.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Efficient regression in metric spaces via approximate Lipschitz extension", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "In SIMBAD,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Bounded geometries, fractals, and low-distortion embeddings", "author": ["A. Gupta", "R. Krauthgamer", "J.R. Lee"], "venue": "In FOCS,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "The hardness of approximate optima in lattices, codes, and systems of linear equations", "author": ["S. Arora", "L. Babai", "J. Stern", "Z. Sweedyk"], "venue": "In FOCS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1993}, {"title": "On the hardness of approximating label-cover", "author": ["I. Dinur", "S. Safra"], "venue": "Info. Proc. Lett.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Cover trees for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "ICML", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Fast construction of nets in low-dimensional metrics and their applications", "author": ["S. Har-Peled", "M. Mendel"], "venue": "SIAM J. on Comput.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Searching dynamic point sets in spaces with bounded doubling dimension", "author": ["R. Cole", "L. Gottlieb"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Relating data compression and learnability, unpublished", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1986}, {"title": "A probabilistic theory of pattern recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1996}, {"title": "Pac-bayesian compression bounds on the prediction error of learning algorithms for classification", "author": ["T. Graepel", "R. Herbrich", "J. Shawe-Taylor"], "venue": "Mach. Learn.,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Scale-sensitive dimensions, uniform convergence, and learnability", "author": ["N. Alon", "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler"], "venue": "J. ACM,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "It is apparently the earliest, having been introduced by Fix and Hodges in 1951 (technical report reprinted in [1]).", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Under mild regularity assumptions, the nearest neighbor classifier\u2019s expected error is asymptotically bounded by twice the Bayesian error, when the sample size tends to infinity [2].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "1 These results have inspired a vast body of research on proximity-based classification (see [4, 5] for extensive background and", "startOffset": 93, "endOffset": 99}, {"referenceID": 2, "context": "1A Bayes-consistent modification of the 1-NN classifier was recently proposed in [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "[6] for a recent refinement of classic results).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "More recently, strong margin-dependent generalization bounds were obtained in [7], where the margin is the minimum distance between opposite labeled points in S.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Further, information-theoretic considerations show that exact NN evaluation requires \u0398(|S|) time in high-dimensional metric spaces [8] (and possibly Euclidean space as well [9]) \u2014 a phenomenon known as the algorithmic curse of dimensionality.", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "Further, information-theoretic considerations show that exact NN evaluation requires \u0398(|S|) time in high-dimensional metric spaces [8] (and possibly Euclidean space as well [9]) \u2014 a phenomenon known as the algorithmic curse of dimensionality.", "startOffset": 173, "endOffset": 176}, {"referenceID": 8, "context": "This last problem can be mitigated by taking the majority vote among k > 1 nearest neighbors [10, 11, 5], or by deleting some sample points so as to attain a larger margin [12].", "startOffset": 93, "endOffset": 104}, {"referenceID": 9, "context": "This last problem can be mitigated by taking the majority vote among k > 1 nearest neighbors [10, 11, 5], or by deleting some sample points so as to attain a larger margin [12].", "startOffset": 93, "endOffset": 104}, {"referenceID": 10, "context": "This last problem can be mitigated by taking the majority vote among k > 1 nearest neighbors [10, 11, 5], or by deleting some sample points so as to attain a larger margin [12].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "Shortcomings in the NN classifier led Hart [13] to pose the problem of sample compression.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "This problem is known to be NP-hard [14, 15], and Hart provided a heuristic with runtime O(n).", "startOffset": 36, "endOffset": 44}, {"referenceID": 13, "context": "This problem is known to be NP-hard [14, 15], and Hart provided a heuristic with runtime O(n).", "startOffset": 36, "endOffset": 44}, {"referenceID": 14, "context": "The runtime was recently improved by [16] to O(n), but neither paper gave performance guarantees.", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "The Nearest Neighbor Condensing problem has been the subject of extensive research since its introduction [17, 18, 19].", "startOffset": 106, "endOffset": 118}, {"referenceID": 16, "context": "The Nearest Neighbor Condensing problem has been the subject of extensive research since its introduction [17, 18, 19].", "startOffset": 106, "endOffset": 118}, {"referenceID": 17, "context": "The Nearest Neighbor Condensing problem has been the subject of extensive research since its introduction [17, 18, 19].", "startOffset": 106, "endOffset": 118}, {"referenceID": 18, "context": "A well-studied problem related to the Nearest Neighbor Condensing problem is that of extracting a small set of simple conjunctions consistent with much of the sample, introduced by [20] and shown by [21] to be equivalent to minimum Set Cover (see [22, 23] for further extensions).", "startOffset": 181, "endOffset": 185}, {"referenceID": 19, "context": "A well-studied problem related to the Nearest Neighbor Condensing problem is that of extracting a small set of simple conjunctions consistent with much of the sample, introduced by [20] and shown by [21] to be equivalent to minimum Set Cover (see [22, 23] for further extensions).", "startOffset": 199, "endOffset": 203}, {"referenceID": 20, "context": "A well-studied problem related to the Nearest Neighbor Condensing problem is that of extracting a small set of simple conjunctions consistent with much of the sample, introduced by [20] and shown by [21] to be equivalent to minimum Set Cover (see [22, 23] for further extensions).", "startOffset": 247, "endOffset": 255}, {"referenceID": 21, "context": "A well-studied problem related to the Nearest Neighbor Condensing problem is that of extracting a small set of simple conjunctions consistent with much of the sample, introduced by [20] and shown by [21] to be equivalent to minimum Set Cover (see [22, 23] for further extensions).", "startOffset": 247, "endOffset": 255}, {"referenceID": 22, "context": "For metric spaces, [24] and [25] gave algorithms for dimensionality reduction via point removal (irrespective of margin size).", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "For metric spaces, [24] and [25] gave algorithms for dimensionality reduction via point removal (irrespective of margin size).", "startOffset": 28, "endOffset": 32}, {"referenceID": 24, "context": "The use of doubling dimension as a tool to characterize metric learning has appeared several times in the literature, initially by [26] in the context of nearest neighbor classification, and then in [27] and [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 25, "context": "The use of doubling dimension as a tool to characterize metric learning has appeared several times in the literature, initially by [26] in the context of nearest neighbor classification, and then in [27] and [28].", "startOffset": 199, "endOffset": 203}, {"referenceID": 26, "context": "The use of doubling dimension as a tool to characterize metric learning has appeared several times in the literature, initially by [26] in the context of nearest neighbor classification, and then in [27] and [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 10, "context": "A series of papers by Gottlieb, Kontorovich and Krauthgamer investigate doubling spaces for classification [12], regression [29], and dimension reduction [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 27, "context": "A series of papers by Gottlieb, Kontorovich and Krauthgamer investigate doubling spaces for classification [12], regression [29], and dimension reduction [25].", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "A series of papers by Gottlieb, Kontorovich and Krauthgamer investigate doubling spaces for classification [12], regression [29], and dimension reduction [25].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "A natural question is whether the Nearest Neighbor Condensing problem of [13] has a direct analogue when the 1-nearest neighbor rule is replaced by a (k > 1)-nearest neighbor \u2013 that is, when the label of a point is determined by the majority vote among its k nearest neighbors.", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "dimension (Euclidean metrics of dimension d have doubling dimension O(d) [30]), low doubling dimension is strictly more general than low Euclidean dimension.", "startOffset": 73, "endOffset": 77}, {"referenceID": 6, "context": "(see, for example [8]).", "startOffset": 18, "endOffset": 21}, {"referenceID": 29, "context": "The Label Cover problem was first introduced by [31] in a seminal paper on the hardness of computation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 30, "context": "Several formulations of this problem have appeared the literature, and we give the description forwarded by [32]: The input is a bipartite graph G = (U, V,E), with two sets of labels: A for U and B for V .", "startOffset": 108, "endOffset": 112}, {"referenceID": 30, "context": "It was shown in [32] that it is NP-hard to approximate Label Cover to within a factor 2 1\u2212o(1) , where n is the total size of the input.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "The construction time can be improved by building a net hierarchy, similar to the one employed by [8], in total time 2n log(1/\u03b3).", "startOffset": 98, "endOffset": 101}, {"referenceID": 31, "context": "(See also [34, 35, 36].", "startOffset": 10, "endOffset": 22}, {"referenceID": 32, "context": "(See also [34, 35, 36].", "startOffset": 10, "endOffset": 22}, {"referenceID": 33, "context": "(See also [34, 35, 36].", "startOffset": 10, "endOffset": 22}, {"referenceID": 30, "context": "The theorem will follow from the hardness of Label Cover [32].", "startOffset": 57, "endOffset": 61}, {"referenceID": 34, "context": "The generalizing power of sample compression was independently discovered by [37, 38], and later elaborated upon by [39].", "startOffset": 77, "endOffset": 85}, {"referenceID": 35, "context": "The generalizing power of sample compression was independently discovered by [37, 38], and later elaborated upon by [39].", "startOffset": 77, "endOffset": 85}, {"referenceID": 36, "context": "The generalizing power of sample compression was independently discovered by [37, 38], and later elaborated upon by [39].", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": ", \u03b5-consistent) S\u0303 \u2282 S constitutes a sample compression scheme of size |S\u0303|, as stipulated in [39].", "startOffset": 94, "endOffset": 98}, {"referenceID": 37, "context": "Finally, (2) relied on some fairly intricate fat-shattering arguments [40, 41], while Corollary 1 is an almost immediate consequence of much simpler Occam-type results.", "startOffset": 70, "endOffset": 78}, {"referenceID": 22, "context": "3 In general, ddim(S) \u2264 cddim(X ) for some universal constant c, as shown in [24].", "startOffset": 77, "endOffset": 81}], "year": 2014, "abstractText": "We present the first sample compression algorithm for nearest neighbors with nontrivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.", "creator": "LaTeX with hyperref package"}}}