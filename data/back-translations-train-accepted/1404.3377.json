{"id": "1404.3377", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2014", "title": "A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing", "abstract": "We introduce a novel approach for building language models based on a systematic, recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case. In this paper we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language models using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity.", "histories": [["v1", "Sun, 13 Apr 2014 12:39:41 GMT  (65kb)", "http://arxiv.org/abs/1404.3377v1", "13 pages, 2 figures, ACL 2014"]], "COMMENTS": "13 pages, 2 figures, ACL 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rene pickhardt", "thomas gottron", "martin k\\\"orner", "paul georg wagner", "till speicher", "steffen staab"], "accepted": true, "id": "1404.3377"}, "pdf": {"name": "1404.3377.pdf", "metadata": {"source": "CRF", "title": "A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing", "authors": ["Rene Pickhardt", "Thomas Gottron", "Martin K\u00f6rner", "Steffen Staab", "Till Speicher"], "emails": ["rpickhardt@uni-koblenz.de", "gottron@uni-koblenz.de", "mkoerner@uni-koblenz.de", "staab@uni-koblenz.de", "mail@typology.de"], "sections": [{"heading": null, "text": "ar Xiv: 140 4.33 77v1 [cs.CL] 1 3A pr2 014"}, {"heading": "A Generalized Language Model as the", "text": "The combination of skipped n-grams and modified Kneser-Ney SmoothingRene Pickhardt, Thomas Gottron, Martin Ko \ufffd rner, Steffen Staab Institute for Web Science and TechnologiesUniversity of Koblenz-Landau, Germany {rpickhardt, gottron, mkoerner, staab} @ uni-koblenz.dePaul Georg Wagner and Till Speicher Typology GbRmail @ typology.en15 April 2014The copyright in this work belongs to the Association for Computational Linguistics (ACL). However, each of the authors and the employers for whom the work was carried out reserve all other rights, in particular the following:... (4) The right to make copies of the work for internal distribution within the author's organization and for external distribution as a pre-print, reprint, technical report or related document category results in a slight change in the language."}, {"heading": "1. Introduction motivation", "text": "rrteeeGsn rf\u00fc ide eeisrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2. Related Work", "text": "In fact, most of them are able to survive on their own if they do not play by the rules."}, {"heading": "2.1. Review of Modified Kneser-Ney Smoothing", "text": "Let us briefly recall the modified Kneser-Ney leveling as represented in [5]. The modified Kneser-Ney implements a leveling by interpolating between language models of higher and lower order n-gram. \u2212 The distribution of the highest order is interpolated as follows: PMKN (wi | w i \u2212 1 i \u2212 n + 1) = max {c (wii \u2212 n + 1) \u2212 D (c (w i \u2212 n + 1))), 0} c (wi \u2212 n + 1) + \u03b3high (wi \u2212 1 i \u2212 n + 1) P \u0445 MKN (wi | w i \u2212 1 i \u2212 n + 2) (2) \u2212 where c (wii \u2212 n \u2212 n \u2212 ede \u2212 n) returns the frequency number that occurs in the training data, D is a discount value (which depends on the frequency of the order Ni \u2212 n)."}, {"heading": "3. Generalized Language Models", "text": "3.1. Notation for Skip n-gram with k SkipsWe express skip n-grams using an operator notation. The operator \u2202 i applies the word at the i position to an n-gram. For example: \u2202 3w1w2w3w4 = w1w2 w4, using a removed word as a placeholder for placeholders for placeholders. The placeholder allows for a greater number of matches. For example, if c (w1w2w3w4) = x and c (w1w2w3bw4) = y then c (w1w2 w4) \u2265 x + y, since at least the two sequences w1w2w3aw4 and w1w2w3bw4 match the sequence w1w2 w4. To align itself with the standard language models, the skip operator applies the first word of a sequence instead of introducing a wildcard. \u2212 K1wii \u2212 n + 1 = w \u2212 polced from \u2212 Moog \u2212 n + 2 \u2212 wits = 1 \u2212 wit \u2212 1."}, {"heading": "3.2. Generalized Language Model", "text": "The interpolation with lower-order models is motivated by the problem of data sparseness in higher-order models. \u2212 wi \u2212 j \u2212 j Low-order models, however, omit only the first word in the local context, which may not necessarily be the reason that the total n-gram is rare. \u2212 This is the motivation for our generalized language models to interpolate not only with a lower-order model in which the first word in a sequence is omitted, but also with all other n-gram models in which a word is omitted. \u2212 Linking this idea to the modified Kneser-Ney smoothing results in a formula similar to (2). PGLM (wi i \u2212 1 i \u2212 n + 1) = max {c (wii \u2212 n \u2212 n \u2212 n model) \u2212 D (w i \u2212 n + 1), 0} c (wi \u2212 n + 1), which is led to a formula similar to (2)."}, {"heading": "4. Experimental Setup and Data Sets", "text": "To evaluate the quality of our generalized language models, we empirically compare their ability to explain word sequences. To this end, we use text corpora, divide them into test and training data, create language models and generalized language models of the training data and apply them to the test data. We use established metrics such as cross entropy and helplessness. Below, we explain the details of our experimental setup."}, {"heading": "4.1. Data Sets", "text": "For evaluation purposes, we used eight different sets of data. The records cover different areas and languages. We considered English (en), German (de), French (fr) and Italian (it) as languages. As general domain records, we used the complete collection of articles from Wikipedia (Wiki) in the corresponding languages. Download data from the dumps is provided in Table 1.Domain data for special purposes is provided by the JRC Multilingual Acquis Corpus of Legislative Texts (JRC) [21]. Table 2 gives an overview of the records and provides some simple statistics on the languages covered and the size of collections. The records come in the form of structured corpora text, which we have cleaned from markup and marked to generate word sequences. We filtered the word marks by removing all character sequences that did not contain letters, digits or common phrases, and finally, the word marks we named for the school sequences, were divided into the algorithms that were the basis for the assessment and the people who were the school sequences."}, {"heading": "4.2. Evaluation Methodology", "text": "All data sets were randomly divided into a training and a test set on a sentence level. Training sets consisted of 80% of the sets used to derive n-grams, skip n-grams, and appropriate sequence counts for values of n between 1 and 5. Note that we trained a predictive model for each set separately, and from the remaining 20% of the sequences we randomly sampled a separate set of 100,000 sequences of 5 words each. These test sequences were also shortened to sequences of length 3 and 4, and provide a basis for our final experiments to evaluate the performance of the various algorithms. We learned the generalized language models on the same split of the training corpus as the standard language model with modified Kneser-Ney smoothing, and we used the same set of test sequences for a direct comparison, to ensure that the rigor openness of the data sets for training as well as the test code and the complete probabilities are matched."}, {"heading": "4.3. Evaluation Metrics", "text": "We use perplexity: a standard measure in the range of language models [16]. First, we calculate the cross entropy of a trained language model using a test set using H (Palg) = \u2212 \u0445 TPMLE (s) \u00b7 log2 Palg (s) (7), where Palg is replaced by the probability estimates of our general language models and the estimates of a language model with modified Kneser-Ney smoothing. PMLE, on the other hand, is a maximum probability estimate for the test sequence occurring in the test corpus. Finally, T is the set of test sequences. Perplexity is defined as: Perplexity (Palg) = 2 H (Palg) (8) Lower perplexity values indicate better results."}, {"heading": "5. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Baseline", "text": "As a basis for our Generalized Language Model (GLM), we have trained standard language models with modified Kneser-Ney Smoothing (MKN) models, which are identical for model lengths 3 to 5. For Uni- and Bigram models, MKN and GLM are identical."}, {"heading": "5.2. Evaluation Experiments", "text": "The perplexity values for all datasets and the different arrangements can be seen in the table. This table also shows the relative reduction of the perplexity compared to the baselines."}, {"heading": "6. Discussion", "text": "In our experiments, we observed an improvement in our generalized language models compared to classical language models by using Kneser-Ney smoothing, which was observed for different languages, different domains, and different sizes of training data. In the experiments, we also saw that the GLM performs well, especially for small training data sets and sparse data, which boosts our initial motivation. This feature of the GLM is of particular value, as data sparseness becomes a more and more inherent problem with higher values of n. This well-known fact is also underscored by the statistics presented in Table 6. The fraction of the total n-grams that occur only once in our Wikipedia corpus increases with higher values of n. However, with the same value of n, the n-gram skips are rarer. Our generalized language models use this additional information to obtain more reliable estimates for the probability of word sequences."}, {"heading": "7. Conclusion and Future Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. Conclusion", "text": "The main strength of our approach is the combination of a simple and elegant idea with an empirically convincing result. Mathematically, it can be seen that the GLM incorporates the standard language model with modified Kneser-Ney smoothing as a partial model and thus represents a true generalization. In an empirical evaluation, we have shown that the GLM exceeds MKN for higher orders in all test cases. The relative improvement of the perplexity is up to 12.7% for large data sets. GLMs also work particularly well for small and sparse training data. In very small training data, we found a 25.7% reduction in the perplexity. Our experiments underline that the generalized language models overcome the weaknesses of modified Kneser-Ney smoothing in particular with sparse training data."}, {"heading": "7.2. Future work", "text": "A desirable extension of our current definition of GLMs would be to combine different lower order models in our general language model with different weights for each model. Such weights can be used to model the statistical reliability of the various lower order models, and the value of the weights would have to be skipped according to the probability or number of n-grams in each model. Another important step that has not yet been considered is the compression and indexing of general language models to improve the performance of the calculation and to be able to store them in main memory. In terms of the scalability of the approach to very large data sets, we intend to apply the map reduction techniques of [11] to our general language models in order to have a more scalable calculation, which also opens the door to another interesting experiment. Goodman [9] noted that increasing the length of n-graphs in combination with modified Kneser-Ney smoothing has not led to improvements in values above 7."}, {"heading": "Acknowledgements", "text": "We would like to thank Heinrich Hartmann for a fruitful discussion on the notation of the skip operator for n-grams. Research that led to these results was funded by the Seventh Framework Programme of the European Community (FP7 / 2007-2013), REVEAL (Grant agree number 610928)."}, {"heading": "A. Discount Values and Weights in Modified Kneser Ney", "text": "The discount value D (c) used in formula (2) is defined as [5]: D (c) = 0 if c = 0D1 if c = 0D1 if c = 1 D2 if c = 2 D3 + if c > 2 (9) The discount values D1, D2 and D3 + are defined as [4] D1 = 1 \u2212 2Y n2 n1 (10a) D2 = 2 \u2212 3Y n3 n2 (10b) D3 + = 3 \u2212 4Y n3 (10c) with Y = n1 n1 i + n2 and ni is the total number of n-grams that appear exactly i-times in the training data \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 n2 (10b) D3 + = 3 \u2212 4Y n3 (10c) with Y = n1 n2 i + n2 and ni is the total number of n-grams that are \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 1 \u00b7 w exactly i-times. The weight is defined as: high (w i \u00b7 1 \u00b7 i \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 1 \u00b7 1 \u00b7 i \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n (w i \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n = 1 \u00d7 1 \u00d7 1 \u00d7 i \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u00d7 1 \u00d7 1 \u00d7 1 \u00d7 1 \u00d7 i \u2212 i \u2212 i \u2212 i \u2212 i \u2212 i \u2212 w \u2212 n \u2212 n \u2212 1 \u00d7 i \u2212 1 \u00d7 i \u2212 w \u2212 w \u2212 n \u2212 1 \u00d7 i \u2212 w \u2212 n \u2212 n \u2212 1 \u00d7 1 \u00d7 i \u2212 w \u2212 w \u2212 w \u2212 w \u2212 n \u2212 n \u2212 1 \u2212 n \u2212 n \u2212 1 \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 n \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 n \u2212 n \u2212 1 \u2212 n \u2212 n \u2212 n \u2212 1 (1 \u2212 1 \u2212 1 \u2212 n \u2212 1"}], "references": [{"title": "Predicting sentences using n-gram language models", "author": ["Steffen Bickel", "Peter Haider", "Tobias Scheffer"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "A statistical approach to machine translation", "author": ["Peter F Brown", "John Cocke", "Stephen A Della Pietra", "Vincent J Della Pietra", "Fredrick Jelinek", "John D Lafferty", "Robert L Mercer", "Paul S Roossin"], "venue": "Computational linguistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": "Comput. Linguist.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley Chen", "Joshua Goodman"], "venue": "Technical report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley Chen", "Joshua Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Long distance dependency in language modeling: An empirical study", "author": ["Jianfeng Gao", "Hisami Suzuki"], "venue": "Natural Language Processing IJCNLP 2004,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "The population frequencies of species and the estimation of population", "author": ["Irwin J. Good"], "venue": "parameters. Biometrika,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1953}, {"title": "Putting it all together: language model combination", "author": ["Joshua T. Goodman"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "A bit of progress in language modeling \u2013 extended version", "author": ["Joshua T. Goodman"], "venue": "Technical Report MSR-TR-2001-72, Microsoft Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "A closer look at skip-gram modelling", "author": ["David Guthrie", "Ben Allison", "Wei Liu", "Louise Guthrie", "York Wilks"], "venue": "In Proceedings LREC\u20192006,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Scalable modified kneser-ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "The sphinx-ii speech recognition system: an overview", "author": ["Xuedong Huang", "Fileno Alleva", "Hsiao-Wuen Hon", "Mei-Yuh Hwang", "Kai-Fu Lee", "Ronald Rosenfeld"], "venue": "Computer Speech & Language,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Interpolated estimation of markov source parameters from sparse data", "author": ["F. Jelinek", "R.L. Mercer"], "venue": "In Proceedings of the Workshop on Pattern Recognition in Practice,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1980}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1987}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Foundations of statistical natural language processing", "author": ["Christopher D. Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Context based spelling correction", "author": ["Eric Mays", "Fred J Damerau", "Robert L Mercer"], "venue": "Information Processing & Management,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "On structuring probabilistic dependences in stochastic language modelling", "author": ["Hermann Ney", "Ute Essen", "Reinhard Kneser"], "venue": "Computer Speech & Language,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1994}, {"title": "Fundamentals of Speech Recognition", "author": ["Lawrence Rabiner", "Biing-Hwang Juang"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Permugram language models", "author": ["Ernst-G\u00fcnter Schukat-Talamazzini", "R Hendrych", "Ralf Kompe", "Heinrich Niemann"], "venue": "In Fourth European Conference on Speech Communication and Technology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "The jrc-acquis: A multilingual aligned parallel corpus", "author": ["Ralf Steinberger", "Bruno Pouliquen", "Anna Widiger", "Camelia Ignat", "Tomaz Erjavec", "Dan Tufis", "Daniel Varga"], "venue": "Proceedings of the 5th International Conference on Language Resources and Evaluation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "word prediction [1], speech recognition [19], machine translation [2], or spelling correction [17].", "startOffset": 16, "endOffset": 19}, {"referenceID": 18, "context": "word prediction [1], speech recognition [19], machine translation [2], or spelling correction [17].", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "word prediction [1], speech recognition [19], machine translation [2], or spelling correction [17].", "startOffset": 66, "endOffset": 69}, {"referenceID": 16, "context": "word prediction [1], speech recognition [19], machine translation [2], or spelling correction [17].", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "Chen and Goodman [5] introduced modified Kneser-Ney Smoothing, which up to now has been considered the state-of-the-art method for language modelling over the last 15 years.", "startOffset": 17, "endOffset": 20}, {"referenceID": 17, "context": "This concept of introducing gaps in n-grams is referred to as skip n-grams [18, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "This concept of introducing gaps in n-grams is referred to as skip n-grams [18, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 8, "context": "Among other techniques, skip n-grams have also been considered as an approach to overcome problems of data sparsity [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "The Good Turing estimator [7], deleted interpolation [13], Katz backoff [14] and Kneser-Ney smoothing [15] are just some of the approaches to be mentioned.", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "The Good Turing estimator [7], deleted interpolation [13], Katz backoff [14] and Kneser-Ney smoothing [15] are just some of the approaches to be mentioned.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "The Good Turing estimator [7], deleted interpolation [13], Katz backoff [14] and Kneser-Ney smoothing [15] are just some of the approaches to be mentioned.", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "The Good Turing estimator [7], deleted interpolation [13], Katz backoff [14] and Kneser-Ney smoothing [15] are just some of the approaches to be mentioned.", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "The state of the art is a modified version of Kneser-Ney smoothing introduced in [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "Smoothing techniques which do not rely on using lower order models involve clustering [3, 18], i.", "startOffset": 86, "endOffset": 93}, {"referenceID": 17, "context": "Smoothing techniques which do not rely on using lower order models involve clustering [3, 18], i.", "startOffset": 86, "endOffset": 93}, {"referenceID": 17, "context": "grouping together similar words to form classes of words, as well as skip n-grams [18, 12].", "startOffset": 82, "endOffset": 90}, {"referenceID": 11, "context": "grouping together similar words to form classes of words, as well as skip n-grams [18, 12].", "startOffset": 82, "endOffset": 90}, {"referenceID": 19, "context": "Yet other approaches make use of permutations of the word order in n-grams [20, 9].", "startOffset": 75, "endOffset": 82}, {"referenceID": 8, "context": "Yet other approaches make use of permutations of the word order in n-grams [20, 9].", "startOffset": 75, "endOffset": 82}, {"referenceID": 8, "context": "However, with their restriction on a subsequence of words, skip n-grams are also used as a technique to overcome data sparsity [9].", "startOffset": 127, "endOffset": 130}, {"referenceID": 8, "context": "fixed patterns [9] or functional words [6]).", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "fixed patterns [9] or functional words [6]).", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "The impact of various extensions and smoothing techniques for language models is investigated in [9, 8].", "startOffset": 97, "endOffset": 103}, {"referenceID": 7, "context": "The impact of various extensions and smoothing techniques for language models is investigated in [9, 8].", "startOffset": 97, "endOffset": 103}, {"referenceID": 9, "context": "In [10], the authors investigated the increase of observed word combinations when including skips in n-grams.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Review of Modified Kneser-Ney Smoothing We briefly recall modified Kneser-Ney Smoothing as presented in [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "As lowest order model we use\u2014just as done for traditional modified Kneser-Ney [5]\u2014a unigram model interpolated with a uniform distribution for unseen words.", "startOffset": 78, "endOffset": 81}, {"referenceID": 20, "context": "Special purpose domain data are provided by the multi-lingual JRC-Acquis corpus of legislative texts (JRC) [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "Evaluation Metrics As evaluation metric we use perplexity: a standard measure in the field of language models [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "Regarding the scalability of the approach to very large data sets we intend to apply the Map Reduce techniques from [11] to our generalized language models in order to have a more scalable calculation.", "startOffset": 116, "endOffset": 120}, {"referenceID": 8, "context": "Goodman [9] observed that increasing the length of n-grams in combination with modified Kneser-Ney smoothing did not lead to improvements for values of n beyond 7.", "startOffset": 8, "endOffset": 11}], "year": 2014, "abstractText": "The Copyright of this work is owned by the Association for Computational Linguistics (ACL). However, each of the authors and the employers for whom the work was performed reserve all other rights, specifically including the following: ... (4) The right to make copies of the work for internal distribution within the author\u2019s organization and for external distribution as a preprint, reprint, technical report, or related class of document. The original paper is to appear in: ACL 2014: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics We introduce a novel approach for building language models based on a systematic, recursive exploration of skip n-gram models which are interpolated using modified KneserNey smoothing. Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case. In this paper we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language models using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity.", "creator": "LaTeX with hyperref package"}}}