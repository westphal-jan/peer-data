{"id": "1404.3862", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2014", "title": "Optimizing the CVaR via Sampling", "abstract": "Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains such as finance. In this work we present a new formula for the gradient of the CVaR in the form of a conditional expectation. Our result is similar to policy gradients in the reinforcement learning literature. Based on this formula, we propose novel sampling-based estimators for the CVaR gradient, and a corresponding gradient descent procedure for CVaR optimization. We evaluate our approach in learning a risk-sensitive controller for the game of Tetris, and propose an importance sampling procedure that is suitable for such domains.", "histories": [["v1", "Tue, 15 Apr 2014 10:32:05 GMT  (79kb,D)", "https://arxiv.org/abs/1404.3862v1", null], ["v2", "Sun, 29 Jun 2014 15:35:36 GMT  (91kb,D)", "http://arxiv.org/abs/1404.3862v2", null], ["v3", "Tue, 16 Sep 2014 15:32:48 GMT  (97kb,D)", "http://arxiv.org/abs/1404.3862v3", null], ["v4", "Sat, 22 Nov 2014 14:44:54 GMT  (115kb,D)", "http://arxiv.org/abs/1404.3862v4", "To appear in AAAI 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["aviv tamar", "yonatan glassner", "shie mannor"], "accepted": true, "id": "1404.3862"}, "pdf": {"name": "1404.3862.pdf", "metadata": {"source": "CRF", "title": "Optimizing the CVaR via Sampling", "authors": ["Aviv Tamar", "Yonatan Glassner", "Shie Mannor"], "emails": ["yglasner}@tx.technion.ac.il,", "shie@ee.technion.ac.il"], "sections": [{"heading": null, "text": "1 Introduction Conditional Value at Risk (CVaR; Rockafellar and Uryasev, 2000) is a commonly used risk metric that has found extensive use in the financial world among other fields. For a random payout R, the distribution of which is understood by a controllable parameter, the \u03b1-CVaR method is defined as the expected payout over the alpha% worst results of Z. (If the payout of the structure R = fordable is (X), where the payout is a deterrent function, and X is random but not dependent on the results of our work. CVaR optimization aims to find a parameter that is maximized. (in principle) If the payout of the structure R = fordable, where the payout is a deterrent function but does not depend on regulation, we can formulate and resolve CVaR optimization as a stochastic program using different approaches (Rockelev and Uryasev; 2013 and Hong Kong Liasu; a general investment strategy)."}, {"heading": "2.1 CVaR Gradient of a 1-Dimensional Variable", "text": "Let us look again at a random variable Z, but leave its probability density function (P.D.F.) for all; fZ (z; \u03b8) is parameterized by a vector equal. \u2212 We leave the formula equal (Z; \u03b8) and equal (Z; equal) the VaR and CVaR of Z (as defined in Eq. (1) and (2) if the parameter is equal. \u2212 We are interested in the sensitivity of the formula CVaR to the parameter vector, as expressed by the gradient. (Z; even in the simplest cases, the calculation of the gradient is not analytically feasible. \u2212 Therefore, we derive a formula in which this formula is expressed as a conditional expectation. \u2212 We express it as a conditional expectation and use it to calculate the gradient."}, {"heading": "2.2 CVaR Gradient Formula \u2013 General Case", "text": "Let X = (X1, X2,. Xn) be a n \u2212 dimensional random variable with finite support (\u2212 b] n, and let Y denote a discrete random variable that holds values in any formula. Let fY (y; \u03b8) be the probability mass function of Y, and let fX (x) give the probability density function of X Y. Let the reward function r be a limited mapping from [\u2212 b] n \u00d7 Y to R, and consider the random variable R r (X, Y). We are interested in a formula for this formula."}, {"heading": "1: Given:", "text": "\u2022 CVaR level \u03b1 \u2022 A reward function r (x, y): Rn \u00b7 Y \u2192 R \u2022 derivative \u2202% of the probability mass function fY (y; \u03b8) and probability density function fX \u00b2 Y (x, y; \u03b8) \u2022 An i.i.d. sequence x1, y1,.., xN, yN \u0445 fX, Y (x, y; \u03b8). 2: Set rs1,.., r s N = Sort (r (x1, y1),., r (xN, yN)) 3: Set v \u00b2 = rs \u00b2, yN \u00b2 4: For j = 1,.., k do \u00b2 j; N = 1\u03b1NN = Sort (r (yi, y1),., r (xN, yN)."}, {"heading": "Variance Reduction by Importance Sampling", "text": "For very low quantities, i.e. \u03b1 close to 0, the GCVaR estimator would actually suffer from high variance, since averaging is effectively only done via 3 samples. This is a well-known problem in sampling-based approaches to VaR and CVaR estimation, and is often mitigated by using variance reduction techniques such as Importance Sampling (IS; Rubinstein and Kroese, 2011; Bardou, Frikha, and Page, 2009). In IS, the variance of an MC estimator is reduced by using sample readings from another sampling distribution, and appropriately modified to keep the estimator impartial. It is easy to integrate IS into LR gradient estimators in general and to our GCVaR estimator in particular. Due to space limitations, and since this is fairly standard textbook material (e.g. Rubinstein and Kroese, we provide full technical details in 2011)."}, {"heading": "5.1 Experimental Results", "text": "We examine Tetris as a test case for our algorithms. Tetris is a popular RL benchmark that has been extensively studied, but the biggest challenge in Tetris is its large government space, which requires some form of approach to the solution technology. However, many approaches to learning control for Tetris are described in the literature, including approximate repetitions (Tsitsiklis and Van Roy 1996), political gradients (Kakade 2001; Furmston and Barber 2012), and modified political iteration (Gabillon, Ghavamzadeh, and Scherrer 2013).The default performance metric in Tetris is the expected number of clarified lines in the game. Here, we are interested in a risk averse performance measurement captured by the CVaR. Our goal in this section is to compare the performance of a policy optimized for the CVaR."}, {"heading": "E.1 Background", "text": "Consider the following general problem: Let us estimate the expectation l = E (H) (X) (G) (G), where X is a random variable with P.D.F. f (X), and H (X) is some function. The MC solution is given by l = 1N + H (X), where xi (F) is drawn. The IS method aims to reduce the variance of the MC estimator by applying a different random distribution to the samples. Let us suppose that we are given a random distribution g (x), and that g dominates in the sense that g (x) = 0 \u21d2 f (x) = 0. Let us let Ef and Eg denote expectations w.r.t. We observe that l = Ef [H) = Eg [H) f (X) f (X) g (X) g (X) g (X) g) g (0) g), and we define the IS estimators."}, {"heading": "1: Given:", "text": "\u2022 CVaR level \u03b1 \u2022 A reward function r (x, y): Rn Y \u2192 R \u2192 R \u2022 A density function fX, Y (x, y; \u03b8) \u2022 A density function gX, Y (x, y; \u03b8) \u2022 A sequence x1, y1,.., xN, yN \u0445 gX, Y, i.i.d. 2: Set xs1, y s 1.., x s N, y s s N = Sort (x1, y1,.., xN, yN) by r (x, y) 3: For i = 1,.., N doL (i) = i \u2211 j = 1fX, Y (x s j, y s j) / gX, Y (x s j, y s j; \u03b8) 4: Set l = arg miniL (i) anymore IS 5: Set v \u00b2 IS = r (xsl, y s l) 6: For j = 1,."}, {"heading": "E.3 CVaR Policy Gradient with Importance Sampling", "text": "In this section, we apply the IS estimator to the RL transition chances. However, we point out that this method actually requires access to a simulator of this modified MDP distribution. In many applications, a simulator of the original system is available anyway, so there should not be a problem. Consider the RL transition chances of Section 5 and consider the original MDP distribution of M. The P.D.F. of an orbit {X, Y} from the MDP M where it is defined."}, {"heading": "E.4 Empirical Results with Importance Sampling", "text": "Figure 2 shows the importance of the IS in optimizing the CVaR when \u03b1 is small. We chose \u03b1 = 0.01 and N = 200 and compared the naive GCVaR with IS GCVaR. As an approximation of our value function, we used the fact that the soft-max policy uses \u03c6 (s, a) \u03b8 as a kind of state-action-value function, and therefore set V (s) = maxa \u03c6 (s, a) \u03b8. We chose \u03c9 with trajectories from the original policy \u03b80. We observe that IS converges GCVaR significantly faster than GCVaR due to the smaller variance in the gradient estimation."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the gradient of the CVaR, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risksensitive controller for the game of Tetris.", "creator": "LaTeX with hyperref package"}}}