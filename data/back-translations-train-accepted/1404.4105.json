{"id": "1404.4105", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2014", "title": "Sparse Compositional Metric Learning", "abstract": "We propose a new approach for metric learning by framing it as learning a sparse combination of locally discriminative metrics that are inexpensive to generate from the training data. This flexible framework allows us to naturally derive formulations for global, multi-task and local metric learning. The resulting algorithms have several advantages over existing methods in the literature: a much smaller number of parameters to be estimated and a principled way to generalize learned metrics to new testing data points. To analyze the approach theoretically, we derive a generalization bound that justifies the sparse combination. Empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. The results are consistent with our theoretical findings and demonstrate the superiority of our approach in terms of classification performance and scalability.", "histories": [["v1", "Tue, 15 Apr 2014 22:55:53 GMT  (209kb,D)", "http://arxiv.org/abs/1404.4105v1", "18 pages. To be published in Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI 2014)"]], "COMMENTS": "18 pages. To be published in Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI 2014)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yuan shi", "aur\u00e9lien bellet", "fei sha"], "accepted": true, "id": "1404.4105"}, "pdf": {"name": "1404.4105.pdf", "metadata": {"source": "CRF", "title": "Sparse Compositional Metric Learning\u2217", "authors": ["Yuan Shi", "Aur\u00e9lien Bellet", "Fei Sha"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The need to measure distance or similarity between data instances is ubiquitous in machine learning and many application areas. However, each problem has its own underlying semantic space for defining distances that include standard metrics (e.g. Euclidean distance), which has led to a growing interest in metric learning processes for the past few years, as summarized in two recent surveys (Bellet et al., 2004; Davis et al., 2008; Weinberger and Saul, 2009; Shen et al, 2012; Ying and Li, 2012). This is equivalent to learning a linear projection of data into a trait that sets the boundaries of learning."}, {"heading": "2 Proposed Approach", "text": "In this section we present the basic idea of sparse compositional metric learning (SCML) and show how it can be used to unify several existing metric learning paradigms and lead to efficient new formulations."}, {"heading": "2.1 Main Idea", "text": "We assume that the data are in RD and focus on learning (squared) Mahalanobis distances dM (x, x \u2032) = (x \u2212 x \u2032) TM (x \u2212 x \u2032) parameterized by a positive semidefinitive (PSD) D \u00b7 D matrix M. Note that M can be represented as a non-negative weighted sum of K rank-1 PSD matrices: 1M = K \u2211 i = 1 wibib T i, with w \u2265 0, (1) where the bi's are D-dimensional column vectors. In this paper, we use the form (1) metric learning as a sparse combination of base elements taken from a base set B = {bi} Ki = 1. The key to our framework is the fact that such a B section is readily available for the algorithm and consists of rank-one metrics that are locally discriminatory. Such base elements we will easily combine the Fisher training space with the individual metric data (we can easily separate the Fisher training space)."}, {"heading": "2.1.1 Global Metric Learning", "text": "In global metric learning, an attempt is made to learn a single metric dw (x, x \u2032) from a set of distance constraints on the training data. We use a set of treble constraints C, each (xi, xj, xk).C suggesting that the distance between xi and xj should be smaller than the distance between xi and xk. C can be constructed from label information, as in LMNN (Weinberger and Saul, 2009), or in an unsupervised manner, such as based on the feedback of implicit users (such as clicks on search engine results).Our formulation for global1Such an expression exists for each PSD matrix M, since the eigenvalue calculation of M is in the form (1).Metric learning, SCML-Global, simply consists of combining the local base elements into a higher-level global metric that meets the constraints in C well: wk, wk, wk, xk = 1, xk = 1, CLj, CLj = 1, CLk = xk (j)."}, {"heading": "2.1.2 Multi-Task Metric Learning", "text": "It is a paradigm for learning several tasks simultaneously, using their commonalities. (Argyriou et al., 2008; Yang et al., 2009; Gong et al., 2012) In general, it is unclear how this is to be achieved in metric learning. In contrast, learning metrics as sparse combinations allows a direct translation of this idea into metric learning with multiple tasks. (Argyriou et al., 2008; Yang et al., 2009; Gong et al., 2012). In contrast, learning metrics as sparse combinations allows a direct translation of this idea into metric learning with multiple tasks. Formally, we get different but somehow related tasks with related constraints setsC1..., CT and we aim at learning a metric dwt (x, x \u2032) for each task, while sharing information across tasks xxxxx. Below, the basic set B is the union of principles B1."}, {"heading": "2.1.3 Local Metric Learning", "text": "It is about the limitations of global methods in the acquisition of complex data patterns (Frome et al., 2007; Weinberger et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012). For heterogeneous data that we are able to capture the semantic distance, it is much better. On the other hand, local metric learning is costly and often suffers from severe overadjustment, as the number of parameters that can be learned can be very large. Below, we show how our framework can be used to derive an efficient local metric learning method. We aim to learn a metric learning method (x) that has a smooth function that matches each instance x to its metric matrix (Ramanan and Baker, 2011; Hauberg et al., 2012). The distance between two points should then be defined as geodesic distance on a multiple level."}, {"heading": "2.2 Optimization", "text": "In our experiments, we use the kernel PCA (Scho \ufffd lkopf et al., 1998), because it offers a simple way to limit the dimension and thus the number of parameters to be learned. We use the RBF kernel with bandwidth adjusted to the mean Euclidean distance in the data. 2009; Xiao, 2010), which switches between a stochastic subgradient step on the hinged loss concept and a proximal operator (for '1 or' 2.1 standard), which explicitly induces the sparseness. We solve SCML-Global and mt-SCML with the Regularized Dual Averaging (Xiao, 2010), which provides a rapid convergence and a degree of thrift in the solution, comparable to batch algorithms. For SCML local problems, we solve the problems with the help of Dual Averaging (Xiao, 2010), which offers a significant convergence and a degree of austerity comparable to the algorithms in the batch."}, {"heading": "3 Theoretical Analysis", "text": "In this section, we offer a theoretical analysis of our approach in the form of a generalization based on algorithmic robustness analysis (Xu and Mannor, 2012) and its adaptation to metric learning (Bellet and Habrard, 2012). For convenience, we focus on SCML-Global, our global metric learning formula in (2).Consider the learning situation in which we have a labeled learning sample S = (xi, yi)} ni = 1 drawn i.e. from some unknown distribution P over Z = X \u00b7 Y. We call a triplet (z, z \u2032, z \u2032) that is permissible if y \u2032 6 = y \u2032 \u2032. \"Let C be the set of permissible triplets built from S and L (w, z \u2032 \u2032)."}, {"heading": "4 Related Work", "text": "In this section we review relevant work in global, multi-task and local metric learning processes. The interested reader should refer to the recent surveys of Kulis (2012) and Bellet et al. (2013) in order to be more detailed. Global Methods Most global metric learning methods learn the Matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger et al., 2009) Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods (Shen, 2012) Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods Methods-BoostML (Shen et al, 2012) (Shen et al, 2012), 2012), using rank-one-matrices as weak learners used to learn a global Metrix-distance to learn a global metrix-metrix-method-method-metrix-distance via a boosting-task-task-task-task-task-task-task-task-task-to learn-task-task-multi-task-task-task-task-task-multi-task-task-task-task-task-methods-task-task-task-"}, {"heading": "5 Experiments", "text": "In this section, we compare our methods with state-of-the-art algorithms for global, multitaske, and local metric learning. [4] In all experiments, we use a classifier with 3 closest neighbors. To generate a set of locally discriminatory rank-one metrics, we first divide the data into regions using clusters. For each regional center, we select closest neighbors from each class J (for J = {10, 20, 50} to take into account different scales), and apply Fisher discrimination analysis followed by self-evaluation to maintain the basics.5 Section 5.1 presents results for global metric learning, Section 5.2 for multitasking, and Section 5.3 for local metric learning."}, {"heading": "5.1 Global Metric Learning", "text": "We use 6 datasets from UCI6 and BBC7 (see Table 1). The dimensionality of USPS and BBC is reduced to 100 and 200 to speed up the calculation. We normalize the data as in (Wang et al., 2012) and divide it into train / validation / test (60% / 20% / 20%), with the exception of letters and USPS, where we use 3,000 / 1,000 / 1,000."}, {"heading": "5.1.1 Proof of Concept", "text": "Setup Global metric learning is a convenient setting to study the effect of combining base elements. To this end, we consider a formulation with the same loss function as SCML-Global, which, however, learns the metric matrix directly by using the Frobenius standard regularization to reduce overmatch. We call it Global-Frob. Both algorithms use the same training triplets generated for each instance by identifying 3 target neighbors (closest neighbors with the same label) and 10 stackers (closest neighbors with different labels). We match the regulation parameters to the validation data. For SCML-Global, we use a base set of 400 elements for Vehicle, Vowel, Segment, and BBC, and 1,000 elements for Letters and USPS.4For all comparable methods, we use the MATLAB code from the authors \"website. The MATLAB code for our methods is available at http: / wwbcfu / us.us."}, {"heading": "5.1.2 Comparison to Other Global Algorithms", "text": "Setup We compare SCML-Global with two state-of-the-art global metric learning algorithms: Large Margin Nearest Neighbor (LMNN, Weinberger and Saul, 2009) and BoostML (Shen et al., 2012). The datasets, preprocessing and setting for SCML-Global are the same as in Section 5.1.1. LMNN uses 3 target neighbors and all scammers, while these are set to 3 and 10 for BoostML, respectively (as in SCML-Global). Table 3 shows the average misclassification rate, along with standard errors and the average rank of each method across all datasets. SCML-Global clearly outperforms LMNN and BoostML and ranks first among 5 out of 6 datasets and achieves the highest overall ranking. Furthermore, its training time is shorter than other methods, especially for high-dimensional data."}, {"heading": "5.2 Multi-task Metric Learning", "text": "Dataset Sentiment Analysis (Blitzer et al., 2007) is a popular multi-task learning dataset consisting of Amazon ratings on four product types: kitchen appliances, DVDs, books and electronics. Each product type is treated as a task and has 1,000 positive and 1,000 negative ratings. To reduce computing costs, we represent each check by a 200-dimensional feature vector by selecting the largest mutual information with the labels. We randomly divide the datasets into training (800 samples) and testing (400 samples), comparing the following metrics: st-Euc (Euclidean distance), st-LMNN and st-SCMNL."}, {"heading": "5.3 Local Metric Learning", "text": "We compare SCMLLocal to MM-LMNN (Weinberger and Saul, 2009), GLML (Noh et al., 2010) and PLML (Wang et al., 2012). The number of anchor points in PLML is set to 20, as the authors have done. MMLMNN use 3 target neighbors and all scammers, while these are set to 3 and 10 in PLML Local. The number of anchor points in PLML is set to 20, as is done by the authors. MLMNN use 3 target neighbors and all scammers."}, {"heading": "6 Conclusion", "text": "This framework combines several paradigms of metric learning, including global, local and multi-task learning. Of particular interest is our local metric learning algorithm, which can principally calculate instance-specific metrics for both training and test points. Theoretically, the soundness of our approach is supported by a generalization, and we have demonstrated in experimental studies that the proposed methods improve state-of-the-art algorithms in terms of accuracy and scalabilty. Recognition This research is partially supported by IARPA through DoD / ARL Treaty # W911NF12-C-0012 and DARPA through Treaty # D11AP00278. The U.S. government is authorized to reproduce and disseminate reprints for government purposes, notwithstanding the copyright notes and conclusions contained therein. The views and conclusions contained therein are those of the authors and should not be interpreted to constitute the DARPA, DARPA or ARD's official or DARL guidelines."}, {"heading": "Appendix A Detailed Analysis", "text": "In this section we will give the details of deriving the generalization boundaries for the global and multifunctional learning formulations shown in Section 3.A.1. We assume that the number of permissible triplets formed from the instances in page 88 can only consist of a subset of all permissible triplets (which is often the case in practice).We assume that the number of permissible triplets from the subset of all permissible triplets (which is often used in practice), a relaxed version of the robustness property, can be used to derive similar results (Bellet and Habrard, 2012)."}, {"heading": "Appendix B Experimental Comparison with Support Vector Machines", "text": "In this section, we compare SCML-Global and SCML-Local to Support Vector Machines (SVM) with a linear and an RBF kernel. We used the software LIBSVM (Chang and Lin, 2011) and adjusted the parameter C and the bandwidth for the RBF kernel on the validation set. Table 7 shows misclassification rates averaging over 20 random splits, as well as default errors and the average rank of each method across all datasets. First, we can see that SCML-Global consistently performs better than linear SVM. Second, SCMLLocal is competitive with kernel SVM. These results show that a simple k-next neighbor strategy can be competitive (and even perform better) with a good metric than SVM classifiers."}], "references": [{"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Mach. Learn.,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Robustness and Generalization for Metric Learning", "author": ["A. Bellet", "A. Habrard"], "venue": "Technical report,", "citeRegEx": "Bellet and Habrard.,? \\Q2012\\E", "shortCiteRegEx": "Bellet and Habrard.", "year": 2012}, {"title": "A Survey on Metric Learning for Feature Vectors and Structured Data", "author": ["Aur\u00e9lien Bellet", "Amaury Habrard", "Marc Sebban"], "venue": "Technical report,", "citeRegEx": "Bellet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2013}, {"title": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In ACL,", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "Mach. Learn.,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "LIBSVM : a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Information-theoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Efficient Online and Batch Learning Using Forward Backward Splitting", "author": ["J. Duchi", "Y. Singer"], "venue": null, "citeRegEx": "Duchi and Singer.,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Learning Globally-Consistent Local Distance Functions for Shape-Based Image Retrieval and Classification", "author": ["A. Frome", "Y. Singer", "F. Sha", "J. Malik"], "venue": "In ICCV,", "citeRegEx": "Frome et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2007}, {"title": "Neighbourhood Components Analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Robust multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "In KDD,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "A Geometric take on Metric Learning", "author": ["S. Hauberg", "O. Freifeld", "M. Black"], "venue": "In NIPS,", "citeRegEx": "Hauberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hauberg et al\\.", "year": 2012}, {"title": "Learning a mixture of sparse distance metrics for classification and dimensionality reduction", "author": ["Y. Hong", "Q. Li", "J. Jiang", "Z. Tu"], "venue": "In CVPR,", "citeRegEx": "Hong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2011}, {"title": "Online Metric Learning and Fast Similarity Search", "author": ["P. Jain", "B. Kulis", "I. Dhillon", "K. Grauman"], "venue": "In NIPS,", "citeRegEx": "Jain et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2008}, {"title": "entropy and -capacity of sets in functional spaces", "author": ["A. Kolmogorov", "V. Tikhomirov"], "venue": "American Mathematical Society Translations,", "citeRegEx": "Kolmogorov and Tikhomirov.,? \\Q1961\\E", "shortCiteRegEx": "Kolmogorov and Tikhomirov.", "year": 1961}, {"title": "Metric Learning: A Survey", "author": ["Brian Kulis"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulis.,? \\Q2012\\E", "shortCiteRegEx": "Kulis.", "year": 2012}, {"title": "Generative Local Metric Learning for Nearest Neighbor Classification", "author": ["Y.-K. Noh", "B.-T. Zhang", "D. Lee"], "venue": "In NIPS,", "citeRegEx": "Noh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2010}, {"title": "Large Margin Multi-Task Metric Learning", "author": ["S. Parameswaran", "K. Weinberger"], "venue": "In NIPS,", "citeRegEx": "Parameswaran and Weinberger.,? \\Q2010\\E", "shortCiteRegEx": "Parameswaran and Weinberger.", "year": 2010}, {"title": "Local Distance Functions: A Taxonomy, New Algorithms, and an Evaluation", "author": ["D. Ramanan", "S. Baker"], "venue": "TPAMI, 33(4):794\u2013806,", "citeRegEx": "Ramanan and Baker.,? \\Q2011\\E", "shortCiteRegEx": "Ramanan and Baker.", "year": 2011}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Neural Comput.,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Positive Semidefinite Metric Learning", "author": ["C. Shen", "J. Kim", "L. Wang", "A. van den Hengel"], "venue": "Using Boostinglike Algorithms. JMLR,", "citeRegEx": "Shen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2012}, {"title": "Sparse Compositional Metric Learning", "author": ["Y. Shi", "A. Bellet", "F. Sha"], "venue": "In AAAI,", "citeRegEx": "Shi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2014}, {"title": "Visualizing Data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "JMLR, 9:2579\u20132605,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Parametric Local Metric Learning for Nearest Neighbor Classification", "author": ["J. Wang", "A. Woznica", "A. Kalousis"], "venue": "In NIPS,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Distance Metric Learning for Large Margin", "author": ["K. Weinberger", "L. Saul"], "venue": "Nearest Neighbor Classification. JMLR,", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization", "author": ["L. Xiao"], "venue": "JMLR, 11:2543\u20132596,", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}, {"title": "Distance Metric Learning with Application to Clustering with Side-Information", "author": ["E. Xing", "A. Ng", "M. Jordan", "S. Russell"], "venue": "In NIPS,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Robustness and Generalization", "author": ["H. Xu", "S. Mannor"], "venue": "Mach. Learn.,", "citeRegEx": "Xu and Mannor.,? \\Q2012\\E", "shortCiteRegEx": "Xu and Mannor.", "year": 2012}, {"title": "Heterogeneous multitask learning with joint sparsity constraints", "author": ["X. Yang", "S. Kim", "E. Xing"], "venue": "In NIPS,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Distance Metric Learning with Eigenvalue Optimization", "author": ["Y. Ying", "P. Li"], "venue": "JMLR, 13:1\u201326,", "citeRegEx": "Ying and Li.,? \\Q2012\\E", "shortCiteRegEx": "Ying and Li.", "year": 2012}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Yuan and Lin.,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2006}, {"title": "Learning instance specific distances using metric propagation", "author": ["D.-C. Zhan", "M. Li", "Y.-F. Li", "Z.-H. Zhou"], "venue": "In ICML,", "citeRegEx": "Zhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhan et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "This has led to a growing interest in metric learning for the past few years, as summarized in two recent surveys (Bellet et al., 2013; Kulis, 2012).", "startOffset": 114, "endOffset": 148}, {"referenceID": 15, "context": "This has led to a growing interest in metric learning for the past few years, as summarized in two recent surveys (Bellet et al., 2013; Kulis, 2012).", "startOffset": 114, "endOffset": 148}, {"referenceID": 26, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 9, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 6, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 13, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 24, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 20, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 29, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 8, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 24, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 31, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 12, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 23, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 16, "context": ", 2012), to the extreme of learning one metric per training instance (Noh et al., 2010).", "startOffset": 69, "endOffset": 87}, {"referenceID": 18, "context": "This line of research is motivated by the fact that locally, simple linear metrics perform well (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 96, "endOffset": 143}, {"referenceID": 11, "context": "This line of research is motivated by the fact that locally, simple linear metrics perform well (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 96, "endOffset": 143}, {"referenceID": 21, "context": "Furthermore, \u2217This document is an extended version of a conference paper (Shi et al., 2014) that provides additional details and results.", "startOffset": 73, "endOffset": 91}, {"referenceID": 4, "context": "The proposed framework also applies to multi-task metric learning, where one wants to learn a global metric for several related tasks while exploiting commonalities between them (Caruana, 1997; Parameswaran and Weinberger, 2010).", "startOffset": 178, "endOffset": 228}, {"referenceID": 17, "context": "The proposed framework also applies to multi-task metric learning, where one wants to learn a global metric for several related tasks while exploiting commonalities between them (Caruana, 1997; Parameswaran and Weinberger, 2010).", "startOffset": 178, "endOffset": 228}, {"referenceID": 18, "context": "This can be seen as learning a smoothly varying metric tensor over the feature space (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 85, "endOffset": 132}, {"referenceID": 11, "context": "This can be seen as learning a smoothly varying metric tensor over the feature space (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 85, "endOffset": 132}, {"referenceID": 7, "context": "All formulations can be solved using scalable optimization procedures based on stochastic subgradient descent with proximal operators (Duchi and Singer, 2009; Xiao, 2010).", "startOffset": 134, "endOffset": 170}, {"referenceID": 25, "context": "All formulations can be solved using scalable optimization procedures based on stochastic subgradient descent with proximal operators (Duchi and Singer, 2009; Xiao, 2010).", "startOffset": 134, "endOffset": 170}, {"referenceID": 24, "context": "C may be constructed from label information, as in LMNN (Weinberger and Saul, 2009), or in an unsupervised manner based for instance on implicit users\u2019 feedback (such as clicks on search engine results).", "startOffset": 56, "endOffset": 83}, {"referenceID": 4, "context": "2 Multi-Task Metric Learning Multi-task learning (Caruana, 1997) is a paradigm for learning several tasks simultaneously, exploiting their commonalities.", "startOffset": 49, "endOffset": 64}, {"referenceID": 0, "context": "Recently, multi-task learning methods have successfully built on the assumption that the tasks should share a common low-dimensional representation (Argyriou et al., 2008; Yang et al., 2009; Gong et al., 2012).", "startOffset": 148, "endOffset": 209}, {"referenceID": 28, "context": "Recently, multi-task learning methods have successfully built on the assumption that the tasks should share a common low-dimensional representation (Argyriou et al., 2008; Yang et al., 2009; Gong et al., 2012).", "startOffset": 148, "endOffset": 209}, {"referenceID": 10, "context": "Recently, multi-task learning methods have successfully built on the assumption that the tasks should share a common low-dimensional representation (Argyriou et al., 2008; Yang et al., 2009; Gong et al., 2012).", "startOffset": 148, "endOffset": 209}, {"referenceID": 30, "context": "where W is a T \u00d7 K nonnegative matrix whose t-th row is the weight vector wt defining the metric for task t, Lwt(xi,xj ,xk) = [1 + dwt(xi,xj)\u2212 dwt(xi,xk)]+ and \u2016W \u20162,1 is the `2/`1 mixed norm used in the group lasso problem (Yuan and Lin, 2006).", "startOffset": 224, "endOffset": 244}, {"referenceID": 8, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 24, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 31, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 16, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 12, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 23, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 18, "context": "We aim at learning a metric tensor T (x), which is a smooth function that (informally) maps any instance x to its metric matrix (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 128, "endOffset": 175}, {"referenceID": 11, "context": "We aim at learning a metric tensor T (x), which is a smooth function that (informally) maps any instance x to its metric matrix (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 128, "endOffset": 175}, {"referenceID": 31, "context": "However, this requires solving an intractable problem, so we use the widely-adopted simplification that distances from point x are computed based on its own metric alone (Zhan et al., 2009; Noh et al., 2010; Wang et al., 2012): dT (x,x \u2032) = (x\u2212 x\u2032)TT (x)(x\u2212 x\u2032) = (x\u2212 x\u2032)T K \u2211", "startOffset": 170, "endOffset": 226}, {"referenceID": 16, "context": "However, this requires solving an intractable problem, so we use the widely-adopted simplification that distances from point x are computed based on its own metric alone (Zhan et al., 2009; Noh et al., 2010; Wang et al., 2012): dT (x,x \u2032) = (x\u2212 x\u2032)TT (x)(x\u2212 x\u2032) = (x\u2212 x\u2032)T K \u2211", "startOffset": 170, "endOffset": 226}, {"referenceID": 23, "context": "However, this requires solving an intractable problem, so we use the widely-adopted simplification that distances from point x are computed based on its own metric alone (Zhan et al., 2009; Noh et al., 2010; Wang et al., 2012): dT (x,x \u2032) = (x\u2212 x\u2032)TT (x)(x\u2212 x\u2032) = (x\u2212 x\u2032)T K \u2211", "startOffset": 170, "endOffset": 226}, {"referenceID": 19, "context": "We can solve them efficiently using stochastic composite optimization (Duchi and Singer, In our experiments, we use kernel PCA (Sch\u00f6lkopf et al., 1998) as it provides a simple way to limit the dimension and thus the number of parameters to learn.", "startOffset": 127, "endOffset": 151}, {"referenceID": 25, "context": "We solve SCML-Global and mt-SCML using Regularized Dual Averaging (Xiao, 2010), which offers fast convergence and levels of sparsity in the solution comparable to batch algorithms.", "startOffset": 66, "endOffset": 78}, {"referenceID": 7, "context": "For SCML-Local, due to local minima, we ensure improvement over the optimal solution w\u2217 of SCML-Global by using a forward-backward algorithm (Duchi and Singer, 2009) which is initialized with A = 0 and ci = \u221a w\u2217 i .", "startOffset": 141, "endOffset": 165}, {"referenceID": 27, "context": "In this section, we provide a theoretical analysis of our approach in the form of a generalization bound based on algorithmic robustness analysis (Xu and Mannor, 2012) and its adaptation to metric learning (Bellet and Habrard, 2012).", "startOffset": 146, "endOffset": 167}, {"referenceID": 1, "context": "In this section, we provide a theoretical analysis of our approach in the form of a generalization bound based on algorithmic robustness analysis (Xu and Mannor, 2012) and its adaptation to metric learning (Bellet and Habrard, 2012).", "startOffset": 206, "endOffset": 232}, {"referenceID": 27, "context": "This is in accordance with other similar learning bounds, for example, the original robustness-based bounds in (Xu and Mannor, 2012).", "startOffset": 111, "endOffset": 132}, {"referenceID": 14, "context": "The interested reader should refer to the recent surveys of Kulis (2012) and Bellet et al.", "startOffset": 60, "endOffset": 73}, {"referenceID": 2, "context": "The interested reader should refer to the recent surveys of Kulis (2012) and Bellet et al. (2013) for more details.", "startOffset": 77, "endOffset": 98}, {"referenceID": 26, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 9, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 6, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 13, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 24, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 20, "context": "An exception is BoostML (Shen et al., 2012) which uses rank-one matrices as weak learners to learn a global Mahalanobis distance via a boosting procedure.", "startOffset": 24, "endOffset": 43}, {"referenceID": 17, "context": "Multi-task methods Multi-task metric learning was proposed in (Parameswaran and Weinberger, 2010) as an extension to the popular LMNN (Weinberger and Saul, 2009).", "startOffset": 62, "endOffset": 97}, {"referenceID": 24, "context": "Multi-task methods Multi-task metric learning was proposed in (Parameswaran and Weinberger, 2010) as an extension to the popular LMNN (Weinberger and Saul, 2009).", "startOffset": 134, "endOffset": 161}, {"referenceID": 24, "context": "Local methods MM-LMNN (Weinberger and Saul, 2009) is an extension of LMNN which learns only a small number of metrics (typically one per class) in an effort to alleviate overfitting.", "startOffset": 22, "endOffset": 49}, {"referenceID": 12, "context": "msNCA (Hong et al., 2011) learns a function that splits the space into a small number of regions and then learns a metric per region using NCA (Goldberger et al.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": ", 2011) learns a function that splits the space into a small number of regions and then learns a metric per region using NCA (Goldberger et al., 2004).", "startOffset": 125, "endOffset": 150}, {"referenceID": 23, "context": "Like SCML-Local, PLML (Wang et al., 2012) is based on a combination of metrics but there are major differences with our work: (i) weights only depend on a manifold assumption: they are not sparse and use no discriminative information, (ii) the basis metrics are full-rank, thus expensive to learn, and (iii) a weight vector is learned explicitly for each training instance, which can result in a large number of parameters and prevents generalization to new instances (in practice, for a test point, they use the weight vector of its nearest neighbor in the training set).", "startOffset": 22, "endOffset": 41}, {"referenceID": 31, "context": "ISD (Zhan et al., 2009) is an attempt to learn the metrics for unlabeled points by propagation, but is limited to the transductive setting.", "startOffset": 4, "endOffset": 23}, {"referenceID": 16, "context": "Unlike the above discriminative approaches, GLML (Noh et al., 2010) learns a metric for each point independently in a generative way by minimizing the 1-NN expected error under some assumption for the class distributions.", "startOffset": 49, "endOffset": 67}, {"referenceID": 9, "context": ", 2011) learns a function that splits the space into a small number of regions and then learns a metric per region using NCA (Goldberger et al., 2004). Again, the metrics are full-rank so msNCA does not scale well with the number of metrics. Like SCML-Local, PLML (Wang et al., 2012) is based on a combination of metrics but there are major differences with our work: (i) weights only depend on a manifold assumption: they are not sparse and use no discriminative information, (ii) the basis metrics are full-rank, thus expensive to learn, and (iii) a weight vector is learned explicitly for each training instance, which can result in a large number of parameters and prevents generalization to new instances (in practice, for a test point, they use the weight vector of its nearest neighbor in the training set). As observed by Ramanan and Baker (2011), the above methods make the implicit assumption that the metric tensor is locally constant (at the class, region or neighborhood level), while SCML-Local learns a smooth function that maps any instance to its specific metric.", "startOffset": 126, "endOffset": 855}, {"referenceID": 23, "context": "We normalize the data as in (Wang et al., 2012) and split into train/validation/test (60%/20%/20%), except for Letters and USPS where we use 3,000/1,000/1,000.", "startOffset": 28, "endOffset": 47}, {"referenceID": 20, "context": "2 Comparison to Other Global Algorithms Setup We now compare SCML-Global to two state-of-the-art global metric learning algorithms: Large Margin Nearest Neighbor (LMNN, Weinberger and Saul, 2009) and BoostML (Shen et al., 2012).", "startOffset": 208, "endOffset": 227}, {"referenceID": 3, "context": "2 Multi-task Metric Learning Dataset Sentiment Analysis (Blitzer et al., 2007) is a popular dataset for multi-task learning that consists of Amazon reviews on four product types: kitchen appliances, DVDs, books and electronics.", "startOffset": 56, "endOffset": 78}, {"referenceID": 17, "context": "Setup We compare the following metrics: st-Euc (Euclidean distance), st-LMNN and st-SCML (singletask LMNN and single-task SCML-Global, trained independently on each task), u-Euc (Euclidean trained on the union of the training data from all tasks), u-LMNN (LMNN on union), u-SCML (SCML-Global on union), multi-task LMNN (Parameswaran and Weinberger, 2010) and finally our own multi-task method mt-SCML.", "startOffset": 319, "endOffset": 354}, {"referenceID": 24, "context": "We compare SCMLLocal to MM-LMNN (Weinberger and Saul, 2009), GLML (Noh et al.", "startOffset": 32, "endOffset": 59}, {"referenceID": 16, "context": "We compare SCMLLocal to MM-LMNN (Weinberger and Saul, 2009), GLML (Noh et al., 2010) and PLML (Wang et al.", "startOffset": 66, "endOffset": 84}, {"referenceID": 23, "context": ", 2010) and PLML (Wang et al., 2012).", "startOffset": 17, "endOffset": 36}], "year": 2014, "abstractText": "We propose a new approach for metric learning by framing it as learning a sparse combination of locally discriminative metrics that are inexpensive to generate from the training data. This flexible framework allows us to naturally derive formulations for global, multi-task and local metric learning. The resulting algorithms have several advantages over existing methods in the literature: a much smaller number of parameters to be estimated and a principled way to generalize learned metrics to new testing data points. To analyze the approach theoretically, we derive a generalization bound that justifies the sparse combination. Empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. The results are consistent with our theoretical findings and demonstrate the superiority of our approach in terms of classification performance and scalability.", "creator": "LaTeX with hyperref package"}}}