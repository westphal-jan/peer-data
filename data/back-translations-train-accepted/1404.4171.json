{"id": "1404.4171", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2014", "title": "Dropout Training for Support Vector Machines", "abstract": "Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. To deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights have closed-form solutions. The similar ideas are applied to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of linear SVMs.", "histories": [["v1", "Wed, 16 Apr 2014 08:54:01 GMT  (55kb,D)", "http://arxiv.org/abs/1404.4171v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ning chen", "jun zhu", "jianfei chen 0001", "bo zhang"], "accepted": true, "id": "1404.4171"}, "pdf": {"name": "1404.4171.pdf", "metadata": {"source": "CRF", "title": "Dropout Training for Support Vector Machines", "authors": ["Ning Chen", "Jun Zhu", "Jianfei Chen", "Bo Zhang"], "emails": ["{ningchen@mail,", "dcszj@mail,", "chenjf10@mails,", "dcszb@mail}.tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "Preliminaries", "text": "We raise the problem in question and examine learning with marginalised, corrupt traits."}, {"heading": "Regularized loss minimization", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x x, x, x, x, x x, x, x, x, x x, x, x x, x, x, x, x x, x, x, x x, x, x, x x, x, x, x, x, x, x x, x, x, x, x, x, x, x, x, x, x x, x, x, x x, x, x, x, x x, x, x, x x, x, x x, x x, x, x, x x, x x, x, x x, x, x x, x x, x x, x, x, x x, x, x, x, x, x x, x, x, x, x x, x, x, x x, x, x, x x, x, x, x, x x, x, x, x, x, x, x, x, x, x x, x, x, x, x, x, x, x, x"}, {"heading": "Learning SVMs with Corrupting Noise", "text": "We now present a simple iterative reweighted least square (IRLS) algorithm for learning SVMs with the expected hinged loss among corrupting distributions. Our method consists of a variable upper limit of the expected loss and a simple algorithm that iteratively minimizes the expectation of a reweighted square loss. We also apply similar ideas to develop a simple IRLS algorithm to minimize the expected logistical loss that allows a systematic comparison of the hinged loss with the logistical and square losses in the context of feature notification."}, {"heading": "A variational bound with data augmentation", "text": "There is a variable upper limit from which we derive an implied wording of the expected loss of sharpness. Let us??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "Iteratively Re-weighted Least Square Algorithm", "text": "In the upper limit, if the variation distribution (\u03bb) is given, the term Ep [2] [3] [4] x [4] x [4] x [5] x [5] x [6] x [6] x [6] x [6] x [6] x [7] x [7] x [7] x [7] x [7] x [7] x [8] x [7] x [8] x [8] x [8] x [8] x [8] x [8] x [8] x x x [8] x x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" \"x\" x \"x\" x \"x\" x \"x\" x \"x\" \"x\" x \"x\" x \"\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"\" x \"x\" x \"\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"\" x \"\" x \"\" x \"x\" \"x\" x \"x\" x \"x\" x \"x\" \"\" x \"\" x \"\" x \"\" \"\" \"\" \"\" x \"\" \"\" \"x\" \"\" \"\" \"\" \"\" x x x x x \"\" \"\" \"x x\" \"\" \"\" \""}, {"heading": "Experiments", "text": "We now present empirical results on both the classification and challenging scenario of the \"test date nightmare\" (Globerson and Roweis 2006) to demonstrate the effectiveness of the dropout training algorithm for SVMs referred to by Dropout-SVM and the new IRLS drop-out training algorithm for logistic loss referred to by Dropout-Logistic. We consider the unbiased drop-out (or blankout) noise model6, i.e. p (x-0 = 0) = q and p (x-0 = 11 \u2212 qx) = 1 \u2212 q, where q [0, 1) is a predetermined level of corruption. The variance of this model for each dimension d is Vp [x-d] = q1 \u2212 qx 2 d."}, {"heading": "Binary classification", "text": "It is indeed the case that we are able to go in search of a solution that is capable, that we have got to grips with."}, {"heading": "Dropout-SVM vs. Explicit corruption", "text": "Figure 2 shows the classification errors on the Amazonbooks dataset when an SVM classifier is trained with the explicit corruption strategy as in Equation (2). We change the number of damaged copies (i.e. M) from 1 to 256. After the previous settings (van der Maaten et al. 2013), for each value of M we select the dropout model with q, which is selected by cross-validation. The hyperparameter of the SVM classifier is also selected by cross-validation of the training data. We can observe a clear trend that the error decreases when the training set contains more corrupt versions of the original training data, i.e. M becomes larger in Equation (2). It also shows that the best performance is achieved when M approaches infinity, which corresponds to our drop-out SVM."}, {"heading": "Multi-class classification", "text": "The CIFAR-10 dataset is a subset of the 80 million tiny images (Torralba, Fergus and Freeman 2008) and consists of 10 classes of 32 x 32 tiny images. We follow the 8http: / / www.cs.toronto.edu / \u0445 kriz / cifar.htmlexperimental setup of the previous work (Krizhevsky 2009; van der Maaten et al. 2013) and present each image as an 8,192 dimensional feature descriptor. We use the same 50,000 images for training and 10,000 for testing. There are different approaches to applying binary dropout SVM and dropout logistics to multiclass classification, including \"one-vs-all.\""}, {"heading": "Nightmare at test time", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "Conclusions", "text": "We present a drop-out training for SVMs using an iteratively reweighted least-square algorithm (IRLS) using data augmentation techniques. Similar ideas are applied to develop a new IRLS algorithm for drop-out training of logistic regression. Our IRLS algorithms provide insights into the context and difference between the different losses in drop-out learning settings. Empirical results on different tasks show the effectiveness of our approaches. It remains open for future work whether the kernel trick can be integrated into drop-out learning. We are also interested in developing more efficient algorithms, such as online drop-out learning, to deal with even larger data sets, and to investigate whether drop-out SVM can be integrated into a deep learning architecture or learning with latent structures (Zhu et al. 2014)."}, {"heading": "Acknowledgments", "text": "This work is supported by the National Key Project for Basic Research of China (grant no.: 2013CB329403, 2012CB316301), the National Natural Science Foundation of China (grant no.: 61305066, 61322308, 61332007), the Tsinghua Self-Innovation Project (grant no.: 20121088071) and the China Postdoctoral Science Foundation (grant no.: 2013T60117, 2012M520281)."}], "references": [{"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["Dredze Blitzer", "J. Pereira 2007] Blitzer", "M. Dredze", "F. Pereira"], "venue": "In Association of Computational Linguistics", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Improving the accuracy and speed of support vector machiens", "author": ["Burges", "C. Scholkopf 1997] Burges", "B. Scholkopf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Burges et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Burges et al\\.", "year": 1997}, {"title": "Generalized relational topic models with data augmentation", "author": ["Chen"], "venue": "In International Joint Conference on Artificial Intelligence", "citeRegEx": "Chen,? \\Q2013\\E", "shortCiteRegEx": "Chen", "year": 2013}, {"title": "Learning to classify with missing and corrpted features", "author": ["Dekel", "O. Shamir 2008] Dekel", "O. Shamir"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Dekel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2008}, {"title": "Nightmare at test time: Robust learning by feature deletion", "author": ["Globerson", "A. Roweis 2006] Globerson", "S. Roweis"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Globerson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2006}, {"title": "Exponentiated gradient algorithms for log-linear structured prediction", "author": ["Globerson"], "venue": null, "citeRegEx": "Globerson,? \\Q2007\\E", "shortCiteRegEx": "Globerson", "year": 2007}, {"title": "The elements of statistical learning: data mining", "author": ["Tibshirani Hastie", "T. Friedman 2009] Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580v1, preprint", "author": ["Hinton"], "venue": null, "citeRegEx": "Hinton,? \\Q2012\\E", "shortCiteRegEx": "Hinton", "year": 2012}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["Liu", "D.C. Nocedal 1989] Liu", "J. Nocedal"], "venue": "Mathematical Programming", "citeRegEx": "Liu et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1989}, {"title": "Data Augmentation for Support Vector Machines. Bayesian Analysis 6(1):1\u201324", "author": ["Polson", "N.G. Scott 2011] Polson", "S.L. Scott"], "venue": null, "citeRegEx": "Polson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Polson et al\\.", "year": 2011}, {"title": "Bayesian Inference for Logistic Models using Polya-Gamma Latent Variables. arXiv:1205.0310v1", "author": ["Scott Polson", "N.G. Windle 2012] Polson", "J.G. Scott", "J. Windle"], "venue": null, "citeRegEx": "Polson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Polson et al\\.", "year": 2012}, {"title": "In defense of one-vs-all classification", "author": ["Rifkin", "R. Klautau 2004] Rifkin", "A. Klautau"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Rifkin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rifkin et al\\.", "year": 2004}, {"title": "Are loss functions all the same? Neural Computation 16(5):1063\u20131076", "author": ["Rosasco"], "venue": null, "citeRegEx": "Rosasco,? \\Q2004\\E", "shortCiteRegEx": "Rosasco", "year": 2004}, {"title": "Convex learning with invariances", "author": ["Teo"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Teo,? \\Q2008\\E", "shortCiteRegEx": "Teo", "year": 2008}, {"title": "A large dataset for nonparametric object and scene recognition", "author": ["Fergus Torralba", "A. Freeman 2008] Torralba", "R. Fergus", "W. Freeman"], "venue": "IEEE Transaction on Pattern Analysis and Machine Intelligence", "citeRegEx": "Torralba et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2008}, {"title": "Learning with marginalized corrupted features", "author": ["van der Maaten"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Maaten,? \\Q2013\\E", "shortCiteRegEx": "Maaten", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Vincent,? \\Q2008\\E", "shortCiteRegEx": "Vincent", "year": 2008}, {"title": "Dropout training as adaptive regularization", "author": ["Wang Wager", "S. Liang 2013] Wager", "S. Wang", "P. Liang"], "venue": "In Advances in Neural Information Processing", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Fast dropout training", "author": ["Wang", "S. Manning 2013] Wang", "C. Manning"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Feature noising for log-linear structured prediction", "author": ["Wang"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Wang,? \\Q2013\\E", "shortCiteRegEx": "Wang", "year": 2013}, {"title": "Gibbs max-margin topic models with data augmentation", "author": ["Zhu"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Zhu,? \\Q2014\\E", "shortCiteRegEx": "Zhu", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "2013; Wager, Wang, and Liang 2013), named entity recognition (Wang et al. 2013), and image classification (Wang and Manning 2013).", "startOffset": 61, "endOffset": 79}, {"referenceID": 18, "context": "induced from a generalized linear model (GLM) (van der Maaten et al. 2013; Wager, Wang, and Liang 2013; Wang et al. 2013), little work has been done on the margin-based hinge loss underlying the very successful support vector machines (SVMs) (Vapnik 1995).", "startOffset": 46, "endOffset": 121}, {"referenceID": 18, "context": "Such an expectation scheme has been widely adopted in previous work (Wager, Wang, and Liang 2013; van der Maaten et al. 2013; Wang et al. 2013; Wang and Manning 2013).", "startOffset": 68, "endOffset": 166}], "year": 2014, "abstractText": "Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. To deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights have closedform solutions. The similar ideas are applied to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of linear SVMs. Introduction Artificial feature noising augments the finite training data with an infinite number of corrupted versions, by corrupting the given training examples with a fixed noise distribution. Among the many noising schemes, dropout training (Hinton et al. 2012) is an effective way to control over-fitting by randomly omitting subsets of features at each iteration of a training procedure. By formulating the feature noising methods as minimizing the expectation of some loss functions under the corrupting distributions, recent work has provided theoretical understandings of such schemes from the perspective of adaptive regularization (Wager, Wang, and Liang 2013); and has shown promising empirical results in various applications, including document classification (van der Maaten et al. 2013; Wager, Wang, and Liang 2013), named entity recognition (Wang et al. 2013), and image classification (Wang and Manning 2013). Regarding the loss functions, though much work has been done on the quadratic loss, logistic loss, or the log-loss Copyright c \u00a9 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. induced from a generalized linear model (GLM) (van der Maaten et al. 2013; Wager, Wang, and Liang 2013; Wang et al. 2013), little work has been done on the margin-based hinge loss underlying the very successful support vector machines (SVMs) (Vapnik 1995). One technical challenge is that the non-smoothness of the hinge loss makes it hard to compute or even approximate its expectation under a given corrupting distribution. Existing methods are not directly applicable, therefore calling for new solutions. This paper attempts to address this challenge and fill up the gap by extending dropout training as well as other feature noising schemes to support vector machines. Previous efforts on learning SVMs with feature noising have been devoted to either explicit corruption or an adversarial worst-case analysis. For example, virtual support vector machines (Burges and Scholkopf 1997) explicitly augment the training data, which are usually support vectors from previous learning iterations for computational efficiency, with additional examples that are corrupted through some invariant transformation models. A standard SVM is then learned on the corrupted data. Though simple and effective, such an approach lacks elegance and the computational cost of processing the additional corrupted examples could be prohibitive for many applications. The other work (Globerson and Roweis 2006; Dekel and Shamir 2008; Teo et al. 2008) adopts an adversarial worst-case analysis to improve the robustness of SVMs against feature deletion in testing data. Though rigorous in theory, a worst-case scenario is unlikely to be encountered in practice. Moreover, the worst-case analysis usually results in solving a complex and computationally demanding problem. In this paper, we show that it is efficient to train linear SVM predictors on an infinite amount of corrupted copies of the training data by marginalizing out the corruption distributions, an average-case analysis. We concentrate on dropout training, but the results are directly applicable to other noising models, such as Gaussian, Poisson and Laplace (van der Maaten et al. 2013). For all these noising schemes, the resulting expected hinge loss can be upperbounded by a variational objective by introducing auxiliary variables, which follow a generalized inverse Gaussian distribution. We then develop an iteratively re-weighted least square (IRLS) algorithm to minimize the variational bounds. At each iteration, our algorithm minimizes the exar X iv :1 40 4. 41 71 v1 [ cs .L G ] 1 6 A pr 2 01 4 pectation of a re-weighted quadratic loss under the given corrupting distribution, where the re-weights are computed in a simple closed form. We further apply the similar ideas to develop a new IRLS algorithm for the dropout training of logistic regression, which extends the well-known IRLS algorithm for standard logistic regression (Hastie, Tibshirani, and Friedman 2009). Our IRLS algorithms shed light on the connection and difference between the hinge loss and logistic loss in the context of dropout training, complementing to the previous analysis (Rosasco et al. 2004; Globerson et al. 2007) in the supervised learning settings. Finally, empirical results on classification and a challenging \u201cnightmare at test time\u201d scenario (Globerson and Roweis 2006) demonstrate the effectiveness of our approaches, in comparison with various strong competitors. Preliminaries We setup the problem in question and review the learning with marginalized corrupted features. Regularized loss minimization Consider the binary classification, where each training example is a pair (x, y) with x \u2208 R being an input feature vector and y \u2208 {+1,\u22121} being a binary label. Given a set of training data D = {(xn, yn)}n=1, supervised learning aims to find a function f \u2208 F that maps each input to a label. To find the optimal candidate, it commonly solves a regularized loss minimization problem min f\u2208F \u03a9(f) + 2c \u00b7 R(D; f), (1) whereR(D; f) is the risk of applying f to the training data; \u03a9(f) is a regularization term to control over-fitting; and c is a non-negative regularization parameter. For linear models, the function f is simply parameterized as f(x;w, b) = w>x+ b, where w is the weight vector and b is an offset. We will denote \u03b8 := {w, b} for clarity. Then, the regularization can be any Euclidean norms1, e.g., the `2-norm, \u03a9(w) = \u2016w\u20162, or the `1-norm, \u03a9(w) = \u2016w\u20161. For the loss functions, the most relevant measure is the training error, \u2211N n=1 \u03b4(f(xn;\u03b8) 6= yn), which however is not easy to optimize. A convex surrogate loss is used instead, which normally upper bounds the training error. Two popular examples are the hinge loss and logistic loss2:", "creator": "LaTeX with hyperref package"}}}