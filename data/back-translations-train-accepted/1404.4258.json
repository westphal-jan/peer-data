{"id": "1404.4258", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2014", "title": "An Analysis of State-Relevance Weights and Sampling Distributions on L1-Regularized Approximate Linear Programming Approximation Accuracy", "abstract": "Recent interest in the use of $L_1$ regularization in the use of value function approximation includes Petrik et al.'s introduction of $L_1$-Regularized Approximate Linear Programming (RALP). RALP is unique among $L_1$-regularized approaches in that it approximates the optimal value function using off-policy samples. Additionally, it produces policies which outperform those of previous methods, such as LSPI. RALP's value function approximation quality is affected heavily by the choice of state-relevance weights in the objective function of the linear program, and by the distribution from which samples are drawn; however, there has been no discussion of these considerations in the previous literature. In this paper, we discuss and explain the effects of choices in the state-relevance weights and sampling distribution on approximation quality, using both theoretical and experimental illustrations. The results provide insight not only onto these effects, but also provide intuition into the types of MDPs which are especially well suited for approximation with RALP.", "histories": [["v1", "Wed, 16 Apr 2014 14:15:43 GMT  (341kb,D)", "https://arxiv.org/abs/1404.4258v1", "Identical to the ICML 2014 paper of the same name, but with full proofs. Please cite the ICML paper"], ["v2", "Thu, 24 Apr 2014 15:50:46 GMT  (341kb,D)", "http://arxiv.org/abs/1404.4258v2", "Identical to the ICML 2014 paper of the same name, but with full proofs. Please cite the ICML paper"]], "COMMENTS": "Identical to the ICML 2014 paper of the same name, but with full proofs. Please cite the ICML paper", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gavin taylor", "connor geer", "david piekut"], "accepted": true, "id": "1404.4258"}, "pdf": {"name": "1404.4258.pdf", "metadata": {"source": "META", "title": "An Analysis of State-Relevance Weights and Sampling Distributions on L1-Regularized Approximate Linear Programming Approximation Accuracy", "authors": ["Gavin Taylor", "Connor Geer", "David Piekut"], "emails": ["TAYLOR@USNA.EDU"], "sections": [{"heading": "1. Introduction", "text": "In recent years, the Reinforcement Learning Community has paid considerable attention to creating value-added approaches that allow for automatic character selection while increasing the accuracy of approaches. One such approach is the 31st International Conference on Machine Learning, Beijing, China, 2014. This approach frees researchers from manual selection and trait matching while increasing approximation accuracy. (RALP) One of these approaches is the 31st International Conference on Machine Learning, Beijing, China, 2014. (JMLR) Volume 32. Proaches, L1-Regularized Approximate Linear Programming (RALP)."}, {"heading": "2. Notation and Problem Statement", "text": "In this section, we formally define Markov decision-making processes and linear value function approximation. A Markov decision process (MDP) is a tuple (S, A, P, R, \u043c) where S is the measurable, possibly infinite set of states, and A is the finite set of actions. The function R: S \u00b7 S \u00b7 S \u00b7 A [0, 1] is the transition function where P (s \u00b2 s, a) represents the probability of transition from state s to state s, \"given action a. The function R: S \u00b7 R is the reward function, and \u03b3, a number between 0 and 1, is the discount factor that represents the comparative desire for reward in the current time stage to desire for reward in the next time stage."}, {"heading": "3. Previous Work", "text": "In fact, it is not so that one sees oneself in a position to embark on a search for a solution that is able to agree on a solution that is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a position, in which it is in a beaten, in which it is in a position, in which it is in a position, in which it is in which it is in a beaten, in which it is in a position, in which it is in which it is in a beaten, in which it is in which it is in a position, in which it is in which it is in which it is in a position, in which it is in which it is in which it is in a beaten, in which it is in which it is in which it is in which it is in a beaten, in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is beaten, in which it is in which it is in which it is in which it is in which it is in which it is beaten, in which it is in which it is in which it is in which it is in which it is in which it is beaten, in which it is beaten it is in which it is in which it is in which it is in which it is in which it is in which it is beaten it is in which it is in which it is in which it is in which it is in which it is in which it is in which it"}, {"heading": "4. State-Relevance Weights", "text": "The theoretical results presented in the literature on RALP so far do not provide any insight into the behaviour of the approach to state relevance, as the weighting of individual weights has changed. (...) Therefore, it is necessary to deduce new boundaries for RALP that understand its effects. (...) The approach we are taking follows the example of a proof introduced by de Farias and Van Roy (2003) to bind the ALP approach, but we extend it to the weaker assumptions of RALP, along with the requirement that the weights L1 be regularized. (...) We start with the definition of the relevant notation. (...) In the following definitions, we will use R + to refer to the set of non-negative real numbers. (...) Definition 1 We introduce an operator H, defined by (Hl) = max."}, {"heading": "If a Lyapunov function \u03a6wL is constructed such that w\u0304 \u2208", "text": "We note that the proof of the existence of a Lyapunov function, as required in this case, is trivial. First, we construct a weight vector wL with all zeros, but for a positive weight corresponding to the bias characteristic; this leads to the fact that we are a valid Lyapunov function. Second, we note that in this case we construct a weight vector wL with all zeros, but for a positive weight corresponding to the bias characteristic. We must now remove the assumption that a sample exists for each state-action pair. To enable us to bind the behavior of the value function between samples, we make the following assumption, similar to the sufficient sampling assumption made by Petrik et al. (2010): Assume 1 is sufficient sampling, that is, for all state-action pairs and an A, there is a corresponding assumption similar to the sufficient sampling of Petrik et al."}, {"heading": "4.1. Discussion", "text": "This limit is not only narrower than the one presented in the previous literature, but also allows us to analyze the RALP approximation quality in a new way. Firstly, we can observe which Lyapunov functions would lead to a better approximation, and discuss the characteristics of the MDPs that enable these Lyapunov functions and are therefore particularly suitable for evaluating the functional approximation by RALP. Secondly, since \u03c1 now appears in our boundary line, the boundary provides an opportunity to relate our choice of \u03c1 approximation quality, which allows for more intuitive and successful parameter assignments. We address these in the inverse. The Lyapunov function \u03a6wL appears in the first term of our boundary line at three locations, namely the Dot product with the dot approximation value, and in the standard that defines the \"optimal\" w with which we compare our approximation is a bad one. We first notice that the MDP will decrease as the MDP is smaller than the MDP."}, {"heading": "5. Sampling Distribution", "text": "Imagine an MDP with a small finite state space and a single action. Ideal sampling would provide a single sample from each state, giving us an objective function from all states. However, if sampling from a distribution within the state space were to occur multiple times, this ideal situation would be unlikely; some states would not be sampled while others would be sampled multiple times. Since the objective function is defined on samples, this means that states that have been sampled multiple times would appear in the objective function, making the linear program exacerbate the constraints in these states at the expense of accuracy in other states. Of course, a similar scenario also occurs in infinite states. Multiple states that are close together can be sampled, while other regions have very few samples; this promotes the linear program for character selection and an approximate value function that exacerbates the constraints in hard-sampled regions."}, {"heading": "6. Experimental Results", "text": "In this section, we will show experimentally the conclusions drawn in the previous sections. Previous literature has already shown the effectiveness of RALP in common benchmark ranges, so the purpose of this section is to clearly illustrate the conclusions of the previous sections. In a simple, easy-to-represent range, we will make a series of comparisons. First, we will compare the approximation accuracy of the sample from a range with a stable Ljapunow function with the accuracy resulting from the sample from a range without such a function. Next, we will compare the accuracy of the approximation resulting from the sample with the accuracy of the approximation resulting from the sample from two different uneven distributions from the sample. Finally, we will compare the approximation accuracy of the calculation of an approximation with grid = 1 with the approximation accuracy of the calculation of an approximation if the sample is uneven. This will be demonstrated using two different, uneven distributions of two different uneven distributions. Finally, we will compare the approximation accuracy of the calculation of an approximation range with grid = 1, by drawing a sampling accuracy of the sampling with one of the sewing accuracy of the sample."}, {"heading": "6.1. Domain", "text": "All experiments were conducted on a domain defined by a 25 by 25 grid world, which comprised four reward regions in the corners of the grid, each consisting of 9 states, as can be seen in Figure 1 (a). Two reward regions (gold colored) had a reward of 1, while the others had a reward of -1 (red colored); the remaining states had a reward of 0. Actions consisted of moving a square in one of the four directions, unless this action was limited by a wall, in which case there would be no movement. The discount factor was set to 0.95. In all experiments, the feature set consisted of symmetric Gaussian features centered by every millisecond with variances of 2, 5, 10, 15, 25, 50, and 75, plus the distortion characteristic, resulting in 9n + 1 characteristics for n samples. The optimal value function can be seen in Figure 1."}, {"heading": "6.2. Lyapunov Stable Domain", "text": "First, we show the improvement of the RALP approach when the domain has a stable Lyapunov function. A stable Lyapunov function can be created by forcing the actor into a defined area in the state area. In order to keep the presentation difficulties of the optimal value functions equal, we created a Lyapunov function by eliminating actions that further distance the actor from the next positive reward. This preserves the optimal policy of the unchanged domain, while the optimal value functions remain identical, allowing the approximation accuracy to be fairly compared.In the domain without a stable Lyapunov function, the actor was able to move freely in the state area based on a random selection between the four actions. However, in the domain with a stable Lyapunov function, the actor was only allowed to move in the two directions that would not move him further from the next target. We note that not all remaining actions are optimal, so the sample still includes non-political samples."}, {"heading": "6.3. Sampling from a Nonuniform Distribution", "text": "Next, we will illustrate the change in approximation accuracy in the sample from an uneven distribution. Section 5 provides evidence that a distribution that is the densest where L (s) is the smallest may be beneficial. To create such a distribution, an agent was started in a random state and allowed to take optimal policy for 25 steps. This was done 10,000 times, and the number of visits in each state was tabulated and normalized, defining our distribution that was heaviest around the edges and reward corners and otherwise increased slightly with increasing proximity to the positive reward regions. We will call this distribution \"sampling.\" For these experiments, 20 samples per run were taken from the stable Lyapunov function, as discussed in Section 6.2. The regulation parameter was set at 1.5, and the average error from the positive reward regions slightly increased."}, {"heading": "6.4. Changing the State-Relevance Weights", "text": "Finally, we illustrate the effect on the approximation of the change in state relevance. 200 samples were taken uniformly from the state space; while the effects from the two previous experiments are most pronounced when the samples are sparse, the effects from the change in state relevance are most pronounced when a number of constraints in a particular region can be tightened. We would prefer to specify the stationary distribution; however, since the domain is non-recurrent, this is not an option. However, the distribution created for the sample in Section 6.3 is large where the Lyapunov function is small, making it a reasonable substitute. In a number of experiments, the value of this distribution in this state was set; in the other, the value was set at 4."}, {"heading": "7. Conclusion", "text": "The experimental success of RALP in the literature to date, together with its easy-to-fulfill assumptions, suggests that its application in real, complicated and complex areas is promising. Despite this promise, and despite the evidence that the effects are dramatic, no theory has been developed to analyze changes in approximate quality due to changes in the objective functional parameter \u03c1 or due to differences in sampling strategies. These considerations are indispensable for the use of RALP in the real world; it is rarely possible to stitch uniformly, and the importance of accuracy throughout the state is seldom existent.In this paper, we demonstrate the importance of understanding these ideas and produce a limitation to the approximation error of RALP, which is narrower and more informative than previous boundaries. This boundary provides intuition in the quality of RALP approximation as a function of state relevance weights and sampling distributions. Furthermore, we have shown that the quality of a RALP approximation is considered to be particularly stable when the RALP is a domain close and the percentage is particularly high."}, {"heading": "Acknowledgements", "text": "Many thanks to the anonymous reviewers for their help in improving this work and for the support from the Naval Research Laboratory Information Management & Decision Architecture Branch (code 5580) and the financial support from the Office of Naval Research, grant numbers N001613WX20992 and N0001414WX20507."}, {"heading": "A. Proof of Lemma 1", "text": "As explained in section 4, this proof is very similar to theorem 3 of de Farias and van Roy (2003), but we still include it for clarity purposes. First, the proof requires a series of additional Lemmas. Lemma 3 for all functions V and V, which provide an approximation with limited errors, and then displays the point selected by RALP (no further than this constructed point). \u2212 Lemma 3 for all functions V and V, the TV \u2212 TV \u2212 V, the TV \u2212 V, the TV \u2212 V, the maximum number of points V \u2212 V, V \u2212 V, the maximum number of points V \u2212 V, the maximum number of points V \u2212 V, the maximum number of V \u2212 V, the TV \u2212 V, the TV \u2212 V, the V \u2212 V, the V \u2212 V and the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the \u2212 V, the V \u2212 V, the V-V-V, the \u2212 V, the \u2212 V-V, the V-V-V-V-V, the \u2212 the V-V, the \u2212 V-V-V, the V-V-V-V, the \u2212 the V-V-V, the \u2212 the V-V-V-V, the V-V-V-V-V, the \u2212 the V-V-V-V, the V-V-V-V, the \u2212 the V-V-V-V-V-V-V, the \u2212 the V-V-V-points, the V-V-V-V-V-V-V, the \u2212 the V-V-V-V-points, the V-V-V-V-V-V-points, the V-V-V-V-V-V-points, the \u2212 the V-V-V-V-V-V-V, the V-V-V-V-V-points, the V-V-V-V-V-points, the \u2212 the V-V-V-V-V-V-V-"}, {"heading": "B. Proof of Lemma 2", "text": "If a constraint does not exist in a state, it does not mean that the value in that state is completely unlimited; because we replace the rate of change of all components of the approximate and true value functions in the assumption 1 that the existence of a constraint that would have been constructed on a nearby state with the existence of what we will call an implicit constraint. This problem explicitly constructs these implicit constraints and quantifies the maximum distance from the true constraint that would have existed by constructing two MDPs that are identical in all respects, but for the reward function. M1 is incomplete sampled, with sample set. Therefore, each state actioner pair has either an explicit or implicit constraint in the corresponding RALP. However, M2 is completely sampled. Each state actioner pair in the set is identical in M2, but all state actioners are unable to produce a constraint in the corresponding RALP."}], "references": [{"title": "Geometric bounds for stationary distributions of infinite markov chains via lyapunov functions", "author": ["Bertsimas", "Dimitris", "Gamarnik", "David", "Tsitsiklis", "John N"], "venue": "Technical report, Massachusetts Institute of Technology,", "citeRegEx": "Bertsimas et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bertsimas et al\\.", "year": 1998}, {"title": "The Linear Programming Approach to Approximate Dynamic Programming", "author": ["de Farias", "Daniela Pucci", "Van Roy", "Benjamin"], "venue": "Operations Research,", "citeRegEx": "Farias et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Farias et al\\.", "year": 2003}, {"title": "On Constraint Sampling for the Linear Programming Approach to Approximate Dynamic Programming", "author": ["de Farias", "Daniela Pucci", "Van Roy", "Benjamin"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Farias et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Farias et al\\.", "year": 2004}, {"title": "A probabilistic production and inventory problem", "author": ["F. d\u2019Epenoux"], "venue": "Management Science,", "citeRegEx": "d.Epenoux,? \\Q1963\\E", "shortCiteRegEx": "d.Epenoux", "year": 1963}, {"title": "Regularized Off-Policy TD-Learning", "author": ["Liu", "Bo", "Mahadevan", "Sridhar", "Ji"], "venue": "In Proceedings of the Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Sparse Q-Learning With Mirror Descent", "author": ["Mahadevan", "Sridhar", "Liu", "Bo"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Mahadevan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 2012}, {"title": "Lyapunov Design for Safe Reinforcement Learning", "author": ["Perkins", "Theodore J", "Barto", "Andrew G"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Perkins et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Perkins et al\\.", "year": 2003}, {"title": "Feature selection using regularization in approximate linear programs for markov decision processes", "author": ["Petrik", "Marek", "Taylor", "Gavin", "Parr", "Ronald", "Zilberstein", "Shlomo"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Petrik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2010}, {"title": "Coarticulation in Markov Decision Processes", "author": ["Rohanimanesh", "Khashayar", "Platt", "Robert", "Mahadevan", "Sridhar", "Grupen", "Roderic"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rohanimanesh et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rohanimanesh et al\\.", "year": 2004}, {"title": "Generalized Polynomial Approximations in Markovian Decision Processes", "author": ["Schweitzer", "Paul J", "Seidmann", "Abraham"], "venue": "Journal of mathematical analysis and applications,", "citeRegEx": "Schweitzer et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Schweitzer et al\\.", "year": 1985}], "referenceMentions": [{"referenceID": 4, "context": "In recent years, the Reinforcement Learning community has paid considerable attention to creating value function approximation approaches which perform automated feature selection while approximating a value function (Kolter & Ng, 2009; Johns et al., 2010; Mahadevan & Liu, 2012; Liu et al., 2012).", "startOffset": 217, "endOffset": 297}, {"referenceID": 7, "context": "proaches, L1-Regularized Approximate Linear Programming (RALP) (Petrik et al., 2010; Taylor & Parr, 2012) is unique in that it results in an approximation of the optimal value function and makes use of off-policy samples.", "startOffset": 63, "endOffset": 105}, {"referenceID": 7, "context": "Therefore, in Section 4, we derive a new error bound for the RALP approximation, which is tighter than those presented by Petrik et al. (2010) and provides new insight into the result of changing the state-relevance weights in the objective function.", "startOffset": 122, "endOffset": 143}, {"referenceID": 7, "context": "Therefore, in Section 4, we derive a new error bound for the RALP approximation, which is tighter than those presented by Petrik et al. (2010) and provides new insight into the result of changing the state-relevance weights in the objective function. In addition, this bound provides an insight into the types of MDPs particularly well suited to the RALP approach. Finally, this section provides evidence that rather than weighting all states equally, as was done by Petrik et al. (2010) and Taylor and Parr (2012), states should instead be weighted in proportion to the stationary distribution under the optimal policy.", "startOffset": 122, "endOffset": 488}, {"referenceID": 7, "context": "Therefore, in Section 4, we derive a new error bound for the RALP approximation, which is tighter than those presented by Petrik et al. (2010) and provides new insight into the result of changing the state-relevance weights in the objective function. In addition, this bound provides an insight into the types of MDPs particularly well suited to the RALP approach. Finally, this section provides evidence that rather than weighting all states equally, as was done by Petrik et al. (2010) and Taylor and Parr (2012), states should instead be weighted in proportion to the stationary distribution under the optimal policy.", "startOffset": 122, "endOffset": 515}, {"referenceID": 3, "context": "(2010) to extend the capabilities of the linear programming approach to value function approximation (d\u2019Epenoux, 1963; Schweitzer & Seidmann, 1985; de Farias & Van Roy, 2003).", "startOffset": 101, "endOffset": 174}, {"referenceID": 6, "context": "RALP was introduced by Petrik et al. (2010) to extend the capabilities of the linear programming approach to value function approximation (d\u2019Epenoux, 1963; Schweitzer & Seidmann, 1985; de Farias & Van Roy, 2003).", "startOffset": 23, "endOffset": 44}, {"referenceID": 4, "context": "Liu et al. (2012) introduced RO-TD, which converges to an approximation of the value function of a given policy, even when trained on offpolicy samples.", "startOffset": 0, "endOffset": 18}, {"referenceID": 8, "context": "Besides stability analysis, Lyapunov functions have also previously appeared in Reinforcement Learning literature, though in different contexts from our application (Perkins & Barto, 2003; Rohanimanesh et al., 2004).", "startOffset": 165, "endOffset": 215}, {"referenceID": 7, "context": "To enable us to bound the behavior of the value function between samples, we make the following assumption, similar to the sufficient sampling assumption made by Petrik et al. (2010):", "startOffset": 162, "endOffset": 183}, {"referenceID": 0, "context": "Bertsimas et al. (1998) demonstrated that a small Lyapunov function value correlates in expectation with a higher probability in the stationary distribution of a Markov chain.", "startOffset": 0, "endOffset": 24}], "year": 2014, "abstractText": "Recent interest in the use of L1 regularization in the use of value function approximation includes Petrik et al.\u2019s introduction of L1-Regularized Approximate Linear Programming (RALP). RALP is unique among L1-regularized approaches in that it approximates the optimal value function using off-policy samples. Additionally, it produces policies which outperform those of previous methods, such as LSPI. RALP\u2019s value function approximation quality is affected heavily by the choice of state-relevance weights in the objective function of the linear program, and by the distribution from which samples are drawn; however, there has been no discussion of these considerations in the previous literature. In this paper, we discuss and explain the effects of choices in the state-relevance weights and sampling distribution on approximation quality, using both theoretical and experimental illustrations. The results provide insight not only onto these effects, but also provide intuition into the types of MDPs which are especially well suited for approximation with RALP.", "creator": "LaTeX with hyperref package"}}}