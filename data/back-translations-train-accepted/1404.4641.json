{"id": "1404.4641", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2014", "title": "Multilingual Models for Compositional Distributed Semantics", "abstract": "We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data.", "histories": [["v1", "Thu, 17 Apr 2014 20:18:03 GMT  (137kb,D)", "http://arxiv.org/abs/1404.4641v1", "Proceedings of ACL 2014 (Long papers)"]], "COMMENTS": "Proceedings of ACL 2014 (Long papers)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karl moritz hermann", "phil blunsom"], "accepted": true, "id": "1404.4641"}, "pdf": {"name": "1404.4641.pdf", "metadata": {"source": "CRF", "title": "Multilingual Models for Compositional Distributed Semantics", "authors": ["Karl Moritz Hermann"], "emails": ["karl.moritz.hermann@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Distributed representations of words form the basis for many ultramodern approaches to various problems in the processing of natural language of today. Such word embeddings are of course richer than those of symbolic or discrete models and have proven to be able to capture both syntactic and semantic information. Successful applications of such models include speech modeling (Bengio et al., 2003), paraphrase recognition (Erk and Pado, 2008) and dialog analysis (Kalchbrenner and Blunsom, 2013). Within a monolingual context, the distribution hypothesis (Firth, 1957) forms the basis of most approaches to learning word representations. In this work, we extend this hypothesis to multilingual data and embeddings in the common space. We present a novel, unsupervised technique for learning semantic representations using semantic corpora and semantic transfer by compositional representations Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus, Vettus, Vettus and Vettus, Vettus, Vettus, Vettus and Vettus, Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vettus, Vettus, Vettus and Vettus, Vettus, Vettus, Vettus and Vettus, Vettus, Vettus, Vettus, Vettus and Vettus, Vettus, Vettus and Vettus, Vett"}, {"heading": "2 Overview", "text": "Distributed representation learning describes the task of learning continuous representations for discrete objects. Here, we focus on learning secondary representations and examine how the use of multilingual data can improve the learning of such representations at word and higher levels. We present a model that learns to represent every word in a lexicon by a continuous vector in Rd. Such distributed representations allow a model to divide the meaning between similar words and have been used to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, among others).We describe a multilingual objective function that uses a noise contrasting update between semantic representations of different languages to learn these word embeddings."}, {"heading": "3 Approach", "text": "Most previous work on learning compositional semantic representations uses parse trees on their training data to structure their compositional functions (Socher et al., 2012; Hermann and Blunsom, 2013, et al.). Furthermore, these approaches typically depend on specific semantic signals such as sentiment or topic margins for their objective functions. While these methods have proven effective in some cases, the need for parse trees and annotated data limits such approaches for resource-favored languages. Our novel method of learning compositional vectors eliminates these requirements and can be more easily applied to languages with limited resources (in particular, we are trying to learn semantics from multilingual data. The idea is that with enough parallel data, a common representation of two parallel sentences would be forced to capture the common elements between these two sentences. Which parallel sentences naturally share their semantic languages x."}, {"heading": "3.1 Two Composition Models", "text": "The objective function in Eq.2 could be coupled with any two predetermined vector composition functions f, g from literature. Since we want to apply our approach to a wide range of languages, we focus on composition functions that do not require syntactical information. We evaluate the following two composition functions. The first model, ADD, represents a sentence by the sum of its word vectors. This is a distributed bag-of-words approach, as the sentence order is not taken into account by the model.Second, the BI model is designed to capture bigram information, using a nonlinearity over bigrampaars in its composition function: f (x) = n \u00b2 i = 1tanh (xi \u2212 1 + xi) (3) The use of a nonlinearity allows the model to learn interesting interactions between words in a document that the bag-of-words approach of the ADD cannot learn."}, {"heading": "3.2 Document-level Semantics", "text": "While most approaches to compositional distributed semantics end at the word level, our model naturally extends to document-level learning by applying the composition and objective function (Eq.2) recursively to the composition of sentences in documents. This is achieved by first calculating semantic representations for each sentence in a document, and then using these representations as inputs in a parent CVM to calculate a semantic representation of a document (Figure 2).This recursive approach integrates document-level representations into the learning process. We can therefore use parallel document companies - whether they are sentence-oriented or not - to propagate a semantic signal back to the individual words. Of course, if sentence orientation is present, the document signal can easily be combined with the sentence signal, as we have done with the experiments in \u00a7 5.3.This concept of learning compositions for certain DOBI representations taking into account the respective DOBIs for the previous year's comparison (to the 2012 Documents) and ADV models."}, {"heading": "4 Corpora", "text": "We use two corpus to learn semantic representations and perform the experiments described in this paper.The Europarl corpus v71 (Koehn, 2005) was used in the initial development and testing of our approach, as well as to learn the presentations used for the task of Cross-Lingual Document Classification described in \u00a7 5.2. We took into account the English-German and English-French language pairs from this corpus, from which the last 100,000 sentences of each pair were reserved for development. Secondly, we developed a massively multilingual corpus based on the TED corpus 2 for IWSLT 2013 (Cettolo et al., 2012).This corpus contains English transcriptions and multilingual, condemnation-oriented translations of lectures from the TED conference. While the corpus is aimed at machine translation tasks, we use the keywords associated with each conversation in order to add a multilingual corpus to the multilingual corpus for multilingual classification."}, {"heading": "5 Experiments", "text": "We report on the results of two experiments. First, we replicate the task of the linguistic document classification by Klementiev et al. (2012), learn distributed representations of the Europarl corpus and evaluate documents from the Reuters RCV1 / RCV2 corpus. Subsequently, we design a1http: / / www.statmt.org / europarl / 2https: / / wit3.fbk.eu / 3http: / www.clg.ox.ac.uk / tedcldc / 4English to Arabic, German, French, Spanish, Italian, Dutch, Polish, Brazilian, Portuguese, Romanian, Russian and Turkish. Chinese, Farsi and Slovenian have been removed due to the small size of these datasets. 5http: / / cdec-decoder.org / multilevel classification task using the TED corpus, both for training and for evaluation. The use of a wider range of experiments in the second multilingual perspective allows us to examine the learning skills from a common perspective in our models."}, {"heading": "5.1 Learning", "text": "All model weights were randomly initialized using a Gaussian distribution (\u00b5 = 0, \u03c32 = 0.1). We used the available development data to determine our model parameters. For each positive sample, we used a number of random sound samples taken from the corpus (k-1, 10, 50) in each training epoch. All our embeddings have dimensionality d = 128, with the margin set to m = d.6. Furthermore, we use the L2 regulation with \u03bb = 1 and increment in {0.01, 0.05}. We use 100 iterations for the RCV task, 500 for the TED single and 5 for the joint corporation. We use the adaptive gradient method AdaGrad (Duchi et al., 2011) to update the weights of our models in a minibatch setting (b-10, 50}). All settings, our model simulations and scripts are available for replication at http / www.m.com."}, {"heading": "5.2 RCV1/RCV2 Document Classification", "text": "We evaluate our models on the basis of a multilingual document classification (CLDC, henceforth), first described in Klementiev et al. (2012). This task involves learning language-independent embedding, which is then used to classify documents within the English-German language pair. CLDC uses a specific type of monitoring, namely the use of monitored training data in one language and the evaluation without further monitoring in another. CLDC can thus be used to determine whether our learned representations in multiple languages are semantically useful. We track that in Klementiev et al. (2012), except that we learn our embedding using only the Europarl data and use it only during classification training and testing. Each document in the classification task is represented by the average of the d-dimensional representations of all its phrases."}, {"heading": "5.3 TED Corpus Experiments", "text": "We describe here our experiments on the TED corpus, which enables us to scale up to multilingual learning. It consists of a large number of relatively short and parallel documents that allow us to measure the performance of the DOC model, which is described in \u00a7 3.2.We use the training data of the corpus to simultaneously learn distributed representations in 12 languages. The training is performed in two settings, with vectors learned from a single language pair (en-X), while in the common mode vectorlearning is performed at all parallel levels at the same time. This setting causes words from all languages to be embedded in a single semantic space. First, we evaluate the effect of the language signal documentation language (s), described in \u00a7 3.2), as well as whether our multilingual learning method can be extended to a greater variety of languages. We train DOC models by using both ADD and BI as CVM (DOC / BI), both in the common as well as in the common and comparative form."}, {"heading": "5.4 Linguistic Analysis", "text": "While the classification experiments focused on determining the semantic content of the sentence-level representations, we also want to briefly examine the induced word embeddings. To this end, we use the BI + model developed at the Europarl corpus. Figure 4 shows the t-SNE projections for a number of English, French and German words. Although the model did not use parallel French-German data during the training, it nevertheless managed to learn semantic word-word similarities in these two languages.Going a step further, Figure 5 shows t-SNE projections for a number of short phrases in these three languages. We use English for the presidential and gender-specific expressions Mr. President and Madam President, as well as gender-specific equivalents in French and German. The projection shows a number of interesting results: First, the model correctly shows the words in three groups that correspond to the three English forms and the related translations. \"Secondly, a division between the male forms can be observed on the lower half of the gender when the most typical forms are superimposed."}, {"heading": "6 Related Work", "text": "In its simplest form, the number of people who live in a country is as large as in no other. In its totality, the number of people who live in a country is as large as in Germany. In its totality, the number of people who live in a country is as large as in Germany. In its totality, the number of people who live in a country is as small as in no other country. In its totality, the number of people who live in a country is as large as in Germany. In its totality, the number of people who live in a country is as large as in Germany. In its totality, the number of people who live in a country is the number of people who live in whom they live, the number of people, the number of people who they live in, the number of people, the number of people who they live in, the number of people, the number of people who live in a country, in whom they live, in whom they live, in whom they live, in whom they live, in whom they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "7 Conclusion", "text": "In summary, we have presented a novel method for learning multilingual word embedding using parallel data in conjunction with a multilingual objective function for compositional vector models. This approach extends the distribution hypothesis to multilingual articulated space representations. In conjunction with very simple compositional functions, vectors learned using this method exceed the state of the art in the task of lingual document classification. Further experiments and analyses support our hypothesis that bilingual signals are a useful tool for learning distributed representations by enabling models to abstract from monolingual surface realisations into a deeper semantic space."}, {"heading": "Acknowledgements", "text": "This work was supported by the Xerox Foundation Award and the EPSRC grant number EP / K036580 / 1."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Al.Rfou. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou. et al\\.", "year": 2013}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan."], "venue": "Journal of Machine Learning Research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Precis of how children learn the meanings of words", "author": ["P. Bloom."], "venue": "Behavioral and Brain Sciences, 24:1095\u20131103. M. Cettolo, C. Girardi, and M. Federico. 2012. Wit: Web inventory of transcribed and translated talks. In", "citeRegEx": "Bloom.,? 2001", "shortCiteRegEx": "Bloom.", "year": 2001}, {"title": "Combining symbolic and distributional models of meaning", "author": ["S. Clark", "S. Pulman."], "venue": "Proceedings of AAAI Spring Symposium on Quantum Interaction. AAAI Press.", "citeRegEx": "Clark and Pulman.,? 2007", "shortCiteRegEx": "Clark and Pulman.", "year": 2007}, {"title": "Machine translation by triangulation: Making effective use of multi-parallel corpora", "author": ["T. Cohn", "M. Lapata."], "venue": "Proceedings of ACL.", "citeRegEx": "Cohn and Lapata.,? 2007", "shortCiteRegEx": "Cohn and Lapata.", "year": 2007}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins."], "venue": "Proceedings of ACLEMNLP.", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston."], "venue": "Proceedings of ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159, July.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Using latent semantic analysis to improve access to textual information", "author": ["S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "S. Deerwester", "R. Harshman."], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.", "citeRegEx": "Dumais et al\\.,? 1988", "shortCiteRegEx": "Dumais et al\\.", "year": 1988}, {"title": "cdec: A Decoder, Alignment, and Learning framework for finite-state and context-free translation models", "author": ["C. Dyer", "A. Lopez", "J. Ganitkevitch", "J. Weese", "F. Ture", "P. Blunsom", "H. Setiawan", "V. Eidelman", "P. Resnik."], "venue": "Proceedings of ACL.", "citeRegEx": "Dyer et al\\.,? 2010", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "A structured vector space model for word meaning in context", "author": ["K. Erk", "S. Pad\u00f3."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Erk and Pad\u00f3.,? 2008", "shortCiteRegEx": "Erk and Pad\u00f3.", "year": 2008}, {"title": "A synopsis of linguistic theory 1930", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth.,? \\Q1957\\E", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["A. Haghighi", "P. Liang", "T. Berg-Kirkpatrick", "D. Klein."], "venue": "Proceedings of ACL-HLT.", "citeRegEx": "Haghighi et al\\.,? 2008", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "The Role of Syntax in Vector Space Models of Compositional Semantics", "author": ["K.M. Hermann", "P. Blunsom."], "venue": "Proceedings of ACL.", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["K.M. Hermann", "P. Blunsom."], "venue": "Proceedings of ICLR.", "citeRegEx": "Hermann and Blunsom.,? 2014", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Recurrent convolutional neural networks for discourse compositionality", "author": ["N. Kalchbrenner", "P. Blunsom."], "venue": "Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai."], "venue": "Proceedings of COLING.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["P. Koehn."], "venue": "Proceedings of the Machine Translation Summit.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Learning multilingual word representations using a bag-of-words autoencoder", "author": ["S. Lauly", "A. Boulanger", "H. Larochelle."], "venue": "Deep Learning Workshop at NIPS.", "citeRegEx": "Lauly et al\\.,? 2013", "shortCiteRegEx": "Lauly et al\\.", "year": 2013}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li."], "venue": "Journal of Machine Learning Research, 5:361\u2013397, December.", "citeRegEx": "Lewis et al\\.,? 2004", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Mallet: A machine learning for language toolkit", "author": ["A.K. McCallum."], "venue": "http://mallet.cs.umass.edu.", "citeRegEx": "McCallum.,? 2002", "shortCiteRegEx": "McCallum.", "year": 2002}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. \u010cernock\u00fd", "S. Khudanpur."], "venue": "Proceedings of INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean."], "venue": "CoRR.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever."], "venue": "CoRR.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata."], "venue": "In Proceedings of ACL.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G. Hinton."], "venue": "Proceedings of NIPS.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng."], "venue": "ICML.", "citeRegEx": "Ngiam et al\\.,? 2011", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Grounded spoken language acquisition: Experiments in word learning", "author": ["D. Roy."], "venue": "IEEE Transactions on Multimedia, 5(2):197\u2013209, June.", "citeRegEx": "Roy.,? 2003", "shortCiteRegEx": "Roy.", "year": 2003}, {"title": "Multilingual deep learning", "author": ["A.P. Sarath Chandar", "M.K. Mitesh", "B. Ravindran", "V. Raykar", "A. Saha."], "venue": "Deep Learning Workshop at NIPS.", "citeRegEx": "Chandar et al\\.,? 2013", "shortCiteRegEx": "Chandar et al\\.", "year": 2013}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng."], "venue": "Proceedings of EMNLPCoNLL, pages 1201\u20131211.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov."], "venue": "Proceedings of NIPS.", "citeRegEx": "Srivastava and Salakhutdinov.,? 2012", "shortCiteRegEx": "Srivastava and Salakhutdinov.", "year": 2012}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio."], "venue": "Proceedings of ACL.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Domain and function: A dualspace model of semantic relations and compositions", "author": ["P.D. Turney."], "venue": "Journal of Artificial Intelligence Research, 44:533\u2013 585.", "citeRegEx": "Turney.,? 2012", "shortCiteRegEx": "Turney.", "year": 2012}, {"title": "Learning discriminative projections for text similarity measures", "author": ["W.-T. Yih", "K. Toutanova", "J.C. Platt", "C. Meek."], "venue": "Proceedings of CoNLL.", "citeRegEx": "Yih et al\\.,? 2011", "shortCiteRegEx": "Yih et al\\.", "year": 2011}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["W.Y. Zou", "R. Socher", "D. Cer", "C.D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "ful applications of such models include language modelling (Bengio et al., 2003), paraphrase detection (Erk and Pad\u00f3, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013).", "startOffset": 59, "endOffset": 80}, {"referenceID": 13, "context": ", 2003), paraphrase detection (Erk and Pad\u00f3, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013).", "startOffset": 30, "endOffset": 50}, {"referenceID": 19, "context": ", 2003), paraphrase detection (Erk and Pad\u00f3, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013).", "startOffset": 74, "endOffset": 106}, {"referenceID": 14, "context": "Within a monolingual context, the distributional hypothesis (Firth, 1957) forms the basis of most approaches for learning word representations.", "startOffset": 60, "endOffset": 73}, {"referenceID": 23, "context": "First, we show that for cross-lingual document classification on the Reuters RCV1/RCV2 corpora (Lewis et al., 2004), we outperform the prior state of the art (Klementiev et al.", "startOffset": 95, "endOffset": 115}, {"referenceID": 20, "context": ", 2004), we outperform the prior state of the art (Klementiev et al., 2012).", "startOffset": 50, "endOffset": 75}, {"referenceID": 4, "context": "mans is widely seen as grounded in sensory-motor experience (Bloom, 2001; Roy, 2003).", "startOffset": 60, "endOffset": 84}, {"referenceID": 31, "context": "mans is widely seen as grounded in sensory-motor experience (Bloom, 2001; Roy, 2003).", "startOffset": 60, "endOffset": 84}, {"referenceID": 4, "context": "mans is widely seen as grounded in sensory-motor experience (Bloom, 2001; Roy, 2003). Based on this idea, there have been some attempts at using multi-modal data for learning better vector representations of words (e.g. Srivastava and Salakhutdinov (2012)).", "startOffset": 61, "endOffset": 256}, {"referenceID": 21, "context": "The Europarl corpus v71 (Koehn, 2005) was", "startOffset": 24, "endOffset": 37}, {"referenceID": 20, "context": "First, we replicate the cross-lingual document classification task of Klementiev et al. (2012), learning distributed representations on the Europarl corpus and evaluating on documents from the Reuters RCV1/RCV2 corpora.", "startOffset": 70, "endOffset": 95}, {"referenceID": 10, "context": "We use the adaptive gradient method, AdaGrad (Duchi et al., 2011), for updating the weights of our models, in a mini-batch setting (b \u2208 {10, 50}).", "startOffset": 45, "endOffset": 65}, {"referenceID": 7, "context": "We train the multiclass classifier using an averaged perceptron (Collins, 2002) with the same settings as in Klementiev et al.", "startOffset": 64, "endOffset": 79}, {"referenceID": 19, "context": "ument classification (CLDC, henceforth) task first described in Klementiev et al. (2012). This task involves learning language independent embeddings which are then used for document classification across the English-German language pair.", "startOffset": 64, "endOffset": 89}, {"referenceID": 19, "context": "ument classification (CLDC, henceforth) task first described in Klementiev et al. (2012). This task involves learning language independent embeddings which are then used for document classification across the English-German language pair. For this, CLDC employs a particular kind of supervision, namely using supervised training data in one language and evaluating without further supervision in another. Thus, CLDC can be used to establish whether our learned representations are semantically useful across multiple languages. We follow the experimental setup described in Klementiev et al. (2012), with the exception that we learn our embeddings using solely the Europarl data and use the Reuters corpora only during for classifier training and testing.", "startOffset": 64, "endOffset": 599}, {"referenceID": 7, "context": "We train the multiclass classifier using an averaged perceptron (Collins, 2002) with the same settings as in Klementiev et al. (2012).", "startOffset": 65, "endOffset": 134}, {"referenceID": 20, "context": "On the RCV task we also report results for d=40 which matches the dimensionality of Klementiev et al. (2012).", "startOffset": 84, "endOffset": 109}, {"referenceID": 20, "context": "Cross-lingual compositional representations (ADD, BI and their multilingual extensions), I-Matrix (Klementiev et al., 2012) translated (MT) and glossed (Glossed) word baselines, and the majority class baseline.", "startOffset": 98, "endOffset": 123}, {"referenceID": 20, "context": "Cross-lingual compositional representations (ADD, BI and their multilingual extensions), I-Matrix (Klementiev et al., 2012) translated (MT) and glossed (Glossed) word baselines, and the majority class baseline. The baseline results are from Klementiev et al. (2012).", "startOffset": 99, "endOffset": 266}, {"referenceID": 6, "context": "A similar idea exists in machine translation where English is frequently used to pivot between other languages (Cohn and Lapata, 2007).", "startOffset": 111, "endOffset": 134}, {"referenceID": 20, "context": "Table 1 shows the results for training on 1,000 documents compared with the results published in Klementiev et al. (2012). Our models outperform the prior state of the art, with the BI models performing slightly better than the ADD models.", "startOffset": 97, "endOffset": 122}, {"referenceID": 12, "context": "We use the cdec decoder (Dyer et al., 2010) with default settings for this purpose.", "startOffset": 24, "endOffset": 43}, {"referenceID": 24, "context": "We use the implementation in Mallet (McCallum, 2002)", "startOffset": 36, "endOffset": 52}, {"referenceID": 9, "context": "Baseline embeddings are Senna (Collobert et al., 2011) and Polyglot (Al-Rfou\u2019 et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 0, "context": ", 2011) and Polyglot (Al-Rfou\u2019 et al., 2013).", "startOffset": 21, "endOffset": 44}, {"referenceID": 9, "context": "We compare our embeddings with the SENNA embeddings, which achieve state of the art performance on a number of tasks (Collobert et al., 2011).", "startOffset": 117, "endOffset": 141}, {"referenceID": 0, "context": "Additionally, we use the Polyglot embeddings of Al-Rfou\u2019 et al. (2013), who published word embeddings across 100 languages, including all languages considered in this paper.", "startOffset": 48, "endOffset": 71}, {"referenceID": 11, "context": "This is related to topic-modelling techniques such as LSA (Dumais et al., 1988), LSI, and LDA (Blei et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 3, "context": ", 1988), LSI, and LDA (Blei et al., 2003), but these methods use a document-level context, and", "startOffset": 22, "endOffset": 41}, {"referenceID": 2, "context": "Neural language models are another popular approach for inducing distributed word representations (Bengio et al., 2003).", "startOffset": 98, "endOffset": 119}, {"referenceID": 2, "context": "Neural language models are another popular approach for inducing distributed word representations (Bengio et al., 2003). They have received a lot of attention in recent years (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010, inter alia) and have achieved state of the art performance in language modelling. Collobert et al. (2011) further popularised using neural network architectures for learning word embeddings from large amounts of largely unlabelled data by showing the embeddings can then be used to improve standard supervised tasks.", "startOffset": 99, "endOffset": 355}, {"referenceID": 3, "context": "Tasks, where the use of distributed representations has resulted in improvements include topic modelling (Blei et al., 2003) or named entity recognition (Turian et al.", "startOffset": 105, "endOffset": 124}, {"referenceID": 36, "context": ", 2003) or named entity recognition (Turian et al., 2010; Collobert et al., 2011).", "startOffset": 36, "endOffset": 81}, {"referenceID": 9, "context": ", 2003) or named entity recognition (Turian et al., 2010; Collobert et al., 2011).", "startOffset": 36, "endOffset": 81}, {"referenceID": 33, "context": "More complex composition functions using matrix-vector composition, convolutional neural networks or tensor composition have proved useful in tasks such as sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), relational similarity (Turney, 2012) or dialogue analy-", "startOffset": 175, "endOffset": 223}, {"referenceID": 17, "context": "More complex composition functions using matrix-vector composition, convolutional neural networks or tensor composition have proved useful in tasks such as sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), relational similarity (Turney, 2012) or dialogue analy-", "startOffset": 175, "endOffset": 223}, {"referenceID": 37, "context": ", 2011; Hermann and Blunsom, 2013), relational similarity (Turney, 2012) or dialogue analy-", "startOffset": 58, "endOffset": 72}, {"referenceID": 19, "context": "sis (Kalchbrenner and Blunsom, 2013).", "startOffset": 4, "endOffset": 36}, {"referenceID": 0, "context": "One has to differentiate between approaches such as Al-Rfou\u2019 et al. (2013), that learn embeddings across a large variety of languages and models such as ours, that learn joint embeddings, that is a projection into a shared semantic space across multiple languages.", "startOffset": 52, "endOffset": 75}, {"referenceID": 35, "context": "Related to our work, Yih et al. (2011) proposed S2Nets to learn joint embeddings of tf-idf vectors for comparable documents.", "startOffset": 21, "endOffset": 39}, {"referenceID": 22, "context": "More recently, Lauly et al. (2013) proposed a bag-of-words autoencoder model, where the bagof-words representation in one language is used to train the embeddings in another.", "startOffset": 15, "endOffset": 35}, {"referenceID": 22, "context": "More recently, Lauly et al. (2013) proposed a bag-of-words autoencoder model, where the bagof-words representation in one language is used to train the embeddings in another. By placing their vocabulary in a binary branching tree, the probabilistic setup of this model is similar to that of Mnih and Hinton (2009). Similarly, Sarath Chandar et al.", "startOffset": 15, "endOffset": 314}, {"referenceID": 22, "context": "More recently, Lauly et al. (2013) proposed a bag-of-words autoencoder model, where the bagof-words representation in one language is used to train the embeddings in another. By placing their vocabulary in a binary branching tree, the probabilistic setup of this model is similar to that of Mnih and Hinton (2009). Similarly, Sarath Chandar et al. (2013) train a cross-lingual encoder,", "startOffset": 15, "endOffset": 355}, {"referenceID": 26, "context": "This is effectively the linguistic extension of Ngiam et al. (2011), who used a similar method for audio and video data.", "startOffset": 48, "endOffset": 68}, {"referenceID": 16, "context": "Hermann and Blunsom (2014) propose a largemargin learner for multilingual word representations, similar to the basic additive model proposed here, which, like the approaches above, relies on a bag-of-words model for sentence representations.", "startOffset": 0, "endOffset": 27}, {"referenceID": 16, "context": "Hermann and Blunsom (2014) propose a largemargin learner for multilingual word representations, similar to the basic additive model proposed here, which, like the approaches above, relies on a bag-of-words model for sentence representations. Klementiev et al. (2012), our baseline in \u00a75.", "startOffset": 0, "endOffset": 267}, {"referenceID": 16, "context": "Earlier work, Haghighi et al. (2008), proposed a method for inducing", "startOffset": 14, "endOffset": 37}, {"referenceID": 25, "context": "This approach has recently been extended by Mikolov et al. (2013a), Mikolov et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 25, "context": "This approach has recently been extended by Mikolov et al. (2013a), Mikolov et al. (2013b), who developed a method for learning transforma-", "startOffset": 44, "endOffset": 91}, {"referenceID": 39, "context": "Using a slightly different approach, Zou et al. (2013), also learned bilingual embeddings for machine translation.", "startOffset": 37, "endOffset": 55}], "year": 2014, "abstractText": "We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data.", "creator": "TeX"}}}