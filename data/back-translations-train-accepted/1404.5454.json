{"id": "1404.5454", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2014", "title": "Stochastic Privacy", "abstract": "Online services such as web search and e-commerce applications typically rely on the collection of data about users, including details of their activities on the web. Such personal data is used to enhance the quality of service via personalization of content and to maximize revenues via better targeting of advertisements and deeper engagement of users on sites. To date, service providers have largely followed the approach of either requiring or requesting consent for opting-in to share their data. Users may be willing to share private information in return for better quality of service or for incentives, or in return for assurances about the nature and extend of the logging of data. We introduce \\emph{stochastic privacy}, a new approach to privacy centering on a simple concept: A guarantee is provided to users about the upper-bound on the probability that their personal data will be used. Such a probability, which we refer to as \\emph{privacy risk}, can be assessed by users as a preference or communicated as a policy by a service provider. Service providers can work to personalize and to optimize revenues in accordance with preferences about privacy risk. We present procedures, proofs, and an overall system for maximizing the quality of services, while respecting bounds on allowable or communicated privacy risk. We demonstrate the methodology with a case study and evaluation of the procedures applied to web search personalization. We show how we can achieve near-optimal utility of accessing information with provable guarantees on the probability of sharing data.", "histories": [["v1", "Tue, 22 Apr 2014 10:55:19 GMT  (274kb,D)", "http://arxiv.org/abs/1404.5454v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adish singla", "eric horvitz", "ece kamar", "ryen white"], "accepted": true, "id": "1404.5454"}, "pdf": {"name": "1404.5454.pdf", "metadata": {"source": "CRF", "title": "Stochastic Privacy", "authors": ["Adish Singla", "Eric Horvitz", "Ece Kamar", "Ryen White"], "emails": ["adish.singla@inf.ethz.ch", "horvitz@microsoft.com", "eckamar@microsoft.com", "ryen.white@microsoft.com"], "sections": [{"heading": "Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "Stochastic Privacy Overview", "text": "Figure 1 provides an overview of stochastic privacy in the context of a particular design of a system implementing the methodology, which consists of three main components: (i) a user preference component, (ii) a system preference component, and (iii) an optimization component to control system data collection. We then provide details of each component and then formally specify the optimization problem for selective sample modules."}, {"heading": "User Preference Component", "text": "The User Preference Component interacts with Users (e.g., during login) and enters into an agreement between Users and Service Providers on the tolerated likelihood that the User's data will be shared in return for better quality of service or incentives; the representation and collection of the tolerated privacy risk allows Users to go beyond the binary choice of yes or no when sharing data; the incentives offered to Users can be personalized based on the metallevel information available to a User (e.g. general location information from a previously shared IP address) and can range from guarantees of improved service (Krause and Horvitz 2010) to complementary software and entries in a lottery to win cash prizes (such as through the comScore service (Wikipedia-comScore 2006). Formally, we let W be the population of Users logged in for a Service (Krause and Horvitz 2010) to supplemental software and entries in a lottery to win cash prizes (such as through the comScore service (Wikipedia-comScore 2006)); Formally, we let W be the population of Users logged in for a Service that is registered for a Service (e.g., Tupel, w, w, w, w, w, w, w, w, w, w, w, w, where the IP is displayed)."}, {"heading": "System Preference Component", "text": "The goal of the service provider is to optimize the quality of the service."}, {"heading": "Optimization Component", "text": "In order to make informed decisions about data access, the system calculates the expected value of the information (VOI) for logging a particular user's activities, i.e. the marginal benefit that the application can expect by logging that user's activity (Krause and Horvitz 2008). In the absence of sufficient information about user attributes, the VOI can be small and must therefore be learned from the data. The system can randomly select a small group of users from the population who can be used to learn and improve the models of VOI calculation (explorative sample in Figure 1). For example, the service optimization system for a user coal that speaks a particular language can be used to collect logs from a subset of users to learn how the languages spoken by users are represented geographically. If privacy risk preferences are not taken into account, the VOI can be used to select which users are targeted with a sample of the service provider's utility log (s)."}, {"heading": "Optimization Problem for Selective Sampling", "text": "We are now focusing primarily on the selective scanning module and formally introducing the optimization problem. The goal is to design a scanning method M that adheres to the guarantees of stochastic privacy but optimizes the utility of the application in decisions about access to user data. In light of a budget constraint B, the goal is to select users SM: SM = arg max S W f (S) (2), subject to the limitations of stochastic privacy guarantee for w. Note that we will specify interchangeable utilities purchased by procedures as f (M) to designate f (SM), where SM is the number of users selected by method M. We will now consider an easier setting of constant privacy risks for all users and unit cost per user (thus reducing the budget constraint to a problem that can be defined more easily than a possible problem)."}, {"heading": "Selective Sampling with Stochastic Privacy", "text": "We will now propose desiderates of the selection process, discuss the severity of the problem and review several different traceable approaches as summarised in Table 1."}, {"heading": "Desirable Properties of Sampling Procedures", "text": "The problem defined in Eq.2 requires the solution of an NPhard problem of discrete optimization, even when stochastic limitations of privacy are eliminated. An algorithm for optimum resolution of this problem without privacy limitations, called OPT, is insoluble (Fig. 1998). We address this intractability by exploiting the submodular structure of the usage function f and offering procedures that provide verifiable near-optimal solutions in polynomial time. Our goal is to design procedures that fulfill the following desirable characteristics: (i) provides competitive usage opt with verifiable guarantees, (ii) preserves stochastic privacy guarantees, and (iii) runs in polynomial time."}, {"heading": "Random Sampling: RANDOM", "text": "The probability that a user w is selected by the algorithm is rRANDOMw = B / W and therefore the guarantees for the data protection risk are trivial, since B \u2264 W \u00b7 r is defined in the sense of problem 2. In general, RANDOM can perform as poorly as it likes in terms of the acquired benefits, especially for applications targeting certain user cohorts."}, {"heading": "Greedy Selection: GREEDY", "text": "Next, we examine a greedy sampling strategy that maximizes the expected marginal benefit with each iteration to guide the decision about selecting a next user to log in. Formally, GREEDY begins with an empty sentence S = \u2205. In an iteration i, it greedily selects a user s * i = arg maxw W\\ S f (S \u00b2 w) \u2212 f (S) and adds him to the current user selection S = S = s \u0445 i}. It ends when | S | = B. A basic finding by Nemhauser, Wolsey and Fisher (1978) states that the benefit obtained by this greedy selection strategy is guaranteed to be at least (1 \u2212 1 / e) (= 0.63) times as high as it is obtained by OPT. This result is narrowly below reasonable complexity assumptions (P 6 = NP) (Fig 1998). However, such greedy selection clearly violates the stochastic privacy restriction in Problem 2 - we consider the user with the highest GREW value likely to be chosen: This one of the highest risks."}, {"heading": "Sampling and Greedy Selection: RANDGREEDY", "text": "We combine the ideas of RANDOM and GREEDY to design the RANDGREEDY method, which provides guarantees of stochastic privacy and competitiveness. RANDGREEDY is an iterative method that tests a small group of users on each iteration, then greedily selects s * * and removes the entire sentence N (s) for further consideration. By maintaining the batch size N (s) \u2264 W \u00b7 r / B, the method ensures that the privacy guarantees are met. Since our user pool W is static, to reduce complexity, we consider a simpler version of RANDGREEDY, which prevents greedy selection. Formally, this corresponds to a first sample of W users using the rate r to create a subset W (W), so that the user set W (W) is static enough to reduce complexity, and then the GEDREY algorithm to execute a group W to select a user from the size B."}, {"heading": "Greedy Selection with Obfuscation: SPGREEDY", "text": "SPGREEDY uses a reverse approach of mixing RANDOM and GREEDY: A greedy selection is made, followed by obfuscation, as illustrated in Method 1. It starts from an underlying distance metric D: W \u00b7 W \u2192 R, which captures the notion of distance or dissimilarity between users. As in GREEDY, it works in iterations and selects the element s with maximum marginal benefit at each iteration. However, to ensure stochastic privacy, it disguises s with similar users, using distance metric D to create a series of E (s) of size 1 / r, then randomly tries a user out of the (s) and removes the entire set for further consideration. The privacy risk guarantees apply to the following arguments: During the execution of the algorithm, each user is selected if the user is part of the iteration (s)."}, {"heading": "Performance Analysis", "text": "We are now analyzing the performance of the proposed methods in terms of benefits compared to OPT as a starting point. First, we analyze the problem in a general context and then make a number of practical assumptions about the structure of the underlying supply function f and the population of users W. The evidence of all results is available at 1.1, available anonymously at: http: / / tinyurl.com / aaai-stocpriv-longer"}, {"heading": "General Case", "text": "s look at a user population W where only one user w * has a usage value of 1 and the rest of the users W\\ w * has a usage value of 0. OPT gets a usage value of 1 by selecting SOPT = {w *}. Let's look at any approach M that has to respect the guarantees of privacy risk. If the data protection rate is w *, M w * can select with only a maximum probability of r. Therefore, the maximum expected usage that any method M can achieve for problem 2 is r.Positive is that a trivial algorithm can always achieve a usage value of (1 \u2212 1 / e) \u00b7 r \u00b7 f (OPT) in expectation. This result can be achieved by running GREEDY to select a certain SGREEDY for problem 2 and then selecting the final solution for SREGY with a probability of default and nr."}, {"heading": "Smoothness and Diversification Assumptions", "text": "In practice, we can hope to perform much better than the worst-case results described in Theorem 1 by exploiting the underlying structures of user attributes and utility functions. We start with the assumption that there is a distance metric D: W \u00b7 W \u2192 R that captures the notion of distance or dissimilarity among users. We assume that the user population is large and that the number of users in the N\u03b1 (w) is large. Formally, we capture these requirements in Theorems 2,3.First, we consider utility functions that change gracefully with changes in input, similar to the concept of Lipschitz functions used in Mirzasoleiman et al. (2013)."}, {"heading": "Performance Bounds", "text": "Assuming smoothness (i.e., limited by GF), we can show the following limit to the usefulness of RANDGREEDY: Theorem 2. Consider problem 2 for function f with limited GF. Let SOPT be the amount returned by OPT for problem 2 after loosening privacy restrictions. For a desired < 1, let \u03b1rg = arg min\u03b1 {\u03b1: | N\u03b1 (s) | \u2265 1 / r \u00b7 log (B /), where N\u03b1 (si) and N\u03b1 (sj) = \u2205 si, sj \u0441 SOPT}. Then, with a probability of at least (1 \u2212), E [f (RANDGREEDY)] \u2265 (1 \u2212 1 / e) \u00b7 (f (OPT) \u2212 rg \u00b7 f \u00b7 GREEDY \u00b7 B)."}, {"heading": "Experimental Evaluation", "text": "We will now report on experiments we have conducted to gain insights into the performance of stochastic privacy practices through a case study on selective collection of user data to support personalization of web search."}, {"heading": "Benchmarks and Metrics", "text": "We compare the performance of the RANDGREEDY and SPGREEDY procedures with the baseline of RANDOM and GREEDY. Although RANDOM is a trivial, lower yardstick for each procedure, GREEDY is a natural upper limit to the benefit, since the OPT itself is insoluble. To analyze the robustness of our procedures, we then vary the privacy risk r. We also conducted experiments to understand the loss resulting from the obfuscation phase during the execution of SPGREEDY."}, {"heading": "Experimental Setup", "text": "We considered the application of providing location-based personalization for queries issued for the business domain (e.g. real estate, financial services, etc.) The goal is to select a number of S users who are experts in web search in this area. We are trying to use the click data of these users to improve the relevance of search results shown for the search for local businesses. The experiments are based on the use of a replacement service feature, as introduced in Equation 1. Since we are interested in certain domains of business queries, we modify the service feature in Equation 1 by limiting S to users who are experts in this area, as described below. The purchased utility can be interpreted as an average reduction in distance for each user w in the population to the closest expert queries. The primary data source for this study is obtained from interaction logs in a large web search engine. We considered a fraction of the October 2013 project query that resulted in at least one of the ten DP users having access to the western domain in the seven months of 2013."}, {"heading": "Results", "text": "Figure 2 (a) shows that both RANDGREEDY and SPGREEDY are competitive and significantly exceed the robustness of RANDGREEDY and SPGREEDY. Results in Figure 2 (b) show that the performance of RANDGREEDY and SPGREEDY vary the data protection risk r: We then vary the level of data protection risk at a fixed budget B = 50 to measure the robustness of RANDGREEDY and SPGREEDY. Results in Figure 2 (b) show that the performance of RANDGREEDY and SPGREEDY gently reduces the data protection risk, depending on the performance analysis in theorems 2,3.Analyzing performance of SPGREEDEY: Finally, we conduct experiments to greatly affect the execution of SPGREEDY and the loss of SPGREEDY relative to the acquired data protection risk of SPGREEDEY 1."}, {"heading": "Discussion", "text": "We introduced stochastic privacy, a new approach to privacy that focuses on service providers that abide by guarantees that they will not exceed a certain likelihood of logging data, and maximized information collection in accordance with those guarantees. We introduced procedures and a general system design to maximize the quality of services while respecting the privacy risks agreed with the population of users. Guidelines for this research include assessing user preferences regarding the likelihood of sharing data, including how users trade increasing privacy risks over time for improved service and monetary incentives. Options include policies and analyses based on sharing data as privacy risks. As an example, systems may one day consider decisions on logging one or more user search sessions, where privacy risks are developed with respect to the risk of sharing search sessions over time as privacy risks. As an example, systems may consider decisions on logging one or more user's search sessions, where privacy risks may be considered in terms of active sharing of data, and where additional risks may be assessed by users for use in other research models."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Online services such as web search and e-commerce applications typically rely on the collection of data about users, including details of their activities on the web. Such personal data is used to enhance the quality of service via personalization of content and to maximize revenues via better targeting of advertisements and deeper engagement of users on sites. To date, service providers have largely followed the approach of either requiring or requesting consent for opting-in to share their data. Users may be willing to share private information in return for better quality of service or for incentives, or in return for assurances about the nature and extend of the logging of data. We introduce stochastic privacy, a new approach to privacy centering on a simple concept: A guarantee is provided to users about the upperbound on the probability that their personal data will be used. Such a probability, which we refer to as privacy risk, can be assessed by users as a preference or communicated as a policy by a service provider. Service providers can work to personalize and to optimize revenues in accordance with preferences about privacy risk. We present procedures, proofs, and an overall system for maximizing the quality of services, while respecting bounds on allowable or communicated privacy risk. We demonstrate the methodology with a case study and evaluation of the procedures applied to web search personalization. We show how we can achieve nearoptimal utility of accessing information with provable guarantees on the probability of sharing data.", "creator": "LaTeX with hyperref package"}}}