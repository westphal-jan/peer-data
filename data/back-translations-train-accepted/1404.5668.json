{"id": "1404.5668", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2014", "title": "An Adversarial Interpretation of Information-Theoretic Bounded Rationality", "abstract": "Recently, there has been a growing interest in modeling planning with information constraints. Accordingly, an agent maximizes a regularized expected utility known as the free energy, where the regularizer is given by the information divergence from a prior to a posterior policy. While this approach can be justified in various ways, including from statistical mechanics and information theory, it is still unclear how it relates to decision-making against adversarial environments. This connection has previously been suggested in work relating the free energy to risk-sensitive control and to extensive form games. Here, we show that a single-agent free energy optimization is equivalent to a game between the agent and an imaginary adversary. The adversary can, by paying an exponential penalty, generate costs that diminish the decision maker's payoffs. It turns out that the optimal strategy of the adversary consists in choosing costs so as to render the decision maker indifferent among its choices, which is a definining property of a Nash equilibrium, thus tightening the connection between free energy optimization and game theory.", "histories": [["v1", "Tue, 22 Apr 2014 23:21:14 GMT  (199kb,D)", "http://arxiv.org/abs/1404.5668v1", "7 pages, 4 figures. Proceedings of AAAI-14"]], "COMMENTS": "7 pages, 4 figures. Proceedings of AAAI-14", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pedro a ortega", "daniel d lee"], "accepted": true, "id": "1404.5668"}, "pdf": {"name": "1404.5668.pdf", "metadata": {"source": "META", "title": "An Adversarial Interpretation of Information-Theoretic Bounded Rationality", "authors": ["Pedro A. Ortega", "Daniel D. Lee"], "emails": ["ope@seas.upenn.edu", "ddlee@seas.upenn.edu"], "sections": [{"heading": null, "text": "Keywords: limited rationality, free energy, game theory, legendary fennel transformation."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able to survive themselves are able to survive themselves. In fact, it is the case that they are able to survive themselves. In fact, it is the case that they are able to survive themselves. In fact, it is the case that they are able to survive themselves. In fact, it is the case that they are able to survive themselves. In fact, it is the case that they are able to survive themselves. In fact, it is the case that they are able to survive themselves. In fact, it is the case that they are able to survive themselves. In fact, it is the case that they are able to survive themselves."}, {"heading": "Aim of this Work", "text": "In spite of the identification of free energy with the security equivalent, the connection to hostile environments is not yet fully understood. Our work takes an important step towards understanding this connection. By applying a legendary fennel transformation to the regularization concept of free energy, it is shown that a one-agent-free energy optimization can be thought of as a game between the agent and an imaginary adversary. Furthermore, this result explains stochastic strategies as a strategy that protects the agent from hostile reactions of the environment."}, {"heading": "Structure of this Article", "text": "This article is divided into four sections. The aim of the next section (Section 2) is to familiarize the reader with the mathematical fundamentals underlying the planning problem in AI, and to give a basic introduction to the IT-bound rationality necessary to contextualize the results of our work. Section 3 contains our central contribution. First, Legendre-Fennel transformations are briefly examined and then applied to the free energy function to reveal the opposing assumptions implicit in the objective function. Finally, the results and conclusions are discussed in Section 4."}, {"heading": "2 Preliminaries", "text": "We review the abstract foundations of planning from the perspective of the expected benefit theory and IT-related rationality."}, {"heading": "Variational Principles", "text": "The behavior of an agent is typically characterized in two ways: (a) by directly describing its policy, or (b) by specifying a variation problem whose solution is politics. Whereas the former is a direct specification of the agent's actions under all eventualities, the latter has the advantage that it provides an explicit (typically convex) objective function that must optimize policy. Therefore, a principle of variation has additional explanatory power: it not only identifies an optimal policy, but also encodes a preference relationship to a set of realizable strategies. Crucially, the qualifier is \"optimal\" only relative to the objective function, and by no means absolute: since in any policy, one can always develop a principle of variation that is extremist by it. In AI, practically all planning algorithms (either explicit or implicit) are grasped as the maximum expected benefit problems, including popular problem classes such as multi-armed bandits, chapter Markov decision-making processes (3)."}, {"heading": "Sequential versus Single-Step Decisions", "text": "We briefly recall a basic theoretical result that will simplify our analysis. In planning, sequential decision problems can be reformulated as single-stage decision problems (i.e., instead of letting the agent select an action at each turn, one can select a single policy that the agent must follow during his interactions with the environment.2 This observation was first made in game theory, where it was proven that any extended play of forms can always be reexpressed as a normal play of forms (von Neumann and Morgenstern 1944). Consequently, we abstract from the sequential nature of the general problem by limiting our discussion to single-stage decisions that involve a single action and observation. The price we pay is to hide the dynamic structure and increase the complexity of politics, but the mathematical results can be expressed more concisely. (Subjective) Expected usefulness (von Neumann and Morgenstern 1944) is the de facto-variational principle of Russell (2010) and (victional)."}, {"heading": "Free Energy", "text": "How does an agent make decisions if he cannot comprehensively evaluate all the alternatives? IT-limited rationality (\u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 p) addresses this question by defining a new objective function (\u03b2 \u03b2 \u03b2 \u03b2 \u03b2 = information costs). Planning is conceived as a process that transforms a previous policy into a subordinate policy that generates costs measured in utilities. It is important that the agent himself is not allowed to think about the costs of this transformation3; rather, he tries to optimize the original expected benefit, but then runs out of resources. In the view of an external observer, this \"interrupted\" planning process will appear as if he explicitly optimizes the costs of this transformation3 optimization of free energy.Formally, X is a limited sentence that corresponds to realizations, and U: X \u2192 R is the utilization function. Furthermore, we let p0 (X) be a prior policy and \u03b2 R be a parameter of limitation."}, {"heading": "3 Adversarial Interpretation", "text": "Before presenting our main results, we give a brief definition of the transformations of Legendre fennel."}, {"heading": "Legendre-Fenchel Transforms", "text": "The Legendre-fennel transformation is a transformation that results in an alternative encoding of a given functional relationship. Specifically, the information contained in the function is expressed by the derivative as an independent variable. Therefore, Legendre-fennel transformations play an important role in thermodynamics and optimization, and for a function f (x): Rn \u2192 R it is defined by the variation formula? (s) = sup x X {< s, x > \u2212 f (x)}, where f? (s): Rn \u2192 R is the convex conjugate and < s, x > is the inner product of two vectors, x in Rn. The following examples are common: 1. Affine function: f (x) = ax \u2212 b f? (s) = {b = f? (s) = (s): b = a, + \u221e (s), if s 6 = a.- 2. Power function: f (Boychette) = = = \u03b1 (f = \u2212 b), f = \u2212 b = \u2212 b)."}, {"heading": "Unveiling the Adversarial Environment", "text": "Using a Legendre-fennel transformation, we can show that the concept of regulation of free energy (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) () (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate (net plate) (net plate) (net plate) (net plate) (net plate) (net plate (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate (net plate) (net plate) (net plate (net plate) (net plate) (net plate) (net plate) (net plate) (net plate (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) (net plate) ("}, {"heading": "Indifference", "text": "Theorem 4. The solution tomax p min C \u2211 x p (x) [U (x) \u2212 C (x)] [U (x) \u2212 C (x)]] [U (x) \u2212 C (x)) [U (x) \u2212 C (x)] [U (x) \u2212 min C (x)] + \u2211 x p0 (x) e \u03b2C (x) has the property that for all x x x x, U (x) \u2212 C (x) \u2212 p (x) [U (x) \u2212 c) [U (x) \u2212 c) [U (x) \u2212 x) the maximum and minimum operations can be replaced, max p (x) p (x) [U (x) \u2212 x), x) the optimal x (x), x) the maximum x (x)."}, {"heading": "4 Discussion", "text": "Based on the function of free energy, we have shown how to construct an alternative hostile interpretation that presents an equivalent problem. Conceptually, our results can be summarized as follows: 1. A regulation of the expected benefit encodes assumptions about deviations from the expected benefit; 2. The transformation of Legendre and fennel interprets free energy as a game against an environmental enemy; 3. In this transformation, it is recognized that regulating equals punishing the costs of the adversary; 4. Stochastic strategies protect against the costs of the adversary. The expected benefit alone is linear in politics and therefore encodes deterministic optimal strategies."}, {"heading": "Indifference and Nash Equilibrium", "text": "Our results establish an interesting relationship to game theory. Theorems 4 and (11) directly imply U (x) \u2212 C \u0445 (x) = U (x) \u2212 1 \u03b2 log p (x) \u03b2p0 (x) = constant for all x-X. This turns out to be a well-known characterization of the equilibrium distribution (7). However, given our current results, it takes on a different twist; it fits the well-known conclusion that a Nash balance is a strategic profile in which each player chooses a (mixed) strategy that makes the other players indifferent to his decisions (Osborne and Rubinstein 1999)."}, {"heading": "Other Regularizers", "text": "The method presented here is general and can also be applied to other regulators to make the assumptions about the imaginary adversary explicit. For example, the following list lists some examples. 1. Expected benefit: Since the regulatory term is zero, the objective function of the resulting adversary is max p min C \u2211 x p (x) [U (x) \u2212 C (x) \u2212 \u03b4C (x) 0. Here we see that zero regulation implies an adversary who is powerless, i.e. one who cannot change the utilities chosen by the agent. 2. Power Function: If the regulator is a power function, then we have the dual power functionmax p min (x) [U (x) \u2212 C (x)] + | \u03b1.This case is interesting because it includes ridge and lasso as special cases. 3. Modern portfolio theory: A remarkable case widely used in practice is modern portfolio theory."}, {"heading": "Conclusions", "text": "Our central finding shows how to extract the adversarial cost function implicit in the objective function of the agent through a suitably chosen Legendre transformation; conversely, the cost function of an adversarial environment can also be assumed and reformulated as regulated optimization; while our main motivation was to apply it to the free energy function, the transformation is general and also works in other well-known frameworks such as modern portfolio theory; the immediate application is that we can switch between the two representations and choose the one that is easier to solve, suggesting novel algorithms for solving the single agent planning problem, based on ideas from differential game theory (Dockner et al. 2001) and convex programming (Zinkevich 2003). Furthermore, this suggests that agents randomizing their decisions essentially express their information constraints by shielding themselves from adverse results."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their valuable comments and suggestions to improve this manuscript. This study was funded by grants from the US National Science Foundation, the Office of Naval Research and the Department of Transportation."}], "references": [{"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge Univeristy Press.", "citeRegEx": "Boyd and Vandenberghe,? 2004", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "A Survey of Monte Carlo Tree Search Methods", "author": ["C. Browne", "E. Powley", "D. Whitehouse", "S. Lucas", "P. Cowling", "P. Rohlfshagen", "S. Travener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games 4(1).", "citeRegEx": "Browne et al\\.,? 2012", "shortCiteRegEx": "Browne et al\\.", "year": 2012}, {"title": "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search", "author": ["R. Coulom"], "venue": "Computer and Games.", "citeRegEx": "Coulom,? 2006", "shortCiteRegEx": "Coulom", "year": 2006}, {"title": "Differential Games in Economics and Management Science", "author": ["E. Dockner", "S. Jorgensen", "N. Long", "G. Sorger"], "venue": "Cambridge University Press.", "citeRegEx": "Dockner et al\\.,? 2001", "shortCiteRegEx": "Dockner et al\\.", "year": 2001}, {"title": "Exit Probabilities and Optimal Stochastic Control", "author": ["W. Fleming"], "venue": "Applied Mathematics and Optimization 4:329\u2013346.", "citeRegEx": "Fleming,? 1977", "shortCiteRegEx": "Fleming", "year": 1977}, {"title": "The free-energy principle: a rough guide to the brain? Trends in Cognitive Science 13:293\u2013301", "author": ["K. Friston"], "venue": null, "citeRegEx": "Friston,? \\Q2009\\E", "shortCiteRegEx": "Friston", "year": 2009}, {"title": "Modification of UCT with Patterns in Monte-Carlo Go", "author": ["S. Gelly", "Y. Wang", "R. Munos", "O. Teytaud"], "venue": "Technical report, Inst. Nat. Rech. Inform. Auto. (INRIA).", "citeRegEx": "Gelly et al\\.,? 2006", "shortCiteRegEx": "Gelly et al\\.", "year": 2006}, {"title": "Robustness", "author": ["L. Hansen", "T. Sargent"], "venue": "Princeton: Princeton University Press.", "citeRegEx": "Hansen and Sargent,? 2008", "shortCiteRegEx": "Hansen and Sargent", "year": 2008}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": "Berlin: Springer.", "citeRegEx": "Hutter,? 2004", "shortCiteRegEx": "Hutter", "year": 2004}, {"title": "Optimal control as a graphical model inference problem", "author": ["H. Kappen", "V. G\u00f3mez", "M. Opper"], "venue": "Machine Learning 1:1\u201311.", "citeRegEx": "Kappen et al\\.,? 2012", "shortCiteRegEx": "Kappen et al\\.", "year": 2012}, {"title": "A linear theory for control of non-linear stochastic systems", "author": ["H. Kappen"], "venue": "Physical Review Letters 95:200201.", "citeRegEx": "Kappen,? 2005a", "shortCiteRegEx": "Kappen", "year": 2005}, {"title": "Path integrals and symmetry breaking for optimal control theory", "author": ["H. Kappen"], "venue": "Journal of Statistical Mechanics: Theory and Experiment.", "citeRegEx": "Kappen,? 2005b", "shortCiteRegEx": "Kappen", "year": 2005}, {"title": "Machine Super Intelligence", "author": ["S. Legg"], "venue": "Ph.D. Dissertation, Department of Informatics, University of Lugano.", "citeRegEx": "Legg,? 2008", "shortCiteRegEx": "Legg", "year": 2008}, {"title": "Portfolio Selection", "author": ["H. Markowitz"], "venue": "The Journal of Finance 7:77\u201391.", "citeRegEx": "Markowitz,? 1952", "shortCiteRegEx": "Markowitz", "year": 1952}, {"title": "Playing Atari with Deep Reinforcement Learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "ArXiv (1312.5602).", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Free Energy and the Generalized Optimality Equations for Sequential Decision Making", "author": ["P. Ortega", "D. Braun"], "venue": "European Workshop on Reinforcement Learning (EWRL\u201910).", "citeRegEx": "Ortega and Braun,? 2012", "shortCiteRegEx": "Ortega and Braun", "year": 2012}, {"title": "Thermodynamics as a Theory of Decision-Making with Information Processing Costs", "author": ["P.A. Ortega", "D.A. Braun"], "venue": "Proceedings of the Royal Society A 20120683.", "citeRegEx": "Ortega and Braun,? 2013", "shortCiteRegEx": "Ortega and Braun", "year": 2013}, {"title": "Monte Carlo Methods for Exact & Efficient Solution of the Generalized Optimality Equations", "author": ["P. Ortega", "D. Braun", "N. Tishby"], "venue": "IEEE International Conference on Robotics and Automation (ICRA).", "citeRegEx": "Ortega et al\\.,? 2014", "shortCiteRegEx": "Ortega et al\\.", "year": 2014}, {"title": "A Course in Game Theory", "author": ["M. Osborne", "A. Rubinstein"], "venue": "MIT Press.", "citeRegEx": "Osborne and Rubinstein,? 1999", "shortCiteRegEx": "Osborne and Rubinstein", "year": 1999}, {"title": "The Complexity of Markov Decision Processes", "author": ["C. Papadimitriou", "J. Tsitsiklis"], "venue": "Mathematics of Operations Research 12(3):441\u2013450.", "citeRegEx": "Papadimitriou and Tsitsiklis,? 1987", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "Relative entropy policy search", "author": ["J. Peters", "K. M\u00fclling", "Y. Alt\u00fcn"], "venue": "AAAI.", "citeRegEx": "Peters et al\\.,? 2010", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Modeling Bounded Rationality", "author": ["A. Rubinstein"], "venue": "Cambridge, MA: MIT Press.", "citeRegEx": "Rubinstein,? 1998", "shortCiteRegEx": "Rubinstein", "year": 1998}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": "Prentice-Hall, Englewood Cliffs, NJ, 3rd edition edition.", "citeRegEx": "Russell and Norvig,? 2010", "shortCiteRegEx": "Russell and Norvig", "year": 2010}, {"title": "Rationality and Intelligence", "author": ["S. Russell"], "venue": "Mellish, C., ed., Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, 950\u2013957. San Francisco: Morgan Kaufmann.", "citeRegEx": "Russell,? 1995", "shortCiteRegEx": "Russell", "year": 1995}, {"title": "The Foundations of Statistics", "author": ["L. Savage"], "venue": "New York: John Wiley and Sons.", "citeRegEx": "Savage,? 1954", "shortCiteRegEx": "Savage", "year": 1954}, {"title": "Theories of Bounded Rationality", "author": ["H. Simon"], "venue": "Radner, C., and Radner, R., eds., Decision and Organization. Amsterdam: North Holland Publ. 161\u2013176.", "citeRegEx": "Simon,? 1972", "shortCiteRegEx": "Simon", "year": 1972}, {"title": "A Bayesian Framework for Reinforcement Learning", "author": ["M. Strens"], "venue": "ICML.", "citeRegEx": "Strens,? 2000", "shortCiteRegEx": "Strens", "year": 2000}, {"title": "A generalized path integral approach to reinforcement learning", "author": ["E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "Journal of Machine Learning Research 11:3137\u20133181.", "citeRegEx": "Theodorou et al\\.,? 2010", "shortCiteRegEx": "Theodorou et al\\.", "year": 2010}, {"title": "Perception-Action Cycle", "author": ["N. Tishby", "D. Polani"], "venue": "Springer New York. chapter Information Theory of Decisions and Actions, 601\u2013636.", "citeRegEx": "Tishby and Polani,? 2011", "shortCiteRegEx": "Tishby and Polani", "year": 2011}, {"title": "Linearly solvable Markov decision problems", "author": ["E. Todorov"], "venue": "Advances in Neural Information Processing Systems, volume 19, 1369\u20131376.", "citeRegEx": "Todorov,? 2006", "shortCiteRegEx": "Todorov", "year": 2006}, {"title": "Legendre-Fenchel Transforms in a Nutshell", "author": ["H. Touchette"], "venue": "Technical report, Rockefeller University.", "citeRegEx": "Touchette,? 2005", "shortCiteRegEx": "Touchette", "year": 2005}, {"title": "Risk Sensitive Path Integral Control", "author": ["B. van den Broek", "W. Wiegerinck", "H. Kappen"], "venue": null, "citeRegEx": "Broek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Broek et al\\.", "year": 2010}, {"title": "A Monte-Carlo AIXI Approximation", "author": ["J. Veness", "M. Ng", "M. Hutter", "W. Uther", "D. Silver"], "venue": "Journal of Artificial Intelligence Research 40:95\u2013142.", "citeRegEx": "Veness et al\\.,? 2011", "shortCiteRegEx": "Veness et al\\.", "year": 2011}, {"title": "Theory of Games and Economic Behavior", "author": ["J. Von Neumann", "O. Morgenstern"], "venue": "Princeton: Princeton University Press.", "citeRegEx": "Neumann and Morgenstern,? 1944", "shortCiteRegEx": "Neumann and Morgenstern", "year": 1944}, {"title": "Complex Engineering Systems", "author": ["D. Wolpert"], "venue": "Perseus Books. chapter Information theory - the bridge connecting bounded rational game theory and statistical physics.", "citeRegEx": "Wolpert,? 2004", "shortCiteRegEx": "Wolpert", "year": 2004}, {"title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent", "author": ["M. Zinkevich"], "venue": "ICML, 928\u2013936.", "citeRegEx": "Zinkevich,? 2003", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 25, "context": "Simon as an alternative account to the model of perfect rationality in the face of complex decisions (Simon 1972).", "startOffset": 101, "endOffset": 113}, {"referenceID": 19, "context": "In artificial intelligence (AI), this development has been largely motivated by the intractability of exact planning in complex and uncertain environments (Papadimitriou and Tsitsiklis 1987) and the difficulty of finding domain-specific simplifications or approximations that would render planning tractable despite the latest theoretical advancements in understanding the AI problem (Hutter 2004).", "startOffset": 155, "endOffset": 190}, {"referenceID": 8, "context": "In artificial intelligence (AI), this development has been largely motivated by the intractability of exact planning in complex and uncertain environments (Papadimitriou and Tsitsiklis 1987) and the difficulty of finding domain-specific simplifications or approximations that would render planning tractable despite the latest theoretical advancements in understanding the AI problem (Hutter 2004).", "startOffset": 384, "endOffset": 397}, {"referenceID": 2, "context": "Newly proposed techniques based on Monte Carlo simulations such as Monte Carlo Tree Search (Coulom 2006; Browne et al. 2012), Thompson sampling (Strens 2000) and Free Energy (Kappen 2005b) have become the focus of much AI and reinforcement learning research due to their wide applicability", "startOffset": 91, "endOffset": 124}, {"referenceID": 1, "context": "Newly proposed techniques based on Monte Carlo simulations such as Monte Carlo Tree Search (Coulom 2006; Browne et al. 2012), Thompson sampling (Strens 2000) and Free Energy (Kappen 2005b) have become the focus of much AI and reinforcement learning research due to their wide applicability", "startOffset": 91, "endOffset": 124}, {"referenceID": 26, "context": "2012), Thompson sampling (Strens 2000) and Free Energy (Kappen 2005b) have become the focus of much AI and reinforcement learning research due to their wide applicability", "startOffset": 25, "endOffset": 38}, {"referenceID": 11, "context": "2012), Thompson sampling (Strens 2000) and Free Energy (Kappen 2005b) have become the focus of much AI and reinforcement learning research due to their wide applicability", "startOffset": 55, "endOffset": 69}, {"referenceID": 6, "context": "and their unprecedented success in difficult problems such as computer Go (Gelly et al. 2006), universal planning (Veness et al.", "startOffset": 74, "endOffset": 93}, {"referenceID": 32, "context": "2006), universal planning (Veness et al. 2011), human-level video game playing (Mnih et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 14, "context": "2011), human-level video game playing (Mnih et al. 2013) and robot learning (Theodorou, Buchli, and Schaal 2010).", "startOffset": 38, "endOffset": 56}, {"referenceID": 16, "context": "The latter group is of special theoretical interest, as it rests on a model of bounded rationality1 that yields randomized optimal policies as a consequence of resource-boundedness (Ortega and Braun 2013).", "startOffset": 181, "endOffset": 204}, {"referenceID": 4, "context": "The initial inspiration comes from the maximum entropy principle of statistical mechanics as a way to model stochastic control problems that are linearly-solvable (Fleming 1977; Kappen 2005a; Todorov 2006).", "startOffset": 163, "endOffset": 205}, {"referenceID": 10, "context": "The initial inspiration comes from the maximum entropy principle of statistical mechanics as a way to model stochastic control problems that are linearly-solvable (Fleming 1977; Kappen 2005a; Todorov 2006).", "startOffset": 163, "endOffset": 205}, {"referenceID": 29, "context": "The initial inspiration comes from the maximum entropy principle of statistical mechanics as a way to model stochastic control problems that are linearly-solvable (Fleming 1977; Kappen 2005a; Todorov 2006).", "startOffset": 163, "endOffset": 205}, {"referenceID": 7, "context": "The methods from robust control were also adopted by economists to characterize model misspecification (Hansen and Sargent 2008).", "startOffset": 103, "endOffset": 128}, {"referenceID": 16, "context": "Later, it was shown that the free energy can be derived from axioms that treat utilities and information as commensurable quantities (Ortega and Braun 2013).", "startOffset": 133, "endOffset": 156}, {"referenceID": 28, "context": "Finally, the free energy has also been motivated from arguments that parallel rate-distortion theory and information geometry by showing that the optimal policy minimizes the decision complexity subject to a constraint on the expected utility (Tishby and Polani 2011).", "startOffset": 243, "endOffset": 267}, {"referenceID": 5, "context": "It is also worth mentioning that similar approaches arise in the context of neuroscience (Friston 2009) and in game the-", "startOffset": 89, "endOffset": 103}, {"referenceID": 21, "context": "There are several approaches to bounded rationality in the literature, most notably those coming from the field of behavioral economics (Rubinstein 1998), which put emphasis on the procedural elements of decision making.", "startOffset": 136, "endOffset": 153}, {"referenceID": 34, "context": "ory (Wolpert 2004).", "startOffset": 4, "endOffset": 18}, {"referenceID": 15, "context": "Second, it has been pointed out that decision trees are nested certaintyequivalent operations (Ortega and Braun 2012).", "startOffset": 94, "endOffset": 117}, {"referenceID": 24, "context": "Expected utility (Von Neumann and Morgenstern 1944; Savage 1954) is the de facto standard variational principle in artificial intelligence (Russell and Norvig 2010).", "startOffset": 17, "endOffset": 64}, {"referenceID": 22, "context": "Expected utility (Von Neumann and Morgenstern 1944; Savage 1954) is the de facto standard variational principle in artificial intelligence (Russell and Norvig 2010).", "startOffset": 139, "endOffset": 164}, {"referenceID": 18, "context": "The Nash equilibrium then yields the decision rules (Osborne and Rubinstein 1999):", "startOffset": 52, "endOffset": 81}, {"referenceID": 23, "context": "Russell (Russell 1995).", "startOffset": 8, "endOffset": 22}, {"referenceID": 30, "context": "We refer the reader to Touchette\u2019s article (Touchette 2005) for a brief introduction or Boyd and Vanderberghe\u2019s book (Boyd and Vandenberghe 2004) for a thorough treatment.", "startOffset": 43, "endOffset": 59}, {"referenceID": 0, "context": "We refer the reader to Touchette\u2019s article (Touchette 2005) for a brief introduction or Boyd and Vanderberghe\u2019s book (Boyd and Vandenberghe 2004) for a thorough treatment.", "startOffset": 117, "endOffset": 145}, {"referenceID": 18, "context": "However, in light of our present results, it acquires a different twist; it fits with the well-known result that a Nash equilibrium is a strategy profile such that each player chooses a (mixed) strategy that renders the others players indifferent to their choices (Osborne and Rubinstein 1999).", "startOffset": 264, "endOffset": 293}, {"referenceID": 13, "context": "Here, an investor trades off asset returns versus portfolio risk, encoded into a regularization term that is quadratic in the policy (Markowitz 1952).", "startOffset": 133, "endOffset": 149}, {"referenceID": 3, "context": "This suggests novel algorithms for solving the single-agent planning problem based on ideas from differential game theory (Dockner et al. 2001) and convex programming (Zinkevich 2003).", "startOffset": 122, "endOffset": 143}, {"referenceID": 35, "context": "2001) and convex programming (Zinkevich 2003).", "startOffset": 29, "endOffset": 45}], "year": 2014, "abstractText": "Recently, there has been a growing interest in modeling planning with information constraints. Accordingly, an agent maximizes a regularized expected utility known as the free energy, where the regularizer is given by the information divergence from a prior to a posterior policy. While this approach can be justified in various ways, including from statistical mechanics and information theory, it is still unclear how it relates to decisionmaking against adversarial environments. This connection has previously been suggested in work relating the free energy to risk-sensitive control and to extensive form games. Here, we show that a single-agent free energy optimization is equivalent to a game between the agent and an imaginary adversary. The adversary can, by paying an exponential penalty, generate costs that diminish the decision maker\u2019s payoffs. It turns out that the optimal strategy of the adversary consists in choosing costs so as to render the decision maker indifferent among its choices, which is a definining property of a Nash equilibrium, thus tightening the connection between free energy optimization and game theory.", "creator": "TeX"}}}