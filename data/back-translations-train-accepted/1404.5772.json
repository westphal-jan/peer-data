{"id": "1404.5772", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2014", "title": "Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks", "abstract": "Click prediction is one of the fundamental problems in sponsored search. Most of existing studies took advantage of machine learning approaches to predict ad click for each event of ad view independently. However, as observed in the real-world sponsored search system, user's behaviors on ads yield high dependency on how the user behaved along with the past time, especially in terms of what queries she submitted, what ads she clicked or ignored, and how long she spent on the landing pages of clicked ads, etc. Inspired by these observations, we introduce a novel framework based on Recurrent Neural Networks (RNN). Compared to traditional methods, this framework directly models the dependency on user's sequential behaviors into the click prediction process through the recurrent structure in RNN. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that our approach can significantly improve the click prediction accuracy, compared to sequence-independent approaches.", "histories": [["v1", "Wed, 23 Apr 2014 10:14:41 GMT  (194kb,D)", "https://arxiv.org/abs/1404.5772v1", "Accepted by AAAI 2014"], ["v2", "Mon, 28 Apr 2014 05:56:18 GMT  (176kb,D)", "http://arxiv.org/abs/1404.5772v2", "Accepted by AAAI 2014"], ["v3", "Mon, 28 Jul 2014 13:59:03 GMT  (175kb,D)", "http://arxiv.org/abs/1404.5772v3", "Accepted by AAAI 2014"]], "COMMENTS": "Accepted by AAAI 2014", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.NE", "authors": ["yuyu zhang", "hanjun dai", "chang xu", "jun feng", "taifeng wang", "jiang bian", "bin wang 0004", "tie-yan liu"], "accepted": true, "id": "1404.5772"}, "pdf": {"name": "1404.5772.pdf", "metadata": {"source": "CRF", "title": "Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks", "authors": ["Yuyu Zhang", "Hanjun Dai", "Chang Xu", "Jun Feng", "Taifeng Wang", "Jiang Bian", "Bin Wang", "Tie-Yan Liu"], "emails": ["zhangyuyu@ict.ac.cn", "daihanjun@gmail.com", "changxu0731@gmail.com", "feng-j13@mails.tsinghua.edu.cn", "taifengw@microsoft.com", "jibian@microsoft.com", "wangbin@ict.ac.cn", "tyliu@microsoft.com"], "sections": [{"heading": "Introduction", "text": "In fact, it is the case that most people who live in the USA, live in the USA, in the USA, in the USA, where they also live, in the USA, in Europe, in Europe, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "Data Analysis on Sequential Dependency", "text": "In fact, most of them are able to move to another world, in which they are able to integrate, and in which they are able to, in which they are able to change the world."}, {"heading": "The Proposed Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Model", "text": "The architecture of a recursive neural network is shown in Figure 4. It consists of an input layer i, an output unit, a hidden layer h, and inner weight matrices. Here, we use t-N to represent the timestamp. For example, h (t) denotes the hidden state in time. Specifically, the recurring connections R between h (t \u2212 1) and h (t) can propagate sequential signals. The input layer consists of a vector i (t) representing the characteristics of the current user behavior, and the vector h (t \u2212 1) represents the values in the hidden layer calculated from the previous step. Activation values of the hidden layer and output layer are ash (t) = f (i (t) UT + h (t \u2212 1) RT), y (t) = \u03c3 (t) = \u03c3 (t) V T), where f (t \u2212 2z \u2212 1 is the predictable function."}, {"heading": "Feature Construction", "text": "In our study, we use advertising impressions as examples for both model training and testing. Based on the wealth of impression-centric information, we construct the input functions that can contain critical information to achieve an accurate CTR prediction for a given impression. All of these features can be grouped into several general categories: 1) Advertising information consists of information about ad ID, ad positioning and ad relevance with query. 2) User features include user ID and (provided by the user) semantic information. 3) Sequential features include time interval since the last impression, dwell time on the landing page of the last click event, and whether the current impression is the sequence header, i.e. whether it is the first impression for that user. With all of these diverse types of features, each ad impression can be described in a large and complex feature space. In this essay, all click prediction models of this input feature setting follow fairly so that we can predict whether its capabilities will be used."}, {"heading": "Data Organization", "text": "To use sequential information effectively to model time dependencies, the training process requires a large amount of data. Fortunately, the data in computer advertising is large enough to make RNN traceable. To get the data for sequential prediction, we rearrange the input functions along with the user dimension (i.e. we rearrange each user's historical behavior according to the timeline), especially for ads in the same search session, we rearrange them in the main and then sidebar according to their natural display order."}, {"heading": "Learning", "text": "Loss Function In our work, the loss function is defined as an averaged cross-entropy aimed at maximizing the probability of correct predictions, L = 1M M M \u2211 i = 1 (\u2212 yilog (pi) \u2212 (1 \u2212 yi) log (1 \u2212 pi)), where M is the number of training samples. The ith sample is calculated using yi \u0432 {0, 1} and \u2212 pi is the predicted click probability of the given display impression. Learning Algorithm (BPTT) RNN can be trained in the same way as normal forward-facing network using backpropagation algorithm. In this way, basically the state of the hidden layer from the previous time step is considered simply as an additional input. With only one hidden layer, the network tries to optimize the prediction of the next sample and the previous hidden state. However, no effort is directly affected by longer contexts, which can affect the N.A."}, {"heading": "Inference", "text": "Unlike traditional neural networks, RNN has a recurring layer to store the previous hidden state. In the inference phase, we still need to store the hidden state of the previous sample, and using the recurring weights R.Figure 5 illustrates the test process. The test data is also organized as ordered sequences of user behavior. We forward current sample characteristics along with the hidden state of the previous sample to obtain the current hidden state. Then, we make the prediction and replace the stored hidden state with current values. Here, we only capture the hidden state of the last test sample, regardless of how many unfolding steps there are in the BPTT training process."}, {"heading": "Experiments", "text": "This section first describes the settings of our experiments and then reports on the experimental results."}, {"heading": "Data Setting", "text": "In order to verify whether the RNN model we propose can really help improve the accuracy of click prediction, we conduct a series of experiments based on the click logs of a commercial search engine. In particular, we collect half of all logs from 9 to 22 November 2013 as our experimental data set. And we sample a group of search engine users (fully anonymised) and collect their corresponding events from the entire original traffic. Finally, we collect over 7 million advertising impressions during this period. Afterwards, we use the data from the first week to train click prediction models, and apply these models to the data from the second week for testing purposes. Detailed statistics of the data set can be found in Table 1."}, {"heading": "Evaluation Metrics", "text": "In our work, there are several models that can be applied to predict the click probability for advertising impressions in the test dataset. We use recorded user actions, i.e. click or non-click, in logs as the true terms. To measure the overall performance of each model, we follow the common practice of earlier click prediction research in sponsored search and use Area Under ROC Curve (AUC) and Relative Information Gain (RIG) as evaluation metrics (Graepel et al. 2010; Xiong et al. 2012; Wang et al. 2013)."}, {"heading": "Compared Methods", "text": "To investigate the effectiveness of the model, we compare the performance of our RNN model with other classic click prediction models, including logistic regression (LR) and neural networks (NN), with identical features as described above. We use LR and NN as baseline models for the following reasons: 1) A number of previous studies (Richardson, Dominowska and Ragno 2007; McMahan et al. 2013; Wang et al. 2013) have shown that they are state-of-the-art click prediction models in the funded search. 2) LR and NN models ignore the sequential dependence between the data, while our RNN-based framework is able to model such information. By comparing them, we will see if RNN can successfully use dependencies in the data sequence to improve the accuracy of click prediction."}, {"heading": "Experimental Results", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "Conclusion and Future Work", "text": "Unlike traditional click prediction models, our method uses time dependence in the order of user behavior through the recurring structure. A number of experiments show that our method outperforms state-of-the-art click prediction models in various settings. In the future, we will continue in this direction in several aspects: 1) The sequence is currently based on the user level. We will study different types of sequence building methods, e.g. through (user, ad) pairs, (user, query) pairs, advertisers, or even all users at the level of the entire system. 2) We will derive the importance of dependence learned from RNN through a deep understanding of the RNN structure, which could help us better utilize the recurring part property. 3) Recently, some research (Hermans and Schrauwen 2013) has been conducted on Deep Recurrent Neural Networks (DRN), which will help us plan the \"structure together with the click.\""}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks. EMNLP", "author": ["Auli"], "venue": null, "citeRegEx": "Auli,? \\Q2013\\E", "shortCiteRegEx": "Auli", "year": 2013}, {"title": "Neural probabilistic language models", "author": ["Bengio"], "venue": "In Innovations in Machine Learning", "citeRegEx": "Bengio,? \\Q2006\\E", "shortCiteRegEx": "Bengio", "year": 2006}, {"title": "Time series analysis: forecasting and control", "author": ["Jenkins Box", "G.E. Reinsel 2013] Box", "G.M. Jenkins", "G.C. Reinsel"], "venue": null, "citeRegEx": "Box et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Box et al\\.", "year": 2013}, {"title": "Sponsored search: A brief history. Bulletin of the American Society for Information Science and Technology 32(2):12\u201313", "author": ["Fain", "D.C. Pedersen 2006] Fain", "J.O. Pedersen"], "venue": null, "citeRegEx": "Fain et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fain et al\\.", "year": 2006}, {"title": "Web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["Graepel"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML-10),", "citeRegEx": "Graepel,? \\Q2010\\E", "shortCiteRegEx": "Graepel", "year": 2010}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 31(5):855\u2013868", "citeRegEx": "Graves,? \\Q2009\\E", "shortCiteRegEx": "Graves", "year": 2009}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "M. Schrauwen 2013] Hermans", "B. Schrauwen"], "venue": null, "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "The sum of its parts: reducing sparsity in click estimation with query segments. Information Retrieval 14(3):315\u2013336", "author": ["Hillard"], "venue": null, "citeRegEx": "Hillard,? \\Q2011\\E", "shortCiteRegEx": "Hillard", "year": 2011}, {"title": "Sponsored search: An overview of the concept, history, and technology", "author": ["Jansen", "B.J. Mullen 2008] Jansen", "T. Mullen"], "venue": "International Journal of Electronic Business", "citeRegEx": "Jansen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jansen et al\\.", "year": 2008}, {"title": "Introduction to modern time series analysis", "author": ["Wolters Kirchgassner", "G. Hassler 2012] Kirchgassner", "J. Wolters", "U. Hassler"], "venue": null, "citeRegEx": "Kirchgassner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kirchgassner et al\\.", "year": 2012}, {"title": "Recurrent neural network based language modeling in meeting recognition", "author": ["Kombrink"], "venue": null, "citeRegEx": "Kombrink,? \\Q2011\\E", "shortCiteRegEx": "Kombrink", "year": 2011}, {"title": "Ad click prediction: a view from the trenches", "author": ["McMahan"], "venue": null, "citeRegEx": "McMahan,? \\Q2013\\E", "shortCiteRegEx": "McMahan", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Mikolov"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov,? \\Q2010\\E", "shortCiteRegEx": "Mikolov", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Mikolov"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov,? \\Q2011\\E", "shortCiteRegEx": "Mikolov", "year": 2011}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["Mikolov"], "venue": "In Proc. IEEE workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Mikolov,? \\Q2011\\E", "shortCiteRegEx": "Mikolov", "year": 2011}, {"title": "Optimizing relevance and revenue in ad search: a query substitution approach", "author": ["Radlinski"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development", "citeRegEx": "Radlinski,? \\Q2008\\E", "shortCiteRegEx": "Radlinski", "year": 2008}, {"title": "Predicting clicks: estimating the click-through rate for new ads", "author": ["Dominowska Richardson", "M. Ragno 2007] Richardson", "E. Dominowska", "R. Ragno"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "Richardson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2007}, {"title": "Learning representations by back-propagating errors. Cognitive modeling 1:213", "author": ["Hinton Rumelhart", "D.E. Williams 2002] Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 2002}, {"title": "Data-driven text features for sponsored search click prediction", "author": ["\u00c7etin Shaparenko", "B. Iyer 2009] Shaparenko", "\u00d6. \u00c7etin", "R. Iyer"], "venue": "In Proceedings of the Third International Workshop on Data Mining and Audience Intelligence for Advertising,", "citeRegEx": "Shaparenko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shaparenko et al\\.", "year": 2009}, {"title": "Exploring consumer psychology for click prediction in sponsored search", "author": ["Wang"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM", "citeRegEx": "Wang,? \\Q2013\\E", "shortCiteRegEx": "Wang", "year": 2013}, {"title": "Relational click prediction for sponsored search", "author": ["Xiong"], "venue": "In Proceedings of the fifth ACM international conference on Web search and data mining,", "citeRegEx": "Xiong,? \\Q2012\\E", "shortCiteRegEx": "Xiong", "year": 2012}, {"title": "Temporal click model for sponsored search", "author": ["Manavoglu Xu", "W. Cantu-Paz 2010] Xu", "E. Manavoglu", "E. Cantu-Paz"], "venue": "In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}], "referenceMentions": [], "year": 2014, "abstractText": "Click prediction is one of the fundamental problems in sponsored search. Most of existing studies took advantage of machine learning approaches to predict ad click for each event of ad view independently. However, as observed in the real-world sponsored search system, user\u2019s behaviors on ads yield high dependency on how the user behaved along with the past time, especially in terms of what queries she submitted, what ads she clicked or ignored, and how long she spent on the landing pages of clicked ads, etc. Inspired by these observations, we introduce a novel framework based on Recurrent Neural Networks (RNN). Compared to traditional methods, this framework directly models the dependency on user\u2019s sequential behaviors into the click prediction process through the recurrent structure in RNN. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that our approach can significantly improve the click prediction accuracy, compared to sequence-independent approaches.", "creator": "LaTeX with hyperref package"}}}