{"id": "1405.0042", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2014", "title": "Learning with Incremental Iterative Regularization", "abstract": "We study the learning algorithm corresponding to the incremental gradient descent defined by the empirical risk over an infinite dimensional hypotheses space. We consider a statistical learning setting and show that, provided with a universal step-size and a suitable early stopping rule, the learning algorithm thus obtained is universally consistent and derive finite sample bounds. Our results provide a theoretical foundation for considering early stopping in online learning algorithms and shed light on the effect of allowing for multiple passes over the data.", "histories": [["v1", "Wed, 30 Apr 2014 21:48:34 GMT  (58kb)", "http://arxiv.org/abs/1405.0042v1", "23 pages,1 figure"], ["v2", "Mon, 15 Jun 2015 13:12:12 GMT  (140kb,D)", "http://arxiv.org/abs/1405.0042v2", "30 pages"]], "COMMENTS": "23 pages,1 figure", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC math.PR", "authors": ["lorenzo rosasco", "silvia villa"], "accepted": true, "id": "1405.0042"}, "pdf": {"name": "1405.0042.pdf", "metadata": {"source": "CRF", "title": "REGULARIZATION BY EARLY STOPPING FOR ONLINE LEARNING ALGORITHMS", "authors": ["LORENZO ROSASCO", "SILVIA VILLA"], "emails": [], "sections": [{"heading": null, "text": "In this context, however, each algorithm variant becomes a new starting paper. In this context, we are interested in multiple results. Online-learning, incremental-gradient-descent, consistency-descent, consistency-descent, consistency-descent, consistency-descent, consistency-descent Algorithm-descent Algorithm-descent, algorithm-descent Algorithm-descent, algorithm-descent, algorithm-descent, consistency-descent Algorithm-descent Algorithm-descent, algorithm-descent-descent, algorithm-descent, algorithm-descent, algorithm-descent, algorithm-descent-descent, algorithm-descent, algorithm-descent, algorithm-descent-descent, algorithm-descent, algorithm-descent-descent, algorithm-descent, algorithm-descent-descent, algorithm-descent, algorithm-descent-descent, algorithm-descent-descent, algorithm-descent-descent, algorithm-descent, algorithm-descent, algorithm-descent-descent, algorithm-descent-we-descent-descent, algorithm-descent, algorithm-descent, algorithm-descent, algorithm-descent-descent, algorithm-descent-descent, algorithm-we-descent, algorithm-descent, algorithm-descent-descent, algorithm-descent-descent, algorithm-we-descent, algorithm-descent, algorithm-descent, algorithm-descent-descent-descent, algorithm-descent, algorithm-descent-we-descent, algorithm-descent, algorithm-descent algorithm-descent-descent, algorithm-descent-we-descent algorithm-descent, algorithm-descent-descent, algorithm-descent, algorithm-descent algorithm-"}], "references": [{"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Trans. Amer. Math. Soc,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1950}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n)", "author": ["F. Bach", "E. Moulines"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "On regularization algorithms in learning theory", "author": ["Frank Bauer", "Sergei Pereverzev", "Lorenzo Rosasco"], "venue": "Journal of complexity,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "A new class of incremental gradient methods for least squares problems", "author": ["Dimitri P. Bertsekas"], "venue": "SIAM J. Optim.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Gradient convergence in gradient methods with errors", "author": ["Dimitri P Bertsekas", "John N Tsitsiklis"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Optimal learning rates for kernel conjugate gradient regression", "author": ["G. Blanchard", "N. Kr\u00e4mer"], "venue": "In Advances in Neural Inf. Proc. Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Optimal rates for regularized least-squares algorithm", "author": ["A. Caponnetto", "E. De Vito"], "venue": "Found. Comput. Math.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Adaptive rates for regularization operators in learning theory", "author": ["A. Caponnetto", "Yuan Yao"], "venue": "Analysis and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "On the mathematical foundations of learning", "author": ["Felipe Cucker", "Steve Smale"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Learning Theory: An Approximation Theory Viewpoint", "author": ["Felipe Cucker", "Ding Xuan Zhou"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Model selection for regularized least-squares algorithm in learning theory", "author": ["E. De Vito", "A. Caponnetto", "L. Rosasco"], "venue": "Found. Comput. Math.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Learning from examples as an inverse problem", "author": ["E. De Vito", "L. Rosasco", "A. Caponnetto", "U. De Giovannini", "F. Odone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "A Probabilistic Theory of Pattern Recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "Number 31 in Applications of mathematics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Sample-based non-uniform random variate generation", "author": ["Luc Devroye"], "venue": "In Proceedings of the 18th conference on Winter simulation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Regularization of inverse problems", "author": ["H.W. Engl", "M. Hanke", "A. Neubauer"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "Random design analysis of ridge regression", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Online learning with kernels", "author": ["Jyrki Kivinen", "Alexander J Smola", "Robert C Williamson"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Stochastic approximation algorithms and applications, volume 35 of Applications of Mathematics (New York)", "author": ["Harold J. Kushner", "G. George Yin"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets", "author": ["Nicolas Le Roux", "Mark Schmidt", "Francis Bach"], "venue": "arXiv preprint arXiv:1202.6258,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Spectral algorithms for supervised learning", "author": ["Laura Lo Gerfo", "Lorenzo Rosasco", "Francesca Odone", "Ernesto De Vito", "Alessandro Verri"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Regularization in kernel learning", "author": ["Shahar Mendelson", "Joseph Neeman"], "venue": "The Annals of Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Incremental subgradient methods for nondifferentiable optimization", "author": ["Angelia Nedic", "Dimitri P Bertsekas"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optim.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k)", "author": ["Y. Nesterov"], "venue": "Doklady AN SSSR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1983}, {"title": "Optimum bounds for the distributions of martingales in Banach spaces", "author": ["Iosif Pinelis"], "venue": "Ann. Probab.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1994}, {"title": "Early stopping for non-parametric regression: An optimal data-dependent stopping rule", "author": ["Garvesh Raskutti", "Martin J Wainwright", "Bin Yu"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Methods of modern mathematical physics. I. Functional analysis", "author": ["Michael Reed", "Barry Simon"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1972}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["Ingo Steinwart"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "Support Vector Machines", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Optimal rates for regularized least squares regression", "author": ["Ingo Steinwart", "Don R. Hush", "Clint Scovel"], "venue": "In COLT,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Online learning as stochastic approximation of regularization paths", "author": ["Pierre Tarr\u00e8s", "Yuan Yao"], "venue": "arXiv preprint arXiv:1103.5538,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Introduction to functional analysis", "author": ["Angus E. Taylor", "David C. Lay"], "venue": "2nd ed. (Reprint of the orig", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1980}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "On early stopping in gradient descent learning", "author": ["Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto"], "venue": "Constructive Approximation,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Online gradient descent learning algorithms", "author": ["Yiming Ying", "Massimiliano Pontil"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Sums and Gaussian vectors, volume 1617 of Lecture Notes in Mathematics", "author": ["Vadim Yurinsky"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1995}, {"title": "Learning bounds for kernel regression using effective data dimensionality", "author": ["Tong Zhang"], "venue": "Neural Computation,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2005}, {"title": "Boosting with early stopping: Convergence and consistency", "author": ["Tong Zhang", "Bin Yu"], "venue": "Annals of Statistics,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 36, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 2, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 5, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 7, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 28, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 19, "context": "The situation typically analyzed in this context is the one where each iteration of the algorithm corresponds to a new input-output pair [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "[25], we are not interested in convergence of the algorithm to the minimum of the empirical risk, but rather to the one of the expected risk.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "In particular, the gradients are not bounded, unlike in most studies in stochastic optimization [26].", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived.", "startOffset": 92, "endOffset": 103}, {"referenceID": 33, "context": "Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived.", "startOffset": 92, "endOffset": 103}, {"referenceID": 37, "context": "Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived.", "startOffset": 92, "endOffset": 103}, {"referenceID": 1, "context": "In [2] a single pass over the data is shown to suffice, however the analysis is restricted to a finite dimensional setting, and the generalization performance is empirically shown to be still increasing after the first epoch.", "startOffset": 3, "endOffset": 6}, {"referenceID": 33, "context": "In [34] and [38] the step size and/or a penalization parameter need to be chosen in a distribution dependent way (or by cross validation) and multiple passes over \u2217 DIBRIS, Universit\u00e0 di Genova, Via Dodecaneso, 35, 16146, Genova, Italy, (lrosasco@mit.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "In [34] and [38] the step size and/or a penalization parameter need to be chosen in a distribution dependent way (or by cross validation) and multiple passes over \u2217 DIBRIS, Universit\u00e0 di Genova, Via Dodecaneso, 35, 16146, Genova, Italy, (lrosasco@mit.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "[32] Lemma A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Further, consider a separable reproducing kernel Hilbert space (RKHS) H [1], with inner product (norm) denoted by \u3008\u00b7, \u00b7\u3009H (\u2016\u00b7\u2016H), and a measurable reproducing kernelK : X\u00d7X \u2192 R.", "startOffset": 72, "endOffset": 75}, {"referenceID": 31, "context": "2) When H is universal [32] this is exactly universal consistency, see for example [15].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "2) When H is universal [32] this is exactly universal consistency, see for example [15].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "As it is known, the latter result can only be derived under suitable assumptions on \u03c1 and it is equivalent to considering learning rates [16, 31].", "startOffset": 137, "endOffset": 145}, {"referenceID": 30, "context": "As it is known, the latter result can only be derived under suitable assumptions on \u03c1 and it is equivalent to considering learning rates [16, 31].", "startOffset": 137, "endOffset": 145}, {"referenceID": 25, "context": "Note however that due to the lack of strong convexity, unboundedness of X and unboundedness of the gradients, the classical assumptions required to apply stochastic gradient descent are not satisfied [26].", "startOffset": 200, "endOffset": 204}, {"referenceID": 4, "context": "We consider the estimator obtained applying the incremental gradient descent (IGD) algorithm [5, 4] to empirical risk minimization, inf f\u2208H \u00ca(f), \u00ca(f) = 1 m m \u2211", "startOffset": 93, "endOffset": 99}, {"referenceID": 3, "context": "We consider the estimator obtained applying the incremental gradient descent (IGD) algorithm [5, 4] to empirical risk minimization, inf f\u2208H \u00ca(f), \u00ca(f) = 1 m m \u2211", "startOffset": 93, "endOffset": 99}, {"referenceID": 24, "context": "stochastic [25], can be considered and might lead to different behaviors, but we leave the analysis of these latter cases for future study.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": ", k = i (c t )k, k 6= i The above algorithm is closely related, yet different, to the one discussed in [19], where an explicit regularization is considered (choose \u03bbk > 0 in the following Equation (3.", "startOffset": 103, "endOffset": 107}, {"referenceID": 40, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 36, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 2, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 5, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 28, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 21, "context": "Indeed, early stopping has long been applied as a heuristic to achieve regularization, especially in the context of neural networks [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 18, "context": "Here, we added an explicit regularization parameter \u03bb as in [19], see also (3.", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "The question of the consistency of online learning algorithms when H can be infinite dimensional has been considered in [34, 38] (see also references therein).", "startOffset": 120, "endOffset": 128}, {"referenceID": 37, "context": "The question of the consistency of online learning algorithms when H can be infinite dimensional has been considered in [34, 38] (see also references therein).", "startOffset": 120, "endOffset": 128}, {"referenceID": 33, "context": "In [34] it is shown that if the step size \u03b3k and the regularization parameter \u03bbk are chosen as suitable functions of the number of points, then the corresponding algorithm can be universally consistent with probability one.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "In [38] it is shown that indeed similar results can be derived for \u03bbk = 0 for all k \u2208 N, but an horizon, that is the total number of points to be considered, needs to be known a priori to appropriately choose the step-size.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "Recently, a similar analysis, \u03bbk = 0 for all k \u2208 N, is developed in [2] in a finite dimensional setting.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "increases, see [5] and references therein.", "startOffset": 15, "endOffset": 18}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "If the data are indeed generated by a stochastic process, then there is a classic approach, sometimes called online-to-batch conversion [9], to convert regret bounds to expected risk bounds.", "startOffset": 136, "endOffset": 139}, {"referenceID": 12, "context": "2) is fairly standard (see [13], and [12, Section 4] for a discussion).", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Lower bounds are known [7, 33] under assumption (4.", "startOffset": 23, "endOffset": 30}, {"referenceID": 32, "context": "Lower bounds are known [7, 33] under assumption (4.", "startOffset": 23, "endOffset": 30}, {"referenceID": 39, "context": "6) This latter property can be interpreted as a measure of the effective dimensionality of the hypotheses space [40, 7].", "startOffset": 112, "endOffset": 119}, {"referenceID": 6, "context": "6) This latter property can be interpreted as a measure of the effective dimensionality of the hypotheses space [40, 7].", "startOffset": 112, "endOffset": 119}, {"referenceID": 10, "context": "2) implies that the infimum of the expected risk over H is achieved [11], and it is known that sharp bounds are harder to get if r < 1/2 (see discussion in [33]).", "startOffset": 68, "endOffset": 72}, {"referenceID": 32, "context": "2) implies that the infimum of the expected risk over H is achieved [11], and it is known that sharp bounds are harder to get if r < 1/2 (see discussion in [33]).", "startOffset": 156, "endOffset": 160}, {"referenceID": 16, "context": "The bound is proved generalizing classical results in inverse problems and is known to be essentially sharp [17].", "startOffset": 108, "endOffset": 112}, {"referenceID": 39, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 6, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 32, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 23, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 17, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 22, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 158, "endOffset": 162}, {"referenceID": 40, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 36, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 2, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 5, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 7, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 28, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 33, "context": "As we mentioned before, online learning algorithms are considered in [34, 38] where optimal bounds are derived in the capacity independent setting.", "startOffset": 69, "endOffset": 77}, {"referenceID": 37, "context": "As we mentioned before, online learning algorithms are considered in [34, 38] where optimal bounds are derived in the capacity independent setting.", "startOffset": 69, "endOffset": 77}, {"referenceID": 1, "context": "The bound in the finite dimensional setting derived in [2] are also optimal.", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "Our analysis follows the one in [8].", "startOffset": 32, "endOffset": 35}, {"referenceID": 28, "context": "We end noting that, while an hold-out procedure requires splitting the data, it does not worsen the computational complexity of the algorithm, unlike other model selection criterions [29].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "Particularly, stochastic variants of the IGD algorithms, where the indices in each cycle are randomly selected [5], and also accelerated version as proposed in [27].", "startOffset": 111, "endOffset": 114}, {"referenceID": 26, "context": "Particularly, stochastic variants of the IGD algorithms, where the indices in each cycle are randomly selected [5], and also accelerated version as proposed in [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 2, "context": "Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration).", "startOffset": 59, "endOffset": 68}, {"referenceID": 5, "context": "Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration).", "startOffset": 59, "endOffset": 68}, {"referenceID": 7, "context": "Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration).", "startOffset": 59, "endOffset": 68}, {"referenceID": 2, "context": "In particular in [3] it is shown that a variant of gradient descent, sometimes called the \u03bd-method, can obtain the same generalization guarantees of non accelerated gradient descent learning, but using much fewer iterations.", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "REFERENCES [1] N.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Dimitri P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Dimitri P Bertsekas and John N Tsitsiklis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Felipe Cucker and Steve Smale.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Felipe Cucker and Ding Xuan Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Luc Devroye.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Daniel Hsu, Sham M Kakade, and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Jyrki Kivinen, Alexander J Smola, and Robert C Williamson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Harold J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Nicolas Le Roux, Mark Schmidt, and Francis Bach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Laura Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Shahar Mendelson and Joseph Neeman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Angelia Nedic and Dimitri P Bertsekas.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Iosif Pinelis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] Garvesh Raskutti, Martin J Wainwright, and Bin Yu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Michael Reed and Barry Simon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] Ingo Steinwart.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] Ingo Steinwart and Andreas Christmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] Ingo Steinwart, Don R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] Pierre Tarr\u00e8s and Yuan Yao.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] Angus E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] Joel A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] Yiming Ying and Massimiliano Pontil.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] Vadim Yurinsky.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] Tong Zhang and Bin Yu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "2) The following concentration inequality due to [28] (see also [39] and [34, Proposition A.", "startOffset": 49, "endOffset": 53}, {"referenceID": 38, "context": "2) The following concentration inequality due to [28] (see also [39] and [34, Proposition A.", "startOffset": 64, "endOffset": 68}, {"referenceID": 35, "context": "While more refined concentration inequality could be considered [36], this would affect only the constants in the bound which are not the main focus of this paper.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "3, we will also use a variant of the previous Bernstein concentration inequality, which is taken from [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 13, "context": "1, taking into account that \u2016T \u2016HS \u2264 \u03ba and \u2016Txi\u2016HS \u2264 \u03ba (see also [14]) LEMMA C.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "22 and Exercise 28 in [30]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "T \u2016 = sup x\u2208[0,1] \u2223", "startOffset": 12, "endOffset": 17}], "year": 2017, "abstractText": "Abstract. We study the learning algorithm corresponding to the incremental gradient descent defined by the empirical risk over an infinite dimensional hypotheses space. We consider a statistical learning setting and show that, provided with a universal step-size and a suitable early stopping rule, the learning algorithm thus obtained is universally consistent and derive finite sample bounds. Our results provide a theoretical foundation for considering early stopping in online learning algorithms and shed light on the effect of allowing for multiple passes over the data.", "creator": "LaTeX with hyperref package"}}}