{"id": "1405.0133", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2014", "title": "Geodesic Distance Function Learning via Heat Flow on Vector Fields", "abstract": "Learning a distance function or metric on a given data manifold is of great importance in machine learning and pattern recognition. Many of the previous works first embed the manifold to Euclidean space and then learn the distance function. However, such a scheme might not faithfully preserve the distance function if the original manifold is not Euclidean. Note that the distance function on a manifold can always be well-defined. In this paper, we propose to learn the distance function directly on the manifold without embedding. We first provide a theoretical characterization of the distance function by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then learn the distance function itself. Specifically, we set the gradient field of a local distance function as an initial vector field. Then we transport it to the whole manifold via heat flows on vector fields. Finally, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm.", "histories": [["v1", "Thu, 1 May 2014 11:10:36 GMT  (6321kb,D)", "https://arxiv.org/abs/1405.0133v1", null], ["v2", "Thu, 8 May 2014 05:07:21 GMT  (6322kb,D)", "http://arxiv.org/abs/1405.0133v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.DG stat.ML", "authors": ["binbin lin", "ji yang", "xiaofei he", "jieping ye"], "accepted": true, "id": "1405.0133"}, "pdf": {"name": "1405.0133.pdf", "metadata": {"source": "CRF", "title": "Geodesic Distance Function Learning via heat flow on Vector Fields", "authors": ["Binbin Lin", "Ji Yang", "Xiaofei He", "Jieping Ye"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "2 Characterization of Distance Functions using Gradient Fields", "text": "Let us define (M, g) a d-dimensional belt manifold, which deals with the question of whether there is a natural measure of the similarity between two data points x and y on the manifold. In this paper we will briefly present the most relevant concepts. A detailed and much more technical introduction can be included in the appendix. We will first show how to map a metric structure on the manifold. For each point on the manifold, a Riemannian metric tensor g to p is an inner product gp."}, {"heading": "3 Geodesic Distance Function Learning", "text": "In the last section, we show that the distance function can be characterized by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then the distance function itself."}, {"heading": "3.1 Geodesic Distance Learning", "text": "rE \"s,\" according to the author, \"is able to be in the position in which he sees himself, in which he is able to be in the position.\" (mA) D \"i\" n, according to the author, \"is in the position in which he is able to be in the position.\" (sDa \"s,\" according to the author, \"he has been able to be in the position.\" (mA)"}, {"heading": "3.2 Implementation", "text": "Considering the fact that we are able to edit the space in which we find ourselves, we are able to edit the space. (D) We will see ourselves able to understand the space in which we find ourselves. (D) We will see ourselves able to understand the world in which we live. (D) We will not perceive the world in which we live. (D) We will not perceive the world in which we live. (D) We will see the world in which we live. (D) We will understand the world in which we live. (D) We will not perceive the world in which we live. (D) We will live the world in which we live. (D) We will live the world in which we live. (D) We will live the world in which we live. (D) We will live the world in which we live. (D) We will live the world in which we live. (D) We will live the world in which we. (D) We will live the world in which we live. (D) We will live the world in which we. (D) We will live the world in which we live. (D) We will live the world in which we. (D) We will live the world in which we. (D) We will live the world in which we. (D) We will live the world in which we. (D) We will live the world in which we. (D) We will live in which we. (D) We will live the world in the world in which we. (D. (D) We will be the world in which we. (D) We will be the world in which we. (D."}, {"heading": "3.3 Computation Complexity Analysis", "text": "The computational complexity of our proposed geodetic distance learning algorithm (GDL) is dominated by three parts: Search for k-nearest neighbors, Calculation of local tangent spaces, Calculation of Qij and Solution of the sparse linear system Eq. (5). For the k-nearest neighbor search, the complexity is O (m + k) n2), where O (mn2) is the complexity of calculating the local tangent space for all data points and O (kn2) is the complexity of searching for the k-nearest neighbor for all data points. The complexity for local PCA is O (mk2). Therefore, the complexity for calculating the local tangent space for all data points is O (mnk2) the complexity of the k-points."}, {"heading": "3.4 Related Work and Discussion", "text": "Our approach is based on the idea of vector-field regularization, which is similar to vector-diffusion maps (VDM, [23]). Both methods use vector fields to discover the geometry of the manifold. However, VDM and our approach differ in several key aspects: First, they solve different problems. Second, they use different approximation methods as we try to learn the geodesic distance function directly on the manifold. It is worth noting that the vector diffusion distance is a variation of geodetic distance. Second, they approach parallel transport by learning an orthogonal transformation and we project it from [15]. The GDL can also be seen as a generalization of the heat flow-field-vector-vector-vector-field-function-vector-method. [6] Both methods deal with the heat flow-field-vector-vector-vector-function-vector-vector-method."}, {"heading": "4 Experiments", "text": "In this section we evaluate empirically the effectiveness of our proposed algorithm for geodetic distance learning (GDL) in comparison to three representative distance-metric learning algorithms: Laplacian eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian eigenmaps (HLLE, [8]) as well as two state-of-the-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]). Since LE, MVU and HLLE cannot obtain the distance function directly, we first calculate the embedding and then calculate the Euclidean distance between the data points in the embedded Euclidean space. We use t = 1 empirically for GDL in all experiments, since GDL operates very stable when t-variations vary."}, {"heading": "4.1 Geodesic Distance Learning", "text": "A simple synthetic example is in Fig. 3. We sample 2000 data points from a torus. It is a 2-dimensional multiplicity in the 3-dimensional Euclidean space. The base point is marked by the black dot on the right side of the torus. Fig. 3 (a) shows the geodetic distance function of the ground, which is calculated by shortest distance. Fig. 3 (b) - (d) and (f) - (h) visualize the distance functions learned by various algorithms. To better evaluate the results, we calculate the error using the equation 1n \u2211 i = 1 | f (xi) \u2212 d (xq, xi) |, where f (xi) represents the distance learned and {d (xq, xi)} the ground truth distance. To better evaluate the effect of the scale effects, {f (xi)} and {d (xq, xi)} are shown on the range [xq], as shown by xb."}, {"heading": "4.2 Image Retrieval", "text": "In this section, we apply our GDL algorithm to the retrieval problem in the real-world image databases. In our experiments, we use two sets of data from the real world. The first set comes from the CMU-PIE face database [22], which contains 32 \u00d7 32 cropped face images of 68 people. We select the frontal pose (C27) with different lighting conditions, giving us 42 images per person. The second set contains 5000 images from 50 semantic categories from the Corel database. Each image is extracted to be a 297-dimensional feature vector. Both of the two image sets we use have category labels. For each set, we randomly select 10 images from each category as queries and the average retrieval performance across all queries. We use precision, callback, and average precision (MAP) precision (MAP, [16]) to evaluate the relevant algorithms."}, {"heading": "5 Conclusion", "text": "In this paper, we examine geodetic distance from a vector field perspective. We offer theoretical analyses to accurately characterize the geodetic distance function, and propose a novel heat flux on vector fields to learn it. Our experimental results on synthetic and real data demonstrate the effectiveness of the proposed method. Future work includes the development of machine learning theory and the development of efficient algorithms that use heat flow on vector fields as well as other general partial differential equations."}, {"heading": "6 Appendix. Justification", "text": "We show first that the solution of Eq. (1) is equivalent to the solution of the thermal equation on vector fields. (1) According to the Bochner technique (20), with the corresponding boundary conditions that we have. (1) It is equivalent to the solution of the thermal equation on vector fields. (3) We define the inner product (\u00b7 \u00b7) in the space of vector fields as (X, Y) = \u2264 M g (X, Y) dx. Then we can define E (V) as E (V) = (V \u2212 V 0, V \u2212 V 0, V (V, V). The necessary condition of E (V) to have an extremity on V is that the functional derivative of E (V) / V = 0 [1]. With the calculation rules of the functional derivative and the fact that we have an optimal derivative."}, {"heading": "7 Appendix. Backgrounds on Riemannian Geometry", "text": "Let (M, g) be a d-dimensional belmannic manifold, where g is a belmannic metric tensor onM. The goal of telemetric learning on the manifold is to find a desired distance function d (x, y) in such a way that it provides a natural measure of the similarity between two data points x and y on the manifold. A distance function d (\u00b7, \u00b7), defined by its rimanic metric g, is often referred to as an intrinsic distance function. In this paper, we examine a basic intrinsic distance function - the geodesic distance function. Similar to many differential geometry textbooks (e.g. [12, 20]), we simply call it the distance function."}, {"heading": "7.1 Tangent Spaces and Vector Fields", "text": "First, we will take an example in Euclidean space to show why a tangent vector is a derivative."}, {"heading": "7.2 Distance Functions", "text": "Next, we will show how to define a metric structure on the manifold. For each point p on the manifold, we can define an indirect function. For each point p on the manifold, a metric tensor g at p is a euclidean inner product gp on each tangent space TpM of M. Furthermore, we assume that gp varies smoothly [20]. This means that for two smooth vector fields X, Y should be the inner product gp (Xp, Yp) a smooth function of p, where Xp and Yp denote the tangent vectors of X and Y at p. The subscript p p p p is omitted if it is clear from context. So we could write g (X, Y) or gp (X, Y) with the understanding that this should be evaluated at each p where X and Y are defined. We define the norm of a tangent vector of X and Y at p."}, {"heading": "7.3 Covariant Derivative", "text": "s let X = ai \u00b7 i be a vector field in Rd in which \u2202 i denotes the default Cartesian coordinate. Then, it is natural to define the covariant derivative of X in the direction of Y, since the coordinate field Y is called a constant vector field i = Y (in other words: Y = 0 for each vector field Y). For general coordinate fields Y \u2212 they are not always constant. Therefore, we should give a coordinate-free definition of the covariant connection Y. Theorem Y 7.1 (The Basic Riemannian Theorem of Geometry, [20]."}, {"heading": "7.4 Geodesics", "text": "Let it be so: [a, b] \u2192 M is a curve inM. Let it be like this: [c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c,"}, {"heading": "8 Appendix. Connection Laplacian and Functional Derivative on Vector Fields", "text": "In this section we present the connection between the laplac operator and the functional derivative on vector fields."}, {"heading": "8.1 Connection Laplacian", "text": "Then it is easy to determine that X + Y and fX are smooth vector fields for any smooth function. Therefore, T can be defined as a linear operator (TX, Y) = g (X, T), for all vector fields X, Y (X, Y), for all vector fields X, Y (TM). We can further define the inner product of two (1, 1) tensors T1, T2 as < T1, T2 >: tr (T, 1 T2) >: a positive operator X, Y (T, Y), for all vector fields X, Y (TM)."}, {"heading": "8.2 Functional Derivative", "text": "Our objective function for learning a vector field V is defined as follows: min V E (V) > V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 e (V) \u00b7 V) \u2212 E (V): min V (V) \u2212 V (V) \u2212 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V (V) \u00b7 V \u00b7 V) \u00b7 V \u00b7 e (V) \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V) \u00b7 V (V) (V) (V) (V) (V) (V)) (V) (V) (V) (V) > V (V) \u00b7 V (V) \u00b7 V) \u00b7 V (V) \u00b7 V (V)."}, {"heading": "9 Appendix. Proof of Main Theorems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Proof of Theorem 2.2", "text": "(a) SinceM is a complete manifold, according to Hopf-Rinow theorem (20). (a) SinceM is a complete manifold, after which we all (20). (a) SinceM is a complete manifold. (c) SinceM is a complete manifold. (c) SinceM is a complete manifold. (c) SinceM is a complete manifold. (c) SinceM is a complete manifold. (c) SinceM is a complete manifold. (c) SinceM is a complete manifold. (c) SinceM is a complete. (c) SinceM is a complete. (c) SinceM is a complete. (c) SinceM is a complete. (c) SinceM is a complete. (c) SinceM is a complete. (c) SinceM is a complete. (c) SinceM is a complete. (c) SinceM is a complete. (c) SinceM is a complete. (c) SinceM is a complete."}, {"heading": "9.2 Proof of Theorem 2.3", "text": "The proof. (\u21d2) applies to the entire proof of theorem 2.2. Compared to theorem 2.2, we need only the (1,1) version of the Hessian of r, S (\u00b7) = v, v, v, r, i.e., Hessr (X, Y) = g (S (X), Y). It is obvious that Hessr is a symmetrical tenor, i.e. Hessr (X, Y) = Hessr (Y, X).For each vector field Y onM, p, p), theng (4, r, Y) = Hessr (4, Y) = Hessr (Y, v) = Hessr (Y, v, v) = g (Y, v, v) = 12 Y (1).The second equality applies to the equality according to symmetry (Y, v, v) and the third applies to the equality according to p, since the equality applies before the equality before the equality before the equality."}], "references": [{"title": "Manifolds", "author": ["R. Abraham", "J.E. Marsden", "T. Ratiu"], "venue": "tensor analysis, and applications, volume 75 of Applied Mathematical Sciences. Springer-Verlag, New York, second edition", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1988}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Heat kernels and Dirac operators", "author": ["N. Berline", "E. Getzler", "M. Vergne"], "venue": "Springer-Verlag", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Spectral Graph Theory", "author": ["F.R.K. Chung"], "venue": "volume 92 of Regional Conference Series in Mathematics. AMS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Diffusion maps", "author": ["R.R. Coifman", "S. Lafon"], "venue": "Applied and Computational Harmonic Analysis, 21(1):5 \u2013 30", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Geodesics in heat: A new approach to computing distance based on heat flow", "author": ["K. Crane", "C. Weischedel", "M. Wardetzky"], "venue": "ACM Trans. Graph., 32(5):152:1\u2013152:11", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Tensor Norms and Operator Ideals", "author": ["A. Defant", "K. Floret"], "venue": "North-Holland Mathematics Studies, North-Holland, Amsterdam", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data", "author": ["D.L. Donoho", "C.E. Grimes"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, 100(10):5591\u20135596", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Parallel field ranking", "author": ["M. Ji", "B. Lin", "X. He", "D. Cai", "J. Han"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201912, pages 723\u2013731", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularized distance metric learning:theory and algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": "Springer-Verlag, New York", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Riemannian Geometry and Geometric Analysis (5", "author": ["J. Jost"], "venue": "ed.). Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Introduction to Smooth Manifolds", "author": ["J.M. Lee"], "venue": "Springer Verlag, New York, 2nd edition", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Parallel vector field embedding", "author": ["B. Lin", "X. He", "C. Zhang", "M. Ji"], "venue": "Journal of Machine Learning Research, 14:2945\u20132977", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised regression via parallel field regularization", "author": ["B. Lin", "C. Zhang", "X. He"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Schtze"], "venue": "Cambridge University Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Hamilton-jacobi equations and distance functions on riemannian manifolds", "author": ["C. Mantegazza", "A.C. Mennucci"], "venue": "Applied Mathematics and Optimization, 47(1):1\u201326", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Metric learning to rank", "author": ["B. McFee", "G. Lanckriet"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 775\u2013782", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast computation of weighted distance functions and geodesics on implicit hyper-surfaces", "author": ["F. M\u00e9moli", "G. Sapiro"], "venue": "Journal of Computational Physics, 173(2):730 \u2013 764", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Riemannian Geometry", "author": ["P. Petersen"], "venue": "Springer, New York", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science, 290(5500):2323\u20132326", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "The CMU pose", "author": ["T. Sim", "S. Baker", "M. Bsat"], "venue": "illuminlation, and expression database. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(12):1615\u20131618", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Vector diffusion maps and the connection Laplacian", "author": ["A. Singer", "H.-T. Wu"], "venue": "Communications on Pure and Applied Mathematics, 65(8):1067\u20131144", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "V", "author": ["J. Tenenbaum"], "venue": "de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319\u20132323", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "J. Blitzer", "L. Saul"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Learning a kernel matrix for nonlinear dimensionality reduction", "author": ["K.Q. Weinberger", "F. Sha", "L.K. Saul"], "venue": "Proceedings of the twenty-first international conference on Machine learning (ICML-04), ICML \u201904, pages 839\u2013846", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell"], "venue": "Advances in Neural Information Processing Systems 15, pages 505\u2013512", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Ranking on data manifolds", "author": ["D. Zhou", "J. Weston", "A. Gretton", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}], "referenceMentions": [{"referenceID": 17, "context": "It has been applied widely in many problems, such as information retrieval [18], classification and clustering [27].", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "It has been applied widely in many problems, such as information retrieval [18], classification and clustering [27].", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "In supervised learning, one often assumes that data points with the same label should have small distance, while data points with different labels should have large distance [27, 25, 10].", "startOffset": 174, "endOffset": 186}, {"referenceID": 24, "context": "In supervised learning, one often assumes that data points with the same label should have small distance, while data points with different labels should have large distance [27, 25, 10].", "startOffset": 174, "endOffset": 186}, {"referenceID": 9, "context": "In supervised learning, one often assumes that data points with the same label should have small distance, while data points with different labels should have large distance [27, 25, 10].", "startOffset": 174, "endOffset": 186}, {"referenceID": 10, "context": "The classical Principal Component Analysis (PCA, [11]) can be considered as linear manifold learning method in which the map F is linear.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].", "startOffset": 103, "endOffset": 107}, {"referenceID": 1, "context": "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 7, "context": "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].", "startOffset": 166, "endOffset": 169}, {"referenceID": 25, "context": "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].", "startOffset": 205, "endOffset": 209}, {"referenceID": 4, "context": "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].", "startOffset": 230, "endOffset": 233}, {"referenceID": 4, "context": "Coifman and Lafon [5] also showed that both LLE and LE belong to the diffusion map framework which preserves the local structure of the manifold.", "startOffset": 18, "endOffset": 21}, {"referenceID": 23, "context": ", [24]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 7, "context": "However, it is well known that computing pairwise shortest path distance is time consuming and it cannot handle the case when the manifold is not geodesically convex [8].", "startOffset": 166, "endOffset": 169}, {"referenceID": 18, "context": "[19] proposes an iterated algorithm for solving the Hamilton-Jacobi equation \u2016\u2207r\u2016 = 1 [17], where \u2207r represents the gradient field of the distance function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] proposes an iterated algorithm for solving the Hamilton-Jacobi equation \u2016\u2207r\u2016 = 1 [17], where \u2207r represents the gradient field of the distance function.", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "Note that the tangent space dimension is equal to the manifold dimension [13] which is usually much smaller than the ambient dimension.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "Inspired by recent work on vector fields [23, 15, 14] and heat flow on scalar fields [6], we propose to learn the geodesic distance function via the characterization of its gradient field and heat flow on vector fields.", "startOffset": 41, "endOffset": 53}, {"referenceID": 14, "context": "Inspired by recent work on vector fields [23, 15, 14] and heat flow on scalar fields [6], we propose to learn the geodesic distance function via the characterization of its gradient field and heat flow on vector fields.", "startOffset": 41, "endOffset": 53}, {"referenceID": 13, "context": "Inspired by recent work on vector fields [23, 15, 14] and heat flow on scalar fields [6], we propose to learn the geodesic distance function via the characterization of its gradient field and heat flow on vector fields.", "startOffset": 41, "endOffset": 53}, {"referenceID": 5, "context": "Inspired by recent work on vector fields [23, 15, 14] and heat flow on scalar fields [6], we propose to learn the geodesic distance function via the characterization of its gradient field and heat flow on vector fields.", "startOffset": 85, "endOffset": 88}, {"referenceID": 11, "context": ", [12, 20]), we call it the distance function.", "startOffset": 2, "endOffset": 10}, {"referenceID": 19, "context": ", [12, 20]), we call it the distance function.", "startOffset": 2, "endOffset": 10}, {"referenceID": 11, "context": ", positivity, symmetry and triangle inequality [12].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "2 (Geodesic, [20]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "1 ([20]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "where \u2016 \u00b7 \u2016HS denotes the Hilbert-Schmidt tensor norm [7] and t > 0 is a parameter.", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "Following [15], the discrete form of our objective functions can be given as follows: E(V ) = V V \u2212 2V 0V + V 0V 0 + tV BV, \u03a6(f) = 2fLf + V\u0302 GV\u0302 \u2212 2V\u0302 Cf, (4)", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "where L is the graph Laplacian matrix [4], B is a dn \u00d7 dn block matrix, G is a dn \u00d7 dn block diagonal matrix and C is a dn \u00d7 n block matrix.", "startOffset": 38, "endOffset": 41}, {"referenceID": 22, "context": "It might be worth noting that one can also approximate the parallel transport by solving a singular value decomposition problem [23].", "startOffset": 128, "endOffset": 132}, {"referenceID": 22, "context": "Our approach is based on the idea of vector field regularization which is similar to Vector Diffusion Maps (VDM, [23]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "VDM approximates the parallel transport by learning an orthogonal transformation and we simply use projection adopted from [15].", "startOffset": 123, "endOffset": 127}, {"referenceID": 5, "context": "GDL can also be regarded as a generalization of the heat method [6] on scalar fields.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "The algorithm proposed in [6] first learns a scalar field by heat flow on scalar fields and then learns the desired vector field by evaluating the gradient field of the obtained scalar field.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).", "startOffset": 220, "endOffset": 223}, {"referenceID": 25, "context": "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).", "startOffset": 259, "endOffset": 263}, {"referenceID": 7, "context": "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).", "startOffset": 294, "endOffset": 297}, {"referenceID": 27, "context": "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).", "startOffset": 369, "endOffset": 373}, {"referenceID": 8, "context": "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).", "startOffset": 408, "endOffset": 411}, {"referenceID": 0, "context": "To remove the effect of scale, {f(xi)} and {d(xq, xi)} are rescaled to the range [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 21, "context": "The first one is from the CMU PIE face database [22], which contains 32 \u00d7 32 cropped face images of 68 persons.", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "We use precision, recall, and Mean Average Precision (MAP, [16]) to evaluate the retrieval results of different algorithms.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "According to the Bochner technique [20], with appropriate boundary conditions we have \u222b M \u2016\u2207V \u2016 2 HSdx = \u222b M g(V,\u2207 \u2217\u2207V )dx, where \u2207\u2217\u2207 is the connection Laplacian operator.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "The necessary condition of E(V ) to have an extremum at V is that the functional derivative \u03b4E(V )/\u03b4V = 0 [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "Given an initial vector field X(t)|t=0 = X0, the heat equation on vector fields [3] is given by \u2202X(t) \u2202t +\u2207 \u2217\u2207X(t) = 0.", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "It is well known for small t, we have the asymptotic expansion of the heat kernel [3]:", "startOffset": 82, "endOffset": 85}, {"referenceID": 11, "context": ", [12, 20]), we simply call it the distance function.", "startOffset": 2, "endOffset": 10}, {"referenceID": 19, "context": ", [12, 20]), we simply call it the distance function.", "startOffset": 2, "endOffset": 10}, {"referenceID": 12, "context": "1 (Tangent space, [13]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": ", \u2202d|p form a basis for TpM [13].", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "2 (Vector field, [13]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 19, "context": "In addition we assume that gp varies smoothly [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 11, "context": ", positivity, symmetry and triangle inequality [12].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "1 (The fundamental theorem of Riemannian geometry, [20]).", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "Since\u2207\u2202i\u2202j is still a vector field on the manifold, we can further represent it as \u2207\u2202i\u2202j = \u0393ij\u2202k, where \u03b3 ij are called Christoffel symbols [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "4 (Geodesic, [20]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "3 ([20]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "5 ([20]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The connection Laplaican\u2207\u2217\u2207 equals to \u2212tr(\u2207) [20].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "where \u2016 \u00b7 \u2016HS denotes the Hilbert-Schmidt tensor norm [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "The necessary condition of E(V ) to have an extremum at V is that the functional derivative \u03b4E(V )/\u03b4V = 0 [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "1 ([1]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "The derivative Df(u0) if it exist, is unique [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 19, "context": "(\u21d2) (a) SinceM is a complete manifold, according to Hopf-Rinow Theorem [20], it is also geodesic complete.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "Let \u03b3 be an integral curve [13] of \u2202r, i.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "Recall that if a vector field is smooth at some point, then there exists a unique integral curve passing through it [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "Since r(x) = \u2016 exp\u22121 p (x)\u2016, according to the Gauss lemma [20], we have \u2202r = D expp(\u2202r).", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "Since the manifold is complete, according to Hopf-Rinow Theorem [20], there always exists a minimal geodesic connecting any two points on the manifold.", "startOffset": 64, "endOffset": 68}], "year": 2014, "abstractText": "Learning a distance function or metric on a given data manifold is of great importance in machine learning and pattern recognition. Many of the previous works first embed the manifold to Euclidean space and then learn the distance function. However, such a scheme might not faithfully preserve the distance function if the original manifold is not Euclidean. Note that the distance function on a manifold can always be well-defined. In this paper, we propose to learn the distance function directly on the manifold without embedding. We first provide a theoretical characterization of the distance function by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then learn the distance function itself. Specifically, we set the gradient field of a local distance function as an initial vector field. Then we transport it to the whole manifold via heat flow on vector fields. Finally, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm.", "creator": "LaTeX with hyperref package"}}}