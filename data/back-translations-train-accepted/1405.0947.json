{"id": "1405.0947", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2014", "title": "Learning Bilingual Word Representations by Marginalizing Alignments", "abstract": "We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments. The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art.", "histories": [["v1", "Mon, 5 May 2014 16:24:09 GMT  (127kb,D)", "http://arxiv.org/abs/1405.0947v1", "Proceedings of ACL 2014 (Short Papers)"]], "COMMENTS": "Proceedings of ACL 2014 (Short Papers)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tom\u00e1s kocisk\u00fd", "karl moritz hermann", "phil blunsom"], "accepted": true, "id": "1405.0947"}, "pdf": {"name": "1405.0947.pdf", "metadata": {"source": "CRF", "title": "Learning Bilingual Word Representations by Marginalizing Alignments", "authors": ["Tom\u00e1\u0161 Ko\u010disk\u00fd", "Karl Moritz Hermann", "Phil Blunsom"], "emails": ["tomas.kocisky@cs.ox.ac.uk", "karl.moritz.hermann@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to fight, to fight, to fight, to move, to fight, to fight, to fight, to fight, to move, to fight, to fight, to fight, to fight, to fight, to move, to fight, to move, to fight, to fight, to fight, to move, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "2 Background", "text": "IBM alignment models introduced by Brown et al. (1993) form the basis of most statistical machine translation systems. In this article, we are based on FASTALIGN (FA), a state-of-the-art variable Xiv: 140 5.09 47v1 [cs.CL] dated 5 May 201 4ation of the IBM Model 2 introduced by Dyer et al. (2013). This model is both fast and generates state-of-the-art alignments. To provide the distributed representations, we incorporate ideas from the log bilinear language model presented by Mnih and Hinton (2007)."}, {"heading": "2.1 IBM Model 2", "text": "Given a parallel corpus of aligned sentences, an alignment model can be used to detect matching words and phrases in different languages; such models are an integral part of most machine translation pipelines; an alignment model learns p (f, a | e) (or p (e, a \u2032 | f)) for the source and target sentences e and f (word strings); a represents the alignment across these two sentences from source to target; IBM Model 2 (Brown et al., 1993) learns alignment and translation probabilities in a generative style as follows: p (f, a | e) = p (J | I) J \u0394j = 1p (aj | j, I, J) p (fj | eaj) p (J | I), with p (J | I) capturing the two sentence lengths; p (j | j, I, J) the alignment size; and p (fj | eaj) the translation probability."}, {"heading": "2.2 Log-Bilinear Language Model", "text": "Language models assign a probability measure to word sequences. We use the logbilinear language model proposed by Mnih and Hinton (2007). It is an n-gram-based model defined in relation to an energy function E (wn; w1: n \u2212 1). The probability of predicting the next word wn based on its previous context of n \u2212 1 words is expressed with the energy function E (wn; w1: n \u2212 1) = \u2212 (n \u2212 1 x i = 1 rTwiCi) rwn \u2212 bTr rwn \u2212 bwnas p (wn | w1: n \u2212 1) = 1Zc exp (\u2212 E (wn; w1: n \u2212 1)), where Zc = wnexp (\u2212 E (wn; w1: n \u2212 1) is the normalizer, rwi \u00b2 Rd are word representations, Ci \u00b2 Rd are context transformation matrices and br \u00b2 Rd, bwn \u00b2 are word representations and biometrics."}, {"heading": "2.3 Multilingual Representation Learning", "text": "Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embedding using word alignments; these two models are not probabilistic and are conditioned by the output of a separate alignment model, in contrast to our model, which defines a probability distribution across translations and marginalizes across all alignments; these models are also highly related to previous work on bilingual lexicon induction (Haghighi et al., 2008); other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information between languages using a composite sentence-level representation; while all of these approaches are related to the model proposed in this paper, it is important to consider that our approach is novel in enabling true word alignment by transferring information between languages using a composite sentence-level alignment model and color-representation."}, {"heading": "3 Model", "text": "The DWA model can be considered a distributed extension of the FA model by using a modified version of the log-bilinear language model instead of the translation probabilities p (fj | ei) at the heart of the FA model. This allows us to learn word representations for both languages, a translation matrix that relates these vector spaces and alignments simultaneously. Our modifications to the log-bilinear model are as follows: Where the original log-bilinear language model uses context words to predict the next word - this is simply the distributed extension of an n-gram language model - we use a word from the source language in a parallel sentence f to predict a target word f."}, {"heading": "3.1 Class Factorization", "text": "We improve training performance through a class factorization strategy (Morin and Bengio, 2005) as follows: We increase the translation probability to p (f | e) = p (cf | e) p (f | cf, e), where cf is a unique predefined class of f; the class probability is modeled using a similar logbilinear model as above, but instead of predicting a word representation rf, we predict the class representation rcf (which is learned with the model) and add appropriate new context matrices and distortions. Note that the probability of the word f depends on both the class and the given context words: It is normalized only using words in the class cf. In our training, we create classes based on the word frequencies in the corpus as follows. If we consider words in the order of their decreasing frequencies, we insert word types into a class, the class F to the total number of the class is currently smaller than the number of the class |"}, {"heading": "4 Learning", "text": "The original FA model optimizes probability using the Expectation Maximization (EM) algorithm, where the parameter update is analytically solvable in the M step, with the exception of the \u03bb parameter (diagonal stress), which is optimized by gradient descent (Dyer et al., 2013). We modified the implementations provided with CDEC (Dyer et al., 2010) while maintaining its standard parameters. In our model, DWA, we also optimize probability using the EM. During the training, however, we correct the number of E steps to those calculated by FA, which were trained for the standard 5 iterations, in order to support the convergence rate and optimize only the M step. Let's be the parameters for our model. Then, the gradient for each sentence is calculated by the digit p (f | e) = J \u00b2 k = 1 I \u00b2 l = 0 [p | k, J | fel) (p)."}, {"heading": "5 Experiments", "text": "First, we evaluate the alignment error rate of our approach, which determines the model's ability to learn both alignments and word representations that explain these alignments. Next, we use a linguistic classification task to verify whether the representations are semantically useful. We also examine the embedding space qualitatively to get an insight into the learned structure."}, {"heading": "5.1 Alignment Evaluation", "text": "We compare the alignments learned here with those of the FASTALIGN model, which provides very good alignments and translation BLEU values. We use the same language pairs and datasets as in Dyer et al. (2013), i.e. the FBIS-Sino-English corpus and the French-English section of the Europarl corpus (Koehn, 2005). We used CDEC's pre-processing tools and replaced all the unique tokens with UNK. We trained our models with 100-dimensional representations for up to 40 iterations and the FA model for 5 iterations like the default.Table 1 shows that our model learns alignments partially with those of the FA model. This corresponds to the expectation that our model was trained based on FA expectations. However, it confirms that the learned word representations are able to explain translation probabilities. Surprisingly, the context seems to have little influence on the alignment itself, which indicates that the alignment is sufficient."}, {"heading": "5.2 Document Classification", "text": "A standard task for evaluating cross-language word representations is document classification, in which the training is performed in one language and the evaluation in another. This task requires semantically plausible embeddings (for classification) that are valid in two languages (for semantic transmission). Therefore, this task requires more word embeddings than the previous task. In contrast to Klementiev et al. (2012), we do not use this corpus during the display learning phase and remove all words that occur less than five times in the data in order to train the word embeddings. We perform the classification task on the Reuters RCV1 / 2 corpus. In contrast to Klementiev et al. (2012), we do not use this corpus during the display learning phase. We remove all words that occur less than five times in the data and learn 40-dimensional word embeddings in accordance with previous work. In order to train a classifier on English data and to project it onto German documents, we project the English word settings accordingly:"}, {"heading": "5.3 Representation Visualization", "text": "Following the task of document classification, we want to gain further insight into the types of characteristics that our embeddings learn by visualizing word representations using t-SNE projections (van der Maaten and Hinton, 2008). Figure 1 shows an excerpt from our projection of the 2,000 most common German words, along with an expected representation of a translated English word at given translation probabilities. Interestingly, the model is able to learn related representations for words chair and presidency, although these words were not aligned by our model. Figure 2 shows an excerpt from the visualization of the 10,000 most common English words trained on a different body. Again, it is obvious that the embeddings are semantically plausible, with similar words closely aligned."}, {"heading": "6 Conclusion", "text": "We have shown that the DWA model is capable of learning alignments on an equal footing with the FASTALIGN alignment model, which produces very good alignments, thereby determining the effectiveness of the learned representations used to calculate the translation probabilities for the alignment task. Subsequently, we have shown that our model can be effectively used to project documents from one language to another. The word representations that our model learns in the alignment process are semantically plausible and useful. By applying these embeddings, we have achieved this on a cross-language task for classifying documents, where we surpass previous work, results on a par with the current state of the art, and provide new, state-of-the-art results on one of the tasks. After providing a probable representation of word representations in multiple languages, we will focus in the future on this model, which is particularly well suited for translation tasks that are less suited to translating these methods into the context."}, {"heading": "Acknowledgements", "text": "This work was supported by the Xerox Foundation Award and the EPSRC grant number EP / K036580 / 1."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155, February.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer."], "venue": "Computational Linguistics, 19(2):263\u2013311, June.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159, July.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Jonathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "A simple, fast, and effective reparameterization of IBM model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of NAACLHLT.", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Improving Vector Space Word Representations Using Multilingual Correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of EACL.", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein."], "venue": "Proceedings of ACLHLT.", "citeRegEx": "Haghighi et al\\.,? 2008", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "The Role of Syntax in Vector Space Models of Compositional Semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of ACL.", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of ICLR.", "citeRegEx": "Hermann and Blunsom.,? 2014a", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Multilingual Models for Compositional Distributional Semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of ACL.", "citeRegEx": "Hermann and Blunsom.,? 2014b", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Semantic Frame Identification with Distributed Word Representations", "author": ["Karl Moritz Hermann", "Dipanjan Das", "Jason Weston", "Kuzman Ganchev."], "venue": "Proceedings of ACL.", "citeRegEx": "Hermann et al\\.,? 2014", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Richard S Zemel", "Ruslan Salakhutdinov."], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Kiros et al\\.,? 2013", "shortCiteRegEx": "Kiros et al\\.", "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "Proceedings of COLING.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Proceedings of the 10th Machine Translation Summit.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Learning multilingual word representations using a bag-of-words autoencoder", "author": ["Stanislas Lauly", "Alex Boulanger", "Hugo Larochelle."], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Lauly et al\\.,? 2013", "shortCiteRegEx": "Lauly et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."], "venue": "CoRR, abs/1309.4168.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proceedings of ICML.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller."], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Robert G. Cowell and Zoubin Ghahramani, editors, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, pages 246\u2013252.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams."], "venue": "Nature, 323:533\u2013536, October.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Multilingual deep learning", "author": ["A P Sarath Chandar", "M Khapra Mitesh", "B Ravindran", "Vikas Raykar", "Amrita Saha."], "venue": "Deep Learning Workshop at NIPS.", "citeRegEx": "Chandar et al\\.,? 2013", "shortCiteRegEx": "Chandar et al\\.", "year": 2013}, {"title": "Smooth bilingual n-gram translation", "author": ["Holger Schwenk", "Marta R. Costa-jussa", "Jose A.R. Fonollosa."], "venue": "Proceedings of EMNLP-CoNLL.", "citeRegEx": "Schwenk et al\\.,? 2007", "shortCiteRegEx": "Schwenk et al\\.", "year": 2007}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Holger Schwenk."], "venue": "Proceedings of COLING: Posters.", "citeRegEx": "Schwenk.,? 2012", "shortCiteRegEx": "Schwenk.", "year": 2012}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio."], "venue": "Proceedings of ACL.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["L.J.P. van der Maaten", "G.E. Hinton."], "venue": "Journal of Machine Learning Research, 9:2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 12, "context": ", 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 0, "context": ", 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 24, "context": ", 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al.", "startOffset": 28, "endOffset": 76}, {"referenceID": 8, "context": ", 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al.", "startOffset": 28, "endOffset": 76}, {"referenceID": 11, "context": ", 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 13, "context": ", 2014), and document classification (Klementiev et al., 2012).", "startOffset": 37, "endOffset": 62}, {"referenceID": 0, "context": ", 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)).", "startOffset": 29, "endOffset": 662}, {"referenceID": 0, "context": ", 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)).", "startOffset": 29, "endOffset": 694}, {"referenceID": 13, "context": "As opposed to previous work in this field, which has relied on hard alignments or bilingual lexica (Klementiev et al., 2012; Mikolov et al., 2013), we marginalize out the alignments, thus capturing more bilingual semantic context.", "startOffset": 99, "endOffset": 146}, {"referenceID": 16, "context": "As opposed to previous work in this field, which has relied on hard alignments or bilingual lexica (Klementiev et al., 2012; Mikolov et al., 2013), we marginalize out the alignments, thus capturing more bilingual semantic context.", "startOffset": 99, "endOffset": 146}, {"referenceID": 10, "context": "Subsequently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art (Hermann and Blunsom, 2014b).", "startOffset": 152, "endOffset": 180}, {"referenceID": 5, "context": "As a by-product we develop a distributed version of FASTALIGN (Dyer et al., 2013), which performs on par with the original model, thereby demonstrating the efficacy of the learned bilingual representations.", "startOffset": 62, "endOffset": 81}, {"referenceID": 1, "context": "The IBM alignment models, introduced by Brown et al. (1993), form the basis of most statistical machine translation systems.", "startOffset": 40, "endOffset": 60}, {"referenceID": 4, "context": "ation of IBM model 2 introduced by Dyer et al. (2013). This model is both fast and produces alignments on par with the state of the art.", "startOffset": 35, "endOffset": 54}, {"referenceID": 4, "context": "ation of IBM model 2 introduced by Dyer et al. (2013). This model is both fast and produces alignments on par with the state of the art. Further, to induce the distributed representations we incorporate ideas from the log-bilinear language model presented by Mnih and Hinton (2007).", "startOffset": 35, "endOffset": 282}, {"referenceID": 1, "context": "IBM model 2 (Brown et al., 1993) learns alignment and translation probabilities in a generative style as follows:", "startOffset": 12, "endOffset": 32}, {"referenceID": 5, "context": "We use FASTALIGN (FA) (Dyer et al., 2013), a log-linear reparametrization of IBM model 2.", "startOffset": 22, "endOffset": 41}, {"referenceID": 17, "context": "We use the log-bilinear language model proposed by Mnih and Hinton (2007). It is an n-gram based model defined in terms of an energy function E(wn;w1:n\u22121).", "startOffset": 51, "endOffset": 74}, {"referenceID": 7, "context": "These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008).", "startOffset": 82, "endOffset": 105}, {"referenceID": 9, "context": "Similar to the model presented here, Klementiev et al. (2012) and Zou et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 9, "context": "Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments.", "startOffset": 37, "endOffset": 84}, {"referenceID": 7, "context": "These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al.", "startOffset": 83, "endOffset": 168}, {"referenceID": 7, "context": "These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b).", "startOffset": 83, "endOffset": 189}, {"referenceID": 7, "context": "These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation. While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information. Related work also includes Mikolov et al. (2013), who learn a transformation matrix to", "startOffset": 83, "endOffset": 836}, {"referenceID": 21, "context": "reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities.", "startOffset": 120, "endOffset": 142}, {"referenceID": 21, "context": "reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities.", "startOffset": 120, "endOffset": 161}, {"referenceID": 6, "context": "Faruqui and Dyer (2014) use a technique based on CCA and alignments to project monolingual word representations to a common vector space.", "startOffset": 0, "endOffset": 24}, {"referenceID": 19, "context": "We improve training performance using a class factorization strategy (Morin and Bengio, 2005) as follows.", "startOffset": 69, "endOffset": 93}, {"referenceID": 5, "context": "The original FA model optimizes the likelihood using the expectation maximization (EM) algorithm where, in the M-step, the parameter update is analytically solvable, except for the \u03bb parameter (the diagonal tension), which is optimized using gradient descent (Dyer et al., 2013).", "startOffset": 259, "endOffset": 278}, {"referenceID": 4, "context": "We modified the implementations provided with CDEC (Dyer et al., 2010), retaining its default parameters.", "startOffset": 51, "endOffset": 70}, {"referenceID": 20, "context": "We compute the gradient for the alignment probabilities in the same way as in the FA model, and the gradient for the translation probabilities using back-propagation (Rumelhart et al., 1986).", "startOffset": 166, "endOffset": 190}, {"referenceID": 3, "context": "For parameter update, we use ADAGRAD as the gradient descent algorithm (Duchi et al., 2011).", "startOffset": 71, "endOffset": 91}, {"referenceID": 14, "context": "(2013), that is the FBIS Chinese-English corpus, and the French-English section of the Europarl corpus (Koehn, 2005).", "startOffset": 103, "endOffset": 116}, {"referenceID": 4, "context": "We use the same language pairs and datasets as in Dyer et al. (2013), that is the FBIS Chinese-English corpus, and the French-English section of the Europarl corpus (Koehn, 2005).", "startOffset": 50, "endOffset": 69}, {"referenceID": 13, "context": "We mainly follow the setup of Klementiev et al. (2012) and use the German-English parallel corpus of the European Parliament proceedings to train the word representations.", "startOffset": 30, "endOffset": 55}, {"referenceID": 13, "context": "We mainly follow the setup of Klementiev et al. (2012) and use the German-English parallel corpus of the European Parliament proceedings to train the word representations. We perform the classification task on the Reuters RCV1/2 corpus. Unlike Klementiev et al. (2012), we do not use that corpus during the representation learning phase.", "startOffset": 30, "endOffset": 269}, {"referenceID": 10, "context": "Our model outperforms the model by Klementiev et al. (2012), and it also outperforms the most comparable models by Hermann and Blunsom (2014b) when training on German data and performs on par with it when training on English data.", "startOffset": 35, "endOffset": 60}, {"referenceID": 8, "context": "(2012), and it also outperforms the most comparable models by Hermann and Blunsom (2014b) when training on German data and performs on par with it when training on English data.", "startOffset": 62, "endOffset": 90}, {"referenceID": 13, "context": "Baselines are the majority class, glossed, and MT (Klementiev et al., 2012).", "startOffset": 50, "endOffset": 75}, {"referenceID": 9, "context": "(2012), BiCVM ADD (Hermann and Blunsom, 2014a), and BiCVM BI (Hermann and Blunsom, 2014b).", "startOffset": 18, "endOffset": 46}, {"referenceID": 10, "context": "(2012), BiCVM ADD (Hermann and Blunsom, 2014a), and BiCVM BI (Hermann and Blunsom, 2014b).", "startOffset": 61, "endOffset": 89}, {"referenceID": 10, "context": "Baselines are the majority class, glossed, and MT (Klementiev et al., 2012). Further, we are comparing to Klementiev et al. (2012), BiCVM ADD (Hermann and Blunsom, 2014a), and BiCVM BI (Hermann and Blunsom, 2014b).", "startOffset": 51, "endOffset": 131}], "year": 2014, "abstractText": "We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments. The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art.", "creator": "TeX"}}}