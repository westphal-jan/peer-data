{"id": "1405.2798", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2014", "title": "Two-Stage Metric Learning", "abstract": "In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric with unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.", "histories": [["v1", "Mon, 12 May 2014 15:18:15 GMT  (287kb,D)", "http://arxiv.org/abs/1405.2798v1", "Accepted for publication in ICML 2014"]], "COMMENTS": "Accepted for publication in ICML 2014", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["jun wang 0017", "ke sun 0001", "fei sha", "st\u00e9phane marchand-maillet", "alexandros kalousis"], "accepted": true, "id": "1405.2798"}, "pdf": {"name": "1405.2798.pdf", "metadata": {"source": "META", "title": "Two-Stage Metric Learning", "authors": ["Jun Wang", "Ke Sun", "Fei Sha", "Stephane Marchand-Maillet"], "emails": ["JUN.WANG@UNIGE.CH", "KE.SUN@UNIGE.CH", "FEISHA@USC.EDU", "STEPHANE.MARCHAND-MAILLET@UNIGE.CH", "ALEXANDROS.KALOUSIS@HESGE.CH"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is as if most people are able to know themselves and understand what they are doing. (...) It is not as if people are able to know themselves and understand themselves. (...) It is not as if people are able to know themselves. (...) It is as if people are able to know themselves and understand themselves. (...) It is as if people are able to know themselves and understand themselves. (...) It is as if people are able to know themselves. (...) It is as if people are able to know themselves. (...) It is as if people are able to know themselves. (...) It is as if people are able to know themselves and understand themselves. (...) It is as if people are able to know themselves and understand themselves. (...) It is as if. (...) It is as if. (...) It is as if. (...) It is as if. (...) It is as if. (...) It is as if."}, {"heading": "2. Preliminaries", "text": "There are a number of learning instances in which we can see the first similarities. (...) We assume that the input feature X can have a smooth multiplicity. (...) We assume that the input feature X has a smooth multiplicity. (...) The probability probability probability simplex space Pd \u2212 1 also has very different types of data multiplicity with possibly different dimensions. (...) We propose a general two-stage metric learning algorithm that can learn a flexible distance in different types of X data. (...) Euclidean, probability simplex, hyperspia, etc. (...) We propose a general two-stage metric learning algorithm that can learn a flexible distance in different types of X data multiplicity. (...)"}, {"heading": "3. Similarity-Based Fisher Information Metric Learning", "text": "We now present our two-stage metric learning algorithm SBFIML. In the following, we first present how to define the similarity-based differential card f: X \u2212 \u2192 P and then how to learn the Fisher information distance."}, {"heading": "3.1. Similarity-Based Differential Map", "text": "Considering a number of anchor points (z1,. zn), the probability of a finite distribution occurring in various ways is very high. (s1,.,.,.,.,.).,.,.,.,.,.,.,.,.,.,.,.,.,..,..,..,..,...,...,....,....,.,....,....,....,......,.....,.....,.....,.....,......,..........,..........,...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.2. Large Margin Fisher Information Metric Learning", "text": "By applying it to the learning instances, the parametric information is defined as follows: (6) We map it to the statistical multiplicity Pn \u2212 1. We are now ready to learn the information distance from the data. (6) As discussed in Section 2, the information distance of Pn \u2212 1 can be calculated precisely by the cosinal distance (6). To parameterize the information distance, we apply the information distance of Fisher to the probability. (7) The intuition is that the effect of the optimal linear transformation L is equivalent to the localization of hidden anchor points, that the similarity of the data is the same as the transformed representation."}, {"heading": "4. Experiments", "text": "We will compare the performance of SBFIML against ten data sets from the UCI Machine Learning and mldata1 repositories, the details of which are described in the first column of Table 1, all of which are pre-processed by standardizing the input functions, comparing the individual methods of SBFIML against three metric learning methods: LMNN (Weinberger & Saul, 2009) 2, KML (Wang et al., 2011) 3, GLML (Noh et al., 2009), and PLML (Wang et al., 2012), the former two learning a global Mahalanobis metric in the input range Rd and the RKHS space, and the latter two learning smooth local metrics in Rd. Furthermore, we will compare SBFIML against Similar-based Mahalanobis Metric Learning (SBMML) to learn the difference from the RKHS space and the latter two respectively."}, {"heading": "5. Conclusion", "text": "In this paper, we present a two-step metric learning algorithm SBFIML. It first maps learning instances to a statistical multiple using a similarity-based differential map and then defines the distance in the input data space by the Fisher information distance at the statistical multiple. This leads to a new family of distance metrics in the input data space with two important properties. Firstly, the induced metrics are resilient to density variations in the original data space and secondly, they have the greatest distance discrimination on the multiple of anchor points. Furthermore, by learning a metric on the statistical multiple SBFIML distances can be learned on different types of input function spaces. The similarity-based map used in SBFIML is natural and flexible; unlike KML, it does not have to be PSD. Furthermore, SBFIML can be interpreted as a local metric learning method with a well-defined distance approximation."}, {"heading": "Acknowledgments", "text": "Jun Wang was partially funded by the Swiss NSF (Grant 200021-122283 / 1). Ke Sun is partially supported by the Swiss State Secretariat for Education, Research and Innovation (SER grant number C11.0043). Fei Sha is supported by the DARPA prize # D11AP00278 and the ARO prize # W911NF-12-1-0241Appendix Proof of of Lemma 2.Proof. Let us have the coordinate of p \"U\" Mn under a smooth coordinate map and vice versa the coordinate of f \"U\" (p) and f \"U\" N \"under a smooth coordinate map. Since p\" approaches of p \"we have, we have the coordinate of p\" U \"and dTB under the coordinate map U\" and dTB is an infinitesimal small change that moves to 0. Since f \"N\" Nm is a differential map, the function of \"p\" ends. \""}], "references": [{"title": "Information geometry of divergence functions", "author": ["S. Amari", "A. Cichocki"], "venue": "Bulletin of the Polish Academy of Sciences: Technical Sciences,", "citeRegEx": "Amari and Cichocki,? \\Q2010\\E", "shortCiteRegEx": "Amari and Cichocki", "year": 2010}, {"title": "Methods of information geometry, volume 191", "author": ["S. Amari", "H. Nagaoka"], "venue": "Amer Mathematical Society,", "citeRegEx": "Amari and Nagaoka,? \\Q2007\\E", "shortCiteRegEx": "Amari and Nagaoka", "year": 2007}, {"title": "Training svm with indefinite kernels", "author": ["Chen", "Jianhui", "Ye", "Jieping"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "Similarity-based classification: Concepts and algorithms", "author": ["Chen", "Yihua", "Garcia", "Eric K", "Gupta", "Maya R", "Rahimi", "Ali", "Cazzanti", "Luca"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Ground metric learning", "author": ["M. Cuturi", "D. Avis"], "venue": "arXiv preprint arXiv:1110.2306,", "citeRegEx": "Cuturi and Avis,? \\Q2011\\E", "shortCiteRegEx": "Cuturi and Avis", "year": 2011}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Efficient projections onto the l 1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In ICML,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "A geometric take on metric learning", "author": ["Hauberg", "Sren", "Freifeld", "Oren", "Black", "Michael"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hauberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hauberg et al\\.", "year": 2012}, {"title": "Stochastic neighbor embedding", "author": ["G. Hinton", "S. Roweis"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Hinton and Roweis,? \\Q2002\\E", "shortCiteRegEx": "Hinton and Roweis", "year": 2002}, {"title": "The angular kernel in machine learning for hyperspectral data classification", "author": ["P. Honeine", "C. Richard"], "venue": "In Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS),", "citeRegEx": "Honeine and Richard,? \\Q2010\\E", "shortCiteRegEx": "Honeine and Richard", "year": 2010}, {"title": "Inductive regularized learning of kernel functions", "author": ["P. Jain", "B. Kulis", "I. Dhillon"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Jain et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2010}, {"title": "Geometrical foundations of asymptotic inference, volume 908", "author": ["R.E. Kass", "P.W. Vos"], "venue": null, "citeRegEx": "Kass and Vos,? \\Q2011\\E", "shortCiteRegEx": "Kass and Vos", "year": 2011}, {"title": "Non-linear metric learning", "author": ["Kedem", "Dor", "Tyree", "Stephen", "Weinberger", "Kilian", "Sha", "Fei", "Lanckriet", "Gert"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kedem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kedem et al\\.", "year": 2012}, {"title": "Metric learning for text documents", "author": ["G. Lebanon"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Lebanon,? \\Q2006\\E", "shortCiteRegEx": "Lebanon", "year": 2006}, {"title": "Introduction to smooth manifolds, volume 218", "author": ["J.M. Lee"], "venue": null, "citeRegEx": "Lee,? \\Q2002\\E", "shortCiteRegEx": "Lee", "year": 2002}, {"title": "Dimensionality reduction and clustering on statistical manifolds", "author": ["S.M. Lee", "A.L. Abbott", "P.A. Araman"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Generative local metric learning for nearest neighbor classification", "author": ["Y.K. Noh", "B.T. Zhang", "D.D. Lee"], "venue": null, "citeRegEx": "Noh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2009}, {"title": "Information and accuracy attainable in the estimation of statistical parameters", "author": ["C. Radhakrishna Rao"], "venue": "Bulletin of the Calcutta Mathematical Society,", "citeRegEx": "Rao,? \\Q1945\\E", "shortCiteRegEx": "Rao", "year": 1945}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Metric learning with multiple kernels", "author": ["J. Wang", "H. Do", "A. Woznica", "A. Kalousis"], "venue": "Advances in Neural Information Processing Systems. MIT Press,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Analysis of svm with indefinite kernels", "author": ["Y. Ying", "C. Campbell", "M. Girolami"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Ying et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "Euclidean, cannot address in a satisfactory manner the multitude of learning problems, a fact that led to the development of metric learning methods which learn problemspecific distance measure directly from the data (Weinberger & Saul, 2009; Wang et al., 2012; Jain et al., 2010).", "startOffset": 217, "endOffset": 280}, {"referenceID": 5, "context": "Learning the distance metric with one global linear transformation is called single metric learning (Weinberger & Saul, 2009; Davis et al., 2007).", "startOffset": 100, "endOffset": 145}, {"referenceID": 16, "context": "Local metric learning addresses this limitation by learning in each neighborhood one local metric (Noh et al., 2009; Wang et al., 2012).", "startOffset": 98, "endOffset": 135}, {"referenceID": 7, "context": "When the local metrics vary smoothly in the feature space, learning local metrics is equivalent to learning the Riemannian metric on the data manifold (Hauberg et al., 2012).", "startOffset": 151, "endOffset": 173}, {"referenceID": 16, "context": "In practice, it is approximated by assuming that the geodesic curves are formed by straight lines and the local metric does not change along these lines (Noh et al., 2009; Wang et al., 2012).", "startOffset": 153, "endOffset": 190}, {"referenceID": 10, "context": "Kernelized Metric Learning (KML) achieves flexibility in a different way (Jain et al., 2010; Wang et al., 2011).", "startOffset": 73, "endOffset": 111}, {"referenceID": 19, "context": "Kernelized Metric Learning (KML) achieves flexibility in a different way (Jain et al., 2010; Wang et al., 2011).", "startOffset": 73, "endOffset": 111}, {"referenceID": 21, "context": "Although Non-PSD kernel could be transformed into PSD kernel (Chen & Ye, 2008; Ying et al., 2009), the new PSD kernel nevertheless cannot keep all original similarity information.", "startOffset": 61, "endOffset": 97}, {"referenceID": 16, "context": "Compared to the previous local metric learning algorithms which produce a non-metric distance (Noh et al., 2009; Wang et al., 2012), the distance approximation in SBFIML is a well defined distance function with a closed form expression.", "startOffset": 94, "endOffset": 131}, {"referenceID": 13, "context": "The probability simplex space Pd\u22121 has also been explored (Lebanon, 2006; Cuturi & Avis, 2011; Kedem et al., 2012).", "startOffset": 58, "endOffset": 114}, {"referenceID": 12, "context": "The probability simplex space Pd\u22121 has also been explored (Lebanon, 2006; Cuturi & Avis, 2011; Kedem et al., 2012).", "startOffset": 58, "endOffset": 114}, {"referenceID": 14, "context": "More details can be found in the monographs (Lee, 2002; Amari & Nagaoka, 2007).", "startOffset": 44, "endOffset": 78}, {"referenceID": 13, "context": "P, the cosine distance is exactly equivalent to the Fisher information distance (Lebanon, 2006; Lee et al., 2007).", "startOffset": 80, "endOffset": 113}, {"referenceID": 15, "context": "P, the cosine distance is exactly equivalent to the Fisher information distance (Lebanon, 2006; Lee et al., 2007).", "startOffset": 80, "endOffset": 113}, {"referenceID": 16, "context": "In addition to approximating dG\u2217(p, p) directly onM by assuming that the geodesic curve is formed by straight lines as previous local metric learning algorithms do (Noh et al., 2009; Wang et al., 2012), Lemma 2 allows us to also approximate it with dG(f(p), f(p)) on N.", "startOffset": 164, "endOffset": 201}, {"referenceID": 12, "context": "However, the similarity function on Pd\u22121 is more appropriate, because it exploits the geometrical structure of Pd\u22121, which, in contrast, is ignored by the similarity function on R (Kedem et al., 2012).", "startOffset": 180, "endOffset": 200}, {"referenceID": 3, "context": "Following the work of similarity-based learning (Chen et al., 2009), we use the Euclidean metric as the GQ in the proximity space Q.", "startOffset": 48, "endOffset": 67}, {"referenceID": 13, "context": "As discussed in section 2, the Fisher information distance on Pn\u22121 can be exactly computed by the cosine distance (Lebanon, 2006; Lee et al., 2007):", "startOffset": 114, "endOffset": 147}, {"referenceID": 15, "context": "As discussed in section 2, the Fisher information distance on Pn\u22121 can be exactly computed by the cosine distance (Lebanon, 2006; Lee et al., 2007):", "startOffset": 114, "endOffset": 147}, {"referenceID": 12, "context": "Unlike LMNN (Weinberger & Saul, 2009), the margin parameter \u03b3 is added in the large margin triplet constraints following the work of (Kedem et al., 2012), since the cosine distance is not linear with LL.", "startOffset": 133, "endOffset": 153}, {"referenceID": 6, "context": "The simplex projection operator on matrix L can be efficiently computed with complexity O(nk log(k)) (Duchi et al., 2008).", "startOffset": 101, "endOffset": 121}, {"referenceID": 13, "context": "Note that, learning distance metric on P has been previously studied by Riemannian Metric Learning (RML) (Lebanon, 2006) and \u03c7-LMNN (Kedem et al.", "startOffset": 105, "endOffset": 120}, {"referenceID": 12, "context": "Note that, learning distance metric on P has been previously studied by Riemannian Metric Learning (RML) (Lebanon, 2006) and \u03c7-LMNN (Kedem et al., 2012).", "startOffset": 132, "endOffset": 152}, {"referenceID": 16, "context": "Given the pullback metric we can approximate the geodesic distance on X by assuming that the geodesic curves are formed by straight lines as local metric learning methods (Noh et al., 2009; Wang et al., 2012) do, which would result in a nonmetric distance.", "startOffset": 171, "endOffset": 208}, {"referenceID": 19, "context": "SBFIML against three metric learning baseline methods: LMNN (Weinberger & Saul, 2009)2, KML (Wang et al., 2011)3, GLML (Noh et al.", "startOffset": 92, "endOffset": 111}, {"referenceID": 16, "context": ", 2011)3, GLML (Noh et al., 2009), and PLML (Wang et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 16, "context": "As in (Noh et al., 2009), GLML uses the Gaussian distribution to model the learning instances of a given class.", "startOffset": 6, "endOffset": 24}, {"referenceID": 12, "context": "Since \u03c7 LMNN and SBFIML apply different distance parametrizations to solve the same optimization problem, the parameters of \u03c7 LMNN are set in exactly the same way as SBFIML, except that the margin parameter \u03b3 of \u03c7 LMNN was selected from {10\u22128, 10\u22126, 10\u22124, 10\u22122}, because \u03c7 LMNN uses the squared \u03c7 distance (Kedem et al., 2012).", "startOffset": 306, "endOffset": 326}], "year": 2014, "abstractText": "In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric with unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.", "creator": "LaTeX with hyperref package"}}}