{"id": "1405.2878", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2014", "title": "Approximate Policy Iteration Schemes: A Comparison", "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration, Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\\infty$), and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all algorithms, we describe performance bounds, and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API/API($\\alpha$), but this comes at the cost of a relative---exponential in $\\frac{1}{\\epsilon}$---increase of the number of iterations. 2) PSDP$_\\infty$ enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP$_\\infty$ is proportional to their number of iterations, which may be problematic when the discount factor $\\gamma$ is close to 1 or the approximation error $\\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis.", "histories": [["v1", "Mon, 12 May 2014 19:11:03 GMT  (4923kb,D)", "http://arxiv.org/abs/1405.2878v1", "ICML (2014)"]], "COMMENTS": "ICML (2014)", "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["bruno scherrer"], "accepted": true, "id": "1405.2878"}, "pdf": {"name": "1405.2878.pdf", "metadata": {"source": "META", "title": "Approximate Policy Iteration Schemes: A Comparison", "authors": ["Bruno Scherrer"], "emails": ["BRUNO.SCHERRER@INRIA.FR"], "sections": [{"heading": null, "text": "We focus on several approximate variations of the Policy Iteration Algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming Algorithm (Bagnell et al., 2003) to the case of the Infinite-horizon (PSDP) and the recently proposed Non-Stationary Policy Iteration (NSPI (m)) (Scherrer & Reader, 2012). For all algorithms, we describe performance limits in terms of pro-iteration error and make a comparison by paying particular attention to the concentration capacity of the constants involved, the number of iterations and the memory required."}, {"heading": "1. Introduction", "text": "We consider an infinite horizon discounted Markov decision-making process (MDP) = infinite horizon discounted (MDP = indexed). (Puterman, 1994; Bertsekas & Tsitsiklis, 1996) (S, A, P, r, \u043c), where S is a potentially infinite state space, A is a finite space of action, P (ds, s, a), for all (s, a), is a probability kernel on S, r: \u2212 S \u2192 [\u2212 Rmax, Rmax] is a reward function limited by Rmax, and vice versa (0, 1) is a discount factor. A stationary deterministic policy \u03c0: S \u2192 A maps indicates states of action. We write p\u03c0 (ds, Rmax, Rmax] = P (s) for the stochastic core associated with politics. The value of a policy v\u03c0 v is a function."}, {"heading": "2. Algorithms", "text": "We start by describing the standard Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996). At each iteration k, the algorithm switches to the policy that is implemented in relation to the value of the previous policy in relation to some distribution policies. (2) If there is no error (k = 0) and it assigns each state a positive rating, it is easy to see that this algorithm generates the same sequence of measures as the exact policy iterations since the equation (1). The policy is exactly greedy.CPI / CPI (\u03b1) / API (\u03b1) We now turn to the description of conservative policy iteration (CPI) proposed by (Kakade & Langford, 2002)."}, {"heading": "3. Analysis", "text": "In order to derive these theoretical guarantees, we must first introduce a few coefficients of concentration, which include the \u00b5 distribution with which to have a guarantee and the distribution used by the algorithms. 1. Let us consider c (1), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c)), c), c)), c), c), c)), c)), c), c))), c)), c)), c)), c))), c)))), c)), c))), c)))), c))))), c), c)))), c))))), c)))))), c), c))))), c)))), c)))), c)))), c), c)))), c)))), c)), c)))), c), c))))), c))), c)), c)), c)))), c), c)), c))))), c))))), c)))), c)), c)), c)))), c))))), c)))))), c))))))), c))))))))))))))))))))))))))))))))"}, {"heading": "4. Experiments", "text": "In this section, we present some experiments to illustrate the empirical behavior of the various algorithms discussed in the paper. We considered the standard API as a baseline. CPI, as described by Kakade & Langford (2002), is very slow (in an example experiment on a 100 degree problem, it made very slow progress and took several million iterations before it was stopped) and we did not evaluate it further. Instead, we considered two variations: CPI +, which is actually identical to CPI, except that it selects the step on each iteration by conducting a line search toward the political output by the greedy operator, and CPI (\u03b1) with a performance of 0.1, which makes \"relative but not too small\" steps on each iteration. To evaluate the usefulness for distributing CPI, we must also consider the distribution methods we use for the approximate greedy step, with a deviation from API (an API of 0.\u03b1)."}, {"heading": "5. Discussion, Summary and Future Work", "text": "We have considered several variations of policy iteration systems for infinite horizon problems: API, CPI, NSPI (m), API (\u03b1) and PSDP \u221e 7. In particular, we have explained the fact - so far unknown to our knowledge - that the recently introduced NSPI (m) algorithm synthesizes the API (which is achieved when m = 1) and PSDP (which is very similar to whenm =). Figure 1 has synthesized the theoretical guarantees about these algorithms. Most of the limits are new.One of the first important messages of our work is that what is normally hidden in the constants of performance plays a role. The constants involved in the limits for API, CPI, PSDP and for the most important (left) terms of NSPI (m) we can be sorted from the worst to the best as follows: C (2,1,0), C (1)."}, {"heading": "A. Proofs for Table 1", "text": "PSDP \u00b2: For all k \u00b2 cases, we have \u2212 k = max \u2212 p = max \u2212 p = max \u2212 p = max \u2212 p = max \u2212 p = max \u2212 p = max \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 we have \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "B. Proofs for Figure 1", "text": "ii). (i). (i). (ii). (i). (i). (i). (i). (i). (i). (i). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii). (ii).). (ii). (ii). (ii). (ii).). (ii). (ii). (ii).). (ii). (ii).). (ii). (ii).). (ii).). (ii). (ii). (ii).). (ii). (ii). (ii). (ii). (ii). (ii). (ii).). (ii).). (ii). (ii). (ii). (ii).). (ii). (ii). (ii). (ii). (ii). (ii). (ii).). (ii). (ii). (ii). (ii).). (ii). (ii). (ii). (ii).). (ii). (ii). (ii). (ii)"}, {"heading": "C. Proof for CPI, CPI(\u03b1), API(\u03b1)", "text": "We start with the proof of the following result: Theorem 1 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212"}, {"heading": "D. More details on the Numerical Simulations", "text": "Domain and Approximations In our experiments, a yarn is parameterized and written by 4 parameters G (nS, nA, b, p): nS is the number of states, nA is the number of actions, b is a branching factor that indicates how many possible next states are possible for each pair of states (b states are uniformly randomly selected and transition probabilities are uniformly determined by scanning a uniformly random principle b-1 is the intersection between 0 and 1) and p is the number of characteristics (for a linear function approximation). The reward depends on the state: for a given randomly generated yarnet problem, the reward for each state is uniformly selected between 0 and 1. Characteristics are randomly selected: It is an nS characteristic matrix of which each component is random and uniformly determined that we have this component between 0 and 1. The discount factor is set to 0.99. All the algorithms we have discussed on paper are uniformly (G = some component must be repeated)."}], "references": [{"title": "On the Generation of Markov Decision Processes", "author": ["T. References Archibald", "K. McKinnon", "L. Thomas"], "venue": "Journal of the Operational Research Society,", "citeRegEx": "Archibald et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Archibald et al\\.", "year": 1995}, {"title": "Policy search by dynamic programming", "author": ["J.A. Bagnell", "S.M. Kakade", "A. Ng", "J. Schneider"], "venue": "In NIPS,", "citeRegEx": "Bagnell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bagnell et al\\.", "year": 2003}, {"title": "Error propagation for approximate policy and value iteration (extended version)", "author": ["A.M. Farahmand", "R. Munos", "Szepesv\u00e1ri", "Cs"], "venue": "In NIPS,", "citeRegEx": "Farahmand et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2010}, {"title": "Conservative and Greedy Approaches to Classification-based Policy Iteration", "author": ["M. Ghavamzadeh", "A. Lazaric"], "venue": "In AAAI,", "citeRegEx": "Ghavamzadeh and Lazaric,? \\Q2012\\E", "shortCiteRegEx": "Ghavamzadeh and Lazaric", "year": 2012}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In ICML,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Reinforcement Learning as Classification: Leveraging Modern Classifiers", "author": ["M. Lagoudakis", "R. Parr"], "venue": "In ICML,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Analysis of a Classification-based Policy Iteration Algorithm", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "In ICML,", "citeRegEx": "Lazaric et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lazaric et al\\.", "year": 2010}, {"title": "Error Bounds for Approximate Policy Iteration", "author": ["R. Munos"], "venue": "In ICML,", "citeRegEx": "Munos,? \\Q2003\\E", "shortCiteRegEx": "Munos", "year": 2003}, {"title": "Performance Bounds in Lp norm for Approximate Value Iteration", "author": ["R. Munos"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Munos,? \\Q2007\\E", "shortCiteRegEx": "Munos", "year": 2007}, {"title": "Performance Bounds for Lambda Policy Iteration and Application to the Game of Tetris", "author": ["B. Scherrer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Scherrer,? \\Q2013\\E", "shortCiteRegEx": "Scherrer", "year": 2013}, {"title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes", "author": ["B. Scherrer", "B. Lesner"], "venue": "In NIPS,", "citeRegEx": "Scherrer and Lesner,? \\Q2012\\E", "shortCiteRegEx": "Scherrer and Lesner", "year": 2012}, {"title": "Approximate Modified Policy Iteration", "author": ["Scherrer", "Bruno", "Ghavamzadeh", "Mohammad", "Gabillon", "Victor", "Geist", "Matthieu"], "venue": "In ICML,", "citeRegEx": "Scherrer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Scherrer et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming algorithm (Bagnell et al., 2003) to the infinite-horizon case (PSDP\u221e), and the recently proposed Non-Stationary Policy Iteration (NSPI(m)) (Scherrer & Lesner, 2012).", "startOffset": 283, "endOffset": 305}, {"referenceID": 7, "context": "Approximate Policy Iteration Schemes: A Comparison (cost-sensitive) classification problem (Lagoudakis & Parr, 2003a; Lazaric et al., 2010).", "startOffset": 91, "endOffset": 139}, {"referenceID": 6, "context": "Approximate Policy Iteration Schemes: A Comparison (cost-sensitive) classification problem (Lagoudakis & Parr, 2003a; Lazaric et al., 2010). With this operator in hand, we shall describe several Policy Iteration schemes in Section 2. Then Section 3 will provide a detailed comparative analysis of their performance guarantees, time complexities, and memory requirements. Section 4 will go on by providing experiments that will illustrate their behavior, and confirm our analysis. Finally, Section 5 will conclude and present future work. 2. Algorithms API We begin by describing the standard Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996). At each iteration k, the algorithm switches to the policy that is approximately greedy with respect to the value of the previous policy for some distribution \u03bd: \u03c0k+1 \u2190 G k+1(\u03bd, v\u03c0k). (2) If there is no error ( k = 0) and \u03bd assigns a positive weights to every state, it can easily be seen that this algorithm generates the same sequence of policies as exact Policy Iterations since from Equation (1) the policies are exactly greedy. CPI/CPI(\u03b1)/API(\u03b1) We now turn to the description of Conservative Policy Iteration (CPI) proposed by (Kakade & Langford, 2002). At iteration k, CPI (described in Equation (3)) uses the distribution d\u03c0k,\u03bd = (1 \u2212 \u03b3)\u03bd(I \u2212 \u03b3P\u03c0k) \u22121\u2014the discounted cumulative occupancy measure induced by \u03c0k when starting from \u03bd\u2014for calling the approximate greedy operator, and uses a stepsize \u03b1k to generate a stochastic mixture of all the policies that are returned by the successive calls to the approximate greedy operator, which explains the adjective \u201cconservative\u201d: \u03c0k+1 \u2190 (1\u2212 \u03b1k+1)\u03c0k + \u03b1k+1G k+1(d\u03c0k,\u03bd , v\u03c0k) (3) The stepsize \u03b1k+1 can be chosen in such a way that the above step leads to an improvement of the expected value of the policy given that the process is initialized according to the distribution \u03bd (Kakade & Langford, 2002). The original article also describes a criterion for deciding whether to stop or to continue. Though the adaptive stepsize and the stopping condition allows to derive a nice analysis, they are in practice conservative: the stepsize \u03b1k should be implemented with a line-search mechanism, or be fixed to some small value \u03b1. We will refer to this latter variation of CPI as CPI(\u03b1). It is natural to also consider the algorithm API(\u03b1) (mentioned by Lagoudakis & Parr (2003a)), a variation of API that is conservative like CPI(\u03b1) in the sense that it mixes the new policy with the previous ones with weights \u03b1 and 1\u2212\u03b1, but that directly uses the distribution \u03bd in the approximate greedy step: \u03c0k+1 \u2190 (1\u2212 \u03b1)\u03c0k + \u03b1G k+1(\u03bd, v\u03c0k) (4) Because it uses \u03bd instead of d\u03c0k,\u03bd , API(\u03b1) is simpler to implement than CPI(\u03b1)1.", "startOffset": 118, "endOffset": 2382}, {"referenceID": 1, "context": "This algorithm is a natural variation of the Policy Search by Dynamic Programming algorithm (PSDP) of Bagnell et al. (2003), originally proposed to tackle finite-horizon problems, to the infinite-horizon case; we thus refer to it as PSDP\u221e.", "startOffset": 102, "endOffset": 124}, {"referenceID": 1, "context": "This algorithm is a natural variation of the Policy Search by Dynamic Programming algorithm (PSDP) of Bagnell et al. (2003), originally proposed to tackle finite-horizon problems, to the infinite-horizon case; we thus refer to it as PSDP\u221e. To the best of our knowledge however, this variation has never been used in an infinite-horizon context. The algorithm is based on finite-horizon non-stationary policies. Given a sequence of stationary deterministic policies (\u03c0k) that the algorithm will generate, we will write \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01 the k-horizon policy that makes the first action according to \u03c0k, then the second action according to \u03c0k\u22121, etc. Its value is v\u03c3k = T\u03c0kT\u03c0k\u22121 . . . T\u03c01r. We will write \u2205 the \u201cempty\u201d non-stationary policy. Note that v\u2205 = r and that any infinite-horizon policy that begins with \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01, which we will (somewhat abusively) denote \u201c\u03c3k . . . \u201d has a value v\u03c3k... \u2265 v\u03c3k \u2212\u03b3Vmax. Starting from \u03c30 = \u2205, the algorithm implicitely builds a sequence of non-stationary policies (\u03c3k) by iteratively concatenating the policies that are returned by the approximate greedy operator: \u03c0k+1 \u2190 G k+1(\u03bd, v\u03c3k) (5) While the standard PSDP algorithm of Bagnell et al. (2003) considers a horizon T and makes T iterations, the algorithm we consider here has an indefinite number of iterations.", "startOffset": 102, "endOffset": 1201}, {"referenceID": 1, "context": "This algorithm is a natural variation of the Policy Search by Dynamic Programming algorithm (PSDP) of Bagnell et al. (2003), originally proposed to tackle finite-horizon problems, to the infinite-horizon case; we thus refer to it as PSDP\u221e. To the best of our knowledge however, this variation has never been used in an infinite-horizon context. The algorithm is based on finite-horizon non-stationary policies. Given a sequence of stationary deterministic policies (\u03c0k) that the algorithm will generate, we will write \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01 the k-horizon policy that makes the first action according to \u03c0k, then the second action according to \u03c0k\u22121, etc. Its value is v\u03c3k = T\u03c0kT\u03c0k\u22121 . . . T\u03c01r. We will write \u2205 the \u201cempty\u201d non-stationary policy. Note that v\u2205 = r and that any infinite-horizon policy that begins with \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01, which we will (somewhat abusively) denote \u201c\u03c3k . . . \u201d has a value v\u03c3k... \u2265 v\u03c3k \u2212\u03b3Vmax. Starting from \u03c30 = \u2205, the algorithm implicitely builds a sequence of non-stationary policies (\u03c3k) by iteratively concatenating the policies that are returned by the approximate greedy operator: \u03c0k+1 \u2190 G k+1(\u03bd, v\u03c3k) (5) While the standard PSDP algorithm of Bagnell et al. (2003) considers a horizon T and makes T iterations, the algorithm we consider here has an indefinite number of iterations. The algorithm can be stopped at any step k. The theory that we are about to describe suggests that one may return any policy that starts by the non-stationary policy \u03c3k. Since \u03c3k is an approximately good finite-horizon policy, and as we consider an infinite-horizon problem, a natural output that one may want to use in practice is the infinitehorizon policy that loops over \u03c3k, that we shall denote (\u03c3k) \u221e. In practice, controlling the greedy step with respect to d\u03c0k,\u03bd requires to generate samples from this very distribution. As explained by Kakade & Langford (2002), one such sample can be done by running one trajectory starting from \u03bd and following \u03c0k, stopping at each step with probability 1 \u2212 \u03b3.", "startOffset": 102, "endOffset": 1888}, {"referenceID": 9, "context": "NSPI(m) We originally devised the algorithmic scheme of Equation (5) (PSDP\u221e) as a simplified variation of the Non-Stationary PI algorithm with a growing period algorithm (NSPI-growing) (Scherrer & Lesner, 2012)2. With respect to Equation (5), the only difference of NSPIgrowing resides in the fact that the approximate greedy step is done with respect to the value v(\u03c3k)\u221e of the policy that loops infinitely over \u03c3k (formally the algorithm does \u03c0k+1 \u2190 G k+1(\u03bd, v(\u03c3k)\u221e)) instead of the value v\u03c3k of only the first k steps here. Following the intuition that when k is big, these two values will be close to each other, we ended up considering PSDP\u221e because it is simpler. NSPIgrowing suffers from the same memory drawback as CPI and PSDP\u221e. Interestingly, the work of Scherrer & Lesner (2012) contains another algorithm, Non-Stationary PI with a fixed period (NSPI(m)), that has a parameter that directly controls the number of policies stored in memory.", "startOffset": 186, "endOffset": 790}, {"referenceID": 7, "context": "Implementing this algorithm in practice can trivially be done through cost-sensitive classification in a way similar to Lazaric et al. (2010). It could also be done with a straight-forward extension of LSTD(\u03bb) to non-stationary policies.", "startOffset": 120, "endOffset": 142}, {"referenceID": 2, "context": "Relaxing the goal to controlling the weighted `p-norm for some p \u2265 2 allows to introduce some finer coefficients (Farahmand et al., 2010; Scherrer et al., 2012).", "startOffset": 113, "endOffset": 160}, {"referenceID": 12, "context": "Relaxing the goal to controlling the weighted `p-norm for some p \u2265 2 allows to introduce some finer coefficients (Farahmand et al., 2010; Scherrer et al., 2012).", "startOffset": 113, "endOffset": 160}, {"referenceID": 7, "context": "(2)) C 1 (1\u2212\u03b3)2 1 1\u2212\u03b3 log 1 1 (Lazaric et al., 2010) (= NSPI(1)) C 1 (1\u2212\u03b3)2 log 1 API(\u03b1) (Eq.", "startOffset": 30, "endOffset": 52}, {"referenceID": 7, "context": "(2)) C 1 (1\u2212\u03b3)2 1 1\u2212\u03b3 log 1 1 (Lazaric et al., 2010) (= NSPI(1)) C 1 (1\u2212\u03b3)2 log 1 API(\u03b1) (Eq. (4) C 1 (1\u2212\u03b3)2 1 \u03b1(1\u2212\u03b3) log 1 CPI(\u03b1) C 1 (1\u2212\u03b3)3 1 \u03b1(1\u2212\u03b3) log 1 CPI (Eq. (3)) C (1,0) 1 (1\u2212\u03b3)3 log 1 1 1\u2212\u03b3 1 log 1 C\u03c0\u2217 1 (1\u2212\u03b3)2 \u03b3 2 (Kakade & Langford, 2002) PSDP\u221e (Eq. (5)) C\u03c0\u2217 1 (1\u2212\u03b3)2 log 1 1 1\u2212\u03b3 log 1 (' NSPI(\u221e)) C (1) \u03c0\u2217 1 1\u2212\u03b3 1 1\u2212\u03b3 log 1 NSPI(m) (Eq. (6)) C 1 (1\u2212\u03b3)(1\u2212\u03b3m) 1 1\u2212\u03b3 log 1 m C m 1 (1\u2212\u03b3)2(1\u2212\u03b3m) log 1 1 1\u2212\u03b3 log 1 C (1) \u03c0\u2217 + \u03b3 mC 1\u2212\u03b3m 1 1\u2212\u03b3 1 1\u2212\u03b3 log 1 C\u03c0\u2217 + \u03b3 m C m(1\u2212\u03b3m) 1 (1\u2212\u03b3)2 log 1 1 1\u2212\u03b3 log 1 Table 1. Upper bounds on the performance guarantees for the algorithms. Except when references are given, the bounds are to our knowledge new. A comparison of API and CPI based on the two known bounds was done by Ghavamzadeh & Lazaric (2012). The first bound of NSPI(m) can be seen as an adaptation of that provided by Scherrer & Lesner (2012) for the more restrictive `\u221e-norm setting.", "startOffset": 31, "endOffset": 750}, {"referenceID": 7, "context": "(2)) C 1 (1\u2212\u03b3)2 1 1\u2212\u03b3 log 1 1 (Lazaric et al., 2010) (= NSPI(1)) C 1 (1\u2212\u03b3)2 log 1 API(\u03b1) (Eq. (4) C 1 (1\u2212\u03b3)2 1 \u03b1(1\u2212\u03b3) log 1 CPI(\u03b1) C 1 (1\u2212\u03b3)3 1 \u03b1(1\u2212\u03b3) log 1 CPI (Eq. (3)) C (1,0) 1 (1\u2212\u03b3)3 log 1 1 1\u2212\u03b3 1 log 1 C\u03c0\u2217 1 (1\u2212\u03b3)2 \u03b3 2 (Kakade & Langford, 2002) PSDP\u221e (Eq. (5)) C\u03c0\u2217 1 (1\u2212\u03b3)2 log 1 1 1\u2212\u03b3 log 1 (' NSPI(\u221e)) C (1) \u03c0\u2217 1 1\u2212\u03b3 1 1\u2212\u03b3 log 1 NSPI(m) (Eq. (6)) C 1 (1\u2212\u03b3)(1\u2212\u03b3m) 1 1\u2212\u03b3 log 1 m C m 1 (1\u2212\u03b3)2(1\u2212\u03b3m) log 1 1 1\u2212\u03b3 log 1 C (1) \u03c0\u2217 + \u03b3 mC 1\u2212\u03b3m 1 1\u2212\u03b3 1 1\u2212\u03b3 log 1 C\u03c0\u2217 + \u03b3 m C m(1\u2212\u03b3m) 1 (1\u2212\u03b3)2 log 1 1 1\u2212\u03b3 log 1 Table 1. Upper bounds on the performance guarantees for the algorithms. Except when references are given, the bounds are to our knowledge new. A comparison of API and CPI based on the two known bounds was done by Ghavamzadeh & Lazaric (2012). The first bound of NSPI(m) can be seen as an adaptation of that provided by Scherrer & Lesner (2012) for the more restrictive `\u221e-norm setting.", "startOffset": 31, "endOffset": 852}, {"referenceID": 0, "context": "More precisely, we consider Garnet problems first introduced by Archibald et al. (1995), which are a class of randomly constructed finite MDPs.", "startOffset": 64, "endOffset": 88}, {"referenceID": 9, "context": "As a matter of fact, several other dynamic programming algorithms, namely AVI (Munos, 2007), \u03bbPI (Scherrer, 2013), AMPI (Scherrer et al.", "startOffset": 78, "endOffset": 91}, {"referenceID": 10, "context": "As a matter of fact, several other dynamic programming algorithms, namely AVI (Munos, 2007), \u03bbPI (Scherrer, 2013), AMPI (Scherrer et al.", "startOffset": 97, "endOffset": 113}, {"referenceID": 12, "context": "As a matter of fact, several other dynamic programming algorithms, namely AVI (Munos, 2007), \u03bbPI (Scherrer, 2013), AMPI (Scherrer et al., 2012), come with guarantees involvWe recall that to our knowledge, the use of PSDP\u221e (PSDP in an infinite-horizon context) is not documented in the literature.", "startOffset": 120, "endOffset": 143}, {"referenceID": 10, "context": "By following the proof technique of Scherrer & Lesner (2012), writing \u0393k,m = (\u03b3P\u03c0k)(\u03b3P\u03c0k\u22121) \u00b7 \u00b7 \u00b7 (\u03b3P\u03c0k\u2212m+1) and ek+1 = max\u03c0\u2032 T\u03c0\u2032v\u03c0k,m \u2212 T\u03c0k+1v\u03c0k,m , one can show that: v\u03c0\u2217 \u2212 v\u03c0k,m \u2264 k\u22121 \u2211 i=0 (\u03b3P\u03c0\u2217) (I \u2212 \u0393k\u2212i,m)ek\u2212i + \u03b3Vmax.", "startOffset": 36, "endOffset": 61}, {"referenceID": 8, "context": "CPI, CPI(\u03b1), API(\u03b1): Conservative steps are addressed by a tedious generalization of the proof for API by Munos (2003). Due to lack of space, the proof is deferred to the Supplementary Material.", "startOffset": 106, "endOffset": 119}], "year": 2014, "abstractText": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming algorithm (Bagnell et al., 2003) to the infinite-horizon case (PSDP\u221e), and the recently proposed Non-Stationary Policy Iteration (NSPI(m)) (Scherrer & Lesner, 2012). For all algorithms, we describe performance bounds with respect the per-iteration error , and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API, but this comes at the cost of a relative\u2014exponential in 1 \u2014increase of the number of iterations. 2) PSDP\u221e enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP\u221e is proportional to their number of iterations, which may be problematic when the discount factor \u03b3 is close to 1 or the approximation error is close to 0; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis. Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).", "creator": "LaTeX with hyperref package"}}}