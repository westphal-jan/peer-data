{"id": "1405.3318", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2014", "title": "Adaptive Monte Carlo via Bandit Allocation", "abstract": "We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages.", "histories": [["v1", "Tue, 13 May 2014 22:29:14 GMT  (1197kb,D)", "http://arxiv.org/abs/1405.3318v1", "The 31st International Conference on Machine Learning (ICML 2014)"]], "COMMENTS": "The 31st International Conference on Machine Learning (ICML 2014)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["james neufeld", "andr\u00e1s gy\u00f6rgy", "csaba szepesv\u00e1ri", "dale schuurmans"], "accepted": true, "id": "1405.3318"}, "pdf": {"name": "1405.3318.pdf", "metadata": {"source": "META", "title": "Adaptive Monte Carlo via Bandit Allocation", "authors": ["James Neufeld", "Andr\u00e1s Gy\u00f6rgy", "Dale Schuurmans", "Csaba Szepesv\u00e1ri"], "emails": ["JNEUFELD@UALBERTA.CA", "GYORGY@UALBERTA.CA", "DAES@UALBERTA.CA", "CSABA.SZEPESVARI@UALBERTA.CA"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2. Background on Bandit Problems", "text": "The Multi-Armed Bandit (MAB) problem is a sequential assignment task in which an agent must select an action at each step to maximize his long-term payout, if only the payout of the selected action can be observed (Cesa-Bianchi & Lugosi, 2006; Bubeck & Cesa-Bianchi, 2012b).In the stochastic MAB problem (Robbins, 1952), the payout for each action is generated individually and identically (i.i.d.) from a fixed but unknown distribution, and the performance of an allocation policy can then be analyzed by defining the cumulative regret for each action."}, {"heading": "3. Combining Monte Carlo Estimators", "text": "Now we formalize the main problem we are looking at in this paper. Suppose we get a finite number of Monte Carlo estimators, k = 1,.., K, where the base estimator k produces a sequence of real estimate variables (Xk, t) (t = 1.2,...), the mean of which converges with the unknown target value. Observations by the different estimators are assumed to be independent of each other. We first assume that drawing a sample from each estimator takes constant time, so the estimators differ only in terms of the speed of their respective sample. Xk, n = 1n; observations by the different estimators n = 1Xk, t converge with \u00b5. The goal is to design a sequential estimation method that works in discrete time increments: for each round t = 1, 2;., based on the previous observations, the method selects an estimator."}, {"heading": "4. Combining Unbiased I.I.D. Estimators", "text": "Our main assumption in this section is the following: Assumption 4.1. Each estimator produces a sequence of i.e. random observations with common mean and finite variance; values from different estimators are independent of each other. Let's specify the distribution of samples from estimator k (Xk, 1). Let Vk = V (Xk, 1) and V (V, 1) fully determine the sequential estimation problem. Let Lk = V (Xk, 1) / t, hence min1 (K, t). We will then have the first main result of this section. Theorem 1 (Regret Identity)."}, {"heading": "5. Non-uniform Estimator Costs", "text": "Next, we look at the case when the base estimators need different amounts of time to generate observations. A consequence of the non-uniform estimation times, which we call non-uniform costs, is that the definitions of loss and regret need to be modified accordingly. If an estimator needs more time to produce an observation, it is less useful than another estimator producing observations with (say) identical variance, but in less time. To develop an appropriate concept of regret for this case, we introduce additional notation. Let Dk, m specify the time needed by estimator k to produce his mth observation, Xk, m. As before, we allow it."}, {"heading": "6. Experiments", "text": "We are conducting experimental studies in a number of scenarios to better understand the effectiveness of multi-armed bandit algorithms for adaptive Monte Carlo estimation."}, {"heading": "6.1. Preliminary Investigation: A 2-Estimator Problem", "text": "First, we look at the performance of allocation strategies based on a simple problem with 2 estimators. Note that this evaluation differs from standard assessments of stochastic bandits by the absence of individual parameter payout distributions, such as the Bernoulli, which may not have identical means but exhibit different deviations. This is an important detail, since stochastic bandit algorithms such as KL-UCB and TS are often evaluated based on individual parameter payout distributions, but their advantages in such scenarios may not extend to adaptive Monte Carlo estimates. Specifically, we look at problems when Xk, t = \u00b5 + sk (Zt \u2212 12), where Zt standard Bernoulli and sk (0, 1) are a separate scale parameter for k-1, 2}, while this design allows the maximum range for deviations by an average within a limited interval."}, {"heading": "6.2. Option Pricing", "text": "Next, we consider a more practical application of the Monte Carlo Adaptive Estimate to the problem of pricing financial instruments = = naive approach to calculating the strategy. In particular, however, it follows (Douc et al., 2007; Arouna, 2004) that we consider the problem of pricing European call options under the assumption that the interest rate evolves over time in accordance with the Cox-Ingersoll-Ross (CIR) model (Cox et al., 1985), a popular model in mathematical finance (details provided in Appendix F. In short, this model assumes that the interest rate r (t), as a function of time, follows t > 0, a square diffusion model; the price of a European caplet option with a \"strike price\" K > 0, \"nominal amount\" M > 0 and \"maturity\" T > 0 is then given by P = M exp (\u2212 T 0 r (t) dt) (r) (T), \u2212 K = 0.00K, the estimated value is 0.00K."}, {"heading": "6.3. Adaptive Annealed Importance Sampling", "text": "(...). (.). (.). (.). (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). (.). \"(.).\" (.). (.). (.). (.). (.). (.). (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.).).\" (.). \"(.).\" (.).). \"(.).\" (.). \"(.).\" (.).). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.).). (. (.).). (.). (.). (.).). (.). (.). (.).). (.). (.). (.).). (.). (.).). (.). (.). (.).). (.).). (.). (.).). (.).). (.).). (.). (.). (.).). (.).).). (.). (.).)."}, {"heading": "7. Conclusion", "text": "In this paper, we have introduced a new sequential decision-making strategy to compete with the most consistent Monte Carlo estimator in a finite pool. If each baseline estimator produces unbiased values at the same cost, we have shown that the sequential estimation problem maps a corresponding bandit problem, allowing future improvements in bandit algorithms to be transferred to the combination of unbiased estimators. We have also shown a weaker reduction in problems where the different estimators need different (possibly random) time to make an observation. We expect this work to inspire further research in this area. For example, one may consider combining not only a finite number but an infinite number of estimators with suitable bandit techniques (Bubeck et al., 2011), and / or exploiting the fact that the observation by an estimator can reveal information about the variance of others. This is the case, for example, when the males continue to use the most important variants of the bandit (although the number of new variants is known)."}, {"heading": "Acknowledgements", "text": "This work was supported by Alberta Innovates Technology Futures and NSERC. Part of this work was carried out during a visit by Cs.Sz. to Technion, Haifa and Microsoft Research, Redmond, whose support and hospitality are greatly appreciated."}, {"heading": "A. Proofs for Section 4", "text": "s (Xt) n be a sequence of i.i.d. random variables, and (X \u2032 t) t is its sequence in such a way that the decision whether to include Xt in the sequence is independent of future values in the sequence, i.e., Xs for s \u2265 t. Then the sequence (X \u2032 t) t is an i.i.d. sequence with the same distribution as (Xt) t, N.Proof. See Theorem 5.2 in Chapter III of (Doob, 1953).We also need Wald's second identity: Lemma 2 (Wald's Second Identity). Let (Xt)."}, {"heading": "B. Handling Unknown Ranges", "text": "The algorithms and limits of the rewards must be modified in relation to their own range, and then they must be compared with the reward method. (This means that we must scale the reward individually.) The algorithms and limits of the rewards must be modified in relation to their own range. (This means that we must scale the reward individually.) The algorithms must be modified by individually adjusting the reward. (This means that the reward is not variable) (This means that the reward does not need to be modified for each reward.) The algorithms (which are prepared for worst case distributions) are sensitive to overestimating the range, which would lead to an unnecessary deterioration of performance. (A better option is to scale each variable reward individually.) The algorithms must be modified by scaling each reward in relation to their own range and then scaling the rewards."}, {"heading": "C. Proofs for Section 5", "text": "In this section we provide the proof for theorem 5,4 A centered random variable X is subgaussian, if P (| X | \u2265 t) \u2264 c exp (\u2212 t2 / (2\u03c32)) for all real t with something c, \u03c3 > 0.We can replace the MSE at a given time t as L (A, t) = E [(S (t) \u2212 (N (t) \u2212 1) N (n).The first step is to replace the denominator with an itsexpecting value, the price for which results from the following result: Lemma 3. Leave S, N are random variables, N \u2265 0 so that E [N] > 0, Jimmy R. Leave D = \u2212 I {N = 0} (a \u2212 \u00b5) + I {N 6 = 0} (S N \u2212 p)."}, {"heading": "D. KL-Based Confidence Bound on Variance", "text": "The following Lemma gives an estimate of variance based on Kullback-Leibler divergence.Lemma 6. Let Y1,., Y2n and Q1,.. \u2212 \u2212 \u2212 Qn have two independent sequences of independent and identically distributed random variables having values in [0, 1]. In addition, for each t = 1,.., n, letV \u00b2 2t = 12t \u00b2 s = 1 Qt (Y2s \u2212 Y2s \u2212 1) 2. (23) Then for each of these variables it is > 0, P \u00b2 nt = 1 {KL (2V \u00b2 2t, 2E [Q1] V [Y1])). \u2212 \u2212 2bt. The Equation (5) of Garivier (2013) states that if Z1,..., Zn = Q1 \u00b2, identically distributed random variables, the values in [0, 1] and Kt \u00b2. \u2212 Z \u00b2."}, {"heading": "E. Details on Synthetic Experiments", "text": "When looking at alternative limited payout distributions, we examined 3 natural choices, abbreviated normal, uniform and scaled-Bernoulli. The latter non-standard distribution is a transformation of a Bernoulli described by three parameters, a midpoint m-R, a scale s-R +, and a Bernoulli parameter p.In particular, if X-0, 1} is a Bernoulli random variable with parameter p, the transformation m + (X \u2212 0.5) s returns the corresponding scaling-Bernoulli sample. We conducted experiments with these different distributions, keeping their deviations the same, to determine whether the shape of the distribution might affect the performance of the various Bandit algorithms. However, we could not detect any case where the shape of the distribution had a non-negligible effect on performance. As a result, we decided to conduct our experiments with scaled Bernoulli distributions, as they allow maximum deviation in the range."}, {"heading": "F. The Cox-Ingersoll-Ross Model", "text": "In the Cox-Ingersoll-Ross (CIR) model, the interest rate at the time t, which is referred to as r (t), follows a square root diffusion given by a stochastic differential equation of formdr (t) = (\u03b7 \u2212 \u03bar (t))) dt (t) dW (t), where \u03b7, \u0432 and \u03c3 are fixed, problem-specific, constants andW being a one-dimensional Brownian standard movement. The payment of an option for this interest rate at the time T (due) is given asM max (r (t) \u2212 K, 0), with the strip price K and nominal amount M being parameters of the option. The actual quantification of the interest rate, the price of an option at the time 0, is asP. = E [exp (\u2212 T 0 r (t) dt) dt) m max (r (T) \u2212 K, 0).The above integrals can approximate with the Monte Carlo method, by first delimiting the interest rate (i) to the point (t)."}, {"heading": "G. Bayesian Logistic Regression Example", "text": "We are considering a standardized Bayesian logistic regression model with the help of a multivariate Gaussian model. To set up the model, we need (for y-models) 0, 1, x-Rd, \u03b8-Rd, let p (y-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}], "references": [{"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "In Conference on Learning Theory, pp", "citeRegEx": "Agrawal and Goyal,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal", "year": 2012}, {"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": null, "citeRegEx": "Alon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2013}, {"title": "Adaptive Monte Carlo technique, a variance reduction technique", "author": ["B. Arouna"], "venue": "Monte Carlo Methods and Applications,", "citeRegEx": "Arouna,? \\Q2004\\E", "shortCiteRegEx": "Arouna", "year": 2004}, {"title": "Regret bounds and minimax policies under partial monitoring", "author": ["J. Audibert", "S. Bubeck"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Audibert and Bubeck,? \\Q2010\\E", "shortCiteRegEx": "Audibert and Bubeck", "year": 2010}, {"title": "Exploration\u2013 exploitation tradeoff using variance estimates in multi-armed bandits", "author": ["J. Audibert", "R. Munos", "Szepesv\u00e1ri", "Cs"], "venue": "Theoretical Computer Science,", "citeRegEx": "Audibert et al\\.,? \\Q1876\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 1876}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721,", "citeRegEx": "Bubeck and Cesa.Bianchi,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "Bandits with heavy tail", "author": ["S. Bubeck", "N. Cesa-Bianchi", "G. Lugosi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "Optimal adaptive policies for sequential allocation problems", "author": ["A. Burnetas", "M. Katehakis"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Burnetas and Katehakis,? \\Q1996\\E", "shortCiteRegEx": "Burnetas and Katehakis", "year": 1996}, {"title": "Kullback-Leibler upper confidence bounds for optimal sequential decision making", "author": ["O. Capp\u00e9", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz"], "venue": "Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Finite time analysis of stratified sampling for Monte Carlo", "author": ["A. Carpentier", "R. Munos"], "venue": "In NIPS-24,", "citeRegEx": "Carpentier and Munos,? \\Q2011\\E", "shortCiteRegEx": "Carpentier and Munos", "year": 2011}, {"title": "Minimax number of strata for online stratified sampling given noisy samples", "author": ["A. Carpentier", "R. Munos"], "venue": "In Algorithmic Learning Theory, pp", "citeRegEx": "Carpentier and Munos,? \\Q2012\\E", "shortCiteRegEx": "Carpentier and Munos", "year": 2012}, {"title": "Adaptive stratified sampling for Monte-Carlo integration of differentiable functions", "author": ["A. Carpentier", "R. Munos"], "venue": "In NIPS, pp", "citeRegEx": "Carpentier and Munos,? \\Q2012\\E", "shortCiteRegEx": "Carpentier and Munos", "year": 2012}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "In NIPS-24,", "citeRegEx": "Chapelle and Li,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li", "year": 2011}, {"title": "A theory of the term structure of interest rates", "author": ["J. Cox", "J. Ingersoll Jr.", "S. Ross"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "Cox et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Cox et al\\.", "year": 1985}, {"title": "Sequential Monte Carlo samplers", "author": ["P. Del Moral", "A. Doucet", "A. Jasra"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Moral et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moral et al\\.", "year": 2006}, {"title": "Minimum variance importance sampling via population Monte Carlo", "author": ["R. Douc", "A. Guillin", "J. Marin", "C. Robert"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "Douc et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Douc et al\\.", "year": 2007}, {"title": "Informational confidence bounds for self-normalized averages and applications", "author": ["A. Garivier"], "venue": "IEEE Information Theory Workshop", "citeRegEx": "Garivier,? \\Q2013\\E", "shortCiteRegEx": "Garivier", "year": 2013}, {"title": "Probability: a graduate course", "author": ["A. Gut"], "venue": null, "citeRegEx": "Gut,? \\Q2005\\E", "shortCiteRegEx": "Gut", "year": 2005}, {"title": "Thompson sampling: An asymptotically optimal finite time analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "In Algorithmic Learning Theory, pp", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Assessing approximate inference for binary gaussian process classification", "author": ["M. Kuss", "C. Rasmussen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Kuss and Rasmussen,? \\Q2005\\E", "shortCiteRegEx": "Kuss and Rasmussen", "year": 2005}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins", "year": 1985}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "In NIPS, pp", "citeRegEx": "Mannor and Shamir,? \\Q2011\\E", "shortCiteRegEx": "Mannor and Shamir", "year": 2011}, {"title": "Simulation studies in optimistic Bayesian sampling in contextual-bandit problems", "author": ["B. May", "D. Leslie"], "venue": "Technical report,", "citeRegEx": "May and Leslie,? \\Q2011\\E", "shortCiteRegEx": "May and Leslie", "year": 2011}, {"title": "Annealed importance sampling", "author": ["R. Neal"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Neal,? \\Q2001\\E", "shortCiteRegEx": "Neal", "year": 2001}, {"title": "Estimating ratios of normalizing constants using linked importance sampling", "author": ["R. Neal"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Neal,? \\Q2005\\E", "shortCiteRegEx": "Neal", "year": 2005}, {"title": "Handbook of Markov Chain Monte Carlo, chapter MCMC using Hamiltonian dynamics, pp. 113\u2013162", "author": ["R. Neal"], "venue": null, "citeRegEx": "Neal,? \\Q2011\\E", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Bayesian computational methods", "author": ["C. Robert"], "venue": null, "citeRegEx": "Robert,? \\Q2012\\E", "shortCiteRegEx": "Robert", "year": 2012}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}], "referenceMentions": [{"referenceID": 30, "context": "In the stochastic MAB problem (Robbins, 1952) the payoff for each action k \u2208 {1, 2, .", "startOffset": 30, "endOffset": 45}, {"referenceID": 30, "context": "The analysis of the stochastic MAB problem was pioneered by Lai & Robbins (1985) who showed that, when the payoff distributions are defined by a single parameter, the asymptotic regret of any sub-polynomially consistent policy (i.", "startOffset": 66, "endOffset": 81}, {"referenceID": 30, "context": "Lai & Robbins (1985) also presented an algorithm based on upper confidence bounds (UCB), which achieves a regret asymptotically matching the lower bound (for certain parametric distributions).", "startOffset": 6, "endOffset": 21}, {"referenceID": 5, "context": "Later, Auer et al. (2002a) proposed UCB1 (Algorithm 1), which broadens the practical use of UCB by dropping the Algorithm 1 UCB1 (Auer et al.", "startOffset": 7, "endOffset": 27}, {"referenceID": 5, "context": "Auer et al. (2002a) proved that, for any finite number of actions n, UCB1\u2019s regret is bounded by", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "A more recent algorithm is KL-UCB (Capp\u00e9 et al., 2013), where the confidence bound for arm k is based on solving", "startOffset": 34, "endOffset": 54}, {"referenceID": 32, "context": "Another approach that has received significant recent interest is Thompson sampling (TS) (Thompson, 1933): a Bayesian method where actions are chosen randomly in proportion to the posterior probability that their mean payoff is optimal.", "startOffset": 89, "endOffset": 105}, {"referenceID": 22, "context": "Indeed, the finite time regret of TS under Bernoulli payoff distributions closely matches the lower bound (3) (Kaufmann et al., 2012):", "startOffset": 110, "endOffset": 133}, {"referenceID": 4, "context": "For example, Audibert et al. (2009) show that with default parameters, UCB1 and UCB-V will select suboptimal arms with probability \u03a9(1/n), making tP(N < E[N ]\u2212 c \u221a E[N ]) = \u03a9(t).", "startOffset": 13, "endOffset": 36}, {"referenceID": 19, "context": "In particular, following (Douc et al., 2007; Arouna, 2004), we consider the problem of pricing European call options under the assumption that the interest rate evolves in time according to the Cox-Ingersoll-Ross (CIR) model (Cox et al.", "startOffset": 25, "endOffset": 58}, {"referenceID": 2, "context": "In particular, following (Douc et al., 2007; Arouna, 2004), we consider the problem of pricing European call options under the assumption that the interest rate evolves in time according to the Cox-Ingersoll-Ross (CIR) model (Cox et al.", "startOffset": 25, "endOffset": 58}, {"referenceID": 17, "context": ", 2007; Arouna, 2004), we consider the problem of pricing European call options under the assumption that the interest rate evolves in time according to the Cox-Ingersoll-Ross (CIR) model (Cox et al., 1985), a popular model in mathematical finance (details provided in Appendix F).", "startOffset": 188, "endOffset": 206}, {"referenceID": 19, "context": "Importantly, the task of adaptively allocating trials between different importance sampling estimators has been previously studied on this problem, using an unrelated technique known as d-kernel Population Monte Carlo (PMC) (Douc et al., 2007).", "startOffset": 224, "endOffset": 243}, {"referenceID": 19, "context": "We approximated the option prices under the same parameter settings as (Douc et al., 2007), namely, \u03bd = 0.", "startOffset": 71, "endOffset": 90}, {"referenceID": 31, "context": "Evaluating such quantities is useful for a variety of purposes, such as Bayesian model comparison and testing/training set evaluation (Robert, 2012).", "startOffset": 134, "endOffset": 148}, {"referenceID": 28, "context": "However, the desired quantities are notoriously difficult to estimate in many important settings, due in no small part to the fact that popular high-dimensional Monte Carlo strategies, such as Markov Chain Monte Carlo (MCMC) methods, cannot be directly applied (Neal, 2005).", "startOffset": 261, "endOffset": 273}, {"referenceID": 27, "context": "Nevertheless, a popular approach for approximating such values is annealed importance sampling (AIS) (Neal, 2001) (or more generally sequential Monte Carlo samplers (Del Moral et al.", "startOffset": 101, "endOffset": 113}, {"referenceID": 1, "context": "This is the case for example when the samplers use importance sampling, leading to the (new) stochastic variant of the problem known as \u201cbandits with side-observations\u201d (Mannor & Shamir, 2011; Alon et al., 2013).", "startOffset": 169, "endOffset": 211}], "year": 2014, "abstractText": "We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages.", "creator": "LaTeX with hyperref package"}}}