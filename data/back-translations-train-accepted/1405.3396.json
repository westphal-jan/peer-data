{"id": "1405.3396", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2014", "title": "Reducing Dueling Bandits to Cardinal Bandits", "abstract": "We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form \"A is preferred to B\" (as opposed to cardinal feedback like \"A has value 2.5\"), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions -- named $\\Doubler$, $\\MultiSbm$ and $\\DoubleSbm$ -- provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting. For $\\Doubler$ and $\\MultiSbm$ we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of $\\DoubleSbm$ which empirically outperforms the other two as well as previous algorithms in our experiments. In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms.", "histories": [["v1", "Wed, 14 May 2014 08:03:08 GMT  (132kb,D)", "http://arxiv.org/abs/1405.3396v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nir ailon", "zohar shay karnin", "thorsten joachims"], "accepted": true, "id": "1405.3396"}, "pdf": {"name": "1405.3396.pdf", "metadata": {"source": "CRF", "title": "Reducing Dueling Bandits to Cardinal Bandits", "authors": ["Nir Ailon", "Zohar Karnin"], "emails": ["nailon@cs.technion.ac.il", "zkarnin@gmail.com", "tj@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "When we interact with an online system, users reveal their preferences through the choices they make. Such a choice - often referred to as implicit feedback - can be clicking or tapping on a specific link in a web search ranking, or watching a particular movie within a set of recommendations. A link to a classic body of work in econometrics and empirical work in Informationsretrieval Joachims et al. (2007), such implicit feedback is typically viewed as an ordinary preference between alternatives (i.e. \"A is better than B\"), but it does not provide reliable cardinal ratings (i.e. \"A is very good, B is mediocre.\").ar Xiv: 140 5.33 96v1 [cs.LTo formalize the problem of learning preferences we consider the following interactive online learning model, which we call the Utility-Based Dueling Bandits Problem (UBDB)."}, {"heading": "2 Definitions", "text": "We assume that we will take a number of actions (or arming) that we define as MAVT. In a standard stochastic MAB (multiarmed bandit), we assume that we support an internal game (multiarmed bandit), each Bandit x-X has an unknown associated utility system (x). This utility system is defined by the algorithm.2 The regret at the time T of an algorithm is defined as R (T) = 1 (x) a random utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-utility-"}, {"heading": "2.1 Classic Stochastic MAB: A Short Review", "text": "We begin with the well-known UCB policy (algorithm 1) for MAB in the finite case. UCB's well-known analysis provides expected remorse limits. For the finite X case, we need a less well-known, robust guarantee that limits the likelihood of a sub-optimal arm too often. Lemma 2.2 is implicitly stated in Auer et al. (2002). For completeness, we provide explicit evidence in Appendix A.4 We have just defined a two-level model in which the distribution of the random variables is determined by the result of two other random variables. For simplicity, the reader is encouraged to assume that (ut, vt) is deterrent (xt) that most of the technical difficulties in what follows are already covered."}, {"heading": "3 UBDB Strategy for Large or Structured X", "text": "In this section, we look at UBDB in the case of a large or possibly infinite number of regrets X, and the linear link function. The setting in which X is large typically occurs when some basic structure for X exists, through which it is possible to obtain information about one arm by queries to another. Our approach, called Doubler, is best explained by considering the UBDB strategy as a competition between two players, with one controlling the choice of the left arm and the other, the choice of the right arm. Each player's goal is to win as many rounds as possible, so both players should play the arms with the most approximate value. As we work with a stochastic environment, it is not clear how to analyze a game in which both players are adaptive, and whether such a game would actually lead to a low regret (see also Section 5 for a related discussion). For this reason, we ensure that at all times a player has a fixed stochastic strategy that is updated frequently."}, {"heading": "4 UBDB Strategy for Unstructured X", "text": "In this section, we present and analyze an alternative reduction strategy, called MultiSBM, particularly suitable for the finite X case, where the elements of X typically have no structure. MultiSBM will not cause an additional logarithmic factor, as our previous approach did. Unlike the algorithms in Yue & Joachims (2011); Yue et al. (2012), we will avoid performing an elimination tournament, but will only resort to a standard MAB strategy by indexing according to the SBM proposal, and the binary choice will be indexed by an element in X. In step t, we will select a left arm xt X in a manner that will be explained shortly."}, {"heading": "5 A Heuristic Approach", "text": "In this section, we describe a heuristic called sparring for playing UBDB, which, however, shows extremely good performance in our experiments. Unfortunately, we have not yet been able to prove performance limits that explain its empirical performance. Sparring uses two SBMs that correspond to the left and right sides. In each round, the weapon pair is selected according to the strategies of the two corresponding SBMs. The SBM that corresponds to the chosen arm receives a feedback of 1, while the other receives 0. Indeed, the formal algorithm will emulate an algorithm for the opposing MAB. (The intuition for this idea comes from the analysis of an opposing version of UDBD, where it can easily be shown that the resulting expected regret of sparring is at most one constant time the regret of the two SBMs that emulate an algorithm for the opposing MAB. (We leave out the exact discussion and analysis for the opposing counterpart of UDBD.) We suspect that the regret of sparring is literally a problem for the opposing MAB."}, {"heading": "6 Notes", "text": "Lower limit: Our results contain upper limits for regretting the dueling bandit problem. We note that a suitable lower limit up to logarithmic terms can be indicated by a simple reduction to the MAB problem. This reduction is the opposite of the other presented here: Simulation of an SBM by using a UBDB solver. It is a simple exercise to achieve such a reduction, the regret of which the MAB problem is at most double: reduction to two SBMs. 1: SL, SR \u2190 two new SBMs via X 2: Reset (SL), Reset (SR), Reset (SR), t \u2190 1 3: while true do 4: xt \u2190 advance (SL); yt \u2190 advance (SR) 5: play (xt, yt), note the choice bt {0, 1} 6: Feedback (SL, 1bt = 0); Feedback (SR, 1bt = 1) 7: Problem (SR)."}, {"heading": "7 Experiments", "text": "We present several experiments comparing our algorithms with state-of-the-art baselines (+ 1). (1) INTERLEAVED FILTER (IF) Yue et al. (2012) and BEAT THE MEAN BANDIT (BTMB) Yue & Joachims (2011). Our experiments are comprehensive in that we include scenarios for which no boundaries have been drawn (e.g. non-linear link functions), as well as the much more general scenario in which BTMB was analyzed Yue & Joachims (2011).TMB, the X set of arms is {A, B, C, D, E, F}. For applications such as the interleaving search engines Chapelle et al. (2012), 6 arms are realistic. We considered 5 decisions about the expected value function \u00b5 (\u00b7) and 3 link functions 78, linear (x, y) = (1 + x \u2212 y)."}, {"heading": "8 Future work", "text": "We also analyzed only the linear selection function. See Appendix D for an extension of the results in Section 4 to other linkage functions. Both algorithms Doubler and MultiSBM treated the left and right side asymmetrically, which did not allow us to consider different expected evaluation functions for the left and right positions. 9 The Sparring algorithm is symmetrical, which additionally motivates the question of verifying its performance warranties.9 Such a case is actually motivated in an environment where, for example, the perceived evaluation of positions that appear lower in the list is lower, leading to distortions compared to positions above. The presumption in Section 5 regarding the regret of sparring is an interesting open problem. Similar to our idea of proof for the warranty of MultiSBM, there is clearly a positive feedback loop between the two SBMs in sparring: Thematically, often the left (or right) side of the arm that approximates the left or right side of the expected environment (for example, the right side of the mowing environment)."}, {"heading": "Acknowledgments", "text": "The authors thank anonymous reviewers for thorough and insightful reviews. This research was partially funded by NSF awards IIS-1217686 and IIS-1247696, a Marie Curie Reintegration Scholarship PIRG07-GA-2010-268403, a scholarship from the Israel Science Foundation 1271 / 33, and a scholarship from the Jacobs Technion-Cornell Innovation Institute."}, {"heading": "A Robustness of the UCB algorithm", "text": "For completeness, we present a proof of robustness for the UCB algorithm, which is presented as algorithm 1. Note that we have made no effort to bind the constants in the proofs. We start by presenting Chernoff's inequality in such a way that it is a tail limit for estimates of the in [0, 1].Lemma A.1. Leave Y1,., Yt be i.i.d variables supported in [0, 1]. Then, for any other number > 0 it applies that it is thatPr [1t]. i = 1 Yi [Yi] > \u03b5 e \u2212 2t\u03b5 e \u2212 2Recall that in our setting that there are K-arms, each with an expected reward. For convenience, we assume that the set of bandits X is the set {1,., K} and further assumption for the purpose of the analysis that arm 1 has the largest expected reward."}, {"heading": "B Proof of Theorem 3.1", "text": "Let B (T) set the upper limit of the expected regret of the SBM S (defined in line 1 of algorithm 2) after T-steps, over all possible supply distributions of the arm X. Fix a phase i in the algorithm. The length Ti of the phase is exactly 2i. For all time steps t within the phase, the left bandit xt is pulled from a fixed distribution. Let \u00b5 \u2032 the general expectation E [ut] = Ext [ut | xt] of the reward of the left arm in all steps t in the phase. Now, the SBM S (defined in line 1) plays a standard MAB2 game over the specified X-side with binary rewards. Let bt name the binary reward in the phase (within the phase). E [bt | vt, ut] = vt \u2212 ut + 12% [0, 1]. (B.1] By conditional expectation, for all y-phase i, E."}, {"heading": "C Proof of Theorem 4.2", "text": "To follow the evidence, it is important to understand that in MultiSBM (algorithm 3) exactly one SBM at each step in line 6. This means that the internal timer of each SBM after each iteration counter can (and usually is) stand strictly behind the iteration counter of the algorithm. We now assume that all coin flips are fixed in advance. This allows us to discuss the regret of the SBM Sx (line 1) after the total number of SBM Sx, even if the internal steps in practice the value of the SBM Sx (t) could be much greater than the total number of armrests T, and in fact it may not even exist. Notice that Sx sees a world in which the reward is binary, and the expected reward for Bandit X is accurate."}, {"heading": "D Extension to more General Models", "text": "Suppose the setting of section 4. In this section, for the sake of simplicity, we assume that for each t and any choice of xt, yt, the tools are deterministic ut = \u00b5 (xt), vt = \u00b5 (yt). In Yue & Joachims (2011) the problem of the dueling bandit is presented, where a more relaxed assumption is made regarding the probability of the results of duels. It is assumed that there is a certain order over the arms and that the Sainsbury's have two properties. \u2022 (Relaxed) Stochastic transitivity: For some \u03b3 1 and any pair of x x y we have the probability that the x solution (x) is the x solution for the x solution. \u2022 (Relaxed) Stochastic transitivity: For each x pair x x y we have both. (x) We assume that the x solution (x), x solution is for the x solution."}, {"heading": "E Proof of Observation 2.1", "text": "By definition, E is [Rchoicet | (xt, yt)] = \u00b5 (x, yt) \u2212 E [U choicet | (xt, yt)].But note that when defining the link function and the U choicet, E [U choicet | (xt, yt)] = \u03c6 (ut, vt) ut + \u03c6 (vt, ut) vt \u2265 ut + vt2using the assumption that for u > v, \u03c6 (u, v) > 1 / 2. Note now that the expression on the right is exactly E [Uav | (xt, yt)]. Therefore, E is [Rchoicet | (xt, yt)] \u2264 \u00b5 (x, yt) \u2212 E [Uavt | (xt, yt)] = E [Ravt]."}], "references": [{"title": "Optimal algorithms for online convex optimization with multi-point bandit feedback", "author": ["Agarwal", "Alekh", "Dekel", "Ofer", "Xiao", "Lin"], "venue": "In COLT, pp", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "Active learning using smooth relative regret approximations with applications", "author": ["Ailon", "Nir", "Begleiter", "Ron", "Ezra", "Esther"], "venue": "Journal of Machine Learning Research Proceedings Track,", "citeRegEx": "Ailon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2012}, {"title": "Toward a classification of finite partial-monitoring games", "author": ["Antos", "Andr\u00e1s", "Bart\u00f3k", "G\u00e1bor", "P\u00e1l", "D\u00e1vid", "Szepesv\u00e1ri", "Csaba"], "venue": "Theoretical Computer Science,", "citeRegEx": "Antos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2012}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f2", "Fischer", "Paul"], "venue": "Mach. Learn.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Decoupling exploration and exploitation in multi-armed bandits", "author": ["Avner", "Orly", "Mannor", "Shie", "Shamir", "Ohad"], "venue": "In ICML,", "citeRegEx": "Avner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Avner et al\\.", "year": 2012}, {"title": "An adaptive algorithm for finite stochastic partial monitoring", "author": ["Bart\u00f3k", "G\u00e1bor", "Zolghadr", "Navid", "Szepesv\u00e1ri", "Csaba"], "venue": "arXiv preprint arXiv:1206.6487,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2012}, {"title": "Large-scale validation and analysis of interleaved search evaluation", "author": ["O. Chapelle", "T. Joachims", "F. Radlinski", "Yue", "Yisong"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Chapelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2012}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Dani", "Varsha", "Hayes", "Thomas P", "Kakade", "Sham M"], "venue": "In COLT, pp", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Computing with noisy information", "author": ["Feige", "Uriel", "Raghavan", "Prabhakar", "Peleg", "David", "Upfal", "Eli"], "venue": "SIAM J. Comput.,", "citeRegEx": "Feige et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Feige et al\\.", "year": 1994}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Freund", "Yoav", "Iyer", "Raj D", "Schapire", "Robert E", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Freund et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2003}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R Herbrich", "Graepel", "Thore", "Obermayer", "Klaus"], "venue": "Book chapter, Advances in Large Margin Classifiers,", "citeRegEx": "Herbrich et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2000}, {"title": "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search", "author": ["T. Joachims", "L. Granka", "Pan", "Bing", "H. Hembrooke", "F. Radlinski", "G. Gay"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Joachims et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2007}, {"title": "Noisy binary search and its applications", "author": ["Karp", "Richard M", "Kleinberg", "Robert"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Karp et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Karp et al\\.", "year": 2007}, {"title": "From bandits to experts: On the value of sideobservations", "author": ["Mannor", "Shie", "Shamir", "Ohad"], "venue": "In NIPS, pp", "citeRegEx": "Mannor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2011}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["Piccolboni", "Antonio", "Schindelhauer", "Christian"], "venue": "In Computational Learning Theory,", "citeRegEx": "Piccolboni et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Piccolboni et al\\.", "year": 2001}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the AMS,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Discrete Choice Methods with Simulation", "author": ["Train", "Keneth"], "venue": null, "citeRegEx": "Train and Keneth.,? \\Q2009\\E", "shortCiteRegEx": "Train and Keneth.", "year": 2009}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Yue", "Yisong", "T. Joachims"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}, {"title": "Beat the mean bandit", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In ICML, pp", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}, {"title": "The k-armed dueling bandits problem", "author": ["Yue", "Yisong", "Broder", "Josef", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Yue et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Connecting to a classic body of work in econometrics and empirical work in information retrieval Joachims et al. (2007), such implicit feedback is typically viewed as an ordinal preference between alternatives (i.", "startOffset": 97, "endOffset": 120}, {"referenceID": 17, "context": "To formalize the problem of learning from preferences, we consider the following interactive online learning model, which we call the Utility-Based Dueling Bandits Problem (UBDB) similar to Yue et al. (2012); Yue & Joachims (2011).", "startOffset": 190, "endOffset": 208}, {"referenceID": 17, "context": "To formalize the problem of learning from preferences, we consider the following interactive online learning model, which we call the Utility-Based Dueling Bandits Problem (UBDB) similar to Yue et al. (2012); Yue & Joachims (2011). At each iteration t, the learning system presents two actions xt, yt \u2208 X to the user, where X is the set (either finite or infinite) of possible actions.", "startOffset": 190, "endOffset": 231}, {"referenceID": 6, "context": "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user\u2019s clicking behavior.", "startOffset": 218, "endOffset": 241}, {"referenceID": 6, "context": "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user\u2019s clicking behavior. The purpose of this paper is to show how UBDB can be reduced to the conventional (cardinal) stochastic Multi-Armed Bandit (MAB) problem1, which has been studied since 1952 Robbins (1952). In MAB, the system chooses only a single action xt \u2208 X in each round and directly observes its cardinal reward ut, which is assumed to be drawn from a latent but fixed distribution attached to xt.", "startOffset": 218, "endOffset": 570}, {"referenceID": 6, "context": "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user\u2019s clicking behavior. The purpose of this paper is to show how UBDB can be reduced to the conventional (cardinal) stochastic Multi-Armed Bandit (MAB) problem1, which has been studied since 1952 Robbins (1952). In MAB, the system chooses only a single action xt \u2208 X in each round and directly observes its cardinal reward ut, which is assumed to be drawn from a latent but fixed distribution attached to xt. The set X in the traditional MAB game is of finite cardinality K. In more general settings Dani et al. (2008); Mannor & Shamir (2011), this set can be infinite but structured in some way.", "startOffset": 218, "endOffset": 878}, {"referenceID": 6, "context": "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user\u2019s clicking behavior. The purpose of this paper is to show how UBDB can be reduced to the conventional (cardinal) stochastic Multi-Armed Bandit (MAB) problem1, which has been studied since 1952 Robbins (1952). In MAB, the system chooses only a single action xt \u2208 X in each round and directly observes its cardinal reward ut, which is assumed to be drawn from a latent but fixed distribution attached to xt. The set X in the traditional MAB game is of finite cardinality K. In more general settings Dani et al. (2008); Mannor & Shamir (2011), this set can be infinite but structured in some way.", "startOffset": 218, "endOffset": 902}, {"referenceID": 6, "context": "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user\u2019s clicking behavior. The purpose of this paper is to show how UBDB can be reduced to the conventional (cardinal) stochastic Multi-Armed Bandit (MAB) problem1, which has been studied since 1952 Robbins (1952). In MAB, the system chooses only a single action xt \u2208 X in each round and directly observes its cardinal reward ut, which is assumed to be drawn from a latent but fixed distribution attached to xt. The set X in the traditional MAB game is of finite cardinality K. In more general settings Dani et al. (2008); Mannor & Shamir (2011), this set can be infinite but structured in some way. Dani et al. (2008), for example, assume a stochastic setting in which X is a convex, bounded subset of R, and the expectation \u03bc(x) of the corresponding value distribution is \u3008\u03bc, x\u3009, where \u03bc \u2208 R is an unknown coefficient vector and \u3008\u00b7, \u00b7\u3009 is the inner product with respect to the standard basis.", "startOffset": 218, "endOffset": 975}, {"referenceID": 1, "context": "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T .", "startOffset": 75, "endOffset": 94}, {"referenceID": 1, "context": "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB.", "startOffset": 75, "endOffset": 1190}, {"referenceID": 1, "context": "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB. In particular, this paper provides general reductions that make it possible to transfer the large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their regret is asymptotically comparable to the tournament elimination strategies in Yue et al. (2012); Yue & Joachims (2011) for the finite case as T \u2192\u221e, and better than the regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the infinite case (albeit in a more restricted setting).", "startOffset": 75, "endOffset": 1639}, {"referenceID": 1, "context": "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB. In particular, this paper provides general reductions that make it possible to transfer the large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their regret is asymptotically comparable to the tournament elimination strategies in Yue et al. (2012); Yue & Joachims (2011) for the finite case as T \u2192\u221e, and better than the regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the infinite case (albeit in a more restricted setting).", "startOffset": 75, "endOffset": 1662}, {"referenceID": 1, "context": "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB. In particular, this paper provides general reductions that make it possible to transfer the large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their regret is asymptotically comparable to the tournament elimination strategies in Yue et al. (2012); Yue & Joachims (2011) for the finite case as T \u2192\u221e, and better than the regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the infinite case (albeit in a more restricted setting).", "startOffset": 75, "endOffset": 1786}, {"referenceID": 1, "context": "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB. In particular, this paper provides general reductions that make it possible to transfer the large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their regret is asymptotically comparable to the tournament elimination strategies in Yue et al. (2012); Yue & Joachims (2011) for the finite case as T \u2192\u221e, and better than the regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the infinite case (albeit in a more restricted setting). In our setting, the reward and feedback of the agent playing the online game are, in some sense, orthogonal to each other, or decoupled. A different type of decoupling was also considered in Avner et al.\u2019s work Avner et al. (2012), although this work cannot be compared to theirs.", "startOffset": 75, "endOffset": 2078}, {"referenceID": 0, "context": "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step.", "startOffset": 0, "endOffset": 37}, {"referenceID": 0, "context": "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al.", "startOffset": 0, "endOffset": 182}, {"referenceID": 0, "context": "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al.", "startOffset": 0, "endOffset": 204}, {"referenceID": 0, "context": "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al.", "startOffset": 0, "endOffset": 225}, {"referenceID": 0, "context": "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al.", "startOffset": 0, "endOffset": 274}, {"referenceID": 0, "context": "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al. (1994), which are not the setting studied here.", "startOffset": 0, "endOffset": 295}, {"referenceID": 0, "context": "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al. (1994), which are not the setting studied here. Finally, our results connect multi-armed bandits and online optimization to the classic econometric theory of discrete choice, with its use of preferential or choice information to recover values of goods (see Train (2009) and references therein).", "startOffset": 0, "endOffset": 559}, {"referenceID": 0, "context": "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al. (1994), which are not the setting studied here. Finally, our results connect multi-armed bandits and online optimization to the classic econometric theory of discrete choice, with its use of preferential or choice information to recover values of goods (see Train (2009) and references therein). Another important topic related to our work is that of partial monitoring games. The idea was introduced by Piccolboni & Schindelhauer (2001). The objective in partial monitoring is to choose at each round an action from some finite set of actions, and receive a reward based on some unknown function chosen by an oblivious process.", "startOffset": 0, "endOffset": 726}, {"referenceID": 2, "context": "In both cases the regret is lower bounded by \u221a T , which is inapplicable to our setting (see Antos et al. (2012) for a characterization of partial monitoring problems).", "startOffset": 93, "endOffset": 113}, {"referenceID": 2, "context": "In both cases the regret is lower bounded by \u221a T , which is inapplicable to our setting (see Antos et al. (2012) for a characterization of partial monitoring problems). Bart\u00f3k et al. Bart\u00f3k et al. (2012) do present problem dependent bounds.", "startOffset": 93, "endOffset": 204}, {"referenceID": 6, "context": "In practice, these two identical alternatives would be displayed as one, as would naturally happen in interleaved retrieval evaluation Chapelle et al. (2012). It should be also clear that playing (x\u2217, x\u2217) is pure exploitation, because the feedback is then an unbiased coin with zero exploratory information.", "startOffset": 135, "endOffset": 158}, {"referenceID": 3, "context": "2 is implicitly proved in Auer et al. (2002). For completeness, we provide an explicit proof in Appendix A.", "startOffset": 26, "endOffset": 45}, {"referenceID": 7, "context": "5 This setting was dealt with by Dani et al. (2008). They provide an algorithm for this setting that could be thought of as linear optimization under noisy feedback.", "startOffset": 33, "endOffset": 52}, {"referenceID": 7, "context": "3 (Dani et al. 2008).", "startOffset": 2, "endOffset": 20}, {"referenceID": 7, "context": "3 (Dani et al. 2008). Algorithm CONFIDENCEBALL1 (resp. CONFIDENCEBALL2) of Dani et al. (2008), provides an expected regret ofO (\u221a dT log T ) (resp.", "startOffset": 3, "endOffset": 94}, {"referenceID": 7, "context": "4 (Dani et al. 2008).", "startOffset": 2, "endOffset": 20}, {"referenceID": 7, "context": "4 (Dani et al. 2008). Assume the \u2206-gap case. Algorithm CONFIDENCEBALL1 (resp. CONFIDENCEBALL2) of Dani et al. (2008), provides an expected regret of O ( \u2206\u22121d2 log T ) (resp.", "startOffset": 3, "endOffset": 117}, {"referenceID": 7, "context": "By setting the SBM S used in Line 1 as the algorithms CONFIDENCEBALL1 or CONFIDENCEBALL2 of Dani et al. (2008), we obtain the following:", "startOffset": 92, "endOffset": 111}, {"referenceID": 17, "context": "Unlike the algorithms in Yue & Joachims (2011); Yue et al. (2012), we will avoid running an elimination tournament, but just resort to a standard MAB strategy by reduction.", "startOffset": 48, "endOffset": 66}, {"referenceID": 3, "context": "Interestingly, the assumption is satisfied by the UCB algorithm Auer et al. (2002) (as detailed in Lemma 2.", "startOffset": 64, "endOffset": 83}, {"referenceID": 3, "context": "Interestingly, the assumption is satisfied by the UCB algorithm Auer et al. (2002) (as detailed in Lemma 2.2). Recall that x\u2217 \u2208 X denotes an arm with largest valuation \u03bc(x), and that \u2206x := \u03bc(x\u2217)\u2212 \u03bc(x) for all x \u2208 X . Assume \u2206x > 0 for all x 6= x\u2217.6 Definition 4.1. Let Tx be the number of times a (sub-optimal) arm x \u2208 X is played when running the policy T rounds. A MAB policy is said to be \u03b1-robust when it has the following property: for all s \u2265 4\u03b1\u2206\u22122 x ln(T ), it holds that Pr[Tx > s] < 2 \u03b1 (s/2) \u2212\u03b1. Recall that as discussed in Section 2.1, in Auer et al.\u2019s (2002) classic UCB policy this property can be achieved by slightly enlarging the confidence region.", "startOffset": 64, "endOffset": 571}, {"referenceID": 16, "context": "We now present several experiments comparing our algorithms with baselines consisting of the state-of-the-art INTERLEAVED FILTER (IF) Yue et al. (2012) and BEAT THE MEAN BANDIT (BTMB) Yue & Joachims (2011).", "startOffset": 134, "endOffset": 152}, {"referenceID": 16, "context": "We now present several experiments comparing our algorithms with baselines consisting of the state-of-the-art INTERLEAVED FILTER (IF) Yue et al. (2012) and BEAT THE MEAN BANDIT (BTMB) Yue & Joachims (2011). Our experiments are exhaustive, as we include scenarios for which no bounds were derived (e.", "startOffset": 134, "endOffset": 206}, {"referenceID": 16, "context": "We now present several experiments comparing our algorithms with baselines consisting of the state-of-the-art INTERLEAVED FILTER (IF) Yue et al. (2012) and BEAT THE MEAN BANDIT (BTMB) Yue & Joachims (2011). Our experiments are exhaustive, as we include scenarios for which no bounds were derived (e.g. nonlinear link functions), as well as the much more general scenario in which BTMB was analyzed Yue & Joachims (2011). Henceforth, the set X of arms is {A,B,C,D,E, F}.", "startOffset": 134, "endOffset": 420}, {"referenceID": 6, "context": "For applications such as the interleaving search engines Chapelle et al. (2012), 6 arms is realistic.", "startOffset": 57, "endOffset": 80}], "year": 2014, "abstractText": "We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form \u201cA is preferred to B\u201d (as opposed to cardinal feedback like \u201cA has value 2.5\u201d), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions \u2013 named Doubler, MultiSBM and Sparring \u2013 provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting. For Doubler and MultiSBM we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of Sparring which empirically outperforms the other two as well as previous algorithms in our experiments. In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms.", "creator": "LaTeX with hyperref package"}}}