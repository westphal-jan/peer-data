{"id": "1405.3536", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2014", "title": "Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques", "abstract": "In many recommendation applications such as news recommendation, the items that can be rec- ommended come and go at a very fast pace. This is a challenge for recommender systems (RS) to face this setting. Online learning algorithms seem to be the most straight forward solution. The contextual bandit framework was introduced for that very purpose. In general the evaluation of a RS is a critical issue. Live evaluation is of- ten avoided due to the potential loss of revenue, hence the need for offline evaluation methods. Two options are available. Model based meth- ods are biased by nature and are thus difficult to trust when used alone. Data driven methods are therefore what we consider here. Evaluat- ing online learning algorithms with past data is not simple but some methods exist in the litera- ture. Nonetheless their accuracy is not satisfac- tory mainly due to their mechanism of data re- jection that only allow the exploitation of a small fraction of the data. We precisely address this issue in this paper. After highlighting the limita- tions of the previous methods, we present a new method, based on bootstrapping techniques. This new method comes with two important improve- ments: it is much more accurate and it provides a measure of quality of its estimation. The latter is a highly desirable property in order to minimize the risks entailed by putting online a RS for the first time. We provide both theoretical and ex- perimental proofs of its superiority compared to state-of-the-art methods, as well as an analysis of the convergence of the measure of quality.", "histories": [["v1", "Wed, 14 May 2014 15:29:02 GMT  (197kb,D)", "http://arxiv.org/abs/1405.3536v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["j\u00e9r\u00e9mie mary", "philippe preux", "olivier nicol"], "accepted": true, "id": "1405.3536"}, "pdf": {"name": "1405.3536.pdf", "metadata": {"source": "META", "title": "Improving offline evaluation of contextual bandit algorithms  via bootstrapping techniques", "authors": ["Olivier Nicol", "J\u00e9r\u00e9mie Mary", "Philippe Preux"], "emails": ["OLI.NICOL@GMAIL.COM", "JEREMIE.MARY@INRIA.FR", "PHILIPPE.PREUX@UNIV-LILLE3.FR"], "sections": [{"heading": null, "text": "Reports from the 31st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W & CP Volume 32. Copyright 2014 by the author (s)."}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Background on bandits and notations", "text": "We motivated the need for online learning solutions to deal with dynamic recommendations, and a natural way to model this situation is an increased learning problem (Sutton & Barto, 1998), or more precisely, the use of the contextual bandit framework (Lu et al., 2010), which was introduced precisely for the purpose of recommending news."}, {"heading": "2.1. Contextual Bandits", "text": "The bandit problem is also known in literature as the multi-arm bandit problem and other variations; this problem can be traced back to Robbins and Munro in 1952 (Robbins, 1952) and even Thompson in 1933 (Thompson, 1933); there are many variations in the definition of the problem; the contextual bandit framework as defined in (Langford & Zhang, 2007) is as follows: Let X be an arbitrary input space and A = {1.. K} be a set of K actions; let D be a distribution across tuples (x, ~ r) with x-X and ~ r exploitation (0, 1) K be a vector of rewards."}, {"heading": "2.2. Evaluation", "text": "We define a contextual algorithm A as input of an ordered list of (x, a, r) triplets (history) and output of a policy \u03c0. A policy \u03c0 maps X to A, i.e. selects an action in a particular context. Note that we are also interested in evaluating strategies. In our environment, which is most popular, an algorithm is called optimal when the expectation of the sum of rewards by T-steps is maximized: GA (T, D) def = ED T \u2211 t = 1 ~ rt [A (xt)].For simplicity's sake, we define the payout per try as the average click rate by T-steps: gA (T, D) def = GA (T) TNote that for a static algorithm (i.e., that always issues the same policy \u03c0) we have the following: T, gA (T, D) = gA (1, D) degf = A (Note that at this point, we take an emergency D)."}, {"heading": "3. The time acceleration issue with replay methodologies", "text": "This is why we consider the importance of our work in terms of the way we view it in terms of the way we view it in terms of the way we view it in terms of the way we view it in terms of the way we view it in terms of the way we view it in terms of the way we view it in terms of the way we view it in terms of the way we view it in terms of the way we view it in terms of the way we view it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of the world, how it in terms of how it in terms of the world, how it in terms of the world, how it in terms of how it in terms of the world, how it in terms of how it in terms of the world, how it in terms of how it in terms of the world, how it in terms of how it in terms of the world, how it in terms of how it in terms of the world, how it in terms of how it in terms of the world, how it in terms of how it in terms of how it in terms of the world, how it in terms of the way it in terms of the world, how it in terms of how it is in terms of how it relates to the way it relates to the world, how it relates to the way it relates to the way it relates to the way it relates to the world, how it relates to the way it relates to the way it relates to the world, how it relates to the way it relates to the way it relates to the world, how it relates to the way it relates to the way it relates to the way it relates to the world, how it relates to the way it relates to the way it relates to the world, how it relates to the way it relates to the way it relates to the way it relates to the world, how it relates to the way it relates to the way it relates"}, {"heading": "4. Bootstrapped Replay on Expanded Data", "text": "Now that the deficiency of the repeat method (T) has been understood, we are looking for another method of offline evaluation that is not affected by the time acceleration. (B) So the idea is to calculate the empirical distribution. (B) The empirical distribution of quantity T is generated by S (Efron, 1979). (B) The new datasets S1,... SB of quantity T are generated by being generated by S (S). (D) This bootstrap procedure is a way to approximate the data. (D) The underlying distribution of the data. (D) The underlying distribution of the data to all fields C (T), an estimate of CTR (T)."}, {"heading": "5. Theoretical analysis", "text": "In this section we present a theoretical analysis of our evaluation method BRED. (The core loop in BRED is an unchangeable distribution loop. (The core loop in BRED is an unchangeable distribution loop.) The number of evaluations - which is also the number of non-rejections - is a random variable, the T (b). Theorem 1. It is a recommendation that generates a fixed policy over time (this hypothesis can be weakened as discussed in remark 2), \u2022 K points can be recommended at any time step (CTRA) allowing an extension as asymptotic escance (CTRA)."}, {"heading": "6. Experiments in realistic settings", "text": "Since we have proven that BRED offers promising theoretical guarantees in the environment presented in (Li et al., 2011), we now want to compare its empirical performance with that of the repetition method."}, {"heading": "6.1. Synthetic data and discussing Jittering", "text": "The first set of experiments was carried out on synthetic data. In fact, we had to be able to compare the errors in estimating the two methods on different fixed-size datasets relative to the basic truth: an evaluation against the model itself. Before going any further, we describe the model we used. It is a linear model with Gaussian noise (as in (Li et al., 2010) and was constructed as follows: \u2022 a fixed plot (or message set) of size K = 10. \u2022 The context space is X = R15. Each context x is generated as the sum c + n, in which c-N (0, 1) and n-N-N (0, 12) are measured. \u2022 The CTR of a message displayed in a context x is given by qi + wTi c. Note the non-contextual element qi and that the noise n is ignored, where c-N (0, 1) and n-N-date (0, 12) are measured."}, {"heading": "6.2. Real data", "text": "Adapting the playback to a real dataset that meets dynamic recommendations is simple, although it leads to distorted estimates. BRED really needs the assumption of a static world to perform the bootstrap resamples. Therefore, BRED must be run on consecutive windows of the dataset on which a static assumption can be made. This creates a bias / variance trade-off: If the windows are too large, some of the dynamic properties of the world can be deleted (bias). On the contrary, a too small window leads to very variable bootstrap estimates. To simplify things, we conducted experiments that assume a static world on small parts of the Yahoo! R6B dataset. We actually took the smallest number of portions, so that a particular part has a fixed pool of messages (about 630 portions). This experiment is similar to what is done in KREA (Li et al., 2011): The authors measured the error of the UCB-Resamples we estimated part of the UCB-Resamples that we have a 630 portions of the UCB-Resi."}, {"heading": "7. Conclusion", "text": "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +"}], "references": [{"title": "Spatio-temporal models for estimating clickthrough rate", "author": ["Agarwal", "Deepak", "Chen", "Bee-Chung", "Elango", "Pradheep"], "venue": "In Proceedings of the 18th international conference on World wide web(WWW),", "citeRegEx": "Agarwal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f2", "Fischer", "Paul"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f2", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM J. Comput.,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Counterfactual reasoning and learning systems", "author": ["Bottou", "L\u00e9on", "Peters", "Jonas", "Qui\u00f1onero Candela", "Joaquin", "Charles", "Denis X", "Chickering", "D. Max", "Portugualy", "Elon", "Ray", "Dipankar", "Simard", "Patrice", "Snelson", "Ed"], "venue": "Technical report,", "citeRegEx": "Bottou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2012}, {"title": "Doubly robust policy evaluation and learning", "author": ["Dud\u0131\u0301k", "Miroslav", "Langford", "John", "Li", "Lihong"], "venue": "CoRR, abs/1103.4601,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Bootstrap methods: Another look at the jackknife", "author": ["B. Efron"], "venue": "The Annals of Statistics,", "citeRegEx": "Efron,? \\Q1979\\E", "shortCiteRegEx": "Efron", "year": 1979}, {"title": "Omnipress", "author": ["USA", "July"], "venue": "ISBN 978-1-4503-1285-1.", "citeRegEx": "USA and July,? 2012", "shortCiteRegEx": "USA and July", "year": 2012}, {"title": "Controlled experiments on the web: Survey and practical guide", "author": ["Kohavi", "Ron", "Longbotham", "Roger", "Sommerfield", "Dan", "Henne", "Randal M"], "venue": "Journal of Data Mining and Knowledge Discovery,", "citeRegEx": "Kohavi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kohavi et al\\.", "year": 2009}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["Langford", "John", "Zhang", "Tong"], "venue": "In Proc. NIPS,", "citeRegEx": "Langford et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2007}, {"title": "Exploration scavenging", "author": ["Langford", "John", "Strehl", "Alexander", "Wortman", "Jennifer"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Langford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2008}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Schapire", "Robert E"], "venue": "In Proceedings of the 19th international conference on World wide web (WWW),", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms", "author": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Wang", "Xuanhui"], "venue": "Proc. Web Search and Data Mining (WSDM),", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Contextual multi-armed bandits", "author": ["T. Lu", "D. P\u00e1l", "M. P\u00e1l"], "venue": "In Proc. of the 13th Artificial Intelligence and Statistics (AI & Stats), JMLR: W&CP", "citeRegEx": "Lu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2010}, {"title": "Data-driven evaluation of Contextual Bandit algorithms and applications to Dynamic Recommendation", "author": ["Nicol", "Olivier"], "venue": "PhD thesis,", "citeRegEx": "Nicol and Olivier.,? \\Q2014\\E", "shortCiteRegEx": "Nicol and Olivier.", "year": 2014}, {"title": "Some aspects of the sequential design of experiments", "author": ["Robbins", "Herbert"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Robbins and Herbert.,? \\Q1952\\E", "shortCiteRegEx": "Robbins and Herbert.", "year": 1952}, {"title": "Multivariate Density Estimation: Theory, Practice, and Visualization", "author": ["D.W. Scott"], "venue": "Wiley Series in Probability and Statistics. Wiley,", "citeRegEx": "Scott,? \\Q1992\\E", "shortCiteRegEx": "Scott", "year": 1992}, {"title": "The bootstrap: To smooth or not to smooth? Biometrika", "author": ["BW Silverman", "Young", "GA"], "venue": null, "citeRegEx": "Silverman et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Silverman et al\\.", "year": 1987}, {"title": "Learning from logged implicit exploration data", "author": ["Strehl", "Alexander L", "Langford", "John", "Li", "Lihong", "Kakade", "Sham"], "venue": "In Proc. NIPS,", "citeRegEx": "Strehl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2010}, {"title": "Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning Series", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "R6B - Yahoo! front page today module user click log dataset, publicly released via the Yahoo! webscope", "author": ["Yahoo! Research"], "venue": null, "citeRegEx": "Research.,? \\Q2012\\E", "shortCiteRegEx": "Research.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "For such systems, the best way to compare the performance of two algorithms is to perform A/B testing on a subset of the web audience (Kohavi et al., 2009).", "startOffset": 134, "endOffset": 155}, {"referenceID": 12, "context": "A natural way to model this situation is as a reinforcement learning problem (Sutton & Barto, 1998), and more precisely using the contextual bandit framework (Lu et al., 2010) that was introduced for the very purpose of news recommendation.", "startOffset": 158, "endOffset": 175}, {"referenceID": 19, "context": "This problem can be traced back to Robbins and Munro in 1952 (Robbins, 1952) and even Thompson in 1933 (Thompson, 1933).", "startOffset": 103, "endOffset": 119}, {"referenceID": 1, "context": "One can for instance mention the UCB algorithm (Auer et al., 2002) that optimistically deals with the dilemma by performing the action with higher upper confidence bound on the estimated reward expectation.", "startOffset": 47, "endOffset": 66}, {"referenceID": 10, "context": "The most popular algorithm is without doubt LinUCB (Li et al., 2010), although a few others exist such as epochgreedy (Langford & Zhang, 2007).", "startOffset": 51, "endOffset": 68}, {"referenceID": 9, "context": "The time acceleration issue with replay methodologies This section describes the replay methodology, that we call replay and that was introduced by (Langford et al., 2008) and analyzed for the setting we consider by (Li et al.", "startOffset": 148, "endOffset": 171}, {"referenceID": 11, "context": ", 2008) and analyzed for the setting we consider by (Li et al., 2011).", "startOffset": 52, "endOffset": 69}, {"referenceID": 11, "context": "First of all, as (Li et al., 2011), we assume that we have a dataset S that was acquired online using an random uniform allocation of the actions for T steps.", "startOffset": 17, "endOffset": 34}, {"referenceID": 11, "context": "This method is proved to be unbiased in some cases (Li et al., 2011).", "startOffset": 51, "endOffset": 68}, {"referenceID": 9, "context": "This problem is well studied and replay can be extended to allow the use of data acquired by a different but known logging policy at a cost of increased variance (Langford et al., 2008).", "startOffset": 162, "endOffset": 185}, {"referenceID": 4, "context": "Some work has been done to reduce this variance and even allow the use of data for which the logging policy is unknown (Dud\u0131\u0301k et al., 2011; Strehl et al., 2010).", "startOffset": 119, "endOffset": 161}, {"referenceID": 17, "context": "Some work has been done to reduce this variance and even allow the use of data for which the logging policy is unknown (Dud\u0131\u0301k et al., 2011; Strehl et al., 2010).", "startOffset": 119, "endOffset": 161}, {"referenceID": 3, "context": "Note also that if the evaluated bandit algorithm is close from the logging policy, we may even further reduce the variance (Bottou et al., 2012).", "startOffset": 123, "endOffset": 144}, {"referenceID": 9, "context": "Finally there exist ways to adapt this method to the case where a list of items can be recommended (Langford et al., 2008).", "startOffset": 99, "endOffset": 122}, {"referenceID": 0, "context": "For instance, in news recommendation a news remains active from a few hours to two days and its CTR systematically decreases over time (Agarwal et al., 2009).", "startOffset": 135, "endOffset": 157}, {"referenceID": 5, "context": "idea we propose stems from the idea of bootstrapping, introduced by (Efron, 1979).", "startOffset": 68, "endOffset": 81}, {"referenceID": 11, "context": "We sketch this algorithm so that it looks very much like replay in (Li et al., 2011).", "startOffset": 67, "endOffset": 84}, {"referenceID": 5, "context": "Also note that this theorem is a reformulation of the bootstrap main convergence result as introduced by (Efron, 1979).", "startOffset": 105, "endOffset": 118}, {"referenceID": 11, "context": "As the policy is fixed, we can use the multiplicative Chernoff\u2019s bound as in (Li et al., 2011) to obtain for all bootstrap step b: Pr (\u2223\u2223\u2223T (b) \u2212 T \u2223\u2223\u2223 \u2265 \u03b31T) \u2264 exp(\u2212T\u03b32 1 3 ) for any \u03b31 > 0 (where Pr(e) denotes the probability of event e).", "startOffset": 77, "endOffset": 94}, {"referenceID": 5, "context": "Furthermore this hypothesis, omnipresent in bootstrap theory (Efron, 1979), is for instance justified in econometrics by the fact that all the common estimators respect it (Horowitz, 2001).", "startOffset": 61, "endOffset": 74}, {"referenceID": 2, "context": "However the behavior of classical learning algorithms are smooth, especially when randomized (see (Auer et al., 2003) for an example of a randomized version of UCB).", "startOffset": 98, "endOffset": 117}, {"referenceID": 11, "context": "(Li et al., 2011) argue that in this case convergence bounds may be derived for replay (which then could be applied to BRED) at the cost of a much more complicated analysis including smoothness assumptions.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "Experiments in realistic settings As we proved that BRED has promising theoretical guarantees in the setting introduced in (Li et al., 2011), let us now compare its empirical performance to that of the replay method 6.", "startOffset": 123, "endOffset": 140}, {"referenceID": 10, "context": "It is a linear model with Gaussian noise (as in (Li et al., 2010)) and was built as follows: \u2022 a fixed action set (or news set) of size K = 10.", "startOffset": 48, "endOffset": 65}, {"referenceID": 10, "context": "Yet LinUCB (Li et al., 2010), a contextual bandit algorithm will do better by learning when to recommend the specific news.", "startOffset": 11, "endOffset": 28}, {"referenceID": 11, "context": "This experiment is similar to what is done in (Li et al., 2011): the authors measured the error of the estimated CTR of UCB (\u03b1 = 1) by the replay method on datasets of various sizes relatively to what they call the ground truth: an evaluation of the same algorithm on a real fraction of the audience.", "startOffset": 46, "endOffset": 63}], "year": 2014, "abstractText": "In many recommendation applications such as news recommendation, the items that can be recommended come and go at a very fast pace. This is a challenge for recommender systems (RS) to face this setting. Online learning algorithms seem to be the most straight forward solution. The contextual bandit framework was introduced for that very purpose. In general the evaluation of a RS is a critical issue. Live evaluation is often avoided due to the potential loss of revenue, hence the need for offline evaluation methods. Two options are available. Model based methods are biased by nature and are thus difficult to trust when used alone. Data driven methods are therefore what we consider here. Evaluating online learning algorithms with past data is not simple but some methods exist in the literature. Nonetheless their accuracy is not satisfactory mainly due to their mechanism of data rejection that only allow the exploitation of a small fraction of the data. We precisely address this issue in this paper. After highlighting the limitations of the previous methods, we present a new method, based on bootstrapping techniques. This new method comes with two important improvements: it is much more accurate and it provides a measure of quality of its estimation. The latter is a highly desirable property in order to minimize the risks entailed by putting online a RS for the first time. We provide both theoretical and experimental proofs of its superiority compared to state-of-the-art methods, as well as an analysis of the convergence of the measure of quality.", "creator": "LaTeX with hyperref package"}}}