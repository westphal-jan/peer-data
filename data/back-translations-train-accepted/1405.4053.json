{"id": "1405.4053", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2014", "title": "Distributed Representations of Sentences and Documents", "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "histories": [["v1", "Fri, 16 May 2014 07:12:16 GMT  (123kb)", "http://arxiv.org/abs/1405.4053v1", null], ["v2", "Thu, 22 May 2014 23:23:19 GMT  (123kb)", "http://arxiv.org/abs/1405.4053v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["quoc v le", "tomas mikolov"], "accepted": true, "id": "1405.4053"}, "pdf": {"name": "1405.4053.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["QVL@GOOGLE.COM", "TMIKOLOV@GOOGLE.COM"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.40 53v1 [cs.CL] 1 6M ay2 01"}, {"heading": "1. Introduction", "text": "In fact, most people who choose another world choose another world because they live for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world."}, {"heading": "2. Algorithms", "text": "We begin by discussing earlier methods for learning word vectors. These methods are the inspiration for our paragraph vector methods."}, {"heading": "2.1. Learning Vector Representation of Words", "text": "Within this framework, each word is assigned to a unique vector represented by a column in a matrix W. The column is indexed from the position of the word in the vocabulary. The association or sum of vectors is then used as a feature for predicting the next word in a sentence.More formally, given a sequence of training words w1, w2, w3, wS, the objective of the word vector modelis is to maximize the average logability1TT (wt \u2212 k, wt \u2212 k) The prediction task is typical of a multiclass-classified one, such as softmax."}, {"heading": "2.2. Paragraph Vector: A distributed memory model", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2.3. Paragraph Vector without word ordering: Distributed bag of words", "text": "The above method considers the concatenation of the paragraph vector with the word vectors in order to predict the next word in a text window. Another possibility is to ignore the context words in the input, but force the model to randomly predict words from the paragraph in the output. In reality, this means that we scan a text window for each iteration of stochastic gradient lineage, then scan a random word from the text window and form a classification task associated with the paragraph vector. This technique is shown in Figure 3. We call this version the Distributed Bag of Words version of Paragraph Vector (PV-DBOW), in contrast to the Distributed Memory version of Paragraph Vector (PV-DM) in the previous section. This model is not only conceptually simple, but requires the storage of less consistent data (BOg of Words version of Paragraph Vector / DBOW) in the Vectortortorch Vector / Vector Vector / Vectortortortorch weights in the previous word Vector / Vmax model, and also in the Vtortortortortortortord weights of the two Vtortortortords in the Vector / DM model."}, {"heading": "3. Experiments", "text": "We conduct experiments to better understand the behavior of paragraph vectors. To achieve this, we measure Paragraph Vector against two text comprehension problems that require vector representations of fixed paragraphs: sentiment analysis and information retrieval. For sentiment analysis, we use two datasets: the Stanford Sentiment Tree Base dataset (Socher et al., 2013b) and the IMDB dataset (Maas et al., 2011). Documents in these datasets differ significantly in length: Each example in Socher et al. (Socher et al., 2013b) is a single sentence, while each example in Maas et al. (Maas et al., 2011) consists of several sentences. We also test our method on an information retrieval task where the goal is to decide whether a document should be retrieved based on a query."}, {"heading": "3.1. Sentiment Analysis with the Stanford Sentiment Treebank Dataset", "text": "Vnlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "3.2. Beyond One Sentence: Sentiment Analysis with IMDB dataset", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. Most of them are not able to survive themselves, because they are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "3.3. Information Retrieval with Paragraph Vectors", "text": "The fact is that we will be able to assert ourselves, that we will be able, that we will be able, that we will be able, that we will be able, and that we will be able to put ourselves in a position, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able, that we will be able."}, {"heading": "3.4. Some further observations", "text": "We are conducting further experiments to understand different aspects of the models. Here are a few observations: \u2022 PV-DM is consistently better than PV-DBOW. PVDM alone can achieve results close to many of the results in this paper (see Table 2). In IMDB, for example, PV-DM reaches only 7.63%. The combination of PV-DM and PV-DBOW often works consistently better (7.42% in IMDB) and is therefore recommended. \u2022 The use of concatenation in PV-DM is often better than the sum. In IMDB, PV-DM can reach only 8.06% with the sum. Perhaps this is because the model loses the ordering information. \u2022 It is better to cross the window size. A good guess of the window size in many applications is between 5 and 12. In IMDB, the variation of the window size between 5 and 12 causes the error rate to fluctuate by 0.7%. \u2022 Vector paragraph can be expensive, but can be performed parallel to the test time."}, {"heading": "4. Related Work", "text": "Distributed representations of words were first proposed in (Rumelhart et al., 1986) and have become a successful paradigm, especially for statistical language modeling (Elman, 1990; Bengio et al., 2006; Mikolov, 2012) Word vectors have therefore been used in NLP applications such as word representation, named entity recognition, word meaning, parsing, tagging, and machine translation (Collobert & Weston, 2008; Turney & Pantel, 2010; Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011b; Huang et al., 2012; Zou et al., 2013). The representation of phrases is a current trend and has received much attention (Mitchell & Lapata, 2010; Zanzotto et al., 2010; Yessenalina & Cardie et al., 2011; Grefenstette et al., 2013; Mikolov et al., 2013c)."}, {"heading": "5. Discussion", "text": "We have described Paragraph Vector, an unattended learning algorithm that learns vector representations for variable-length texts such as sentences and documents, and the vector representations are learned to predict surrounding words in contexts taken from the paragraph. Our experiments on various text classification tasks such as Stanford Treebank and IMDB mood analysis data sets show that the method competes with state-of-the-art methods, and the good performance demonstrates the merits of Paragraph Vector in capturing the semantics of paragraphs. Indeed, paragraph vectors have the potential to overcome many weaknesses of dictionary models. Although the focus of this work is on representing texts, our method can be applied to learn representations for sequential data. In non-text areas where parsing is not available, we expect that Paragraph Vector is a strong alternative to dictionary models and bag models."}], "references": [{"title": "Neural probabilistic language models", "author": ["Bengio", "Yoshua", "Schwenk", "Holger", "Sen\u00e9cal", "JeanS\u00e9bastien", "Morin", "Fr\u00e9deric", "Gauvain", "Jean-Luc"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Ronan", "Weston", "Jason"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Training Restricted Boltzmann Machines on word observations", "author": ["Dahl", "George E", "Adams", "Ryan P", "Larochelle", "Hugo"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Finding structure in time", "author": ["Elman", "Jeff"], "venue": "In Cognitive Science, pp", "citeRegEx": "Elman and Jeff.,? \\Q1990\\E", "shortCiteRegEx": "Elman and Jeff.", "year": 1990}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jonathon", "Bengio", "Samy", "Dean", "Jeffrey", "Ranzato", "Marc\u2019Aurelio", "Mikolov", "Tomas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["E. Grefenstette", "G. Dinu", "Y. Zhang", "M. Sadrzadeh", "M. Baroni"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Grefenstette et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["Jaakkola", "Tommi", "Haussler", "David"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jaakkola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1999}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Dan", "Manning", "Chris D"], "venue": "In Proceedings of Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "A neural autoregressive topic model", "author": ["Larochelle", "Hugo", "Lauly", "Stanislas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas", "Andrew L", "Daly", "Raymond E", "Pham", "Peter T", "Huang", "Dan", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Statistical Language Models based on Neural Networks", "author": ["Mikolov", "Tomas"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov and Tomas.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tomas.", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "CoRR, abs/1309.4168,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In Advances on Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Yih", "Scott Wen-tau", "Zweig", "Geoffrey"], "venue": "In NAACL HLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Jeff", "Lapata", "Mirella"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2008}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In Proceedings of the International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["Perronnin", "Florent", "Dance", "Christopher"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Perronnin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Perronnin et al\\.", "year": 2007}, {"title": "Large-scale image retrieval with compressed fisher vectors", "author": ["Perronnin", "Florent", "Liu", "Yan", "Sanchez", "Jorge", "Poirier", "Herve"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Perronnin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Perronnin et al\\.", "year": 2010}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher", "Richard", "Huang", "Eric H", "Pennington", "Jeffrey", "Manning", "Chris D", "Ng", "Andrew Y"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Ng", "Andrew", "Manning", "Chris"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Modeling documents with deep boltzmann machines", "author": ["Srivastava", "Nitish", "Salakhutdinov", "Ruslan", "Hinton", "Geoffrey"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Peter D", "Pantel", "Patrick"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Baselines and bigrams: Simple, good sentiment and text classification", "author": ["Wang", "Sida", "Manning", "Chris D"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Compositional matrix-space models for sentiment analysis", "author": ["Yessenalina", "Ainur", "Cardie", "Claire"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Yessenalina et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2011}, {"title": "Estimating linear models for compositional distributional semantics", "author": ["Zanzotto", "Fabio", "Korkontzelos", "Ioannis", "Fallucchi", "Francesca", "Manandhar", "Suresh"], "venue": "In COLING,", "citeRegEx": "Zanzotto et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}, {"title": "Combining heterogeneous models for measuring relational similarity", "author": ["A. Zhila", "W.T. Yih", "C. Meek", "G. Zweig", "T. Mikolov"], "venue": "In NAACL HLT,", "citeRegEx": "Zhila et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhila et al\\.", "year": 2013}, {"title": "Bilingual word embeddings for phrasebased machine translation", "author": ["Zou", "Will", "Socher", "Richard", "Cer", "Daniel", "Manning", "Christopher"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "For example, the neural network language model proposed in (Bengio et al., 2006) uses the concatenation of several previous word vectors to form the input of a neural network, and tries to predict the next word.", "startOffset": 59, "endOffset": 80}, {"referenceID": 29, "context": "Following these successful techniques, researchers have tried to extend the models to go beyond word level to achieve phrase-level or sentence-level representations (Mitchell & Lapata, 2010; Zanzotto et al., 2010; Yessenalina & Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013c).", "startOffset": 165, "endOffset": 291}, {"referenceID": 6, "context": "Following these successful techniques, researchers have tried to extend the models to go beyond word level to achieve phrase-level or sentence-level representations (Mitchell & Lapata, 2010; Zanzotto et al., 2010; Yessenalina & Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013c).", "startOffset": 165, "endOffset": 291}, {"referenceID": 0, "context": "This type of models is commonly known as neural language models (Bengio et al., 2006).", "startOffset": 64, "endOffset": 85}, {"referenceID": 0, "context": "These properties make word vectors attractive for many natural language processing tasks such as language modeling (Bengio et al., 2006; Mikolov, 2012), natural language understanding (Collobert & Weston, 2008; Zhila et al.", "startOffset": 115, "endOffset": 151}, {"referenceID": 30, "context": ", 2006; Mikolov, 2012), natural language understanding (Collobert & Weston, 2008; Zhila et al., 2013), statistical machine translation (Mikolov et al.", "startOffset": 55, "endOffset": 101}, {"referenceID": 31, "context": ", 2013), statistical machine translation (Mikolov et al., 2013b; Zou et al., 2013), image understanding (Frome et al.", "startOffset": 41, "endOffset": 82}, {"referenceID": 5, "context": ", 2013), image understanding (Frome et al., 2013) and relational extraction (Socher et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 10, "context": ", 2013b) and IMDB dataset (Maas et al., 2011).", "startOffset": 26, "endOffset": 45}, {"referenceID": 10, "context": "(Maas et al., 2011)\u2019s dataset consists of several sentences.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "(Maas et al., 2011) as a benchmark for sentiment analysis.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "The most significant improvement happened in 2012 in the work of (Dahl et al., 2012) where they combine a Restricted Boltzmann Machines model with bag-ofwords.", "startOffset": 65, "endOffset": 84}, {"referenceID": 10, "context": "BoW (bnc) (Maas et al., 2011) 12.", "startOffset": 10, "endOffset": 29}, {"referenceID": 10, "context": "20 % BoW (b\u2206t\u2019c) (Maas et al., 2011) 11.", "startOffset": 17, "endOffset": 36}, {"referenceID": 10, "context": "77% LDA (Maas et al., 2011) 32.", "startOffset": 8, "endOffset": 27}, {"referenceID": 10, "context": "58% Full+BoW (Maas et al., 2011) 11.", "startOffset": 13, "endOffset": 32}, {"referenceID": 10, "context": "67% Full+Unlabeled+BoW (Maas et al., 2011) 11.", "startOffset": 23, "endOffset": 42}, {"referenceID": 3, "context": "WRRBM (Dahl et al., 2012) 12.", "startOffset": 6, "endOffset": 25}, {"referenceID": 3, "context": "58% WRRBM + BoW (bnc) (Dahl et al., 2012) 10.", "startOffset": 22, "endOffset": 41}, {"referenceID": 0, "context": ", 1986) and have become a successful paradigm, especially for statistical language modeling (Elman, 1990; Bengio et al., 2006; Mikolov, 2012).", "startOffset": 92, "endOffset": 141}, {"referenceID": 2, "context": "Word vectors have been used in NLP applications such as word representation, named entity recognition, word sense disambiguation, parsing, tagging and machine translation (Collobert & Weston, 2008; Turney & Pantel, 2010; Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011b; Huang et al., 2012; Zou et al., 2013).", "startOffset": 171, "endOffset": 325}, {"referenceID": 31, "context": "Word vectors have been used in NLP applications such as word representation, named entity recognition, word sense disambiguation, parsing, tagging and machine translation (Collobert & Weston, 2008; Turney & Pantel, 2010; Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011b; Huang et al., 2012; Zou et al., 2013).", "startOffset": 171, "endOffset": 325}, {"referenceID": 29, "context": "Representing phrases is a recent trend and received much attention (Mitchell & Lapata, 2010; Zanzotto et al., 2010; Yessenalina & Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013c).", "startOffset": 67, "endOffset": 193}, {"referenceID": 6, "context": "Representing phrases is a recent trend and received much attention (Mitchell & Lapata, 2010; Zanzotto et al., 2010; Yessenalina & Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013c).", "startOffset": 67, "endOffset": 193}, {"referenceID": 10, "context": "In this direction, autoencoder-style models have also been used to model paragraphs (Maas et al., 2011; Larochelle & Lauly, 2012; Srivastava et al., 2013).", "startOffset": 84, "endOffset": 154}, {"referenceID": 25, "context": "In this direction, autoencoder-style models have also been used to model paragraphs (Maas et al., 2011; Larochelle & Lauly, 2012; Srivastava et al., 2013).", "startOffset": 84, "endOffset": 154}, {"referenceID": 21, "context": "Our approach of computing the paragraph vectors via gradient descent bears resemblance to a successful paradigm in computer vision (Perronnin & Dance, 2007; Perronnin et al., 2010) known as Fisher kernels (Jaakkola & Haussler, 1999).", "startOffset": 131, "endOffset": 180}], "year": 2014, "abstractText": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \u201cpowerful,\u201d \u201cstrong\u201d and \u201cParis\u201d are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}