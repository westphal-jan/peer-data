{"id": "1405.4273", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2014", "title": "Compositional Morphology for Word Representations and Language Modelling", "abstract": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.", "histories": [["v1", "Fri, 16 May 2014 19:08:14 GMT  (756kb,D)", "http://arxiv.org/abs/1405.4273v1", "Proceedings of the 31st International Conference on Machine Learning (ICML)"]], "COMMENTS": "Proceedings of the 31st International Conference on Machine Learning (ICML)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan a botha", "phil blunsom"], "accepted": true, "id": "1405.4273"}, "pdf": {"name": "1405.4273.pdf", "metadata": {"source": "META", "title": "Compositional Morphology for Word Representations and Language Modelling", "authors": ["Jan A. Botha", "Phil Blunsom"], "emails": ["JAN.BOTHA@CS.OX.AC.UK", "PHIL.BLUNSOM@CS.OX.AC.UK"], "sections": [{"heading": "1 Introduction", "text": "The prevalence of word forms in morphologically rich languages poses challenges to the statistical language models (LMs) that play a key role in machine translation and speech recognition. Conventional back-off LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2006; Mikolov et al., 2010) use parameterizations that do not explicitly encode morphological regularities among related forms, such as abstract, abstract, and abstract. Such models suffer from data sparseness resulting from morphological processes and lack a coherent method of mapping probabilities or representations to invisible word formulas."}, {"heading": "2 Additive Word Representations", "text": "A generic CSLM associates with each word type v in the vocabulary V a \u2192 dimensional attribute vector rv = Rd. Regularities between words are captured in an opaque manner \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 This approach reflects linguistic intuitions only in an extremely rudimentary way, as opposed to hand-processed linguistic characteristics, which aim at very specific phenomena often used in supervised learning environments. We seek a compromise that maintains the unattended nature of CSLM characteristics vectors, but also integrates basic linguistic knowledge in a flexible and efficient way. In particular, morphologically related words should improve statistical strength in surface forms.1Our source code for language model training and integration into cdec is available from http: / / bothamusive.ioar Xiv: 140 5.42 73v1 [cs.CL] 6M \"6M."}, {"heading": "3 Log-Bilinear Language Models", "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models; the probability of a sentence w is broken down by its words, each depending on the n-1 preceding words: P (w) \u2248 i P (wi | wi \u2212 1i \u2212 n + 1); these distributions are modeled by a smooth scoring function \u03bd (\u00b7) over vector representations of words; in contrast, discrete n-gram models are estimated by smoothing and hedging over empirical distributions (Kneser & Ney, 1995; Chen & Goodman, 1998); the LBL predicts the vector p for the next word as a function of context vectors qj-Rd of the preceding words, p = n \u2212 1 = qjCj, (1) where Cj-specific transformations exist."}, {"heading": "3.1 Additive Log-Bilinear Model", "text": "We introduce a hard variant of the LBL that uses additive representations (\u00a7 2) by associating the compound word vectors r \u0442 and q \u0435j with the target or context words. Therefore, the representation matrices Q (f), R (f), R (f), R | F | \u00b7 d contain a vector for each factor type. This model is called LBL + + and has the parameters \u0394LBL + + = (Cj, Q (f), R (f), b). Word sharing factors are linked together, which is intended to improve the performance of less common word forms. The representation of the mapping \u00b5 with a sparse transformation matrix M \u0445ZV \u00d7 | F | +, where a row vector mv has some non-zero elements to select factor vectors, represents the relationship between word and factor representation matrices such as R = MR (f) and Q = MLQ = test LQ (f) for the time we use in these word vectors."}, {"heading": "3.2 Class-based Model Decomposition", "text": "The main obstacle to the use of CSLMs in a decoder is expensive normalization via the vocabulary. Our approach to reducing the computational cost of normalization is to use a class-based decomposition of the probability model (Goodman, 2001; Mikolov et al., 2011). Using broad clustering (Brown et al., 1992), we break down the vocabulary into classes, calling Cc the set of vocabulary items in class c, so that V = C1, \u00b7 C | C |. In this model, the probability of a word conditioned on the h history of n \u2212 1 preceding words is defined as P (w | h) = P (w | h, c) P (w | h, c) P (c). (c) (w | h, c). (3) This class-based model, CLBL, extends across the LBL by associating a representation vector sc and a bias parameter in each class."}, {"heading": "3.3 Training & Initialisation", "text": "Training CLBL and its additive variants directly against this goal is fast, because the normalization of the model values required for calculating gradients goes beyond a small number of events, and for classless LBLs, we use a Noise-Contrastive Estimate (NCE) (Gutmann & Hyva \ufffd rinen, 2012; Mnih & Teh, 2012) to avoid normalization during training, thus keeping the costly normalization of the test time of LBLs unchanged and excluding their use during decoding. Bias terms b (or t) are initialized to the protocol unigram probabilities of words (or classes) in the training corpus, smoothing Laplace, while all other parameters are randomly initialized using sharp, mediocre Gaussian applications. Thus, representations are learned from scratch and not based on publicly available embeddings."}, {"heading": "4 Experiments", "text": "The overall objective of our evaluation is to investigate the effects of using the proposed additive representations between languages of varying morphological complexity. 4L = 10k-40k, \u0442 = 0.05-0.08, depending on | V | and data size. Our evaluation of the intrinsic language model consists of two parts. First, we conduct a small data model selection experiment to consider the relative advantages of using additive representations for context words, target words, or both, and to validate the use of class-based decomposition. Then, we look at class-based additive models trained on tens of millions of tokens and large vocabularies. These larger language models are applied in two extrinsic tasks: i) a word similarity evaluation experiment on multiple languages, with the aim of measuring the quality of induced word and morphology representation vectors; ii) a machine translation expert, who is specifically interested in testing the effects of a linguistic LM in a feature-rich translation."}, {"heading": "4.1 Data & Methods", "text": "We use data from the ACL Workshop on Machine Translation.5 We first describe the data used for translation experiments = 2012, as the monolingual data sets used for language model training were derived from it. Language pairs are English \u2192 {German, French, Spanish, Russian} and English \u2194 Czech. Our parallel data included Europarl v7 and news commentaries, except English-Russian, where we used news comments and the Yandex parallel corpus. 6 Pre-processing included lowercasing, tokenization and filtering to exclude sentences of more than 80 tokens or significantly different lengths. 4-gram language models were trained on the target data in two batches: DATA-1M consists of only the first million tokens, while DATA-MAIN is the full target data. Statistics are used in Table 1. newstestestest2011 as development data7 to match language models hyperparameters."}, {"heading": "4.2 Intrinsic Language Model Evaluation", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.3 Task 1: Word Similarity Rating", "text": "rE \"s rf\u00fc ide r\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f"}, {"heading": "4.4 Task 2: Machine Translation", "text": "The last aspect of our evaluation is the inclusion in the limbs of the limbs of the limbs in the limbs of the limbs of the limbs of the limbs in the limbs of the limbs of the limbs."}, {"heading": "5 Related Work", "text": "Factored Language Models (FLMs) were used to integrate morphological information into discrete n-gram LMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexandrescu & Kirchhoff, 2006) by considering a word as a number of factors. Alexandrescu & Kirchhoff (2006) demonstrated how factorizing the representations of context words can help deal with non-vocabulary words, but they did not evaluate the effect of factorizing output words and did not perform extrinsic valuations.A variety of strategies were researched to make CSLMs act on machine translation.The consideration of lattices with a CSLM proved beneficial for ASR (Schwenk et al, 2004) and was subsequently applied to translation (Schwenk et al, 2006; Schwenk & Koehn, 2008)."}, {"heading": "6 Conclusion", "text": "Our empirical evaluation focused on several MRLs and different tasks. The primary results are that (i) our morphology-led CSLMs improve intrinsic language model performance compared to base models of CSLMs and n-gram MKN; (ii) word and morpheme representations learned in the process are favorably compared to a newer, more complex model in terms of word similarity task, which uses more data while making great progress in some languages; (iii) the quality of machine translation as measured by BLEU was consistently improved over six language pairs when using CSLMs during decoding, although morphology-based representations resulted in further improvements beyond the level of optimization variance only for English and Czech."}], "references": [{"title": "Factored Neural Language Models", "author": ["A. Alexandrescu", "K. Kirchhoff"], "venue": "In Proc. HLT-NAACL: short papers. ACL,", "citeRegEx": "Alexandrescu and Kirchhoff,? \\Q2006\\E", "shortCiteRegEx": "Alexandrescu and Kirchhoff", "year": 2006}, {"title": "Factored Language Models and Generalized Parallel Backoff", "author": ["J.A. Bilmes", "K. Kirchhoff"], "venue": "In Proc. NAACL-HLT: short papers. ACL,", "citeRegEx": "Bilmes and Kirchhoff,? \\Q2003\\E", "shortCiteRegEx": "Bilmes and Kirchhoff", "year": 2003}, {"title": "Class-Based n-gram Models of Natural Language", "author": ["P.F. Brown", "P.V. DeSouza", "R.L. Mercer", "V.J. Della Pietra", "J.C. Lai"], "venue": "Comp. Ling.,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Translating into Morphologically Rich Languages with Synthetic Phrases", "author": ["V. Chahuneau", "E. Schlinger", "N.A. Smith", "C. Dyer"], "venue": "In Proc. EMNLP,", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "An Empirical Study of Smoothing Techniques for Language Modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Technical report,", "citeRegEx": "Chen and Goodman,? \\Q1998\\E", "shortCiteRegEx": "Chen and Goodman", "year": 1998}, {"title": "Hierarchical Phrase-Based Translation", "author": ["D. Chiang"], "venue": "Comp. Ling.,", "citeRegEx": "Chiang,? \\Q2007\\E", "shortCiteRegEx": "Chiang", "year": 2007}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proc. ICML. ACM,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Unsupervised Models for Morpheme Segmentation and Morphology Learning", "author": ["M. Creutz", "K. Lagus"], "venue": "ACM Trans. on Speech and Language Processing,", "citeRegEx": "Creutz and Lagus,? \\Q2007\\E", "shortCiteRegEx": "Creutz and Lagus", "year": 2007}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models", "author": ["C. Dyer", "A. Lopez", "J. Ganitkevitch", "J. Weese", "F. Ture", "P. Blunsom", "H. Setiawan", "V. Eidelman", "P. Resnik"], "venue": "In Proc. ACL: demonstration session,", "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "In Proc. NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Trans. on Information Systems,", "citeRegEx": "Finkelstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Classes for Fast Maximum Entropy Training", "author": ["J. Goodman"], "venue": "In Proc. ICASSP, pp. 561\u2013564", "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Using the Structure of a Conceptual Network in Computing Semantic Relatedness", "author": ["I. Gurevych"], "venue": "In Proc. IJCNLP,", "citeRegEx": "Gurevych,? \\Q2005\\E", "shortCiteRegEx": "Gurevych", "year": 2005}, {"title": "Noise-Contrastive Estimation of Unnormalized Statistical Models", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "with Applications to Natural Image Statistics. JMLR,", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? \\Q2012\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2012}, {"title": "Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge", "author": ["S. Hassan", "R. Mihalcea"], "venue": "In Proc. EMNLP,", "citeRegEx": "Hassan and Mihalcea,? \\Q2009\\E", "shortCiteRegEx": "Hassan and Mihalcea", "year": 2009}, {"title": "KenLM: Faster and Smaller Language Model Queries", "author": ["K. Heafield"], "venue": "In Proc. Workshop on Statistical Machine Translation,", "citeRegEx": "Heafield,? \\Q2011\\E", "shortCiteRegEx": "Heafield", "year": 2011}, {"title": "The Role of Syntax in Vector Space Models of Compositional Semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In Proc. ACL,", "citeRegEx": "Hermann and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2013}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In Proc. ACL,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Comparison of Semantic Similarity for Different Languages Using the Google N-gram Corpus and Second- Order Co-occurrence Measures", "author": ["C. Joubarne", "D. Inkpen"], "venue": "In Proc. Canadian Conference on Advances in AI,", "citeRegEx": "Joubarne and Inkpen,? \\Q2011\\E", "shortCiteRegEx": "Joubarne and Inkpen", "year": 2011}, {"title": "Recurrent Continuous Translation Models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Improved Backing-off for m-gram Language Modelling", "author": ["R. Kneser", "H. Ney"], "venue": "In Proc. ICASSP,", "citeRegEx": "Kneser and Ney,? \\Q1995\\E", "shortCiteRegEx": "Kneser and Ney", "year": 1995}, {"title": "Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics", "author": ["A. Lazaridou", "M. Marelli", "R. Zamparelli", "M. Baroni"], "venue": "In Proc. ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Structured Output Layer Neural Network Language Model", "author": ["Le", "H.-S", "I. Oparin", "A. Allauzen", "Gauvain", "J.-L", "F. Yvon"], "venue": "In Proc. ICASSP,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Luong", "M.-T", "R. Socher", "C.D. Manning"], "venue": "In Proc. of CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. \u010cernock\u00fd", "S. Khudanpur"], "venue": "In Proc. Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of Recurrent Neural Network Language Model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. \u010cernock\u00fd", "S. Khudanpur"], "venue": "In Proc. ICASSP,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Proc. ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "Yih", "W.-t", "G. Zweig"], "venue": "In Proc. HLTNAACL. ACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three New Graphical Models for Statistical Language Modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "In Proc. ICML,", "citeRegEx": "Mnih and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "A Scalable Hierarchical Distributed Language Model", "author": ["A. Mnih", "G. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Mnih and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2008}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "In Proc. ICML,", "citeRegEx": "Mnih and Teh,? \\Q2012\\E", "shortCiteRegEx": "Mnih and Teh", "year": 2012}, {"title": "Minimum Error Rate Training in Statistical Machine Translation", "author": ["F.J. Och"], "venue": "In Proc. ACL, pp", "citeRegEx": "Och,? \\Q2003\\E", "shortCiteRegEx": "Och", "year": 2003}, {"title": "Contextual Correlates of Synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein and Goodenough,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein and Goodenough", "year": 1965}, {"title": "Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging", "author": ["H. Schmid", "F. Laws"], "venue": "In Proc. COLING,", "citeRegEx": "Schmid and Laws,? \\Q2008\\E", "shortCiteRegEx": "Schmid and Laws", "year": 2008}, {"title": "Efficient Training of Large Neural Networks for Language Modeling", "author": ["H. Schwenk"], "venue": "In Proc. IEEE Joint Conference on Neural Networks,", "citeRegEx": "Schwenk,? \\Q2004\\E", "shortCiteRegEx": "Schwenk", "year": 2004}, {"title": "Large and Diverse Language Models for Statistical Machine Translation", "author": ["H. Schwenk", "P. Koehn"], "venue": "In Proc. IJCNLP,", "citeRegEx": "Schwenk and Koehn,? \\Q2008\\E", "shortCiteRegEx": "Schwenk and Koehn", "year": 2008}, {"title": "Continuous Space Language Models for Statistical Machine Translation", "author": ["H. Schwenk", "D. Dchelotte", "Gauvain", "J.-L"], "venue": "In Proc. COLING/ACL,", "citeRegEx": "Schwenk et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2006}, {"title": "Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation", "author": ["H. Schwenk", "A. Rousseau", "M. Attik"], "venue": "In In Proc. NAACL-HLT Workshop: On the Future of Language Modeling for HLT,", "citeRegEx": "Schwenk et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2012}, {"title": "SRILM \u2013 An extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Proc. ICSLP, pp", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Visualizing Data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "JMLR, 9:2579\u20132605,", "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Decoding with Large-Scale Neural Language Models Improves Translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "In Proc. EMNLP,", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Automatically creating datasets for measures of semantic relatedness", "author": ["T. Zesch", "I. Gurevych"], "venue": "In Proc. Workshop on Linguistic Distances,", "citeRegEx": "Zesch and Gurevych,? \\Q2006\\E", "shortCiteRegEx": "Zesch and Gurevych", "year": 2006}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["W.Y. Zou", "R. Socher", "D. Cer", "C.D. Manning"], "venue": "In Proc. EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 37, "context": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted.", "startOffset": 102, "endOffset": 167}, {"referenceID": 25, "context": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted.", "startOffset": 102, "endOffset": 167}, {"referenceID": 22, "context": "Conversely, compositional vector-space modelling has recently been applied to morphology to good effect (Lazaridou et al., 2013; Luong et al., 2013), but lacked the probabilistic basis necessary for use with a machine translation decoder.", "startOffset": 104, "endOffset": 148}, {"referenceID": 24, "context": "Conversely, compositional vector-space modelling has recently been applied to morphology to good effect (Lazaridou et al., 2013; Luong et al., 2013), but lacked the probabilistic basis necessary for use with a machine translation decoder.", "startOffset": 104, "endOffset": 148}, {"referenceID": 22, "context": "Our additive composition function can be regarded as an instantiation of the weighted addition strategy that performed well in a distributional compositional approach to derivational morphology (Lazaridou et al., 2013).", "startOffset": 194, "endOffset": 218}, {"referenceID": 22, "context": "Our additive composition function can be regarded as an instantiation of the weighted addition strategy that performed well in a distributional compositional approach to derivational morphology (Lazaridou et al., 2013). Unlike the recursive neural-network method of Luong et al. (2013), we do not impose a single tree structure over a word, which would ignore the ambiguity inherent in words like un[[lock]able] vs.", "startOffset": 195, "endOffset": 286}, {"referenceID": 12, "context": "Our approach to reducing the computational cost of normalisation is to use a class-based decomposition of the probabilistic model (Goodman, 2001; Mikolov et al., 2011).", "startOffset": 130, "endOffset": 167}, {"referenceID": 26, "context": "Our approach to reducing the computational cost of normalisation is to use a class-based decomposition of the probabilistic model (Goodman, 2001; Mikolov et al., 2011).", "startOffset": 130, "endOffset": 167}, {"referenceID": 2, "context": "Using Brownclustering (Brown et al., 1992),3 we partition the vocabulary into |C| classes, denoting as Cc the set of vocabulary items in class c, such that V = C1 \u222a \u00b7 \u00b7 \u00b7 \u222a C|C|.", "startOffset": 22, "endOffset": 42}, {"referenceID": 26, "context": "In preliminary experiments, Brown clusters gave better perplexities than frequency-binning (Mikolov et al., 2011).", "startOffset": 91, "endOffset": 113}, {"referenceID": 35, "context": "Other methods for achieving more drastic complexity reductions exist in the form of frequency-based truncation, shortlists (Schwenk, 2004), or casting the vocabulary as a full hierarchy (Mnih & Hinton, 2008) or partial hierarchy (Le et al.", "startOffset": 123, "endOffset": 138}, {"referenceID": 23, "context": "Other methods for achieving more drastic complexity reductions exist in the form of frequency-based truncation, shortlists (Schwenk, 2004), or casting the vocabulary as a full hierarchy (Mnih & Hinton, 2008) or partial hierarchy (Le et al., 2011).", "startOffset": 229, "endOffset": 246}, {"referenceID": 8, "context": "We apply AdaGrad (Duchi et al., 2011) and tune the stepsize \u03be on development data.", "startOffset": 17, "endOffset": 37}, {"referenceID": 24, "context": "We evaluate first using the English rare-word dataset (RW) created by Luong et al. (2013). Its 2034 word pairs contain more morphological complexity than other well-established word similarity datasets, e.", "startOffset": 70, "endOffset": 90}, {"referenceID": 18, "context": "Moreover, the csmRNNs were initialised with high-quality, publicly available word embeddings trained over weeks on much larger corpora of 630\u2013990m words (Collobert & Weston, 2008; Huang et al., 2012), in contrast to ours that are trained from scratch on much less data.", "startOffset": 153, "endOffset": 199}, {"referenceID": 24, "context": "(Luong et al., 2013) Our models", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "But also on the standard English WS353 dataset (Finkelstein et al., 2002), we get a 26% better correlation with the human ratings.", "startOffset": 47, "endOffset": 73}, {"referenceID": 13, "context": "10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).", "startOffset": 46, "endOffset": 62}, {"referenceID": 13, "context": "10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006). Table 4. Word-pair similarity task (multi-language), showing Spearman\u2019s \u03c1\u00d7100 and the number of word pairs in each dataset. As benchmarks, we include the best results from Luong et al. (2013), who relied on more training data and pre-existing embeddings not available in all languages.", "startOffset": 47, "endOffset": 360}, {"referenceID": 41, "context": "Aside from the choice of language pairs, this evaluation diverges from Vaswani et al. (2013) by using normalised probabilities, a process made tractable by the class-based decomposition and caching of context-specific normaliser terms.", "startOffset": 71, "endOffset": 93}, {"referenceID": 41, "context": "Aside from the choice of language pairs, this evaluation diverges from Vaswani et al. (2013) by using normalised probabilities, a process made tractable by the class-based decomposition and caching of context-specific normaliser terms. Vaswani et al. (2013) relied on unnormalised model scores for efficiency, but do not report on the performance impact of this assumption.", "startOffset": 71, "endOffset": 258}, {"referenceID": 9, "context": "We use cdec (Dyer et al., 2010; 2013) to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007).", "startOffset": 12, "endOffset": 37}, {"referenceID": 5, "context": ", 2010; 2013) to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007).", "startOffset": 108, "endOffset": 122}, {"referenceID": 39, "context": "This includes a baseline 4-gram MKN language model, trained with SRILM (Stolcke, 2002) and queried efficiently using KenLM (Heafield, 2011).", "startOffset": 71, "endOffset": 86}, {"referenceID": 16, "context": "This includes a baseline 4-gram MKN language model, trained with SRILM (Stolcke, 2002) and queried efficiently using KenLM (Heafield, 2011).", "startOffset": 123, "endOffset": 139}, {"referenceID": 32, "context": "11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012.", "startOffset": 57, "endOffset": 68}, {"referenceID": 3, "context": "A future task is thus to combine it with a system that can do so (Chahuneau et al., 2013).", "startOffset": 65, "endOffset": 89}, {"referenceID": 35, "context": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al.", "startOffset": 63, "endOffset": 78}, {"referenceID": 37, "context": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al., 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al.", "startOffset": 123, "endOffset": 168}, {"referenceID": 38, "context": ", 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al., 2012).", "startOffset": 76, "endOffset": 98}, {"referenceID": 41, "context": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013).", "startOffset": 122, "endOffset": 144}, {"referenceID": 43, "context": "Recent work has moved beyond monolingual vector-space modelling, incorporating phrase similarity ratings based on bilingual word embeddings as a translation model feature (Zou et al., 2013), or formulating translation purely in terms of continuous-space models (Kalchbrenner & Blunsom, 2013).", "startOffset": 171, "endOffset": 189}, {"referenceID": 24, "context": "Accounting for linguistically derived information such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words.", "startOffset": 69, "endOffset": 113}, {"referenceID": 22, "context": "Accounting for linguistically derived information such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words.", "startOffset": 69, "endOffset": 113}], "year": 2014, "abstractText": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.", "creator": "LaTeX with hyperref package"}}}