{"id": "1405.5096", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2014", "title": "Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms", "abstract": "We consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope 2009, Yu 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in non-stationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure.", "histories": [["v1", "Tue, 20 May 2014 14:15:54 GMT  (63kb)", "http://arxiv.org/abs/1405.5096v1", "ICML 2014 (technical report). arXiv admin note: text overlap witharXiv:1307.7309"]], "COMMENTS": "ICML 2014 (technical report). arXiv admin note: text overlap witharXiv:1307.7309", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["richard combes", "alexandre prouti\u00e8re"], "accepted": true, "id": "1405.5096"}, "pdf": {"name": "1405.5096.pdf", "metadata": {"source": "META", "title": "Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms", "authors": ["Richard Combes", "Alexandre Proutiere"], "emails": ["RCOMBES@KTH.SE", "ALEPRO@KTH.SE"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.50 96v1 [cs.LG] 2 0M ayWe look at stochastic multi-armed bandits where the expected reward is an unimodal function compared to partially ordered weapons, an important class of problems recently explored in (Cope, 2009; Yu & Mannor, 2011) The group of weapons is either discrete, with weapons corresponding to the vertices of a finite diagram whose structure is similarity in rewards, or continuous, with weapons belonging to a limited interval. For discrete unimodal bandits, we derive asymptotic lower limits for the regret achieved under each algorithm and propose OSUB, an algorithm whose regret corresponds to this lower limit. Our algorithm makes optimal use of the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of weapons."}, {"heading": "1. Introduction", "text": "In such problems, the decision maker selects an arm in each round and observes a realization of the corresponding unknown reward distribution. Each decision is based on past decisions and observed rewards. The goal is to maximize the expected cumulative reward over a certain time horizon by balancing exploitation (poor people with higher observed rewards should be frequently selected) and research (all poor people should be researched to know their average rewards). Likewise, the performance of a decision rule or algorithm is to be maximized by its expected remorse, defined as the gap between the expected reward achieved by the algorithm and that achieved by an oracle. MAB problems have found many fields of application, including sequential clinical studies, economic systems, see e.g. (Cesa-Bianchi Luosi, 2006; Biesa Biesa, 2006)."}, {"heading": "2. Related work", "text": "They are specific examples of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).In this paper, we add unimodality and show how this structure can be optimally exploited. Unimodal bandits were specifically treated in (Cope, 2009; Yu & Mannor, 2011).In (Cope, 2009), bandits with a continuous array of weapons are investigated, and the author shows that the Kiefer-Wolfowitz approach algorithms achieves a regret in the order of O (T) under some strong assumptions of regularity in the reward function. In (Yu & Mannor, 2011), for the same problem, the authors present LSE, an algorithm whose regrets we consider to be O (T) without the need for strong regularity."}, {"heading": "3. Model and Objectives", "text": "We will consider a stochastic, multi-armed bandit problem with K \u2265 2 arms. We will discuss problems where the number of arms is continuous, in Section 6. Time runs in rounds marked n = 1, 2,... Let Xk (n) be the reward obtained at a point in time n when arm k is selected. For each k, the order of rewards (Xk (n) n \u2265 1 i.i.d. with distribution and expectation denoted by \u03bdk or \u00b5k. Rewards are independent in all arms. Let \u00b5 = (\u00b51,.., \u00b5K) represent the expected rewards of the different arms. In each round, a decision rule or algorithm selects an arm depending on the arms chosen in previous rounds and their observed rewards. We denote the arm selected in round n below \u03c0 in round n. The set of all possible decision rules consists of guidelines that are satisfactory: for each n \u2265 1 if Fn is of the arms chosen in previous rounds and their observed rewards."}, {"heading": "3.1. Unimodal Structure", "text": "More precisely, there is an undirected diagram G = (V, E), whose vertices correspond to the arms, i.e. V = {1,.., K}, and whose edges indicate a partial order (initially unknown to the decision-maker) of the rewards to be expected. We assume that there is a unique arm k * with maximum expected reward \u00b5, and that for each suboptimal arm k 6 = k \u00b2 there is a path p = (k1 = k,., km = k) of length m (depending on k), so that for all i = 1,.., m \u2212 1, (ki, ki + 1) E and \u00b5ki < \u00b5ki + 1. We refer to UG as the amount of vectors \u00b5 that satisfy this unimodal structure. This concept of unimodality is fairly general and includes, as a special case, the imperfection, the imperfection of the line G (which we know best) and the imperfection of the edge line."}, {"heading": "3.2. Stationary and non-stationary environments", "text": "The model presented above concerns stationary environments in which the expected rewards for the various arms do not evolve over time. In this paper, we also consider non-stationary environments in which these expected rewards may evolve over time according to a certain deterministic dynamic. In such scenarios, we call the expected reward of arm k at the time n, i.e. E [Xk (n) = \u00b5k (n), and (Xk (n)) n \u2265 1 represents a sequence of independent random variables with an evolving mean. In non-stationary environments, the reward sequences are still assumed to be independent between arms. Furthermore, at any time n, \u00b5 (n) = (\u00b51 (n),..."}, {"heading": "3.3. Regrets", "text": "The way in which repentance is defined varies depending on the type of environment.Stationary environments. In such environments, the algorithm's repentance R\u03c0 (T) is conceived by simply defining the number of times in which repentance is defined. Our goals are (1) to identify an asymptotic arm (if T) = k) that is satisfied by an algorithm over time, and (2) to develop an algorithm that achieves that lower boundary. Non-stationary environments are (1) to identify an asymptotic arm (if T) that is satisfied by an algorithm over time. Let us denounce the algorithm that achieves that lower boundary. Non-stationary environments are (T). In such environments, the regret of an algorithm quantifies how well the best arm is tracked over time."}, {"heading": "4. Stationary environments", "text": "In this section, we look at unimodal bandit problems in stationary environments. We derive an asymptotic lower limit of regret when the reward distributions belong to a parameterized family of distributions, and propose OSUB, an algorithm whose regret corresponds to this lower limit."}, {"heading": "4.1. Lower bound on regret", "text": "To simplify the representation, we assume here that the reward distributions belong to a parametrized family of distributions (SO = SO = SO = SO = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O ="}, {"heading": "4.2. The OSUB Algorithm", "text": "We describe OSUB, a simple algorithm whose regrets correspond to the lower limit derived in the theory of 4.1 for Bernoulli rewards, i.e. OSUB is asymptotically optimal. OSUB is based on KL-UCB proposed in (Lai, 1987; Cappe \u0301 et al., 2013), and uses KL divergence upper confidence limits to define an index for each arm. OSUB can easily be extended to systems in which reward distributions within a parameter are exponential families, simply by modifying the definition of arm indices, as in (Cappe \u0301 et al., 2013). In OSUB, an index similar to the KL-UCB index is attached to each arm, but the arm selected at a certain time is the arm with the maximum index within the neighborhood in G of the arm, which yields the highest empirical reward."}, {"heading": "4.3. Finite-time analysis of OSUB", "text": "Next, we provide a finite time analysis of the regret that was achieved under OSUB. Let us use the minimum separation between an arm and its best neighboring arm (< M = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K ="}, {"heading": "5. Non-stationary environments", "text": "We assume that the expected reward of each arm will vary smoothly over time, i.e. that it will be continuous for all n, n \u2032 \u2265 1 and 1 \u2264 k \u2264 K: | \u00b5k (n) \u2212 \u00b5k (n \u2032) | \u2264 \u03c3 | n \u2212 n \u2032 |. We also assume that the unimodal structure will be preserved (in relation to the same graph G): for all n \u2265 1, \u00b5 (n) \u0445 UG. Considering slightly varying rewards is more difficult than scenarios in which the environment changes abruptly. The difficulty stems from the fact that the rewards of two or more arms may be arbitrarily close to each other (this happens every time the optimal arm changes), and in such situations regret is difficult to control. To have a chance to design an algorithm that efficiently tracks the best arm, we must make a certain assumption to limit the amount of time. & < < < <"}, {"heading": "5.1. OSUB with a Sliding Window", "text": "To cope with the changing environment, we modify the OSUB algorithm so that decisions are based on past decisions and observations about a time window of fixed duration equal to \u03c4 + 1 rounds. The idea of adding a sliding window to algorithms originally designed for stationary environments is not new (Garivier & Moulines, 2008); but in this case, the unimmodal structure and the smooth development of the rewards make the analysis of regret more difficult. Define: t\u03c4k (n) =.Define t = n \u2212 Define t = n \u2212 Define (n) = 0 Define (k) = 0 Define (n).Define (n).Define (n).Define (n).Define (n).Define (n).Define (n).Define (n).Define (n).Define (n).Define (n).Define (n). (n).Define (n).Define (n). (n).Define (n).Define (n). (n).Define (n)."}, {"heading": "5.2. Regret Analysis", "text": "In non-stationary environments, achieving sublinear repentance is often not possible. In (Garivier & Moulines, 2008), the environment is subject to abrupt changes or breakpoints. It is shown that if the breakpoint density is strictly positive, which is typically true in practice, the regret of each algorithm must be scaled linearly over time. We are interested in similar scenarios and consider smoothly varying environments where the number of times the optimal arm changes has a positive density. The next theorem provides an upper limit of regret per unit of time reached under SW-OSUB. This applies to all non-stationary environments with \u03c3-Lipschitz rewards. \u2212 Theorem 5.1 Let us expect the number of setpoint changes to have a positive density. < < \u2206 < 0. Let's assume that for each n-unit of regret, the OSW-SW is reached."}, {"heading": "6. Continuous Set of Arms", "text": "In this section we briefly discuss the case where the decision space is continuous. The sentence of regret is [0, 1] and the expected reward function \u00b5: [0, 1] \u2192 R is assumed to be Lipschitz continuous and unimodal: There is a regret (0, 1] so that \u00b5 (x) \u2265 \u00b5 (x), if x, \"x,\" or \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"x,\" \"x,\" \"x,\" \"x,\" \"x,\" \"x,\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"x,\" \"x,\" x, \"\" x, \"\" x, \"x,\" x, \"\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \",\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\", \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \",\" x, \",\" x, \"x,\" x, \"x,\", \",\" x, \"x,\" x, \"x,\" x, \"x,\", \"x,\" x, \",\" x, \",\""}, {"heading": "7. Numerical experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. Discrete bandits", "text": "We compare the performance of our algorithm with that of KL-UCB (Cappe \u0301 et al., 2013), LSE (Yu & Mannor, 2011), UCB (Auer et al., 2002) and UCB-U. The latter algorithm is achieved by restricting UCB to the arms adjacent to the current market leader, as in OSUB. We add the prefix \"SW\" to refer to sliding window versions of these algorithms. Stationary environments. In our first experiment, we look at K = 17 arms with Bernoulli rewards of respective averages \u00b5 = (0.1, 0.2,...., 0.9, 0.8,., 0.1). The rewards are unimodal (graph G is simply a line). Regrets achieved among the various algorithms are presented in Figure 1 and Table 1. The parameters of the LSE algorithm are explained as in Proposition 4.5 (Yu & Mannor, 2011)."}, {"heading": "7.2. Continuous bandits", "text": "The expected rewards form a triangle: \u00b5 (x) = 1 / 2 \u2212 | x \u2212 1 / 2 | so that \u00b5 values = 1 / 2 and x values = 1 / 2. The parameters used in LSE are those given in Yu & Mannor, 2011, while the discretization parameter in UCB (\u03b4) is set to \u03b4 = log (T) / \u221a T. UCB (\u03b4) clearly outperforms the LSE at all times: Appropriate discretization of continuous bandit problems may actually be more efficient than other meth values based on ideas from classical optimization theory. Figure 6 compares the regrets of the discrete version of LSE (with optimized parameters) and OSUB, as the number of arms K becomes larger than the number of UCB values. The average rewards of the arms are extracted from the triangle in the large B values, even if we reach the larger B values (USE values) and the lower number of the USE (500)."}, {"heading": "8. Conclusion", "text": "In this paper, we deal with stochastic bandit problems with an unimodal structure and a limited number of weapons. We provide asymptotic remorse thresholds for these problems and design an algorithm that asymptotically achieves the least possible remorse. Therefore, our algorithm makes optimal use of the unimodal structure of the problem. Our preliminary analysis of the continuous version of this bandit problem suggests that it may be wiser to limit the amount of weapons before executing an algorithm if the number of weapons becomes very large and comparable to the time horizon."}, {"heading": "A. Proof of Theorem 4.1", "text": "To this end, we apply the techniques used by Graves and Lai (Graves & Lai, 1997) to investigate efficient adaptive decision rules in controlled Markov chains. Let's recall their general framework here. Let's consider a controlled Markov chain (Xt) t \u2265 0 to a finite state space S with a control set U. The transition probabilities given by control u (U) are parameterized by taking the values in a compact metric space. The probability of moving from state x to state y is the control u and the parameters that control p (x, y; u).The parameters are unknown.The decision maker is with a finite set of stationary control laws G = {g1,., gK} where each control law gj is mapped from S to U: if control law gj is applied in state x."}, {"heading": "B. Concentration inequalities and Preliminaries", "text": "B. 1. Evidence of Lemma 4.3We prove Lemma 4.3, a new concentration inequality that extends Hoeffding's inequality and is used for regret analysis in the following sections. \u2212 We believe that Lemma 4.3 could be useful for a variety of bandit problems where an upper limit is required for the deviation of the empirical mean at a given time. \u2212 An example would be the probability that the empirical reward of the k-th arm deviates from its expectation when it is actually recorded for the s-th timeframe. Proof. Let it happen > 0, and define Gn = exp (Sn \u2212 sp) 1 {n \u2264 reward of the k-th arm. \u2212 We have this: P [Sought-th reward] if it is for the s-th timeframe."}, {"heading": "C. Proofs for stationary environments", "text": "(K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K)"}, {"heading": "D. Proofs for non-stationary environments", "text": "To simplify the notation, we remove the superscript dots in the proofs. < / p > p (n) and l (n) are replaced by tk (n) and lk (n).D.1. A problem for sums over a sliding window We will use D.1 repeatedly to bind the number of events over a sliding window of size."}, {"heading": "E. Proof of Proposition 1", "text": "The regret of the UCB (\u03b4) is defined as: \"We divide the arms into three different groups.\" (.) \"We divide the arms into three different groups.\" (.) \"We\" (\"we\") \"we\" (\"we\") \"we\" (\"we\") \"we\" (\"we\") \"we\" (\"we\") \"we\" (we \"we\") \"we\" (we \"we\") (we \"we\" (we \"we\") (we \"we\") (we \"we\" (we \"we\") (we \"we\") (we \"we\" (we \"we\") (we \"we\") (we \"we\") (we \"we\") (we \"we\" (we \"we\") (we \"we\") (we \"we\") (we \"we\") (we \"we\" (we \"we\" we \") (we\" we \"we\" (we \"we\" we \") (we\" we \"we\" we \"(we\" we \"we\" we \"we\") (we \"we\" we \"we\" we \"we\" we \"\" we \"\" we \"\" we \"\" we \"\" we \"\" \"we\" we \"\" we \"we\" we \"\" \"we\" we \"\" we \"\" we \"\" we \"\" we \"\" we \"\" we \"we\" \"we\" we \"we\" we \"(we\" we \"we\" we \"we\") (we \"we\" we \"we\" we \") (we\" we \"we\" we \"we\" we \"\" we \"we\" we \"we\" we \") (we\" we \"we\" we \"we\" we \") (we\" we \"we\" we \"we\" we \"(we\" we \") (we\" we \"we\" we \"we\") (we \"we\" we \"\" we \"we\" we \") (we\" we \"we\" \"we\") (we \"we\" we \"we\" we \"\" \"we\" we \") (we\" we \"we\" \"we\" we \"we\" (\"we\" we \"we\") (\"we\" we \"we\" we \"we\" we \"we\" we \"we\" we \") (we\" we \"we\" \""}], "references": [{"title": "The continuum-armed bandit problem", "author": ["R. Agrawal"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Agrawal,? \\Q1995\\E", "shortCiteRegEx": "Agrawal", "year": 1995}, {"title": "Finite time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Strategic bidder behavior in sponsored search auctions", "author": ["Edelman"], "venue": "In Proc. of Workshop on Sponsored Search Auctions, ACM Electronic Commerce,", "citeRegEx": "B. and Edelman.,? \\Q2005\\E", "shortCiteRegEx": "B. and Edelman.", "year": 2005}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "Online optimization in x-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bubeck et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2008}, {"title": "Kullback-leibler upper confidence bounds for optimal sequential allocation", "author": ["O. Capp\u00e9", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz"], "venue": "Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Regret and convergence bounds for a class of continuum-armed bandit problems", "author": ["E.W. Cope"], "venue": "IEEE Trans. Automat. Contr.,", "citeRegEx": "Cope,? \\Q2009\\E", "shortCiteRegEx": "Cope", "year": 2009}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["V. Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In Proc. of ACM/SIAM symposium on Discrete Algorithms (SODA), pp", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Garivier and Capp\u00e9,? \\Q2011\\E", "shortCiteRegEx": "Garivier and Capp\u00e9", "year": 2011}, {"title": "On upper-confidence bound policies for non-stationary bandit problems", "author": ["A. Garivier", "E. Moulines"], "venue": "In Proc. of Algorithmic Learning Theory (ALT),", "citeRegEx": "Garivier and Moulines,? \\Q2008\\E", "shortCiteRegEx": "Garivier and Moulines", "year": 2008}, {"title": "Bandit Processes and Dynamic Allocation Indices", "author": ["J.C. Gittins"], "venue": "John Wiley,", "citeRegEx": "Gittins,? \\Q1989\\E", "shortCiteRegEx": "Gittins", "year": 1989}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled markov chains", "author": ["T.L. Graves", "T.L. Lai"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Graves and Lai,? \\Q1997\\E", "shortCiteRegEx": "Graves and Lai", "year": 1997}, {"title": "Change point detection and meta-bandits for online learning in dynamic environments", "author": ["C. Hartland", "N. Baskiotis", "S. Gelly", "O. Teytaud", "M. Sebag"], "venue": "In Proc. of confe\u0301rence francophone sur l\u2019apprentissage automatique (CAp07),", "citeRegEx": "Hartland et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hartland et al\\.", "year": 2007}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "In Proc. of the 40th annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Nearly tight bounds for the continuumarmed bandit problem", "author": ["R.D. Kleinberg"], "venue": "In Proc. of the conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Kleinberg,? \\Q2004\\E", "shortCiteRegEx": "Kleinberg", "year": 2004}, {"title": "Adaptive treatment allocation and the multiarmed bandit problem", "author": ["T.L. Lai"], "venue": "The Annals of Statistics,", "citeRegEx": "Lai,? \\Q1987\\E", "shortCiteRegEx": "Lai", "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins", "year": 1985}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Adapting to a changing environment: the brownian restless bandits", "author": ["A. Slivkins", "E. Upfal"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Slivkins and Upfal,? \\Q2008\\E", "shortCiteRegEx": "Slivkins and Upfal", "year": 2008}, {"title": "Bound on E|ET |. We can show as in (Garivier & Capp\u00e9, 2011) (the analysis of KL-UCB) that E|ET | = O(log(log(T ))) (more precisely, this result is a simple application of Theorem 10 in (Garivier", "author": ["E|D\u03b4"], "venue": null, "citeRegEx": "\u221e.,? \\Q2011\\E", "shortCiteRegEx": "\u221e.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "This important class of problems has been recently investigated in (Cope, 2009; Yu & Mannor, 2011).", "startOffset": 67, "endOffset": 98}, {"referenceID": 20, "context": "Introduction Stochastic Multi-Armed Bandits (MAB) (Robbins, 1952; Gittins, 1989) constitute the most fundamental sequential decision problems with an exploration vs.", "startOffset": 50, "endOffset": 80}, {"referenceID": 12, "context": "Introduction Stochastic Multi-Armed Bandits (MAB) (Robbins, 1952; Gittins, 1989) constitute the most fundamental sequential decision problems with an exploration vs.", "startOffset": 50, "endOffset": 80}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB (Auer et al., 2002) and its extensions, e.", "startOffset": 45, "endOffset": 64}, {"referenceID": 5, "context": "KL-UCB (Garivier & Capp\u00e9, 2011; Capp\u00e9 et al., 2013) (note that KL-UCB algorithm was initially proposed in (Lai, 1987), see (2.", "startOffset": 7, "endOffset": 51}, {"referenceID": 18, "context": ", 2013) (note that KL-UCB algorithm was initially proposed in (Lai, 1987), see (2.", "startOffset": 62, "endOffset": 73}, {"referenceID": 0, "context": ", Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al.", "startOffset": 12, "endOffset": 72}, {"referenceID": 16, "context": ", Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al.", "startOffset": 12, "endOffset": 72}, {"referenceID": 4, "context": ", Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al.", "startOffset": 12, "endOffset": 72}, {"referenceID": 8, "context": ", 2008), linear (Dani et al., 2008), convex (Flaxman et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 9, "context": ", 2008), convex (Flaxman et al., 2005).", "startOffset": 16, "endOffset": 38}, {"referenceID": 17, "context": "They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).", "startOffset": 56, "endOffset": 118}, {"referenceID": 16, "context": "They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).", "startOffset": 56, "endOffset": 118}, {"referenceID": 4, "context": "They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).", "startOffset": 56, "endOffset": 118}, {"referenceID": 7, "context": "Unimodal bandits have been specifically addressed in (Cope, 2009; Yu & Mannor, 2011).", "startOffset": 53, "endOffset": 84}, {"referenceID": 7, "context": "In (Cope, 2009), bandits with a continuous set of arms are studied, and the author shows that the Kiefer-Wolfowitz stochastic approximation algorithm achieves a regret of the order of O( \u221a T ) under some strong regularity assumptions on the reward function.", "startOffset": 3, "endOffset": 15}, {"referenceID": 14, "context": "(Hartland et al., 2007; Garivier & Moulines, 2008; Slivkins & Upfal, 2008; Yu & Mannor, 2011).", "startOffset": 0, "endOffset": 93}, {"referenceID": 18, "context": "The algorithm is based on KL-UCB proposed in (Lai, 1987; Capp\u00e9 et al., 2013), and uses KL-divergence upper confidence bounds to define an index for each arm.", "startOffset": 45, "endOffset": 76}, {"referenceID": 5, "context": "The algorithm is based on KL-UCB proposed in (Lai, 1987; Capp\u00e9 et al., 2013), and uses KL-divergence upper confidence bounds to define an index for each arm.", "startOffset": 45, "endOffset": 76}, {"referenceID": 5, "context": "OSUB can be readily extended to systems where reward distributions are within one-parameter exponential families by simply modifying the definition of arm indices as done in (Capp\u00e9 et al., 2013).", "startOffset": 174, "endOffset": 194}, {"referenceID": 5, "context": "(i) When k is the leader, the algorithm behaves like KL-UCB restricted to the arms around k, and the regret at these rounds can be analyzed as in (Capp\u00e9 et al., 2013).", "startOffset": 146, "endOffset": 166}, {"referenceID": 17, "context": "We show that combining discretization and the UCB algorithm as initially proposed in (Kleinberg, 2004) yields lower regrets than LSE in practice (see Section 7), and is orderoptimal, i.", "startOffset": 85, "endOffset": 102}, {"referenceID": 1, "context": "We denote by UCB(\u03b4) the UCB algorithm (Auer et al., 2002) applied to the discretized bandit.", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "Discrete bandits We compare the performance of our algorithm to that of KL-UCB (Capp\u00e9 et al., 2013), LSE (Yu & Mannor, 2011), UCB (Auer et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 1, "context": ", 2013), LSE (Yu & Mannor, 2011), UCB (Auer et al., 2002), and UCB-U.", "startOffset": 38, "endOffset": 57}, {"referenceID": 15, "context": "As proven by Hoeffding (Hoeffding, 1963)[eq.", "startOffset": 23, "endOffset": 40}, {"referenceID": 1, "context": "By (Auer et al., 2002) (the analysis of UCB), for all k, E[tk(T )] \u2264 8 log(T )/(\u03bck\u2217 \u2212 \u03bck).", "startOffset": 3, "endOffset": 22}], "year": 2014, "abstractText": "We consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope, 2009; Yu & Mannor, 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in nonstationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure. Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).", "creator": "LaTeX with hyperref package"}}}