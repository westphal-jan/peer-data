{"id": "1405.5156", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2014", "title": "Gaussian Approximation of Collective Graphical Models", "abstract": "The Collective Graphical Model (CGM) models a population of independent and identically distributed individuals when only collective statistics (i.e., counts of individuals) are observed. Exact inference in CGMs is intractable, and previous work has explored Markov Chain Monte Carlo (MCMC) and MAP approximations for learning and inference. This paper studies Gaussian approximations to the CGM. As the population grows large, we show that the CGM distribution converges to a multivariate Gaussian distribution (GCGM) that maintains the conditional independence properties of the original CGM. If the observations are exact marginals of the CGM or marginals that are corrupted by Gaussian noise, inference in the GCGM approximation can be computed efficiently in closed form. If the observations follow a different noise model (e.g., Poisson), then expectation propagation provides efficient and accurate approximate inference. The accuracy and speed of GCGM inference is compared to the MCMC and MAP methods on a simulated bird migration problem. The GCGM matches or exceeds the accuracy of the MAP method while being significantly faster.", "histories": [["v1", "Tue, 20 May 2014 17:12:56 GMT  (56kb,D)", "http://arxiv.org/abs/1405.5156v1", "Accepted by ICML 2014. 10 page version with appendix"]], "COMMENTS": "Accepted by ICML 2014. 10 page version with appendix", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["li-ping liu", "daniel sheldon", "thomas g dietterich"], "accepted": true, "id": "1405.5156"}, "pdf": {"name": "1405.5156.pdf", "metadata": {"source": "META", "title": "Gaussian Approximation of Collective Graphical Models", "authors": ["Li-Ping Liu", "Daniel Sheldon", "Thomas G. Dietterich"], "emails": ["LIULI@EECS.OREGONSTATE.EDU", "SHELDON@CS.UMASS.EDU", "TGD@EECS.OREGONSTATE.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of us are able to go in search of a solution that is capable, in that they are able to find a solution that is capable of finding a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, and that they are able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution that is able to find a solution, that is able to find a solution."}, {"heading": "2. Problem Statement and Notation", "text": "The second line shows the model in exponential family form Wainwright & Jordan (2008), where I am an indicator of the event or expression, and the second line shows the model in exponential family form Wainwright & Jordan (2008)."}, {"heading": "3. Approximating CGM by the Normal Distribution", "text": "In this section we will develop a Gaussian approach, the GCGM, the CGM, and show that it is the asymptotically correct distribution as M goes to infinity. We will then show that the GCGM has the same conditional independence structure as the CGM, and we will explicitly derive the conditional distributions, which allow us to use the GCGM distribution as a practical approximate inference method for CGMs. We will follow the most natural approach of approaching the CGM distribution by a multivariate Gaussian with the same mean and covariance matrix. The moments of the CGM distribution directly follow those of the indicator variables of the individual model: Fix a result x x x = (x1)., xn) from the individual model and for each set A (I)."}, {"heading": "3.1. Conditional Distributions", "text": "In fact, most of us are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...)"}, {"heading": "3.2. Explicit Factored Density for Trees", "text": "We assume that only the number of individual nodes is observed. In this case, we can define the conditional distribution only by the number of nodes. (11) By defining z'in Proposition (3) and the property of conditional independence, we can define the number of nodes (z '1,.., z'n) = p'r (z'r), v's (z'v), v's (z'z), v's (z's), z \"s (z'r), z\" s (z'r), z \"s (z's), z\" s \"s\" s (z \"s), z\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s."}, {"heading": "4. Inference with Noisy Observations", "text": "In this case, the cliques correspond to edges and the delimiters correspond to nodes. We also assume that only the nodes are observed. To simplify the notation, we assume that each node is observed (with noise). (It is easy to exclude unobserved nodes, if at all.) From now on, we use uv instead of {u, v} to represent edge clique. Finally, we assume that the entries have been removed from the z vector, as described in the previous section, so that it has the factor density described in the equation. 11.Consider the observation variable for nodes u of yu and assume that there is a Poisson distribution. In the (exact) CGM form, this would be written as an approximation."}, {"heading": "4.1. Inferring Node Counts", "text": "In the GCGM with its observations is the potential at each edge (u, v, to). (14) The common distribution of (zv, to). (14) The common distribution of (zv, to). (14) The common distribution of (zv, to). (12). (14) The model approaches the potential at the edge (u, v). (14) The model approaches the normal distribution at the edge (u, v). (14) The normal distribution at the edge (u, v). (14) The common distribution at the edge (u, to). (17) The contexts for the edge (u, v)."}, {"heading": "4.2. Complexity analysis", "text": "How complex is the computational complexity of the conclusion with the GCGM? If we conclude the number of nodes, we have to solve the optimization problem and calculate a fixed number of matrix inverses. Each matrix inverse takes time L2.5. In the Laplace approximation step, each gradient calculation takes time O (L2). Suppose that in the outer loop m iterations are required. Suppose we have to perform r passes of the EP message and each iteration sweeps through the entire tree. Then, the total time is O (r | E | max (mL2, L2.5)). The maximization problem in the Laplace approximation is smooth and concave, so it is relatively simple. In our experiments, EP normally converts within 10 iterations. In the task of determining edge counts, we only consider the complexity of calculating the mean value, as this is all that is solved in our inference structure."}, {"heading": "5. Experimental Evaluation", "text": "In this section, we evaluate the performance of our method and compare it with the MAP approximation by Sheldon, Sun, Kumar, and Dietterich (2013).This model simulates the migration of a population of M birds on an L = '\u00d7 \"map. The entire population is initially located in the lower left corner of the map. Each bird then makes independent migration decisions for T = 20 time steps.The transition from cell i to cell j in each time step is determined by a logistic regression equation that employs four characteristics, which encode the distance from cell i to cell j, the degree in which the cell falls from the upper right corner to the target cell, the degree in which the cell j is located in the direction in which the wind is blowing, and a factor that encourages the bird to stay in cell i."}, {"heading": "6. Concluding Remarks", "text": "In this paper, the Gaussian approach (GCGM) to the Collective Graphic Model (CGM) was introduced. We showed that the GCGM is the limiting distribution of the CGM as population variable N \u2192 \u221e for the case where the observations depend only on the separators. We showed that the GCGM covariance matrix maintains the conditional independence structure of the CGM, and we presented a method for efficiently reversing this covariance matrix. By applying the expectation multiplier, we developed an efficient algorithm for transmitting messages within the GCGM with non-Gaussian observations. Experiments on a bird migration simulation showed that the GCGM method is at least as accurate as the MAP approach by Sheldon et al. (2013), that it has a much lower variance and that it is calculable 6 times faster."}, {"heading": "Acknowledgement", "text": "This material is based on research funded by the National Science Foundation under grant number 1125228."}, {"heading": "A. Proof of Proposition 1", "text": "The usual notation of the CGM distribution is to replace f (n; \u03b8) in equation. (3) byf \"(n; \u03b8) = VP (iC), iC (iC) nC (iC), iS (iS) nS (iS))) \u03bd (S) (21) We will show that f (n; \u03b8) = f\" (n) count for all n so that h (n) > 0 by showing that both are the probability of a ordered sample with sufficient statistics n. In fact, we assume that there is a ordered sample X = (x1,., xN) with sufficient statistics n. Then, from the inspection of Equation (3) and Equation (21) it becomes clear that f (n; \u03b8) = N m = 1 p (xm,.) stenables = f \"(n,."}, {"heading": "B. Proof of Theorem 1: Additional Details", "text": "Suppose that {nN} is a sequence of random vectors converging in the distribution to n, and that nNA, n N B, and n N S are subvectors satisfactory to all N. Let \u03b1, \u03b2, and \u03b3 be measurable sentences in the corresponding spaces, and definiez = Pr (nA, nB, \u03b2, nS) - (23) Pr (nA, \u03b1 | nS) Pr (nB, \u03b2, nS) Also let zN be the same expression, but with all instances of n replaced by nN, and note that zN = 0 is for allN by the assumed conditional property of independence from equation (22). Since the sequence {nN} converges in the distribution to n, we have a convergence of each term in zN with the corresponding term in z, meaning that thatz = lim N = lim is N = 0 = 0, so the conditional property of equality (Eq) also applies in the boundary."}, {"heading": "C. Proof of Theorem 3: Linear Function from", "text": "Suppose the last indicator variable in IA is i0A, which corresponds to the setting that all nodes in A take the value L. Let me \"A be a set of indicators that contain all entries in IA, but the last i0A. Then IA of I\" A can be obtained by constraining iA IA (iA) = 1. Now all we have to do is show that I \"A\" can be recovered linearly from IA +. We claim that there is an invertable matrix H, so that H I \"A\" = I \"A +. Let us show the existence of H. Let me\" A \"+ (iD) the iD entry of I\" A \"+, which stands for the iD configuration, D\" A.I \"A\" I \"I\" D \"I\" D \"I\" D \"I\" I \"I\" I \"D\" I \"I\" D \"I\" I \"D\" I \"I\" D \"I\" I \"D\" I \"D\" I \"I\" D \"I\" D \"D\" D \"D\" D \"D\" D. \""}], "references": [{"title": "Hyper Markov laws in the statistical analysis of decomposable graphical models", "author": ["A.P. Dawid", "S.L. Lauritzen"], "venue": "The Annals of Statistics,", "citeRegEx": "Dawid and Lauritzen,? \\Q1993\\E", "shortCiteRegEx": "Dawid and Lauritzen", "year": 1993}, {"title": "An Introduction to Probability", "author": ["W. Feller"], "venue": "Theory and Its Applications,", "citeRegEx": "Feller,? \\Q1968\\E", "shortCiteRegEx": "Feller", "year": 1968}, {"title": "Graphical models", "author": ["S.L. Lauritzen"], "venue": null, "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses", "author": ["Loh", "Po-Ling", "Wainwright", "Martin J"], "venue": "The Annals of Statistics,", "citeRegEx": "Loh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Loh et al\\.", "year": 2013}, {"title": "Collective Graphical Models", "author": ["Sheldon", "Daniel", "Dietterich", "Thomas G"], "venue": "In NIPS 2011,", "citeRegEx": "Sheldon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sheldon et al\\.", "year": 2011}, {"title": "Approximate Inference in Collective Graphical Models", "author": ["Sheldon", "Daniel", "Sun", "Tao", "Kumar", "Akshat", "Dietterich", "Thomas G"], "venue": "In Proceedings of ICML 2013, pp", "citeRegEx": "Sheldon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sheldon et al\\.", "year": 2013}, {"title": "Some results about decomposable (or Markov-type) models for multidimensional contingency tables: distribution of marginals and partitioning of tests", "author": ["R. Sundberg"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "Sundberg,? \\Q1975\\E", "shortCiteRegEx": "Sundberg", "year": 1975}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Novel approximations for inference in nonlinear dynamical systems using expectation propagation", "author": ["Ypma", "Alexander", "Heskes", "Tom"], "venue": null, "citeRegEx": "Ypma et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ypma et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Our development relies on the existence of a junction tree (Lauritzen, 1996) on the cliques of C to write the relevant CGM and GCGM distributions in closed form.", "startOffset": 59, "endOffset": 76}, {"referenceID": 2, "context": "Our development relies on the existence of a junction tree (Lauritzen, 1996) on the cliques of C to write the relevant CGM and GCGM distributions in closed form. Henceforth, we assume that such a junction tree exists. In practice, this means that one may need to add fill-in edges to the original model to obtain the triangulated graph G, of which C is the set of maximal cliques. This is a clear limitation for graphs with high tree-width. Our methods apply directly to trees and are most practical for low treewidth graphs. Since we use few properties of the junction tree directly, we review only the essential details here and review the reader to Lauritzen (1996) for further details.", "startOffset": 60, "endOffset": 669}, {"referenceID": 5, "context": "This proposition was first proved in nearly this form by Sundberg (1975) (see also Lauritzen (1996)).", "startOffset": 57, "endOffset": 73}, {"referenceID": 2, "context": "This proposition was first proved in nearly this form by Sundberg (1975) (see also Lauritzen (1996)).", "startOffset": 83, "endOffset": 100}, {"referenceID": 2, "context": "This proposition was first proved in nearly this form by Sundberg (1975) (see also Lauritzen (1996)). Proposition 1 differs from those presentations by writing f(n;\u03b8) in terms of the original parameters \u03b8 instead of the clique and separator marginals {\u03bcC ,\u03bcS}, and by including hard constraints in the base measure h(n). The hard constraints enforce consistency of the sufficient statistics of all cliques on their adjacent separators, and were treated implicitly prior to Sheldon & Dietterich (2011). A proof of the equivalence between our expression for f(n;\u03b8) and the expressions from prior work is given in the supplementary material.", "startOffset": 83, "endOffset": 501}, {"referenceID": 2, "context": "This proposition was first proved in nearly this form by Sundberg (1975) (see also Lauritzen (1996)). Proposition 1 differs from those presentations by writing f(n;\u03b8) in terms of the original parameters \u03b8 instead of the clique and separator marginals {\u03bcC ,\u03bcS}, and by including hard constraints in the base measure h(n). The hard constraints enforce consistency of the sufficient statistics of all cliques on their adjacent separators, and were treated implicitly prior to Sheldon & Dietterich (2011). A proof of the equivalence between our expression for f(n;\u03b8) and the expressions from prior work is given in the supplementary material. Dawid & Lauritzen (1993) refer to the same distribution as the hyper-multinomial distribution due to the fact that it follows conditional independence properties analogous to those in the original graphical model.", "startOffset": 83, "endOffset": 664}, {"referenceID": 1, "context": ", I with mean \u03bc and covariance \u03a3 (Feller, 1968).", "startOffset": 33, "endOffset": 47}, {"referenceID": 4, "context": "The evaluation data are generated from the bird migration model introduced in Sheldon et al. (2013). This model simulates the migration of a population of M birds on an L = ` \u00d7 ` map.", "startOffset": 78, "endOffset": 100}, {"referenceID": 4, "context": "We compare our method to the approximate MAP method introduced by Sheldon et al. (2013). By treating counts as continuous and approximating the log factorial function, their MAP method finds the approximate mode of the posterior distribution by solving a convex optimization problem.", "startOffset": 66, "endOffset": 88}, {"referenceID": 4, "context": "Experiments on a bird migration simulation showed that the GCGM method is at least as accurate as the MAP approximation of Sheldon et al. (2013), that it exhibits much lower variance, and that it is 6 times faster to compute.", "startOffset": 123, "endOffset": 145}], "year": 2014, "abstractText": "The Collective Graphical Model (CGM) models a population of independent and identically distributed individuals when only collective statistics (i.e., counts of individuals) are observed. Exact inference in CGMs is intractable, and previous work has explored Markov Chain Monte Carlo (MCMC) and MAP approximations for learning and inference. This paper studies Gaussian approximations to the CGM. As the population grows large, we show that the CGM distribution converges to a multivariate Gaussian distribution (GCGM) that maintains the conditional independence properties of the original CGM. If the observations are exact marginals of the CGM or marginals that are corrupted by Gaussian noise, inference in the GCGM approximation can be computed efficiently in closed form. If the observations follow a different noise model (e.g., Poisson), then expectation propagation provides efficient and accurate approximate inference. The accuracy and speed of GCGM inference is compared to the MCMC and MAP methods on a simulated bird migration problem. The GCGM matches or exceeds the accuracy of the MAP method while being significantly faster.", "creator": "LaTeX with hyperref package"}}}