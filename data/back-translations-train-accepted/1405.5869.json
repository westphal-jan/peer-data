{"id": "1405.5869", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2014", "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)", "abstract": "We present the first provably sublinear time algorithm for approximate \\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first hashing algorithm for searching with (un-normalized) inner product as the underlying similarity measure. Finding hashing schemes for MIPS was considered hard. We formally show that the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, and then we extend the existing LSH framework to allow asymmetric hashing schemes. Our proposal is based on an interesting mathematical phenomenon in which inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search. This key observation makes efficient sublinear hashing scheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we provide an explicit construction of provably fast hashing scheme for MIPS. The proposed construction and the extended LSH framework could be of independent theoretical interest. Our proposed algorithm is simple and easy to implement. We evaluate the method, for retrieving inner products, in the collaborative filtering task of item recommendations on Netflix and Movielens datasets.", "histories": [["v1", "Thu, 22 May 2014 19:42:57 GMT  (379kb)", "http://arxiv.org/abs/1405.5869v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.IR cs.LG", "authors": ["anshumali shrivastava", "ping li 0001"], "accepted": true, "id": "1405.5869"}, "pdf": {"name": "1405.5869.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["anshu@cs.cornell.edu", "pingli@stat.rutgers.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.58 69v1 [st at.M L] 21Originally filed in February 2014."}, {"heading": "1 Introduction and Motivation", "text": "The focus of this paper is on the problem of maximum internal product search (MIPS), in which we get a huge data vector collection S of size N, where S-RD and a specific query point q-RD are available. We are interested in finding p-S that maximizes (or approximately maximizes) the internal product qT p. Formally, we are interested in an efficient calculation p = argmin x-S qTx (1) The MIPS problem is related to the problem of searching near neighbours (NNS), which instead requires the calculation p = argmin x-S q-x-x-22 = argmin x-S. (2) These two problems are equivalent if the standard is constant for each element x-S. Note that the value of the q-q-2 standard has no effect, as it is a constant solution and does not change the identity of argmax or argmin problems."}, {"heading": "1.1 Recommender Systems", "text": "Recommendation systems are often based on collaborative filtering based on the behavior of past users, e.g. past purchases and ratings. A latent factor modeling based on matrix factorization [19] is a popular approach to solving collaborative filtering. In a typical matrix factorization model, a user i is associated with a latent user characteristic vector ui, and similarly, an item j is associated with a latent item characteristic vj. The evaluation ri, j of item j by user i is modeled as an internal product between the corresponding characteristic vectors. A popular generalization of this framework, which combines neighborhood information with a latent factor approach [18], typically leads to the following model: ri, j = \u00b5 + bi + bj + uTi vj (3), where \u00b5 is above all the constant user values, or rather, the average value of the evaluation bij, and system xy are vase, respectively, vj, and system xy, respectively."}, {"heading": "1.2 Large-Scale Object Detection with DPM", "text": "Deformable partial model (DPM) based representation of images is the state of the art in object recognition tasks [11]. In the DPM model, a series of partial filters are first learned from the train data set. In detection, these learned filter activations are used across different areas of the test image to evaluate the test image. Activating a filter on an image patch is an internal product between them. Typically, the number of possible filters is large (e.g. millions) and therefore evaluating the test image is costly. Recently, it has been shown that scoring works well in practice only on filters with high activations [10]. Identifying filters from a large collection of possible filters that have high activations on a particular image patch requires the calculation of top interior products. Consequently, an efficient solution to the MIPS problem will benefit from large-scale object detection based on DPM."}, {"heading": "1.3 Structural SVM", "text": "The most expensive step in the iteration of layers is to call the separation oracle that identifies the most violated constraint. Given the current SVM estimate w, the separation oracle computesy \u044bi = argmax y-y-Y (yi, y-i) + wTN (xi, y-i) (5), where \"xi, y\" is the common feature of the representation of data with the possible designation \"y\" and \"i\" (yi, y) is the loss function. This is clearly another example of the MIPS problem. This step is expensive in that the number of possible elements, i.e. the size of Y, may be exponential. Many heuristics have been used to hopefully improve the calculation of argmax in (5), for example in caching [16]. An efficient MIPS routine can make structural SVM faster and more scalable."}, {"heading": "1.4 Multi-Class Label Prediction", "text": "Multi-level SVM (or logistic regression) models learn a weight vector wi for each of the class names i. Once the weights are learned, predicting their class names is essentially a MIPS problem: ytest = argmax i-L xTtest wi (6), where L is the set of possible class names. Note that the standards of the weight vectors \u0430\u0441wi-2 are not constant. The size of the class names varies from application to application. Classification with a large number of possible class names is common, for example, in fine-grain object classification, prediction task with 100,000 classes [10] (i.e., L-L = 100,000). Calculation of such high-dimensional vector multiplications to predict the class name of a single instance can be expensive, for example, in user-oriented applications."}, {"heading": "1.5 The Need for Hashing Inner Products", "text": "Remember the MIPS problem, which consists in finding x-S, which maximizes the inner product between x and the given query q, i.e. maxx-S qTx. A brute force scan of all elements of S can be prohibitively expensive in applications that handle massive data and pay attention to latency (e.g. search). Due to the importance of the problem, there was an attempt to solve MIPS efficiently by combining tree data structures with branch and bound spatial partitioning techniques [27, 17] similar to k-d trees [12]. This method had no verifiable runtime guarantees. In fact, techniques based on spatial partition (such as k-d trees) are known to suffer from the curse of dimensionality. For example, in [30] (both empirically and theoretically) it was shown that all current techniques (based on spatial partition) are suitable for a linear search, even for locality 20."}, {"heading": "1.6 Our Contributions", "text": "We are developing asymmetric LSH (ALSH), an advanced LSH scheme to efficiently solve the approximate MIPS problem. Finding hash-based algorithms for MIPS was considered difficult [27, 17]. In this paper, we formally demonstrate that this is indeed the case in the current framework of LSH and that there can be no LSH to solve MIPS. Despite this negative result, we show that it is possible to loosen the current LSH scheme to enable asymmetric hash functions that MIPS can efficiently solve. This generalization is free of charge, and the advanced scheme inherits all the theoretical guarantees of LSH. Our asymmetric LSH design is based on an interesting mathematical phenomenon that reduces the original MIPS problem after asymmetric transformations to the problem of approximate neighbor search. Based on this key observation, we show an explicit construction of asymmetric sublinear function for the first time verifiable hash algorithm."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Locality Sensitive Hashing (LSH)", "text": "Approximate versions of the search problem near neighbours [15] have been proposed to break the linear bottleneck in the query time.The following formula is generally assumed: Definition: (c-Approximate Near Neighbor or c-NN) If there is a set of points in a D-dimensional space RD and the parameters S0 > 0, \u03b4 > 0, construct a data structure that causes at each query point q with probability 1 \u2212 \u03b4: If there is a S0-near neighbour of q in P, there is a cS0-near neighbour of q in P. The usual notion of S0-near neighbours is in relation to distance. As we deal with similarities, we can define S0-near neighbours of point q as point p with Sim (q, p) as point p) with S0 being equivalent, with Sim the similarity function being of interest."}, {"heading": "2.2 Fast Similarity Search with LSH", "text": "In a typical similarity search task, we get a query q, and our goal is to find x-S with high value of sim (q, x). LSH provides a clean mechanism for creating hash tables [2]. The idea is to link K-independent hash functions to form a meta-hash function Bl (x) = [h1 (x); h2 (x);...; hK (x)] (7), where hi, i = {1,2,..., K} are K-independent hash functions that are sampled from the LSH family. The LSH algorithm requires L-independent meta hash functions Bl (x), l = 1,2,..., L. \u2022 Pre-processing step: During pre-processing, we assign xi-S to the bucket Bl (xi) in the hash table l, the ability l = 1,2,..., L. \u2022 Query Step: Given a query, we place all elements out of the Eq."}, {"heading": "2.3 LSH for L2 distance", "text": "[9] presents a novel LSH family for all Lp (p) (0,2]) distances. In particular, if p = 2, this scheme provides an LSH family for L2 distances. Formally, with a fixed number r, we select a random vector a with each component that is normally generated from i.i.d., i.e., ai \u00b2 N (0,1), and a scalar b that is uniformly generated from [0, r]. The hash function is defined as: hL2a, b (x) = \"aTx + br\" (8), with the collision probability under this scheme being Pr (hL2a, b (x) = hL2a, b (y)) = Fr (d), (9) Fr (d) =. \"oty\" (2 \u2212 norm \")."}, {"heading": "3 Hashing for MIPS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 A Negative Result", "text": "First, we show that under the current LSH framework, it is impossible to obtain a locality-sensitive hashing for MIPS. Theorem 1 There can be no LSH family for MIPS.Proof: Suppose there is such a hash function h. For non-normalized internal products, the self-similarity of a point x to itself is Sim (x, x) = xTx = x-x-x-x-x and there can be another point y, so that Sim (x, y) = yTx-x-x-x-x-x-x-x-x-x is the collision probability of the event {h (x) = h (x)} is always 1. Thus, if h is an LSH an internal product, then the event {h (x) = h-x-x-x-x-x-x cannot be the same."}, {"heading": "3.2 Our Proposal: Asymmetric LSH (ALSH)", "text": "The basic idea of LSH is probable truncation, and it is more general than the requirement to have a single hash function. In the LSH algorithm (Section 2.2), we use the same hash function h for both the preprocessing step and the query step. We assign buckets in the hash table to all candidates x x x x. We use the same h on query q to identify relevant buckets. The only requirement for proof, fact 1, to work is that the collision probability of the event {h (q) = h (x)} increases with the similarity Sim (q, x)}. Theory [13] behind LSH still works if we use hash function h1 for preprocessing x and other hash function h2 for the query as long as the probability of the event {h2 (q) {h2 (q) = h1} (x} increases."}, {"heading": "3.3 From MIPS to Near Neighbor Search (NNS)", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "3.4 Fast Algorithms for MIPS", "text": "(18) shows that the MIPS is reduced to the default search problem close to EQ (2), which can be efficiently solved. (8) As the error term \"EQ\" (2) shows, the additional error in the approximation parameter \"c.1\" (2) can be quickly neglected for all practical purposes. (8) In fact, from a theoretical point of view, we are interested in guarantees for c-approximated solutions, since this additional error is in the approximation parameter \"c.2\" (2) formally, we can specify the following theorems. \"Theorem 3\" (2) Given a c-approximated instance of MIPS, \"i.e., Sim (q) x\" qTx, \"and a query q.\" 2 \"(2), which is executed in such a way that we have the collection\" x. \"(2).We let ourselves hash the vector transformations in Eq.\" (12) and \"Eq.\" We have the following two conditions (2)."}, {"heading": "3.5 Practical Recommendation of Parameters", "text": "In practice, the actual choice of S0 and c depends on the data and is often unknown. Figure 2 shows that m = 2,3,4, U = 0,8, 0,85 and r = 1,5, 3 are reasonable decisions. For the sake of simplicity, we recommend using m = 3, U = 0,83 and r = 2,5. With this choice of parameters, Figure 3 shows that the \u03c1 values using these parameters are very close to the optimal \u043c values."}, {"heading": "3.6 More Insight: The Trade-off between U and m", "text": "To see this, we consider the following figure 4 for Fr (d), i.e., the probability of collision defined in Eq. (10). If we assume that the deviation between U2 m + 1 is small, then p1 = Fr (1 + m / 4 \u2212 2qTx) is small, then p1 = Fr (1 + m / 4 \u2212 2cqTx) and p2 = Fr (1 + m / 4 \u2212 2cqTx). Due to the limited norms, we have \u2212 U2 m + 2, the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation \u2212 from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the 1, and the deviation from the deviation from the deviation from the deviation from the deviation from the 2 from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the deviation from the 1, + the deviation from the deviation from the deviation from the deviation from the deviation from the 1 from the deviation from the deviation from the deviation from the deviation from the 1 from the deviation from the deviation from the deviation from the deviation from the 2."}, {"heading": "3.7 Parallelization", "text": "To conclude this section, we should mention that the hash table-based scheme is massively parallelizable. Different nodes on clusters must maintain their own hash tables and hash functions. Operating the retrieval from buckets and calculating the maximum internal product via these retrieved candidates is a local operation. Calculating the final maximum can be done efficiently simply by communicating a single number per node. Scalability of hash-based methods is one of the reasons for their popularity in industrial practice."}, {"heading": "4 Evaluations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We evaluate our proposed hash function for the MIPS problem using two popular collaborative filter datasets (via the task of item recommendations): \u2022 Movielens. We select the largest Movielens dataset available, the Movielens 10M, which contains approximately 10 million movie ratings from 70,000 users over 10,000 movie titles. Ratings range from 1 to 5, with steps of 0.5 (i.e. a total of 10 possible ratings). \u2022 Netflix. This dataset contains 100 million movie ratings from 480,000 users over 17,000 movie titles. Ratings are performed on a scale from 1 to 5 (integers). Each dataset forms a sparse user item matrix R, where the value of R (i, j) indicates the user rating i for the movie j. Given the user item matrix R, we follow the PureSVD method described in [6] to generate user and item matrix matrix vectors. We call the SVrix-D matrix."}, {"heading": "4.2 Baseline Hash Function", "text": "Since our asymmetric transformation hash function uses P (12) and Q (13) hashing for the L2 distance, we would like to know whether such transformations are even necessary and, in addition, to get an estimate of the improvements achieved by these transformations. Therefore, we compare our proposal with L2LSH, the hashscheme described by Equation (8) hL2a, b, and implement it in the LSH package for searching near neighbors at Euclidean distance. Although L2LSH is not optimal for searching for unstandardized internal products, it does offer some indexing capability. Our experiments will show that the proposed method often significantly outperforms L2LSH when retrieving internal products, which is not surprising since we know that the ranking of L2 distances may differ from the ranking of internal products."}, {"heading": "4.3 Evaluations", "text": "rE \"s tis rf\u00fc ide r\u00fc \u00fc ide r\u00fc the r the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f"}, {"heading": "5 Conclusion and Future Work", "text": "Considering a query data vector, the task of MIPS is to find data vectors from the repository that are most similar to the query in terms of (un-normalized) internal product (rather than distance).This problem is challenging and prior to our work there was no proven sublinear time algorithm for MIPS. The current framework of LSH (location-dependent hashing) is not sufficient to solve MIPS. In this study, we develop ALSH (asymmetric LSH), which generalizes the existing LSH system by (appropriately chosen) asymmetric transformations to the input query vector and the data vectors in the repository. We present an implementation of ALSH through a novel transformation that converts the original inner products into transformative space."}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "STOC, pages 557\u2013563, Seattle, WA,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "E2lsh: Exact euclidean locality sensitive hashing", "author": ["A. Andoni", "P. Indyk"], "venue": "Technical report,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "On the resemblance and containment of documents", "author": ["A.Z. Broder"], "venue": "the Compression and Complexity of Sequences, pages 21\u201329, Positano, Italy,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "STOC, pages 380\u2013388, Montreal, Quebec, Canada,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Lsh-preserving functions and their applications", "author": ["F. Chierichetti", "R. Kumar"], "venue": "SODA, pages 1078\u2013 1094,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["P. Cremonesi", "Y. Koren", "R. Turrin"], "venue": "Proceedings of the fourth ACM conference on Recommender systems, pages 39\u201346. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A.S. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "Proceedings of the 16th international conference on World Wide Web, pages 271\u2013280. ACM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast locality-sensitive hashing", "author": ["A. Dasgupta", "R. Kumar", "T. Sarl\u00f3s"], "venue": "KDD, pages 1073\u20131081,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokn"], "venue": "SCG, pages 253 \u2013 262, Brooklyn, NY,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["T. Dean", "M.A. Ruzon", "M. Segal", "J. Shlens", "S. Vijayanarasimhan", "J. Yagnik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1814\u20131821. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627\u20131645,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A projection pursuit algorithm for exploratory data analysis", "author": ["J.H. Friedman", "J.W. Tukey"], "venue": "IEEE Transactions on Computers, 23(9):881\u2013890,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1974}, {"title": "Approximate nearest neighbor: Towards removing the curse of dimensionality", "author": ["S. Har-Peled", "P. Indyk", "R. Motwani"], "venue": "Theory of Computing, 8(14):321\u2013350,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding near-duplicate web pages: a large-scale evaluation of algorithms", "author": ["M.R. Henzinger"], "venue": "SIGIR, pages 284\u2013291,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "STOC, pages 604\u2013613, Dallas, TX,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning, 77(1):27\u201359,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient retrieval of recommendations in a matrix factorization framework", "author": ["N. Koenigstein", "P. Ram", "Y. Shavitt"], "venue": "CIKM, pages 535\u2013544,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Y. Koren"], "venue": "KDD, pages 426\u2013434. ACM,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "A sketch algorithm for estimating two-way and multi-way associations", "author": ["P. Li", "K.W. Church"], "venue": "Computational Linguistics (Preliminary results appeared in HLT/EMNLP 2005), 33(3):305\u2013354,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Very sparse random projections", "author": ["P. Li", "T.J. Hastie", "K.W. Church"], "venue": "KDD, pages 287\u2013296, Philadelphia, PA,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "b-bit minwise hashing for estimating three-way similarities", "author": ["P. Li", "A.C. K\u00f6nig", "W. Gui"], "venue": "Advances in Neural Information Processing Systems, Vancouver, BC,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Coding for random projections and approximate near neighbor search", "author": ["P. Li", "M. Mitzenmacher", "A. Shrivastava"], "venue": "Technical report, arXiv:1403.8144,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "One permutation hashing", "author": ["P. Li", "A.B. Owen", "C.-H. Zhang"], "venue": "NIPS, Lake Tahoe, NV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting Near-Duplicates for Web-Crawling", "author": ["G.S. Manku", "A. Jain", "A.D. Sarma"], "venue": "WWW, Banff, Alberta, Canada,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Maximum inner-product search using cone trees", "author": ["P. Ram", "A.G. Gray"], "venue": "KDD, pages 931\u2013939,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond pairwise: Provably fast algorithms for approximate k-way similarity search", "author": ["A. Shrivastava", "P. Li"], "venue": "NIPS, Lake Tahoe, NV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Densifying one permutation hashing via rotation for fast near neighbor search", "author": ["A. Shrivastava", "P. Li"], "venue": "ICML, Beijing, China,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "A quantitative analysis and performance study for similaritysearch methods in high-dimensional spaces", "author": ["R. Weber", "H.-J. Schek", "S. Blott"], "venue": "VLDB, pages 194\u2013205,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 16, "context": "There are many scenarios in which MIPS arises naturally at places where the norms of the elements in S have very significant variations [17] and can not be controlled.", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "A popular generalization of this framework, which combines neighborhood information with latent factor approach [18], leads to the following model: ri,j = \u03bc + bi + bj + ui vj (3) where \u03bc is the over all constant mean rating value, bi and bj are user and item biases, respectively.", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "Recently, [6] showed that a simple computation of ui and vj based on naive SVD of the sparse rating matrix outperforms existing models, including the neighborhood model, in recommending top-ranked items.", "startOffset": 10, "endOffset": 13}, {"referenceID": 16, "context": ", \u2225vj\u22252, which often has a wide range in practice [17].", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "2 Large-Scale Object Detection with DPM Deformable Part Model (DPM) based representation of images is the state-of-the-art in object detection tasks [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "Very recently, it was shown that scoring based only on filters with high activations performs well in practice [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "3 Structural SVM Structural SVM, with cutting plane training [16], is one of the popular methods for learning over structured data.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "Many heuristics were deployed to hopefully improve the computation of argmax in (5), for instance caching [16].", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "Classifying with large number of possible class labels is common in fine grained object classification, for instance, prediction task with 100,000 classes [10] (i.", "startOffset": 155, "endOffset": 159}, {"referenceID": 24, "context": "Owing to the significance of the problem, there was an attempt to efficiently solve MIPS by making use of tree data structure combined with branch and bound space partitioning technique [27, 17] similar to k-d trees [12].", "startOffset": 186, "endOffset": 194}, {"referenceID": 16, "context": "Owing to the significance of the problem, there was an attempt to efficiently solve MIPS by making use of tree data structure combined with branch and bound space partitioning technique [27, 17] similar to k-d trees [12].", "startOffset": 186, "endOffset": 194}, {"referenceID": 11, "context": "Owing to the significance of the problem, there was an attempt to efficiently solve MIPS by making use of tree data structure combined with branch and bound space partitioning technique [27, 17] similar to k-d trees [12].", "startOffset": 216, "endOffset": 220}, {"referenceID": 27, "context": "For example, it was shown in [30] (both empirically and theoretically) that all current techniques (based on space partitioning) degrade to linear search, even for dimensions as small as 10 or 20.", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "Locality Sensitive Hashing (LSH) [15] based randomized techniques are common and successful in industrial practice for efficiently solving NNS (near neighbor search).", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "Finding hashing based algorithms for MIPS was considered hard [27, 17].", "startOffset": 62, "endOffset": 70}, {"referenceID": 16, "context": "Finding hashing based algorithms for MIPS was considered hard [27, 17].", "startOffset": 62, "endOffset": 70}, {"referenceID": 8, "context": "Our evaluations support the theoretical results and clearly show that the proposed asymmetric hash function is superior for retrieving inner products, compared to the well known hash function based on pstable distribution for L2 norm [9] (which is also part of standard LSH package [2]).", "startOffset": 234, "endOffset": 237}, {"referenceID": 1, "context": "Our evaluations support the theoretical results and clearly show that the proposed asymmetric hash function is superior for retrieving inner products, compared to the well known hash function based on pstable distribution for L2 norm [9] (which is also part of standard LSH package [2]).", "startOffset": 282, "endOffset": 285}, {"referenceID": 14, "context": "1 Locality Sensitive Hashing (LSH) Approximate versions of the near neighbor search problem [15] were proposed to break the linear query time bottleneck.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "The popular technique for c-NN uses the underlying theory of Locality Sensitive Hashing (LSH) [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "LSH provides a clean mechanism of creating hash tables [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "This makes LSH a widely popular technique in industrial practice [14, 25, 7].", "startOffset": 65, "endOffset": 76}, {"referenceID": 23, "context": "This makes LSH a widely popular technique in industrial practice [14, 25, 7].", "startOffset": 65, "endOffset": 76}, {"referenceID": 6, "context": "This makes LSH a widely popular technique in industrial practice [14, 25, 7].", "startOffset": 65, "endOffset": 76}, {"referenceID": 8, "context": "3 LSH for L2 distance [9] presented a novel LSH family for all Lp (p \u2208 (0,2]) distances.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "This scheme is also the part of LSH package [2].", "startOffset": 44, "endOffset": 47}, {"referenceID": 21, "context": "Very recently [23] reported an improvement of this well-known hashing scheme when the data can be normalized (for example, when x and y both have unit L2 norm).", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "However, in our problem setting, since the data can not be normalized, we can not take advantage of the new results of [23], at the moment.", "startOffset": 119, "endOffset": 123}, {"referenceID": 24, "context": "In [27, 17], the authors also argued that finding locality sensitive hashing for inner products could be hard, but to the best of our knowledge we have not seen a formal proof.", "startOffset": 3, "endOffset": 11}, {"referenceID": 16, "context": "In [27, 17], the authors also argued that finding locality sensitive hashing for inner products could be hard, but to the best of our knowledge we have not seen a formal proof.", "startOffset": 3, "endOffset": 11}, {"referenceID": 3, "context": "\u25fb Note that in [4] it was shown that we can not have a hash function where the collision probability is equal to the inner product, because \u201c1 - inner product\u201d does not satisfy the triangle inequality.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "The theory [13] behind LSH still works if we use hash function h1 for preprocessing x \u2208 S and", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "The proof can be completed by exact same arguments used for proving Fact 1 (See [13] for details).", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "Given the user-item ratings matrix R, we follow the PureSVD procedure described in [6] to generate user and item latent vectors.", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "The PureSVD procedure, despite its simplicity, outperforms other popular recommendation algorithms for the task of top-ranking recommendations (see [6] for more details) on these two datasets.", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "Following [6], we use the same choices for the latent dimension f , i.", "startOffset": 10, "endOffset": 13}], "year": 2014, "abstractText": "We1 present the first provably sublinear time algorithm for approximate Maximum Inner Product Search (MIPS). Our proposal is also the first hashing algorithm for searching with (un-normalized) inner product as the underlying similarity measure. Finding hashing schemes for MIPS was considered hard. We formally show that the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, and then we extend the existing LSH framework to allow asymmetric hashing schemes. Our proposal is based on an interesting mathematical phenomenon in which inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search. This key observation makes efficient sublinear hashing scheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we provide an explicit construction of provably fast hashing scheme for MIPS. The proposed construction and the extended LSH framework could be of independent theoretical interest. Our proposed algorithm is simple and easy to implement. We evaluate the method, for retrieving inner products, in the collaborative filtering task of item recommendations on Netflix and Movielens datasets. Initially submitted in Feb. 2014.", "creator": "LaTeX with hyperref package"}}}