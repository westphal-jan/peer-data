{"id": "1405.5960", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2014", "title": "LASS: A Simple Assignment Model with Laplacian Smoothing", "abstract": "We consider the problem of learning soft assignments of $N$ items to $K$ categories given two sources of information: an item-category similarity matrix, which encourages items to be assigned to categories they are similar to (and to not be assigned to categories they are dissimilar to), and an item-item similarity matrix, which encourages similar items to have similar assignments. We propose a simple quadratic programming model that captures this intuition. We give necessary conditions for its solution to be unique, define an out-of-sample mapping, and derive a simple, effective training algorithm based on the alternating direction method of multipliers. The model predicts reasonable assignments from even a few similarity values, and can be seen as a generalization of semisupervised learning. It is particularly useful when items naturally belong to multiple categories, as for example when annotating documents with keywords or pictures with tags, with partially tagged items, or when the categories have complex interrelations (e.g. hierarchical) that are unknown.", "histories": [["v1", "Fri, 23 May 2014 04:28:29 GMT  (387kb)", "http://arxiv.org/abs/1405.5960v1", "20 pages, 4 figures. A shorter version appears in AAAI 2014"]], "COMMENTS": "20 pages, 4 figures. A shorter version appears in AAAI 2014", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["miguel \u00e1 carreira-perpi\u00f1\u00e1n", "weiran wang"], "accepted": true, "id": "1405.5960"}, "pdf": {"name": "1405.5960.pdf", "metadata": {"source": "CRF", "title": "LASS: a simple assignment model with Laplacian smoothing", "authors": ["Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n", "Weiran Wang"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 5.59 60v1 [cs.LG] 2 3M ay"}, {"heading": "1 Introduction", "text": "A major success of machine learning in recent years has been the development of semi-supervised learning (SSL) (Chapelle et al., 2006), where we get labels for only a few of the training points. Many SSL approaches rely on a neighborhood diagram based on the training data (labeled and not labeled), but typically weighted with similarity values. Laplacian-based formulations of this diagram are used to construct a square nonnegative function that measures the consistency of possible labels with the graph structure, and minimizing the existing labels has the effect of spreading them across the graph. Laplacian-based formulations are conceptually simple, compatibly efficient (since the Laplacian is usually frugal), have a solid foundation in graph theory and linear algebra (Chung, 1997; Doyle and Snell, 1984), and are most important in practice."}, {"heading": "2 The Laplacian assignment (LASS) model", "text": "We look at the following mapping problem. We have N items andK categories, and we want to determine soft mappings znk of each item n to each category k, where znk [0, 1] and \u2211 Kk = 1 znk = 1 for each item n = 1,.., N. We get two items of similarity matrices, appropriately defined, and typically sparse: an item-item similarity matrix W, which is an N \u00b7 N matrix of affinities wnm 0 between each item n and m; and an item category similarity matrix G, which is an N \u00b7 K matrix of affinities gnk - R between each item n and category k."}, {"heading": "2.1 Extreme values of \u03bb", "text": "We can determine the solution (s) for the following extreme values of \u03bb: \u2022 If \u03bb = 0, then the LASS problem is a linear program (LP) and separates over each element n = 1,.., N. The solution is znk = \u03b4 (k \u2212 kmax (n)), where kmax (n) = argmax {gnk, k = 1,..., K}, i.e., each element is assigned to its most similar category. This tells us what the linear term can do for itself. (If the maximum is reached for more than one category for a given point, then there is an infinite number of \"mixed\" solutions corresponding to any assignment value for these categories, and zero for the rest.) \u2022 If \u03bb = xi or equivalentlyG = 0, then the LASS problem is a square program with an infinite number of solutions of the form zn, m = 1,."}, {"heading": "2.2 Existence and unicity of the solution", "text": "The LASS problem is a convex QP = 0, so general results of the convex optimization tell us that all minima are global minima. However, since the Hessian concept of objective function is positive, there may be several minima. The following theorem characterizes the qualities of the LASS task (1). Any other solution has the form Z + 1NpT, in which the p-RK fulfills the conditions: pT1K = 0, p T-RN = 0, Z + 1Np T = 0. (2) In particular, for each k = 1,., K for which we have the form Z + 1NpT in which the p-RK fulfills the conditions: pT1K = 0, p T = 0, Z + 1Np T = 0."}, {"heading": "2.3 Particular cases", "text": "Theorem 2.7. Let us assume that the graph Laplacian L corresponds to a contiguous graph and let Z-RN-K be a solution to the LASS problem (1). Then: 1. If gn = 0 then zn = ZTwn 1T N wn = \u2211 N m = 1 wnm-N m-N = 1 w-nm \u2032 zm, \u00b5n = 0 and \u03c0n = 0.2. If gk \u2264 0 then zk = 0, \u00b5n = \u2212 gk and \u03c0n = 0, Proof. Both statements follow from the substitution of the values in the KT conditions (4). Conditions (4b) - (4e) are trivially fulfilled, so that we can only prove state (4a). For statement 1 we can write the line n of L as ln = \u2212 wn + (1TNwn) en, where wn is the line n of W and en is a vector with the input enm = (n), \u2212 vector and we can write all the line rectors as a spector (then a zalector)."}, {"heading": "2.4 Lagrange multipliers for a solution", "text": "Given a feasible point ZN \u00b7 K in the parameter space, we can test whether it is a solution to the LASS problem (max. QP, the KKT conditions are necessary and sufficient for a solution (Nocedal and Wright, 2006). For our problem, and written in matrix form, the KKT conditions are: 2\u03bbLZ \u2212 G \u2212 \u03c01TK \u2212 M = 0 (4a) Z1K \u2212 \u2212 \u2212 n (4b) Z 0 (4c) M \u2265 0 (4d) M \u00b2 Z = 0 (4e), where \"K\" means an elementary product, and \"M\" are the Lagrange multipliers associated with the point Z for equality and inequality constraints, respectively. \"We must calculate Z, the KKT system (4) has 2NK linear equations for N + NK 1nowns, and its solution is unique if Z is feasible, as we will see."}, {"heading": "3 A simple, efficient algorithm to solve the QP", "text": "It is possible to solve problem (1) in different ways, but one has to be careful in the development of an effective algorithm, since the number of variables and the number of constraints increases proportionally to the number of data points and can then be very large. We describe here an algorithm that is very simple, has a guaranteed convergence without line search and exploits the structure of the problem and the thrift of L. It is based on the method of alternating direction of multipliers (ADMM), combined with a direct linear solver that uses the Schur complement and caches the Cholesky factorization of L."}, {"heading": "3.1 QP solution using ADMM", "text": "(2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) ("}, {"heading": "3.2 Application to our QP", "text": "We are now writing the ADMM updates for our QP (1), where we determine: P = 2\u03bbdiag (L,.., L) of NK \u00b7 NK \u00b7 \u00b7 q = \u2212 vec (G) of NK \u00b7 1 A = (IN,.., IN) of N \u00b7 NK b = 1N of N \u00b7 1where vec (\u00b7) concatenates the columns of its argument into a single column vector. Given the structure of these matrices, the solution of the KKT system (15) is greatly simplified by the use of Schur supplement (16). The basic reasons for this are that (1) the matrix P is block diagonally identical to K copies of the graph L, which is itself usually economical; and (2) the particularly simple form of the equality condition Y-matrix A. Although the x update involves the solution of a large linear system of NK equations, it is equivalent to the solution of K."}, {"heading": "3.3 Remarks", "text": "Theorem 3.1. For each iteration in the algorithm updates (17), Z1K = 1N, U \u2264 0, Y \u2265 0 and Y \u0445 U = 0, Proof. For Z, the substitution of eq. (17a) in (17b) must be: Z1K = (2\u03bbL + \u03c1I) \u2212 1 (\u03c1 (Y \u2212 U) + G \u2212 empirical. For Z = (Y \u2212 U) 1K + empirical. (G1K + empirical. (2\u03bbL + empirical.) \u2212 empirical. (The last step results from (2\u03bbL + \u2212 empirical.) 1N = empirical. \u2212 empirical. (Z \u2212 empirical.) For U, from eqs. (17c) - (17d) we have these U + empirical. (U + empirical.) Empirical. (U + empirical. (U + empirical.) Empirical. (U + empirical.) Empirical. (U + empirical.)"}, {"heading": "3.4 Computational complexity", "text": "Each step in (17) is O (NK), except for the linear system solution in (17b). If L is sparse, the use of the cholesky factor also makes this step O (NK) and adds the one-time setup costs for calculating the cholesky factor (which is linear even in N with sufficiently sparse matrices). Therefore, any iteration of the algorithm is cheap. In practice, the algorithm for good values of \u03c1 quickly approaches the solution in the first iterations and then converts slowly, as is generally known from ADMM algorithms. However, since each iteration is so cheap, we can perform a large number of them when high accuracy is required."}, {"heading": "3.5 Initialization", "text": "If the LASS problem itself is a subproblem in a larger problem (as in the Laplacian K modes clustering algorithm; Wang and Carreira-Perpin-A \u00b2 n, 2014b), one should heat up the iteration of eq. (17) from the values of Y + U in the previous outer loop iteration. Otherwise, we can simply initialize Y = U \u2212 Z = 0, which (substitution in eqs. (17a) - (17b))) yields Z0 = (2\u03bbL + \u03c1I) \u2212 1 (G \u2212 1 K G1K1 T K) + 1 K \u2212 1N1 T K (where G \u2212 1KG1K1TK is the matrix G with centered rows, and 1 K 1N1 T K is the simplest bore center). This initialization is closely related to the projection at the simplest optimum of the LASS problem, as we will show next."}, {"heading": "3.6 Stopping criterion", "text": "We stop when the change in absolute conditions in Z in the last \"iterations\" falls below a specified tolerance tol (e.g. 10 \u2212 5). Using an absolute criterion here is tantamount to using a relative criterion, because \"zn\" 1 = 1, n = 1,.., N. Because our iterations are so cheap, evaluating \"Z\" -Z saves a runtime comparable to that of the updates in (17) (possibly with the exception of the Z update), so that checking the stop criterion for each \"Z\" = 10-100 iterations saves about 10% runtime. Another possible stop criterion is checking whether the KT conditions (4) are met to a specified tolerance (possibly with the exception of the Z update)."}, {"heading": "3.7 Optimal penalty parameter \u03c1", "text": "The speed at which ADMM converges depends on the square penalty parameter \u03c1 (Boyd et al., 2011), where we set positive similarity values for a point in each cluster, which results in each cluster being assigned to a different category as expected. 8000 parameters are the problem, and we performed 104 iterations that lasted 11 seconds. There is little work on how to select a point to achieve the fastest convergence. Lately, Ghadimi et al. (2013) have proposed for QPs to use the relative error themselves when Manfred and Manfred F are the smallest (non-zero) and largest eigenvalue of the laptop loader. In Fig. 1, we show the relative error that Z- Zopt-F / Zopt-F are the least (non-zero) relative to the number of iterations and the largest eigenvalue of the laptop loader."}, {"heading": "3.8 Matlab code", "text": "The following Matlab code implements the algorithm assuming a direct solution of the linear Z update system. function [Z, Y, U, nu] = lass (L, l, G, r, Y, U, maxit, tol) [N, K] = size (G); LI = 2 * l * L + r * speye (N, N); h = (-sum (G, 2) + r) / K; Zold = zeros (N, K); for i = 1: maxitnu = (r / K) * sum (Y-U, 2) - h; Z = LI\\ bsxfun (@ minus, r * (Y-U) + G, nu); Y = max (Z + U, 0); U = U + Z - Y; if max (abs (Z (:) -Zold (:)))) < tol break end; Zold = Z; end"}, {"heading": "4 Out-of-sample mapping", "text": "This means that the optimal mappings Z for the obtained values will only be achieved during the training. (This means that the optimal mappings Z for the obtained values will only be achieved during the training.) It will be a new, test element x (for example, a new dot x-RD), along with its item item and item category similarities w = (wn), n = 1,., N and g = (gk), k = 1,., and we will find its mapping to each category. We will follow the reasoning of Carreira-Perpin and Lu (2007) to derive an out-sample mapping. While you could re-augment the whole system, this would be very time consuming, and the mappraisals of all points would change (although very marginal). A more practical and even natural way to define an out-sample mapping z (x) is to solve a problem of the form (1) consisting of a dataset."}, {"heading": "5 Related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Semisupervised learning with a Laplacian penalty (SSL)", "text": "In semivised learning (SSL) with a Laplacian penalty (Zhu et al.) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTTLZ) (ZTLZ) (ZTLZ) (ZZTLZ) (ZTLZ) (ZZZS) (ZS) (ZS) (ZS) (ZS) (ZS) (ZS ZLZ) (ZS) (ZS) (ZS) (ZLZ) (ZTL) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZTLZ) (ZLZ) (ZTLZ) (ZLZ) (ZTLZ (ZLZ) (ZLZ) (ZTLZ) (ZTLZ) (ZTLZ (ZLZ) (ZTLZ) (ZTLZ) (ZLZ) (ZLZ) (ZTLZ) (ZLZ) (ZLZ) (ZLZ) (ZTLZ) (ZLZ) (ZLL) (Z"}, {"heading": "5.2 Assignments and probabilities", "text": "The semantics of the allocations differ from that of the probabilities. Given a discrete probability distribution p over the values in B = {1,.., K} means that \"p (Z = k) = zk\" means that Z corresponds exactly to one of the values in B, but that we observe the value k with a probability zk (frequentist or Bayesian) for each k value. In contrast, K are possible investments, and zk is the portion of a $1 capital allocated to the investment k, not the probability that we allocate all $1 to the investment k. However, in some cases, the allocations can actually be used as proxies for probabilities, and zk is the portion of a $1 capital allocated to the investment k, not the probability that we allocate all $1 to the investment k."}, {"heading": "5.3 Other applications of LASS", "text": "The LASS problem (1) appears in the assignment step of the training algorithm for clustering in the K modes Laplacs (Wang and Carreira-Perpin), i.e. data points x1,.., xN-RD to be bundled in K clusters, and the categories are the cluster centroids c1,..., cK-RD. Both affinities are Gaussian: the W matrix has the entries wnm = G (xn \u2212 xm) / \u03c3 2), n, m = 1,.., N, and the G matrix has the entries gnk = G (xn \u2212 ck) / \u03c3 2), n = 1,., N, k = 1,., K, where G (\u00b7 2) results in a Gaussian core. If the Laplacian K modes are optimal, the cluster problem is grouped."}, {"heading": "6 Experiments", "text": "We have constructed a simple example to show the difference between LASS and SSL, and the role of positive and negative item-category similarities is not given. The data consists of N = 14 points a-g and a-g \"and K = 4 categories C1-C4, which are related to each other. A graph is based on the data set, which is characterized by the edges between the data points. We consider a-g (filled circles) to be partially labeled and a-g.\" This is very different from the usual classifications in which each point is assigned to only one category. A chart is based on the edges between the data points. We consider a-g (filled circles) to be partially labeled and a-g. \"The basic truth (true assignments from the Venn chart), positive and negative labels that we use are given in fig. 2 (top right)."}, {"heading": "7 Conclusion", "text": "We have proposed a simple quadratic programming model for the allocation of objects to categories combining two complex and possibly contradictory sources of information: crowd wisdom and the Precision Recall F-1 Score1 2 4 5 6253040SSL SL2.: The Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland-Greenland"}], "references": [{"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "Belkin and Niyogi.,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Machine Learning Research,", "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Carreira-Perpi\u00f1\u00e1n. Fast nonparametric clustering with Gaussian blurring mean-shift", "author": ["\u00c1. M"], "venue": "Proc. of the 23rd Int. Conf. Machine Learning", "citeRegEx": "M.,? \\Q2006\\E", "shortCiteRegEx": "M.", "year": 2006}, {"title": "The Laplacian Eigenmaps Latent Variable Model", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "Z. Lu"], "venue": "Proc. of the 11th Int. Workshop on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Lu.,? \\Q2007\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Lu.", "year": 2007}, {"title": "LASS: A simple assignment model with Laplacian smoothing", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "W. Wang"], "venue": "Proc. of the 28th National Conference on Artificial Intelligence (AAAI", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.,? \\Q2014\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.", "year": 2014}, {"title": "Semi-Supervised Learning. Adaptive Computation and Machine Learning Series", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "editors"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Spectral Graph Theory. Number 92 in CBMS Regional Conference Series in Mathematics", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "Chung.,? \\Q1997\\E", "shortCiteRegEx": "Chung.", "year": 1997}, {"title": "Random Walks and Electric Networks, volume 22 of Carus Mathematical Monographs", "author": ["P.G. Doyle", "J.L. Snell"], "venue": "Mathematical Association of America,", "citeRegEx": "Doyle and Snell.,? \\Q1984\\E", "shortCiteRegEx": "Doyle and Snell.", "year": 1984}, {"title": "Efficient projections onto the l1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "Proc. of the 25th Int. Conf. Machine Learning", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Optimal parameter selection for the alternating direction method of multipliers (ADMM): Quadratic problems", "author": ["E. Ghadimi", "A. Teixeira", "I. Shames", "M. Johansson"], "venue": "[math.OC],", "citeRegEx": "Ghadimi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ghadimi et al\\.", "year": 2013}, {"title": "Random walks for image segmentation", "author": ["L. Grady"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Grady.,? \\Q2006\\E", "shortCiteRegEx": "Grady.", "year": 2006}, {"title": "TagProp: Discriminative metric learning in nearest neighbor models for image auto-annotation", "author": ["M. Guillaumin", "T. Mensink", "J. Verbeek", "C. Schmid"], "venue": "In Proc. 12th Int. Conf. Computer Vision (ICCV\u201909),", "citeRegEx": "Guillaumin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guillaumin et al\\.", "year": 2009}, {"title": "Constrained spectral clustering through affinity propagation", "author": ["Z. Lu", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "In Proc. of the 2008 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201908),", "citeRegEx": "Lu and Carreira.Perpi\u00f1\u00e1n.,? \\Q2008\\E", "shortCiteRegEx": "Lu and Carreira.Perpi\u00f1\u00e1n.", "year": 2008}, {"title": "Numerical Optimization. Springer Series in Operations Research and Financial Engineering", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "Nocedal and Wright.,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "Nearest-Neighbor Methods in Learning and Vision. Neural Information Processing Series", "author": ["G. Shakhnarovich", "P. Indyk", "T. Darrell", "editors"], "venue": null, "citeRegEx": "Shakhnarovich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shakhnarovich et al\\.", "year": 2006}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shi and Malik.,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik.", "year": 2000}, {"title": "Semi-supervised learning with measure propagation", "author": ["A. Subramanya", "J. Bilmes"], "venue": "J. Machine Learning Research,", "citeRegEx": "Subramanya and Bilmes.,? \\Q2011\\E", "shortCiteRegEx": "Subramanya and Bilmes.", "year": 2011}, {"title": "A signal processing approach to fair surface design", "author": ["G. Taubin"], "venue": "Proc. of the 22nd Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH", "citeRegEx": "Taubin.,? \\Q1995\\E", "shortCiteRegEx": "Taubin.", "year": 1995}, {"title": "Labeling images with a computer game", "author": ["L. von Ahn", "L. Dabbish"], "venue": "In Proc. ACM Int. Conf. Human Factors in Computing Systems (CHI", "citeRegEx": "Ahn and Dabbish.,? \\Q2004\\E", "shortCiteRegEx": "Ahn and Dabbish.", "year": 2004}, {"title": "Manifold blurring mean shift algorithms for manifold denoising", "author": ["W. Wang", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "In Proc. of the 2010 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Wang and Carreira.Perpi\u00f1\u00e1n.,? \\Q2010\\E", "shortCiteRegEx": "Wang and Carreira.Perpi\u00f1\u00e1n.", "year": 2010}, {"title": "The role of dimensionality reduction in classification", "author": ["W. Wang", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Proc. of the 28th National Conference on Artificial Intelligence (AAAI", "citeRegEx": "Wang and Carreira.Perpi\u00f1\u00e1n.,? \\Q2014\\E", "shortCiteRegEx": "Wang and Carreira.Perpi\u00f1\u00e1n.", "year": 2014}, {"title": "Laplacian K-modes clustering", "author": ["W. Wang", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Unpublished manuscript,", "citeRegEx": "Wang and Carreira.Perpi\u00f1\u00e1n.,? \\Q2014\\E", "shortCiteRegEx": "Wang and Carreira.Perpi\u00f1\u00e1n.", "year": 2014}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Zhou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2004}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "Proc. of the 20th Int. Conf. Machine Learning", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "A major success in machine learning in recent years has been the development of semisupervised learning (SSL) (Chapelle et al., 2006), where we are given labels for only a few of the training points.", "startOffset": 110, "endOffset": 133}, {"referenceID": 7, "context": "Laplacian-based formulations are conceptually simple, computationally efficient (since the Laplacian is usually sparse), have a solid foundation in graph theory and linear algebra (Chung, 1997; Doyle and Snell, 1984), and most importantly work very well in practice.", "startOffset": 180, "endOffset": 216}, {"referenceID": 8, "context": "Laplacian-based formulations are conceptually simple, computationally efficient (since the Laplacian is usually sparse), have a solid foundation in graph theory and linear algebra (Chung, 1997; Doyle and Snell, 1984), and most importantly work very well in practice.", "startOffset": 180, "endOffset": 216}, {"referenceID": 24, "context": "The graph Laplacian has been widely exploited in machine learning, computer vision and graphics, and other areas: as mentioned, in semisupervised learning, manifold regularization and graph priors (Zhu et al., 2003; Belkin et al., 2006; Zhou et al., 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 197, "endOffset": 255}, {"referenceID": 1, "context": "The graph Laplacian has been widely exploited in machine learning, computer vision and graphics, and other areas: as mentioned, in semisupervised learning, manifold regularization and graph priors (Zhu et al., 2003; Belkin et al., 2006; Zhou et al., 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 197, "endOffset": 255}, {"referenceID": 23, "context": "The graph Laplacian has been widely exploited in machine learning, computer vision and graphics, and other areas: as mentioned, in semisupervised learning, manifold regularization and graph priors (Zhu et al., 2003; Belkin et al., 2006; Zhou et al., 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 197, "endOffset": 255}, {"referenceID": 11, "context": ", 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 94, "endOffset": 107}, {"referenceID": 16, "context": ", 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 182, "endOffset": 203}, {"referenceID": 13, "context": ", 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 231, "endOffset": 263}, {"referenceID": 0, "context": ", 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 303, "endOffset": 328}, {"referenceID": 4, "context": ", 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 381, "endOffset": 413}, {"referenceID": 20, "context": ", 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 517, "endOffset": 590}, {"referenceID": 18, "context": ", 2004) for regression, classification and applications such as supervised image segmentation (Grady, 2006), where one solves a Laplacian-based linear system; in spectral clustering (Shi and Malik, 2000), possibly with constraints (Lu and Carreira-Perpi\u00f1\u00e1n, 2008), and spectral dimensionality reduction (Belkin and Niyogi, 2003) and probabilistic spectral dimensionality reduction (Carreira-Perpi\u00f1\u00e1n and Lu, 2007), where one uses eigenvectors of the Laplacian; in clustering, manifold denoising and surface smoothing (Carreira-Perpi\u00f1\u00e1n, 2006; Wang and Carreira-Perpi\u00f1\u00e1n, 2010; Taubin, 1995), where one iterates products of the data with the Laplacian; etc.", "startOffset": 517, "endOffset": 590}, {"referenceID": 5, "context": "A shorter version of this work appears in a conference paper (Carreira-Perpi\u00f1\u00e1n and Wang, 2014).", "startOffset": 61, "endOffset": 95}, {"referenceID": 14, "context": "For a QP, the KKT conditions are necessary and sufficient for a solution (Nocedal and Wright, 2006).", "startOffset": 73, "endOffset": 99}, {"referenceID": 2, "context": "1 QP solution using ADMM We briefly review how to solve a QP using the alternating direction method of multipliers (ADMM), following (Boyd et al., 2011).", "startOffset": 133, "endOffset": 152}, {"referenceID": 2, "context": "7 Optimal penalty parameter \u03c1 The speed at which ADMM converges depends on the quadratic penalty parameter \u03c1 (Boyd et al., 2011).", "startOffset": 109, "endOffset": 128}, {"referenceID": 2, "context": "7 Optimal penalty parameter \u03c1 The speed at which ADMM converges depends on the quadratic penalty parameter \u03c1 (Boyd et al., 2011). We illustrate this with the \u201c2 moons\u201d dataset in fig. 1 (N = 4 000 points, K = 2 categories, 5-nearestneighbor graph, \u03bb = 1), where we set positive similarity values for one point in each cluster, resulting in each cluster being assigned to a different category, as expected. The problem has 8 000 parameters and we ran 10 iterations, which took 11 s. Little work exists on how to select \u03c1 so as to achieve fastest convergence. Recently, for QPs, Ghadimi et al. (2013) suggest to use \u03c1 = 2\u03bb \u221a \u03c3min\u03c3max where \u03c3min and \u03c3max are the smallest (nonzero) and largest eigenvalue of the Laplacian.", "startOffset": 110, "endOffset": 599}, {"referenceID": 4, "context": "We follow the reasoning of Carreira-Perpi\u00f1\u00e1n and Lu (2007) to derive an outof-sample mapping.", "startOffset": 27, "endOffset": 59}, {"referenceID": 9, "context": "This can be efficiently computed, in a finite number of steps, with a simple O(K logK) algorithm (Duchi et al., 2008; Wang and Carreira-Perpi\u00f1\u00e1n, 2014a).", "startOffset": 97, "endOffset": 152}, {"referenceID": 15, "context": "With large N , one should use some form of hashing (Shakhnarovich et al., 2006) to retrieve approximate neighbors quickly.", "startOffset": 51, "endOffset": 79}, {"referenceID": 24, "context": "1 Semisupervised learning with a Laplacian penalty (SSL) In semisupervised learning (SSL) with a Laplacian penalty (Zhu et al., 2003), the basic idea is that we are given an affinity matrix W and corresponding graph Laplacian L = D\u2212W on N items, and the labels for a subset of the items.", "startOffset": 115, "endOffset": 133}, {"referenceID": 8, "context": "That Zu \u2265 0 follows from the maximum principle for harmonic functions (Doyle and Snell, 1984): each of the unknowns must lie between the minimum and maximum label values, i.", "startOffset": 70, "endOffset": 93}, {"referenceID": 24, "context": "if the given labels are not valid assignments, or in other widely used variations of SSL, such as using class mass normalization (Zhu et al., 2003), or using the normalized graph Laplacian instead of the unnormalized one, or using label penalties (Zhou et al.", "startOffset": 129, "endOffset": 147}, {"referenceID": 23, "context": ", 2003), or using the normalized graph Laplacian instead of the unnormalized one, or using label penalties (Zhou et al., 2004).", "startOffset": 107, "endOffset": 126}, {"referenceID": 24, "context": "In the latter case (also similar to the \u201cdongle\u201d variation of SSL; Zhu et al., 2003), one minimizes the Laplacian penalty plus a term equal to the squared distance of the labeled points (considered free parameters as well) to the labels Zl provided.", "startOffset": 19, "endOffset": 84}, {"referenceID": 11, "context": "as used for supervised clustering in Grady, 2006). However, in general SSL does not produce valid assignments, e.g. if the given labels are not valid assignments, or in other widely used variations of SSL, such as using class mass normalization (Zhu et al., 2003), or using the normalized graph Laplacian instead of the unnormalized one, or using label penalties (Zhou et al., 2004). In the latter case (also similar to the \u201cdongle\u201d variation of SSL; Zhu et al., 2003), one minimizes the Laplacian penalty plus a term equal to the squared distance of the labeled points (considered free parameters as well) to the labels Zl provided. Thus, this penalizes the labeled points from deviating from their intended labels, rather than forcing them to equal them. This was extended by Subramanya and Bilmes (2011) (replacing squared losses with Kullback-Leibler divergences and adding an additional entropy term) to learning probability distributions, i.", "startOffset": 37, "endOffset": 807}, {"referenceID": 14, "context": "For example, the Markowitz portfolio model (Nocedal and Wright, 2006) seeks the portfolio (soft assignment of an individual investor\u2019s $1 capital to K investments) that maximizes the expected return and minimizes the variance, and has the form of a QP.", "startOffset": 43, "endOffset": 69}, {"referenceID": 24, "context": "With SSL (Zhu et al., 2003), since the labels are used as constraints (eq.", "startOffset": 9, "endOffset": 27}, {"referenceID": 24, "context": "We compare with a nearest neighbor classifier (NN), one-vs-all kernel support vector machine (KSVM) using RBF kernel of width \u03c3 = 5 and hinge loss penalty parameter C selected from {10\u22123, 10, 10, 1, 10, 10, 103} and SSL (Zhu et al., 2003; Grady, 2006) using 1-out-of-10 coding for the labeled points.", "startOffset": 220, "endOffset": 251}, {"referenceID": 11, "context": "We compare with a nearest neighbor classifier (NN), one-vs-all kernel support vector machine (KSVM) using RBF kernel of width \u03c3 = 5 and hinge loss penalty parameter C selected from {10\u22123, 10, 10, 1, 10, 10, 103} and SSL (Zhu et al., 2003; Grady, 2006) using 1-out-of-10 coding for the labeled points.", "startOffset": 220, "endOffset": 251}, {"referenceID": 24, "context": "We also include two variants of SSL: SSL1 (Zhu et al., 2003) normalizes the assignments from SSL so that the prior distribution of the different classes is respected.", "startOffset": 42, "endOffset": 60}, {"referenceID": 23, "context": "SSL2 uses the normalized graph Laplacian instead of the unnormalized one in SSL (Zhou et al., 2004).", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "We demonstrate LASS on a subset of the ESP game (von Ahn and Dabbish, 2004) images used by Guillaumin et al. (2009). We select the images in the training set that are tagged with at least 6 categories (words), resulting in 6 100 images with a total of 267 non-empty categories, with 7.", "startOffset": 91, "endOffset": 116}, {"referenceID": 11, "context": "We demonstrate LASS on a subset of the ESP game (von Ahn and Dabbish, 2004) images used by Guillaumin et al. (2009). We select the images in the training set that are tagged with at least 6 categories (words), resulting in 6 100 images with a total of 267 non-empty categories, with 7.2 categories per image on average. We use the same image feature sets as Guillaumin et al. (2009) to compute distances between images and build a 10-nearest neighbor graph.", "startOffset": 91, "endOffset": 383}], "year": 2014, "abstractText": "We consider the problem of learning soft assignments of N items to K categories given two sources of information: an item-category similarity matrix, which encourages items to be assigned to categories they are similar to (and to not be assigned to categories they are dissimilar to), and an item-item similarity matrix, which encourages similar items to have similar assignments. We propose a simple quadratic programming model that captures this intuition. We give necessary conditions for its solution to be unique, define an out-of-sample mapping, and derive a simple, effective training algorithm based on the alternating direction method of multipliers. The model predicts reasonable assignments from even a few similarity values, and can be seen as a generalization of semisupervised learning. It is particularly useful when items naturally belong to multiple categories, as for example when annotating documents with keywords or pictures with tags, with partially tagged items, or when the categories have complex interrelations (e.g. hierarchical) that are unknown.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}