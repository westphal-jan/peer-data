{"id": "1406.1078", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2014", "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "histories": [["v1", "Tue, 3 Jun 2014 17:47:08 GMT  (875kb,D)", "http://arxiv.org/abs/1406.1078v1", null], ["v2", "Thu, 24 Jul 2014 20:07:13 GMT  (460kb,D)", "http://arxiv.org/abs/1406.1078v2", "EMNLP 2014"], ["v3", "Wed, 3 Sep 2014 00:25:02 GMT  (551kb,D)", "http://arxiv.org/abs/1406.1078v3", "EMNLP 2014"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE stat.ML", "authors": ["kyunghyun cho", "bart van merrienboer", "\u00e7aglar g\u00fcl\u00e7ehre", "dzmitry bahdanau", "fethi bougares", "holger schwenk", "yoshua bengio"], "accepted": true, "id": "1406.1078"}, "pdf": {"name": "1406.1078.pdf", "metadata": {"source": "CRF", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "authors": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "emails": ["kyunghyun.cho@umontreal.ca", "vanmerb@iro.umontreal.ca", "gulcehrc@iro.umontreal.ca", "firstname.lastname@lium.univ-lemans.fr", "find.me@on.the.web"], "sections": [{"heading": "1 Introduction", "text": "Recent years have shown that the number of people who are able to move, to move and to move, to move, to move, to move, to move and to move, to move, to move, to move, to move and to move, to move, to move, to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move, to move, to move and to move, to move, to move and to move."}, {"heading": "2 RNN Encoder\u2013Decoder", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Preliminary: Recurrent Neural Networks", "text": "A recursive neural network (RNN) is a neural network consisting of a hidden state h and an optional output y operating with a variable length sequence x = (x1,.., xT). At each step t, f can be as simple as an elementary logistic sigmoid function and as complex as a long-term time memory unit (LSTM) (Hochreiter and Schmidhuber, 1997). An RNN can learn a probability distribution over a sequence by training it to predict the next symbol in a sequence. In this case, the output at each time depth t isar Xiv: 140 6.10 78v1 [cs.CL] 3 Jun 201 4the conditional distribution p (xt, xt, \u2212 j = 1 = 1 x-j = 1 distribution) can be at each time depth t isar Xiv: 140 6.10 78v1 [cs.CL] 3 Jun 201 4the conditional distribution p (xt, xt, \u2212 j = 1 = 1 x-j = 1 distribution."}, {"heading": "2.2 RNN Encoder\u2013Decoder", "text": "In this paper, we propose a novel neural network architecture that learns to encode a variable sequence > > However, the proposed sequence is decoded into a fixed-length vector representation, and a given fixed-length vector representation is decoded back into a variable-length sequence. < From a probabilistic perspective, this new model is a general method of learning conditional distribution over a variable-length sequence (see also the proposed variable-length sequence, e.g. p (y1,., yT), bearing in mind that the input and output sequence lengths T and T \u2032 may be different. The encoder is an RNN that reads each symbol of a sequence sequence x sequentially. As it reads each symbol, the hidden state of the RNN changes correspondingly to the equation. (1) After reading the end of the model (marked by the end of the sequence), the model c is hidden."}, {"heading": "2.3 Hidden Unit that Adaptively Remembers and Forgets", "text": "In addition to a novel model architecture, we also propose a new type of hidden units (f > in Eq. (1)) that are motivated by the LSTM unit, but much easier to calculate and implement. 1 Fig. 2 shows the graphical representation of the proposed hidden unit. Let's describe how the activation of the j-th hidden unit is calculated. Firstly, the LSTM unit, which has shown impressive results in several applications, has a memory cell and four gating units that adaptively control the flow of information within the unit. For details on LSTM networks, see e.g. (Graves, 2012). Where the logistic sigmoid unit is, and [.] j denotes the j-th element of a vector. x and ht \u2212 1 are the previous hidden state, or the actual unit."}, {"heading": "3 Statistical Machine Translation", "text": "In a commonly used statistical machine translation system (SMT), the goal of the system (decoder, in particular) is to find a translation f, specifying a source sentence e that maximizes p (f | e), p (e | f) + log p (f), with the first term on the right being a translation model and the second a language model (see e.g. (Koehn, 2005). In practice, however, most SMT systems model p (f | e) as a log-linear model with additional properties and corresponding weights: log p (f | e).N = 1 wnfn (f, e), (9), with fn and wn being the nth characteristic and weight, respectively. Weights are often optimized to match the BLEU score based on a development set.New translations have been introduced in the sentence-based SMT model."}, {"heading": "3.1 Scoring Phrase Pairs with RNN Encoder\u2013Decoder", "text": "Here we propose to train the RNN encoder decoder (see Section 2.2) on a table of phrase pairs and to use its results as additional features in the log-linear model in Equation (9) when tuning the SMT decoder. When training the RNN encoder decoder, we ignore the (normalized) frequencies of each phrase pair in the original corpora. This measure was taken to (1) reduce the computational effort of the randomly selected phrase pairs from a large phrase table according to the normalized frequencies and (2) to ensure that the RNN encoder decoder decoder does not simply learn to classify the phrase pairs according to their number of occurrences. An underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequency of the phrase2 without losing the generality we refer to as a phrase translator problem for each pair of phrases | in the translation table."}, {"heading": "3.2 Related Approaches: Neural Networks in Machine Translation", "text": "Before presenting the empirical results, we will discuss a number of recent papers that have suggested neural networks in the context of SMT.Schwenk in (Schwenk, 2012) suggested a similar approach for scoring phrase pairs. Instead of the neural network RNNB, he used a feedback neural system that has a fixed size of inputs (7 words in his case, with zeros for shorter phrases) and fixed size of outputs (7 words in the target language). When used specifically for scoring phrases, the maximum phrase length is often chosen to be small. However, as the length of the phrases increases or as we apply neural networks to other variable lengths, it is important that the neural network can handle variable length input phrases for the SMT system."}, {"heading": "4 Experiments", "text": "We evaluate our approach to the English / French translation task of the WMT '14 workshop."}, {"heading": "4.1 Data and Baseline System", "text": "Large amounts of resources are available to set up an English / French SMT system in the context of the WMT '14 translation task. Bilingual corporas include Europarl (61M words), news commentary (5.5M), UN (421M) and two crawling corpora of 90M and 780M words respectively, the latter two corpora being quite noisy. To train the French language model, there are approximately 712M words from crawled newspaper material next to the target page of Bitexts. All word counts refer to French words after tokenization. It is generally accepted that the formation of statistical models on the concatenation of all this data does not necessarily result in optimal performance, and the results in extremely large models that are difficult to handle. Instead, one should focus on the most relevant subset of data for a particular task. We have done this by applying the data selection method proposed in (Moore, 2010), and its extension (Axelrod), and its 2011 (Exits)."}, {"heading": "4.1.1 RNN Encoder\u2013Decoder", "text": "The RNN encoder decoder used in the experiment had 1000 hidden units with the proposed gates on the encoder and the decoder. The input matrix between each input symbol x < t > and the hidden unit is approximated with two lower-level matrices, and the output matrix is approximated in a similar manner. We used RNN-100 matrices, which is equivalent to an embedding dimension of 100 for each word. The activation function used for h in equivalent (8) is a hyperbolic tangent function. Calculation from the hidden state in the decoder into the output is implemented as a deep neural network (Pascanu et al., 2014) with a single intermediate layer with 500 maxout units per input (Goodfellow et al., 2013).All weight parameters in the encoder decoder were sampled from an isotropic zero value (white mean) with multiple input lengths (Goodfellow), or Goodfellow (two)."}, {"heading": "4.1.2 Neural Language Model", "text": "To assess the effectiveness of the scoring phrase pairs with the proposed RNN encoder decoder, we also tried a more traditional approach of using a neural network to learn a target language model (CSLM, 2007). In particular, the comparison between the SMT system with CSLM and the proposed approach of scoring phrases from RNN encoder decoder decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT system add up or are redundant.We trained the CSLM model to 7-grams from the target corpus. Each input word was projected into the embedding space R512, and they were linked to form a 3072-dimensional vector. The concatenated vector was fed through two rectified layers (size 1536 and 1024) (Glorot et al al al al al al, 2011).The output layer was a simple EQ (see max layer)."}, {"heading": "4.2 Quantitative Analysis", "text": "We have tried the following combinations: 1. Baseline configuration 2. Baseline + CSLM 3. Baseline + RNN 4. Baseline + CSLM + RNN 5. Baseline + CSLM + RNN + Word penaltyThe results are presented in Table 1. as expected, add features computed by neural networks continuously improved the performance over the baseline performance (CSLM).This is a significant improvement considering that the additional computer complexity calculated by the proposed RNN encoder decoder decoder (RNN) is minimal when you do the decoder. The best performance was achieved when we used both CSLM and the phrase scores scores from the RNN encoder decoder decoder decoder. We suggest that the contributions of the CSLM and the RNN encoder decoder decoder decoder are minimal."}, {"heading": "4.3 Qualitative Analysis", "text": "To understand where the performance improvement comes from, we analyze the phrase pair calculated by the RNN encoder decoders against those p (f | e), the so-called inverse phrase translation probabilities from the translation model. Since the existing translation model relies exclusively on the statistics of the phrase pairs in the corpus, we expect the phrase pairs to be more responsive to the linguistic regularities than to the statistics of their occurrences in Sec. 3.1, we expect that the RNN encoder decoders collect the information generated without frequency to capture the phrase pairs that focus on the linguistic regularities rather than on the statistics of their operations in the corpus, etc. We focus on those pairs whose original formulation is long (more than 3 words per source phrase) and accumulate to obtain the phrase pairs."}, {"heading": "4.4 Word and Phrase Representations", "text": "Since the proposed RNN encoder decoder is not specifically designed for the task of machine translation only, we will briefly consider the characteristics of the trained model. It has been known for some time that continuous spatial language models using neural networks can learn semantically significant embedding (See e.g. (Bengio et al., 2003; Mikolov et al., 2013). Since the proposed RNN encoder decoder also shows the embedding of words learned by the RNN encoder decoder decoder in a continuous spatial vector projected and mapped, we expect a similar property with the proposed model. The left plot in Fig. 4 shows the 2-D embedding of words with the word embedding matrix learned by the encoder decoder decoder. The projection was performed by the recently proposed Barnes-HutSNE (van der Maaten, 2013)."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a new neural network architecture called an RNN encoder decoder, which is capable of learning the mapping from a sequence of arbitrary length to another sequence, possibly from a different sentence, of arbitrary length. The proposed RNN encoder decoder decoder is capable of either obtaining a sequence pair (interms of a conditional probability) or generating a target sequence that specifies a source sequence. Together with the new architecture, we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively controls how much each hidden unit remembers when reading / generating a sequence. We evaluated the proposed model with the task of statistical machine translation, in which we used the RNN encoder decoder decoration to evaluate each phrase pair in the phrase table table. Qualitatively, we were able to show that the new model is good at grasping the laws."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the support of the following research funding and computer support agencies: NSERC, Calcul Que'bec, Compute Canada, the Canada Research Chairs and CIFAR."}, {"heading": "A RNN Encoder\u2013Decoder", "text": "This document describes in detail the architecture of the RNN Encoder Decoder used in the experiments & < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "B Word and Phrase Representations", "text": "Here we show enlarged diagrams of the word and phrase representations in Fig. 4-5. Figure 6: 2-D embedding of the learned word representation. The upper left illustration shows the complete embedding space, while the other three illustrations show the zoomed-in view of certain regions (color coded). Figure 7: 2-D embedding of the learned phrase representation. The upper left illustration shows the complete display space (5000 randomly selected points), while the other three illustrations show the zoomed-in view of certain regions (color coded)."}], "references": [{"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Xiaodong He", "Jianfeng Gao"], "venue": "In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Axelrod et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Advances in optimizing recurrent networks", "author": ["Bengio et al.2013] Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "In Proceedings of the 38th International Conference on Acoustics,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha"], "venue": null, "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition", "author": ["Dahl et al.2012] George E. Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Deep sparse rectifier neural networks. In AISTATS", "author": ["Glorot et al.2011] X. Glorot", "A. Bordes", "Y. Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks", "author": ["Alex Graves"], "venue": "Studies in Computational Intelligence. Springer", "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Two recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "In Machine Translation Summit X,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A phrase-based, joint probability model for statistical machine translation", "author": ["Marcu", "Wong2002] Daniel Marcu", "William Wong"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10,", "citeRegEx": "Marcu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Marcu et al\\.", "year": 2002}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Intelligent selection of language model training data", "author": ["Moore", "Lewis2010] Robert C. Moore", "William Lewis"], "venue": "In Proceedings of the ACL 2010 Conference Short Papers,", "citeRegEx": "Moore et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2010}, {"title": "How to construct deep recurrent neural networks", "author": ["Pascanu et al.2014] R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of the Second International Conference on Learning Representations (ICLR", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe et al.2014] Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "venue": "In Proceedings of the Second International Conference on Learning Representations (ICLR", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Continuous space language models for the iwslt 2006", "author": ["Marta R. Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa"], "venue": null, "citeRegEx": "Schwenk et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2006}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Comput. Speech Lang.,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Continuous space translation models with neural networks", "author": ["Son et al.2012] Le Hai Son", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Son et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Son et al\\.", "year": 2012}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "Technical report,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel M. Cer", "Christopher D. Manning"], "venue": "In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": ", (Krizhevsky et al., 2012)) and speech recognition (see, e.", "startOffset": 2, "endOffset": 27}, {"referenceID": 6, "context": ", (Dahl et al., 2012)).", "startOffset": 2, "endOffset": 21}, {"referenceID": 21, "context": "These include, but are not limited to, paraphrase detection (Socher et al., 2011), word embedding extraction (Mikolov et al.", "startOffset": 60, "endOffset": 81}, {"referenceID": 15, "context": ", 2011), word embedding extraction (Mikolov et al., 2013) and language modeling (Bengio et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 2, "context": ", 2013) and language modeling (Bengio et al., 2003).", "startOffset": 30, "endOffset": 51}, {"referenceID": 8, "context": ", (Graves, 2012).", "startOffset": 2, "endOffset": 16}, {"referenceID": 3, "context": "Furthermore, this may be considered an adaptive variant of a leaky-integration unit (Bengio et al., 2013).", "startOffset": 84, "endOffset": 105}, {"referenceID": 12, "context": ", (Koehn, 2005)).", "startOffset": 2, "endOffset": 15}, {"referenceID": 11, "context": "In the phrase-based SMT framework introduced in (Koehn et al., 2003) and (Marcu and Wong, 2002), the translation model log p(e | f) is factorized into the translation probabilities of matching phrases in the source and target sentences.", "startOffset": 48, "endOffset": 68}, {"referenceID": 2, "context": "Since the neural net language model was proposed in (Bengio et al., 2003), neural networks have been used widely in SMT systems.", "startOffset": 52, "endOffset": 73}, {"referenceID": 19, "context": ", (Schwenk et al., 2006)).", "startOffset": 2, "endOffset": 24}, {"referenceID": 22, "context": ", (Schwenk, 2012), (Son et al., 2012) and (Zou et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 25, "context": ", 2012) and (Zou et al., 2013).", "startOffset": 12, "endOffset": 30}, {"referenceID": 25, "context": "Although it is not exactly a neural network they train, the authors of (Zou et al., 2013) proposed to learn a bilingual embedding of words/phrases.", "startOffset": 71, "endOffset": 89}, {"referenceID": 5, "context": "In (Chandar et al., 2014), a feedforward neural network was trained to learn a mapping from a bag-of-words representation of an input phrase to an output phrase.", "startOffset": 3, "endOffset": 25}, {"referenceID": 21, "context": "Earlier, a similar encoder\u2013decoder model using two recursive neural networks was proposed in (Socher et al., 2011), but their model was restricted to a monolingual setting, i.", "startOffset": 93, "endOffset": 114}, {"referenceID": 25, "context": "One important difference between the proposed RNN Encoder\u2013Decoder and the approaches in (Zou et al., 2013) and (Chandar et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 5, "context": ", 2013) and (Chandar et al., 2014) is that the order of the words in source and target phrases is taken into account.", "startOffset": 12, "endOffset": 34}, {"referenceID": 0, "context": "We have done so by applying the data selection method proposed in (Moore and Lewis, 2010), and its extension to bitexts (Axelrod et al., 2011).", "startOffset": 120, "endOffset": 142}, {"referenceID": 17, "context": "The computation from the hidden state in the decoder to the output is implemented as a deep neural network (Pascanu et al., 2014) with a single intermediate layer having 500 maxout units each", "startOffset": 107, "endOffset": 129}, {"referenceID": 18, "context": "01), following (Saxe et al., 2014).", "startOffset": 15, "endOffset": 34}, {"referenceID": 24, "context": "95 (Zeiler, 2012).", "startOffset": 3, "endOffset": 17}, {"referenceID": 20, "context": "In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder\u2013Decoder, we also tried a more traditional approach of using a neural network for learning a target language model (CSLM) (Schwenk, 2007).", "startOffset": 212, "endOffset": 227}, {"referenceID": 7, "context": "The concatenated vector was fed through two rectified layers (of size 1536 and 1024) (Glorot et al., 2011).", "startOffset": 85, "endOffset": 106}, {"referenceID": 23, "context": "The model was used to score partial translations during the decoding process, which generally leads to higher gains in BLEU score than n-best list rescoring (Vaswani et al., 2013).", "startOffset": 157, "endOffset": 179}, {"referenceID": 4, "context": "This allows us to perform fast matrix-matrix multiplication on GPU using Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 80, "endOffset": 125}, {"referenceID": 1, "context": "This allows us to perform fast matrix-matrix multiplication on GPU using Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 80, "endOffset": 125}, {"referenceID": 20, "context": "For CSLMs this shortcoming can be addressed by using a separate back-off n-gram language model that only contains non-shortlisted words (see (Schwenk, 2007)).", "startOffset": 141, "endOffset": 156}, {"referenceID": 2, "context": ", (Bengio et al., 2003; Mikolov et al., 2013)).", "startOffset": 2, "endOffset": 45}, {"referenceID": 15, "context": ", (Bengio et al., 2003; Mikolov et al., 2013)).", "startOffset": 2, "endOffset": 45}], "year": 2014, "abstractText": "In this paper, we propose a novel neural network model called RNN Encoder\u2013Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2013Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "creator": "LaTeX with hyperref package"}}}