{"id": "1406.1222", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2014", "title": "Discovering Structure in High-Dimensional Data Through Correlation Explanation", "abstract": "We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively, the optimization searches for the simplest set of factors that can explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests, DNA, and human language.", "histories": [["v1", "Wed, 4 Jun 2014 21:46:30 GMT  (7891kb,D)", "https://arxiv.org/abs/1406.1222v1", "15 pages, 6 figures"], ["v2", "Fri, 31 Oct 2014 02:43:28 GMT  (7088kb,D)", "http://arxiv.org/abs/1406.1222v2", "15 pages, 6 figures. Includes supplementary material and link to code. Published in the proceedings of the 28th Annual Conference on Neural Information Processing Systems, NIPS 2014"]], "COMMENTS": "15 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["greg ver steeg", "aram galstyan"], "accepted": true, "id": "1406.1222"}, "pdf": {"name": "1406.1222.pdf", "metadata": {"source": "CRF", "title": "Discovering Structure in High-Dimensional Data Through Correlation Explanation", "authors": ["Greg Ver Steeg", "Aram Galstyan"], "emails": ["gregv@isi.edu", "galstyan@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "If the variables are uncorrelated, then the system is not really high-dimensional, but should be considered a collection of unrelated universal systems. However, if correlations exist, then common causes or causes must be responsible for their generation. Without assuming a specific model for these hidden common causes, is it still possible to reconstruct them? We propose an information theory principle, which we call \"correlation explanation,\" which codifies this problem in a model-free, mathematically principle-driven way. Essentially, we are looking for latent factors, so that, due to these factors, the correlations in the data are minimized (measured by multivariate mutual information). In other words, we are looking for the simplest explanation that takes into account most of the correlations in the data. As a bonus, we show that sec-based fundamentals naturally lead to an innovative paradigm for learning hierarchical representations that are more comprehensible than Bayesian structures that we explain with most of the data correlations. \""}, {"heading": "2 Correlation Explanation", "text": "Using the standard definition [2], capital X denotes a discrete random variable whose instances are written in lowercase letters (TC = 1 TC). A probability distribution over a random variable X, pX = x, is summarized as follows: XX 6.12 22v2 [cs.LG] 3 1O ctto p (x), unless there is an ambiguity of the set of values a random variable can assume, is always limited and denoted by X. If we have n random variables, then G is a subset of indices G'Nn = {1,., n} and XG is the corresponding subset of random variables (XNn), which is shortened to X. Entropy is usually defined as H (X) [\u2212 log p (x)]. Higherorder entropies can be constructed in various ways from this standard definition."}, {"heading": "3 CorEx: Efficient Implementation of Correlation Explanation", "text": "We start by optimizing in Eq. 4 in relation to mutual information using Eq. 2.max G, p (yj) m (yj) m (yj) m (yj) m (yj) m (yj) p (yj) p (yj) p (yj) p (yj) p (yj) p (yj) p (yj) p (yj) p (yj) p (yj) m (yj) m (yj) p (yj) p (yj) p (yj) p (yj) p (jp) p (j) p (j) p (j) p (j) jp (j) p (j) jp (jp) (j) p (jp) (jp) (jp) (jp) (jp) (jp) p (jp) (jp) p (p) p) p (jp) p (jp) p (jp) p (jp) p (jp) p (jp) p (jp) p (jp) p) p (jp) p (jp) p (jp) p (jp) p) p (jp) p (jp) p (jp) p) p (jp) p (p) p) p (p) p) p (p) p) p (p) p) p) p (p) p) p (p) p) p (p) p) p (p) p) p (p) p) p) (p) (p) (p) p) p) (p) (p) (p) (p) p) (p) (p) (p) (p) (p) p) (p) (p) (p) (p) p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) p) (p"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Synthetic data", "text": "To test CorEx's ability to recover latent structure from data, we begin by generating synthetic data according to the latent tree model shown in Fig. 1, in which all variables except the leaf nodes are hidden, and the most difficult part of reconstructing this tree is the cluster technique. If a cluster method can do this, the latent variables for each cluster can be easily reconstructed by restoring the basic truth. We look at many different cluster methods, typically with multiple variations on each technique, the details of which are described in seconds. We use the Adjusted Edge Index (ARI) to measure the accuracy with which the inferred cluster restores the basic truth. 3We generated samples from the model in Fig. = 8 and varied c, the number of leaves per branch. The Xi's depend on Yj's through a binary erasure channel (BEC) with the probability."}, {"heading": "4.2 Discovering Structure in Diverse Real-World Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Personality Surveys and the \u201cBig Five\u201d Personality Traits", "text": "Psychological theory suggests that there are five characteristics that largely reflect differences in personality types: extraversion, neuroticism, acceptance, conscientiousness, and openness to experience. Psychologists have developed various tools aimed at measuring whether individuals exhibit these characteristics. We look at a survey in which subjects rate fifty statements, such as, \"I am the life of the party,\" on a five-point scale: (1) disagreement, (3) neutral, (4) slightly agree, and (5) agree. Data consists of answers to these questions from a ten3Rand index that ranks the proportion of couples whose relative classification matches in both clusters. ARI adds a correction so that random clustering results in a score of zero, while an ARI of 1 corresponds to perfect matching. 4A video is available online at http: / isi.edu / corp.mp.list of structure.5 and a full list of data are available."}, {"heading": "4.2.2 DNA from the Human Genome Diversity Project", "text": "Next, we look at DNA data from 952 individuals of different geographical and ethnic origin [13]. The data consists of 4170 variables describing different SNPs (Single Nucleotide Polymorphisms).6 We use CorEx to learn a hierarchical representation, shown in Figure 3. To evaluate the quality of the representation, we use the Adjusted Rand Index (ARI) to compare clusters induced by each latent variable in the hierarchical representation with various demographic variables in the data. Latent variables, which essentially correspond to demographic variables, are shown in Figure 3. The representation learned on the first layer (unattended) contains perfect correspondence with Oceania (the Pacific Islands) and near-perfect matches with America (Native Americans), Sub-Saharan Africa and gender. The second layer has three variables that correspond very closely to wide geographical regions: Sub-Saharan Africa, East Asia, Asia (including Asia), and Asia."}, {"heading": "4.2.3 Text from the Twenty Newsgroups Dataset", "text": "Most of them are able to play by the rules they have imposed on themselves."}, {"heading": "5 Connections and Related Work", "text": "While the basic metrics used in Eq. 1 and Eq. 2 appear in several contexts [7, 17, 4, 3, 18], the interpretation of these metrics is an active area of research [19, 20]. The optimizations we7An interactive tool for exploring the complete hierarchy is available at http: / / bit.ly / corexvis.define some interesting but less obvious links. For example, the optimization in Eq. 3 is comparable to a recently introduced metric of \"common information\" [21]. The goal in Eq. 6 (for a single Yj) appears exactly like a binding to \"ancestral\" information [22]."}, {"heading": "6 Conclusion", "text": "The most difficult open problems today relate to high-dimensional data from various sources, including human behavior, language, and biology.8 The complexity of the underlying systems makes modeling difficult. We have demonstrated a model-free approach to successfully learning coarser-grained representations of complex data by efficiently optimizing an information-theoretical goal. The principle of explaining as much correlation in the data as possible provides an intuitive and fully data-driven method of discovering previously inaccessible structures in high-dimensional systems. It may come as a surprise that CorEx should fully recreate the structure in various areas without using labeled data or prior knowledge. On the other hand, the patterns found from the right point of view are \"low-hanging fruits.\" Intelligent systems should be able to learn robust and general patterns even when there are no labels to define what is important. Information that is highly redundant in high-dimensional data provides a good starting point."}, {"heading": "Acknowledgments", "text": "We thank Virgil Griffith, Shuyang Gao, Hsuan-Yi Chu, Shirley Pepke, Bilal Shaw, Jose-Luis Ambite, and Nathan Hodas for their helpful conversations. This research was supported in part by the AFOSR grant FA9550-12-1-0417 and the DARPA grant W911NF-12-1-0034."}, {"heading": "A Derivation of Eqs. 7 and 8", "text": "We want to optimize the following objectification: max \u03b1, p (yj | x) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (sample) x (x) x (sample) x (x) x (sample) x (x) x (sample) x (sample) x (x) x (sample) x (x) x (sample) x (sample) x (x) x (sample) x (x) x (sample) x (x) x (sample) x (x) x (x) x (sample) x (x) x (x) x) x (x) x (x) x) x (x) x (x) x (x) x) x (x) x (x) x (x) x (x) x (x) x) x (x) x (x) x (x) x (x) x (x) x (x) x) x (x) x (x) x (x) x (x) x (x) x (x) x) x (x) x) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log) x (log"}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>We introduce a method to learn a hierarchy of successively more abstract repre-<lb>sentations of complex data based on optimizing an information-theoretic objec-<lb>tive. Intuitively, the optimization searches for a set of latent factors that best ex-<lb>plain the correlations in the data as measured by multivariate mutual information.<lb>The method is unsupervised, requires no model assumptions, and scales linearly<lb>with the number of variables which makes it an attractive approach for very high<lb>dimensional systems. We demonstrate that Correlation Explanation (CorEx) auto-<lb>matically discovers meaningful structure for data from diverse sources including<lb>personality tests, DNA, and human language.<lb>", "creator": "LaTeX with hyperref package"}}}