{"id": "1406.1305", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2014", "title": "Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets", "abstract": "The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections - the computational bottleneck in many applications - replacing it by a linear optimization step. Despite this advantage, the convergence rates of the FW method fall behind standard gradient methods for most settings of interest. It is an active line of research to derive faster FW algorithms for various settings of convex optimization.", "histories": [["v1", "Thu, 5 Jun 2014 09:25:22 GMT  (59kb,D)", "https://arxiv.org/abs/1406.1305v1", null], ["v2", "Fri, 14 Aug 2015 18:15:14 GMT  (69kb,D)", "http://arxiv.org/abs/1406.1305v2", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG", "authors": ["dan garber", "elad hazan"], "accepted": true, "id": "1406.1305"}, "pdf": {"name": "1406.1305.pdf", "metadata": {"source": "META", "title": "Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets", "authors": ["Dan Garber", "Elad Hazan"], "emails": ["DANGAR@TX.TECHNION.AC.IL", "EHAZAN@CS.PRINCETON.EDU"], "sections": [{"heading": null, "text": "In this paper, we look at the specific case of strongly convex set optimization, where we demonstrate that the vanilla-FW method converges at a convergence rate of 1t2, resulting in a quadratic improvement in the convergence rate compared to the general case where convergence is in the order of 1t and is notoriously narrow. We show that different spheres induced by \"p standards, shadow norms and group norms are strongly convex on the one hand, and linear optimization over these sets is simple and allows for a closed solution on the other."}, {"heading": "1. Introduction", "text": "The method introduced by Frank and Wolfe in the 1950s is a first-order method for minimizing a smooth convex function in a convex range. Its main advantage is that it is a first-class and projecting method - i.e. that the algorithm has a linear optimization problem and remains within the convex range."}, {"heading": "1.1. Related Work", "text": "The Frank Wolfe Method goes back to the original work of Frank and Wolfe (Frank & Wolfe, 1956), which is an algorithm for minimizing a square function over a polytopic with only linear optimization steps beyond the achievable requirement. Recent results from Clarkson (Clarkson, 2008), Hazan (Hazan, 2008) and Jaggi (Jaggi, 2013) show that the convergence rate of the method is in the order of 1 / 2 million euros and that it generally cannot be improved, even though the objective function is strongly convex and arbitrarily convex. It has been shown in numerous papers that the convergence rate of the method is in the order of 1 / 2 billion euros and that it is strongly convex as shown (Clarkson, 2008; Jaggi, 2013), although it is known that the projected method achieves an exponentially fast convergence convergence method."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Smoothness and Strong Convexity", "text": "For the following definitions, we let E be a finite vector space, and we assume that a pair of double norms exceeds E. Definition 1 (smooth function). We say that a function f: E \u2212 R is contained above a convex set K \u2212 E in relation to a strongly convex function. We say that a function f: E \u2192 R above a convex set K \u00b2 in relation to a convex set K \u00b2 in relation to a convex set K \u00b2 in relation to a convex set K \u00b2 in relation to a convex set K \u00b2 in relation to a convex set K \u00b2 in relation to a convex set K \u00b2 in relation to the following two equivalent conditions 1."}, {"heading": "2.2. The Frank-Wolfe Algorithm", "text": "The Frank Wolfe algorithm, also known as the conditional gradient algorithm, is an algorithm for minimizing a convex function f: E \u00b7 R, which is assumed to be smooth relative to a standard \u03b2f \u2212, via a convex and compact set K \u00b7 E. The algorithm implicitly assumes that the convex set K is given relative to a linear optimization oracle OK: E \u00b7 K, which gives a linear objective c \u00b7 E a point x = OK (c). The algorithm implicitly assumes that the convex set K y \u00b7 c is given. The algorithm proceeds in iterations and records each iteration, with the new iteration xt xt xt + 1 being a convex combination between the previous executable iterate xt and a executable point called K y \u00b7 c."}, {"heading": "2.3. Our Results", "text": "In this paper, we consider the case in which the optimization function of f is not only in relation to the control control control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control over the control of the control over the control over the control over the control of"}, {"heading": "3. Proof of Theorem 2", "text": "We call the approximation error of the iteration xt generated by the algorithm a big problem. This is ht = f (xt) \u2212 f (x), where x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x), x (x), x (x), x (x), x (x), x (x), x), x (x), x (x)."}, {"heading": "4. Derivation of Previous Fast Rates Results and Extensions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Deriving the Linear Rate of Polayk & Levitin", "text": "Polyak & Levitin considered in (Levitin & Polyak, 1966) the case where the realizable quantity is strongly convex, the objective function is smooth and there is a constant g > 0, so that the algorithm 1 converges at a linear rate, i.e. with e \u2212 \u044b (t), assuming a lower gradient. By including equivalents (9) in Lemma 1, we clearly have tht + 1 \u2264 ht \u00b7 max {1 2, 1 \u2212 \u03b1kg 8\u00dff}. resulting in the same exponentially fast convergence rate as in (Levitin & Polyak, 1966) and subsequent works such as (Demyanov & Rubinov, 1970; Dunn, 1979)."}, {"heading": "4.2. Deriving a Linear Rate for Arbitrary Convex Sets in case x\u2217 is in the Interior of the Set", "text": "Let us now assume that the realizable setK is convex, but not necessarily strongly convex. We assume that the objective function f smooth, convex, Eq. (2) fulfills a certain constant \u03b1f and permits a minimizer (not necessarily unique) x \u00b2, which is inside K, i.e. there is a parameter r > 0, so that the sphere with the radius r is fully contained in K with respect to the norm \u0430 \u2212 centered on x \u00b2. Gue \u00b7 Lat and Marcotte (Gue'Lat & Marcotte, 1986) showed that the Frank Wolfe algorithm converges at a linear rate. We now show how a slight change in the proof for Lemma 1 leads to this linear convergence result. Let us not behave as in the proof for Lemma 1, that we can define the optimal space with x."}, {"heading": "4.3. Relaxing the Strong Convexity of f", "text": "So far, we have considered the case where the objective function f is strongly convex. > > Note, however, that our main instrument for demonstrating the accelerated convergence rate, i.e. Lemma 1, is not directly dependent on the strong convexity of f, but on the size of the gradient. (2) We have seen in Eq. (2) that if f is strongly convex than the gradient is at least on the order of p \u2212 ht. We now show that there are functions that are not strongly convex, but nevertheless Eq. (2) and therefore our results apply to them. Let us consider the function (x) = 12% Ax \u2212 b \u2012 22.where x-Rn, A-Rm \u00b7 n, b-Rm. Let us assume that m < n and all rows of A are independent. (In this case, the optimization problem minx-K f (x) is the problem of convergence of K, which is a subordinate problem that best satisfies it."}, {"heading": "5. Examples of Strongly Convex Sets", "text": "In this section, we will examine convex sets for which Theorem 2 is applicable, that is, convex sets that are strongly convex on the one hand, and on the other allow an easy and efficient implementation of a linear optimization oracle on the other. We show that different standards that induce natural regulation functions in machine learning induce convex sets that meet both of the above requirements. A summary of our results can be found in Table 5. We find that in all cases where the standard parameter p is less than 2 (or one of the parameters s, p in the case of group norms), we are not aware of a practical algorithm for calculating the projection."}, {"heading": "5.1. Partial Characterization of Strongly Convex Sets", "text": "The following problem is that the function f (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x) x (x) x (x) x (x) x x (x) x) x x (x) x (x) x) x (x) x (x) x (x) x (x) x) x x x (x) x x x (x) x) x x (x) x) x x (x) x x) x (x) x (x) x) x (x) x x x (x) x) x x (x) x (x) x x (x) x) x x x x x x (x) x (x) x) x x x x (x) x (x) x) x x x (x) x x x x x (x) x x (x) x) x x x x (x) x x x x x x x (x) x x x x x x (x) x x x x x x x x x (x) x) x x x x x x (x) x x x x x x x x x x x x x x x x x (x) x x x x x x x x x x x x x x x x (x) x x x x x x x x x x x x x x (x) x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "6. Conclusions and Open Problems", "text": "In this paper, we have demonstrated that the Frank Wolfe algorithm converges at an accelerated rate of O (1 / t2) for smooth and strongly convex optimization compared to strongly convex sets, exceeding the known narrow convergence rate of the method for general smooth and convex optimization, which is one of the few known results to achieve such an improvement in convergence rate under natural and standard assumptions (i.e. strong convexity, etc.) We have also shown that various regulation functions in machine learning lead to strongly convex sets. We have also shown how earlier rapid convergence rates easily follow from our analyses. Of course, the following questions arise.It is well known that in the case of a smooth and strongly convex objective function, projection / projection methods achieve a convergence rate of O (log (1 /))."}, {"heading": "Acknowledgments", "text": "The research leading to these results was funded by the Seventh Framework Programme of the European Union (FP7 / 2007-2013) under Funding Agreement 336078 - ERCSUBLRN."}, {"heading": "A. Proof of Theorem 1", "text": "Proof. Fix an iteration t. By the \u03b2f-glassiness of f we have thatht + 1 = f (xt + \u03b7t (pt \u2212 xt)) \u2212 f (x *) \u2264 f (xt) \u2212 f (x *) + \u03b7t (pt \u2212 xt) \u00b7 \u0445 f (xt) + \u03b72t \u03b2f2 = f (pt \u2212 xt) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}, {"heading": "B. Proofs of Lemmas and Corollaries from", "text": "Section 5B.1. Proof of Lemma 3Proof. It is enough to show that the given x, y, q, q, q, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}], "references": [{"title": "Linear convergence of a modified frank-wolfe algorithm for computing minimum-volume enclosing ellipsoids", "author": ["References Ahipasaoglu", "S. Damla", "Sun", "Peng", "Todd", "Michael J"], "venue": "Optimization Methods and Software,", "citeRegEx": "Ahipasaoglu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ahipasaoglu et al\\.", "year": 2008}, {"title": "A conditional gradient method with linear rate of convergence for solving convex linear systems", "author": ["Beck", "Amir", "Teboulle", "Marc"], "venue": "Math. Meth. of OR,", "citeRegEx": "Beck et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2004}, {"title": "Coresets, sparse greedy approximation, and the frank-wolfe algorithm", "author": ["Clarkson", "Kenneth L"], "venue": "In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Clarkson and L.,? \\Q2008\\E", "shortCiteRegEx": "Clarkson and L.", "year": 2008}, {"title": "Approximate methods in optimization problems", "author": ["Demyanov", "Vladimir F", "Rubinov", "Aleksandr M"], "venue": null, "citeRegEx": "Demyanov et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Demyanov et al\\.", "year": 1970}, {"title": "Lifted coordinate descent for learning with trace-norm regularization", "author": ["Dud\u0131\u0301k", "Miroslav", "Harchaoui", "Za\u0131\u0308d", "Malick", "J\u00e9r\u00f4me"], "venue": "Journal of Machine Learning Research Proceedings Track,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2012}, {"title": "Rates of Convergence for Conditional Gradient Algorithms Near Singular and Nonsingular Extremals", "author": ["Dunn", "Joseph C"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Dunn and C.,? \\Q1979\\E", "shortCiteRegEx": "Dunn and C.", "year": 1979}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "Frank and Wolfe,? \\Q1956\\E", "shortCiteRegEx": "Frank and Wolfe", "year": 1956}, {"title": "Playing non-linear games with linear oracles", "author": ["Garber", "Dan", "Hazan", "Elad"], "venue": "In 54th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Garber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Garber et al\\.", "year": 2013}, {"title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "author": ["Garber", "Dan", "Hazan", "Elad"], "venue": "CoRR, abs/1301.4666,", "citeRegEx": "Garber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Garber et al\\.", "year": 2013}, {"title": "Some comments on Wolfe\u2019s \u2018away step", "author": ["Gu\u00e9Lat", "Jacques", "Marcotte", "Patrice"], "venue": "Mathematical Programming,", "citeRegEx": "Gu\u00e9Lat et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Gu\u00e9Lat et al\\.", "year": 1986}, {"title": "Large-scale image classification with trace-norm regularization", "author": ["Harchaoui", "Za\u0131\u0308d", "Douze", "Matthijs", "Paulin", "Mattis", "Dud\u0131\u0301k", "Miroslav", "Malick", "J\u00e9r\u00f4me"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Harchaoui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Harchaoui et al\\.", "year": 2012}, {"title": "Sparse approximate solutions to semidefinite programs", "author": ["Hazan", "Elad"], "venue": "In 8th Latin American Theoretical Informatics Symposium, LATIN,", "citeRegEx": "Hazan and Elad.,? \\Q2008\\E", "shortCiteRegEx": "Hazan and Elad.", "year": 2008}, {"title": "Projection-free online learning", "author": ["Hazan", "Elad", "Kale", "Satyen"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["Jaggi", "Martin"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Jaggi and Martin.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi and Martin.", "year": 2013}, {"title": "A simple algorithm for nuclear norm regularized problems", "author": ["Jaggi", "Martin", "Sulovsk\u00fd", "Marek"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Jaggi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2010}, {"title": "Generalized power method for sparse principal component analysis", "author": ["Journ\u00e9e", "Michel", "Nesterov", "Yurii", "Richt\u00e1rik", "Peter", "Sepulchre", "Rodolphe"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "Regularization techniques for learning with matrices", "author": ["Kakade", "Sham M", "Shalev-Shwartz", "Shai", "Tewari", "Ambuj"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kakade et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2012}, {"title": "An affine invariant linear convergence analysis for frank-wolfe algorithms", "author": ["Lacoste-Julien", "Simon", "Jaggi", "Martin"], "venue": "CoRR, abs/1312.7864,", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2013}, {"title": "Block-coordinate frank-wolfe optimization for structural svms", "author": ["Lacoste-Julien", "Simon", "Jaggi", "Martin", "Schmidt", "Mark W", "Pletscher", "Patrick"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2013}, {"title": "The complexity of large-scale convex programming under a linear optimization oracle", "author": ["Lan", "Guanghui"], "venue": "CoRR, abs/1309.5550,", "citeRegEx": "Lan and Guanghui.,? \\Q2013\\E", "shortCiteRegEx": "Lan and Guanghui.", "year": 2013}, {"title": "A hybrid algorithm for convex semidefinite optimization", "author": ["Laue", "S\u00f6ren"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Laue and S\u00f6ren.,? \\Q2012\\E", "shortCiteRegEx": "Laue and S\u00f6ren.", "year": 2012}, {"title": "Constrained minimization methods", "author": ["Levitin", "Evgeny S", "Polyak", "Boris T"], "venue": "USSR Computational mathematics and mathematical physics,", "citeRegEx": "Levitin et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Levitin et al\\.", "year": 1966}, {"title": "A regularization of the frankwolfe method and unification of certain nonlinear programming methods", "author": ["Migdalas", "Athanasios"], "venue": "Mathematical Programming,", "citeRegEx": "Migdalas and Athanasios.,? \\Q1994\\E", "shortCiteRegEx": "Migdalas and Athanasios.", "year": 1994}, {"title": "Large-scale convex minimization with a low-rank constraint", "author": ["Shalev-Shwartz", "Shai", "Gonen", "Alon", "Shamir", "Ohad"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "For matrix completion problems, metric learning, sparse PCA, structural SVM and other large-scale machine learning problems, this feature enabled the derivation of algorithms that are practical on one hand and come with provable convergence rates on the other (Jaggi & Sulovsk\u00fd, 2010; Lacoste-Julien et al., 2013; Dud\u0131\u0301k et al., 2012; Harchaoui et al., 2012; Hazan & Kale, 2012; Shalev-Shwartz et al., 2011; Laue, 2012).", "startOffset": 260, "endOffset": 419}, {"referenceID": 4, "context": "For matrix completion problems, metric learning, sparse PCA, structural SVM and other large-scale machine learning problems, this feature enabled the derivation of algorithms that are practical on one hand and come with provable convergence rates on the other (Jaggi & Sulovsk\u00fd, 2010; Lacoste-Julien et al., 2013; Dud\u0131\u0301k et al., 2012; Harchaoui et al., 2012; Hazan & Kale, 2012; Shalev-Shwartz et al., 2011; Laue, 2012).", "startOffset": 260, "endOffset": 419}, {"referenceID": 10, "context": "For matrix completion problems, metric learning, sparse PCA, structural SVM and other large-scale machine learning problems, this feature enabled the derivation of algorithms that are practical on one hand and come with provable convergence rates on the other (Jaggi & Sulovsk\u00fd, 2010; Lacoste-Julien et al., 2013; Dud\u0131\u0301k et al., 2012; Harchaoui et al., 2012; Hazan & Kale, 2012; Shalev-Shwartz et al., 2011; Laue, 2012).", "startOffset": 260, "endOffset": 419}, {"referenceID": 23, "context": "For matrix completion problems, metric learning, sparse PCA, structural SVM and other large-scale machine learning problems, this feature enabled the derivation of algorithms that are practical on one hand and come with provable convergence rates on the other (Jaggi & Sulovsk\u00fd, 2010; Lacoste-Julien et al., 2013; Dud\u0131\u0301k et al., 2012; Harchaoui et al., 2012; Hazan & Kale, 2012; Shalev-Shwartz et al., 2011; Laue, 2012).", "startOffset": 260, "endOffset": 419}, {"referenceID": 0, "context": "Also relevant in this context is the work of Ahipasaoglu, Sun and Todd (Ahipasaoglu et al., 2008) who showed that in the specific case of minimizing a smooth and strongly convex function over the unit simplex, a variant of the Frank-Wolfe method that also uses away steps converges with a linear rate.", "startOffset": 71, "endOffset": 97}, {"referenceID": 15, "context": "The following lemma is taken from (Journ\u00e9e et al., 2010) (Theorem 12).", "startOffset": 34, "endOffset": 56}, {"referenceID": 16, "context": "The following lemma is taken from (Kakade et al., 2012).", "startOffset": 34, "endOffset": 55}], "year": 2015, "abstractText": "The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections the computational bottleneck in many applications replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization. In this paper we consider the special case of optimization over strongly convex sets, for which we prove that the vanila FW method converges at a rate of 1 t2 . This gives a quadratic improvement in convergence rate compared to the general case, in which convergence is of the order 1t , and known to be tight. We show that various balls induced by `p norms, Schatten norms and group norms are strongly convex on one hand and on the other hand, linear optimization over these sets is straightforward and admits a closed-form solution. We further show how several previous fastrate results for the FW method follow easily from our analysis.", "creator": "LaTeX with hyperref package"}}}