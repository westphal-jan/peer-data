{"id": "1406.1584", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2014", "title": "Learning to Discover Efficient Mathematical Identities", "abstract": "In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a set of grammar rules we build trees that combine different rules, looking for branches which yield compositions that are analytically equivalent to a target expression, but of lower computational complexity. However, as the size of the trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions. Consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search. The first of these is a simple n-gram model, the other being a recursive neural-network. We show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation.", "histories": [["v1", "Fri, 6 Jun 2014 05:28:48 GMT  (617kb,D)", "https://arxiv.org/abs/1406.1584v1", null], ["v2", "Tue, 10 Jun 2014 03:49:51 GMT  (619kb,D)", "http://arxiv.org/abs/1406.1584v2", null], ["v3", "Thu, 6 Nov 2014 02:56:34 GMT  (7556kb,D)", "http://arxiv.org/abs/1406.1584v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wojciech zaremba", "karol kurach", "rob fergus"], "accepted": true, "id": "1406.1584"}, "pdf": {"name": "1406.1584.pdf", "metadata": {"source": "CRF", "title": "Learning to Discover Efficient Mathematical Identities", "authors": ["Wojciech Zaremba", "Karol Kurach"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we explore how machine learning can be applied to the discovery of efficient mathematical identities. We introduce a grammatical framework for attributes to represent symbolic expressions. Considering the grammar of mathematical operators, we build trees that combine them in different ways in search of compositions that analytically correspond to a target expression but are less complex in computation. However, as the space of the trees grows exponentially with the complexity of the target expression, the search for raw violence is impractical for all but the simplest expressions. Consequently, we introduce two new learning approaches that are able to learn from simpler expressions to guide the tree search: the first is a simple n-gram model, the other a recursive neural network. We show how these approaches enable us to derive complex identities that lie beyond the reach of the crude search for violence or the human derivation."}, {"heading": "1 Introduction", "text": "This year we have it in the hand in which we are, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand."}, {"heading": "1.1 Related work", "text": "The problem raised in this paper is that of indecision, persistence and the enormous space of potential arithmetic, but we confine ourselves to the calculation of strong-grade polymers where indecision does not exist, and the space of calculation becomes exponential but not exponential."}, {"heading": "2 Problem Statement", "text": "Problem definition: We get a symbolic target expression T, which combines a series of variables V to produce output O, i.e. O = T (V). We look for an alternative expression S, so that S (V) = T (V) has a lower computational complexity, i.e. O (S) < O (T).In this essay, we consider the limited setting in which (i) T is a homogeneous polynomial of the graph \u0445, (ii) V contains a single matrix or vector A, and (iii) O a scalar. Although these assumptions may seem quite restrictive, they allow us to explore a rich family of expressions for our algorithm. For example, by combining several polynomic terms, an efficient Taylor sequence approximation can be found for expressions that use trigonometric or exponential operators. Regarding (ii), our framework can easily deal with multiple variables, such as mapping 1, for example, which shows expressions such as"}, {"heading": "3 Attribute Grammar", "text": "Since T contains only polynomials, we use the grammatical rules listed in Table 1. With the help of these rules, we can develop trees that combine rules to form expressions with V that are a single matrix A for the purposes of this work. Knowing that T are expressions of degree \u043a, i.e., it contains only terms of degree k. For example, ab + a2 + ac is a homogeneous polynomial of degree 2, but a2 + b is not homogeneous (b is of degree 1, but a2 is of degree 2).k, each tree must use exactly k times. Since the output is a scalar, each tree must also calculate a scalar quantity. These two constraints limit the depth of each tree. For some targets T, whose complexity is only O () n3, we remove the matrix matrix multiplied to ensure that the solution is in most cases a solution (see S2)."}, {"heading": "4 Representation of Symbolic Expressions", "text": "We need an efficient method to verify that the expression generated by a particular tree or combination of trees (see Section 5) matches T. The conventional approach would be to perform this test symbolically, but this is too slow for our purposes and cannot be integrated with learning methods, so we are looking at two alternative approaches."}, {"heading": "4.1 Numerical Representation", "text": "In this representation, each expression is represented by the evaluation of a randomly drawn group of N points, where N is large (typically 1000). Specifically, for each variable in V, different copies are made, each of which is endowed with randomly drawn elements. The target expression evaluates each of these copies and generates a scalar value for each, resulting in a vector t of length N that uniquely characterizes T. Formally, we call this numerical vector t the descriptor of the symbolic expression T. The size of the descriptor N must be sufficiently large to ensure that different expressions are not assigned to the same descriptor. In addition, N, if the descriptors are used in the linear system of Eqn. 5 below, must also be larger than the number of linear equations. Any expression S formed by the grammar can be used to produce a different N-length descriptor, which can then be used as a whole."}, {"heading": "4.2 Learned Representation", "text": "We are considering how to learn a continuous representation for symbolic expressions, that is, we are learning a projection that maps expressions S to l-dimensional vectors. (S) We are considering how to learn a continuous representation for symbolic expressions. (S) We are considering how to learn a continuous representation for symbolic expressions. (S) We are considering how to learn a continuous representation for symbolic expressions. (D) We are considering how to learn these expressions. (D) We are considering how to learn them. (D) We are considering how to learn them. (D) We are considering how to learn them. (D) We are considering how to learn them. (D) We are considering how to learn them. (D) We are considering how to use them. (D) We are considering how to learn them. (D) We are considering how to learn them."}, {"heading": "5 Linear Combinations of Trees", "text": "For simple targets, an expression that matches the target can be contained within a single grammar tree. However, more complex expressions typically require a linear combination of expressions from different tree populations. To handle this, we can use the integer descriptors for each tree in a linear system and find a solution to match the target descriptor (if there is one). If we create a series of M trees each with its own integer descriptor f, we form a linear M by N equation system and solve it: Fw = t mod Zp, where F = [f1,..., fM] contains the tree representations, w is the weighting on each tree and t is the target representation. The system is solved by means of Gaussian elimination, where addition and multiplication modulo p is performed. The number of solutions may vary: (a) there can be no solution, which means that the target state cannot match the current expression."}, {"heading": "6 Search Strategy", "text": "So far, we have proposed a grammar that defines the permitted calculations (such as the grammar of a programming language), but it does not give any guidance on how to explore the space of possible expressions, nor do the representations we have introduced help us - they simply allow us to determine whether an expression matches or not. We now describe how to explore the space efficiently by learning which paths are likely to lead to a match. Our framework uses two components: a scheduler and a strategy. The scheduler is set and traverses the space of expressions according to the recommendations of the chosen strategy (e.g. \"coincidence,\" \"n-gram\" or \"RNN\"). The strategy evaluates which of the possible grammar rules is likely to lead to a solution, since the current expression is used. Starting with variables V (in our case a single element A, or more generally one of the elements A, B, etc.), the scheduler receives points for each rule and chooses the one with the highest number of points at each step."}, {"heading": "6.1 Random Strategy", "text": "For simple targets, this strategy can be successful, as the planner can stumble upon a match with the target within a reasonable timeframe, but for complex target expressions of high degree k, the search space is huge and the approach fails. 6,2 n-gram In this strategy, we simply count how often sub-trees of depth n occur in solutions for previously solved targets. As the number of different sub-trees of depth n is large, the numbers become very sparse when n grows. Therefore, we use a weighted linear combination of values from all depths up to n. We found an effective weighting of 10k, where k is the depth of the tree."}, {"heading": "6.3 Recursive Neural Network", "text": "In Section 4.2, it was shown how to use an RNN to learn a continuous representation of grammar trees. Remember that the RNN expressions are mapped to continuous vectors: \u03c6 (S) \u2192 Rl. To build a search strategy from this, we form a Softmax layer on top of the RNN to predict which rule should be applied to the current expression (or expressions), since some rules have two inputs so that we match the target. Formally, we have two current branches b1 and b2 (each corresponding to an expression) and would like to predict the root operation r that connects them (e.g.) from the valid grammar rules (| r | total). First, we use the previously trained RNN to calculate the structures (b1) and (b2), which are then presented to a | r | -manner Softmax layer, which they (whose weight matrix U of the size 2l \u00d7 r | ekt) adjust to the training layer (if a branch of V.eu exists), then a specific one of the training layer (if only a branch of V.eu exists) is specified."}, {"heading": "7 Experiments", "text": "First, we present results on the learned representation of symbolic expressions (Section 4.2), then we present our framework for discovering efficient identities. For the sake of brevity, the discovered identities are listed in the supplementary material [29]."}, {"heading": "7.1 Expression Classification using Learned Representation", "text": "Table 2 shows the accuracy of the RNN model on expressions of varying degrees, ranging from k = 3 to k = 6. The difficulty of the task can be assessed from the examples in Fig. 1. The low error rate of \u2264 5%, despite the use of a simple Softmax classifier, shows the effectiveness of our learned representation. Finished initializations. The tasks become more difficult as the number of classes grows and the arithmetic trees deepen. However, our data set is also growing (the training uses 80% of the examples)."}, {"heading": "7.2 Efficient Identity Discovery", "text": "In our experiments we look at 5 different families of expressions that fall within the scope of our grammar rules (A = A = A).A (A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A (A = A).A (A).A (A).A (A).A).A (A).A (A).A (A).A (A).A (A).A (A).A (A).A (.A).A (.A).A (.A).A (.A).A (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A (.A) (.A) (.A) (.A) (.A (.A) (.A (.A) (.A) (.A) (.A (.A) (.A (.A) (.A) (.A) (.A) (.A (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A (.A) (.A) (.A) (.A) (.A) (.A) (.A) (.A"}, {"heading": "8 Discussion", "text": "We have introduced a framework based on a grammar of symbolic operations for the discovery of mathematical identities. By applying novel learning methods, we have shown how exploring the search space can be learned from previously successful solutions to simpler expressions, allowing us to discover complex expressions that random or brute force strategies cannot find (the identities are given in the supplementary material [29]). Some of the families considered in this paper are similar to expressions that occur frequently in machine learning. Thus, a dropout contains an exponential sum of binary masks associated with the RBM-1 family. Also, the partition function of an RBM can be approximated by the RBM-2 family. Therefore, the identities we discover could potentially be used to give a closed version of this dropout or to efficiently calculate the RBM partition function (i.e., in polynomic time)."}, {"heading": "Acknowledgements", "text": "The authors thank Facebook and Microsoft Research for their support."}, {"heading": "9 Supplementary material", "text": "We present the efficient expressions discovered by our system using Matlab Art syntax, and we visualize the calculation trees. (Each example contains: (i) code that calculates the original target formulas; (ii) the formula derived from our system lae and (iii) the sum that confirms the accuracy of the expression. (The source files for this paper are available at https: / / github.com / kkurach / math _ learning / paper /. 9.1 (AAT) kk = 1 n = 00; m = 2 00; A = randn (n, m); o r i (a, m); o r i = a (a, m); o r i = a (a); o r i = a (a); b (a) (a); a = (a) (a = a); a = b; (a) (a) (a = b)."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "In this paper we explore how machine learning techniques can be applied to the<lb>discovery of efficient mathematical identities. We introduce an attribute gram-<lb>mar framework for representing symbolic expressions. Given a grammar of math<lb>operators, we build trees that combine them in different ways, looking for compo-<lb>sitions that are analytically equivalent to a target expression but of lower compu-<lb>tational complexity. However, as the space of trees grows exponentially with the<lb>complexity of the target expression, brute force search is impractical for all but<lb>the simplest of expressions. Consequently, we introduce two novel learning ap-<lb>proaches that are able to learn from simpler expressions to guide the tree search.<lb>The first of these is a simple n-gram model, the other being a recursive neural-<lb>network. We show how these approaches enable us to derive complex identities,<lb>beyond reach of brute-force search, or human derivation.", "creator": "LaTeX with hyperref package"}}}