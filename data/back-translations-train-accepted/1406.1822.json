{"id": "1406.1822", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2014", "title": "Logarithmic Time Online Multiclass prediction", "abstract": "We study the problem of multiclass classification with an extremely large number of classes, with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves small error rates relative to more common O(k) approaches and simultaneously achieves significant improvement in test error compared to other logarithmic training time approaches.", "histories": [["v1", "Fri, 6 Jun 2014 21:52:25 GMT  (31kb,D)", "http://arxiv.org/abs/1406.1822v1", null], ["v2", "Wed, 6 Aug 2014 08:44:27 GMT  (29kb,D)", "http://arxiv.org/abs/1406.1822v2", null], ["v3", "Thu, 7 Aug 2014 16:53:01 GMT  (29kb,D)", "http://arxiv.org/abs/1406.1822v3", null], ["v4", "Sat, 25 Oct 2014 22:57:31 GMT  (27kb,D)", "http://arxiv.org/abs/1406.1822v4", null], ["v5", "Sun, 14 Dec 2014 20:43:46 GMT  (28kb,D)", "http://arxiv.org/abs/1406.1822v5", null], ["v6", "Fri, 6 Feb 2015 00:54:09 GMT  (59kb,D)", "http://arxiv.org/abs/1406.1822v6", null], ["v7", "Fri, 27 Mar 2015 21:04:14 GMT  (59kb,D)", "http://arxiv.org/abs/1406.1822v7", null], ["v8", "Mon, 18 May 2015 00:08:13 GMT  (371kb,D)", "http://arxiv.org/abs/1406.1822v8", null], ["v9", "Tue, 19 May 2015 01:14:10 GMT  (371kb,D)", "http://arxiv.org/abs/1406.1822v9", null], ["v10", "Fri, 22 May 2015 23:35:53 GMT  (371kb,D)", "http://arxiv.org/abs/1406.1822v10", null], ["v11", "Wed, 3 Jun 2015 16:29:53 GMT  (371kb,D)", "http://arxiv.org/abs/1406.1822v11", null], ["v12", "Thu, 6 Aug 2015 04:02:41 GMT  (372kb,D)", "http://arxiv.org/abs/1406.1822v12", null], ["v13", "Sat, 14 Nov 2015 23:02:33 GMT  (372kb,D)", "http://arxiv.org/abs/1406.1822v13", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anna choromanska", "john langford"], "accepted": true, "id": "1406.1822"}, "pdf": {"name": "1406.1822.pdf", "metadata": {"source": "CRF", "title": "Logarithmic Time Online Multiclass prediction", "authors": ["Anna Choromanska", "John Langford"], "emails": ["aec2163@columbia.edu", "jcl@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "1.1 Prior Work", "text": "Few authors have dealt with logarithmic time training before, and the filter tree [3] addresses consistent (and robust) multicultural classification, which shows that it is possible within the statistical boundaries; the filter tree does not address the partition problem as we do here, but in our experiments we can empirically compare it with the filter tree and find that tackling the partition problem is often helpful; the partition problem is addressed in the conditional probability tree [4], but the paper addresses conditional probability estimation can be converted into multicultural class predictions, but this is not a logarithmic time operation.Quite a few authors have dealt with the logarithmic test time, while the training time is O (k) or worse. While these approaches are limited to our larger scale problems, we describe them here for context."}, {"heading": "2 Framework and theoretical analysis", "text": "In this section we describe the essential elements of the approach and outline the theoretical properties of the resulting framework. We begin by presenting the ideas at the highest level."}, {"heading": "2.1 Setting", "text": "We use a hierarchical approach to learn a multi-level decision tree structure, balancing this structure from top to bottom. We assume that we get examples x-X-Rd, with the designations y {1, 2,..., k}. We also assume that we get access to a hypothesis class H, in which each h-H is a binary classifier, h: X 7 \u2192 {\u2212 1, 1}. The overall goal is to learn a tree of depth O (log k), in which each node in the tree is formed from a classifier h-H. The classifiers are trained so that hn (x) = 1 means that the example x is sent to the right subtree of the node n, while h (x) = \u2212 1 x sends to the left subtree. When we reach a leaf node, we simply predict according to the label with the highest frequency among the examples reaching this leaf."}, {"heading": "2.2 An objective and analysis of resulting partitions", "text": "We now define a criterion for measuring the quality of a hypothesis h = > H when creating partitions on a fixed node n in the tree. Let's leave \u03c0i the proportion of the denomination i among the examples reaching this node. Let's leave P (h) = 0.5 k) > 0) and P (h) > 0 | i) the proportion of examples reaching n, for h (x) > 0, marginal and determined by class i. Then, let's define the goal: J (h) = 2 k \u00b2 i = 1 \u03c0i | P (x) > 0) \u2212 P (h) > 2) \u2212 P (h), trying to maximize the objective J (h) in order to obtain high-quality partitions. Intuitively, the lens encourages the proportion of examples going from class i to the left significantly from the background fraction i. As a concrete simple scenario, if P (h) > 0) = 0.5% of the pure class i."}, {"heading": "2.3 Quality of the entire tree", "text": "The above section helps us to understand the quality of an individual division, which is achieved by effectively maximizing J (h) = \u03b2 \u03b2 = \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 (n). However, we need to think about the quality of the entire tree in a hierarchical setting, as we add more and more nodes. Does each new node ensure a sufficient improvement in statistical quality? Can we obtain high quality forecasts with flat trees? While we do not generally do this, we take a starting point to this question. We measure the quality of the trees that use average entropy across all leaves in the tree and track the decrease in this entropy as a function of the number of nodes. In doing so, we borrow from the theoretical analysis of the Decisiontree algorithms in Kearns and Mansour, which were originally developed to show the properties of the decision trees for binary classification problems. We generalize their theoretical analysis for multiclass-setting.In view of a tree quality, we consider the function of tree entropy G (the measure of tree T)."}, {"heading": "3 The LOMTree Algorithm", "text": "The objective function of section 2 has another convenient equivalent form, which results in a simple online algorithm for tree construction and training. Note that the objective function defined in Equation 1 can be written as -J (h) = 1 (h) > 0 (h) -Ex, i [h (x) -Lease (v) > 0 (f) -Lease (v) -Lease (n) -Lease (n) -Lease (n) -Lease (n) -Lease (n) -Lease (n) -Lease (n) -Lease (n) -n (n) -Lease (n) -( n) -Lease (n) -Lease (n) -n (n) -Lease (n) -( n) -Lease (n) -Lease (n) -( n) -Lease (n) -( n) -Lease (n) -( n) -Lease (n) -( n) -Lease (n) -( n (n) -Lease (n) -( n (n) -Lease (n) -( n) -Lease (n) -( n (n) -Lease (n) -( n) -Lease (n (n) -n (n) -Lease (n) -n (n (n) -Lease (n) -n (n) -n (n (n) Lease (n) -n (n (n) - n (n) Lease (n (n) -n (n) Lease (n (n) -n (n) Lease (n (n) -n (n (n) Lease (n) -n (n (n) - n (n) Lease (n (n) Lease (n) -n (n (n) Lease (n) -n (n (n) Lease (n) -n (n (n) Lease (n) -n (n (n) Lease (n (n) - (n) Lease (n) -n (n) Lease (n) -n (n (n) Lease (n (n) Lease (n) Lease (n (n) Lease (n (n) Lease (n)"}, {"heading": "3.1 Swapping", "text": "It is a scenario in which the current training example descends to the size of the tree. The leaf can split (create two children) if the examples it has achieved in the past come from at least two different classes. However, if the number of non-leasing nodes of the tree reaches the threshold T, no further examples can be expanded and thus no children can be created. As the tree construction is done online, some nodes created in the early stages of training can become unusable because no examples reach them later. This prevents potentially useful splits such as leaf j. This problem can be solved by recycling nodes created in algorithms. The general idea behind recycling is to split nodes if a certain condition is met. In particular, node j splits if the following holds: Cj \u2212 max i = 1.2, k lj (i) > RS (C r + 1), where Cj is the size of the smallest leaf."}, {"heading": "4 Experiments", "text": "We conducted experiments on a variety of benchmark multicultural datasets: Isolet, Sector, Alois, ImageNet (ImNet) and ODP2. Details of the datasets are listed in Table 1. Data sets were divided into training (90% and exam 10%) and 10% of the training dataset was used as validation set. We compared LOMtree with a balanced random tree (Rtree), filter tree (OAA) and all methods were implemented in the open source Vowpal Wabbit system."}, {"heading": "5 Conlusion and Future Work", "text": "The LOMTree algorithm reduces the multiclass problem to a series of binary problems organized in a tree structure, where the division in each tree node is restored by optimizing a carefully crafted partition criterion, guaranteeing pure (inductive) and balanced divisions that result in logarithmic training and test time for the tree classifier. We provide a theoretical justification through a boosting statement and an empirical evaluation of multiple multiclass records. Empirically, we find that this is the best logarithmic time approach available, where performance approaches all smaller datasets where this is a viable baseline."}, {"heading": "Acknowledgments", "text": "We would like to thank Alekh Agarwal, Dean Foster, Robert Schapire and Matus Telgarsky for their valuable discussions."}, {"heading": "6 Bottom-up partitions do not work", "text": "Here, we show that the most natural bottom-up construction for creating partitions cannot be done with an example. Bottom-up construction techniques begin by pairing labels either randomly or arbitrarily, and then by creating a predictor for whether the class label on the left or right depends on the class label being one of the paired labels. To construct a complete tree, this process must be composed and pairs trees of size 2 to create trees of size 4.Here, we show that the simple approach to composition failure.Let's say we have a one-dimensional attribute space with examples of class i with attribute value i and we work with threshold predictors. Let's say we have 4 classes 1, 2, 3, 4 and we come up with pairs (1, 3) and (2, 4). It's easy to build a linear predictor for each of these splits, with the next step consisting of building a predictor, vs (4), (1) and (4) for each (1) and (1)."}, {"heading": "7 Proof of Lemma 1", "text": "We start with the derivation of an upper limit to J (h). Again, for the simplicity of the indication we leave Pi = Pr (h) > 0 (h) > 0 (h) > 0 (h) > 0 (h) \u2212 P (h) > 0 (h) > 0 (h) > 0 (h) > 0 (min (Pi) = 1 (p) and remember the purity factor (p) = 1 (p) and the compensating factor \u03b2 = P (h) > 0 (h) > 0). Without loss of generality we leave L1 (p) = 1 (p)."}, {"heading": "8 Proof of Lemma 2", "text": "We begin by deriving an upper limit on J (\u03b2) (\u03b2), where h (\u03b2) H is a hypothesis in the hypothesis class. \u2212 For the simplicity of notation, we leave Pi = Pr (h) = Pr (x) > 0 | i). ThusJ (h) = 2 k (h) = 2 p (h) (h) > 0 | i) \u2212 P (h) > 0) > 0 (6) = 2 k) i = 1 (h). The objective J (h) is certainly maximized to the extremes of the [0, 1] interval. Thus, the upper limit on J (h) can be reached by setting some of the Pi's to 0's and the remaining ones to 0's."}, {"heading": "10 Proof of Theorem 1", "text": "Proof. Note that the condition \u03b3 (0, min (\u03b2n, 1 \u2212 \u03b2n)] implies that \u03b3 \u2264 12. It follows from the weak hypothesis that for each n, \u03b2n cannot be too close to 0 or 1 since 1 \u2212 \u03b3 \u2265 \u03b2n \u2265 \u03b3. In section 9 we have shown that \u2206 t \u2265 J2Gt8\u03b2 (1 \u2212 \u03b2) t ln k. We will now move to another lower hypothesis. Note that the weak hypothesis J (h) \u2265 2\u03b3 guarantees further yields amounting to \u04452Gt2 (1 \u2212 \u03b3) 2t ln k. Let it happen."}, {"heading": "11 Experiments - dataset details", "text": "Below we present the details of the data sets we used for the experiments in Section 4: \u2022 Isolet: downloaded from http: / / www.cs.huji.ac.il / \u02dc shais / datasets / ClassificationDatasets.html \u2022 Sector and Aloi: downloaded from http: / / www.csie.ntu.edu.tw / \u02dc cjlin / libsvmtools / datasets / multiclass.html \u2022 ImageNet [17]: features extracted from the authors according to http: / / www.di.ens.fr / willow / research / cnn /. \u2022 ODP [12]: received from Paul Bennett. Our version has significantly more classes than specified in this paper because we use the entire data set."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We study the problem of multiclass classification with an extremely large number<lb>of classes, with the goal of obtaining train and test time complexity logarithmic<lb>in the number of classes. We develop top-down tree construction approaches for<lb>constructing logarithmic depth trees. On the theoretical front, we formulate a new<lb>objective function, which is optimized at each node of the tree and creates dy-<lb>namic partitions of the data which are both pure (in terms of class labels) and<lb>balanced. We demonstrate that under favorable conditions, we can construct loga-<lb>rithmic depth trees that have leaves with low label entropy. However, the objective<lb>function at the nodes is challenging to optimize computationally. We address the<lb>empirical problem with a new online decision tree construction procedure. Exper-<lb>iments demonstrate that this online algorithm quickly achieves small error rates<lb>relative to more common O(k) approaches and simultaneously achieves signif-<lb>icant improvement in test error compared to other logarithmic training time ap-<lb>proaches.", "creator": "LaTeX with hyperref package"}}}