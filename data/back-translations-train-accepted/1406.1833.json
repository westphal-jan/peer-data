{"id": "1406.1833", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2014", "title": "Unsupervised Feature Learning through Divergent Discriminative Feature Accumulation", "abstract": "Unlike unsupervised approaches such as autoencoders that learn to reconstruct their inputs, this paper introduces an alternative approach to unsupervised feature learning called divergent discriminative feature accumulation (DDFA) that instead continually accumulates features that make novel discriminations among the training set. Thus DDFA features are inherently discriminative from the start even though they are trained without knowledge of the ultimate classification problem. Interestingly, DDFA also continues to add new features indefinitely (so it does not depend on a hidden layer size), is not based on minimizing error, and is inherently divergent instead of convergent, thereby providing a unique direction of research for unsupervised feature learning. In this paper the quality of its learned features is demonstrated on the MNIST dataset, where its performance confirms that indeed DDFA is a viable technique for learning useful features.", "histories": [["v1", "Fri, 6 Jun 2014 23:45:03 GMT  (75kb,D)", "https://arxiv.org/abs/1406.1833v1", "9 pages, 2 figures"], ["v2", "Tue, 10 Jun 2014 03:37:45 GMT  (75kb,D)", "http://arxiv.org/abs/1406.1833v2", "Corrected citation formatting"]], "COMMENTS": "9 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["paul a szerlip", "gregory morse", "justin k pugh", "kenneth o stanley"], "accepted": true, "id": "1406.1833"}, "pdf": {"name": "1406.1833.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Feature Learning through Divergent Discriminative Feature Accumulation", "authors": ["Paul A. Szerlip", "Gregory Morse", "Justin K. Pugh"], "emails": ["pszerlip@eecs.ucf.edu,", "jpugh@eecs.ucf.edu,", "kstanley@eecs.ucf.edu,", "gregorymorse07@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "The increasing realization that people who do not see themselves as able to realize themselves are also unable to deny themselves is one reason why they are not able. (...) It is a fact that people who are not able to deny themselves. (...) It is a fact that people who are not able to deny themselves. (...) It is a fact that people who are not able to deny themselves. (...) It is a fact that people who are not able to deny themselves. (...) It is a fact that people who are not able to deny themselves. (...) It is another person who is unable to recognize themselves. (...) It is a fact that people who are not able to deny themselves are able to act themselves. (...) It is another person who is not able to recognize themselves. (...) It is a fact that people deny themselves. (...) It is a fact that people deny themselves. (...) It is a fact that people deny themselves. (...)"}, {"heading": "2 Background", "text": "In this section, the two algorithms novelty search and HyperNEAT, which are the basis of the DDFA approach, are discussed."}, {"heading": "2.1 Novelty Search", "text": "The problem of collecting novel instances of a class differs from the more familiar problem of minimizing errors. While error minimization aims to move toward a minimum in the search space, collecting novelty requires deviating from past discoveries and fanning across the search space in all directions that seem to lead toward further novelty. Thus, this fan-out process lends itself well to a population-based approach that accumulates and remembers novel discoveries in order to continually push the search toward even more novelty as it progresses. However, it is important to note that novelty itself implements traditional evolutionary algorithms (EAs) in practice through an evolutionary approach that naturally provides the population-driven context appropriate for the search for novelty."}, {"heading": "2.2 HyperNEAT", "text": "The term for algorithms, the search for ANNs through an evolutionary process, is Neuroevolution (Floreano et al., 2008; Stanley and Miikkulainen, 2002). It is important to note that modern Neuroevolution algorithms do not function like conventional EAs based on bit strings, but rather implement a variety of complex heuristics and encodings that allow the discovery of large and well-organized networks. This section is designed to introduce the main ideas of Neuroevolution algorithms without understanding details that are unnecessary to understand the operation of the proposed DFA algorithms. Full details of HyperNEAT can be found in their primary sources (Gauci and Stanley)."}, {"heading": "3 Approach: Divergent Discriminative Feature Accumulation (DDFA)", "text": "However, this is only possible because the imperative of reconstruction requires that the learned traits ultimately reflect an aspect of the underlying structure of education. Of course, a potential problem with this approach is that there is no assurance that the learned traits are actually align with a particular classification or discrimination for which they could be used in the future. However, this traditional approach to learning traits also raises some interesting deeper questions. For example, there is another way to extract meaningful traits and thus learn representations from a set of examples without explicit supervision. There are some known simple alternatives, although they are not usually labeled as characterizing algorithms."}, {"heading": "4 Experiment", "text": "The key question raised in this paper is whether a deviant, discriminatory feature can learn a deeper trait, which means that it should help in effectively generalizing on the test set. If this is possible, the implication is that DDFA is a viable alternative to other types of unattended pre-training. To investigate this question, DDFA is trained and tested during the training as a vector of 60,000 real values. Since the structure of the networks produced by HyperNEAT can include as many hidden layers as users choose how many hidden layers should be allowed in individual features learned by HyperNEAT, this consideration is substantial, because the DDFA can in principle learn individual features at once, which is different."}, {"heading": "5 Results", "text": "The most important results are presented in Table 1. DDFA achieved test errors of 1.42% and 1.25% from collections of 1,500 and 3,000 features, respectively, both well below the 1.6% error of the similar flat network, which was trained without pre-processing by Simard et al. (2003). In fact, the result for the 3,000 feature network is even closer to the 1.2% error of the much deeper network by Hinton et al. (2006), which shows that flat networks can generalize surprisingly well by finding sufficiently high-quality feature sets, even despite a lack of distortion during training. It also appears that more collected features lead to better generalization, at least at these sizes. It took 338 and 676 generations of feature collection to obtain the 1,500 and 3,000 features, respectively. Collecting 3,000 features took about 36 hours of compression to 12 3.0 GHz cores. Figure 2 shows a typical set of DFA characteristics, which were collected by DFA."}, {"heading": "6 Discussion and Future Work", "text": "The results suggest that DDFA can indeed collect useful functions and thus serve as an alternative to uncontrolled learning processes. While it may ultimately lead to better training performance in some cutting-edge problems, future work with more layers and larger problems is clearly necessary to investigate its full potential for exceeding peak results. However, it is important to recognize that there is significantly more at stake in the dissemination of alternative uncontrolled training techniques based on new principles than performance. Deep learning faces several fundamental challenges that are not just about test performance. For example, the recent surprising results from Szegedy et al. (2013) need to show that very small but abnormal disturbances in training images that are imperceptible to the human eye can deceive several different types of deep networks that are nevertheless ominously well evaluated in the test group. The effects of these anomalies are not yet understood to be well able to take these anomalies, as Bengio (2013) shows that local pedigree problems are not sufficient to address in their own way."}], "references": [{"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "Statistical Language and Speech Processing, 1\u201337. Springer.", "citeRegEx": "Bengio,? 2013", "shortCiteRegEx": "Bengio", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 1798\u20131928.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems 19 (NIPS). Cambridge, MA: MIT Press.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["D. Cire\u015fan", "U. Meier", "L. Gambardella", "J. Schmidhuber"], "venue": "Neural computation, 22(12):3207\u20133220.", "citeRegEx": "Cire\u015fan et al\\.,? 2010", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2010}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "International Conference on Artificial Intelligence and Statistics, 215\u2013223.", "citeRegEx": "Coates et al\\.,? 2011", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Behavioral repertoire learning in robotics", "author": ["A. Cully", "Mouret", "J.-B."], "venue": "Proceeding of the fifteenth annual conference on Genetic and evolutionary computation conference, GECCO \u201913, 175\u2013182. New York, NY, USA: ACM.", "citeRegEx": "Cully et al\\.,? 2013", "shortCiteRegEx": "Cully et al\\.", "year": 2013}, {"title": "Neuroevolution: from architectures to learning", "author": ["D. Floreano", "P. D\u00fcrr", "C. Mattiussi"], "venue": "Evolutionary Intelligence, 1:47\u201362.", "citeRegEx": "Floreano et al\\.,? 2008", "shortCiteRegEx": "Floreano et al\\.", "year": 2008}, {"title": "A case study on the critical role of geometric regularity in machine learning", "author": ["J. Gauci", "K.O. Stanley"], "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI-2008). Menlo Park, CA: AAAI Press.", "citeRegEx": "Gauci and Stanley,? 2008", "shortCiteRegEx": "Gauci and Stanley", "year": 2008}, {"title": "Autonomous evolution of topographic regularities in artificial neural networks", "author": ["J. Gauci", "K.O. Stanley"], "venue": "Neural Computation, 22(7):1860\u20131898.", "citeRegEx": "Gauci and Stanley,? 2010", "shortCiteRegEx": "Gauci and Stanley", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W."], "venue": "Neural Computation, 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Exploiting generative models in discriminative classifiers. Advances in neural information processing", "author": ["T. Jaakkola", "D Haussler"], "venue": null, "citeRegEx": "Jaakkola and Haussler,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola and Haussler", "year": 1999}, {"title": "Convolutional networks for images, speech, and time-series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "Arbib, M. A., editor, The Handbook of Brain Theory and Neural Networks. MIT Press.", "citeRegEx": "LeCun and Bengio,? 1995", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes", "year": 1998}, {"title": "Abandoning objectives: Evolution through the search for novelty alone", "author": ["J. Lehman", "K.O. Stanley"], "venue": "Evolutionary Computation, 19(2):189\u2013223.", "citeRegEx": "Lehman and Stanley,? 2011", "shortCiteRegEx": "Lehman and Stanley", "year": 2011}, {"title": "Sparse feature learning for deep belief networks", "author": ["R. Marc\u2019Aurelio", "L. Boureau", "Y. LeCun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Marc.Aurelio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Marc.Aurelio et al\\.", "year": 2007}, {"title": "On discriminative vs", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems, 2:841\u2013 848.", "citeRegEx": "Ng and Jordan,? 2002", "shortCiteRegEx": "Ng and Jordan", "year": 2002}, {"title": "On fast deep nets for agi vision", "author": ["J. Schmidhuber", "D. Cire\u015fan", "U. Meier", "J. Masci", "A. Graves"], "venue": "The Fourth Conference on Artificial General Intelligence (AGI), 243\u2013246. New York, NY: Springer.", "citeRegEx": "Schmidhuber et al\\.,? 2011", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 2011}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "International Conference on Document Analysis and Recogntion (ICDAR), vol. 3, 958\u2013962.", "citeRegEx": "Simard et al\\.,? 2003", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Compositional pattern producing networks: A novel abstraction of development", "author": ["K.O. Stanley"], "venue": "Genetic Programming and Evolvable Machines Special Issue on Developmental Systems, 8(2):131\u2013162.", "citeRegEx": "Stanley,? 2007", "shortCiteRegEx": "Stanley", "year": 2007}, {"title": "A hypercube-based indirect encoding for evolving large-scale neural networks", "author": ["K.O. Stanley", "D.B. D\u2019Ambrosio", "J. Gauci"], "venue": null, "citeRegEx": "Stanley et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Stanley et al\\.", "year": 2009}, {"title": "Evolving neural networks through augmenting topologies", "author": ["K.O. Stanley", "R. Miikkulainen"], "venue": "Evolutionary Computation, 10:99\u2013127.", "citeRegEx": "Stanley and Miikkulainen,? 2002", "shortCiteRegEx": "Stanley and Miikkulainen", "year": 2002}, {"title": "A taxonomy for artificial embryogeny", "author": ["K.O. Stanley", "R. Miikkulainen"], "venue": "Artificial Life, 9(2):93\u2013130.", "citeRegEx": "Stanley and Miikkulainen,? 2003", "shortCiteRegEx": "Stanley and Miikkulainen", "year": 2003}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I.J. Goodfellow", "R. Fergus"], "venue": "CoRR, abs/1312.6199.", "citeRegEx": "Szegedy et al\\.,? 2013", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Evolving static representations for task transfer", "author": ["P. Verbancsics", "K.O. Stanley"], "venue": "Journal of Machine Learning Research (JMLR), 11:1737\u20131769.", "citeRegEx": "Verbancsics and Stanley,? 2010", "shortCiteRegEx": "Verbancsics and Stanley", "year": 2010}, {"title": "Constraining connectivity to encourage modularity in HyperNEAT", "author": ["P. Verbancsics", "K.O. Stanley"], "venue": "GECCO \u201911: Proceedings of the 13th annual conference on Genetic and evolutionary computation, 1483\u20131490. Dublin, Ireland: ACM.", "citeRegEx": "Verbancsics and Stanley,? 2011", "shortCiteRegEx": "Verbancsics and Stanley", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "The increasing realization in recent years that artificial neural networks (ANNs) can learn many layers of features (Bengio et al., 2007; Cire\u015fan et al., 2010; Hinton et al., 2006; Marc\u2019Aurelio et al., 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al.", "startOffset": 116, "endOffset": 207}, {"referenceID": 3, "context": "The increasing realization in recent years that artificial neural networks (ANNs) can learn many layers of features (Bengio et al., 2007; Cire\u015fan et al., 2010; Hinton et al., 2006; Marc\u2019Aurelio et al., 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al.", "startOffset": 116, "endOffset": 207}, {"referenceID": 9, "context": "The increasing realization in recent years that artificial neural networks (ANNs) can learn many layers of features (Bengio et al., 2007; Cire\u015fan et al., 2010; Hinton et al., 2006; Marc\u2019Aurelio et al., 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al.", "startOffset": 116, "endOffset": 207}, {"referenceID": 14, "context": "The increasing realization in recent years that artificial neural networks (ANNs) can learn many layers of features (Bengio et al., 2007; Cire\u015fan et al., 2010; Hinton et al., 2006; Marc\u2019Aurelio et al., 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al.", "startOffset": 116, "endOffset": 207}, {"referenceID": 1, "context": ", 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al., 2013).", "startOffset": 71, "endOffset": 92}, {"referenceID": 2, "context": "While the beginning of this renaissance focused on the sequential unsupervised training of individual layers one upon another (Bengio et al., 2007; Hinton et al., 2006), the number of approaches and variations that have proven effective at training in such deep learning has since exploded (Bengio et al.", "startOffset": 126, "endOffset": 168}, {"referenceID": 9, "context": "While the beginning of this renaissance focused on the sequential unsupervised training of individual layers one upon another (Bengio et al., 2007; Hinton et al., 2006), the number of approaches and variations that have proven effective at training in such deep learning has since exploded (Bengio et al.", "startOffset": 126, "endOffset": 168}, {"referenceID": 1, "context": ", 2006), the number of approaches and variations that have proven effective at training in such deep learning has since exploded (Bengio et al., 2013; Schmidhuber et al., 2011).", "startOffset": 129, "endOffset": 176}, {"referenceID": 16, "context": ", 2006), the number of approaches and variations that have proven effective at training in such deep learning has since exploded (Bengio et al., 2013; Schmidhuber et al., 2011).", "startOffset": 129, "endOffset": 176}, {"referenceID": 1, "context": "This explosion has in turn raised the question of what makes a good representation, and how it is best learned (Bengio et al., 2013).", "startOffset": 111, "endOffset": 132}, {"referenceID": 3, "context": "For example, supervised approaches such as stochastic gradient descent (Cire\u015fan et al., 2010) that aim to minimize the error in a classification problem in effect encourage the exclusive discovery of features that help to discriminate among the target classifications.", "startOffset": 71, "endOffset": 93}, {"referenceID": 9, "context": "In contrast, unsupervised approaches, which include both generative representations such as restricted Boltzmann machines (RBMs) (Hinton et al., 2006) and autoencoders that are trained to reproduce their inputs (Bengio et al.", "startOffset": 129, "endOffset": 150}, {"referenceID": 2, "context": ", 2006) and autoencoders that are trained to reproduce their inputs (Bengio et al., 2007), yield a more general feature set that captures dimensions of variation that may or may not be essential to the classification objective.", "startOffset": 68, "endOffset": 89}, {"referenceID": 15, "context": "generative versus discriminative features have proven both subtle and complex (Jaakkola et al., 1999; Ng and Jordan, 2002).", "startOffset": 78, "endOffset": 122}, {"referenceID": 13, "context": "Thus a well-suited algorithm for implementing this idea in practice is the recent novelty search algorithm (Lehman and Stanley, 2011), which is a divergent evolutionary algorithm that is rewarded for moving away in the search space of candidate behaviors (such as discriminations) from where it has already visited to where it has not.", "startOffset": 107, "endOffset": 133}, {"referenceID": 12, "context": "To demonstrate the potential of DDFA to collect useful features, it is tested in this paper by collecting single-layer features for the MNIST digital handwriting recognition benchmark (LeCun and Cortes, 1998).", "startOffset": 184, "endOffset": 208}, {"referenceID": 11, "context": "accumulating multilayer features or searching for novel features that are built above already-discovered features) and convolution (LeCun and Bengio, 1995), just like other deep learning algorithms.", "startOffset": 131, "endOffset": 155}, {"referenceID": 13, "context": "The novelty search algorithm (Lehman and Stanley, 2011) implements such a process in practice through an evolutionary approach, which naturally provides the population-driven context appropriate for finding novelty.", "startOffset": 29, "endOffset": 55}, {"referenceID": 0, "context": "For example, when discussing the future of representation learning, Bengio (2013) notes:", "startOffset": 68, "endOffset": 82}, {"referenceID": 13, "context": "Novelty search (Lehman and Stanley, 2011) can be viewed as an embodiment of just such a \u201cgenetic evolution\u201d that is suited to accumulating discoveries free from the pitfalls of \u201clocal descent.", "startOffset": 15, "endOffset": 41}, {"referenceID": 13, "context": "\u201d In fact, while novelty search was originally shown sometimes to find the objective of an optimization problem more effectively than objective-based optimization (Lehman and Stanley, 2011), Cully and Mouret (2013) recently raised the intriguing notion of novelty search as a repertoire collector.", "startOffset": 163, "endOffset": 189}, {"referenceID": 13, "context": "Novelty search (Lehman and Stanley, 2011) can be viewed as an embodiment of just such a \u201cgenetic evolution\u201d that is suited to accumulating discoveries free from the pitfalls of \u201clocal descent.\u201d In fact, while novelty search was originally shown sometimes to find the objective of an optimization problem more effectively than objective-based optimization (Lehman and Stanley, 2011), Cully and Mouret (2013) recently raised the intriguing notion of novelty search as a repertoire collector.", "startOffset": 16, "endOffset": 407}, {"referenceID": 6, "context": "The term for algorithms that search for ANNs through an evolutionary process is neuroevolution (Floreano et al., 2008; Stanley and Miikkulainen, 2002).", "startOffset": 95, "endOffset": 150}, {"referenceID": 20, "context": "The term for algorithms that search for ANNs through an evolutionary process is neuroevolution (Floreano et al., 2008; Stanley and Miikkulainen, 2002).", "startOffset": 95, "endOffset": 150}, {"referenceID": 19, "context": "The complete details of HyperNEAT can be found in its primary sources (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010).", "startOffset": 70, "endOffset": 154}, {"referenceID": 23, "context": "The complete details of HyperNEAT can be found in its primary sources (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010).", "startOffset": 70, "endOffset": 154}, {"referenceID": 21, "context": "Neuroevolution algorithms have responded to this concern with a class of representations called indirect encodings (Stanley and Miikkulainen, 2003), wherein the weight pattern is generated by an evolving genetic encoding that is biased towards contiguity and regularity by design.", "startOffset": 115, "endOffset": 147}, {"referenceID": 19, "context": "HyperNEAT, which stands for Hypercube-based NeuroEvolution of Augmenting Topologies (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010) is a contemporary neuroevolution algorithm based on such an indirect encoding.", "startOffset": 84, "endOffset": 168}, {"referenceID": 23, "context": "HyperNEAT, which stands for Hypercube-based NeuroEvolution of Augmenting Topologies (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010) is a contemporary neuroevolution algorithm based on such an indirect encoding.", "startOffset": 84, "endOffset": 168}, {"referenceID": 18, "context": "In short, HyperNEAT evolves an encoding network called a compositional pattern producing network (CPPN; Stanley 2007) that describes the pattern of connectivity within the ANN it encodes.", "startOffset": 97, "endOffset": 117}, {"referenceID": 8, "context": "Because the CPPN encoding is designed to describe patterns of weights across the geometry of the encoded network, the weights in HyperNEAT ANNs tend to deform in contiguity-preserving and regularity-preserving ways (as seen in figure 1), thereby providing a useful bias (Gauci and Stanley, 2010; Stanley et al., 2009).", "startOffset": 270, "endOffset": 317}, {"referenceID": 19, "context": "Because the CPPN encoding is designed to describe patterns of weights across the geometry of the encoded network, the weights in HyperNEAT ANNs tend to deform in contiguity-preserving and regularity-preserving ways (as seen in figure 1), thereby providing a useful bias (Gauci and Stanley, 2010; Stanley et al., 2009).", "startOffset": 270, "endOffset": 317}, {"referenceID": 7, "context": "and has appeared in mainstream venues such as AAAI (Gauci and Stanley, 2008), Neural Computation (Gauci and Stanley, 2010), and JMLR (Verbancsics and Stanley, 2010).", "startOffset": 51, "endOffset": 76}, {"referenceID": 8, "context": "and has appeared in mainstream venues such as AAAI (Gauci and Stanley, 2008), Neural Computation (Gauci and Stanley, 2010), and JMLR (Verbancsics and Stanley, 2010).", "startOffset": 97, "endOffset": 122}, {"referenceID": 23, "context": "and has appeared in mainstream venues such as AAAI (Gauci and Stanley, 2008), Neural Computation (Gauci and Stanley, 2010), and JMLR (Verbancsics and Stanley, 2010).", "startOffset": 133, "endOffset": 164}, {"referenceID": 2, "context": "Unsupervised pretraining in deep learning has historically focused on approaches such as autoencoders and RBMs (Bengio et al., 2007; Hinton et al., 2006) that attempt to reconstruct training examples by first translating them into a basis of features different from the inputs, and then from those features regenerating the inputs.", "startOffset": 111, "endOffset": 153}, {"referenceID": 9, "context": "Unsupervised pretraining in deep learning has historically focused on approaches such as autoencoders and RBMs (Bengio et al., 2007; Hinton et al., 2006) that attempt to reconstruct training examples by first translating them into a basis of features different from the inputs, and then from those features regenerating the inputs.", "startOffset": 111, "endOffset": 153}, {"referenceID": 4, "context": "For example clustering algorithms such as K-means or Gaussian mixture models in effect extract structure from data that can then assist in classification; in fact at least one study has shown that such clustering algorithms can yield features as effective or more so for classification than autoencoders or RBMs (Coates et al., 2011).", "startOffset": 312, "endOffset": 333}, {"referenceID": 13, "context": "However, a critical facet of novelty-based searches that are combined with HyperNEAT-based neuroevolution is that the complexity of features (and hence distinctions) tends to increase over the run (Lehman and Stanley, 2011).", "startOffset": 197, "endOffset": 223}, {"referenceID": 12, "context": "To investigate this question DDFA is trained and tested on the MNIST handwritten digit recognition dataset (LeCun and Cortes, 1998), which consists of 60,000 training images and 10,000 test images.", "startOffset": 107, "endOffset": 131}, {"referenceID": 16, "context": "In particular, Simard et al. (2003) obtained one of the best such results of 1.", "startOffset": 15, "endOffset": 36}, {"referenceID": 9, "context": "2% result from Hinton et al. (2006) on a four-layer network pretrained by a RBM, would hint at DDFA\u2019s potential utility in the future for pretraining deeper networks.", "startOffset": 15, "endOffset": 36}, {"referenceID": 24, "context": "A simple variant of HyperNEAT called HyperNEAT-LEO (Verbancsics and Stanley, 2011) (which leads to less connectivity) was the main neuroevolution engine.", "startOffset": 51, "endOffset": 82}, {"referenceID": 2, "context": "This whole procedure is similar to how autoencoders are trained before gradient descent in deep learning (Bengio et al., 2007).", "startOffset": 105, "endOffset": 126}, {"referenceID": 6, "context": "The training and validation procedure mirrors that followed by Hinton et al. (2006): first training is run on 50,000 examples for 50 epochs to find the network that performs best on a 10,000-example validation set.", "startOffset": 63, "endOffset": 84}, {"referenceID": 16, "context": "6% error of the similar shallow network trained without preprocessing from Simard et al. (2003). In fact, the result for the 3,000-feature network even approaches the 1.", "startOffset": 75, "endOffset": 96}, {"referenceID": 9, "context": "2% error of the significantly deeper network of Hinton et al. (2006), showing that shallow networks can generalize surprisingly well by finding sufficiently high-quality feature sets, even despite a lack of exposure to distortions during training.", "startOffset": 48, "endOffset": 69}, {"referenceID": 21, "context": "For example, recent surprising results from Szegedy et al. (2013) show that very small yet anomalous perturbations of training images that are imperceptible to the human eye can fool several different kinds of deep networks that nevertheless ominously score well on the test set.", "startOffset": 44, "endOffset": 66}, {"referenceID": 0, "context": "At the same time, as Bengio (2013) points out, local descent on its own will not ultimately be enough to tackle the most challenging problems, suggesting the need for radical new kinds of optimization that are more global.", "startOffset": 21, "endOffset": 35}], "year": 2014, "abstractText": "Unlike unsupervised approaches such as autoencoders that learn to reconstruct their inputs, this paper introduces an alternative approach to unsupervised feature learning called divergent discriminative feature accumulation (DDFA) that instead continually accumulates features that make novel discriminations among the training set. Thus DDFA features are inherently discriminative from the start even though they are trained without knowledge of the ultimate classification problem. Interestingly, DDFA also continues to add new features indefinitely (so it does not depend on a hidden layer size), is not based on minimizing error, and is inherently divergent instead of convergent, thereby providing a unique direction of research for unsupervised feature learning. In this paper the quality of its learned features is demonstrated on the MNIST dataset, where its performance confirms that indeed DDFA is a viable technique for learning useful features.", "creator": "LaTeX with hyperref package"}}}