{"id": "1406.1853", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2014", "title": "Model-based Reinforcement Learning and the Eluder Dimension", "abstract": "We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as $\\tilde{O}(\\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is the Kolmogorov dimension and $d_E$ is the \\emph{eluder dimension}. This represents the first unified framework for model-based reinforcement learning and provides state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm \\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds.", "histories": [["v1", "Sat, 7 Jun 2014 03:02:09 GMT  (18kb)", "https://arxiv.org/abs/1406.1853v1", null], ["v2", "Fri, 31 Oct 2014 23:36:00 GMT  (22kb)", "http://arxiv.org/abs/1406.1853v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ian osband", "benjamin van roy"], "accepted": true, "id": "1406.1853"}, "pdf": {"name": "1406.1853.pdf", "metadata": {"source": "CRF", "title": "Model-based Reinforcement Learning and the Eluder Dimension", "authors": ["Ian Osband", "Benjamin Van Roy"], "emails": ["iosband@stanford.edu", "bvr@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.18 53v2 [st at.M L] 31 October 2 \u221a dKdET), where T is the elapsed time, dK is the Kolmogorov dimension and dE the Eluder dimension. These represent the first unified boundaries for model-based amplification learning and provide state-of-the-art guarantees in several important areas. In addition, we present a simple and computationally efficient algorithm posterior sampling for reinforcement learning (PSRL) that meets these boundaries."}, {"heading": "1 Introduction", "text": "We consider the enhancement of learning (RL) to be a problem of the optimization of rewards in an unknown Markov decision process (MDP) compared to the deficiency criteria. We consider the enhancement of learning (RL) to be a problem of the optimization of rewards in an unknown Markov decision process (MDP). In this context, an agent makes sequential decisions within his Enironment in order to maximize his cumulative rewards over time. We model the environment as an MDP planning problem, but unlike the standard MDP planning problem, the agent is uncertain of the underlying reward and transition functions. By researching poorly understood policies, an agent can improve his understanding of his environment, but it can improve his short-term rewards by exploiting his existing knowledge [2, 3].The focus of the literature in this area is the development of algorithms whose performance will in a sense be close to optimal coristers (there may be numerous statistical and computational criteria), which are most likely to be taken into account (e.g., there are statistical and numerous MMB)."}, {"heading": "2 Problem formulation", "text": "We consider the problem of learning to optimize a random finite horizon MDP M = (S, A, RM, PM, \u0440) in repeated finite episodes of interaction. S is the state space, A is the action space, RM (s, a) is the reward distribution over R and PM (s, a) is the transition distribution over S in selecting a sphere of action in state s, A is the time horizon and R is the initial state distribution. All random variables we will consider are on a probability space (s, F, P).A policy is a function mapping for each state s, S and i = 1,..., in response to an action that represents a pronounced M and policy, we define a value function V: V M\u00b5, i (s): = EM, \u00b5 (s)."}, {"heading": "3 Main results", "text": "The algorithm starts with a previous distribution via MDPs. At the beginning of episode k, PSRL sampling for Reinforcement Learning (PSRL) 1: Input: Prior distribution for M, t = 1: 2 for episodes k = 1,.. do 3: sample Mk. (\u00b7 Ht) 4: compute mk = \u00b5S 5: for timesteps j = 1:."}, {"heading": "4 Eluder dimension", "text": "To quantify the complexity of learning in a potentially infinite MDP, we extend the existing concept of the eluder dimension for real-world evaluated functions [19] to vector-evaluated functions. If we look at sequential observations yi \u00b2 G \u00b2 (xi), we can equate them as yi = f \u00b2 (xi) + i for some f \u00b2 (xi) = E [y | y \u00b2 G] for G \u00b2 (xi) and vice versa. Intuitively, the eluder dimension of F is the length of the longest possible sequence x1,.. xd so that for all i, knowing the functional values of f (x1),.., f (xi), do not show the independent dimensions of F (xi + 1)."}, {"heading": "4.1 Eluder dimension for specific function classes", "text": "Theorem 1 indicates the limits of regret in relation to the eluder dimension, which is well defined for each F function. However, the actual calculation of the eluder dimension may require additional analysis, and a counter argument shows that for some common function classes the eluder dimension has a similar approach to previous functions [14]. This evidence is available in Appendix C.Proposition 1 (eluder dimension for finite X functions). Leave F = {f | f (x) = finite, arbitrary components > 0 and each function class F: dimE (F,) \u2264 X This limit is narrow in the case of independent measurements."}, {"heading": "5 Confidence sets", "text": "We now follow the standard argument, which relates the regret of an optimistic or posterior sampling algorithm to the construction of confidence sets [7, 11]. We will use the Eluder dimension to create trust sets for reward and transition, which most likely contain the true functions, and then limit the regret of our algorithm by the maximum deviation within the trust sets. For observations from f \u00b2 F, we center the sets around the square prediction errors. The trust sets are defined Ft = Ft (\u03b2t): = {f \u00b2 F (f), where L2, t (f): = 1 \u00b2 f (xt) \u2212 i = 1 \u00b2 f (xt) \u2212 yt \u00b2 is the cumulative square prediction error. The trust sets are defined Ft = Ft (\u03b2t): = F (f), f \u2212 f \u00b2, St \u2212 f \u00b2, F \u00b2, F \u00b2, F \u00b2 F \u00b2, F \u00b2, F \u00b2 F \u00b2, F \u00b2, F \u00b2 F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F \u00b2, F, F \u00b2, F \u00b2, F, F \u00b2, F, F \u00b2, F, F \u00b2, F, F \u00b2, F, F, F \u00b2, F, F, F \u00b2, F, F, F \u00b2, F, F, F \u00b2, F, F, F, F \u00b2, F, F, F, F \u00b2, F, F, F, F, and F \u00b2, F \u00b2, F, F \u00b2, F, F, F, F, F, F, and F \u00b2."}, {"heading": "5.1 Bounding the sum of set widths", "text": "We now limit the deviation of f \u00b2 to the maximum deviation within the confidence set. Definition 4 (set widths): For each set of functions F \u00b2, we define the width of the set at x \u00b2 to be the maximum L2 deviation between any two members of F, which are evaluated at x.wF (x): = sup f, f \u00b2 F \u00b2 f (x) - f (x) - 2We can for the number of large widths in relation to the eluder dimension.Lemma 1 (limitation of the number of large widths).If {\u03b2t > 0, f \u00b2 t \u00b2 n \u00b2 have an undecreasing sequence with Ft = Ft (\u03b2t) then m \u00b2 k \u2212 t \u00b2 t \u00b2 s \u00b2 i = 11 {wFtk + i) > Lemm \u00b2 (4\u03b2T \u00b2) dimE (F \u00b2) dimE (F) dimE \u00b2 t \u00b2 s \u00b2 s (F), wt \u00b2 s \u00b2 T = 14 T \u00b2 T (but in a small result)."}, {"heading": "6 Analysis", "text": "We will now reproduce the decomposition of the expected regret in the sense of the Bellman error [11] From here, we will apply the confidence results of Section 5 to reach our limits of regret. We will streamline our discussion of PM, RM, V M\u00b5, i, U M i and T M\u00b5 by simply adding and subtracting the regret by adding and subtracting the imaginary optimum reward of \u00b5k under the MDP Mk. [K] (V) (V) (V) k) (V) k) (V) k (V) k) = (V) k (V) k) (V) k) = (V) k (V) k) (V), (V), (V), (V), (V), (V), (V), (V, K,), (K, (), (K,), (K, (), (K,), (K, (), (K,), (K,), (), (K, (), (K,), (), (K, (), (K,), (), (K, (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (, (), (), (), (), (), (), (, (), (), (), (), (), (), (), (), (), (, (), (), (), (), (, (), (, (), (), (, (, (), (), (), (, (, (), (), (), (, (, (), (), (, (, (), (, (), (), (, (), (), (, (), (), (, (, (), (), (, (), (, (, (), (), (, (, (), (), (), (,"}, {"heading": "6.1 Lipschitz continuity", "text": "A natural idea is that nearby states could have similar optimal values, or that the optimal value function could be Lipschitz in the sense of Equation (3). This will usually lead to uncertain value functions, so that this assumption is violated in many areas of interest. Since P -sub-Gaussian noise we write st + 1 = pM (st, at). We will now use Equation (12) to reduce repentance to a sum of set latitudes. To follow the notation of section 4 more closely, we will use xk + i, i = stk."}, {"heading": "7 Conclusion", "text": "We present a new analysis of the posterior sample for reinforcement learning, which leads to a general regret tied to the dimensionality rather than the cardinality of the underlying MDP. These are the first limits of regret for reinforcement learning in such a general environment, and provide new state-of-the-art guarantees when specializing in several important issues. However, there are some clear shortcomings that we do not address in the paper. First, we assume that it is possible to accurately take samples from posterior distribution, and in some cases this requires extensive computational effort. Second, we wonder whether it is possible to extend our analysis to learning in MDPs without episodic re-hiring. Finally, there is a fundamental hurdle for model-reinforcement learning that planning for optimal policy can be insoluble even in a known MDP. We are assuming access to an approximate MDP planner from, but this generally requires lengthy calculations."}, {"heading": "Acknowledgments", "text": "Osband is supported by Stanford Graduate Fellowships courtesy of PACCAR inc. This work was supported in part by the National Science Foundation's CMMI-0968707 award."}, {"heading": "A Confidence sets with high probability", "text": "In this appendix, we will provide proof for Proposition 5 that the confidence sets defined in Eq.7 are highly likely to be valid. We will begin with some basic results from the theory of martyrdom. \u2212 apRead more about this in the printed issue of the Allg\u00e4uer Zeitung on Thursday, 2 September. \u2212 apRead more about this in the Passauer Neue Presse (Issue Pocking / Bad F\u00fcssing / Bad Griesbach) \u2212 apRead more about this in the Passauer Neue Presse (Issue Pocking / Bad F\u00fcssing / Bad Griesbach) \u2212 apRead more about this in the Passauer Neue Presse (Issue Pocking / Bad F\u00fcssing / Bad Griesbach) \u2212 apRead more about this in the Passauer Neue Presse (Issue Pocking / Bad F\u00fcssing / Bad Griesbach) of Thursday, 2 September. \u2212 apbach (Issue Pocking / Bad F\u00fcssing / Bad Griesbach / Bad Griesbach)."}, {"heading": "B Bounding the number of large widths", "text": "Lemma 1 (Limitation of the number of large breadths) If 0 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "C Eluder dimension for specific function classes", "text": "In this section of the Annex, we will show the limits of the elusive dimension for some canonical function classes (cf. Definition 3, cf. Definition 2, cf. Definition 2, cf. Definition 2, cf. Definition 2, cf. Definition 2, cf. Definition 2, cf. Definition 2, cf. Definition V). (cf. Definition 2, cf. Definition V). (cf. Definition 2, cf. Definition V). (cf. Definition 2, cf. Definition V). (cf. Definition 2, cf. Definition V). (cf. Definition 2, cf. Definition V). (cf. Definition 2, cf. 2, cf. Definition V). (cf. 2, cf. (cf.) ll. (cf.). (v. (v). (v). (v)."}, {"heading": "D Bounded LQR control", "text": "s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s s s s s \u00b2 s s \u00b2 s \u00b2 s s s s \u00b2 s s \u00b2 s s s s s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s s s s s s s s s s s s s"}, {"heading": "E UCRL-Eluder", "text": "For the sake of completeness, we explicitly outline an optimistic algorithm that uses the confidence sets in our analysis of the PSFD to guarantee similar remorse limits with a high probability over all MDP M *. The algorithm follows the style of UCRL2 [7], so that at the beginning of the next episode the algorithm Mk = {M | PM, RM, Rk} forms and then resolves for the optimistic policy that provides the highest reward over each M in Mk algorithm 2 UCRL-Eluder1: Input: trust parameters \u03b4 > 0, t = 1 2: for episodes k = 1, 2,.. do 3: formal confidence sets Rk (\u03b2, GOP, 1 / k2) and Pk (\u03b2, CCRL, 1 / k2))) 4: Calculation \u00b5k of optimistic policy over Mk = {M | PM, Rk} 5: formal confidence sets Rk (R, CCS, 1 / k\u03b2), and CCRK (1 / k\u03b2), and CCK (1)."}], "references": [{"title": "Optimal adaptive policies for Markov decision processes", "author": ["Apostolos Burnetas", "Michael Katehakis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1985}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "arXiv preprint cs/9605103,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "A theory of the learnable", "author": ["Leslie G Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1984}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["Nick Littlestone"], "venue": "Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Lihong Li", "Michael L Littman", "Thomas J Walsh", "Alexander L Strehl"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen Brafman", "Moshe Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Pac modelfree reinforcement learning", "author": ["Alexander Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael Littman"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "More) Efficient Reinforcement Learning via Posterior Sampling", "author": ["Ian Osband", "Daniel Russo", "Benjamin Van Roy"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Learning to optimize via posterior sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "CoRR, abs/1301.2609,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Near-optimal regret bounds for reinforcement learning in factored MDPs", "author": ["Ian Osband", "Benjamin Van Roy"], "venue": "arXiv preprint arXiv:1403.3741,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Improved algorithms for linear stochastic bandits", "author": ["Yassin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Efficient reinforcement learning for high dimensional linear quadratic systems", "author": ["Morteza Ibrahimi", "Adel Javanmard", "Benjamin Van Roy"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Online regret bounds for undiscounted continuous reinforcement learning", "author": ["Ronald Ortner", "Daniil Ryabko"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Eluder dimension and the sample complexity of optimistic exploration", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William Thompson"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1933}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Malcom Strens"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Dynamic programming and optimal control, volume 1", "author": ["Dimitri Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction We consider the reinforcement learning (RL) problem of optimizing rewards in an unknown Markov decision process (MDP) [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Through exploring poorlyunderstood policies, an agent may improve its understanding of its environment but it may improve its short term rewards by exploiting its existing knowledge [2, 3].", "startOffset": 182, "endOffset": 188}, {"referenceID": 2, "context": "Through exploring poorlyunderstood policies, an agent may improve its understanding of its environment but it may improve its short term rewards by exploiting its existing knowledge [2, 3].", "startOffset": 182, "endOffset": 188}, {"referenceID": 3, "context": "Some of the most common include PAC (Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It Knows) [6] and regret [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "Some of the most common include PAC (Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It Knows) [6] and regret [7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Some of the most common include PAC (Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It Knows) [6] and regret [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "Some of the most common include PAC (Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It Knows) [6] and regret [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Algorithms of both type have been developed to provide PAC-MDP bounds polynomial in the number of states S and actions A [8, 9, 10].", "startOffset": 121, "endOffset": 131}, {"referenceID": 8, "context": "Algorithms of both type have been developed to provide PAC-MDP bounds polynomial in the number of states S and actions A [8, 9, 10].", "startOffset": 121, "endOffset": 131}, {"referenceID": 9, "context": "Algorithms of both type have been developed to provide PAC-MDP bounds polynomial in the number of states S and actions A [8, 9, 10].", "startOffset": 121, "endOffset": 131}, {"referenceID": 6, "context": "The only near-optimal regret bounds to time T of \u00d5(S \u221a AT ) have only been attained by modelbased algorithms [7, 11].", "startOffset": 109, "endOffset": 116}, {"referenceID": 10, "context": "The only near-optimal regret bounds to time T of \u00d5(S \u221a AT ) have only been attained by modelbased algorithms [7, 11].", "startOffset": 109, "endOffset": 116}, {"referenceID": 6, "context": "Worse still, there is a lower bound \u03a9( \u221a SAT ) for the expected regret in an arbitrary MDP [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 11, "context": "parameterization is the degenerate MDP with no transitions, the mutli-armed bandit [12, 13, 14].", "startOffset": 83, "endOffset": 95}, {"referenceID": 12, "context": "parameterization is the degenerate MDP with no transitions, the mutli-armed bandit [12, 13, 14].", "startOffset": 83, "endOffset": 95}, {"referenceID": 14, "context": "Papers here establigh regret bounds \u00d5( \u221a T ) for linear quadratic control [16], but with constants that grow exponentially with dimension.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Later works remove this exponential dependence, but only under significant sparsity assumptions [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "The most general previous analysis considers rewards and transitions that are \u03b1-H\u00f6lder in a d-dimensional space to establish regret bounds \u00d5(T ) [18].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "In this paper we analyse the simple and intuitive algorithm posterior sampling for reinforcement learning (PSRL) [20, 21, 11].", "startOffset": 113, "endOffset": 125}, {"referenceID": 19, "context": "In this paper we analyse the simple and intuitive algorithm posterior sampling for reinforcement learning (PSRL) [20, 21, 11].", "startOffset": 113, "endOffset": 125}, {"referenceID": 10, "context": "In this paper we analyse the simple and intuitive algorithm posterior sampling for reinforcement learning (PSRL) [20, 21, 11].", "startOffset": 113, "endOffset": 125}, {"referenceID": 19, "context": "PSRL was initially introduced as a heuristic method [21], but has since been shown to satisfy state of the art regret bounds in finite MDPs [11] and also exploit the structure of factored MDPs [15].", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "PSRL was initially introduced as a heuristic method [21], but has since been shown to satisfy state of the art regret bounds in finite MDPs [11] and also exploit the structure of factored MDPs [15].", "startOffset": 140, "endOffset": 144}, {"referenceID": 13, "context": "PSRL was initially introduced as a heuristic method [21], but has since been shown to satisfy state of the art regret bounds in finite MDPs [11] and also exploit the structure of factored MDPs [15].", "startOffset": 193, "endOffset": 197}, {"referenceID": 17, "context": "To characterize the complexity of this learning problem we extend the definition of the eluder dimension, previously introduced for bandits [19], to capture the complexity of the reinforcement learning problem.", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "However, we can represent the discrete state as a probability vector st \u2208 S = [0, 1] \u2282 R with a single active component equal to 1 and 0 otherwise.", "startOffset": 78, "endOffset": 84}, {"referenceID": 18, "context": "3 Main results We now review the algorithm PSRL, an adaptation of Thompson sampling [20] to reinforcement learning.", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "PSRL was first proposed by Strens [21] and later was shown to satisfy efficient regret bounds in finite MDPs [11].", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "PSRL was first proposed by Strens [21] and later was shown to satisfy efficient regret bounds in finite MDPs [11].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "(6) Here \u03bb1 is the largest eigenvalue of the matrix Q given as the solution of the Ricatti equations for the unconstrained optimal value function V (s) = \u2212sTQs [22].", "startOffset": 160, "endOffset": 164}, {"referenceID": 12, "context": "Algorithms based upon posterior sampling are intimately linked to those based upon optimism [14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "Further, we believe that PSRL will generally be more statistically efficient than an optimistic variant with similar regret bounds since the algorithm is not affected by loose analysis [11].", "startOffset": 185, "endOffset": 189}, {"referenceID": 17, "context": "4 Eluder dimension To quantify the complexity of learning in a potentially infinite MDP, we extend the existing notion of eluder dimension for real-valued functions [19] to vector-valued functions.", "startOffset": 165, "endOffset": 169}, {"referenceID": 17, "context": "In fact, a family learnable in constant time for supervised learning may require arbitrarily long to learn to control well [19].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "We now provide bounds on the eluder dimension for some common function classes in a similar approach to earlier work for real-valued functions [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 6, "context": "5 Confidence sets We now follow the standard argument that relates the regret of an optimistic or posterior sampling algorithm to the construction of confidence sets [7, 11].", "startOffset": 166, "endOffset": 173}, {"referenceID": 10, "context": "5 Confidence sets We now follow the standard argument that relates the regret of an optimistic or posterior sampling algorithm to the construction of confidence sets [7, 11].", "startOffset": 166, "endOffset": 173}, {"referenceID": 12, "context": "The argument is essentially the same as Proposition 6 in [14], but extends statements about R to vector-valued functions.", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "This result follows from proposition 8 in [14] but with a small adjustment to account for episodes.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "Once again we follow the analysis of Russo [14] and strealine notation by letting wt = wFtk (xtk+i) abd d = dimE(F , T ).", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "6 Analysis We will now show reproduce the decomposition of expected regret in terms of the Bellman error [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Through repeated application of the dynamic programming operator and taking expectation of martingale differences we can mirror earlier analysis [11] to equate expected regret with the cumulative Bellman error: E[\u2206k] = \u03c4", "startOffset": 145, "endOffset": 149}], "year": 2014, "abstractText": "We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as \u00d5( \u221a dKdET ) where T is time elapsed, dK is the Kolmogorov dimension and dE is the eluder dimension. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm posterior sampling for reinforcement learning (PSRL) that satisfies these bounds.", "creator": "LaTeX with hyperref package"}}}