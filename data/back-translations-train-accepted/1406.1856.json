{"id": "1406.1856", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2014", "title": "A Drifting-Games Analysis for Online Learning and Applications to Boosting", "abstract": "We provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework. Different online learning settings (Hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games. The original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function. With different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties. Moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates. Finally, we translate our new Hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round.", "histories": [["v1", "Sat, 7 Jun 2014 03:11:05 GMT  (46kb)", "https://arxiv.org/abs/1406.1856v1", null], ["v2", "Thu, 30 Oct 2014 17:40:59 GMT  (47kb)", "http://arxiv.org/abs/1406.1856v2", "In NIPS2014"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haipeng luo", "robert e schapire"], "accepted": true, "id": "1406.1856"}, "pdf": {"name": "1406.1856.pdf", "metadata": {"source": "CRF", "title": "A Drifting-Games Analysis for Online Learning and Applications to Boosting", "authors": ["Haipeng Luo", "Robert E. Schapire"], "emails": ["haipengl@cs.princeton.edu", "schapire@cs.princeton.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.18 56v2 [cs.LG] 3 0"}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in"}, {"heading": "2 Reviewing Drifting Games", "text": "We are looking at a simplified version of drifting similar to that described in [29, Fig. 13] (also called chip games), which runs through T rounds and is played between a player and an opponent controlling N chips on the real line. The positions of these chips at the end of the round t are denoted by st-RN, with each coordinate st, i corresponding to the position of the chip i. Initially, all chips are set to position 0, so that s0 = 0. In each round t = 1,.., T: the player first selects a pt distribution over the chips, then the opponent decides on the chip movements zt, so that thenew positions are updated as st-st \u2212 zt. Here, each zt chip must be selected from a given set of B-R, and more importantly, satisfying the condition pt-zt-0 for some fixed constant \u03b2. At the end of the game, each chip is associated with a non-negative loss defined by L (sT-i)."}, {"heading": "3 Online Learning as a Drifting Game", "text": "The link between drift games and some specific settings of online learning has been noted before ([28, 23]). We aim to find deeper connections or even equivalence between variants of drift games and more general settings of online learning and provide insights into the design of learning algorithms through Minimax analysis. We start with a simple but classic hedge setting."}, {"heading": "3.1 Algorithmic Equivalence", "text": "In the hedge setting [14], a player tries to earn as much as possible (or lose as little as possible) by distributing a fixed amount of money to bet on a series of actions each day. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "3.2 Relaxations", "text": "From now on, we will focus only on the direction of converting a drifting game algorithm into a hedge algorithm (1). To derive a minimax hedge algorithm from this, Theorem 1 tells us that it is sufficient to derive minimax DGv1 algorithms from it. Precise minimax analysis is usually difficult, and appropriate loosening seems to be necessary. In order to use the existing analysis for standard drift games, the first obvious loosening is proposed to remove the additional restriction in DGv1, i.e., i \u2212 zt, j | 1 for all i and j. This will lead to the exact setting discussed in [23], where an almost optimal strategy is proposed using the recipe in Eq. (1) It turns out that this loosening is reasonable and does not give too much more power to the opponent."}, {"heading": "3.3 Designing Potentials and Algorithms", "text": "We are now ready to restore existing algorithms and develop new ones by selecting a suitable potential \u03a6T (s), as Algorithm 3 suggests. We will discuss three different algorithms and summarize these examples in Table 1 (see Appendix C). It turns out that this will lead to the known exponential weight algorithm [14, 15]. Specifically, we will select the exact replacement function of the 0-1 loss function in the literature, which has exactly upper limits 1 {s \u2264 \u2212 R}. To calculate these, we simply need to select the exponential weight algorithms [14, 15]."}, {"heading": "3.4 High Probability Bounds", "text": "s regret about the best action: RT (i1: T, 1: T) = \u2211 T = 1: t, es \u2212 mini \u2211 T = 1: t, i. Note that regret is now a random variable, and we are interested in a limit that is likely to hold. By looking at the inequality of Azuma, the standard analysis (see for example [9, Lemma 4.1]) shows that the player can simply draw it according to pt = H (1: t \u2212 1), and the output of a standard hedge algorithm, and suffers from regret in most RT (H) + \u221a T with probability 1 \u2212 zt. Below, we note similar results as a simple by-product of our drift analyses."}, {"heading": "4 Generalizations and Applications", "text": "The only difference between hedge (randomized version) and the non-stochastic MAB problem (6) is that in each round an unbiased estimator of the losses can be formed, which in this case might not be able to capture the entire vector. (7) The goal is to compete with the best action. (8) Then algorithms such as EXP can be used by replacing them with an unbiased estimator leading to the EXP3 algorithms. (6) One might expect that algorithm 3 would also work by showing an important property of movements, i: boundedness. (3)"}, {"heading": "A Summary of Drifting Game Variants", "text": "2. DGv1Given: a loss function L (s) = 1 {s \u2264 \u2212 R}. For t = 1,.., T: 1. The player chooses a distribution pt over N chips. 2. The opponent determines the movement of each chip zt, i (s) [\u2212 1, 1] subject to pt \u00b7 zt \u2265 0 and | zt, i \u2212 zt, j \u2212 zt 1 for all i and j. The player suffers a loss \u2211 N i = 1 L (\u2211 T = 1 zt, i).DGv2Given: a loss function L (s) = 1 {s \u2264 \u2212 R}. For t = 1 (..., T: 1.) the player chooses a distribution pt over N chips. 2. The opponent randomly decides the movement of each chip."}, {"heading": "B Proof of Theorem 1", "text": "The proof: We first show that both conversions are valid. In Algorithm 1 it is clear that \"t,\" \"i,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t. \"In Algorithm 1 it is clear that\" t \"t,\" i, \"\" t, \"i,\" t, \"i,\" i, \"t.,\" t."}, {"heading": "C Summary of Hedge Algorithms and Proofs of Lemma 1, Lemma 2 and", "text": "It is sufficient to show [s \u2212 1] 2 \u2212 \u03b2\u03b2k = \u03b2\u03b21 (s + 1) 2 \u2212 \u2264 2 [s] 2 \u2212 + 2. If s \u2265 0, LHS = [s \u2212 1] 2 \u2212 \u03b2\u03b21 < 2 = RHS. If s < 0, LHS \u2264 (s + 1) 2 + (s + 1) 2 = 2s2 + 2 = RHS.Proof of Lemma 2. Leave F (s) = exp (s \u2212 1] 2 \u2212 dt) + exp (s + 1] 2 \u2212 dt (s) 2 \u2212 dt (s) 2 \u2212 d (t \u2212 1). It is sufficient to show that F (s) \u2264 2 (bt \u2212 bt \u2212 1) = exp (4dt) \u2212 1, which clearly applies to the following 3 cases: F (s) = 0, exp (s) = 1; exp (s) (s \u2212 1) 2 dt."}, {"heading": "D A General MAB Algorithm and Regret Bounds", "text": "Input: A convex, non-increasing, non-negative function \u03a6T (s) - C2, with non-increasing second derivative. For t = T down to 1 do Find a convex function \u03a6t \u2212 1 (s) s.t. The conditions of Theorem 4 apply. Set: s0 = 0. for t = 1 to T doSet: pt, i = es \u00b7 t (st \u2212 1, i \u2212 1) \u2212 t (st \u2212 1, i + 1). Set: zt, i = 1 {i = es \u00b7 t, it \u2212 t, it \u2212 t (st \u2212 1, i \u2212 1). Set: st = st \u2212 1 + zt.Algorithm 4: A General MAB AlgorithmTheorem 4. Suppose t (s) is convex, distinguishable twice (i.e.), i, we have the second derivative not, and satisfactory: (1)."}, {"heading": "E A General OCO Algorithm and Regret Bounds", "text": "Input: A convex, non-increasing, non-negative function \u03a6T (s) for t = T up to 1 doFind a convex function \u03a6t \u2212 1 (s) s.t. (s) \u2212 p (s) \u2212 p (s) \u2212 p (s) \u2212 p (s) \u2212 p (s) \u2212 p (s). Set: s0 (x). Receivable loss function ft from opponent. Set: zt (x) = ft (x) \u2212 p (x). Set: st (st) \u2212 p (st) \u2212 1 (st \u2212 1 (x) \u2212 p (st \u2212 1) \u2212 p (st \u2212 1) \u2212 p (st \u2212 1 (x). Set: zt (x) = ft (x) \u2212 ft (xt). Set: st (st) \u2212 1 (x)."}, {"heading": "F NH-Boost.DT, NH-Boost and Proof of Theorem 3", "text": "Input: Training examples (xi, yi): Training examples (xi, yi): Training examples (xi, yi): Training examples (xi, yi): Training examples (xi, yi): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): Training examples (x): (training examples): (x): (training examples): (x): (training examples): (x): (training examples): (x): (x): (training examples): (x): (x): (training examples): (x): (x): (training examples): (x): (x): (x): (training examples): (x): (x): (x): (x): (x): (x): (x): (x): (): (x): (x): (): (x): (x): (): (x): (x): (): (x):: (x): (x): (x): (x):: (x): (x):: (x): (x: ():: (x): (x): (): (x): (x): (x): (x)::: (x):: (x): (x):: (x): (x):: (x):: (x): (x):: (x):: (x):: (x):: (x):: (x):"}, {"heading": "G Experiments in a Boosting Setting", "text": "We conducted experiments to compare the performance of three binary classification boosting algorithms: AdaBoost [14], NH-Boost (algorithm 7) and NH-Boost.DT (algorithm 6), using a range of benchmark data available from UCI repository 3 and LIBSVM datasets4. Some datasets are pre-processed according to [27]. The number of features, training examples and test examples can be found in Table 2.All features are binary. The weak learning algorithm is a simple (exhaustive) decision stump (see about [29]). In each round, the weak learning algorithm lists all features, and for each feature calculates the weighted error of the corresponding stump on the weighted training examples."}], "references": [{"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Minimax games with bandits", "author": ["Jacob Abernethy", "Manfred K. Warmuth"], "venue": "In Proceedings of the 22st Annual Conference on Learning Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Repeated games against budgeted adversaries", "author": ["Jacob Abernethy", "Manfred K. Warmuth"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Regret bounds and minimax policies under partial monitoring", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "How to use expert advice", "author": ["Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P. Helmbold", "Robert E. Schapire", "Manfred K. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Potential-based algorithms in on-line prediction and game theory", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A parameter-free hedging algorithm", "author": ["Kamalika Chaudhuri", "Yoav Freund", "Daniel Hsu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Prediction with advice of unknown number of experts", "author": ["Alexey Chernov", "Vladimir Vovk"], "venue": "arXiv preprint arXiv:1006.0475,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Universal portfolios", "author": ["Thomas M. Cover"], "venue": "Mathematical Finance,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1991}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Yoav Freund"], "venue": "Information and Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Additive logistic regression: A statistical view of boosting", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Anytime algorithms for multi-armed bandit problems", "author": ["Robert Kleinberg"], "venue": "In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Online decision problems with large strategy sets", "author": ["Robert David Kleinberg"], "venue": "PhD thesis,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Towards Minimax Online Learning with Unknown Time Horizon", "author": ["Haipeng Luo", "Robert E. Schapire"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations", "author": ["H Brendan McMahan", "Francesco Orabona"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Learning with continuous experts using drifting games", "author": ["Indraneel Mukherjee", "Robert E. Schapire"], "venue": "Theoretical Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Random walk approach to regret minimization", "author": ["Hariharan Narayanan", "Alexander Rakhlin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Simultaneous model selection and optimization through parameter-free stochastic learning", "author": ["Francesco Orabona"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Relax and localize: From value to algorithms", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "How boosting the margin can also boost classifier complexity", "author": ["Lev Reyzin", "Robert E. Schapire"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Drifting games", "author": ["Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Boosting: Foundations and Algorithms", "author": ["Robert E. Schapire", "Yoav Freund"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "This is possible for some special cases ([7, 1, 3, 21]) but difficult in general.", "startOffset": 41, "endOffset": 54}, {"referenceID": 0, "context": "This is possible for some special cases ([7, 1, 3, 21]) but difficult in general.", "startOffset": 41, "endOffset": 54}, {"referenceID": 2, "context": "This is possible for some special cases ([7, 1, 3, 21]) but difficult in general.", "startOffset": 41, "endOffset": 54}, {"referenceID": 20, "context": "This is possible for some special cases ([7, 1, 3, 21]) but difficult in general.", "startOffset": 41, "endOffset": 54}, {"referenceID": 13, "context": "On the other hand, many other efficient algorithms with optimal regret rate (but not exactly minimax optimal) have been proposed for different learning settings (such as the exponential weights algorithm [14, 15], and follow the perturbed leader [18]).", "startOffset": 204, "endOffset": 212}, {"referenceID": 14, "context": "On the other hand, many other efficient algorithms with optimal regret rate (but not exactly minimax optimal) have been proposed for different learning settings (such as the exponential weights algorithm [14, 15], and follow the perturbed leader [18]).", "startOffset": 204, "endOffset": 212}, {"referenceID": 17, "context": "On the other hand, many other efficient algorithms with optimal regret rate (but not exactly minimax optimal) have been proposed for different learning settings (such as the exponential weights algorithm [14, 15], and follow the perturbed leader [18]).", "startOffset": 246, "endOffset": 250}, {"referenceID": 25, "context": "[26] built a bridge between these two classes of methods by showing that many existing algorithms can indeed be derived from a minimax analysis followed by a series of relaxations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Drifting games [28] (reviewed in Section 2) generalize Freund\u2019s \u201cmajority-vote game\u201d [13] and subsume some well-studied boosting and online learning settings.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "Drifting games [28] (reviewed in Section 2) generalize Freund\u2019s \u201cmajority-vote game\u201d [13] and subsume some well-studied boosting and online learning settings.", "startOffset": 85, "endOffset": 89}, {"referenceID": 27, "context": "A nearly minimax optimal algorithm is proposed in [28].", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "We then generalize the minimax analysis in [28] based on one key idea: relax a 0-1 loss function by a convex surrogate.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "Hedge Settings: (Section 3) The Hedge problem [14] investigates how to cleverly bet across a set of actions.", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "3) bears some similarities with the NormalHedge algorithm [10] and enjoys a similar \u01eb-regret bound simultaneously for all \u01eb and horizons.", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "Our analysis is also arguably simpler and more intuitive than the one in [10] and easy to be generalized to more general settings.", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "Multi-armed Bandit Problems: (Section 4) The multi-armed bandit problem [6] is a classic example for learning with incomplete information where the learner can only obtain feedback for the actions taken.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Again the minimax analysis is generalized and the EXP3 algorithm [6] is recovered.", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "Our results could be seen as a preliminary step to answer the open question [2] on exact minimax optimal algorithms for the multi-armed bandit problem.", "startOffset": 76, "endOffset": 79}, {"referenceID": 11, "context": "Fortunately, it turns out that all results from the Hedge setting are ready to be used here, recovering the continuous EXP algorithm [12, 17, 24] and also generalizing our new algorithms to this general setting.", "startOffset": 133, "endOffset": 145}, {"referenceID": 16, "context": "Fortunately, it turns out that all results from the Hedge setting are ready to be used here, recovering the continuous EXP algorithm [12, 17, 24] and also generalizing our new algorithms to this general setting.", "startOffset": 133, "endOffset": 145}, {"referenceID": 23, "context": "Fortunately, it turns out that all results from the Hedge setting are ready to be used here, recovering the continuous EXP algorithm [12, 17, 24] and also generalizing our new algorithms to this general setting.", "startOffset": 133, "endOffset": 145}, {"referenceID": 28, "context": "Boosting: (Section 4) Realizing that every Hedge algorithm can be converted into a boosting algorithm ([29]), we propose a new boosting algorithm (NH-Boost.", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "DT is then translated into training error and margin distribution bounds that previous analysis in [29] using nonadaptive algorithms does not show.", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "Similar concepts have widely appeared in the literature [8, 5], but unlike our work, they are not related to any minimax analysis and might be hard to interpret.", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": "Similar concepts have widely appeared in the literature [8, 5], but unlike our work, they are not related to any minimax analysis and might be hard to interpret.", "startOffset": 56, "endOffset": 62}, {"referenceID": 10, "context": "The existence of parameter free Hedge algorithms for unknown number of actions was shown in [11], but no concrete algorithms were given there.", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "Boosting algorithms that ignore some examples on each round were studied in [16], where a heuristic was used to ignore examples with small weights and no theoretical guarantee is provided.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "For instance, binary classification via boosting can be translated into a drifting game by treating each training example as a chip (see [28] for details).", "startOffset": 137, "endOffset": 141}, {"referenceID": 27, "context": "A nearly optimal strategy and its analysis is originally given in [28], and a derivation by directly tackling the above minimax expression can be found in [29, chap.", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": "(2) It has been shown in [28] that this upper bound on the loss is optimal in a very strong sense.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": "With the loss function L(s) being 1{s \u2264 0}, these can be further simplified and eventually give exactly the boost-by-majority algorithm [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 27, "context": "3 Online Learning as a Drifting Game The connection between drifting games and some specific settings of online learning has been noticed before ([28, 23]).", "startOffset": 146, "endOffset": 154}, {"referenceID": 22, "context": "3 Online Learning as a Drifting Game The connection between drifting games and some specific settings of online learning has been noticed before ([28, 23]).", "startOffset": 146, "endOffset": 154}, {"referenceID": 13, "context": "1 Algorithmic Equivalence In the Hedge setting [14], a player tries to earn as much as possible (or lose as little as possible) by cleverly spreading a fixed amount of money to bet on a set of actions on each day.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "action i incurs loss lt,i \u2208 [0, 1]) which are revealed to the player.", "startOffset": 28, "endOffset": 34}, {"referenceID": 19, "context": "Here, we consider an even more general notion of regret studied in [20, 19, 10, 11], which we call \u01eb-regret.", "startOffset": 67, "endOffset": 83}, {"referenceID": 18, "context": "Here, we consider an even more general notion of regret studied in [20, 19, 10, 11], which we call \u01eb-regret.", "startOffset": 67, "endOffset": 83}, {"referenceID": 9, "context": "Here, we consider an even more general notion of regret studied in [20, 19, 10, 11], which we call \u01eb-regret.", "startOffset": 67, "endOffset": 83}, {"referenceID": 10, "context": "Here, we consider an even more general notion of regret studied in [20, 19, 10, 11], which we call \u01eb-regret.", "startOffset": 67, "endOffset": 83}, {"referenceID": 22, "context": "Doing this will lead to the exact setting discussed in [23] where a near optimal strategy is proposed using the recipe in Eq.", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "To see this, first recall that results from [23], written in our notation, state that minDR LT (DR) \u2264 1 2T \u2211 T\u2212R 2 j=0 ( T+1 j ) , which, by Hoeffding\u2019s inequality, is upper bounded by 2 exp ( \u2212 (R+1) 2 2(T+1) )", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "However, the algorithm proposed in [23] is not computationally efficient since the potential functions \u03a6t(s) do not have closed forms.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "It turns out that this will lead to the well-known exponential weights algorithm [14, 15].", "startOffset": 81, "endOffset": 89}, {"referenceID": 14, "context": "It turns out that this will lead to the well-known exponential weights algorithm [14, 15].", "startOffset": 81, "endOffset": 89}, {"referenceID": 25, "context": "More importantly, as in [26], this derivation may shed light on why this algorithm works and where it comes from, namely, a minimax analysis followed by a series of relaxations, starting from a reasonable surrogate of the 0-1 loss function.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "We call this the \u201c2-norm\u201d algorithm since it resembles the p-norm algorithm in the literature when p = 2 (see [9]).", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "In fact, our algorithm bears a striking similarity to NormalHedge [10], the first algorithm that has this kind of adaptivity.", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "Similar potential was also proposed in recent work [22, 25] for a different setting.", "startOffset": 51, "endOffset": 59}, {"referenceID": 24, "context": "Similar potential was also proposed in recent work [22, 25] for a different setting.", "startOffset": 51, "endOffset": 59}, {"referenceID": 9, "context": "DT is more computationally efficient especially when N is very large, since it does not need a numerical search for each round; 2) our analysis is arguably simpler and more intuitive than the one in [10]; 3) as we will discuss in Section 4, NormalHedge.", "startOffset": 199, "endOffset": 203}, {"referenceID": 9, "context": "DT can be easily extended to deal with the more general online convex optimization problem where the number of actions is infinitely large, while it is not clear how to do that for NormalHedge by generalizing the analysis in [10].", "startOffset": 225, "endOffset": 229}, {"referenceID": 5, "context": "4 Generalizations and Applications Multi-armed Bandit (MAB) Problem: The only difference between Hedge (randomized version) and the non-stochastic MAB problem [6] is that on each round, after picking it, the player only sees the loss for this single action lt,it instead of the whole vector lt.", "startOffset": 159, "endOffset": 162}, {"referenceID": 5, "context": "Then algorithms such as EXP can be used by replacing lt with l\u0302t, leading to the EXP3 algorithm [6] with regret O( \u221a TN lnN).", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "We conjecture, however, that there is a potential function that could recover the poly-INF algorithm [4, 5] or give its variants that achieve the optimal regret O( \u221a TN).", "startOffset": 101, "endOffset": 107}, {"referenceID": 4, "context": "We conjecture, however, that there is a potential function that could recover the poly-INF algorithm [4, 5] or give its variants that achieve the optimal regret O( \u221a TN).", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "Let S \u2282 R be a compact convex set, and F be a set of convex functions with range [0, 1] on S.", "startOffset": 81, "endOffset": 87}, {"referenceID": 29, "context": "There are two general approaches to OCO: one builds on convex optimization theory [30], and the other generalizes EXP to a continuous space [12, 24].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "There are two general approaches to OCO: one builds on convex optimization theory [30], and the other generalizes EXP to a continuous space [12, 24].", "startOffset": 140, "endOffset": 148}, {"referenceID": 23, "context": "There are two general approaches to OCO: one builds on convex optimization theory [30], and the other generalizes EXP to a continuous space [12, 24].", "startOffset": 140, "endOffset": 148}, {"referenceID": 16, "context": "Nevertheless, this is addressed by Theorem 6 using similar techniques in [17], giving the usual O( \u221a dT lnT ) regret bound.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Applications to Boosting: There is a deep and well-known connection between Hedge and boosting [14, 29].", "startOffset": 95, "endOffset": 103}, {"referenceID": 28, "context": "Applications to Boosting: There is a deep and well-known connection between Hedge and boosting [14, 29].", "startOffset": 95, "endOffset": 103}, {"referenceID": 28, "context": "DT converges to the optimal margin 2\u03b3; this is known not to be true for AdaBoost (see [29]).", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "References [1] Jacob Abernethy, Peter L.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Jacob Abernethy and Manfred K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Jacob Abernethy and Manfred K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Jean-Yves Audibert and S\u00e9bastien Bubeck.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Jean-Yves Audibert, S\u00e9bastien Bubeck, and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Peter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, and Robert E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Nicol\u00f2 Cesa-Bianchi, Yoav Freund, David Haussler, David P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Alexey Chernov and Vladimir Vovk.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Thomas M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Yoav Freund.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Yoav Freund and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Yoav Freund and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Jerome Friedman, Trevor Hastie, and Robert Tibshirani.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Elad Hazan, Amit Agarwal, and Satyen Kale.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Adam Kalai and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Robert Kleinberg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Robert David Kleinberg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Haipeng Luo and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] H Brendan McMahan and Francesco Orabona.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Indraneel Mukherjee and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Hariharan Narayanan and Alexander Rakhlin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Francesco Orabona.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Lev Reyzin and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For Algorithm 2, zt,i lies in B = [\u22121, 1] since lt,i \u2208 [0, 1], and direct computation showspt\u00b7zt = 0 \u2265 \u03b2(= 0) and |zt,i\u2212zt,j| = |lt,i\u2212lt,j| \u2264 1 for all i and j.", "startOffset": 55, "endOffset": 61}, {"referenceID": 29, "context": "The player\u2019s strategy is thus pt,i \u221d exp(\u2212\u03b7 \u2211t\u22121 \u03c4=1 l\u0302\u03c4,i) (recall l\u0302t,i = 1{i = it} \u00b7 lt,it/pt,it is the estimated loss), which is exactly the same as EXP3 (in fact a simplified version of the original EXP3, see for example [30]).", "startOffset": 226, "endOffset": 230}, {"referenceID": 13, "context": "G Experiments in a Boosting Setting We conducted experiments to compare the performance of three boosting algorithms for binary classification: AdaBoost [14], NH-Boost (Algorithm 7) and NH-Boost.", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "Some datasets are preprocessed according to [27].", "startOffset": 44, "endOffset": 48}, {"referenceID": 28, "context": "The weak learning algorithm is a simple (exhaustive) decision stump (see for instance [29]).", "startOffset": 86, "endOffset": 90}], "year": 2014, "abstractText": "We provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework. Different online learning settings (Hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games. The original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function. With different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties. Moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates. Finally, we translate our new Hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round.", "creator": "LaTeX with hyperref package"}}}