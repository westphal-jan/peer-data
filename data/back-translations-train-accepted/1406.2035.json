{"id": "1406.2035", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2014", "title": "Learning Word Representations with Hierarchical Sparse Coding", "abstract": "We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We apply an efficient online and distributed learning method. Experiments on various benchmark tasks---word similarity ranking, analogies, sentence completion, and sentiment analysis---demonstrate that the method outperforms or is competitive with state-of-the-art neural network representations. Our word representations are available at \\url{", "histories": [["v1", "Sun, 8 Jun 2014 22:35:09 GMT  (1145kb,D)", "http://arxiv.org/abs/1406.2035v1", null], ["v2", "Thu, 6 Nov 2014 14:26:21 GMT  (1142kb,D)", "http://arxiv.org/abs/1406.2035v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["dani yogatama", "manaal faruqui", "chris dyer", "noah a smith"], "accepted": true, "id": "1406.2035"}, "pdf": {"name": "1406.2035.pdf", "metadata": {"source": "CRF", "title": "Learning Word Representations with Hierarchical Sparse Coding", "authors": ["Dani Yogatama", "Manaal Faruqui", "Chris Dyer", "Noah A. Smith"], "emails": ["dyogatama@cs.cmu.edu", "mfaruqui@cs.cmu.edu", "cdyer@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "When machine learning is applied to text, the classical categorization of words as indexes of a vocabulary is unable to capture syntactic and semantic similarities that are easily discernible in data (e.g., pretty, beautiful and beautiful have similar meanings, as opposed to unattractive, ugly and repellent). In contrast, newer approaches to word representation use neural networks to obtain dense, low-dimensional, continuous embedding of words [18, 3, 7, 13, 15]. In this work, we propose an alternative approach based on the decomposition of a high-dimensional matrix that captures surface statistics of association between a word and its \"contexts\" with sparse coding. As in the past, contexts are words that occur near running texts [26]. Learning is performed by minimizing a reconstruction loss function to find the best factorization of the matrix."}, {"heading": "2 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Background and Notation", "text": "The observable representation of the word v is understood as a vector xv \u0445RC of coexistence statistics with C different contexts. Most often, each context is a possible adjacent word within a fixedar Xiv: 140 6,20 35v1 [cs.CL] 8 Jun 2fendow.1 Following many others, we leave xv, c the meaningless mutual information (PMI) between the occurrence of the context word c within a five-word window of occurrence of the word v [26, 19, 4].In sparse coding, the goal is to represent each input vector x-RC as a sparse linear combination of base vectors. In view of a stacked input matrix X-RC \u00d7 V, in which V is the number of words, we try to minimize the regulators: arg min D-D, A-DA-22 + ecular (A), in which D-RC \u00d7 M is the dictionary of the basis of the calculation unit > the A, which is the most common one with the D-sparing unit."}, {"heading": "2.2 Structured Regularization for Word Representations", "text": "In this context, it should be noted that this is a very complex and complex matter."}, {"heading": "2.3 Learning", "text": "The function is not convex in relation to D \u2212 \u2212 \u2212 and A \u2212, but it is convex in relation to both when the other is fixed. Alternating minimization routines have proved quite good in practice for such problems [10], but they are too expensive here due to (i) the size of X \u2212 RC \u00b7 V (C and V are respectively in the order of 105) and (ii) the many overlapping groups in the structured regulator (A). Our solution is based on the online dictionary learning method of Mairal et al. [12]. For T iterations, we try (i) a mini batch of words and (parallel) solve an a for each using the method of alternating directions of Valizer (Valizer, shown to work well for group overlapping [20, 28]; then (ii) we update the word D using the fixed block of Valix for all."}, {"heading": "3 Experiments", "text": "We present a controlled comparison of the Forest Regulator with several strong word representations learned on a fixed data set across multiple tasks. In \u00a7 3.4 we compare publicly available word vectors trained on different data.2Since our groups form tree structures, other methods such as FISTA [2] could also be used."}, {"heading": "3.1 Setup and Baselines", "text": "The size of our vocabulary is 180.834.4In our experiments, we use forests similar to those in Figure 1 to organize the latent word space. Note that the example has 26 nodes (2 trees). We choose to evaluate performance with M = 52 (4 trees) and M = 520 (40 trees).5 We refer to the sparse coding method as regular '1 punishment by SC, and our method as structured regulation (\u00a7 2.2) by FOREST. We use performance with M = 52 (4 trees) and M = 520 (40 trees).5 We compare the sparse coding method with regular' 1 punishment by SC, and our method with structured regulation (\u00a7 2.2) by FOREST. We use it as a minibatch of size 20,000 in both cases. We compare it with the following baseline methods: \u2022 Turney and Pantel [26]: Main Component Analysis (PCA) by truncated decomposition > X."}, {"heading": "3.2 Evaluation", "text": "We evaluate the following benchmark tasks. Word similarity The first task evaluates how well the representations capture word similarity. For example, beautiful and beautiful should be closer to the distance than beautiful and unattractive. We evaluate a number of word similarity data sets, some of which have been considered in the previous paper (more details are given in complementary materials). Following standard practices, we calculate cosmic distances between word pairs in word similarity data sets, then rank and report Spearman's rank correlation coefficient [24] between the model rankings and human rankings. Syntactic and semantic analogies The second assessment of the data sets is two analogies proposed by Mikolov et al. These questions evaluate syntactic and semantic relationships between words."}, {"heading": "3.3 Results", "text": "Table 1 shows results for all evaluation tasks for M = 52 and M = 520. In the tasks of similarity and sentiment analysis, our method performed best for both low and high-dimensional embedding. In the challenge of sentence completion, our method performed best in the high-dimensional case and second best in the low-dimensional case. While it lags behind other methods in analogy tasks, its second or third placement is competitive. It is important that FOREST exceeds the unstructured sparse coding (SC) for each task."}, {"heading": "3.4 Other Comparisons", "text": "In Table 2, we compare four basic methods for which pre-trained 50-dimensional word representations are available: \u2022 Collobert et al. [3]: a language model for neural networks (CW) based on Wikipedia data.7 \u2022 Huang et al. [7]: a neural network model that uses additional global document context (RNN-DC).86 We would like to point out that some of the models based on matrix decomposition can directly calculate the ratings of context words to which a possible answer is given [15]. We select average similarities for a fair comparison of representations. 7http: / / ronan.collobert.com / senna / 8 http: / / www.socher.org / index.php / Main / ImprovingWordRepresentationsViaGlobalContextAndMultipleWordPrototypes / These methods have all been trained at different companies so that they are not always included in the comparative tasks."}, {"heading": "3.5 Discussion", "text": "Our method produces sparse word representations with exact zeros. We observe that the sparse coding method produces sparse representations without a structured regulator, but it results in a deterioration of our assessment tasks, suggesting that it excludes meaningful dimensions. For FOREST with M = 52 and M = 520, the average numbers of unequal entries are 91% and 85%, respectively. While our word representations are not extremely sparse, this makes intuitive sense as we try to represent about 180,000 contexts in only 52 (520) dimensions (we also have no settings of M, we get sparser representations); we visualize our M = 52 word representations (FOREST) in relation to animals (10 words) and countries (10 words); we show the coefficient patterns for these words in Figure 2. We can see that in both cases there are dimensions where coefficient signs match positively (or negatively)."}, {"heading": "3.6 Runtime complexity", "text": "It is indeed the case that we will be able to find a solution with which we can identify."}, {"heading": "4 Conclusion", "text": "We introduced a new method for learning word representation based on a hierarchical, sparse encoding. The regulator promotes the hierarchical organization of the latent dimensions of vector-space word embedding. We demonstrated that our method is superior to the most modern methods in the areas of word similarity ranking, sentence completion and mood analysis."}, {"heading": "Acknowledgements", "text": "The authors thank Sam Thomson, Bryan R. Routledge, Jesse Dodge, and Fei Liu for their helpful feedback on an earlier draft of this paper. This work was supported by computer resources provided by a grant from the Pittsburgh Supecomputing Center and the National Science Foundation through the grant IIS-1352440."}, {"heading": "1 Additional Results", "text": "In Table 1, we compare FOREST with three additional baselines: \u2022 Murphy et al. [9]: a word representation that was trained using non-negative sparse embedding (NNSE) in our corpus. \u2022 Mikolov et al. [7]: a bilinear log model that predicts a word by its context, trained usinghierarchical Softmax with a binary Huffman tree (continuous bag of words, CBOW-HS). We use an implementation from https: / / / code.google.com / p / word2vec /. \u2022 Mikolov et al. [7]: a bilinear log bilinear model that predicts context words with a target word, SBOW-486 with a binary Huffman tree (skip-gram, SG-HS). We use an implementation from https: / / ved2c /."}, {"heading": "2 Additional Two Dimensional Projections", "text": "For FOREST, SG and NCE with M = 520, we project the learned word representations into two dimensions using the t-SNE tool [12] from http: / / homepage.tudelft.nl / 19j49 / t-SNE. html. We show projections of words related to the concept of \"good\" vs. \"bad\" and \"man\" vs. \"woman\" in Figure 1.ar Xiv: 140 6.20 35v1 [cs.CL] 8 Jun 201 4"}, {"heading": "3 List of Word Similarity Datasets", "text": "In our experiments, we use the following word similarity datasets: \u2022 Finkelstein et al. [3]: WordSimilarity dataset (353 pairs). \u2022 Agirre et al. [1]: a subset of WordSimilarity dataset to evaluate similarity (203 pairs). \u2022 Agirre et al. [1]: a subset of WordSimilarity dataset to evaluate kinship (252 pairs). \u2022 Miller and Charles [8]: semantic similarity dataset (30 pairs). \u2022 Rubenstein and Goodenough [11]: contains only nouns (65 pairs). \u2022 Luong et al. [5]: rare words (2034 pairs). \u2022 Bruni et al. [2]: common words (3,000 pairs). \u2022 Radinsky et al. [10]: MTurk-287 dataset (287 pairs). \u2022 Halawi and Dror [4]: MTurk-771 dataset (71 pairs). \u2022 Poverbs (13 pairs)."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["E. Agirre", "E. Alfonseca", "K. Hall", "J. Kravalova", "M. Pasca", "A. Soroa"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Distributional semantics in technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "Tran", "N.-K"], "venue": "In Proc. of ACL", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "The word relatedness mturk-771 test collection", "author": ["G. Halawi", "G. Dror"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong", "M.-T", "R. Socher", "C.D. Manning"], "venue": "In Proc. of CONLL", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Proc. of Workshop at ICLR", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["G.A. Miller", "W.G. Charles"], "venue": "Language and Cognitive Processes,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding", "author": ["B. Murphy", "P. Talukdar", "T. Mitchell"], "venue": "In Proc. of COLING", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A word at a time: Computing word relatedness using temporal semantic analysis", "author": ["K. Radinsky", "E. Agichtein", "E. Gabrilovich", "S. Markovitch"], "venue": "In Proc. of WWW", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Contextual correlates of synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1965}, {"title": "Visualizing data using t-sne", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Verb similarity on the taxonomy of wordnet", "author": ["D. Yang", "D.M.W. Powers"], "venue": "In Proc. of GWC", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}], "referenceMentions": [{"referenceID": 2, "context": "In contrast, recent approaches to word representation learning apply neural networks to obtain dense, low-dimensional, continuous embeddings of words [18, 3, 7, 13, 15].", "startOffset": 150, "endOffset": 168}, {"referenceID": 6, "context": "In contrast, recent approaches to word representation learning apply neural networks to obtain dense, low-dimensional, continuous embeddings of words [18, 3, 7, 13, 15].", "startOffset": 150, "endOffset": 168}, {"referenceID": 12, "context": "In contrast, recent approaches to word representation learning apply neural networks to obtain dense, low-dimensional, continuous embeddings of words [18, 3, 7, 13, 15].", "startOffset": 150, "endOffset": 168}, {"referenceID": 7, "context": "[30], and it has since been shown useful for other applications [8].", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "1 Following many others, we let xv,c be the pointwise mutual information (PMI) between the occurrence of context word c within a five-word window of an occurrence of word v [26, 19, 4].", "startOffset": 173, "endOffset": 184}, {"referenceID": 10, "context": "Here, we use the squared loss for the reconstruction error, but other loss functions could also be used [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "The most common regularizer is the `1 penalty, which results in sparse codes [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "[8] proposed a related penalty with only one tree for learning image and document representations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Others include: global context [7], multilingual context [4], geographic context [1], brain activation data [5], and second-order context [22].", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Others include: global context [7], multilingual context [4], geographic context [1], brain activation data [5], and second-order context [22].", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "Others include: global context [7], multilingual context [4], geographic context [1], brain activation data [5], and second-order context [22].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "Others include: global context [7], multilingual context [4], geographic context [1], brain activation data [5], and second-order context [22].", "startOffset": 108, "endOffset": 111}, {"referenceID": 9, "context": "Alternating minimization routines have been shown to work reasonably well in practice for such problems [10], but they are too expensive here due to (i) the size of X \u2208 RC\u00d7V (C and V are each on the order of 10) and (ii) the many overlapping groups in the structured regularizer \u03a9(A).", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Since our groups form tree structures, other methods such as FISTA [2] could also be used.", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "[13]: a recursive neural network (RNN) language model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "\u2022 Mnih and Teh [18]: a log bilinear model that predicts a word given its context, trained using noise-contrastive estimation (NCE, [6]).", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "[3]: a neural network language model trained on Wikipedia data for 2 months (CW).", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7]: a neural network model that uses additional global document context (RNN-DC).", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "14 Since we use block coordinate descent [12], Since t-SNE is a non convex method, we run it 10 times and choose the plots with the lowest t-SNE error.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "[12], we only perform one pass through the columns of D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We apply an efficient online and distributed learning method. Experiments on various benchmark tasks\u2014word similarity ranking, analogies, sentence completion, and sentiment analysis\u2014demonstrate that the method outperforms or is competitive with state-of-the-art neural network representations. Our word representations are available at http://www.ark.cs.cmu.edu/dyogatam/wordvecs/", "creator": "LaTeX with hyperref package"}}}