{"id": "1406.2080", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2014", "title": "Training Convolutional Networks with Noisy Labels", "abstract": "We propose several simple approaches to training deep neural networks on data with noisy labels. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark, showing how additional noisy data can improve state-of-the-art recognition models.", "histories": [["v1", "Mon, 9 Jun 2014 05:45:12 GMT  (6370kb,D)", "http://arxiv.org/abs/1406.2080v1", null], ["v2", "Sat, 20 Dec 2014 21:10:03 GMT  (6391kb,D)", "http://arxiv.org/abs/1406.2080v2", null], ["v3", "Tue, 3 Mar 2015 21:13:47 GMT  (859kb,D)", "http://arxiv.org/abs/1406.2080v3", null], ["v4", "Fri, 10 Apr 2015 16:44:00 GMT  (859kb,D)", "http://arxiv.org/abs/1406.2080v4", "Accepted as a workshop contribution at ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["sainbayar sukhbaatar", "joan bruna", "manohar paluri", "lubomir bourdev", "rob fergus"], "accepted": true, "id": "1406.2080"}, "pdf": {"name": "1406.2080.pdf", "metadata": {"source": "CRF", "title": "Learning from Noisy Labels with Deep Neural Networks", "authors": ["Sainbayar Sukhbaatar"], "emails": ["sainbayar@cs.nyu.edu", "fergus@cs.nyu.edu"], "sections": [{"heading": null, "text": "GsrrsrrteeeeGsrsrrteeeee\u00fccnlhsrteeeeeGsrsrrteee\u00fccsrteeeee\u00fccsrteeeee\u00fccsrrrrrrrteeeeirsrrrsrteeeeeeeeeeteerrrrrrrrrrrrrrrrrrrsrteeeeerrrsrrrrrrsrteeeeeeteerrrrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}], "references": [{"title": "Decontamination of training samples for supervised pattern recognition methods", "author": ["R. Barandela", "E. Gasca"], "venue": "In Advances in Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Label-noise robust logistic regression and its applications. In Machine Learning and Knowledge Discovery in Databases, volume 7523 of Lecture", "author": ["J. Bootkrajang", "A. Kabn"], "venue": "Notes in Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Identifying mislabeled training data", "author": ["C.E. Brodley", "M.A. Friedl"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Classification in the presence of label noise: A survey", "author": ["B. Frenay", "M. Verleysen"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Discovering informative patterns and data cleaning", "author": ["I. Guyon", "N. Matic", "V. Vapnik"], "venue": "In Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Design of robust neural network classifiers", "author": ["J. Larsen", "L. Nonboe", "M. Hintz-Madsen", "L. Hansen"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Estimating a kernel fisher discriminant in the presence of label noise", "author": ["N.D. Lawrence", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Learning to label aerial images from noisy data", "author": ["V. Mnih", "G. Hinton"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Learning with noisy labels", "author": ["N. Natarajan", "I. Dhillon", "P. Ravikumar", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A study of the effect of different types of noise on the precision of supervised learning techniques", "author": ["D. Nettleton", "A. Orriols-Puig", "A. Fornells"], "venue": "Artificial Intelligence Review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Class noise and supervised learning in medical domains: The effect of feature extraction", "author": ["M. Pechenizkiy", "A. Tsymbal", "S. Puuronen", "O. Pechenizkiy"], "venue": "In Computer-Based Medical Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "In International Conference on Learning Representations (ICLR", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W. Freeman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1958}], "referenceMentions": [{"referenceID": 12, "context": "2 Related Work In any classification model, degradation of performance is inevitable when there is noise in training labels [13, 15].", "startOffset": 124, "endOffset": 132}, {"referenceID": 14, "context": "2 Related Work In any classification model, degradation of performance is inevitable when there is noise in training labels [13, 15].", "startOffset": 124, "endOffset": 132}, {"referenceID": 0, "context": "A simple approach to handle noisy labels is a data preprocessing stage, where labels suspected to be incorrect are removed or corrected [1, 3].", "startOffset": 136, "endOffset": 142}, {"referenceID": 2, "context": "A simple approach to handle noisy labels is a data preprocessing stage, where labels suspected to be incorrect are removed or corrected [1, 3].", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "However, a weakness of this approach is the difficulty of distinguishing informative hard samples from harmful mislabeled ones [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 4, "context": "See [5] for comprehensive review.", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "A more recent work [2] proposed a generic unbiased estimator for binary classification with noisy labels.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "A cost function similar to ours is proposed in [2] to make logistic regression robust to label noise.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Considering the recent success of deep learning [8, 17, 16], there are very few works about deep learning from noisy labels.", "startOffset": 48, "endOffset": 59}, {"referenceID": 16, "context": "Considering the recent success of deep learning [8, 17, 16], there are very few works about deep learning from noisy labels.", "startOffset": 48, "endOffset": 59}, {"referenceID": 15, "context": "Considering the recent success of deep learning [8, 17, 16], there are very few works about deep learning from noisy labels.", "startOffset": 48, "endOffset": 59}, {"referenceID": 10, "context": "In [11, 9], noise modeling is incorporated to neural network in the same way as our proposed model.", "startOffset": 3, "endOffset": 10}, {"referenceID": 8, "context": "In [11, 9], noise modeling is incorporated to neural network in the same way as our proposed model.", "startOffset": 3, "endOffset": 10}, {"referenceID": 10, "context": "However, only binary classification is considered in [11], and [9] assumed symmetric label noise (noise is independent of the true label).", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "However, only binary classification is considered in [11], and [9] assumed symmetric label noise (noise is independent of the true label).", "startOffset": 63, "endOffset": 66}, {"referenceID": 9, "context": "1 Bottom-up Noise Model We assume that label noise is random conditioned on the true class, but independent of the input x (see [10] for more detail about this type of noise).", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "An unbiased estimator for binary classification is introduced in [1], where the cost fun tion s replaced by a surrogate cost function that combines the two costs (costs for class +1 and -1) with certain coefficients that depends on the noise level.", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "An u biased estim tor for binary classificati n is introduc d in [12], whe e the cost unction is replaced by a surrogate objective that combines the two costs (costs for class +1 and -1) with coefficients that depends on the noise level.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "First, we experiment on the Google street-view house number dataset (SVHN) [14], which consists of 32x32 images of house number digits captured from Google Streetview.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Next, we experiment on CIFAR-10 [7], a more challenging dataset consisting from 60k small images of 10 object categories.", "startOffset": 32, "endOffset": 35}, {"referenceID": 17, "context": "The first dataset consist from clean images from CIFAR-10 dataset and noisy images from Tiny Images dataset [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 3, "context": "The second dataset consists of clean images from ImageNet [4] and noisy images downloaded from web search engines.", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "[8] on the clean and combined datasets, with several types of noise modeling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] 18.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] 15M full ImageNet 16.", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "We propose several simple approaches to training deep neural networks on data with noisy labels. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark, showing how additional noisy data can improve state-of-the-art recognition models. 1 Introduction In recent years, deep learning methods have shown impressive results on image classification tasks. However, this achievement is only possible because of large amount of labeled images. Labeling images by hand is a laborious task and takes a lot of time and money. An alternative approach is to generate labels automatically. This includes user tags from social web sites and keywords from image search engines. Considering the abundance of such noisy labels, it is important to find a way to utilize them in deep learning. Unfortunately, those labels are very noisy and unlikely to help training deep networks without additional tricks. Our goal is to study the effect label noise on deep networks, and explore simple ways of improvement. We focus on the robustness of deep networks instead of data cleaning methods, which are well studied and can be used together with robust models directly. Although many noise robust classifiers are proposed so far, there are not many works on training deep networks on noisy labeled data, especially on large scale datasets. Our contribution in this paper is a novel way of modifying deep learning models so they can be effectively trained on data with high level of label noise. The modification is simply done by adding a linear layer on top of the softmax layer, which makes it easy to implement. This additional layer changes the output from the network to give better match to the noisy labels. Also, it is possible to learn the noise distribution directly from the noisy data. Using real-world image classification tasks, we demonstrate that the model actually works very well in practice. We even show that random images without labels (complete noise) can improve the classification performance. 2 Related Work In any classification model, degradation of performance is inevitable when there is noise in training labels [13, 15]. A simple approach to handle noisy labels is a data preprocessing stage, where labels suspected to be incorrect are removed or corrected [1, 3]. However, a weakness of this approach is the difficulty of distinguishing informative hard samples from harmful mislabeled ones [6]. Instead, in this paper, we focus on models robust to presence of label noise. 1 ar X iv :1 40 6. 20 80 v1 [ cs .C V ] 9 J un 2 01 4 The effect of label noise is well studied in common classifiers (e.g., SVMs, kNN, logistic regression), and their label noise robust variants have been proposed. See [5] for comprehensive review. A more recent work [2] proposed a generic unbiased estimator for binary classification with noisy labels. They employ a surrogate cost function that can be expressed by a weighted sum of the original cost functions, and gave theoretical bounds on the performance. In this paper, we will also consider this idea and extend it multiclass. A cost function similar to ours is proposed in [2] to make logistic regression robust to label noise. They also proposed a learning algorithm for noise parameters. However, we consider deep networks, a more powerful and complex classifier than logistic regression, and propose a different learning algorithm for noise parameters that is more suited for back-propagation training. Considering the recent success of deep learning [8, 17, 16], there are very few works about deep learning from noisy labels. In [11, 9], noise modeling is incorporated to neural network in the same way as our proposed model. However, only binary classification is considered in [11], and [9] assumed symmetric label noise (noise is independent of the true label). Therefore, there is only a single noise parameter, which can be tuned by cross-validation. In this paper, we consider multiclass classification and assume more realistic asymmetric label noise, which makes it impossible to use cross-validation to adjust noise parameters (there can be a million parameters). 3 Approach In this paper, we consider two approaches to make an existing classification model, which we call the base model, robust against noisy labels: bottom-up and top-down noise models. In the bottomup model, we add an additional layer to the model that changes the label probabilities output by the base model so it would better match to noisy labels. Top-down model, on other hand, changes given noisy labels before feeding them to the base model. Both models require a noise model for training, so we will give an easy way to estimate noise levels using clean data. Also, it is possible to learn noise distribution from noisy data in the bottom-up model. Although only deep neural networks are used in our experiments, the both approaches can be applied to any classification model with a cross entropy cost. 3.1 Bottom-up Noise Model We assume that label noise is random conditioned on the true class, but independent of the input x (see [10] for more detail about this type of noise). Based on this assumption, we add an additional layer to a deep network (see Figure 1) that changes its output so it would better match to the noisy labels. The weights of this layer corresponds to the probabilities of a certain class being mislabeled to another class. Because those probabilities are often unknown, we will show how estimate them from additional clean data, or from the noisy data itself. Let D be the true data distribution generating correctly labeled samples (x, y\u2217), where x is an input vector and y\u2217 is the corresponding label. However, we only observe noisy labeled samples (x, \u1ef9) that generated from a some noisy distribution D\u0303. We assume that the label noise is random conditioned on the true labels. Then, the noise distribution can be parameterized by a matrix Q = {qji}: qji := p(\u1ef9 = j|y\u2217 = i). Q is a probability matrix because its elements are positive and each column sums to one. The probability of input x being labeled as j in D\u0303 is given by p(\u1ef9 = j|x, \u03b8) = \u2211 i p(\u1ef9 = j|y\u2217 = i)p(y\u2217 = i|x) = \u2211 i qjip(y \u2217 = i|x, \u03b8). (1) where p(y\u2217 = i|x, \u03b8) is the probabilistic output of the base model with parameters \u03b8. If the true noise distribution is known, we can modify this for noisy labeled data. During training, Q will act as an adapter that transforms the model\u2019s output to better match the noisy labels. Deep\t\r  network Learnin from noisy labels in deep neural networks Sainbayar Sukhbaatar Dept. of Computer Science, NYU, 715 Broadway, New Y rk, NY 10003 sainbar@cs.nyu.edu Rob Fergus Courant Institute, NYU, 715 Broadway, New York, NY 10003 fergus@cs.nyu.edu", "creator": "LaTeX with hyperref package"}}}