{"id": "1406.2083", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2014", "title": "On the Decreasing Power of Kernel and Distance Based Nonparametric Hypothesis Tests in High Dimensions", "abstract": "This paper is about two related methods for two sample testing and independence testing which have emerged over the last decade: Maximum Mean Discrepancy (MMD) for the former problem and Distance Correlation (dCor) for the latter. Both these methods have been suggested for high-dimensional problems, and sometimes claimed to be unaffected by increasing dimensionality of the samples. We will show theoretically and practically that the power of both methods (for different reasons) does actually decrease polynomially with dimension. We also analyze the median heuristic, which is a method for choosing tuning parameters of translation invariant kernels. We show that different bandwidth choices could result in the MMD decaying polynomially or even exponentially in dimension.", "histories": [["v1", "Mon, 9 Jun 2014 05:59:21 GMT  (96kb,D)", "http://arxiv.org/abs/1406.2083v1", "15 pages, 7 figures"], ["v2", "Mon, 24 Nov 2014 00:23:35 GMT  (97kb,D)", "http://arxiv.org/abs/1406.2083v2", "19 pages, 9 figures, published in AAAI-15: The 29th AAAI Conference on Artificial Intelligence (with author order reversed from ArXiv)"]], "COMMENTS": "15 pages, 7 figures", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT math.ST stat.ME stat.TH", "authors": ["aaditya ramdas", "sashank jakkam reddi", "barnab\u00e1s p\u00f3czos", "aarti singh", "larry a wasserman"], "accepted": true, "id": "1406.2083"}, "pdf": {"name": "1406.2083.pdf", "metadata": {"source": "CRF", "title": "Kernel MMD, the Median Heuristic and Distance Correlation in High Dimensions", "authors": ["Sashank J. Reddi", "Aaditya Ramdas", "Barnab\u00e1s P\u00f3czos", "Larry Wasserman"], "emails": ["sjakkamr@cs.cmu.edu", "aramdas@cs.cmu.edu", "bapoczos@cs.cmu.edu", "aarti@cs.cmu.edu", "larry@stat.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Non-parametric two-sample tests and independence tests are two related problems of paramount importance in statistics. In the former, we have two sets of samples, and we want to determine whether they come from the same or different distributions; in the latter, we have a set of samples from a multivariate distribution, and we want to determine whether the compound is the product of marginal or not; the two problems are related because an algorithm for testing the former can be used to test the latter. Kernel Maximum Mean Discrepancy is a quantity introduced in Gretton et al. [2012a] To address the first problem with the reproduction of Kernel Hilbert Spaces (RKHSs), one can embed the two probability distributions as functions in the RKHS and calculate the square RKHS standard of their difference (called the MMD)."}, {"heading": "2 Summary of Contributions", "text": "While the result is correct, it may be misleading; we will see that the true population value of MMD can be both polynomic and exponentially small (we have been advised that a specific case of Corollary 1 was previously noted independently of the population of Balakrishnan, but know no other examples); even though it is known that MMD2 is smaller than the KL divergence, we are giving several examples for the first time where it can be polynomic or exponentially smaller than KL, suggesting (not implicitly) that the test may have low power, and we are actually demonstrating experimentally that the power against fair alternatives (discussed later) may be polynomic or exponentially smaller than KL."}, {"heading": "3 The Power of MMD in High Dimensions", "text": "Let P be a class of continuous distributions on topological space X. Our goal is to:"}, {"heading": "H0 : p = q against H1 : p 6= q", "text": "Where p, q, P We construct a test for the hypothesis from the samples (x1,.., xn) and (y1,.., ym) from the distributions p and q, respectively. To do this, we define a deviation measurement \u03c1 (p, q) so that: (a) R (p, q) 0 for all p, q) F and (b) P (p, q) = 0, if and only if p = q. We are interested in the high-dimensional regime i.e X Rd for large d, possibly greater than n. Most existing non-parametric methods for this problem, such as the KL divergence, suffer from the curse of dimensionality - if the smoothness of the density p and q does not grow with d, then the estimators generally require many samples in dimension to get a good estimate of the measurement value (see Birge and Massart [1995], Laurent [1996], Kerkyacharian Picard and 1999, Bickel]."}, {"heading": "3.1 The Difficulty of Analytically Characterizing Power", "text": "The performance of a test depends on the distribution of the numbers under H0 and H1. If the distributions are close to Gaussian, the mean statistical number and its standard deviation (see below) under both H0 (\u00b50) and H1 (\u00b51, \u03c31) play a role in determining the power (the probability mass of the alternative distribution beyond a given distribution). Characterization of the asymptotic variables Gretton et al. [2012a] Behavior of the test statistics under the zero and alternative variables is usually hard. For example, the above MD2b estimator has a zero distribution which is an infinitely weighted sum of chi-quared variables. [2012a] Another linear time statistic called MMD2l is unbiased and has an asymptotic normal distribution Gretton al."}, {"heading": "3.3 Example: Gaussian Kernel for Different Mean, Same Covariance Normal Distributions", "text": "Theorem 2. Suppose p: N (\u00b51, \u03a3) and q: N (\u00b52, \u03a3). Then MMD2 between p and q using a Gaussian kernel with bandwidth \u03b3 is, MMD2 (p, q) = 2 (\u03b322) d / 2 1 \u2212 exp (\u2212 (\u00b51 \u2212 \u00b52) > (\u03a3 + \u03b32I / 2) \u2212 1 (\u00b51 \u2212 \u00b52) | \u03a3 + \u03b32I / 2 | 1 / 2. Let us use Taylor's theorem for 1 \u2212 e \u2212 x \u00b2 x and ignore \u2212 x 22 and other minor residual terms for clarification, then the above expression simplifies to MMD2 (p, q)."}, {"heading": "3.4 Example: Gaussian Kernel for Same Mean, Different Covariance Normal Distribution", "text": "The next example refers to the Gaussian nucleus with product distributions having the same mean and different deviations. Example 3 in paragraph 4.2 of Sriperumbudur et al. [2012] has related calculations with another objective, which we discuss in section 3.7.Theorem 3. let us assume p: d \u2212 1i = 1N (0, \u03c32) N (0, \u03c42) and q: di = 1N (0, \u03c32). Then MMD 2 between q and p using a Gaussian kernel with bandwidth suitability is MMD2 (p, q) through4 (1 + 4\u03c32 / \u03b32) d / 2 \u2212 1 / 2. According to Taylor's theorem for log x, the KL divergence in this case is approximately equal to KL (p, q) = 12 (tr: \u2212 11 \u04450) \u2212 d \u2212 log (det: 0 / det: 1) = 2 (2) (2) (2) (2) (2) (2)."}, {"heading": "3.5 Bandwidth Choice and the Median Heuristic", "text": "We study how the bandwidth choice affects the population MMD2 for the example in Theorem 2 (the consequences for Theorem 3 are similar). Below, the scaling of the bandwidth choice by a constant does not change the qualitative behavior, so we ignore constants for simplicity reasons. To provide clarity in the following conclusions, we also ignore the Taylor residuals and use (1 + 1 / d) d \u2248 e for large d.Underestimated bandwidth correlation 1. Suppose \u03a3 = \u03c32I. If we use \u03b3 = \u03c3d1 / 2 \u2212 for 0 < \u2264 1 / 2, thenMMD2 (p, q) for large d.1 \u2212 \u00b52 (d1 \u2212 2 + 2) exp (d2 \u2212 2) exp (d2 / 2).Hence, the population MMD2 for very large group goes exponentially fast in d (confirmed by experiments that follow).The special case of a constant bandwidth with 2 = 1 / 2."}, {"heading": "3.6 Example : Laplace Kernel for Different Mean, Same Variance Laplace Distributions", "text": "We assume, p: iLaplace (\u00b51, i, \u03c3) and q: iLaplace (\u00b51, i, \u03c3) and q: iLaplace (\u00b52, i, \u03c3) and q: iLaplace (\u00b52, \u03c3). If we use a Laplace kernel with bandwidth \u03b3, we actually have MMD2 (p, q) = e \u2212 i1 \u2212 i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i..i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i."}, {"heading": "4 The Power of Distance Correlation (dCor) in high dimensions", "text": "Considering the fact that n samples (xi, yi) \"Rdx + dy\" (dx 6 = dy is allowed) of a common distribution PXY with marginal PX, PY, we would like to test H0: PXY = PXPY versus H1: PXY 6 = PXPY. The authors of Sze \"kely et al.\" (1) The authors of Sze \"kely et al.\" (1) The authors of Sze \"kely et al.\" (1) The authors of Sze \"kely et al.\" (1) The authors of Sze \"kely et al.\" (1) The authors of Sze \"kely et al.\" (1) The authors of HAH = HBH, where H = I \u2212 11T / n is a centering matrix, and A, B are distance matrices for X, Y respectively i.e."}, {"heading": "4.1 Experimental Verification", "text": "Here we carefully design a simple simulation experiment to demonstrate this decrease in power with dimension, with some subtleties in the choice of alternative hypothesis that are crucial. For the null hypothesis of independent variables, we let (x, y) be normally sampled by a d-dimensional standard, as in Sze \u0301 kely and Rizzo [2013]. For the alternative hypothesis, we have to make a choice about how to change the covariance matrix, so that the problem does not become easier or more difficult with d rises. We choose to make a constant number of off-diagonal elements non-zero, i.e. we do not change the number or value of non-zero off-diagonal elements as d increments. The margins are still standard Gaussians; the non-zero elements are only present in the cross-diagonal blocks that indicate dependence between X, Y and zero x. It can be argued that non-selection is not a constant with number growing."}, {"heading": "5 Conclusion", "text": "In summary, we believe that we have convincingly demonstrated for the first time that the performance of closely related nuclear and distance-based tests both suffer from the curse of dimensionality versus fair alternatives. In the process, we have also conducted a detailed study of bandwidth choice, explicitly showing cases when and why median heuristics work and fail, and providing an argument for overestimating bandwidth. Reasons for the observed performance decline can be complicated, and a better theory is needed to understand the zero and alternative distributions in high dimensions."}, {"heading": "A Proof of Lemma 1", "text": "Proof. From the definition of MMD2 we get MMD2 (p, q) = x, x \"k (x, x\") p (x) p (x \") dxdx\" + x, x \"k (x, x\") q (x \") q (x\") dxdx \"\u2212 2 x, x\" k (x, x \") p (x) q (x\") dxdx. \"From Bochner's theorem (see [Rudin, 1962]) for translation invariant cores we know k (x, x\") = x \"w\" s (w) eiw > xe \u2212 iw > x \"dw, where s is the faster transformation of the grain. If we replace the aforementioned equality in the definition of MMD2, we have the required result."}, {"heading": "B Proof of Theorem 2", "text": "Evidence. Since the Gaussian kernel is an implicit kernel, we can use Lemma 1 to derive the MMD2 in this case. It is generally known that the Fourier transformation s (w) of the Gaussian kernel is the Gaussian distribution. As a substitute for the characteristic function of the normal distribution in Lemma 1, we have MMD2 (p, q) = explicit (p, q) = explicit (p, p) explicit (p, p) explicit (\u2212 2, p) explicit (\u2212 2, p) explicit (\u2212 2, p) explicit (\u2212 2, p) explicit (\u2212 w, p) explicit (\u2212 w, p) explicit (\u2212 w, p) explicit (\u2212 w, p) explicit (\u2212 w, p), 2, (\u2212 w, p, 2) (\u2212), (\u2212 w, w / p (\u2212, 2), (\u2212 w, p, 2) (\u2212, 2), (\u2212 f, p), \u2212 p (\u2212), p (\u2212 p), p (\u2212), p (\u2212 p), p (\u2212) explicit (\u2212, p), p (\u2212), p (\u2212), p (\u2212, w, p (\u2212, p), p (\u2212, p), p (\u2212, p), p (\u2212, p (\u2212 p), p (\u2212, p), p (\u2212, p) explicit (\u2212, p (\u2212), p (\u2212 p), p (\u2212, p (\u2212, p), p (\u2212 p), p (\u2212, p, p (\u2212 p), p (\u2212, p), p (\u2212, p, p (\u2212 p), p (\u2212 p), p (\u2212 p, p (\u2212, p, p), p (\u2212, p, p (\u2212 p, p), p (\u2212 p, p, p (\u2212 p, p), p (\u2212 p, p, p) explicit (\u2212 p, p, p (\u2212 p), p, p (\u2212, p, p, p, p, p, p, p (\u2212 p) explicit (\u2212, p, p, p, p, p, p, p, p"}, {"heading": "C Proof of Proposition 1", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "D Proof of Proposition 2", "text": "Proposition 2 (2 + 2) (1 + 2) (1 + 2) (2 + 2) (2 + 2) (2 + 2) (2 + 2) (2 + 2) (2 + 2) (2 + 2) (2) (2 + 2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2)) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) () (2) (2) (2) (2) (2) (2) (2) (2) (2) () (2) (2) (2) (2) (2) () (2) (2) (2) (2) () (2) (2) () (2) (2) (2 (2) (2) (2) () (2) (2) (2) (2) (2) () (2) (2) (2) (2) () (2) (2) (2 (2) () (2) (2) () (2) (2) () (2) (2 () ("}, {"heading": "E Proof of Theorem 4", "text": "Remember that we use the Laplace kernel, i.e., k (x, x) p (x) p (x) k (x) q (x) q (x) x (x) q (x) x (x)) k (x) dxdx. \"(4) Consider the term x, x (x) p (x) k (x) k (x, x) dxdx.\" The other terms can be calculated in a similar way. Let's leave the Laplace kernel (x, x) x (x) p (x) q (x) k (x) k (x) dxdx. \"The other terms can be calculated similarly."}, {"heading": "G Biased MMD for Gaussian Distribution", "text": "H Verifying Power Plots Decay Polynomial I Standard Deviation of udCor, dCor"}, {"heading": "J MMD between Gaussians with same mean, different variances", "text": "Suppose P = di = 1N (0, \u03c32) N (0, a2) n (0, a2) and Q = di = 1N (0, \u03c32) N (0, b2). If a, b are of the same order of magnitude as \u03c3, then the mean heurist will still select \u03b3 \u00b2 d for the bandwidth of the Gaussian kernel. First, we note that for distributions with the same mean, according to Taylor's theorem, KL (P, Q) = 12 (tr (tr (\u03a3 \u2212 11) n \u00b2 0 \u2212 d \u2212 log (det 0) / det 1) = 1 2 (a2 / b2 \u2212 log (a2 / b2)) \u2248 (a 2 / b2 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 1 \u2212 1 \u2212 1 / 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 (+ 2) + 2 (+ 2) \u2212 2 + 2 \u2212 2 + 2 + 2 \u2212 2 + 2 (2) \u2212 2 + 2 \u2212 1 / 2 \u2212 2 \u2212 2 \u2212 2 + 2 (2) \u2212 2 / 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 / 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 / 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 / 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 / 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 / 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 / 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212"}], "references": [{"title": "Finding and Leveraging Structure in Learning Problems", "author": ["S. Balakrishnan"], "venue": "PhD thesis,", "citeRegEx": "Balakrishnan.,? \\Q2013\\E", "shortCiteRegEx": "Balakrishnan.", "year": 2013}, {"title": "Estimating integrated squared density derivatives: sharp best order of convergence estimates", "author": ["P. Bickel", "Y. Ritov"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "Bickel and Ritov.,? \\Q1988\\E", "shortCiteRegEx": "Bickel and Ritov.", "year": 1988}, {"title": "Estimation of integral functionals of a density", "author": ["L. Birge", "P. Massart"], "venue": "The Annals of Statistics,", "citeRegEx": "Birge and Massart.,? \\Q1995\\E", "shortCiteRegEx": "Birge and Massart.", "year": 1995}, {"title": "Measuring statistical dependence with Hilbert-Schmidt norms", "author": ["A. Gretton", "O. Bousquet", "A. Smola", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of Algorithmic Learning Theory,", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Schoelkopf", "A. Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "B. Sriperumbudur", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Estimation of R\u00e9nyi information divergence via pruned minimal spanning trees", "author": ["A. Hero", "O. Michel"], "venue": "In Higher-Order Statistics,", "citeRegEx": "Hero and Michel.,? \\Q1999\\E", "shortCiteRegEx": "Hero and Michel.", "year": 1999}, {"title": "Estimating nonquadratic functionals of a density using haar wavelets", "author": ["G. Kerkyacharian", "D. Picard"], "venue": "The Annals of Statistics,", "citeRegEx": "Kerkyacharian and Picard.,? \\Q1996\\E", "shortCiteRegEx": "Kerkyacharian and Picard.", "year": 1996}, {"title": "Efficient estimation of integral functionals of a density", "author": ["B. Laurent"], "venue": "The Annals of Statistics,", "citeRegEx": "Laurent.,? \\Q1996\\E", "shortCiteRegEx": "Laurent.", "year": 1996}, {"title": "Distance covariance in metric spaces", "author": ["R. Lyons"], "venue": "Annals of Probability,", "citeRegEx": "Lyons.,? \\Q2013\\E", "shortCiteRegEx": "Lyons.", "year": 2013}, {"title": "Fourier analysis on groups", "author": ["W. Rudin"], "venue": null, "citeRegEx": "Rudin.,? \\Q1962\\E", "shortCiteRegEx": "Rudin.", "year": 1962}, {"title": "Equivalence of distance-based and RKHS-based statistics in hypothesis testing", "author": ["D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu"], "venue": "The Annals of Statistics,", "citeRegEx": "Sejdinovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sejdinovic et al\\.", "year": 2013}, {"title": "On the empirical estimation of integral probability metrics", "author": ["B. Sriperumbudur", "K. Fukumizu", "A. Gretton", "B. Schoelkopf", "G. Lanckriet"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2012}, {"title": "The distance correlation t-test of independence in high dimension", "author": ["G.J. Sz\u00e9kely", "M.L. Rizzo"], "venue": "J. Multivariate Analysis,", "citeRegEx": "Sz\u00e9kely and Rizzo.,? \\Q2013\\E", "shortCiteRegEx": "Sz\u00e9kely and Rizzo.", "year": 2013}, {"title": "Measuring and testing dependence by correlation of distances", "author": ["G.J. Sz\u00e9kely", "M.L. Rizzo", "Bakirov N.K"], "venue": "The Annals of Statistics,", "citeRegEx": "Sz\u00e9kely et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sz\u00e9kely et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "Kernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs).", "startOffset": 60, "endOffset": 83}, {"referenceID": 3, "context": "Kernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs). In brief, one can embed the two probability distributions as functions in the RKHS, and calculate the squared RKHS-norm of their difference (called the MMD). For characteristic kernels like the Gaussian and Laplace kernels we will later consider, the MMD is zero iff the distributions are equal (see Gretton et al. [2012a]).", "startOffset": 60, "endOffset": 483}, {"referenceID": 3, "context": "Kernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs). In brief, one can embed the two probability distributions as functions in the RKHS, and calculate the squared RKHS-norm of their difference (called the MMD). For characteristic kernels like the Gaussian and Laplace kernels we will later consider, the MMD is zero iff the distributions are equal (see Gretton et al. [2012a]). The corresponding test uses empirical distributions for plug-in estimators (described later) and is consistent (for fixed dimension, power tends to one as number of samples becomes infinite) against any single fixed alternative. Distance correlation is a quantity introduced in Sz\u00e9kely et al. [2007] to tackle the second problem using distances between pairs of points.", "startOffset": 60, "endOffset": 785}, {"referenceID": 2, "context": "2 Summary of Contributions Kernel Maximum Mean Discrepancy (MMD) Gretton et al. [2012a] showed that the estimated MMD converges to the true MMD at rate O(n\u22121/2) independently of dimension d.", "startOffset": 65, "endOffset": 88}, {"referenceID": 0, "context": "We will see that the true value of the population MMD can be polynomially or even exponentially small in d (we were notified that a special case of Corollary 1 was earlier independently noted by Balakrishnan [2013], but do not know other examples).", "startOffset": 195, "endOffset": 215}, {"referenceID": 3, "context": "This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially.", "startOffset": 36, "endOffset": 59}, {"referenceID": 3, "context": "This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially. A similar conclusion holds for a second example of same mean Gaussians with different variances. In a third example, separated Laplace distributions with Laplace kernel, if the median heuristic is used, the population MMD will (theoretically) drop to 0 exponentially in d; with a larger bandwidth choice, however, the MMD drops to zero only polynomially in d. All our theoretical predictions are also validated by simulations. In the above examples, choosing the bandwidth optimally to maximize the MMD did also experimentally maximize the power against fair alternatives (it has been noted in Gretton et al. [2012b] that choosing the bandwidth to maximize MMD is sometimes better than the median heuristic).", "startOffset": 36, "endOffset": 965}, {"referenceID": 3, "context": "This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially. A similar conclusion holds for a second example of same mean Gaussians with different variances. In a third example, separated Laplace distributions with Laplace kernel, if the median heuristic is used, the population MMD will (theoretically) drop to 0 exponentially in d; with a larger bandwidth choice, however, the MMD drops to zero only polynomially in d. All our theoretical predictions are also validated by simulations. In the above examples, choosing the bandwidth optimally to maximize the MMD did also experimentally maximize the power against fair alternatives (it has been noted in Gretton et al. [2012b] that choosing the bandwidth to maximize MMD is sometimes better than the median heuristic). However, in all simulations, power goes to zero as d\u2192\u221e for all settings of the bandwidth. Distance Correlation (dCor) Sz\u00e9kely and Rizzo [2013] studied distance correlation in high dimensions where they considered the following example.", "startOffset": 36, "endOffset": 1200}, {"referenceID": 2, "context": "Most existing non-parametric methods for this problem, like KL divergence, suffer from the curse of dimensionality - if the smoothness of densities p and q doesn\u2019t grow with d, then the estimators generally require exponentially many samples in dimension to obtain a good estimate of the measure (see Birge and Massart [1995], Laurent [1996], Kerkyacharian and Picard", "startOffset": 301, "endOffset": 326}, {"referenceID": 2, "context": "Most existing non-parametric methods for this problem, like KL divergence, suffer from the curse of dimensionality - if the smoothness of densities p and q doesn\u2019t grow with d, then the estimators generally require exponentially many samples in dimension to obtain a good estimate of the measure (see Birge and Massart [1995], Laurent [1996], Kerkyacharian and Picard", "startOffset": 301, "endOffset": 342}, {"referenceID": 1, "context": "[1996], Bickel and Ritov [1988], Hero and Michel [1999]).", "startOffset": 8, "endOffset": 32}, {"referenceID": 1, "context": "[1996], Bickel and Ritov [1988], Hero and Michel [1999]).", "startOffset": 8, "endOffset": 56}, {"referenceID": 1, "context": "[1996], Bickel and Ritov [1988], Hero and Michel [1999]). However, there is some folklore that MMD does not suffer from such a curse. Let us first introduce some known results before delving into our detailed analysis that will show that this folklore is false and that MMD\u2019s power decays with d against fair alternatives. Let F be a class of functions f : X \u2192 R. The MMD is defined as: MMD(F , p, q) := sup f\u2208F Ex\u223cp[f(x)]\u2212 Ey\u223cq[f(y)]. We restrict our attention to the case where function class F is a unit ball in a RKHS (H, k) where we assume that k is a bounded, continuous and positive definite kernel function. In this case, it can be shown that MMD(p, q) := \u2016\u03bcp \u2212 \u03bcq\u2016H where \u03bcp = Ex\u223cp[k(x, .)] for any distribution p \u2208 P . Furthermore, it is also well-known that MMD(p, q) = 0 iff p = q when we use characteristic kernels Gretton et al. [2012a]. The following is a biased estimator for MMD from Gretton et al.", "startOffset": 8, "endOffset": 851}, {"referenceID": 1, "context": "[1996], Bickel and Ritov [1988], Hero and Michel [1999]). However, there is some folklore that MMD does not suffer from such a curse. Let us first introduce some known results before delving into our detailed analysis that will show that this folklore is false and that MMD\u2019s power decays with d against fair alternatives. Let F be a class of functions f : X \u2192 R. The MMD is defined as: MMD(F , p, q) := sup f\u2208F Ex\u223cp[f(x)]\u2212 Ey\u223cq[f(y)]. We restrict our attention to the case where function class F is a unit ball in a RKHS (H, k) where we assume that k is a bounded, continuous and positive definite kernel function. In this case, it can be shown that MMD(p, q) := \u2016\u03bcp \u2212 \u03bcq\u2016H where \u03bcp = Ex\u223cp[k(x, .)] for any distribution p \u2208 P . Furthermore, it is also well-known that MMD(p, q) = 0 iff p = q when we use characteristic kernels Gretton et al. [2012a]. The following is a biased estimator for MMD from Gretton et al. [2012a]:", "startOffset": 8, "endOffset": 924}, {"referenceID": 3, "context": "A similar unbiased estimator exists without the k(xi, xi), k(yj , yj) terms Gretton et al. [2012a]. The convergence of the above estimator to their respective measures is shown by the following result.", "startOffset": 76, "endOffset": 99}, {"referenceID": 3, "context": "A similar unbiased estimator exists without the k(xi, xi), k(yj , yj) terms Gretton et al. [2012a]. The convergence of the above estimator to their respective measures is shown by the following result. Theorem 1. Gretton et al. [2012a] Suppose 0 \u2264 k(x, x) \u2264 K, then with probability at least 1\u2212 \u03b4, we have", "startOffset": 76, "endOffset": 236}, {"referenceID": 3, "context": "For example, the above MMDb estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMDl is unbiased and has an asymptotic normal distribution Gretton et al.", "startOffset": 121, "endOffset": 144}, {"referenceID": 3, "context": "For example, the above MMDb estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMDl is unbiased and has an asymptotic normal distribution Gretton et al. [2012a]. However, the associated quantities \u03bc0, \u03c30, \u03bc1, \u03c31 actually vary with d and n.", "startOffset": 121, "endOffset": 268}, {"referenceID": 3, "context": "For example, the above MMDb estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMDl is unbiased and has an asymptotic normal distribution Gretton et al. [2012a]. However, the associated quantities \u03bc0, \u03c30, \u03bc1, \u03c31 actually vary with d and n. Further, in the highdimensional setting, classical large sample theory does not apply as d can be comparable to or larger than n, and calculations assuming the \u201casymptotically\u201d normal distribution can be misleading. Specifically, consider Q := \u221a n(MMD2l\u2212MMD ) \u03c31 N(0, 1) d =: Z as shown in Gretton et al. [2012a]. Thus, an asymptotic level \u03b1 test rejects when MMDl > \u03c30z\u03b1/ \u221a n.", "startOffset": 121, "endOffset": 660}, {"referenceID": 3, "context": "Hence, one might be tempted to use \u03bdn,d to measure the effectiveness of the test, and indeed choosing the kernel (or bandwidth) to maximize \u03bdn,d was studied by Gretton et al. [2012b]. However, in the high dimensional setting the normal approximation used in the last step can be extremely poor, as we have experimentally verified.", "startOffset": 160, "endOffset": 183}, {"referenceID": 12, "context": "A more general version of the above lemma for all kernels (with a different constant than 1) is presented in Sriperumbudur et al. [2012] (Proposition 5.", "startOffset": 109, "endOffset": 137}, {"referenceID": 12, "context": "2 of Sriperumbudur et al. [2012] has related calculations, with a different aim that we discuss in Section 3.", "startOffset": 5, "endOffset": 33}, {"referenceID": 0, "context": "The special case of a constant bandwidth with = 1/2 has already been noted by Balakrishnan [2013].", "startOffset": 78, "endOffset": 98}, {"referenceID": 3, "context": "The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error).", "startOffset": 35, "endOffset": 58}, {"referenceID": 3, "context": "The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error). We contrast this with the choice of Fig. 3 in Sriperumbudur et al. [2012], which was not a power study but an empirical study of convergence rates for estimation of MMD, where the authors choose to let the mean separation be (1, 1, 1, .", "startOffset": 35, "endOffset": 286}, {"referenceID": 3, "context": "The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error). We contrast this with the choice of Fig. 3 in Sriperumbudur et al. [2012], which was not a power study but an empirical study of convergence rates for estimation of MMD, where the authors choose to let the mean separation be (1, 1, 1, ..., 1) which makes the problem easier with dimension. Also, they use it to argue that the mean squared error (as summarised by Thm 1) with increasing n is indeed independent of d. Another relevant comparison is with Fig. 5A in Gretton et al. [2012a] where they show an extremely slow decrease in power with dimension for the same example of mean-shifted Gaussians with Gaussian kernel that we consider.", "startOffset": 35, "endOffset": 698}, {"referenceID": 14, "context": "The authors of Sz\u00e9kely et al. [2007] introduce a test statistic called (squared) distance covariance which is defined as", "startOffset": 15, "endOffset": 37}, {"referenceID": 9, "context": "One can use other negative definite metrics instead of Euclidean norms to generalize the definition to metric spaces Lyons [2013]. The above expression is different from the presentation in the original papers (but mathematically equivalent).", "startOffset": 117, "endOffset": 130}, {"referenceID": 11, "context": "dCor n is always between [0, 1], and unlike correlation, the population dCor 2 = 0 iff X,Y are independent, and Sz\u00e9kely et al. [2007] proves it is consistent against any fixed alternatives (with finite second moments).", "startOffset": 112, "endOffset": 134}, {"referenceID": 3, "context": "The MMD between \u03bcPXY and \u03bcPX\u00d7PY is called HSIC (see Gretton et al. [2005]), which has the sample expression: HSICn = 1 n2 tr(K\u0303L\u0303) = 1 n2 n \u2211", "startOffset": 52, "endOffset": 74}, {"referenceID": 11, "context": "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov.", "startOffset": 34, "endOffset": 59}, {"referenceID": 11, "context": "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov. Hence, dCor and MMD are very strongly related quantities, for very related problems. In Sz\u00e9kely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d.", "startOffset": 34, "endOffset": 374}, {"referenceID": 11, "context": "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov. Hence, dCor and MMD are very strongly related quantities, for very related problems. In Sz\u00e9kely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d. We will describe experiments that demonstrate that dCor and udCor as test statistics both suffer in high d, but for a slightly different reason than for MMD. When (X,Y ) are drawn from a standard Gaussian, the authors of Sz\u00e9kely and Rizzo [2013] show that the biased dCorn \u2192 1, if n is kept fixed and dx, dy are increased.", "startOffset": 34, "endOffset": 735}, {"referenceID": 11, "context": "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov. Hence, dCor and MMD are very strongly related quantities, for very related problems. In Sz\u00e9kely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d. We will describe experiments that demonstrate that dCor and udCor as test statistics both suffer in high d, but for a slightly different reason than for MMD. When (X,Y ) are drawn from a standard Gaussian, the authors of Sz\u00e9kely and Rizzo [2013] show that the biased dCorn \u2192 1, if n is kept fixed and dx, dy are increased. Then, they show that their unbiased udCorn hovers around 0 in the same situation (even when dx, dy n), and conclude that it performs well in high-dimensions. The bias is indeed zero, but we argue that the variance of udCorn remains the same order as dCorn. Sz\u00e9kely and Rizzo [2013] show that when the null is true, udCorn is well behaved.", "startOffset": 34, "endOffset": 1094}, {"referenceID": 13, "context": "For the null hypothesis of independent variables, we let (x, y) be sampled from a d-dimensional standard normal, like in Sz\u00e9kely and Rizzo [2013]. For the alternative hypothesis, we need to make a choice about how to change the covariance matrix, such that as d increases, the problem neither gets easier nor harder.", "startOffset": 121, "endOffset": 146}, {"referenceID": 13, "context": "The left panel shows that dCorn \u2192 1, udCorn \u2248 0, as predicted by Sz\u00e9kely and Rizzo [2013]. The middle panel shows that dCorn \u2192 1, udCorn \u2192 0, similar to the null.", "startOffset": 65, "endOffset": 90}], "year": 2017, "abstractText": "This paper is about two related methods for two sample testing and independence testing which have emerged over the last decade: Maximum Mean Discrepancy (MMD) for the former problem and Distance Correlation (dCor) for the latter. Both these methods have been suggested for high-dimensional problems, and sometimes claimed to be unaffected by increasing dimensionality of the samples. We will show theoretically and practically that the power of both methods (for different reasons) does actually decrease polynomially with dimension. We also analyze the median heuristic, which is a method for choosing tuning parameters of translation invariant kernels. We show that different bandwidth choices could result in the MMD decaying polynomially or even exponentially in dimension.", "creator": "LaTeX with hyperref package"}}}