{"id": "1406.2390", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2014", "title": "Unsupervised Deep Haar Scattering on Graphs", "abstract": "We introduce a deep scattering network, which computes invariants with iterated contractions adapted to training data. It defines a deep convolution network model, whose contraction properties can be analyzed mathematically. A cascade of wavelet transform convolutions are computed with a multirate filter bank, and adapted with permutations. Unsupervised learning of permutations optimize the contraction directions, by maximizing the average discriminability of training data. For Haar wavelets, it is solved with a polynomial complexity pairing algorithm. Translation and rotation invariance learning is shown with classification experiments on hand-written digits.", "histories": [["v1", "Mon, 9 Jun 2014 23:51:30 GMT  (29kb,D)", "http://arxiv.org/abs/1406.2390v1", null], ["v2", "Mon, 3 Nov 2014 15:25:16 GMT  (1277kb,D)", "http://arxiv.org/abs/1406.2390v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["xu chen", "xiuyuan cheng", "st\u00e9phane mallat"], "accepted": true, "id": "1406.2390"}, "pdf": {"name": "1406.2390.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning by Deep Scattering Contractions", "authors": ["Mia Xu Chen", "Xiuyuan Cheng", "St\u00e9phane Mallat"], "emails": ["xuchen@princeton.edu,", "xiuyuan.cheng@ens.fr,", "mallat@di.ens.fr"], "sections": [{"heading": "1 Introduction", "text": "The curse of high-dimensional learning results from the huge volume of space. Uniform scanning requires a number of examples that increase exponentially with dimension. Scattering reduces the volume of space through iterative application of contractive operators, with a deep convolution of network architecture [2, 5]. These contractions calculate invariants and increasingly reduce the dimension of space to circumvent the curse of dimensionality. We present a deep scattering architecture that is implemented with a standard multi-rate filter bank in which learning with permutation operators is introduced. It rotates space with orthogonal waveforms and implements contractions with modular perators. It offers a simple, deep scattering architecture whose properties can be mathematically analyzed because the rotation operations are not modified."}, {"heading": "2 Scattering Transforms", "text": "A scatter transformation is calculated using a cascade of wave network transformations and modular nonlinearity [2]. We reintroduce this transformation as a product of multi-rate filter bank operators closer to deep folding networks [5], and allow to generalize it for learning."}, {"heading": "2.1 Modulus Filter-Bank", "text": "J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J"}, {"heading": "2.2 Scattering as Iterated Wavelet Transforms", "text": "The scattering transformation of x-Rd is defined by a product of modulus filter operators: -Jx = \"Jx\" -Operators: \"Jx\" -Operators: \"Jx\" -Operators: \"Jx\" -Operators: -Jx \"-Operators: -Jx\" -Operators: -Jx \"-Operators: -Jx\" -Operators: -Jx \"-Operators: -Jx\" -Operators: \"Jx\" -Operators: \"Jx\" -Operators: \"Jx\" -Operators: \"Jx\" -Operators: \"Jx\" Operators: \"Jx\" Operators: \"Jx\" -Operators: \"Operators:\" Jx \"-Operators:\" Jx \"-Operators:\" -Jx: \"Operators:\" Jx \"-Operators:\" -Jx: \"-Operators:\" -Jx \"-Operators:\" -Jx \"-Operators:\" -Jx \"-Operators:\" -Operators: \"-Jx\" -Operators: \"-Jx\" -Operators: \"-Jx\" -Operators: \"-Operators:\" -Jx \"-Operators:\" -Jx \"-Operators:\" -Operators: \"-Jx\" -Operators: \"-Operators:\" -Jx \"-Operators:\" -Jx \"-Operators:\" -Jx \"-Operators:\" -Operators: \"-Jx\" -Operators: \"-Jx\" -Operators: \"-Jx\" -Operators: \"-Jx\" -Operators: \"-Operators: -Jx\" -Operators: -Jx \"-Operators: -Operators: -Jx\" -Operators: -Jx \"-Operators: -Operators: -Jx\" -Operators: -Jx \"-Operators: -Operators: -Jx\" -Operators: -Jx \"-Operators: -Operators: -Jx\" -Operators: -Operators: -Jx \"-Operators: -Jx\" -Operators: -Jx \"-Operators: -Jx\" -Operators: -Operators: -"}, {"heading": "3 Adapted Scattering", "text": "Scattering transformations are calculated by a deep folding network, whereby the network weights are calculated by the filters h and g. Learning network weights in folding networks can be interpreted as filter matching. As long as h remains an average filter and g is a high-pass filter, the modification of the filter values has marginal effects. Cascading such subsampled filtering still defines a wave conversion. The number of vanishing moments or regularity of the wave conversion can be modified [1], but the properties of the wave coefficient are not significantly changed. However, major modifications of the wave coefficient are achieved by modifying the orbit (the order of coefficients) along which the wave conversions are performed. For example, rotating invariant scattering transformations are calculated by also performing the windings along rotation parameters in a deep scattering network [8]."}, {"heading": "SJ = |W |\u03c0J ... |W |\u03c02 |W |\u03c01 .", "text": "Since each \u03c0j is a linear orthogonal operator, SJ remains contractive and maintains the signal norm. (5) Geometrically, W\u03c0j is a rotation of the signal space. It depends on \u03c0j, which modifies the \"directions\" in which the module contractions operate. The group of permutations is a group of rotations under which we will search for specific rotations that optimize the contraction directions for learning. Optimizing permutations are typically NP-hard problems. However, the problem is not as bad as it seems, because | W | calculates revolutions with filters h and g with little support. Therefore, the initial values depend on the local ordering properties. For hair filters, Section 4.1 shows that the sequence is reduced to a pairing problem."}, {"heading": "3.1 Adapted Haar Scattering", "text": "A Hair modulus filtering computesHx (n) = x (2n) J + x (2n + 1) \u221a 2 and | G | x (n) = | x (2n) \u2212 x (2n + 1) | \u221a 2. Note that (Hx (n), | x (n) pairs (x (n))) are an invariant representation of (x (2n), x (2n + 1))), representing the two values of x (2n) and x (2n) becausemax (x (2n), x (n), x (n), x (n), x (2), x (2), x (n), x (n), x (n) = Hx (n) \u2212 | g | x (n) becausemax (x (2n), x), x."}, {"heading": "4 Unsupervised Learning", "text": "Learning scattering means finding the most suitable directions along which the space can be contracted, thus optimizing permutations. At hair wavelengths, we show that this optimization can be reduced to mating problems. Afterwards, we focus on unsupervised learning."}, {"heading": "4.1 Haar Pairing", "text": "To a permutation \u03c0 we associate one by the disordered set of d / 2 pairs {(\u03c0 (2n), \u03c0 (2n + 1)} n \u2264 d / 2. Many permutations \u03c00 and \u03c01 have the same pairing p\u03c00 = p\u03c01. Hair scattering metrics are specified only by these mating operators. In fact, if p\u03c00j = p\u03c01j for 1 \u2264 j \u2264 J, it can be verified by induction to J that there is a permutation, so that S0J = | W | \u03c00JxJ... | W | \u03c00j and S1J = | W | \u03c01J... | \u03c01J = \u03c0S0J = \u03c0S1J. Thus, the scattering metrics are identical:"}, {"heading": "4.2 Unsupervised Contraction Learning", "text": "Contraction can reduce too much the distance between two points that do not belong to the same class, thus greatly reducing their discriminability. In general, this cannot be completely avoided with unattended data. However, contractions can be optimized by maximizing the average distance between training samples, so that this loss of discriminability becomes less likely. We consider unlabeled examples as realizations of a random vector X-Rd, which is an unknown mixture of different classes. The operator can greatly contract the signal space, but it should reduce the average Euclidean distance between realizations of Sj-1X as little as possible to avoid mixing up elements of different unknown classes."}, {"heading": "5 Numerical Experiments", "text": "Unsupervised contraction learning does not discriminate between subclasses and can therefore mostly learn sources of variability common to most classes. Geometric variability such as translations, rotations or deformations are such examples. MNIST number recognition databases provide a simple framework for studying these sources of variability.Learning translations and deformations is evaluated in Section 5.1 on the original MNIST database, which includes 60,000 training samples and 10,000 test samples. A modified MNIST database of 3D number rotations is examined in Section 5.2."}, {"heading": "5.1 MNIST Digit Recognition", "text": "We consider a random permutation of the MNIST image pixel, illustrated in Figure 2a = 3. Each image is therefore considered an unordered bag of pixels. These experiments test several aspects of the algorithm: the ability to provide spatial neighborhood information and classification accuracy without location information.Permutation learning is initially performed with inner node pairing optimizations, which means that for each plane j the same pairing function can be used to spatially match the coefficients of each Sj, kx for 0 \u2264 k < 2j. These pairings perform a multi-scale estimation of relative spatial locations. We say that two coefficients Sj, k) and Sj, which are spatially connected if they are compatially connected with operators whose support is spatially connected to size 2j. We only consider coefficients whose amplitude is not negligible and thus play a role in classification."}, {"heading": "5.2 MNIST with 3D Rotations", "text": "To test the ability of the algorithm to invariably rely on various sources of geometric variability, we use the 3D rotated MNIST database constructed in [3]. The digit \"9\" is removed from the dataset as it corresponds to the rotation of the digit \"6.\" Each digit is projected onto a 3D sphere that is scanned over d = 4096 points and rotated randomly on the sphere, with a rotation variance \u03c32 = 0.2 [3].Translations in the plane are now replaced by rotations above the sphere on which, after scanning, no folding operators can be defined. The classification algorithms in [3] introduce an elegant solution that replaces folding operators (diagonally in Fourier) with operators arranged diagonally above the laplac eigenvectors on a graph. These algorithms use the 3D neighborhood of points on the sphere to define the connectivity of the net-based graph% 2, which gives a property-based% in a table of 19%, 5.2%, which gives a property-based result."}], "references": [{"title": "A Wavelet Tour of Signal Processing: The Sparse Way", "author": ["S. Mallat"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Invariant Scattering Convolution Networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Trans. PAMI, 35(8): 1872- 1886, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1872}, {"title": "Spectral Networks and Deep Locally Connected Networks on Graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "ICLR 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutional Networks and Applications in Vision", "author": ["Y. LeCun", "K. Kavukvuoglu", "C. Farabet"], "venue": "Proc. IEEE Int. Sump. Circuits and Systems 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "A dual tree complex wavelet transform", "author": ["G. Selesnick", "I. Baraniuk", "Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Wavelet analysis and signal processing\u201d, in Wavelets and Their Applications, Jones and Barlett", "author": ["R. Coifman", "Y. Meyer", "M. Wickerhauser"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination", "author": ["L. Sifre", "S. Mallat"], "venue": "CVPR 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Paths, trees, and flowers", "author": ["J. Edmonds"], "venue": "Canadian Journal of Mathematics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1965}, {"title": "Gabow\u2019s, \u201cAn Efficient Implementation of Edmond\u2019s Algorithm for Maximum Matching on Graphs.", "author": ["H.E. Rothberg of"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1976}, {"title": "On signal reconstruction without phase", "author": ["R. Balana", "P. Casazza", "D. Edidinb"], "venue": "Applied and Computational Harmonic Analysis, 2006. 9", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": "Scattering transforms reduce the space volume by iteratively applying contractive operators, with a deep convolution network architecture [2, 5].", "startOffset": 138, "endOffset": 144}, {"referenceID": 4, "context": "Scattering transforms reduce the space volume by iteratively applying contractive operators, with a deep convolution network architecture [2, 5].", "startOffset": 138, "endOffset": 144}, {"referenceID": 3, "context": "We show that such optimizations amounts to computing sparse signal representations, similarly to sparse auto-encoders [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "A scattering transform is computed with a cascade of wavelet transforms and modulus non-linearities [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "We reintroduce this transformation as a product of multirate filter bank operators, which is closer to deep convolution networks [5], and allows to generalize it for learning.", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "It however requires to slightly modify the filter bank by first extending the dimension of x from d to 2d with a linear interpolation [6].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "One can prove thatW = (H,G) is then an orthogonal operator in R and that {\u03c6J(n \u2212 k2), \u03c8j(n \u2212 k2)}0\u2264k\u2264d2\u2212j is a wavelet orthonormal basis of R [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": "It is then a linear wavelet packet orthogonal transform introduced in [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "The number of vanishing moments or the wavelet regularity may be modified [1], but it does not modify much the wavelet coefficient properties.", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "For example, rotation invariant scattering transforms are calculated by also computing convolutions along rotation parameters in a deep scattering network [8].", "startOffset": 155, "endOffset": 158}, {"referenceID": 10, "context": "In the real case, for almost all pairs of unitary operators (A,B) it has been proved [11] that the operator (|A|, |B|) is invertible on R.", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "then p\u03c0\u2217 = arg minp\u03c0 C(p\u03c0) is computed with O(d ) operations with the Blossom Algorithm of Edmonds [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "Computations in this paper use the implementation in [10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "53%, [5]), and scattering networks with Gabor wavelet (0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "43% [2]).", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "To test the algorithm ability to build invariant to different source of geometric variability, we use the 3D rotated MNIST data basis constructed in [3].", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "2 [3].", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "The classification algorithms in [3] introduces an elegant solution which replaces convolution operators (diagonal in Fourier) by operators which are diagonal over the Laplacian eigenvectors on a graph.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Table 2 gives the results reported in [3], with 19% error for a nearest neighbor algorithm, 5.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "[3] Spectral Net.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Adapted Haar Scat.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Table 2: Percentage of errors on MNIST 3D rotation data set [3], with a nearest neighbor classifier, a fully connected two layer neural network, a locally connected network, a spectral network [3], and an adapted Haar scattering.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Table 2: Percentage of errors on MNIST 3D rotation data set [3], with a nearest neighbor classifier, a fully connected two layer neural network, a locally connected network, a spectral network [3], and an adapted Haar scattering.", "startOffset": 193, "endOffset": 196}], "year": 2014, "abstractText": "We introduce a deep scattering network, which computes invariants with iterated contractions adapted to training data. It defines a deep convolution network model, whose contraction properties can be analyzed mathematically. A cascade of wavelet transform convolutions are computed with a multirate filter bank, and adapted with permutations. Unsupervised learning of permutations optimize the contraction directions, by maximizing the average discriminability of training data. For Haar wavelets, it is solved with a polynomial complexity pairing algorithm. Translation and rotation invariance learning is shown with classification experiments on hand-written digits.", "creator": "LaTeX with hyperref package"}}}