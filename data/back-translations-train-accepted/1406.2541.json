{"id": "1406.2541", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions", "abstract": "We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.", "histories": [["v1", "Tue, 10 Jun 2014 13:29:09 GMT  (3864kb,D)", "http://arxiv.org/abs/1406.2541v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jos\u00e9 miguel hern\u00e1ndez-lobato", "matthew w hoffman", "zoubin ghahramani"], "accepted": true, "id": "1406.2541"}, "pdf": {"name": "1406.2541.pdf", "metadata": {"source": "CRF", "title": "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions", "authors": ["Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Matthew W. Hoffman", "Zoubin Ghahramani"], "emails": ["jmh233@cam.ac.uk", "mwh30@cam.ac.uk", "zoubin@eng.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The goal of these methods is to find the global optimizer of a nonlinear and generally unconventional function whose derivatives are not available. (In addition, the additional calculations are usually corrupted by the noise and process necessary to find global optimization.) Optimization problems are widespread in science and technology, and as a result, Bayesian optimizations have been successfully used in robotics to adjust the parameters of a robot to speed and smoothness."}, {"heading": "2 Predictive entropy search", "text": "We propose to follow the information theory method of active data collection described in [17]. We are interested in maximizing information about the position x? of the global maximum, whose posterior distribution is p (x? | Dn). Our current information about x? can be measured in relation to the negative differential entropy of p (x? | Dn). Therefore, our strategy is to select xn + 1, which maximizes the expected decrease in this quantity. (2) The corresponding capture function is related to the negative differential entropy of p (x? | Dn) \u2212 Ep (y | Dn, x) [H [p (x? | Dn], which maximizes the anticipated decrease in this quantity)]]], (2) where H [p (x) = p (x) is logged."}, {"heading": "2.1 Sampling from the posterior over global maxima", "text": "In this section we will show how to approximate the sample from the approximate distribution of the global maximizer x? given the observed data Dn, that is, p (x? | Dn) = p (f (x?) = maxx (x). If domain X is limited to a finite series of m-points, the latent function f takes the form of a m-dimensional vector f. The probability that the ith element of f is optimal can then be described as p (f | Dn). This procedure is limited to an finite series of m-points, which suggests the following generative process: i) Draw a sample from the posterior distribution p (f | Dn) and ii) return the index of the maximum element in the sampled vector. This process is known as Thompson sampling or probability matching when used as an arm selection strategy in multiarmed bandits."}, {"heading": "2.2 Approximating the predictive entropy", "text": "We assume that we use the argument in this expression as p (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x, x (x), x (x), x (x), x (x, x (x), x (x), x (x), x (x, x (x), x (x), x (x, x (x), x (x), x (x, x (x), x (x), x (x (x), x (x), x (x, x, x (x), x (x), x (x, x (x), x (x, x), x (x (x), x (x), x (x, x (x), x, x (x), x (x), x (x), x, x (x, x, x (x), x, x (x), x (x (x), x, x (x, x (x), x, x (x), x (x (x), x, x (x), x (x (x), x (x (x), x), x (x (x), x, x (x), x (x (x), x, x, x (x (x), x, x (x), x, x, x (x), x (x (x), x"}, {"heading": "2.3 Hyperparameter learning and the PES acquisition function", "text": "This acquisition function performs a formal treatment of the hyperparameters. Let us designate a vector of hyperparameters that includes all kernel parameters and noise variance \u03c32. Let us p (credit | Dn) p (credit | credit) p (credit) p (credit) p (credit) p (credit | credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) n) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) p (credit) credit). The corresponding integral has no analytical expression and must be approximated using Monte Carlo (credit). This approach is also taken in [24]. We draw M samples (i) from p (credit) x)."}, {"heading": "3 Experiments", "text": "In our experiments we use Gaussian process priors for f with quadratic exponential nuclei k = 10 previous measurements = 10 previous measurements = 2 exp {\u2212 0.5 \u2211 i (xi \u2212 x \u2032 i) 2 / '2i}. The corresponding spectral density is zero-mean Gaussian with covariance given by diag ([' \u2212 2i]) and its normalizing constant x x x x x x x x. We compare the PES approximation (10) with the approximation applied by the entropy search (ES). We also compare the accuracy of PES in the task of approximating differential entropy (2). We compare the method of entropy (ES) that we use."}, {"heading": "3.1 Experiments with real-world functions", "text": "Finally, we optimize various real cost functions. The first (NNet) returns the predictive accuracy of a neural network on a random pull / test partition of the Boston Housing Dataset [3].The variables to be optimized are the weight-decay parameter and the number of training iterations for the neural network; the second (hydrogen) returns the amount of hydrogen production of a certain bacterium in relation to the PH and nitrogen levels of the growth medium [7]; the third (portfolio) returns the ratio of the mean and the standard deviation (the sharpe ratio) of the 1-year hydrogen numbers generated by a certain bacterium in relation to the PH and nitrogen levels of the growth medium [7]; the third (portfolio) returns the ratio of the mean and the standard deviation (the sharpe ratio) of the 1-year hydrogen numbers generated by a particular bacterium BA, which are generated by simulations from a multivant of the growth medium [7]; the third (portfolio) returns the ratio of the mean and the standard deviation (the sharpe ratio) of the 1-year hydrogen numbers generated by a particular bacterium; the hydrogen numbers BA, which are generated by simulations from a hydrogen portfolio of a time model, the hydrogen, the XCH, and the hydrogen model is adapted to the X."}, {"heading": "4 Conclusions", "text": "Our method, the predictive entropy search (PES), greedily maximizes the amount of one-step information about the position x? of the global maximum through its rear differential entropy. Since this objective function is insoluble, PES approaches the original goal through a repair parameterization that measures entropy in the rear distribution of its hyperparameters. PES produces more accurate approximations than entropy search (ES), a method based on the original, untransformed acquisition function. In addition, PES can slightly marginalize its approximation in terms of the rear distribution of its hyperparameters, while ES cannot. Experiments with synthetic and real functions show that PES ES ES ES often performs better in terms of immediate regret than ES. In these experiments, we also observe that PES often achieves better results than expected improvements (EI), a popular heuristic method for Bayesian functions, while it often seems that PES excels with more effective functions."}, {"heading": "A Details on approximating GP sample paths", "text": "In this section, we will give more details on the approach used in Section 2.1 to approximate a GP using random features. These random features can be used to approximate sample paths from the GP posterior. By optimizing these sample paths, we obtain random samples via the global maxima x?. We derive the kernel approximation in more detail (5). Formally, the theorem of [4] states is theorem 1 (Bochner's theorem). A continuous, shift-invariant kernel is positively defined if and only if it is the Fourier transformation of a non-negative, finite measurement. As a result, we can expect some kernels k (x, x) = x-x-x-x-x-x-x, 0) there must be an associated density s (w), known as its spectral density, which is the Fourier dual of k."}, {"heading": "B Details on approximating the predictive variance", "text": "In this context we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random variables, at the random variables, we will look at the random variables, we will look at the random variables, at the random variables, we will look at the random variables, we will look at the random variables, we will look at the random observations, at the random observations, at the random variations, the underlying, the underlying, the underlying."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>We propose a novel information-theoretic approach for Bayesian optimization<lb>called Predictive Entropy Search (PES). At each iteration, PES selects the next<lb>evaluation point that maximizes the expected information gained with respect to<lb>the global maximum. PES codifies this intractable acquisition function in terms<lb>of the expected reduction in the differential entropy of the predictive distribu-<lb>tion. This reformulation allows PES to obtain approximations that are both more<lb>accurate and efficient than other alternatives such as Entropy Search (ES). Fur-<lb>thermore, PES can easily perform a fully Bayesian treatment of the model hy-<lb>perparameters while ES cannot. We evaluate PES in both synthetic and real-<lb>world applications, including optimization problems in machine learning, finance,<lb>biotechnology, and robotics. We show that the increased accuracy of PES leads to<lb>significant gains in optimization performance.<lb>", "creator": "LaTeX with hyperref package"}}}