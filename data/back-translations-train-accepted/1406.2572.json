{"id": "1406.2572", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.", "histories": [["v1", "Tue, 10 Jun 2014 14:52:14 GMT  (1044kb,D)", "http://arxiv.org/abs/1406.2572v1", "The theoretical review and analysis in this article draw heavily fromarXiv:1405.4604[cs.LG]"]], "COMMENTS": "The theoretical review and analysis in this article draw heavily fromarXiv:1405.4604[cs.LG]", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["yann n dauphin", "razvan pascanu", "\u00e7aglar g\u00fcl\u00e7ehre", "kyunghyun cho", "surya ganguli", "yoshua bengio"], "accepted": true, "id": "1406.2572"}, "pdf": {"name": "1406.2572.pdf", "metadata": {"source": "CRF", "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "authors": ["Yann N. Dauphin", "Razvan Pascanu"], "emails": ["dauphiya@iro.umontreal.ca", "r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@umontreal.ca", "sganguli@standford.edu", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": null, "text": "A key challenge for many areas of science and technology is the minimization of non-convex error functions over continuous, high-dimensional spaces. Gradient descent or quasi-Newton methods are used almost ubiquitously to perform such minimizations, and it is often thought that a major source of the difficulty of these local methods to find the global minimum is the proliferation of local minimums with much higher errors than the global minimum. At this point, we argue, based on results from statistical physics, random matrix theory, neural network theory and empirical evidence, that a deeper and deeper difficulty emanates from the proliferation of local minimums, especially in high-dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning and give the illusory impression that there is a local minimum. Motivated by these arguments, we propose a new approach to high-order quantum algorithms, which can rapidly advance the second-order algorithm algorithm optimization method."}, {"heading": "1 Introduction", "text": "It is often the case that our geometric intuition, derived from experience in a low-dimensional physical world, is insufficient to think about the geometry of typical fault surfaces in high-dimensional spaces. (Rasmussen and Williams, 2005) have shown that such a random error function would have many local minima and maxima, with high probability about the choice of function, but saddles would occur with negligible probability. On the other hand, as we look below, random error functions via N scale variables, or dimensions, are always highly probable about the use of the function."}, {"heading": "2 The prevalence of saddle points in high dimensions", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to pave the way for the future."}, {"heading": "3 Experimental validation of the prevalence of saddle points", "text": "In this section, we are experimentally testing whether the theoretical predictions of Bray and Dean (2007) apply to random Gaussian fields for neural networks. To our knowledge, this is the first attempt to measure the relevant statistical properties of fault surfaces of neural networks and to test whether the theory developed for random Gaussian fields is generalized to such cases. In particular, we are interested in how the critical points of a single layer of MLP are distributed at the -\u03b1 plane and how the properties of the Hessian matrix are distributed at these critical points. We used a small MLP based on a sampled version of MNIST and CIFAR-10. The Newton method was used to identify critical points of fault function.The results are shown in Figure 1. Further details of the setup are in Appendix C. This empirical test confirms that the observations of Bray and Dean (2007) apply qualitatively to neural networks."}, {"heading": "4 Dynamics of optimization algorithms near saddle points", "text": "Given the prevalence of saddle points, it is important to understand how different optimization algorithms behave in their proximity. Let's focus on non-degenerated saddle points for which Hessian is not unique; these critical points can be analyzed locally by following Morse's lesson (see Chapter 7.3, Theorem 7.16 in Callahan (2010) or Appendix B: f (3) + 12nbsp; (1), where the Hessian model represents the eigenvalue of the Hessian method and the new parameters of the model that correspond to the motion along the eigenvectors are. A step of the gradient method always points in the right direction close to a saddle point. (SGD in Fig. 2) If a eigenvalue is positive, then the step moves in the direction of the Hessian."}, {"heading": "5 Generalized trust region methods", "text": "To address the saddle problem and overcome the shortcomings of the above methods, we define a class of generalized trust region methods and seek an algorithm within that space. This class includes a straightforward extension of the classical trust region methods with two simple changes: (1) We allow the minimization of a first-order Taylor extension, rather than always relying on a second-order Taylor extension, as is typically done in trust region methods, and (2) we replace the restriction to the norm of the step \"000\" with a restriction to the distance between the two steps. The choice of distance function and Taylor expansion sequence therefore specifies an algorithm. If we define Tk (f, \u03b8, \u0430) to define the k-th Taylor expansion of the fifth order around it, we can summarize a generalized trust region method as follows:"}, {"heading": "6 Attacking the saddle point problem", "text": "We cannot simply solve the problem of the saddle by taking into account the value of each individual value without it being obvious. In particular, the analysis of the optimization algorithms near saddle points discussed in Sec. 4 suggests a simple heuristic solution that implements the gradients along the respective learning dynamics. The idea of taking the absolute value of the eigenvalues of the Hessian Method was proposed even before the introduction, see for example (Nocedal and Wright, 2006, chapter 3.4) or Murray, chapter 4.1). We are not aware of an adequate justification of these algorithms or even a detailed exploration of this idea."}, {"heading": "7 Experimental validation of the saddle-free Newton method", "text": "In this section we evaluate empirically the theory that suggests the existence of many saddle points in high-dimensional functions by training neural networks."}, {"heading": "7.1 Feedforward Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1.1 Existence of Saddle Points in Neural Networks", "text": "In this section, we validate the existence of saddle points in the cost function of neural networks and see how each of the previously described algorithms behaves in their proximity. To minimize the impact of any kind of approach used in the algorithms, we train small neural networks on the stripped-down version of MNIST and CIFAR-10, where we can accurately calculate the update directions through any algorithm. Both MNIST and CIFAR-10 were calculated to be of size 10 \u00d7 10. We compare Minibatch stochastic gradient descent (MSGD), dampen Newton and the proposed saddle-free Newton method (SFN). Hyperparameters of SGD were selected by random search (Bergstra and Bengio, 2012), and the attenuation coefficients for the damped saddle position of Newton and the saddle-free saddle position of Newton methods become clear."}, {"heading": "7.1.2 Effectiveness of saddle-free Newton Method in Deep Neural Networks", "text": "Here we further demonstrate the effectiveness of the proposed saddle-free Newton method in a larger neural network with seven hidden layers. The neural network is a deep auto encoder trained on (holistic) MNIST, which is considered the standard yardstick for evaluating the performance of optimization algorithms in neural networks (Sutskever et al., 2013). For this large-scale problem, we used the previously described Krylov subspace descent with 500 subspace vectors. We first trained the model with SGD and observed that learning stables after reaching the Meansquared Error (MSE) of 1.0. We then continued with the saddle-free Newton method, which quickly escaped the (approximate) plateau on which SGD adhered to (see Fig. 4 (a))). Furthermore, even in these large-scale experiments, we were able to confirm that the distribution of the Hessianic eigenvalues varies according to the previous 57% Newton error, if the SSE is faster than the previous one."}, {"heading": "7.2 Recurrent Neural Networks: Hard Optimization Problem", "text": "Recurrent neural networks are known to be more difficult to train than forward-facing neural networks (see e.g. Bengio et al., 1994; Pascanu et al., 2013). In practice, they tend not to fit, and in this section we want to test whether the proposed saddle-free Newton method can help prevent underfit, provided it is caused by saddle points. We trained a small relapsing neural network with 120 hidden units for the task of character-level linguistic modeling on the Penn Treebank corpus. Similar to the previous experiment, we trained the model with SGD until it was clear that learning was stalled. From then on, the training continued with the saddle-free Newton method. In Fig. 4 (c) we see a trend similar to that we observed in the previous experiments with forward-facing neural networks, where the SGD method exhibits significantly fewer errors than the method proposed by the attacker."}, {"heading": "8 Conclusion", "text": "In summary, (a) non-convex fault surfaces in high-dimensional spaces generally suffer from a proliferation of saddle points, and (b) contrary to conventional wisdom derived from low-dimensional intuition, these tests were made possible by a novel application of the Newton method to search for critical points of each index (fraction of negative eigenvalues), and they confirmed the most important qualitative predictions of the theory that the index of a critical point is in neural network fault surfaces."}, {"heading": "Acknowledgments", "text": "We would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) as well as CIFAR and the Canada Research Chairs for funding and Comute Canada and Calcul Que \u0301 bec for providing computing resources. Razvan Pascanu is supported by a DeepMind Google Fellowship. Surya Ganguli thanks the Burroughs Wellcome and Sloan Foundations for their support."}, {"heading": "A Description of the different types of saddle-points", "text": "A critical point is by definition a point at which the gradient of f (\u03b8) disappears. All critical points of f (\u03b8) can be further characterized by the curvature of the function in its environment, as described by the eigenvalues of Hessian. Note that the Hessian is symmetrical and therefore the eigenvalues are real numbers. In the following, the four possible scenarios: \u2022 If all eigenvalues are not zero and positive, then the critical point is a local minimum. \u2022 If all eigenvalues are not zero and negative, then the critical point is a local maximum. \u2022 If the eigenvalues are not zero and we have both positive and negative eigenvalues, then the critical point is a saddle point with a minimum-max structure (see Figure 5 (b). This is when we restrict the function f to the subspace spanned by the eigenvectors that correspond to positive (negative) eigenvalues, then the saddle point is a minimum."}, {"heading": "B Reparametrization of the space around saddle-points", "text": "This repairment is given by a Taylor extension of the function f by the critical point. If we assume that the Hessian is not singular, then around this critical point there is a neighborhood in which this approximation is reliable, and since the first derivatives of the order disappear, the Taylor expansion is given by the following values: f (\u03b8 *) = f (\u03b8 *) + 12 (\u0432) > H (6) Now let us rewrite the eigenvectors of the Hessian H with e1,... and with \u03bb1,...,... the corresponding eigenvalues. We can now change the coordinates into the span by these eigenvectors: \u0441v = 12 e1 >.... enuml > (7) f (\u03b8 *) = f (\u03b8 *) + 12n\u043d * i = 1\u0441\u0442i (ei > \u0432) 2 = f (habi *) + n."}, {"heading": "C Empirical exploration of properties of critical points", "text": "To get the plot on MNIST, we used the Newton method to detect nearby critical points along the path taken by the saddle-free Newton algorithm. We look at 20 different runs of the saddle-free algorithm, each of which uses a different random seed. We then execute 200 runs. The first 100 runs look for critical points close to the value of the parameters obtained by a random number of epochs (between 0 and 20) of a randomly selected run (among the 20 different runs) of the saddle-free Newton method. To this starting point, a uniform noise of the small amplitude is added (the amplitude is randomly selected between the different values {10 \u2212 1, 10 \u2212 2, 10 \u2212 3, 10 \u2212 4} The last 100 runs look for critical points near uniformly stamped weights (the range of the weights is given by the cube unit of measurement).The task (dataset and model) is the same as the one previously used on the other parameters."}, {"heading": "D Proof of Lemma 1", "text": "Lemma 2 is a non-singular quadratic matrix in Rn > Rn, and x > Rn is any vector. Then, the different eigenvectors of A and B \u2212 x are the corresponding eigenvalues. - We rewrite the identity by expressing the vector x in relation to these eigenvalues: | x > Ax | = the different eigenvectors of A and B \u2212 1. We rewrite the identity by expressing the vector x in relation to these eigenvalues: | x > Ax | = the different eigenvalues of A and B."}, {"heading": "F Experiments", "text": "Q. 1 Existence of Saddle Points in Neural Networks \u2022 Size of minibatch \u2022 Momentum coefficient For the random search we draw the following hyperparameters using the random search strategy (Bergstra and Bengio, 2012): \u2022 Learning rate \u2022 Size of minibatch \u2022 Momentum coefficient, we draw 80 samples and pick the best one.2 Effectiveness of satdle-free Newton Method in Deep Neural NetworksThe deep auto-encoder was first trained using the protocol used by Sutskever et al. (2013) we use classical momentum.F.3 Recurrent Neural Networks: Hard Optimization ProbleWe was initialized the recurural weight of RNN to be orthogonal search, we is hidden number of Saxe."}], "references": [{"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks, 2(1), 53\u201358.", "citeRegEx": "Baldi and Hornik,? 1989", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "5(2), 157\u2013166. Special Issue on Recurrent Neural Networks, March 94.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, 13, 281\u2013305.", "citeRegEx": "Bergstra and Bengio,? 2012", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy).", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Statistics of critical points of gaussian fields on large-dimensional spaces", "author": ["A.J. Bray", "D.S. Dean"], "venue": "Physics Review Letter, 98, 150201.", "citeRegEx": "Bray and Dean,? 2007", "shortCiteRegEx": "Bray and Dean", "year": 2007}, {"title": "Advanced Calculus: A Geometric View", "author": ["J. Callahan"], "venue": "Undergraduate Texts in Mathematics. Springer.", "citeRegEx": "Callahan,? 2010", "shortCiteRegEx": "Callahan", "year": 2010}, {"title": "Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity", "author": ["Y.V. Fyodorov", "I. Williams"], "venue": "Journal of Statistical Physics, 129(5-6), 1081\u20131116.", "citeRegEx": "Fyodorov and Williams,? 2007", "shortCiteRegEx": "Fyodorov and Williams", "year": 2007}, {"title": "On-line learning theory of soft committee machines with correlated hidden units steepest gradient descent and natural gradient descent", "author": ["M. Inoue", "H. Park", "M. Okada"], "venue": "Journal of the Physical Society of Japan, 72(4), 805\u2013810.", "citeRegEx": "Inoue et al\\.,? 2003", "shortCiteRegEx": "Inoue et al\\.", "year": 2003}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["N. Le Roux", "Manzagol", "P.-A.", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Roux et al\\.,? 2007", "shortCiteRegEx": "Roux et al\\.", "year": 2007}, {"title": "Deep learning via hessian-free optimization", "author": ["J. Martens"], "venue": "International Conference in Machine Learning, pages 735\u2013742.", "citeRegEx": "Martens,? 2010", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "An analysis on negative curvature induced by singularity in multi-layer neural-network learning", "author": ["E. Mizutani", "S. Dreyfus"], "venue": "Advances in Neural Information Processing Systems, pages 1669\u20131677.", "citeRegEx": "Mizutani and Dreyfus,? 2010", "shortCiteRegEx": "Mizutani and Dreyfus", "year": 2010}, {"title": "Newton-type methods", "author": ["W. Murray"], "venue": "Technical report, Department of Management Science and Engineering, Stanford University.", "citeRegEx": "Murray,? 2010", "shortCiteRegEx": "Murray", "year": 2010}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "Springer.", "citeRegEx": "Nocedal and Wright,? 2006", "shortCiteRegEx": "Nocedal and Wright", "year": 2006}, {"title": "Mean field theory of spin glasses: statistics and dynamics", "author": ["G. Parisi"], "venue": "Technical Report Arxiv 0706.0094.", "citeRegEx": "Parisi,? 2007", "shortCiteRegEx": "Parisi", "year": 2007}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Pascanu and Bengio,? 2014", "shortCiteRegEx": "Pascanu and Bengio", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML\u20192013.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "On the saddle point problem for non-convex optimization", "author": ["R. Pascanu", "Y. Dauphin", "S. Ganguli", "Y. Bengio"], "venue": "Technical Report Arxiv 1405.4604.", "citeRegEx": "Pascanu et al\\.,? 2014", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Fast exact multiplication by the hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation, 6, 147\u2013 160.", "citeRegEx": "Pearlmutter,? 1994", "shortCiteRegEx": "Pearlmutter", "year": 1994}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "The MIT Press.", "citeRegEx": "Rasmussen and Williams,? 2005", "shortCiteRegEx": "Rasmussen and Williams", "year": 2005}, {"title": "Natural Gradient Descent for On-Line Learning", "author": ["M. Rattray", "D. Saad", "S.I. Amari"], "venue": "Physical Review Letters, 81(24), 5461\u20135464.", "citeRegEx": "Rattray et al\\.,? 1998", "shortCiteRegEx": "Rattray et al\\.", "year": 1998}, {"title": "On-line learning in soft committee machines", "author": ["D. Saad", "S.A. Solla"], "venue": "Physical Review E, 52, 4225\u20134243.", "citeRegEx": "Saad and Solla,? 1995", "shortCiteRegEx": "Saad and Solla", "year": 1995}, {"title": "Learning hierarchical category structure in deep neural networks", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": "Proceedings of the 35th annual meeting of the Cognitive Science Society, pages 1271\u20131276.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural network", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Saxe et al\\.,? 2014", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["J. Sohl-Dickstein", "B. Poole", "S. Ganguli"], "venue": "ICML\u20192014.", "citeRegEx": "Sohl.Dickstein et al\\.,? 2014", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "S. Dasgupta and D. Mcallester, editors, Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pages 1139\u20131147. JMLR Workshop and Conference Proceedings.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Krylov Subspace Descent for Deep Learning", "author": ["O. Vinyals", "D. Povey"], "venue": "AISTATS.", "citeRegEx": "Vinyals and Povey,? 2012", "shortCiteRegEx": "Vinyals and Povey", "year": 2012}, {"title": "On the distribution of the roots of certain symmetric matrices", "author": ["E.P. Wigner"], "venue": "The Annals of Mathematics, 67(2), 325\u2013327.", "citeRegEx": "Wigner,? 1958", "shortCiteRegEx": "Wigner", "year": 1958}, {"title": "The number of hidden units of RNN is fixed to 120", "author": ["We initialized the recurrent weights of RNN to be orthogonal as suggested by Saxe"], "venue": "For recurrent neural networks using SGD, we choose the following hyperparameters using the random search strategy:", "citeRegEx": "Saxe,? 2014", "shortCiteRegEx": "Saxe", "year": 2014}, {"title": "Since it is costly to compute the exact Hessian for RNN\u2019s, we used the eigenvalues of the Hessian in the Krylov subspace to plot the distribution of eigenvalues for Hessian matrix in Fig", "author": ["Pascanu"], "venue": "4 (d). 14", "citeRegEx": "Pascanu,? 2013", "shortCiteRegEx": "Pascanu", "year": 2013}], "referenceMentions": [{"referenceID": 16, "context": "This work extends the results of Pascanu et al. (2014).", "startOffset": 33, "endOffset": 55}, {"referenceID": 19, "context": "(Rasmussen and Williams, 2005) have shown that such a random error function would have many local minima and maxima, with high probability over the choice of the function, but saddles would occur with negligible probability.", "startOffset": 0, "endOffset": 30}, {"referenceID": 16, "context": "In this work, which is an extension of the previous report Pascanu et al. (2014), we first want to raise awareness of this issue, and second, propose an alternative approach to second-order optimization that aims to rapidly escape from saddle points.", "startOffset": 59, "endOffset": 81}, {"referenceID": 27, "context": "We know that for a large Gaussian random matrix the eigenvalue distribution follows Wigner\u2019s famous semicircular law (Wigner, 1958), with both mode and mean at 0.", "startOffset": 117, "endOffset": 131}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach).", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach).", "startOffset": 0, "endOffset": 51}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach).", "startOffset": 0, "endOffset": 201}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach). One particular result by Bray and Dean (2007) derives how critical points are distributed in the vs \u03b1 plane, where \u03b1 is the index, or the fraction of negative eigenvalues of the Hessian at the critical point, and is the error attained at the critical point.", "startOffset": 0, "endOffset": 286}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach). One particular result by Bray and Dean (2007) derives how critical points are distributed in the vs \u03b1 plane, where \u03b1 is the index, or the fraction of negative eigenvalues of the Hessian at the critical point, and is the error attained at the critical point. Within this plane, critical points concentrate on a monotonically increasing curve as \u03b1 ranges from 0 to 1, implying a strong correlation between the error and the index \u03b1: the larger the error the larger the index. The probability of a critical point to be an O(1) distance off the curve is exponentially small in the dimensionality N , for large N . This implies that critical points with error much larger than that of the global minimum, are exponentially likely to be saddle points, with the fraction of negative curvature directions being an increasing function of the error. Conversely, all local minima, which necessarily have index 0, are likely to have an error very close to that of the global minimum. Intuitively, in high dimensions, the chance that all the directions around a critical point lead upward (positive curvature) is exponentially small w.r.t. the number of dimensions, unless the critical point is the global minimum or stands at an error level close to it, i.e., it is unlikely one can find a way to go further down. These results may also be understood via random matrix theory. We know that for a large Gaussian random matrix the eigenvalue distribution follows Wigner\u2019s famous semicircular law (Wigner, 1958), with both mode and mean at 0. The probability of an eigenvalue to be positive or negative is thus /2. Bray and Dean (2007) showed that the eigenvalues of the Hessian at a critical point are distributed in the same way, except that the semicircular spectrum is shifted by an amount determined by .", "startOffset": 0, "endOffset": 1861}, {"referenceID": 22, "context": "They qualitatively recapitulate aspects of the hierarchical development of semantic concepts in infants (Saxe et al., 2013).", "startOffset": 104, "endOffset": 123}, {"referenceID": 21, "context": "In (Saad and Solla, 1995) the dynamics of stochastic gradient descent are analyzed for soft committee machines.", "startOffset": 3, "endOffset": 25}, {"referenceID": 20, "context": "The slow learning dynamics within this submanifold originates from saddle point structures (caused by permutation symmetries among hidden units), and their associated plateaus (Rattray et al., 1998; Inoue et al., 2003).", "startOffset": 176, "endOffset": 218}, {"referenceID": 8, "context": "The slow learning dynamics within this submanifold originates from saddle point structures (caused by permutation symmetries among hidden units), and their associated plateaus (Rattray et al., 1998; Inoue et al., 2003).", "startOffset": 176, "endOffset": 218}, {"referenceID": 0, "context": "However, is this result for generic error landscapes applicable to the error landscapes of practical problems of interest? Baldi and Hornik (1989) analyzed the error surface of a multilayer perceptron (MLP) with a single linear hidden layer.", "startOffset": 123, "endOffset": 147}, {"referenceID": 0, "context": "However, is this result for generic error landscapes applicable to the error landscapes of practical problems of interest? Baldi and Hornik (1989) analyzed the error surface of a multilayer perceptron (MLP) with a single linear hidden layer. Such an error surface shows only saddle-points and no local minima. This result is qualitatively consistent with the observation made by Bray and Dean (2007). Indeed Saxe et al.", "startOffset": 123, "endOffset": 400}, {"referenceID": 0, "context": "However, is this result for generic error landscapes applicable to the error landscapes of practical problems of interest? Baldi and Hornik (1989) analyzed the error surface of a multilayer perceptron (MLP) with a single linear hidden layer. Such an error surface shows only saddle-points and no local minima. This result is qualitatively consistent with the observation made by Bray and Dean (2007). Indeed Saxe et al. (2014) analyzed the dynamics of learning in the presence of these saddle points, and showed that they arise due to scaling symmetries in the weight space of a deep linear MLP.", "startOffset": 123, "endOffset": 427}, {"referenceID": 0, "context": "However, is this result for generic error landscapes applicable to the error landscapes of practical problems of interest? Baldi and Hornik (1989) analyzed the error surface of a multilayer perceptron (MLP) with a single linear hidden layer. Such an error surface shows only saddle-points and no local minima. This result is qualitatively consistent with the observation made by Bray and Dean (2007). Indeed Saxe et al. (2014) analyzed the dynamics of learning in the presence of these saddle points, and showed that they arise due to scaling symmetries in the weight space of a deep linear MLP. These scaling symmetries enabled Saxe et al. (2014) to find new exact solutions to the nonlinear dynamics of learning in deep linear networks.", "startOffset": 123, "endOffset": 648}, {"referenceID": 5, "context": "In this section, we experimentally test whether the theoretical predictions presented by Bray and Dean (2007) for random Gaussian fields hold for neural networks.", "startOffset": 89, "endOffset": 110}, {"referenceID": 5, "context": "This empirical test confirms that the observations by Bray and Dean (2007) qualitatively hold for neural networks.", "startOffset": 54, "endOffset": 75}, {"referenceID": 6, "context": "16 in Callahan (2010) or Appendix B:", "startOffset": 6, "endOffset": 22}, {"referenceID": 13, "context": "This can be done regardless of the approximation strategy used for the Newton method such as a truncated Newton method or a BFGS approximation (see Nocedal and Wright (2006) chapters 4 and 7).", "startOffset": 148, "endOffset": 174}, {"referenceID": 11, "context": "Numerically, this means that the Gauss-Newton direction can be orthogonal to the gradient at some distant point from \u03b8\u2217 (Mizutani and Dreyfus, 2010), causing optimization to converge to some non-stationary point.", "startOffset": 120, "endOffset": 148}, {"referenceID": 17, "context": "It is argued by Rattray et al. (1998); Inoue et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 8, "context": "(1998); Inoue et al. (2003) that natural gradient descent can address certain saddle point structures effectively.", "startOffset": 8, "endOffset": 28}, {"referenceID": 8, "context": "(1998); Inoue et al. (2003) that natural gradient descent can address certain saddle point structures effectively. Specifically, it can resolve those saddle points arising from having units behaving very similarly. Mizutani and Dreyfus (2010), however, argue that natural gradient descent also suffers with negative curvature.", "startOffset": 8, "endOffset": 243}, {"referenceID": 15, "context": "Similar to (Pascanu and Bengio, 2014), we use Lagrange multipliers to obtain the solution of this constrained optimization.", "startOffset": 11, "endOffset": 37}, {"referenceID": 26, "context": "Instead we use an approach similar to Krylov subspace descent (Vinyals and Povey, 2012).", "startOffset": 62, "endOffset": 87}, {"referenceID": 3, "context": "The hyperparameters of SGD were selected via random search (Bergstra and Bengio, 2012), and the damping coefficients for the damped Newton and saddle-free Newton2 methods were selected from a small set at each update.", "startOffset": 59, "endOffset": 86}, {"referenceID": 25, "context": "The neural network is a deep autoencoder trained on (full-scale) MNIST and considered a standard benchmark problem for assessing the performance of optimization algorithms on neural networks (Sutskever et al., 2013).", "startOffset": 191, "endOffset": 215}, {"referenceID": 10, "context": "69 achieved by the Hessian-Free method (Martens, 2010).", "startOffset": 39, "endOffset": 54}, {"referenceID": 16, "context": "Recurrent neural networks are widely known to be more difficult to train than feedforward neural networks (see, e.g., Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 106, "endOffset": 160}, {"referenceID": 24, "context": "The first direction is to explore methods beyond Kyrylov subspaces, such as one in (Sohl-Dickstein et al., 2014), that allow the saddlefree Newton method to scale to high dimensional problems, where we cannot easily compute the entire Hessian matrix.", "startOffset": 83, "endOffset": 112}, {"referenceID": 4, "context": "We would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 48, "endOffset": 93}, {"referenceID": 1, "context": "We would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 48, "endOffset": 93}], "year": 2014, "abstractText": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance. This work extends the results of Pascanu et al. (2014).", "creator": "LaTeX with hyperref package"}}}