{"id": "1406.2582", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Probabilistic ODE Solvers with Runge-Kutta Means", "abstract": "Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state-of-the-art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.", "histories": [["v1", "Tue, 10 Jun 2014 15:13:24 GMT  (628kb,D)", "https://arxiv.org/abs/1406.2582v1", "18 pages (9 page conference paper, plus supplements)"], ["v2", "Fri, 24 Oct 2014 11:45:49 GMT  (866kb,D)", "http://arxiv.org/abs/1406.2582v2", "18 pages (9 page conference paper, plus supplements); appears in Advances in Neural Information Processing Systems (NIPS), 2014"]], "COMMENTS": "18 pages (9 page conference paper, plus supplements)", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NA math.NA", "authors": ["michael schober", "david k duvenaud", "philipp hennig"], "accepted": true, "id": "1406.2582"}, "pdf": {"name": "1406.2582.pdf", "metadata": {"source": "CRF", "title": "Probabilistic ODE Solvers with Runge-Kutta Means", "authors": ["Michael Schober", "David Duvenaud", "Philipp Hennig"], "emails": ["mschober@tue.mpg.de", "dkd23@cam.ac.uk", "phennig@tue.mpg.de"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is not a purely theoretical question, but a purely theoretical one."}, {"heading": "2 Background", "text": "S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\".S. \".S.\".S. \".S.\".S. \".S.\" S. \".S.\".S. \".S.\".S. \".S.\".S. \".S.\".S. \".S.\".S. \".S\".S \".S\".S. \"S.\".S. \".S.\".S. \".S.\".S. \".S\".S. \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \"S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S"}, {"heading": "3 Results", "text": "The described GP-ODE estimation shares the algorithmic structure of the RK methods (i.e. both use weighted sums of the constructed estimates for extrapolation), but in the RK methods weights and valuation positions are found by careful analysis of the Taylor series of f, so that terms of lower order are cancelled out. In the GP-ODE solvers they arise, perhaps more naturally, but also with less structure, through the choice of ci and the kernel. In previous work [6, 7] both were selected ad hoc, without guarantee of convergence order. As shown in the additions, the decisions in these two works - quadratic-exponential nucleus with finite length scale, evaluations in the predictive mean - do not even result in the convergence of the first order of the Euler method. In the following, we present three specific regression models which are based on integrated Viennese covariance functions and specific evaluation points equating Gabilia probability. Each model of a Gauss-Gauss function is the inadequate function of a probability-Gauss distribution according to a boundary."}, {"heading": "3.1 Design choices and desiderata for a probabilistic ODE solver", "text": "Although we are not the first to attempt to construct an ODE solver that returns a probability distribution, open questions remain about what exactly the properties of such a probabilistic numerical method should be. Chkrebtii et al. [8] Previously, we have made the case that Gaussian measures are uniquely suited because solution spaces of ODEs refer to consistency and provide results to consistency. [8] In addition, we have added the desideratum for the posterior method, i.e. to reproduce the Runge-Kutta estimate, three additional problems become apparent: motivation of the evaluation points Both Skilling, Hennigs and Hauberg suggest the \"nodes x x\" (t0 + hci) in the current posterior means of belief. We will find that this is consistent with the order requirement of the RK methods of the first and second order."}, {"heading": "3.2 Gauss-Markov methods matching Euler\u2019s method", "text": "Theorem 1. The uniquely integrated Viennese process before p (x) = GP (x; 0, k1) with k1 (t, t) = t, t (k0 (u, v) dudv = \u03c32 (min 3 (t, t) 3 + t (min 2 (t, t) 2) (8) by selecting weighting nodes at the rear mean results in Euler's method. Proof. We show that the corresponding butcher's table from Table 1 applies. After \"observing\" the initial value, the second observation y1, constructed by selecting f at the rear mean at t0, isy1 = f (\u00b5 x0 (t0), t0) = f (k (t0, t0) k (t0) x0 (t0), t0 (t0), t0 (t4), (0)."}, {"heading": "3.3 Gauss-Markov methods matching all Runge-Kutta methods of second order", "text": "Extending to the second order is not as easy as integrating the Vienna Process a second time. Theorem 2. Consider the double integrated Viennese Process before p (x) = GP (x; 0, k2) with k2 (t, t) = t 2, t (u, v) dudv = 2 (min 5 (t, t) 20 + x 2 (t, t) min3 (t, t) \u2212 vancher 4 (t, t) 2))). (11) The selection of valuation nodes at the rear mean leads to the RK family of the second order methods in the limit of the second order. (The double integrated Viennese Process is a proper Gauss-Markov process for all finite values and the difference in the difference."}, {"heading": "3.4 A Gauss-Markov method matching Runge-Kutta methods of third order", "text": "The transition from the second to the third order, in addition to the boundary to an inappropriate previous one, also requires a departure from the policy of placing extrapolation nodes at the rear mean. Theorem 3. Consider the triple integrated Viennese process before p (x) = GP (x; 0, k3) withk3 (t, t \u2032) = t (t, t \u2032) + 2tt \u2032 (t, v \u00b2) \u2212 \u2212 \u2212 u \u2212 2 (t, t \u2032) 252 + x \u2212 t \u2032 min4 (t \u2032) 720 (5 max2 (t \u2032) + 2 min2 (t \u2032) + 3 min2 (t \u2032) + 3 min2 (t, t \u2212) \u2212 \u2212 h value of double appreciation at the rear mean and a third time at a specific element of the rear covariance functions' RKHS leads to the entire family of RK methods of third order, within the boundary of the procedure."}, {"heading": "3.5 Choosing the output scale", "text": "The above theorems have shown that the first three families of the Runge-Kutta methods can be constructed by repeatedly integrated Wiener Process Priors, which is a strong argument for the use of such Priors in probabilistic numerical methods. However, this assignment to a specific Runge-Kutta family is not yet uniquely identifiable in itself to be used: the rear middle ground of a Gaussian process, which comes from noise-free observations, is independent of the output scale (in our notation: 2) of the covariance function (this can also be seen by inspecting Eq. (3) Thus, the parameter \u03c32 can be chosen independently of the other parts of the algorithm without breaking the agreement with Runge-Kutta."}, {"heading": "4 Experiments", "text": "Since Runge-Kutta methods have been extensively studied for over a century, it is not necessary to re-evaluate their estimation performance. Instead, we focus on an open conceptual question for the further development of the probabilistic Runge-Kutta methods: If we accept a high convergence order as a prerequisite to choose a probabilistic model, how should it continue after the first steps? It seems unnatural to introduce new valuations of x (as opposed to x-ten) for n = 1,2,. Even with the exception of the Euler case, posterior covariance after such a form of evaluation is inevitable that its renewed use in the next interval will not give Runge-Kutta estimates. Three options suggest themselves: Na\u00efve Chaining One could simply repeat the algorithms several times as if the previous step had created a novel IVP. This boils down to the classic Runge-Kutta setup."}, {"heading": "5 Conclusions", "text": "We derived an interpretation of the Runge-Kutta methods with regard to the boundary of Gauss process regression with integrated Viennese covariance functions and a structured but not trivial extrapolation model. The result is a class of probabilistic numerical methods that return Gauss process-induced posterior distributions whose means can exactly match Runge-Kutta estimates. This class of methods has practical value, especially for machine learning, where previous work has shown that the probability distribution returned by GP-ODE solvers adds important functionalities compared to those of point counters. However, these results also raise pressing open questions about probabilistic ODE solvers, including how the GP interpretation of RK methods can be extended beyond the 3rd order and how ODE solvers should proceed after the first evaluation phase."}, {"heading": "Acknowledgments", "text": "The authors thank Simo S\u00e4rkk\u00e4 for a helpful discussion."}, {"heading": "A Multivariate extension", "text": "The GMRK model can be extended to the multivariate case analogous to the Runge-Kutta methods. All equations in the RK framework also work with vector-weighted function values, and all the derivatives presented in the paper and in this supplement are transferred without modification to the non-scalar case: Consider the dimension j (1,..., N). The projected results are the same as if j were an independent one-dimensional problem that can be modeled with a separate Gaussian process. For a common notation, vectorize the N dimensions with a Kronecker product: If k (t, t) is a one-dimensional covariance function, the function (t, t) = Dijk (ti, t \u2032 j), (16), where D is a N \u00d7 N positive semi-defined matrix, a covariance function over N dimensions, if t and t \u2032 are dimensions."}, {"heading": "B Covariance functions of integrated Wiener processes", "text": "It has been observed that integrated Wiener processes generate different orders with a higher number of integrations that lead to higher orders. (Here we present the derivation of the covariance functions of the applied Wiener process. (22) It is only defined for t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t,"}, {"heading": "C Posterior predictive GP distributions", "text": "In order to develop GMRK methods, it is necessary to calculate closed forms of the resulting mean and covariance functions after the evaluation. In cases where a derivative is omitted, the results were obtained using the MATLAB symbol mathematical toolbox. Code is available online. C.1 Posterior prediction functions and covariance functions of the once-integrated WPow are the formulas of the posterior mean and the covariance of the once-integrated WP after each step. (t) = k (t) (t) k (t0, t0) x0 = t30 / 3 + 1 (t30 / 3) t0 (t30 / 3) \u2212 \u2212 t0 (t0 / 3 x0) x0 (t0) x0 (t0) x0 (t0) x0 (t), t0 (t) k (t), t0 t0 t (t), t0 t0 (t), 0 tk k (k k k k) k (k) t (0), 0 tk (k), 0 tk (k), 0 tk (k (k), 0 tk (k), 0 tk (k), 0 tk (k (k), 0 tk (k), 0 tk (k (k), 0 tk (k), 0 tk (k (k), 0 tk (k (k k), 0 tk (k), 0 tk (k (k k k), 0 tk (k), 0 tk (k k (k), 0 tk (k), 0 tk (k k k k (k), 0 tk (k), 0 tk (k), 0 tk (k (k (k), 0 tk (k (k k), 0 tk (k (k), 0 tk (k), 0 tk (k (k), 0 tk (k (k), tk (k (k), tk (k (k), tk (k (k), 0 tk (k (k (k), tk (k (k), tk (k (k (k), tk (k (k), 0"}, {"heading": "D Square-exponential kernel cannot yield Euler\u2019s method", "text": "We show that the quadratexponential (SE, aka. RBF, Gaussian) kernel cannot provide Euler's method for finite length scalations.The SE kernel and its derivatives are k (t, t, t, t) = empirical methods (\u2212 t, t \u00b2 \u00b2) = empirical methods (\u2212 t \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2) = empirical methods (\u2212 t \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 2) (53) k \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (t \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (55) To show that this choice does not yield Euler's method, we proceed as in the case of GMRK methods. The predictive mean after observation of x0 and y1 is given as an empirical method."}], "references": [{"title": "Solving noisy linear operator equations by Gaussian processes: Application to ordinary and partial differential equations", "author": ["T. Graepel"], "venue": "In: International Conference on Machine Learning (ICML)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Accelerating Bayesian inference over nonlinear differential equations with Gaussian processes.", "author": ["B. Calderhead", "M. Girolami", "N. Lawrence"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "ODE parameter inference using adaptive gradient matching with Gaussian processes", "author": ["F. Dondelinger"], "venue": "Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Gaussian Processes for Bayesian Estimation in Ordinary Differential Equations", "author": ["Y. Wang", "D. Barber"], "venue": "In: International Conference on Machine Learning (ICML)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Bayesian solution of ordinary differential equations", "author": ["J. Skilling"], "venue": "In: Maximum Entropy and Bayesian Methods, Seattle ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "Probabilistic Solutions to Differential Equations and their Application to Riemannian Statistics", "author": ["P. Hennig", "S. Hauberg"], "venue": "In: Proc. of the 17th int. Conf. on Artificial Intelligence and Statistics (AISTATS). Vol. 33. JMLR, W&CP", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic shortest path tractography in DTI using Gaussian Process ODE solvers", "author": ["M. Schober"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Bayesian Uncertainty Quantification for Differential Equations", "author": ["O. Chkrebtii"], "venue": "arXiv prePrint", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "\u00dcber die numerische Aufl\u00f6sung von Differentialgleichungen", "author": ["C. Runge"], "venue": "In: Mathematische Annalen 46 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1895}, {"title": "Beitrag zur n\u00e4herungsweisen Integration totaler Differentialgleichungen", "author": ["W. Kutta"], "venue": "In: Zeitschrift f\u00fcr Mathematik und Physik 46 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1901}, {"title": "Solving Ordinary Differential Equations I \u2013 Nonstiff Problems", "author": ["E. Hairer", "S. N\u00f8rsett", "G. Wanner"], "venue": "Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "A family of embedded Runge-Kutta formulae", "author": ["J.R. Dormand", "P.J. Prince"], "venue": "In: Journal of computational and applied mathematics 6.1 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1980}, {"title": "Coefficients for the study of Runge-Kutta integration processes", "author": ["J. Butcher"], "venue": "In: Journal of the Australian Mathematical Society 3.02 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1963}, {"title": "Probl\u00e8mes diff\u00e9rentiels de conditions initiales (m\u00e9thodes num\u00e9riques)", "author": ["F. Ceschino", "J. Kuntzmann"], "venue": "Dunod Paris", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1963}, {"title": "Solutions of Differential Equations by Evaluations of Functions", "author": ["E.B. Shanks"], "venue": "In: Mathematics of Computation 20.93 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1966}, {"title": "Numerical solution of ordinary differential equations", "author": ["E. Hairer", "C. Lubich"], "venue": "In: The Princeton Companion to Applied Mathematics, ed. by N. Higham. PUP", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Extrapolation", "author": ["N. Wiener"], "venue": "interpolation, and smoothing of stationary time series with engineering applications\u201d. In: Bull. Amer. Math. Soc. 56 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1950}, {"title": "Bayesian filtering and smoothing", "author": ["S. S\u00e4rkk\u00e4"], "venue": "Cambridge University Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian Processes for Machine Learning", "author": ["C. Rasmussen", "C. Williams"], "venue": "MIT", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "An approach to time series smoothing and forecasting using the EM algorithm", "author": ["R. Shumway", "D. Stoffer"], "venue": "In: Journal of time series analysis 3.4 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1982}], "referenceMentions": [{"referenceID": 0, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 172, "endOffset": 184}, {"referenceID": 1, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 172, "endOffset": 184}, {"referenceID": 2, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 172, "endOffset": 184}, {"referenceID": 3, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 172, "endOffset": 184}, {"referenceID": 4, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 275, "endOffset": 287}, {"referenceID": 5, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 275, "endOffset": 287}, {"referenceID": 6, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 275, "endOffset": 287}, {"referenceID": 7, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 275, "endOffset": 287}, {"referenceID": 8, "context": "Runge-Kutta (RK) methods [9, 10] are standard tools for this purpose.", "startOffset": 25, "endOffset": 32}, {"referenceID": 9, "context": "Runge-Kutta (RK) methods [9, 10] are standard tools for this purpose.", "startOffset": 25, "endOffset": 32}, {"referenceID": 10, "context": "Over more than a century, these algorithms have matured into a very well-understood, efficient framework [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": "As recently pointed out by Hennig and Hauberg [6], since Runge-Kutta methods are linear extrapolation methods, their structure can be emulated by Gaussian process (GP) regression algorithms.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "Such an algorithm was envisioned by Skilling in 1991 [5], and the idea has recently attracted both theoretical [8] and practical [6, 7] interest.", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "Such an algorithm was envisioned by Skilling in 1991 [5], and the idea has recently attracted both theoretical [8] and practical [6, 7] interest.", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "Such an algorithm was envisioned by Skilling in 1991 [5], and the idea has recently attracted both theoretical [8] and practical [6, 7] interest.", "startOffset": 129, "endOffset": 135}, {"referenceID": 6, "context": "Such an algorithm was envisioned by Skilling in 1991 [5], and the idea has recently attracted both theoretical [8] and practical [6, 7] interest.", "startOffset": 129, "endOffset": 135}, {"referenceID": 6, "context": "Solution candidates can be drawn from the posterior and marginalized [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "This can allow probabilistic solvers to stop earlier, and to deal (approximately) with probabilistically uncertain inputs and problem definitions [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 10, "context": "Table 1: All consistent Runge-Kutta methods of order p \u2264 3 and number of stages s = p (see [11]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "Runge-Kutta methods1 [9, 10] are carefully designed linear extrapolation methods operating on small contiguous subintervals [tn, tn + h] \u2282 T of length h.", "startOffset": 21, "endOffset": 28}, {"referenceID": 9, "context": "Runge-Kutta methods1 [9, 10] are carefully designed linear extrapolation methods operating on small contiguous subintervals [tn, tn + h] \u2282 T of length h.", "startOffset": 21, "endOffset": 28}, {"referenceID": 11, "context": "by combining the same observations into two different RK predictions [12]).", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": ", bs], often presented compactly in a Butcher tableau [13]: c1 0 c2 w21 0 c3 w31 w32 0 \u22ee \u22ee \u22ee \u22f1 \u22f1 cs ws1 ws2 \u22ef ws,s\u22121 0 b1 b2 \u22ef bs\u22121 bs As Hennig and Hauberg [6] recently pointed out, the linear structure of the extrapolation steps in Runge-Kutta methods means that their algorithmic structure, the Butcher tableau, can be constructed naturally from a Gaussian process regression method over x(t), where the yi are treated as \u201cobservations\u201d of \u1e8b(t0 + hci) and the x\u0302i are subsequent posterior estimates (more below).", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": ", bs], often presented compactly in a Butcher tableau [13]: c1 0 c2 w21 0 c3 w31 w32 0 \u22ee \u22ee \u22ee \u22f1 \u22f1 cs ws1 ws2 \u22ef ws,s\u22121 0 b1 b2 \u22ef bs\u22121 bs As Hennig and Hauberg [6] recently pointed out, the linear structure of the extrapolation steps in Runge-Kutta methods means that their algorithmic structure, the Butcher tableau, can be constructed naturally from a Gaussian process regression method over x(t), where the yi are treated as \u201cobservations\u201d of \u1e8b(t0 + hci) and the x\u0302i are subsequent posterior estimates (more below).", "startOffset": 157, "endOffset": 160}, {"referenceID": 10, "context": "The method is then said to be of order p [11].", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "This is only possible for p < 5 [14, 15].", "startOffset": 32, "endOffset": 40}, {"referenceID": 14, "context": "This is only possible for p < 5 [14, 15].", "startOffset": 32, "endOffset": 40}, {"referenceID": 10, "context": "Many generalizations can be found in [11].", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "A recursive algorithm analogous to RK methods can be constructed [5, 6] by setting the prior mean to the constant \u03bc(t) = x0, then recursively estimating x\u0302i in some form from the current posterior over x.", "startOffset": 65, "endOffset": 71}, {"referenceID": 5, "context": "A recursive algorithm analogous to RK methods can be constructed [5, 6] by setting the prior mean to the constant \u03bc(t) = x0, then recursively estimating x\u0302i in some form from the current posterior over x.", "startOffset": 65, "endOffset": 71}, {"referenceID": 5, "context": "The choice in [6] is to set x\u0302i = \u03bc(t0 + hci).", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "In previous work [6, 7], both were chosen ad hoc, with no guarantee of convergence order.", "startOffset": 17, "endOffset": 23}, {"referenceID": 6, "context": "In previous work [6, 7], both were chosen ad hoc, with no guarantee of convergence order.", "startOffset": 17, "endOffset": 23}, {"referenceID": 7, "context": "[8] previously made the case that Gaussian measures are uniquely suited because solution spaces of ODEs are Banach spaces, and provided results on consistency.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Motivation of evaluation points Both Skilling [5] and Hennig and Hauberg [6] propose to put the \u201cnodes\u201d x\u0302(t0 + hci) at the current posterior mean of the belief.", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "Motivation of evaluation points Both Skilling [5] and Hennig and Hauberg [6] propose to put the \u201cnodes\u201d x\u0302(t0 + hci) at the current posterior mean of the belief.", "startOffset": 73, "endOffset": 76}, {"referenceID": 15, "context": "From the second interval onward, the RK step solves an estimated IVP, and begins to accumulate a global estimation error not bounded by the convergence order (an effect termed \u201cLady Windermere\u2019s fan\u201d by Wanner [16]).", "startOffset": 210, "endOffset": 214}, {"referenceID": 16, "context": "All covariance functions in question are integrals over the kernel k(t\u0303, t\u0303) = \u03c3 min(t\u0303 \u2212 \u03c4, t\u0303 \u2212 \u03c4) (parameterized by scale \u03c3 > 0 and off-set \u03c4 \u2208 R; valid on the domain t\u0303, t\u0303 > \u03c4 ), the covariance of the Wiener process [17].", "startOffset": 221, "endOffset": 225}, {"referenceID": 17, "context": "Such integrated Wiener processes are Gauss-Markov processes, of increasing order, so inference in these methods can be performed by filtering, at linear cost [18].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "Since Runge-Kutta methods have been extensively studied for over a century [11], it is not necessary to evaluate their estimation performance again.", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": "Figure 3: Comparison of a 2nd order GMRK method and the method from [6].", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "2 (right column) to the ad-hoc choice of a square-exponential (SE) kernel model, which was used by Hennig and Hauberg [6] (Fig.", "startOffset": 118, "endOffset": 121}], "year": 2014, "abstractText": "Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state of the art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.", "creator": "LaTeX with hyperref package"}}}