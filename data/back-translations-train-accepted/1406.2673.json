{"id": "1406.2673", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Mondrian Forests: Efficient Online Random Forests", "abstract": "Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically re-trained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.", "histories": [["v1", "Tue, 10 Jun 2014 19:34:51 GMT  (435kb,D)", "http://arxiv.org/abs/1406.2673v1", null], ["v2", "Mon, 16 Feb 2015 14:57:52 GMT  (594kb,D)", "http://arxiv.org/abs/1406.2673v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["balaji lakshminarayanan", "daniel m roy", "yee whye teh"], "accepted": true, "id": "1406.2673"}, "pdf": {"name": "1406.2673.pdf", "metadata": {"source": "CRF", "title": "Mondrian Forests: Efficient Online Random Forests", "authors": ["Balaji Lakshminarayanan", "Daniel M. Roy"], "emails": ["balaji@gatsby.ucl.ac.uk."], "sections": [{"heading": "1 Introduction", "text": "Although Random Forests were introduced over a decade ago, they remain one of the most popular tools for machine learning in the real world due to their accuracy, scalability and robustness in real-world classification tasks. [3] (We refer to [5] for an excellent up-to-date overview of Random Forests.) In this paper, we present a novel random forest - Mondrian forests (MF), since the underlying tree structure of each classifier within the ensemble is a so-called Mondrian process. Using the properties of mondrian processes, we present an efficient online algorithm that matches its batch counterpart with each iteration. Mondrian's online forests are not only faster and more precise than the recent proposals for online random forest methods, but they are almost identical to the accuracy of modern batch random forest methods trained on the same dataset. The paper is organized as follows section 2, we describe the work on our approach of 7, on the high level, and @ in the structure of 3."}, {"heading": "2 Approach", "text": "Given N-marked examples (x1, y1),., (xN, yN), \"RD \u00b7 Y\" as training data, our task is to predict labels y-Y for blank test points x-RD. We will focus on multi-class classification, with Y: = {1,., K}, but it is possible to extend the methodology to other monitored learning tasks such as regression. Let X1: n: (x1,., xn), Y1: n: (y1,., yn), and D1: n: (X1: n, Y1: n). A Mondrian forest classifier is constructed in a similar way to a random forest: given the training data D1: N, we stamp an independent collection T1,.., TM of the so-called Mondrian trees, which we will describe in the next section."}, {"heading": "3 Mondrian trees", "text": "For our purposes, a decision tree on RD is a hierarchical, binary division of RD = = j = j = j = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3.1 Mondrian process distribution over decision trees", "text": "Mondrian processes, introduced by Roy and Teh [14], are families {Mt: t = > u = > distribution rate (0, \u221e) of random, hierarchical binary partitions of RD such that Mt is a refinement of Ms when t > s.1 Mondrian processes are natural candidates for the partition structure of random decision trees, but Mondrian processes on RD are, in general, infinite structures that we cannot represent all at once. Since we only care about the partition on a finite amount of observed data, we introduce Mondrian trees that are limitations of mondrian processes to a finite number of points. A Mondrian tree T can be represented by a tuple (T) in which (T,) is a decision tree in which we have a decision tree = {\u03c4j} j j j Jump T, and the time of partition 0 denotes the time of the split associated with the node j."}, {"heading": "4 Label distribution: model, hierarchical prior, and pre-", "text": "In this section, we focus on the distribution of labels at a node. We achieve this smoothing through a hierarchical approach within each tree. For each tree T, we introduce latent parameters G, which specify a distribution across each node, which is denoted by pT (y | x, G). Next, we define a previous hierarchical pT (G) that encourages distribution at a node to be similar. Finally, we discuss how the probability and hierarchical approach are combined to obtain the distribution of labels pT (y | x, G)."}, {"heading": "5 Online training and prediction", "text": "In this section we describe the distribution MTx (\u03bb, T, Dn + 1), which incrementally splits the data point Dn + 1 into a tree. These updates are based on the conditional Mondrian algorithm [14], which specializes in a finite series of points. In general, we have two of the following three operations2Taking the discount parameter to zero leads to a Dirichlet process. Hierarchies of the Mondrian algorithms allow more comprehensible approximations than hierarchies of the Dirichlet processes, hence our choice.At iteration 1, we have two training data points, labeled as a, b. Figures 2 (a) and 2 (g) show the partition and tree structure of the Mondrian tree. Note that there is even a partition x2 > 0.23 at time t = 2.42, we commit this split Bxj (shown by the gray rectangle).At iteration 2, a new data point c is added."}, {"heading": "6 Related work", "text": "The mentioned the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green"}, {"heading": "7 Empirical evaluation", "text": "The reason why it has come to this is that it has come to this point in recent years, that it has come to such an increase in the United States. (...) \"It is not that there has been such an increase.\" (...) \"It is not that there has been such an increase.\" (...) \"\" It is not that there has been such an increase. \"(...)\" \"\" It is not that there has been such an increase. \"(...)\" (...) \"(\") \"(\") ((((()) \"(() (()\" () () (() () (() () () () () () () () () () () () () () () () () () () () () ()) (()) (()) (() () () () () () () () () () () () (() () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() () () (() () (() () () (() () ((() () () (() () ((() () (() () () (((() () (() () ((() (() ((() () ((() ((() (() () ((() (() (() (() ((() (() (() (() (() () (((() () (((() () ((() (() (() ((() ((() ((() () () ((() ((((("}, {"heading": "8 Discussion", "text": "We have introduced Mondrian Forests, a new random forest variant that can be trained incrementally in an efficient manner. MF surpasses existing online random forests in terms of training time as well as number of training instances required to a4https: / / archive.ics.uci.edu / ml / machine-learning-databases / mushroom / agaricus-lepiota.names 5https: / / www.sgi.com / tech / mlc / db / DNA.namesparticular test accuracy. Remarkably, MF achieves a competitive test accuracy to stack random forests trained on the same fraction of data. MF is not able to handle many irrelevant features (since splits are selected independently of the labels) - one way to use labels to pass splits is the recently proposed Monte Carlo sequential algorithm for decision trees [which could be costly for future computational work]."}, {"heading": "Acknowledgments", "text": "We would like to thank Charles Blundell, Gintare Dziugaite, Creighton Heaukulani, Jose \"Miguel Herna\" ndez-Lobato, Maria Lomeli, Alex Smola, Heiko Strathmann and Srini Turaga for helpful discussions and feedback on designs. BL would like to thank the Gatsby Charitable Foundation for their generous support. This research was carried out in part during a research fellowship at Emmanuel College in Cambridge, which was also funded by a Newton International Fellowship through the Royal Society. YWT's research that led to these results was funded by the European Research Council under the Seventh Framework Programme (FP7 / 2007-2013) of the European Union ERC Funding Agreement No. 617411."}, {"heading": "A Posterior inference and prediction using the HNSP", "text": "In fact, it is so that in most countries of the world, in which most people are able to put themselves in the world, in which they are able to put themselves in the world, to put themselves in the world, to put themselves in the world, in which they put themselves in the world, to put themselves in the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to think into the world, to think into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to look into the world, to think into the world, to look into the world, to look into the world, to think into the world, to think into the world, to think into the world, to think into the world, to think into the world, to think into the world, to look into the world, to look into the world, to think into the world, to think into the world, to think into the world, to think into the world, to think into the world, to think into the world, to think into the world, to think into the world, to think into the world, to think into the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in"}, {"heading": "9: tabj\u2032y = min(cj\u2032y, 1) . IKN approximation", "text": "10: If j \u2032 = then 11: return 12: else 13: j \u2032 \u2190 parent (j \u2032) Predictive posterior computation Considering the counts cj, k and the table assignments tabj, k, the predictive probability (i.e. the trailing mean) at the node j can be recursively calculated as follows: Gjk = {cj, k \u2212 djtabj, kcj, \u00b7 + djtabj, \u00b7 cj, \u00b7 Gparent (j), k cj, \u00b7 > 0, Gparent (j), k cj, \u00b7 = 0, (7) where cj, \u00b7 = \u2211 k cj, k, tabj, \u00b7 = \u2211 k tabj, k, and dj: = exp (\u2212 \u03b3 (\u03c4j \u2212 \u03c4parent (j)))) is the \"discount\" for the node j, defined in Section 4. Informally, the discount interpolates between the counts c and the predictions. If the discount dj 1 at least, then the Gj can be calculated as the single parent (j)."}, {"heading": "B Prediction using Mondrian tree", "text": "We are interested in the predictive probability of y on x, which is denoted by pT (y-x, D1: N). As in typical decision trees, the process contains a tree sequence running from top to bottom, starting from the root. If x is already \"contained\" in the tree T, i.e. if x-x indicates the posterior probability of y (k) to node j (k), the process is running from top to bottom, starting from the posterior predictive distribution given by (7). Gjk denotes the posterior probability of y (k) to node j 3: Initialize of the ordered proposition J = {} 4: while J do not empty 5: if the first element of J 6: if j = then 7: Gparent () = H8: Set d = exp (\u2212)."}, {"heading": "C Computational complexity", "text": "The complexity of a forest is simply M times the complexity of a single tree, but this calculation can be trivially paralleled because there is no interaction between the trees. Suppose that the N data points are processed one by one. Suppose that the data points form a balanced binary tree after each update. At most, the total cost of processing N data points is O (log n) (logN!), which tends to O (N logN logN) for large N (using the Stirling approximation for the factor function). For offline RF and ERT, the expected complexity for n data points is O (n log n). The complexity of the right-to-large N tends to N (N logN logN) (using the Stirling approximation for the factor function), with the expected complexity for n data points being O (n log)."}, {"heading": "D Pseudocode for paused Mondrians", "text": "Algorithm m m 9 SampleMondrianBlock (j), DN (j), \u03bb) version, which depends on labels (max.) (max.): Addition j to T 2: D, Set (j), Set (6), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Set (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max.), Max (max), Max (max.), Max (max.), Max (max.), Max (max."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>Ensembles of randomized decision trees, usually referred to as random forests, are<lb>widely used for classification and regression tasks in machine learning and statistics.<lb>Random forests achieve competitive predictive performance and are computationally<lb>efficient to train and test, making them excellent candidates for real-world prediction<lb>tasks. The most popular random forest variants (such as Breiman\u2019s random forest and<lb>extremely randomized trees) operate on batches of training data. Online methods are<lb>now in greater demand. Existing online random forests, however, require more training<lb>data than their batch counterpart to achieve comparable predictive performance. In<lb>this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of<lb>random decision trees we call Mondrian forests. Mondrian forests can be grown in an<lb>incremental/online fashion and remarkably, the distribution of online Mondrian forests<lb>is the same as that of batch Mondrian forests. Mondrian forests achieve competitive<lb>predictive performance comparable with existing online random forests and periodically<lb>re-trained batch random forests, while being more than an order of magnitude faster,<lb>thus representing a better computation vs accuracy tradeoff.", "creator": "LaTeX with hyperref package"}}}