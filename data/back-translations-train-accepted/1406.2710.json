{"id": "1406.2710", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations", "abstract": "In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.", "histories": [["v1", "Tue, 10 Jun 2014 20:29:10 GMT  (350kb,D)", "http://arxiv.org/abs/1406.2710v1", "11 pages. An earlier version was accepted to the ICML-2014 Workshop on Knowledge-Powered Deep Learning for Text Mining"]], "COMMENTS": "11 pages. An earlier version was accepted to the ICML-2014 Workshop on Knowledge-Powered Deep Learning for Text Mining", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["ryan kiros", "richard s zemel", "ruslan salakhutdinov"], "accepted": true, "id": "1406.2710"}, "pdf": {"name": "1406.2710.pdf", "metadata": {"source": "CRF", "title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations", "authors": ["Ryan Kiros", "Richard S. Zemel", "Ruslan Salakhutdinov"], "emails": ["rsalakhu}@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to move to another world, in which they are able, in which they are able to move, in which they are able to move, in which they are able, and in which they are able to move, in which they are able, in which they are able, in which they are able to move, in which they are able, in which they are able to move, in which they are able, in which they are able to move."}, {"heading": "2 Methods", "text": "In this section, we describe the proposed models. First, we review the logbilinear neural language model of [14], as it forms the basis for much of our work. Next, we describe a tensor for embedding words and show how it can be taken into account and introduced into a multiplicative neural language model. Finally, we describe in detail how our attribute vectors are learned."}, {"heading": "2.1 Log-bilinear neural language models", "text": "The log-bilinear language model (LBL) [14] is a deterministic model that can be considered a forward-facing neural network with a single linear hidden plane. Each word w in the vocabulary is represented as a K-dimensional real value vector rw-RK. Let R denote the V \u00b7 K matrix of word representation vectors, where V is the word size. Let (w1,.. wn \u2212 1) be a tuple of n \u2212 1 words, where n \u2212 1 is the context size. The LBL model makes a linear prediction of the next word representation asr = n \u2212 1 \u2211 i = 1 C (i) rwi, (1) where C (i), i = 1,.. n \u2212 1 K are context parameters matrices."}, {"heading": "2.2 A word embedding tensor", "text": "Traditionally, word representation matrices are represented as matrix R-RV-K, e.g. in the case of the logbilinear model. In this work, we represent words instead as tensortT-RV-K-D, where D corresponds to the number of tensor sections. With an attribute vector x-RD, we can calculate associated word representations as T x-RV-I = 1 xiT (i), i.e. word representations with respect to x are calculated as a linear combination of discs weighted by each component xi of x. It is often unnecessary to use a completely factless tensor. Following [15, 16], we represent T in terms of three matrices Wfk-RF-K, Wfd-RF-D, and Wfv-RF-V-V-V."}, {"heading": "2.3 Multiplicative neural language models", "text": "Let us now show how to embed our word representation tensor T in the log-bilinear neural language model. Let's have E = (Wfk) > Wfv denote a \"folded\" K \u00b7 V matrix of word embeddings. In the context w1,..., wn \u2212 1, the predicted next word representation r: r = n \u2212 1 \u2211 i = 1 C (i) E (:, wi), (4) where E (:, wi) denotes the E column for the word representation of wi and C (i), i = 1,..., n \u2212 1 are K context matrices. In a predicted next word representation r \u0442, the factor aref = (Wfkr) \u2022 (Wfdx) gives, (5) where \u2022 is a component-wise product, the conditional probability P (wn = i | w1, x) of the given w1,."}, {"heading": "2.4 Unshared vocabularies across attributes", "text": "Our formulation for T assumes that word representations are used in common across all attributes. In some cases, words can only be specific to certain attributes and not to others. An example of this is translingual modeling, where it is necessary to have language-specific vocabulary. Let's consider as a running example the case where each attribute corresponds to a linguistic representation vector. Let x denote the attribute vector for language \"and x\" for language \"(e.g. English and French), then calculate language-specific word representations T\" by splitting our decomposition into language-dependent and independent components (see Figure 1c): T '= (Wfv') > \u00b7 diag (Wfdx) \u00b7 Wfk (6), where (Wfv ') > is a language-specific V '\u00d7 F matrix."}, {"heading": "2.5 Learning attribute representations", "text": "Let us remember that in the training of neural language models, the word representations of w1,.., wn \u2212 1 are updated by backward propagation by the word embedding matrix. We can imagine this as a linear level in which the input to this level is a uniform vector with the i-th position active for word wi. Then, multiplying this vector by the embedding matrix results in the word vector for wi. Thus, the columns of the word representation matrix resulting from words of w1,.., wn \u2212 1 have unequal gradients in terms of loss. This allows us to consistently modify the word representations throughout the training. We construct attribute representations in a similar manner. Suppose that L is an attribute search table, where x = f (:, x) and f is an optional non-linearity."}, {"heading": "3 Experiments", "text": "In this section, we describe our experimental evaluation and results. During this section, we refer to our model as Attributes Tensor Decomposition (ATD). All models are trained using stochastic gradient lineage with exponential learning rate decay and linear (per epoch) increase of the moment. We first demonstrate initial qualitative results to get a sense of the tasks our model can perform using the small Gutenberg Corpus project, which consists of 18 books, some of which have the same author. We first trained a multiplicative neural language model with a context size of 5, in which each attribute is presented as a book. This results in 18 learned attribute vectors, one for each book. After the training, we can condition a book vector and generate samples from the model. Table 2 illustrates some of the samples generated. Our model learns to capture the \"style\" associated with different books."}, {"heading": "3.1 Sentiment classification", "text": "Our first quantitative experiments are conducted on the Sentiment Treebank of [3]. A common challenge for sentiment classification tasks is that the global feeling of a sentence does not correspond to local feelings expressed in sub-phrases of the sentence. [3] To address this problem, we have been able to achieve significant performance gains with recursive networks via word baselines. We follow the same experimental procedure proposed by [3] for evaluation on two tasks: fine-grained classification of categories [very negative, neutral, positive, very positive] and binary classification of words baselines. We follow the same experimental procedure reported by [3] evaluation."}, {"heading": "3.2 Cross-lingual document classification", "text": "Following the experimental approach of the [19], for which several existing baselines are available to compare our results, the experiment proceeds as follows: We first use the Europarl method [23] to produce word representations in different languages. Let's be a sentence with words in the language and let x be the corresponding language vector. Letv (S) = [S) = the sum of linguistic word representations for each language we use. (Wfdx) \u00b7 Wfk (7) denotes the sentence representation of S, defined as the sum of linguistically conditioned word representations for each language. (S) Equally, we define a sentence representation for the translation of S. \"(S) We then optimize the following ranking goal: minimize the number of S-words. (S) We have the following ranking goal."}, {"heading": "3.3 Blog authorship attribution", "text": "For us, it is important that we are able to assert ourselves, that we are able to hide ourselves, and that we are able to hide ourselves, \"he said."}, {"heading": "3.4 Conditional word similarity", "text": "One of the key characteristics of our tensor formulation is the notion of conditional word similarity, namely how neighbors of word representations change depending on the conditioned attributes. To investigate the effects of this development, we performed two qualitative comparisons: one with blog attribute vectors and the other with language vectors. These results are illustrated in Table 5. For the first comparison on the left, we selected two attributes from the blog corpus and a query word. We identified each of these attribute pairs as A and B. Next, we computed a ranking of the closest neighbors (due to cosmic similarity) of words conditioned to each attribute, and identified the top 15 words in each. From these 15 words, we show the top three words that are common in both rankings, as well as three words that are unique for a particular attribute. Our results show that the model can capture distinctive notions of word similarities depending on which attributes are conditioned."}, {"heading": "4 Conclusion", "text": "There are several future directions from which this work can be expanded, and one area of interest is to learn presentations of authors from papers they review themselves to improve automation of reviewer-essay matching. [25] Since authors contribute to different research topics, it might be more useful to consider instead a mix of attribute vectors that can enable different representations of the same author across research areas. Another interesting application is the learning of graphs representations. [26] Recently, an approach to embedding nodes in social networks has been proposed, and the introduction of network indicator vectors could allow us to learn potentially full graph representations, which would enable a new way of comparing structural similarities between different types of social networks. Finally, it would be interesting to train a multiplicate neural language model in dozens of languages simultaneously to better determine what types of characteristics and language similarities can be represented."}], "references": [{"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner"], "venue": "In ACL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Zero-shot learning for semantic utterance classification", "author": ["Yann N Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeffrey Dean"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Factored conditional restricted boltzmann machines for modeling motion style", "author": ["Graham W Taylor", "Geoffrey E Hinton"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Richard S Zemel", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Unsupervised learning of image transformations", "author": ["Roland Memisevic", "Geoffrey Hinton"], "venue": "In CVPR, pages", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["Alex Krizhevsky", "Geoffrey E Hinton"], "venue": "In AISTATS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In EMNLP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai"], "venue": "In COLING,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh M Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha"], "venue": "arXiv preprint arXiv:1402.1454,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "ICLR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee"], "venue": "In ACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Effects of age and gender on blogging", "author": ["Jonathan Schler", "Moshe Koppel", "Shlomo Argamon", "James W Pennebaker"], "venue": "In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "A framework for optimizing paper matching", "author": ["Laurent Charlin", "Richard S Zemel", "Craig Boutilier"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Distributed word representations have enjoyed success in several NLP tasks [1, 2].", "startOffset": 75, "endOffset": 81}, {"referenceID": 1, "context": "Distributed word representations have enjoyed success in several NLP tasks [1, 2].", "startOffset": 75, "endOffset": 81}, {"referenceID": 2, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 156, "endOffset": 168}, {"referenceID": 3, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 156, "endOffset": 168}, {"referenceID": 4, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 156, "endOffset": 168}, {"referenceID": 5, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 156, "endOffset": 168}, {"referenceID": 6, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 197, "endOffset": 203}, {"referenceID": 7, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 197, "endOffset": 203}, {"referenceID": 8, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 242, "endOffset": 249}, {"referenceID": 9, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 242, "endOffset": 249}, {"referenceID": 5, "context": "This allows us to learn sentence and language vectors, similar to the proposed model of [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "Another way of thinking of our model would be the language analogue of [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "When our factorization is embedded into a neural language model, it allows us to generate text conditioned on different attributes in the same manner as [11] could generate motions from different styles.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "[12] introduced a multiplicative model where images are used for gating word representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Our framework can be seen as a generalization of [12] and in the context of their work an attribute would correspond to a fixed representation of an image.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "[13] introduced a multiplicative recurrent neural network for generating text at the character level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We first review the log-bilinear neural language model of [14] as it forms the basis for much of our work.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "The log-bilinear language model (LBL) [14] is a deterministic model that may be viewed as a feedforward neural network with a single linear hidden layer.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Following [15, 16], we re-represent T in terms of three matrices W \u2208 RF\u00d7K , W \u2208 RF\u00d7D and W \u2208 RF\u00d7V , such that T x = (Wfv)> \u00b7 diag(Wx) \u00b7W (3)", "startOffset": 10, "endOffset": 18}, {"referenceID": 15, "context": "Following [15, 16], we re-represent T in terms of three matrices W \u2208 RF\u00d7K , W \u2208 RF\u00d7D and W \u2208 RF\u00d7V , such that T x = (Wfv)> \u00b7 diag(Wx) \u00b7W (3)", "startOffset": 10, "endOffset": 18}, {"referenceID": 5, "context": "To accommodate for this, we use an inference step similar to that proposed by [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 16, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 133, "endOffset": 137}, {"referenceID": 2, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 213, "endOffset": 216}, {"referenceID": 5, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 243, "endOffset": 246}, {"referenceID": 18, "context": "Methods include statistical machine translation (SMT), IMatrix [19], Bag-of-words autoencoders (BAE-*) [20] and BiCVM, BiCVM+ [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Methods include statistical machine translation (SMT), IMatrix [19], Bag-of-words autoencoders (BAE-*) [20] and BiCVM, BiCVM+ [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Methods include statistical machine translation (SMT), IMatrix [19], Bag-of-words autoencoders (BAE-*) [20] and BiCVM, BiCVM+ [21].", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "Our first quantitative experiments are performed on the sentiment treebank of [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "To address this issue, [3] collected annotations from the movie reviews corpus of [22] of all subphrases extracted from a sentence parser.", "startOffset": 23, "endOffset": 26}, {"referenceID": 21, "context": "To address this issue, [3] collected annotations from the movie reviews corpus of [22] of all subphrases extracted from a sentence parser.", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "By incorporating local sentiment into their recursive architectures, [3] was able to obtain significant performance gains with recursive networks over bag of words baselines.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "We follow the same experimental procedure proposed by [3] for which evaluation is reported on two tasks: fine-grained classification of categories {very negative, negative, neutral, positive, very positive } and binary classification {positive, negative }.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "Here, each attribute is represented as a sentence vector, as in [6].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "In order to compute subphrases for unseen sentences, we apply an inference procedure similar to [6], where the weights of the network are frozen and gradient descent is used to infer representations for each unseen vector.", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "size of 8, 100 dimensional word vectors initialized from [2] and 100 dimensional sentence vectors initialized by averaging vectors of words from the corresponding sentence.", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "Our method is outperformed by the two recently proposed approaches of [5] (a convolutional network trained on sentences) and Paragraph Vector [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "Our method is outperformed by the two recently proposed approaches of [5] (a convolutional network trained on sentences) and Paragraph Vector [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "We suspect that a much more extensive hyperparameter search over context sizes, word and sentence embedding sizes as well as inference initialization schemes would likely close the gap between our approach and [6].", "startOffset": 210, "endOffset": 213}, {"referenceID": 18, "context": "We follow the experimental procedure of [19], for which several existing baselines are available to compare our results.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "We first use the Europarl corpus [23] for inducing word representations across languages.", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "This type of cross-language ranking loss was first used by [21] but without the norm constraint which we found significantly improved the stability of training.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "Following [19], we only consider documents which have been assigned to one of the top 4 categories in the label hierarchy.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "Following [19] we use an averaged perceptron classifier.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "We are competitive with the current state-of-the-art approaches, being outperformed only by BiCVM+ [21] and BAE-corr [20] on EN \u2192 DE.", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "We are competitive with the current state-of-the-art approaches, being outperformed only by BiCVM+ [21] and BAE-corr [20] on EN \u2192 DE.", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "For this task, we compare against a separation baseline, which is the same as our model but with no parameter sharing across languages (and thus resembles [21]).", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "For our final task, we use the Blog corpus of [24] which contains 681,288 blog posts from 19,320 authors.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "We used 100 dimensional word vectors initialized from [2], 100 dimensional attribute vectors with random initialization and a context size of 5.", "startOffset": 54, "endOffset": 57}, {"referenceID": 13, "context": "As an additional baseline we include a log-bilinear language model [14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "One application area of interest is in learning representations of authors from papers they choose to review as a way of improving automating reviewer-paper matching [25].", "startOffset": 166, "endOffset": 170}], "year": 2014, "abstractText": "In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.", "creator": "LaTeX with hyperref package"}}}