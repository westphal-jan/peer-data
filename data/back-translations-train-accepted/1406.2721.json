{"id": "1406.2721", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Learning Latent Variable Gaussian Graphical Models", "abstract": "Gaussian graphical models (GGM) have been widely used in many high-dimensional applications ranging from biological and financial data to recommender systems. Sparsity in GGM plays a central role both statistically and computationally. Unfortunately, real-world data often does not fit well to sparse graphical models. In this paper, we focus on a family of latent variable Gaussian graphical models (LVGGM), where the model is conditionally sparse given latent variables, but marginally non-sparse. In LVGGM, the inverse covariance matrix has a low-rank plus sparse structure, and can be learned in a regularized maximum likelihood framework. We derive novel parameter estimation error bounds for LVGGM under mild conditions in the high-dimensional setting. These results complement the existing theory on the structural learning, and open up new possibilities of using LVGGM for statistical inference.", "histories": [["v1", "Tue, 10 Jun 2014 21:03:22 GMT  (436kb,D)", "http://arxiv.org/abs/1406.2721v1", "To appear in The 31st International Conference on Machine Learning (ICML 2014)"]], "COMMENTS": "To appear in The 31st International Conference on Machine Learning (ICML 2014)", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST stat.TH", "authors": ["zhaoshi meng", "brian eriksson", "alfred o hero iii"], "accepted": true, "id": "1406.2721"}, "pdf": {"name": "1406.2721.pdf", "metadata": {"source": "CRF", "title": "Learning Latent Variable Gaussian Graphical Models", "authors": ["Zhaoshi Meng", "Brian Eriksson", "Alfred O. Hero III"], "emails": ["mengzs@umich.edu,", "brian.eriksson@technicolor.com,", "hero@eecs.umich.edu"], "sections": [{"heading": "1 Introduction", "text": "The ability to extract and estimate distributional characteristics from observations is critical for many statistical sequencing tasks in complex real-world systems, such as prediction and detection. Unfortunately, the high-dimensional system in which such model estimates often lead to poorly defined problems, especially when the number of observations n (or sample size) is comparable in many real-world applications ranging from recommendation systems, gene microarray data, and financial data, to name a few. In order to perform accurate model parameter estimates and subsequent statistical inferences, we are often forced to regulate a low dimensional structure (Negahban et al, 2012).For Gaussian distributed data, the central problem is often to estimate inverse covariance (alternatively known as precision, concentration, or information)."}, {"heading": "2 Background and Related Work", "text": "The problem of learning GGM with sparse inverse covariance matrices with \"1-regulated maximum probability estimation, often referred to as the graphic lasso (Glasso) problem, has been investigated in Friedman et al. (2008); Ravikumar et al. (2011); Choi et al. (2010), the authors of Ravikumar et al. (2011), propose model selection consistency (i.e.,\" thrift \") under certain incoherence conditions. (In addition, Choi et al. (2010), they propose a multi-resolution extension of GGM with sparse interval correlations, while in Choi et al, the authors consider latent tree-structured graphical models. Both models lead to computationally efficient inferences and learning algorithms that limit the latent structure to trees."}, {"heading": "3 Problem Setup", "text": "In this section we review Gaussian graphical models and formulate the problem of latently variable Gaussian graphical model estimates by means of a regulated maximum probability optimization."}, {"heading": "3.1 Gaussian Graphical Models", "text": "Consider a p-dimensional random vector x associated with an undirected graph G = (VG, EG), where VG is a set of nodes corresponding to elements of x and EG, a set of edges connecting nodes (including self-edges for each node). Then, x follows a graphical model distribution if it meets the Markov property with respect to G: for each pair of non-adjacent nodes in G, the corresponding pair of variables in x is conditionally independent of the remaining variables, i.e., xi-xj | x-i, j, for all (i, j) / E-G. If x follows a multivariate Gaussian distribution, the corresponding graphic model is called a Gaussian model (GGM).We assume without loss of generality that x has an average value of zero. The Markov property in GGM is manifested in the economy pattern of inverse codiance matrix J: Ji-j = all problems."}, {"heading": "3.2 Latent Variable Gaussian Graphical Models", "text": "Unfortunately, the presence of global factors destroying thrift often does not exactly conform to a sparse GGM in realworld observations (Choi et al., 2010, 2011). By introducing latent variables (referred to as an r-dimensional random vector xL), we can generalize the GGM. We assume a common distribution of (p + r) -dimensional random vectors x = (xO, xL) followed by a sparse precision matrix with knowledge of latent variables, xL.Defining the p observed variables as xO, we assume the common distribution of (p + r) -dimensional random vectors x = (xO, xL)."}, {"heading": "3.3 Effective Rank of Covariance Matrix", "text": "We introduce the effective rank of a matrix, which will be useful for deriving high-dimensional error boundaries. The effective rank of a matrix \u03a3 is defined as follows (Vershynin, 2010): reff (\u03a3): = tr (\u03a3) / improved \u03a3 2. (3) The effective rank can be considered a measure of the concentration level of the spectrum of \u03a3. As we will show in Section 5.1, the effective rank of the covariance matrix, which corresponds to an LVGGM, is in many situations much lower than p. Under this condition, our theoretical results subsequently provide a narrow Frobenius standard error boundary, which is significantly improved compared to the error boundary, which is derived without the effective ranking."}, {"heading": "3.4 Regularized ML Estimation of LVGGM", "text": "n samples x1, x2,.., xn are available from an LVGGM model xO, concatenated in a data matrix X-Rp \u00b7 n. The negative log probability function is L (\u044b; X) = < \u03a3, \u0432 > \u2212 log (\u044b), (4) where the regulation parameter \u03bb > 0 and the regulation function R (\u0432) is the covariance matrix of the sample. Similar to Chandrasekaran et al. (2012), we consider the following regularized ML estimation problem: min S, L L (S + L; X) and the regulation function R to enforce the sparse plus low structure on the basis. \u2212 L 0, S + L 0, (5) where the regulation function ML and the corresponding regulation functions (S + L) are each defined in the order of L."}, {"heading": "4 Error Bounds on ML LVGGM Estimation", "text": "We analyze the regularized ML estimation problem (5) and provide Frobenius standard error limits for estimating the precision matrix in a high-dimensional environment in order to derive these limits. In contrast to this earlier work, we focus here on several decomposable regulators that interact with the non-square log likelihood loss function of the LVGGM. Two important components in the derivatives are the limited strong convexity of the loss function and an incoherence condition between the two structured subspaces containing the sparse and subordinate components (S and L). We show that these two conditions are verified under assumptions based on Fisher information. In the following subsections, we first define some necessary notations, then we adjust the assumptions and place them in the context of the previous literature and finally in theory 1 and theorem 2."}, {"heading": "4.1 Decomposable Regularizers and Subspace Notation", "text": "In this subsection we present the concept of the decomposable regularizers and the corresponding sub-space pairs. We refer to Negahban et al. (2012) for more detailed details. Consider a pair of sub-spaces (M, M) in which M + v) = R (u) + R (v). For the sparse and low-weighted matrix parameters, the following two sub-space pairs and their corresponding decomposable regularizers are considered: \u2022 Sparse matrices. Let E {1,.., p} {1,., p} be a subset of index pairs (edges). DefineM (E) = M (E) as the subspace of all sparse matrices supported in sub-spaces."}, {"heading": "4.2 Assumptions on Fisher Information", "text": "We characterize the interaction between the elements in the two subranges based on their internal products using the Hessian loss function (also known as Fisher information of distribution). We define the Fisher information matrix of a Gaussian distribution as follows: \"A,\" \"B,\" \"F,\" \"D,\" \"D,\" \"D,\" \"D,\" \"D,\" \"D,\" \"D,\" \"D,\" \"D,\" \"D,\" \"D,\" \"D,\" \"D.,\"., \"\" D, \"\" D., \"D.,\", \"D.,\" D., \"D.,\" D., \"D.,\", \"D.,\" D., \"D.,\" D., \"D.,\" D., \"D.,\" D., \"D.,\" D., \"D.,\" D., \"D.,\" D., \"D.,\" D., \"D.,\" D., \",\" D., \"D.,\" D., \"D.,\" D., \"D.,\" D., \"D.,\" D., \"D.,\" D., \"D,\" D., \"D.,\" D, \"D,\" D, \"D.\" D, \"D,\" D. \"D,\" D, \"D,\" D, \"D.\" D, \"D,\" D, \"D,\" D. \",\" D, \"D,\" D. \"D,\" D, \"D,\" D. \"D,\", \"D,\" D, \"D.\" D, \"D,\" D, \"D,\" D, \"D.\" D, \",\" D, \"D,\" D. \"D.\", \",\" D. \",\" D. \",\" D. \",\" D. \",\", \"D.\", \",\" D. \",\", \"D.\", \"D.\", \",\" D. \"D,\", \"D.\", \",\" D. \",\", \"D.\", \",\" D. \",\", \"D.\" D. \",\", \"D.\", \"D.\", \",\" D. \",\", \",\", \",\", \"D.\", \",\" D. \",\", \",\", \""}, {"heading": "4.3 Error Bounds for LVGGM Estimation", "text": "We have the following parameter error of the estimated precision matrix of LVGGM = U 13, where we (S + L) s (S + S) s (S + S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S (S) s (S) s (S) s (S) s (S (S) s (S) s (S) s (S) s (S (S) s (S) s (S (S) s) s (S (S) s (S (S) s) s (S (S (S) s) s (S (S (S) s) s (S (S (S (S) s) s (S (S (S (S) s) s) s (S (S (S (S (S (S) s) s) s (S (S (S (S (S (S (S (S) s) s) s) s (S (S (S (S (S (S (S (S (S (S (S) s) s) s) s (S (S (S (S (S (S (S (S (S (S) s) s) s (S (S (S (S (S (S (S) s) s) s (S (S (S (S (S (S (S (S (S (S) s) s) s) s) s (S (S (S (S (S (S (S) s"}, {"heading": "5 Experiments", "text": "We use a series of simulations of synthetic data to verify our reduced effective ranking on the covariance matrix of LVGGM and the error limits derived from it in Theorem 2."}, {"heading": "5.1 Effective Rank of Covariance of LVGGM", "text": "In order to better understand the effective rank of the covariance matrix of LVGGM, it is advisable to consider a hierarchical generation process for the observed variables: xO \u0445 AxL + z, where xL \u0445 N (0, L, L) are the latent variables, A: J \u2212 1O, L Rp \u00b7 r, and z \u00b2 N (0, S \u2212 1) capture the conditional effects. The marginal covariance matrix of the observed variables can be represented as AxL."}, {"heading": "5.2 Frobenius Norm Error of LVGGM Estimation", "text": "We simulate LVGGM data with the number of observed variables p = {160, 200, 320, 400} and the number of latent variables in the set r = {0.1, 0.15, 0.2, 0.3} p. The sparse conditional GGM is a chain graph whose corresponding precision matrix is tridiagonal with the extra-diagonal elements Si, i \u2212 1 = Si, i + 1 = 0.4Si, i for i = {2,.., p \u2212 1}. For each configuration of p and r, we draw n samples from the LVGGM where n is in the range of 200 to 1000. On the basis of these samples, the precision matrix is learned by solving the regulated ML estimation problem (5). As shown in section 5.1, the effective rank of the covariance matrix in the models p and p is calculated."}, {"heading": "6 Conclusions", "text": "We look at a family of latently variable Gaussian graphical models whose precision matrix has a sparse plus low structure. We derive parameter error limits for a regulated maximum probability estimate. Future work will include extending the scope to other distributions and applying it to tasks such as prediction and detection."}, {"heading": "Acknowledgement", "text": "This work is supported in part by the ARO grant W911NF-11-1-0391. The authors thank the anonymous reviewers for their valuable comments and Jason Lee and Yuekai Sun for helpful discussions."}, {"heading": "A Motivating Real-World Examples", "text": "This year is the highest in the history of the country."}, {"heading": "B Proof of Theorem 1", "text": "In Yang & Ravikumar (2013), the authors have provided a general overarching estimate of the error rate (> SR) using the decomposable regulated framework (2012). Similarly, theorem 1 can be achieved by specializing the result in Yang & Ravikumar (2013) on the LVGGM learning problem (5), then it is sufficient to verify the two critical conditions (C3) and (C4) in Yang & Ravikumar (2013) (the other two conditions are trivial to verify for our problem) that we have in this Section.Restricted Strong Confexity. Let the Restricted Strong Confexity L (2008) on the Restricted Term in the Firder Taylor series approximate the loss function L () with respect to the true parameters."}, {"heading": "C Proof of Lemma 2", "text": "The rest of the term in the first order of the Taylor series of the negative probability (4) of the GGM results in the following form: \u03b4L (3) = L (4), L (4), L (4), L (4), L (4), L (5), L (4), L (4), L (4), L (4), L (4), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), L (5), K, K (5), K, K (5), K, K (5), K (5), K (5), K (5), K (5)."}, {"heading": "D Proof of Lemma 3", "text": "s inner product between elements from the two sectors. (Assuming assumptions 1 and 2 apply to the true boundary precision matrix.) Then the following inequality applies to all S-C (E) and L-C (U), so that max. (Assumed) S-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2M-2M. The evidence for Lemma 4 follows in a similar way to Proposition 2 in Yang & Ravikumar (2013), and is therefore not taken into account. (Next, we prove Lemma 3 using the above result.) we follow similar derivatives to the evidence for Lemma 2, the inconsistency of the SI condition."}, {"heading": "E Proof of Corollary 1", "text": "Proof. Theorem 1 is a deterministic statement, however, the condition with respect to regularization parameters (11) and the error limit depends on the covariance number of the sample, which is random. Note that the error limit follows directly from the deterministic error, which is limited in Theorem 1 and the selection of regularization parameters according to Equation (16). To prove the corollary number 1, it remains to be verified that condition (11) in Theorem 1 is highly likely guaranteed. Specifically, this requires the deviation of the sample covariance matrix with respect to \"and spectral norms.\" First, we use the following problem to characterize the elementary deviation of the sample covariance matrix. Lemma 5 (Ravikumar et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al. (2011): For a p-dimensional gaussionix matrix, the corix matrix with the coance matrix results."}], "references": [{"title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "author": ["Agarwal", "Alekh", "Negahban", "Sahand", "Wainwright", "Martin J"], "venue": "The Annals of Statistics,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["Bickel", "Peter J", "Ritov", "Yaacov", "Tsybakov", "Alexandre B"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Robust principal component analysis", "author": ["Cand\u00e8s", "Emmanuel J", "Li", "Xiaodong", "Ma", "Yi", "Wright", "John"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Latent variable graphical model selection via convex optimization", "author": ["Chandrasekaran", "Venkat", "Parrilo", "Pablo A", "Willsky", "Alan S"], "venue": "Annals of Statistics,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q1935\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 1935}, {"title": "Robust shrinkage estimation of high-dimensional covariance matrices", "author": ["Chen", "Yilun", "Wiesel", "Ami", "Hero", "Alfred O"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Gaussian multiresolution models: Exploiting sparse Markov and covariance structure", "author": ["Choi", "Myung Jin", "Chandrasekaran", "Venkat", "Willsky", "Alan S"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Choi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2010}, {"title": "Learning latent tree graphical models", "author": ["Choi", "Myung Jin", "Tan", "Vincent YF", "Anandkumar", "Animashree", "Willsky", "Alan S"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Choi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "BIG & QUIC: Sparse inverse covariance estimation for a million variables", "author": ["Hsieh", "Cho-Jui", "Sustik", "M\u00e1ty\u00e1s A", "Dhillon", "Inderjit", "Ravikumar", "Pradeep", "Poldrack", "Russell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hsieh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2013}, {"title": "Learning exponential families in high-dimensions: Strong convexity and sparsity", "author": ["Kakade", "Sham", "Shamir", "Ohad", "Sindharan", "Karthik", "Tewari", "Ambuj"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kakade et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2010}, {"title": "Residual component analysis: Generalising PCA for more flexible inference in linear-Gaussian models", "author": ["Kalaitzis", "Alfredo", "Lawrence", "Neil D"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Kalaitzis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalaitzis et al\\.", "year": 2012}, {"title": "Graphical models, volume 17", "author": ["S.L. Lauritzen"], "venue": null, "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Improved estimation of the covariance matrix of stock returns with an application to portfolio selection", "author": ["Ledoit", "Olivier", "Wolf", "Michael"], "venue": "Journal of Empirical Finance,", "citeRegEx": "Ledoit et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ledoit et al\\.", "year": 2003}, {"title": "On model selection consistency of penalized M-estimators: a geometric theory", "author": ["Lee", "Jason", "Sun", "Yuekai", "Taylor", "Jonathan E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Learning Gaussian graphical models with observed or latent FVSs", "author": ["Liu", "Ying", "Willsky", "Alan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "High-dimensional covariance matrix estimation with missing observations", "author": ["Lounici", "Karim"], "venue": "arXiv preprint arXiv:1201.2577,", "citeRegEx": "Lounici and Karim.,? \\Q2012\\E", "shortCiteRegEx": "Lounici and Karim.", "year": 2012}, {"title": "Alternating direction methods for latent variable Gaussian graphical model selection", "author": ["Ma", "Shiqian", "Xue", "Lingzhou", "Zou", "Hui"], "venue": "Neural computation,", "citeRegEx": "Ma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2013}, {"title": "Walk-sums and belief propagation in Gaussian graphical models", "author": ["Malioutov", "Dmitry M", "Johnson", "Jason K", "Willsky", "Alan S"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Malioutov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Malioutov et al\\.", "year": 2006}, {"title": "A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers", "author": ["Negahban", "Sahand N", "Ravikumar", "Pradeep", "Wainwright", "Martin J", "Yu", "Bin"], "venue": "Statistical Science,", "citeRegEx": "Negahban et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2012}, {"title": "High-dimensional covariance estimation by minimizing `1-penalized logdeterminant divergence", "author": ["Ravikumar", "Pradeep", "Wainwright", "Martin J", "Raskutti", "Garvesh", "Yu", "Bin"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Ravikumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2011}, {"title": "Sparse permutation invariant covariance estimation", "author": ["Rothman", "Adam J", "Bickel", "Peter J", "Levina", "Elizaveta", "Zhu", "Ji"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Rothman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rothman et al\\.", "year": 2008}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Vershynin", "Roman"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin and Roman.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2010}, {"title": "Robust PCA via outlier pursuit", "author": ["Xu", "Huan", "Caramanis", "Constantine", "Sanghavi", "Sujay"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Dirty statistical models", "author": ["Yang", "Eunho", "Ravikumar", "Pradeep"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "2012), the authors introduce the restricted strong convexity (RSC) condition, which specifies that given some set C \u2286 Rp\u00d7p, there exists some curvature parameter \u03baL > 0 and tolerance function \u03c4L", "author": ["Negahban"], "venue": null, "citeRegEx": "Negahban,? \\Q2012\\E", "shortCiteRegEx": "Negahban", "year": 2012}, {"title": "Note there is an important difference between the RSC condition considered here and the condition introduced in Agarwal et al", "author": ["Agarwal"], "venue": null, "citeRegEx": "Agarwal,? \\Q2012\\E", "shortCiteRegEx": "Agarwal", "year": 2012}, {"title": "\u221a 2 suffices for the above relation to hold (see Sec", "author": ["Kakade"], "venue": null, "citeRegEx": "Kakade,? \\Q2010\\E", "shortCiteRegEx": "Kakade", "year": 2010}, {"title": "For a p-dimensional Gaussian random vector with covariance matrix \u03a3\u2217, the sample covariance matrix obtained from n", "author": ["Ravikumar"], "venue": null, "citeRegEx": "Ravikumar,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar", "year": 2011}, {"title": "For a p-dimension Gaussian random vector with covariance matrix \u03a3\u2217 and let \u03c1\u2217", "author": ["Chandrasekaran"], "venue": null, "citeRegEx": "Chandrasekaran,? \\Q2012\\E", "shortCiteRegEx": "Chandrasekaran", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "To perform accurate model parameter estimation and subsequent statistical inference, low dimensional structure is often imposed for regularization (Negahban et al., 2012).", "startOffset": 147, "endOffset": 170}, {"referenceID": 11, "context": "Gaussian graphical models (GGM) provide an efficient representation of the precision matrix through a graph that represents non-zeros in the matrix (Lauritzen, 1996).", "startOffset": 148, "endOffset": 165}, {"referenceID": 8, "context": "On the computational side, sparsity also leads to reduced complexity of the estimator (Hsieh et al., 2013).", "startOffset": 86, "endOffset": 106}, {"referenceID": 5, "context": "Examples are the price of oil on the airlines\u2019 stock price variables (Choi et al., 2010), and the genres on movie rating variables.", "startOffset": 69, "endOffset": 88}, {"referenceID": 9, "context": "By utilizing the almost strong convexity (Kakade et al., 2010) of the log-likelihood, we derive a non-asymptotic parameter error bound for the regularized ML estimator.", "startOffset": 41, "endOffset": 62}, {"referenceID": 18, "context": "Our derived bounds apply to the high-dimensional setting of p n due to restricted strong convexity (Negahban et al., 2012) and certain structural incoherence between the sparse and low-rank components of the precision matrix (Yang & Ravikumar, 2013).", "startOffset": 99, "endOffset": 122}, {"referenceID": 6, "context": "Gaussian graphical models (GGM) provide an efficient representation of the precision matrix through a graph that represents non-zeros in the matrix (Lauritzen, 1996). In high-dimensional regimes, this graph can be forced to be sparse, imposing a low-dimensional structure on the GGM. For sufficiently sparse GGM, statistically consistent estimates of the model structure (i.e., sparsistency) can be achieved (e.g., Ravikumar et al. (2011)).", "startOffset": 149, "endOffset": 439}, {"referenceID": 3, "context": "The resulting marginal precision matrix of the LVGGM has a sparse plus low-rank structure, therefore we consider a regularized maximum likelihood (ML) approach for parameter estimation (previously considered by Chandrasekaran et al. (2012)).", "startOffset": 211, "endOffset": 240}, {"referenceID": 4, "context": "The problem of learning GGM with sparse inverse covariance matrices using `1regularized maximum likelihood estimation, often referred to as the graphical lasso (Glasso) problem, has been studied in Friedman et al. (2008); Ravikumar et al.", "startOffset": 198, "endOffset": 221}, {"referenceID": 4, "context": "The problem of learning GGM with sparse inverse covariance matrices using `1regularized maximum likelihood estimation, often referred to as the graphical lasso (Glasso) problem, has been studied in Friedman et al. (2008); Ravikumar et al. (2011); Rothman et al.", "startOffset": 198, "endOffset": 246}, {"referenceID": 4, "context": "The problem of learning GGM with sparse inverse covariance matrices using `1regularized maximum likelihood estimation, often referred to as the graphical lasso (Glasso) problem, has been studied in Friedman et al. (2008); Ravikumar et al. (2011); Rothman et al. (2008). In particular, the authors of Ravikumar et al.", "startOffset": 198, "endOffset": 269}, {"referenceID": 4, "context": "The problem of learning GGM with sparse inverse covariance matrices using `1regularized maximum likelihood estimation, often referred to as the graphical lasso (Glasso) problem, has been studied in Friedman et al. (2008); Ravikumar et al. (2011); Rothman et al. (2008). In particular, the authors of Ravikumar et al. (2011) study the model selection consistency (i.", "startOffset": 198, "endOffset": 324}, {"referenceID": 4, "context": "Beyond sparse GGM, Choi et al. (2010) propose a multiresolution extension of a GGM augmented with sparse inter-level correlations, while in Choi et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 4, "context": "Beyond sparse GGM, Choi et al. (2010) propose a multiresolution extension of a GGM augmented with sparse inter-level correlations, while in Choi et al. (2011) the authors consider latent tree-structured graphical models.", "startOffset": 19, "endOffset": 159}, {"referenceID": 4, "context": "Beyond sparse GGM, Choi et al. (2010) propose a multiresolution extension of a GGM augmented with sparse inter-level correlations, while in Choi et al. (2011) the authors consider latent tree-structured graphical models. Both models lead to computationally efficient inference and learning algorithms but restrict the latent structure to trees. Recently, Liu & Willsky (2013) consider a computationally efficient learning algorithm for a class of conditionally tree-structured LVGGM.", "startOffset": 19, "endOffset": 376}, {"referenceID": 3, "context": "The work that is most relevant to ours is by Chandrasekaran et al. (2012), who study the LVGGM learning problem, but focus on the simultaneous model selection consistency of both the sparse and low-rank components.", "startOffset": 45, "endOffset": 74}, {"referenceID": 3, "context": "The work that is most relevant to ours is by Chandrasekaran et al. (2012), who study the LVGGM learning problem, but focus on the simultaneous model selection consistency of both the sparse and low-rank components. In contrast, in this paper we focus on the Frobenius norm error bounds for estimating the precision matrix of LVGGM. Although structural consistency can be useful for deriving insights, parameter estimation error analysis is of equal or greater importance in practice. Since it provides additional, and usually more direct, insights into factors influencing the performance of the subsequent statistical inference tasks, such as prediction and detection. Also, compared with Chandrasekaran et al. (2012), our Frobenius norm error bounds are derived under mild condition on the Fisher information of the distribution.", "startOffset": 45, "endOffset": 719}, {"referenceID": 2, "context": "We note that there is a fundamentally different line of work on estimating models with a similar structural composition, known as robust PCA (Cand\u00e8s et al., 2011).", "startOffset": 141, "endOffset": 162}, {"referenceID": 22, "context": "This model has been applied to extracting the salient foreground from background in videos, and detecting malicious user ratings in recommender system data (Xu et al., 2012).", "startOffset": 156, "endOffset": 173}, {"referenceID": 2, "context": "We note that there is a fundamentally different line of work on estimating models with a similar structural composition, known as robust PCA (Cand\u00e8s et al., 2011). In robust PCA, the data matrix is modeled as \u201clow-rank plus sparse\u201d. This model has been applied to extracting the salient foreground from background in videos, and detecting malicious user ratings in recommender system data (Xu et al., 2012). In contrast, the equivalent covariance model of our LVGGM can be decomposed into a low-rank plus a dense matrix whose inverse is sparse. A similar covariance model has recently been studied by Kalaitzis & Lawrence (2012), in which an EM algorithm is proposed for estimation but no theoretical error bounds are derived.", "startOffset": 142, "endOffset": 629}, {"referenceID": 17, "context": "The precision matrix parameterization arises in many statistical inference problems for Gaussian distributions, in areas such as belief propagation (Malioutov et al., 2006), linear prediction, portfolio selection in financial data (Ledoit & Wolf, 2003), and anomaly detection (Chen et al.", "startOffset": 148, "endOffset": 172}, {"referenceID": 4, "context": ", 2006), linear prediction, portfolio selection in financial data (Ledoit & Wolf, 2003), and anomaly detection (Chen et al., 2011).", "startOffset": 111, "endOffset": 130}, {"referenceID": 3, "context": "Similar to Chandrasekaran et al. (2012), we consider the following regularized ML estimation problem:", "startOffset": 11, "endOffset": 40}, {"referenceID": 18, "context": "where the corresponding regularization function is the sum of two regularizers: R(\u0398) = \u2016S\u20161 + \u03bc\u03bb\u2016L\u2016\u2217, each of which has been shown to promote sparse (lowrank) structure in S (L, respectively) (Negahban et al., 2012).", "startOffset": 192, "endOffset": 215}, {"referenceID": 16, "context": "Efficient convex solver, such as Ma et al. (2013), can be used to solve.", "startOffset": 33, "endOffset": 50}, {"referenceID": 17, "context": "We adopt the decomposable regularization framework of Negahban et al. (2012); Agarwal et al.", "startOffset": 54, "endOffset": 77}, {"referenceID": 0, "context": "(2012); Agarwal et al. (2012); Yang & Ravikumar (2013) to derive these bounds.", "startOffset": 8, "endOffset": 30}, {"referenceID": 0, "context": "(2012); Agarwal et al. (2012); Yang & Ravikumar (2013) to derive these bounds.", "startOffset": 8, "endOffset": 55}, {"referenceID": 18, "context": "We refer the reader to Negahban et al. (2012) for more details.", "startOffset": 23, "endOffset": 46}, {"referenceID": 18, "context": "For the true model parameter \u0398\u2217, we define its associated structural error set with respect to a subspaceM as (Negahban et al., 2012): C(M,M; \u0398\u2217) := { \u2206 \u2208 Rn\u00d7p | R(\u2206M\u22a5) \u2264 3R(\u2206M) + 4R(\u0398 \u2217 M ) } .", "startOffset": 110, "endOffset": 133}, {"referenceID": 9, "context": "Similar to prior work of Kakade et al. (2010), we define the induced Fisher norm of a matrix \u2206 as \u2016\u2206\u2016F\u2217 := vec(\u2206)TF\u2217vec(\u2206) (8)", "startOffset": 25, "endOffset": 46}, {"referenceID": 1, "context": "This RFE condition generalizes the restricted eigenvalue (RE) condition for sparsity-promoting linear regression problems Bickel et al. (2009). It assumes that the minimum eigenvalue of the Fisher information is bounded away from zero along the directions C(E) and C(U).", "startOffset": 122, "endOffset": 143}, {"referenceID": 1, "context": "This RFE condition generalizes the restricted eigenvalue (RE) condition for sparsity-promoting linear regression problems Bickel et al. (2009). It assumes that the minimum eigenvalue of the Fisher information is bounded away from zero along the directions C(E) and C(U). Due to the identity (8) and properties of the Kronecker product, a trivial lower bound for \u03bamin is \u03bb 2 min(\u0398 \u2217), where \u03bbmin(\u00b7) denotes the minimum eigenvalue. In the high-dimensional setting, the RFE parameter \u03bamin, which is defined only with respect to the above restricted set of directions, can be substantially larger than \u03bbmin(\u0398 \u2217). As a result, the derived error bounds, which depend on \u03bamin, are generally tighter than the bounds depending on \u03bbmin(\u0398 \u2217) (cf. Theorem 1). Due to the sparse plus low-rank superpositioned structure, we impose a type of incoherence between the two structural error sets to ensure consistent estimation of the combined model. The incoherence condition will limit the interaction between elements from the two sets. For our problem, such interaction occurs through their inner products with the Fisher information, which motivates the following Structural Fisher Incoherence (SFI) assumption (which generalizes the C-Linear assumption proposed in Yang & Ravikumar (2013)).", "startOffset": 122, "endOffset": 1276}, {"referenceID": 17, "context": "In Ravikumar et al. (2011), a form of irrepresentability condition is assumed, which limits the induced `1 norm of a matrix that is similar to the projected Fisher information onto the sparse matrix subspace pair.", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.", "startOffset": 3, "endOffset": 32}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.e., sparse and low-rank), but detailed behaviors of the projected Fisher information are controlled (see the main assumption on page 1949 of Chandrasekaran et al. (2012)).", "startOffset": 3, "endOffset": 273}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.e., sparse and low-rank), but detailed behaviors of the projected Fisher information are controlled (see the main assumption on page 1949 of Chandrasekaran et al. (2012)). For model selection consistency, a more general form of irrepresentability has been shown to be necessary for model selection consistency, see Lee et al. (2013) for a recent discussion.", "startOffset": 3, "endOffset": 436}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.e., sparse and low-rank), but detailed behaviors of the projected Fisher information are controlled (see the main assumption on page 1949 of Chandrasekaran et al. (2012)). For model selection consistency, a more general form of irrepresentability has been shown to be necessary for model selection consistency, see Lee et al. (2013) for a recent discussion. In contrast to the above line of work, the SFI assumption we make only controls the maximum singular values of the projected Fisher information. This can be explained as we are interested in bounding a weaker quantity, the Frobenius norm of the parameter estimation error, instead of establishing the stronger model selection consistency of Ravikumar et al. (2011) or the algebraic consistency as in Chandrasekaran et al.", "startOffset": 3, "endOffset": 826}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.e., sparse and low-rank), but detailed behaviors of the projected Fisher information are controlled (see the main assumption on page 1949 of Chandrasekaran et al. (2012)). For model selection consistency, a more general form of irrepresentability has been shown to be necessary for model selection consistency, see Lee et al. (2013) for a recent discussion. In contrast to the above line of work, the SFI assumption we make only controls the maximum singular values of the projected Fisher information. This can be explained as we are interested in bounding a weaker quantity, the Frobenius norm of the parameter estimation error, instead of establishing the stronger model selection consistency of Ravikumar et al. (2011) or the algebraic consistency as in Chandrasekaran et al. (2012).", "startOffset": 3, "endOffset": 890}, {"referenceID": 9, "context": "To deal with this difficulty, we leverage the almost strong convexity properties (Kakade et al., 2010) to characterize the convergence behavior of the sum of higher-order terms in the Taylor series of the log-likelihood loss function.", "startOffset": 81, "endOffset": 102}, {"referenceID": 23, "context": "The proof is inspired by Yang & Ravikumar (2013), in which a parameter estimation error bound is proven for estimating a class of superpositionstructured parameters, such as sparse plus low-rank, through M-estimation with decomposable regularizers.", "startOffset": 32, "endOffset": 49}, {"referenceID": 17, "context": "The RSC condition (which originally proposed in Negahban et al. (2012)) specifies the loss function to be sufficiently curved (i.", "startOffset": 48, "endOffset": 71}, {"referenceID": 17, "context": "The RSC condition (which originally proposed in Negahban et al. (2012)) specifies the loss function to be sufficiently curved (i.e. lower bounded by a quadratic function) along a restricted set of directions (defined by C(E) and C(U)). On the other hand, the SI condition effectively limits certain interaction between elements from the above two structural error sets. In Yang & Ravikumar (2013), under certain C-linear assumptions, the RSC and SI conditions are verified for several problems with quadratic loss functions.", "startOffset": 48, "endOffset": 397}, {"referenceID": 9, "context": "To deal with this difficulty, we leverage the almost strong convexity properties (Kakade et al., 2010) to characterize the convergence behavior of the sum of higher-order terms in the Taylor series of the log-likelihood loss function. We show that in the regime specified by condition (12), the loss function can be wellapproximated by the sum of a quadratic function and a residual term. Under this condition, the RFE assumption (Assumption 1) guarantees the RSC condition (cf. Lemma 2), and the SFI assumption (Assumption 2) leads to SI condition to hold (cf. Lemma 4). Theorem 1 can then be proven by the general theorem in Yang & Ravikumar (2013). A detailed proof of Theorem 1 can be found in Appendix B.", "startOffset": 82, "endOffset": 651}, {"referenceID": 0, "context": "\u2022 We finally remark that the SFI assumption can be relaxed to an even milder incoherence condition, \u2016L\u2016\u221e \u2264 \u03b1, as considered in Agarwal et al. (2012). Following similar derivations as in the proof of Theorem 1, the corresponding error bound can be obtained.", "startOffset": 127, "endOffset": 149}, {"referenceID": 19, "context": "In particular, the first term in (17) was on the same order as the estimation error of a sparse GGM (Ravikumar et al., 2011).", "startOffset": 100, "endOffset": 124}, {"referenceID": 0, "context": ", Agarwal et al. (2012); Yang & Ravikumar (2013)) and the derived bound in Chandrasekaran et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 0, "context": ", Agarwal et al. (2012); Yang & Ravikumar (2013)) and the derived bound in Chandrasekaran et al.", "startOffset": 2, "endOffset": 49}, {"referenceID": 0, "context": ", Agarwal et al. (2012); Yang & Ravikumar (2013)) and the derived bound in Chandrasekaran et al. (2012). In particular, the first term in (17) was on the same order as the estimation error of a sparse GGM (Ravikumar et al.", "startOffset": 2, "endOffset": 104}], "year": 2014, "abstractText": "Gaussian graphical models (GGM) have been widely used in many highdimensional applications ranging from biological and financial data to recommender systems. Sparsity in GGM plays a central role both statistically and computationally. Unfortunately, real-world data often does not fit well to sparse graphical models. In this paper, we focus on a family of latent variable Gaussian graphical models (LVGGM), where the model is conditionally sparse given latent variables, but marginally non-sparse. In LVGGM, the inverse covariance matrix has a low-rank plus sparse structure, and can be learned in a regularized maximum likelihood framework. We derive novel parameter estimation error bounds for LVGGM under mild conditions in the high-dimensional setting. These results complement the existing theory on the structural learning, and open up new possibilities of using LVGGM for statistical inference.", "creator": "LaTeX with hyperref package"}}}