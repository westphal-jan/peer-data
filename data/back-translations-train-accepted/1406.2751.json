{"id": "1406.2751", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2014", "title": "Reweighted Wake-Sleep", "abstract": "Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure, which also provides a natural way to estimate the likelihood itself. Based on this interpretation, we propose that a sigmoid belief network is not sufficiently powerful for the layers of the inference network, in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.", "histories": [["v1", "Wed, 11 Jun 2014 00:44:31 GMT  (593kb,D)", "http://arxiv.org/abs/1406.2751v1", null], ["v2", "Fri, 5 Dec 2014 23:30:10 GMT  (593kb,D)", "http://arxiv.org/abs/1406.2751v2", null], ["v3", "Sat, 20 Dec 2014 04:25:43 GMT  (643kb,D)", "http://arxiv.org/abs/1406.2751v3", null], ["v4", "Thu, 16 Apr 2015 17:22:58 GMT  (643kb,D)", "http://arxiv.org/abs/1406.2751v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["j\\\"org bornschein", "yoshua bengio"], "accepted": true, "id": "1406.2751"}, "pdf": {"name": "1406.2751.pdf", "metadata": {"source": "CRF", "title": "Reweighted Wake-Sleep", "authors": ["J\u00f6rg Bornschein", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is true that it is a pure mental game in which it is a matter of changing the world in order to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change."}, {"heading": "2 Reweighted Wake-Sleep", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The Wake-Sleep Algorithm", "text": "The wake-sleep algorithm has been proposed as a way to train Helmholtz machines that are a specified distribution layer, which include deep graphical models p (x, h) of visible variables x and latent variables h, where the latent variables are organized in layers hk, with the k-th layer that uses as input the random vector generated by the previous layer in the generating sequence, hk + 1. In the Helmholtz machine (Hinton et al., 1995), the top layer, hL, has a factorized unconditional distribution, so that the ancestral sampling from hL down to h1 and then the generated sample x is generated by the lower layer, given h1. In the deep belief network (DBN) (Hinton et al., 2006), the top layer is instead generated by a restricted Boltzmann machine (BM), i.e."}, {"heading": "2.2 An Importance Sampling View yields Reweighted Wake-Sleep", "text": "If we assume that q (h | x) p (h | x) estimates and q (which is basically what the sleep phase of waking sleep does) q (what the sleep phase does), then we can rephrase the probability as a meaning-weighted average: p (x) = \u2211 h (x, h) q (h | x) p (h) q (h) | x) q (1) Eq (h | x) [p, h) q (h | x)] '1 KK \u00b2 k = 1h (k) \u0445 q (h | x) p (h) q (h) | x) q (1) Eq (h). (1) is a consistent and unbiased estimator of the marginal probability p (x). This already gives us something interesting if we want to estimate the probability associated with a test sample x. But depending on the q (h | x), it may suffer from a very high variance."}, {"heading": "2.3 Training by Reweighted Wake-Sleep", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "2.4 Relation to Wake-Sleep and Variational Bayes", "text": "There has been a resurgence of interest in algorithms related to the wake-sleep algorithm for directed graphical models such as the sigmoidal belief networks (SBN) and the Helmholtz machine (which is a generative SBN paired with an approximate inference SBN). In neural variation and learning (NVIL, Mnih and Gregor (2014), the authors propose to maximize the variable lower limit on the log likelihood in order to achieve a common goal for both P2 and Q2. It was known that this approach leads to a gradient estimate of very high variance for the recognition network q (Dayan and Hinton, 1996). In the NVIL paper, the authors use variance reproduction techniques such as baseline to obtain a practical algorithm that significantly exceeds the original wake-sleep algorithm."}, {"heading": "3 Component Layers", "text": "Although the framework can be easily applied to continuous variables, we restrict ourselves here to distributions of binary visible and binary latent variables. < < < < < < < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >"}, {"heading": "4 Experiments", "text": "Here we present a series of experiments on MNIST and the CalTech Silhouette datasets. The supplement describes additional experiments on various known but smaller datasets. With these experiments, we want to (1) quantitatively analyze the influence of the number of samples K; (2) show that using a more powerful layer model for the inference network q can significantly improve the results, even if the generative model is a simple SBN; and (3) that we can achieve (nearly) state-of-the-art performance, especially when using high-performance layer models such as a conditional NADE. Our implementation is available at https: / / github.com / jbornschein / reweighted-ws /."}, {"heading": "4.1 MNIST", "text": "We use the MNIST dataset downloaded from Murray and Salakhutdinov (2009) and in its binary form from (Larochelle, 2011). We use the last 1000 datapoints of validation set and do early stop samples with a glance 10 years ago. For training, we use stochastic gradients with decent momentum (\u03b2 = 0.95) and set mini batch size to 25. The experiments in this paragraph were performed at learning rates of {0.003, 0.001 and 0.003}. From these three, we always report on the experiment with the highest validation log likelihood probability. In the majority of our experiments a learning rate of 0.001 gave the best results, even across different shift models (SBN, DARN and NADE). If we are not advised that we are trained K = 5 samples during training and K = 500 samples to estimate the final log likelihood liquidity."}, {"heading": "4.2 CalTech 101 Silhouettes", "text": "This dataset consists of 4100 examples in the training set, 2264 examples in the validation set and 2307 examples in the test set. We trained different architectures on this dataset with the same hyperparameter as for the MNIST experiments. Table 2 (right) summarizes our results. Note that our best SBN / SBN model is a relatively deep network with 4 hidden layers (300-100-50-10) and reaches an estimated LL of -116.9 in the test set. Our best network, a flat NADE / NADE-150 network, reaches -104.3 and is improving over the previous state of the art (\u2212 107.8, an RBM with 4000 hidden units by Cho et al. (2013))."}, {"heading": "5 Conclusions", "text": "We have introduced a novel training method for deep generative models, which can have either discrete or continuous latent variables that reweight the wake-sleep. It generalizes and improves using the wake-sleep algorithm a lower bias and lower variance estimators of the log probability and gradient, at the price of more samples from the inference network. We were able to build models to achieve or improve the state of the art at several discrete data distributions (Caltech silhouettes in the main paper, various smaller data sets from the UCI repository in additional material). For the MNIST data set, we did not reach the current state of the art (85.23 \u00b1 0.43 for RWS compared to 84.55 for a DBN). Nevertheless, we were able to show that training directed models with reweighted wake-sleep results in competing models that produce high-quality samples and that are increasingly trained on network-level with the most efficient models and literature in terms of log probability."}, {"heading": "Acknowledgments", "text": "We would like to thank Laurent Dinh, Vincent Dumoulin and Li Yao for helpful discussions and the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for their powerful software, CIFAR and Canada Research Chairs for funding and Compute Canada, and Calcul Que \ufffd bec for providing computing resources."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Now Publishers", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Modeling high-dimensional discrete data with multi-layer neural networks", "author": ["Y. Bengio", "S. Bengio"], "venue": "In NIPS\u201999,", "citeRegEx": "Bengio and Bengio,? \\Q2000\\E", "shortCiteRegEx": "Bengio and Bengio", "year": 2000}, {"title": "Better mixing via deep representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML\u201913)", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "In ICML\u20192012", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Enhanced gradient for training restricted boltzmann machines", "author": ["K. Cho", "T. Raiko", "A. Ilin"], "venue": "Neural computation,", "citeRegEx": "Cho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2013}, {"title": "The Helmholtz machine", "author": ["P. Dayan", "G.E. Hinton", "R.M. Neal", "R.S. Zemel"], "venue": "Neural computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "Graphical models for machine learning and digital communication", "author": ["B.J. Frey"], "venue": null, "citeRegEx": "Frey,? \\Q1998\\E", "shortCiteRegEx": "Frey", "year": 1998}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "A. Mnih", "D. Wierstra"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "The wake-sleep algorithm for unsupervised neural networks", "author": ["G.E. Hinton", "P. Dayan", "B.J. Frey", "R.M. Neal"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Efficient gradient-based inference through transformations between bayes nets and neural nets", "author": ["D.P. Kingma", "M. Welling"], "venue": "Technical report,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Binarized mnist dataset. http://www.cs.toronto.edu/ \u0303larocheh/ public/datasets/binarized_mnist/binarized_mnist_[train|valid|test].amat", "author": ["H. Larochelle"], "venue": null, "citeRegEx": "Larochelle,? \\Q2011\\E", "shortCiteRegEx": "Larochelle", "year": 2011}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u20192011),", "citeRegEx": "Larochelle and Murray,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "A deep and tractable density estimator", "author": ["B.U.I. Murray", "H. Larochelle"], "venue": null, "citeRegEx": "Murray and Larochelle,? \\Q2014\\E", "shortCiteRegEx": "Murray and Larochelle", "year": 2014}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["I. Murray", "R. Salakhutdinov"], "venue": "In NIPS\u201908,", "citeRegEx": "Murray and Salakhutdinov,? \\Q2009\\E", "shortCiteRegEx": "Murray and Salakhutdinov", "year": 2009}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "Technical report,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov and Murray,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray", "year": 2008}, {"title": "Mean field theory for sigmoid belief networks", "author": ["L.K. Saul", "T. Jaakkola", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Saul et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1996}], "referenceMentions": [{"referenceID": 11, "context": "This is unfortunate because, as has been argued previously (Hinton et al., 2006; Bengio, 2009), a deeper generative model has the potential to capture high-level abstractions and thus generalize better.", "startOffset": 59, "endOffset": 94}, {"referenceID": 1, "context": "This is unfortunate because, as has been argued previously (Hinton et al., 2006; Bengio, 2009), a deeper generative model has the potential to capture high-level abstractions and thus generalize better.", "startOffset": 59, "endOffset": 94}, {"referenceID": 7, "context": "The exact log-likelihood gradient is intractable, be it for Helmholtz machines (Dayan et al., 1995), sigmoid belief networks (SBNs), or deep belief networks (DBNs) (Hinton et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 11, "context": ", 1995), sigmoid belief networks (SBNs), or deep belief networks (DBNs) (Hinton et al., 2006), which are directed, or deep Boltzmann machines (DBMs), which are undirected.", "startOffset": 72, "endOffset": 93}, {"referenceID": 10, "context": "Past proposals to train Helmholtz machines and DBNs rely on maximizing a variational bound as proxy for the log-likelihood (Hinton et al., 1995; Kingma and Welling, 2014b; Rezende et al., 2014).", "startOffset": 123, "endOffset": 193}, {"referenceID": 19, "context": "Past proposals to train Helmholtz machines and DBNs rely on maximizing a variational bound as proxy for the log-likelihood (Hinton et al., 1995; Kingma and Welling, 2014b; Rezende et al., 2014).", "startOffset": 123, "endOffset": 193}, {"referenceID": 10, "context": "The first of these is the wake-sleep algorithm (Hinton et al., 1995), which relies on combining a \u201crecognition\u201d network (which we call an approximate inference network, here, or simply inference network) with a \u201cgenerative\u201d network.", "startOffset": 47, "endOffset": 68}, {"referenceID": 3, "context": "Unlike in the case of DBMs, which rely on a Markov chain to get samples and estimate the gradient by a mean over those samples, here the samples are iid, avoiding the very serious problem of mixing between modes that can plague MCMC methods (Bengio et al., 2013) when training undirected graphical models.", "startOffset": 241, "endOffset": 262}, {"referenceID": 10, "context": "In the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995), the top layer, hL, has a factorized unconditional distribution, so that ancestral sampling can proceed from hL down to h1 and then the generated sample x is generated by the bottom layer, given h1.", "startOffset": 25, "endOffset": 66}, {"referenceID": 7, "context": "In the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995), the top layer, hL, has a factorized unconditional distribution, so that ancestral sampling can proceed from hL down to h1 and then the generated sample x is generated by the bottom layer, given h1.", "startOffset": 25, "endOffset": 66}, {"referenceID": 11, "context": "In the deep belief network (DBN) (Hinton et al., 2006), the top layer is instead generated by a restricted Boltzmann machine (RBM), i.", "startOffset": 33, "endOffset": 54}, {"referenceID": 11, "context": "In the case of a DBN (where the top layer is generated by an RBM), there is an intermediate solution, which has been proposed in (Hinton et al., 2006) and called contrastive wake-sleep.", "startOffset": 129, "endOffset": 150}, {"referenceID": 16, "context": "In Neural variational inference and learning (NVIL, Mnih and Gregor (2014)) the authors propose to maximize the variational lower bound on the log-likelihood to get a joint objective for both p\u03b8", "startOffset": 52, "endOffset": 75}, {"referenceID": 19, "context": "Recent examples for continuous latent variables include the auto-encoding variational Bayes (Kingma and Welling, 2014a) and stochastic backpropagation papers (Rezende et al., 2014).", "startOffset": 158, "endOffset": 180}, {"referenceID": 21, "context": "In the following we will describe experiments containing three kinds of layers: Signoid Belief Network (SBN): A SBN layer (Saul et al., 1996) is a directed graphical model with independent variables xi given the parents y.", "startOffset": 122, "endOffset": 141}, {"referenceID": 8, "context": "Deep AutoRegressive Network (DARN): An autoregressive sigmoid belief network layer (Frey, 1998; Bengio and Bengio, 2000; Gregor et al., 2014) is similar to an SBN layer but with the output units xi not being independent of each other, given the layer\u2019s input y.", "startOffset": 83, "endOffset": 141}, {"referenceID": 2, "context": "Deep AutoRegressive Network (DARN): An autoregressive sigmoid belief network layer (Frey, 1998; Bengio and Bengio, 2000; Gregor et al., 2014) is similar to an SBN layer but with the output units xi not being independent of each other, given the layer\u2019s input y.", "startOffset": 83, "endOffset": 141}, {"referenceID": 9, "context": "Deep AutoRegressive Network (DARN): An autoregressive sigmoid belief network layer (Frey, 1998; Bengio and Bengio, 2000; Gregor et al., 2014) is similar to an SBN layer but with the output units xi not being independent of each other, given the layer\u2019s input y.", "startOffset": 83, "endOffset": 141}, {"referenceID": 2, "context": "Instead of a logistic regression, that dependency is mediated by an MLP (Bengio and Bengio, 2000).", "startOffset": 72, "endOffset": 97}, {"referenceID": 12, "context": "Conditional NADE: The Neural Autoregressive Distribution Estimator (NADE, Larochelle and Murray (2011)) is a model that uses an internal, accumulating hidden layer to predict an observed variable xi given the vector containing all previously observed variables xj .", "startOffset": 74, "endOffset": 103}, {"referenceID": 16, "context": "In the third column we cite the numbers reported by Mnih and Gregor (2014). Columns three and four report the variational NLL bounds; columns 5 to 8 report the NLL estimates (See Section 2.", "startOffset": 52, "endOffset": 75}, {"referenceID": 8, "context": "For a SBN layer we obtain a factorized Bernoulli distribution, for the DARN layer we obtain a fully-visible sigmoid belief network (FVSBN, (Frey, 1998)) and for the conditional NADE layer we obtain a regular, unconditioned NADE.", "startOffset": 139, "endOffset": 151}, {"referenceID": 14, "context": "1 MNIST We use the MNIST dataset that was binarized according to Murray and Salakhutdinov (2009) and downloaded in it\u2019s binarized form from (Larochelle, 2011).", "startOffset": 140, "endOffset": 158}, {"referenceID": 17, "context": "1 MNIST We use the MNIST dataset that was binarized according to Murray and Salakhutdinov (2009) and downloaded in it\u2019s binarized form from (Larochelle, 2011).", "startOffset": 65, "endOffset": 97}, {"referenceID": 15, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al.", "startOffset": 144, "endOffset": 173}, {"referenceID": 17, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al.", "startOffset": 179, "endOffset": 208}, {"referenceID": 20, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al.", "startOffset": 214, "endOffset": 246}, {"referenceID": 18, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al.", "startOffset": 252, "endOffset": 284}, {"referenceID": 6, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al., 2013).", "startOffset": 290, "endOffset": 308}, {"referenceID": 6, "context": "8, an RBM with 4000 hidden units by Cho et al. (2013)).", "startOffset": 36, "endOffset": 54}, {"referenceID": 4, "context": "We would like to thank Laurent Dinh, Vincent Dumoulin and Li Yao for helpful discussions and the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for their powerful software.", "startOffset": 118, "endOffset": 163}, {"referenceID": 0, "context": "We would like to thank Laurent Dinh, Vincent Dumoulin and Li Yao for helpful discussions and the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for their powerful software.", "startOffset": 118, "endOffset": 163}], "year": 2014, "abstractText": "Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure, which also provides a natural way to estimate the likelihood itself. Based on this interpretation, we propose that a sigmoid belief network is not sufficiently powerful for the layers of the inference network, in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.", "creator": "LaTeX with hyperref package"}}}