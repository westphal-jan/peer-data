{"id": "1406.3339", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2014", "title": "Algorithms for CVaR Optimization in MDPs", "abstract": "In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.", "histories": [["v1", "Thu, 12 Jun 2014 19:56:16 GMT  (94kb)", "https://arxiv.org/abs/1406.3339v1", "Submitted to NIPS 14"], ["v2", "Tue, 17 Jun 2014 18:05:38 GMT  (93kb)", "http://arxiv.org/abs/1406.3339v2", "Submitted to NIPS 14"], ["v3", "Thu, 10 Jul 2014 21:59:26 GMT  (99kb)", "http://arxiv.org/abs/1406.3339v3", "Submitted to NIPS 14"]], "COMMENTS": "Submitted to NIPS 14", "reviews": [], "SUBJECTS": "cs.AI math.OC", "authors": ["yinlam chow", "mohammad ghavamzadeh"], "accepted": true, "id": "1406.3339"}, "pdf": {"name": "1406.3339.pdf", "metadata": {"source": "CRF", "title": "Algorithms for CVaR Optimization in MDPs", "authors": ["Yinlam Chow"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 6.33 39v3 [cs.AI] 10 Ju"}, {"heading": "1 Introduction", "text": "A standard optimization criterion for an infinite horizon Markov Decision Process (MDP) is the expected sum of (discounted) costs (i.e., to find a policy that minimizes the value function of the initial state of the system). However, in many applications we may prefer to minimize a certain amount of risk in addition to this standard optimization criterion. In such cases, we would like to use a criterion that includes a penalty for variability (due to the stochastic nature of the system) induced by a certain policy. In risk-sensitive MDPs [18], the goal is to minimize a risk-sensitive criterion such as the expected exponential usefulness [18], a variance-related metric [32, 16] or percentile performance [17]. The problem of constructing such criteria in a way that will be both conceptually meaningful and mathematically tractable is still an open question."}, {"heading": "2 Preliminaries", "text": "We consider problems where the agent's interaction with the environment is modeled as an MDP system (\u00b7 x distribution); an MDP is a tuple M = (X, A, C, P, P0), where X = {1,.., n} and A = {1,.., m) are the state and action spaces; C (x, a) is the probability distribution of the transition; and P0 (\u00b7 Cmax, Cmax] is the limited cost variable whose expectation is denoted by c (x, a) = E (x, a)]; P (\u00b7 x, a) is the probability distribution; and P0 (\u00b7) is the initial state distribution. For simplification, we assume that the system has a single initial state x0, i.e., P0 (x) = 1 {x = x0}. All the results of the paper can easily be extended to the case that the system has more than one initial state."}, {"heading": "3 CVaR Optimization in MDPs", "text": "V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1clav V\u00e1"}, {"heading": "4 A Trajectory-based Policy Gradient Algorithm", "text": "In this section, we present a policy gradient algorithm to solve the optimization problem (4). The unit of observation in this algorithm is a system curve generated by following the current policy. At each iteration, the algorithm generates N trajectories by following the current policy, and uses them to estimate the gradients in (5) - (7) and then use these estimates to update the parameters."}, {"heading": "5 Incremental Actor-Critic Algorithms", "text": "As mentioned in Section 4, the unit of observation in our gradient algorithm (algorithm 1) is a system trajectory. This can lead to high variance in gradient estimates, especially if the length of the trajectories is long. To address this problem, we should show in this section how the gradations of (5) - (7) are estimated incrementally. We will show this in the next four subsections, followed by a subsection containing the algorithms. Algorithm 1 Trajectory-based gradient algorithm for CVaR optimization input: parameterized policies (\u00b7; 2001), confidence level, loss tolerance \u03b2 \u2212 and lagrange threshold conversion."}, {"heading": "5.1 Gradient w.r.t. the Policy Parameters \u03b8", "text": "The gradient of our objective function w.r.t. the political parameters \u03b8 in (5) can be rewritten in (5) as: \"empirical\" (empirical), \"empirical\" (empirical), \"empirical\" (empirical), \"empirical\" (empirical), \"empiirical\" (empirical), \"empirical\" (empirical), \"empirical\" (empirical), \"empirical\" (empirical), \"empirical\" (empirical), \"empirical\" (empirical), \"empirical\" (empirical), \"empirically\" (empirical), \"empirically\" (empirical), \"empirically (empirical),\" empirically (empirical), \"(empirical),\" empirically \"(empirical),\" (empirical), \"empirically\" (empirical), \"(empirically),\" (empirically), \"(empirically),\" (empirically), \"(empirically\" (empirically), \"(empirically),\" (empirically \"(empirically),\" (empirically), \"(empirically),\" (empirically (empirically), \"(empirically),\" (empirically), \"(empirically (empirically),\" (empirically), \"(empirically (empirically),\" (empirically), \"(empirically),\" (empirically (empirically), \"(empirically),\" (empirically (empirically), \"(empirically),\" (empirically (empirically), \"(empirically (empirically),\" (empirically), \"(empi"}, {"heading": "5.2 Gradient w.r.t. the Lagrangian Parameter \u03bb", "text": "We can describe the gradient of our objective function w.r.t. the Lagrange parameters \u03bb in (7) as \u2211 \u03bbL (\u03b8, \u03bd) = \u03bd \u2212 \u03bd \u2212 \u03bd \u2212 \u03bd (E [D\u03b8 (x0)] + \u03bb (1 \u2212 \u03b1) E [(D\u03b8 (x0) \u2212 \u03bd) +]) (a) = \u03bd \u2212 \u03bbV \u03b8 (x0, \u03bd). (11) Similar to Section 5.1, (a), the quantity in brackets in (11) derives from the fact that the quantity in (x0) is considered a criterion (x0, \u03bd), the value of the policy approach in state (x0, \u03bd) in the extended MDP-M question. Note that the dependence of the V approach (x0, \u03bd) is based on the definition of the cost function C in M-question. We now derive an expression in relation to the policy that x0, x4) that will give us an expression in relation to the precedent-L."}, {"heading": "5.3 Sub-Gradient w.r.t. the VaR Parameter \u03bd", "text": "We can rewrite the sub-gradient of our objective function w.r.t. the VaR parameters \u03bd in (6) as XI procedure. (3) From the definition of the extended MDP M profile, the probability in (13) can be written as P (sT \u2264 0 | x0 = x0, s0 = x0; \u03b8). (13) From the definition of the extended MDP M profile, the probability in (13) can be written as P (sT \u2264 0 | x0 = x0, s0 =; \u03b8), where sT is the s part of the state in M profile, i.e. x = xT (see Section 5.1). Therefore, we can rewrite the SDP profile value in this state. (1 \u2212 1 \u2212 \u03b1) and the SD profile value in this state. (1 \u2212 \u03b1) P (s0 = x0, s0 = sp profile value). (14) From this point on, it is easy to see."}, {"heading": "5.4 An Alternative Approach to Compute the Gradients", "text": "In this section, we present an alternative method for calculating gradients, in particular those where the DP-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P"}, {"heading": "5.5 Actor-Critic Algorithms", "text": "In this section, we present two stakeholder-critic algorithms to optimize the risk-sensitive metric (4). These algorithms are based on the gradient estimates of sections 5.1-5.3. While the first algorithm (SPSA-based) is fully incremental and all parameters \u03b8, \u03bd, \u03bb are updated at each time step, the second algorithms are updated at each time step, and they are updated only at the end of each path, i.e. called semi-trajectory-based. Algorithm 2 contains the pseudo-code of these algorithms. The projection operators are defined as in section 4 and are necessary to ensure convergence of the algorithms. Step-size plans meet the standard conditions for stochastic approximation algorithms and ensure that the critical update takes place at the fastest time scale."}, {"heading": "6 Experimental Results", "text": "We look at an optimal stop problem where the state at each time step k \u2264 T consists of the costs and the time k, i.e., x = (ck, k), where T is the stop time. The broker (buyer) should either accept or wait for the current costs. If he accepts or if k = T, the system reaches a final state and the costs are invoiced, otherwise he receives the cost ph and the new state is (ck + 1, k + 1) where ck + 1 fuck w.p and fdck w.p (fu > 1 and fd < 1 are constants). Furthermore, there is a discounted factor (0, 1) to take into account the affordability of the purchaser. The problem was described in more detail in Appendix C. Note, if we change the costs to reward and minimize, this is exactly the American option problem, a standard test bed for evaluating risk-sensitive algorithms."}, {"heading": "7 Conclusions and Future Work", "text": "We demonstrated convergence (in the appendix) to locally risk-sensitive optimal strategies for the proposed algorithms, and, using an optimal stop problem, we observed that our algorithms resulted in strategies whose loss distributions were lower right-tailed than their risk-neutral counterparts, which is critical for a risk-averse policymaker, especially if the right tail contained catastrophic losses. 2) Here, we set asymptotic limits for our algorithms, and to the best of our knowledge, there are no convergence proofs for our AC algorithms if the results are available for multi-time, multi-sequencing, and multi-sequencing algorithms."}, {"heading": "A Technical Details of the Trajectory-based Policy Gradient Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Assumptions", "text": "We base our algorithms on the following assumptions for the number of steps: (A1) For each pair of states (x, a), the Markov chain induced by any policy is continuously differentiable and aperiodic. (A3) The number of steps is {3 (i), {3 (i)}, {3 (i)} and {3 (i)} satisfactory. (1 (i) = 1 (i) = 1 (i) = 1 (i) (i), (23): 1 (i) 2 (i), 2 (i) 2 (i), {3 (i)} and {3 (i)}. (1 (i) = 1 (i) = 1 (i) (i) = 1 (i) (i), (23): 1 (i) 2 (i), 2 (i) 2 (i), {3 (i)}} and {3 (i)} satisfactory. (1 (i) = 1 (i) = 1 (i) = 1 (i) (i), (i) = 1 (i), (i), (i (i), 4 (i) 2 (i (i), 5 (i), 5 (i, 5 (i), 5 (i, 5, 5, 5 (i), 5 (i, 5, 5, 5, 5 (i, 5), 5 (i, 5, i, 5, 5 (i, 5), 5 (i, 5, 5, 5, i, 5, 5 (i, 5), 5 (i, 5, 5, 5, i, 5, 5 (i, 5), 5 (i, 5, 5, i, 5, 5, 5 (i, i, 5, 5), 5 (i, 5, 5, 5, 5 (i), 5 (i, i, 5, 5, 5 (i), 5 (i, 5, 5 (i, 5, 5, 5), 5 (i, 5, 5 (i, 5, 5, 5, i, 5, 5, 5, 5, 5, 5, i), 5 (i, 5, i, 5, 5, 5, 5, 5 (i)."}, {"heading": "A.2 Computing the Gradients", "text": "i): Nominal Value (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal) (Nominal Value) (Nominal) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (Nominal Value) (nominal Value) (nominal Value) (nominal Value) (nominal Value) (nominal Value (nominal Value) (nominal Value) (nominal Value) (nominal Value) (nominal Value) (nominal Value (nominal Value) (nominal Value"}, {"heading": "A.3 Proof of Convergence of the Policy Gradient Algorithm", "text": "In this section, we demonstrate the convergence of our political process algorithm (algorithm 1) Theorem 2 (acceptance 2) (acceptance 2) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (truth) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (truth) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (acceptance 3) (truth) (acceptance 3) (acceptance 3) (acceptance 3) (truth) (acceptance 3) (acceptance) (acceptance 3) (acceptance) (acceptance 3) (acceptance) (acceptance) (acceptance 3) (acceptance) (acceptance) (acceptance 3) (acceptance) (acceptance) (acceptance 3) (acceptance) (acceptance 3) (acceptance) (acceptance 3) (acceptance) (acceptance 3) (acceptance 3) (acceptance) (acceptance 3) (acceptance) (3) (3) (3) (3) (3) (3) (3) (3)) (3) (3) (3) (3)) (3) (3)) (3) (3) (3) (3)) (3) (3) (3) (3)) (3) (3)) (3) (3)) (3) (3)) (3) (3) (3) (3)) (3) (3)) (3) (3) (3) (3) (3)) (3) (3)) (3) (3) (3) (3) (3) (3)) (3) (3) (3) (3) (3) (3) (3) (3) (3) (3)) (3) (3) (3) (3) (3) (3) (3) (3)"}, {"heading": "B Technical Details of the Actor-Critic Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Assumptions", "text": "We make the following assumptions for proof of our actor-critic algorithms: (B1) For each state-action pair (x, s, a) on the extended MDP scale, the extended Markov chain induced by any policy is continuously distinguishable (a | x, s; \u03b8) is a Lipschitz function in any a \u00b2 A, x \u00b2 X, and s \u00b2 R. (B2) The extended Markov chain induced by any policy is irreducible and aperiotic. (B3) The basic functions (i) \u00b2 i = 1 are linear independent. In particular, the extended Markov chain is fully rank.4 In addition, for each v \u00b2 Rauf2, v 6 = e, where e is the n-dimensional vector with all entries. (B4) The basis for each (x \u00b2, s \u00b2, s \u00b2, a \u00b2 n \u00b2, and \u00b2 is complete conversion."}, {"heading": "B.2 Gradient with Respect to \u03bb (Proof of Lemma 1)", "text": "The proof: Take the course of V (x0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,"}, {"heading": "B.4 Convergence of the Actor Critic Algorithms", "text": "In this section we will derive the following convergence results: Theorem 6 = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K (K = K = K = K = K = K = K = K (K = K = K = K = K = K = K (K = K = K = K = K = K = K = K = K = K (K = K = K = K = K = K = K = K = K = K (K = K = K))."}, {"heading": "B.4.1 Proof of Theorem 6: TD(0) Critic Update (v\u2212update)", "text": "The step length conditions indicate that {vk} converges in a faster time scale than {vk}, {vk} and {vk}, it can be assumed that (vk, sp) in the v \u2212 update is a fixed value. The critic's update can be rewritten as follows: vk + 1 = vk + 4 (k) \u03c6 (xk, sk) \u03b4k (vk, ak).is known as the time difference (TD).DefineA = \u00b2 y, a \u00b2, s \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, c + 1) v + C (xk, ak).is known as the time difference (TD). DefineA = \u00b2 y, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, c + 1), c \u00b2, c \u00b2, c), c \u00b2, c), c \u00b2, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c)."}, {"heading": "B.4.2 Proof of Theorem 7", "text": "Step 1 (Convergence of v \u2212 update) The proof of the critical parameters convergence follows directly from Theorem 6.Step 2 (Convergence of SPSA based on setpoint \u2212 update) In this section we present the setpoints \u2212 update for the incremental critical method. This update is based on the SPSA fault method. The idea of this method is to estimate the sub-fault, which is certain to disappear. SPSA-based estimation for a sub-gradient g (setpoint) is given with two simulated value functions corresponding to setpoint \u2212 and setpoint + = sub-fault (setpoint) is a positive random fault that disappears asymptotically. SPSA-based estimation for a sub-gradient g (setpoint) is given by: g (setpoint)."}, {"heading": "C Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Problem Setup and Parameters", "text": "The problem of buying a house can be reformulated as follows: E [DTB (x0)] is subject to CVaR\u03b1 (DTB (x0) \u2264 \u03b2. (76), where DTB (x0) = \u2211 Tk = 0 \u03b3 k (1 {uk = 1} ck + 1 {uk = 0} ph) | x0 = x, \u00b5. We define the parameters of the MDP as follows: x0 = [1; 0], ph = 0.1, T = 20, \u03b3 = 0.95, fu = 1.5, fd = 0.8 and p = 0.65. For the risky political gradient algorithm, the step length is given as follows: 1 (i) = 0.1i, 2 (i) = 0.05 i0.8, 3 (i) = 0.01 i0.55, 5 (i) = 1 i. The CVaR parameter and the restriction threshold are given by \u03b1 = 0.9 and \u03b2 = 1.9."}, {"heading": "C.2 Trajectory Based Algorithms", "text": "In this section, we have implemented the following trajectory algorithms: 1. PG: This is a strategy gradient algorithm that minimizes the expected discounted cost function without taking into account any risk criteria; 2. PG-CVaR: This is the CVaR-limited simulated strategy gradient algorithm found in Section 4.It is well known that a near optimal strategy function was achieved using the LSPI algorithm with 2-dimensional radial base function (RBF); we will also implement the 2-dimensional RBF function and consider the Family Boltzmann strategies for political parameterization phases."}, {"heading": "D Bellman Equation and Projected Bellman Equation for Expected Utility Function", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Bellman Operator for Expected Utility Functions", "text": "First, we want the Bellman equation for the objective functionE [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] [DTB] (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB)) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB)) (DTB) (DTB) (DTB), DTB (DTB) (DTB) (DTB), DTB (DTB) (DTB), DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB), DTB) (DTB) (DTB) (DTB"}, {"heading": "D.2 The Projected Bellman Operator", "text": "Consider the v \u2212 dependent linear value adjustment of the V-forecast (x, s). Our goal is to estimate the v \u2212 dependent value adjustment of the V-forecast of the V-forecast (x, s). Consider the v-dependent linear value adjustment of the V-forecast (x, s). Consider the v \u2212 dependent value adjustment of the V-forecast (x, s). Consider the v \u2212 dependent value adjustment of the V-forecast (x, s). Consider the v \u2212 dependent value adjustment of the V-forecast (x, s). Consider the v-dependent value adjustment of the V-forecast (x, s). Consider the v \u2212 dependent value adjustment of the V-forecast (v). Consider the v \u2212 dependent value adjustment of the V-forecast (V). Consider the v \u2212 dependent value adjustment of the V-forecast (V)."}, {"heading": "E Supplementary: Gradient with Respect to \u03b8", "text": "Considering the course of V (1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1, s1"}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.", "creator": "LaTeX with hyperref package"}}}