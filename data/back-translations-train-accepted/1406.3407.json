{"id": "1406.3407", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2014", "title": "Restricted Boltzmann Machine for Classification with Hierarchical Correlated Prior", "abstract": "Restricted Boltzmann machines (RBM) and its variants have become hot research topics recently, and widely applied to many classification problems, such as character recognition and document categorization. Often, classification RBM ignores the interclass relationship or prior knowledge of sharing information among classes. In this paper, we are interested in RBM with the hierarchical prior over classes. We assume parameters for nearby nodes are correlated in the hierarchical tree, and further the parameters at each node of the tree be orthogonal to those at its ancestors. We propose a hierarchical correlated RBM for classification problem, which generalizes the classification RBM with sharing information among different classes. In order to reduce the redundancy between node parameters in the hierarchy, we also introduce orthogonal restrictions to our objective function. We test our method on challenge datasets, and show promising results compared to competitive baselines.", "histories": [["v1", "Fri, 13 Jun 2014 02:19:26 GMT  (760kb,D)", "https://arxiv.org/abs/1406.3407v1", "13 pages, 5 figures"], ["v2", "Mon, 20 Apr 2015 18:39:18 GMT  (583kb,D)", "http://arxiv.org/abs/1406.3407v2", "13 pages, 5 figures"]], "COMMENTS": "13 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen", "sargur h srihari"], "accepted": true, "id": "1406.3407"}, "pdf": {"name": "1406.3407.pdf", "metadata": {"source": "CRF", "title": "CATION WITH HIERARCHICAL CORRELATED PRIOR", "authors": ["Gang Chen", "Sargur N. Srihari"], "emails": ["gangchen@buffalo.edu,", "srihari@cedar.buffalo.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able to assert ourselves, that we are able to put ourselves at the top, \"he said."}, {"heading": "2 CLASSIFICATION RESTRICTED BOLTZMANN MACHINE WITH HIERARCHICAL CORRELATED PRIOR", "text": "Throughout the work, matrix variables are labeled with bold capital letters, and vector sizes are written in bold lowercase letters. In Matrix W, we specify their i-th line and j-th column element as Wij, their i-th row vector Wi. and their j-th column vector W.j. For different matrices, we use different subscriptions to distinguish them. For example, A12 and A21 are different matrices characterized by different subscriptions."}, {"heading": "2.1 CLASSIFICATION RESTRICTED BOLTZMANN MACHINE", "text": "Suppose we have a training set D = {(xi, yi)} that exists for the i-th pair: an input vector xi-X and a target class yi-Y, where xi-Rd and yi-Y, where xi-Rd and yi-II, {1,..., K}. An RBM with n hidden units is a parametric model of the common distribution between a layer of hidden variables h = (h1,..., hn) and the observations x = (x1,..., xd) and y.The classification RBM was first proposed in Hinton (2007) and was further developed in Larochelle & Bengio (2008); Larochelle et al. (2012) with discriminatory training model. The common probability of classification RBM takes the following form: p (y, x, h) that the x-yj class (y, h)."}, {"heading": "2.2 RESTRICTED BOLTZMANN MACHINE WITH HIERARCHICAL PRIOR", "text": "D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D"}, {"heading": "2.3 ALGORITHM", "text": "In other words, we can update all parameters with CD, except U. Since U is the function of A, we can calculate the derivative of U w.r.t. A and update A with gradient descent. After we get A, we can calculate U, which can be used in the next iteration. We list the pseudo code below in Alg. 1.Algorithm 1 Learning RBM with hierarchical correlation prior Input: Training data D = {(xi, yi)}, the number of hidden nodes n, learning rate \u03b7, C and the maximum epoch T Output: \u044b = {W, b, c, d, U} 1: Initialize the parameters W, b, c, d, U; 2: Divide the training data in stacks; 3: for t = 1 to T do 4: for each stack Update 5: Use 1-step Gibbs to update the update."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "In fact, most people are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "4 RELATED WORK", "text": "The hierarchical structure is organized according to the similarity of classes. Two classes are considered to be similar when it is difficult to distinguish one from the other on the basis of their representation.The similarity of classes increases as we descend the hierarchy. Thus, the hierarchical approach before over3http: / / people.csail.mit.edu / jrennie / 20Newsgroups / 4http: / / www.cs.toronto.edu / \u02dc larocheh / public / datasets / 20newsgroups _ train, valid, test} _ binary _ 5000 _ voc.tkategories provides semantic meaning and valuable information between different classes; and therefore it may to some extent support classification problems in the hands of Shahbaba & Neal (2007); Xiao et al al al al. (2011); Rohit et al al al. (2013) Much work has been done in recent years to use hierarchical labels for classification."}, {"heading": "5 CONCLUSION", "text": "We look at restricted Boltzmann machines (RBM) for classification problems, with prior knowledge of information exchange between classes in a hierarchy. Essentially, our model breaks down the classification of RBM into traditional RBM for representation learning and a multi-level logistical model for classification, and then introduces a hierarchical primacy model over a multi-level logistics model. To reduce redundancy between node parameters, we also introduce orthogonal constraints in our target function. To the best of our knowledge, this is the first work to include hierarchical primacy over the RBM framework for classification. We test our method on challenging data sets and show promising results compared to benchmarks."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Hierarchical document categorization with support vector machines", "author": ["Cai", "Lijuan", "Hofmann", "Thomas"], "venue": "In Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Cai et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2004}, {"title": "Hierarchical classification: Combining bayes with svm", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Gentile", "Claudio", "Zaniboni", "Luca"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "Large margin hierarchical classification", "author": ["Dekel", "Ofer", "Keshet", "Joseph", "Singer", "Yoram"], "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning,", "citeRegEx": "Dekel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2004}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "jia Li", "Li", "Kai", "Fei-fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Hierarchical classification of web content", "author": ["Dumais", "Susan", "Chen", "Hao"], "venue": "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Dumais et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 2000}, {"title": "ed.). WordNet An Electronic Lexical Database", "author": ["Fellbaum", "Christiane"], "venue": null, "citeRegEx": "Fellbaum and Christiane,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and Christiane", "year": 1998}, {"title": "Classes for fast maximum entropy training", "author": ["Goodman", "Joshua"], "venue": null, "citeRegEx": "Goodman and Joshua.,? \\Q2001\\E", "shortCiteRegEx": "Goodman and Joshua.", "year": 2001}, {"title": "Bayesian models for large-scale hierarchical classification", "author": ["Gopal", "Siddharth", "Yang", "Yiming", "Bai", "Bing", "Niculescu-Mizil", "Alexandru"], "venue": null, "citeRegEx": "Gopal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gopal et al\\.", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G E Hinton", "Salakhutdinov", "R R"], "venue": "Science,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Comput.,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "Learning multiple layers of representation", "author": ["Hinton", "Geoffrey E"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "Hinton and E.,? \\Q2007\\E", "shortCiteRegEx": "Hinton and E.", "year": 2007}, {"title": "Hierarchically classifying documents using very few words", "author": ["Koller", "Daphne", "Sahami", "Mehran"], "venue": "In Proceedings of the Fourteenth International Conference on Machine Learning,", "citeRegEx": "Koller et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Koller et al\\.", "year": 1997}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["Larochelle", "Hugo", "Mandel", "Michael", "Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "Semantic hierarchies for visual object recognition", "author": ["Marszalek", "Marcin", "Schmid", "Cordelia"], "venue": "In CVPR. IEEE Computer Society,", "citeRegEx": "Marszalek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Marszalek et al\\.", "year": 2007}, {"title": "Improving text classification by shrinkage in a hierarchy of classes", "author": ["McCallum", "Andrew", "Rosenfeld", "Ronald", "Mitchell", "Tom M", "Ng", "Andrew Y"], "venue": "In Proceedings of the Fifteenth International Conference on Machine Learning,", "citeRegEx": "McCallum et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 1998}, {"title": "On flat versus hierarchical classification in large-scale taxonomies", "author": ["Rohit", "Babbar", "Ioannis", "Partalas", "Eric", "Gaussier", "Massih-Reza", "Amini"], "venue": "In Neural Information Processsing Systems (NIPS),", "citeRegEx": "Rohit et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohit et al\\.", "year": 2013}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Improving classification when a class hierarchy is available using a hierarchy-based prior", "author": ["Shahbaba", "Babak", "Neal", "Radford M"], "venue": "Bayesian Analysis,", "citeRegEx": "Shahbaba et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shahbaba et al\\.", "year": 2007}, {"title": "Exploiting hierarchy in text categorization", "author": ["Weigend", "Andreas S", "Wiener", "Erik D", "Pedersen", "Jan O"], "venue": "Inf. Retr.,", "citeRegEx": "Weigend et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Weigend et al\\.", "year": 1999}, {"title": "Hierarchical classification via orthogonal transfer", "author": ["Xiao", "Lin", "Zhou", "Dengyong", "Wu", "Mingrui"], "venue": "In ICML, pp", "citeRegEx": "Xiao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "They have attracted significant attention recently on many machine learning problems, such as dimension reduction Hinton & Salakhutdinov (2006), text categorization Larochelle et al. (2012), collaborative filtering Salakhutdinov et al.", "startOffset": 165, "endOffset": 190}, {"referenceID": 10, "context": "They have attracted significant attention recently on many machine learning problems, such as dimension reduction Hinton & Salakhutdinov (2006), text categorization Larochelle et al. (2012), collaborative filtering Salakhutdinov et al. (2007) and object recognition Krizhevsky et al.", "startOffset": 165, "endOffset": 243}, {"referenceID": 10, "context": "(2007) and object recognition Krizhevsky et al. (2012). A recent survey Bengio et al.", "startOffset": 30, "endOffset": 55}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us.", "startOffset": 16, "endOffset": 37}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al.", "startOffset": 16, "endOffset": 323}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al.", "startOffset": 16, "endOffset": 346}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al.", "startOffset": 16, "endOffset": 362}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization.", "startOffset": 16, "endOffset": 383}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al.", "startOffset": 16, "endOffset": 874}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively.", "startOffset": 16, "endOffset": 906}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al.", "startOffset": 16, "endOffset": 1068}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al. (2012) treats the category structure as flat and little work has been done to explore the interclass relationship.", "startOffset": 16, "endOffset": 1094}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al. (2012) treats the category structure as flat and little work has been done to explore the interclass relationship. In this paper, we generalize RBM with hierarchical prior for classification problems. Basically, we divide the classification RBM into traditional RBM for representation learning and multinomial logit model for classification, see Fig. 1(a) for intuitive understanding. For the traditional RBM (red in Fig. 1(a)), we can extend it into deep belief network (DBN), while for the multinomial logit model (green in Fig. 1(a)), we can incorporate the interclass relationship to it. In this work, we focus on the hierarchical prior over the classification RBM, and we take a similar strategy as corrMNL, that means we use sums of parameters along paths from root to a specific leaf in the tree as model parameters for hierarchical classification. However, we consider it in a rather different way from the previous work. We can think our method is a kind of mixture of corrMNL Shahbaba & Neal (2007) and the orthogonal SVM model Xiao et al.", "startOffset": 16, "endOffset": 2096}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al. (2012) treats the category structure as flat and little work has been done to explore the interclass relationship. In this paper, we generalize RBM with hierarchical prior for classification problems. Basically, we divide the classification RBM into traditional RBM for representation learning and multinomial logit model for classification, see Fig. 1(a) for intuitive understanding. For the traditional RBM (red in Fig. 1(a)), we can extend it into deep belief network (DBN), while for the multinomial logit model (green in Fig. 1(a)), we can incorporate the interclass relationship to it. In this work, we focus on the hierarchical prior over the classification RBM, and we take a similar strategy as corrMNL, that means we use sums of parameters along paths from root to a specific leaf in the tree as model parameters for hierarchical classification. However, we consider it in a rather different way from the previous work. We can think our method is a kind of mixture of corrMNL Shahbaba & Neal (2007) and the orthogonal SVM model Xiao et al. (2011). However, our model inherits the advantage of", "startOffset": 16, "endOffset": 2144}, {"referenceID": 13, "context": "RBM, which can learn the hidden representation for better classification Hinton & Salakhutdinov (2006); Larochelle et al. (2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al.", "startOffset": 104, "endOffset": 129}, {"referenceID": 13, "context": "RBM, which can learn the hidden representation for better classification Hinton & Salakhutdinov (2006); Larochelle et al. (2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al.", "startOffset": 104, "endOffset": 187}, {"referenceID": 3, "context": "(2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al. (2004); Xiao et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 3, "context": "(2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al. (2004); Xiao et al. (2011). Moreover, we only have a single RBM in our model, while there are multiple SVMs in the orthogonal hierarchical SVM Xiao et al.", "startOffset": 86, "endOffset": 126}, {"referenceID": 3, "context": "(2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al. (2004); Xiao et al. (2011). Moreover, we only have a single RBM in our model, while there are multiple SVMs in the orthogonal hierarchical SVM Xiao et al. (2011). Our contributions are: (1) we introduce the hierarchical semantic prior over labels into restricted Boltzmann machine; (2) we add orthogonal constraints over adjacent layers in the hierarchy, which makes our model more robust for classification problems.", "startOffset": 86, "endOffset": 261}, {"referenceID": 14, "context": "The classification RBM was first proposed in Hinton (2007) and was further developed in Larochelle & Bengio (2008); Larochelle et al. (2012) with discriminative training model.", "startOffset": 116, "endOffset": 141}, {"referenceID": 19, "context": "As shown in Salakhutdinov et al. (2007), this conditional distribution has explicit formula and can be calculated", "startOffset": 12, "endOffset": 40}, {"referenceID": 3, "context": "As in Dekel et al. (2004), we also define the path for each node \u03bd \u2208 T , define P (\u03bd) to be the set of nodes along the path from root to v, P (\u03bd) = {\u03bc \u2208 T : \u2203i \u03bc = A(\u03bd)} (6) Now we can define the coefficient parameters for each leaf node \u03bd as", "startOffset": 6, "endOffset": 26}, {"referenceID": 19, "context": "For our model, each leaf node is associated to one class, which takes the same methodology as in Salakhutdinov et al. (2007). Fig.", "startOffset": 97, "endOffset": 125}, {"referenceID": 22, "context": "just as in Xiao et al. (2011) to reduce redundancy between adjacent layers.", "startOffset": 11, "endOffset": 30}, {"referenceID": 14, "context": "RBM or RBM for classification was first proposed in Hinton & Salakhutdinov (2006) and later was further developed in Larochelle et al. (2012). Its mathematical formula is shown in Eq.", "startOffset": 117, "endOffset": 142}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.", "startOffset": 7, "endOffset": 32}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.", "startOffset": 7, "endOffset": 73}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.8 HDRBM Larochelle et al. (2012) 23.", "startOffset": 7, "endOffset": 109}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.8 HDRBM Larochelle et al. (2012) 23.8 HRBMh (\u03b7 = 0.1, n = 2000) 30.6 HRBMs (\u03b7 = 0.1, n = 2000) 63.7 HHRBM (\u03b7 = 0.1, n = 1000, 500, 200 and 200) 32.0 Ours (\u03b7 = 0.01, n = 1500 and C = 0) 30.1 Ours (\u03b7 = 0.01, n = 1500 and C = 0.1) 23.6 MNL 30.8 corrMNL Shahbaba & Neal (2007) 79.", "startOffset": 7, "endOffset": 349}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.8 HDRBM Larochelle et al. (2012) 23.8 HRBMh (\u03b7 = 0.1, n = 2000) 30.6 HRBMs (\u03b7 = 0.1, n = 2000) 63.7 HHRBM (\u03b7 = 0.1, n = 1000, 500, 200 and 200) 32.0 Ours (\u03b7 = 0.01, n = 1500 and C = 0) 30.1 Ours (\u03b7 = 0.01, n = 1500 and C = 0.1) 23.6 MNL 30.8 corrMNL Shahbaba & Neal (2007) 79.3 SVM Larochelle et al. (2012) 32.", "startOffset": 7, "endOffset": 383}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.8 HDRBM Larochelle et al. (2012) 23.8 HRBMh (\u03b7 = 0.1, n = 2000) 30.6 HRBMs (\u03b7 = 0.1, n = 2000) 63.7 HHRBM (\u03b7 = 0.1, n = 1000, 500, 200 and 200) 32.0 Ours (\u03b7 = 0.01, n = 1500 and C = 0) 30.1 Ours (\u03b7 = 0.01, n = 1500 and C = 0.1) 23.6 MNL 30.8 corrMNL Shahbaba & Neal (2007) 79.3 SVM Larochelle et al. (2012) 32.8 NNet Larochelle et al. (2012) 28.", "startOffset": 7, "endOffset": 418}, {"referenceID": 15, "context": "categories provides semantic meaning and valuable information among different classes; and thus to some extent it can assist classification problems in hand Shahbaba & Neal (2007); Xiao et al. (2011); Rohit et al.", "startOffset": 181, "endOffset": 200}, {"referenceID": 14, "context": "(2011); Rohit et al. (2013). Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 14, "context": "(2011); Rohit et al. (2013). Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al.", "startOffset": 8, "endOffset": 208}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al.", "startOffset": 180, "endOffset": 203}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007).", "startOffset": 180, "endOffset": 226}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007).", "startOffset": 180, "endOffset": 248}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007).", "startOffset": 180, "endOffset": 270}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to leverage hierarchical prior can be categorized below.", "startOffset": 180, "endOffset": 319}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to leverage hierarchical prior can be categorized below. The first approach classifies each node recursively, by choosing the label of which the associated vector has the largest output score among its siblings till to a leaf node. An variant way is to compute the conditional probability for each class at each level, and then multiply these probabilities along every branch to compute the final assignment probability for each class. Xiao et al. introduced a hierarchical classification method with orthogonal transfer Xiao et al. (2011), which requires the parameters of children nodes are orthogonal to those of its parents as much as possible.", "startOffset": 180, "endOffset": 888}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to leverage hierarchical prior can be categorized below. The first approach classifies each node recursively, by choosing the label of which the associated vector has the largest output score among its siblings till to a leaf node. An variant way is to compute the conditional probability for each class at each level, and then multiply these probabilities along every branch to compute the final assignment probability for each class. Xiao et al. introduced a hierarchical classification method with orthogonal transfer Xiao et al. (2011), which requires the parameters of children nodes are orthogonal to those of its parents as much as possible. Another example is the nested multinomial logit model Shahbaba & Neal (2007), in which the nested classification model for each node is statistically independent, conditioned on its parent in the upper levels.", "startOffset": 180, "endOffset": 1074}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to leverage hierarchical prior can be categorized below. The first approach classifies each node recursively, by choosing the label of which the associated vector has the largest output score among its siblings till to a leaf node. An variant way is to compute the conditional probability for each class at each level, and then multiply these probabilities along every branch to compute the final assignment probability for each class. Xiao et al. introduced a hierarchical classification method with orthogonal transfer Xiao et al. (2011), which requires the parameters of children nodes are orthogonal to those of its parents as much as possible. Another example is the nested multinomial logit model Shahbaba & Neal (2007), in which the nested classification model for each node is statistically independent, conditioned on its parent in the upper levels. One weakness of this strategy for hierarchical classification is that errors will propagate from parents to children, if any misclassification happened in the top level. The other methodology for hierarchical classification prefers to use the sum of parameters along the tree for classifying cases ended at leaf nodes. Cai and Hoffmann Cai & Hofmann (2004) proposed a hierarchical larger margin multi-class SVM with tree-induced loss functions.", "startOffset": 180, "endOffset": 1564}, {"referenceID": 2, "context": "Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hierarchical classification.", "startOffset": 11, "endOffset": 47}, {"referenceID": 2, "context": "Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hierarchical classification.", "startOffset": 11, "endOffset": 77}, {"referenceID": 2, "context": "Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hierarchical classification. Recently, Shahbaba et al. proposed a correlated multinomial logit model (corrMNL) Shahbaba & Neal (2007), whose regression coefficients for each leaf node are represented by the sum of parameters on all the branches leading to that class.", "startOffset": 11, "endOffset": 238}, {"referenceID": 2, "context": "Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hierarchical classification. Recently, Shahbaba et al. proposed a correlated multinomial logit model (corrMNL) Shahbaba & Neal (2007), whose regression coefficients for each leaf node are represented by the sum of parameters on all the branches leading to that class. Apart from the two approaches mentioned above, there are also other methods proposed in the past. Dumais and Chen trained different classifiers kind of layer by layer by exploring the hierarchical structure Dumais & Chen (2000). Cesa-Bianchi et al.", "startOffset": 11, "endOffset": 600}, {"referenceID": 2, "context": "Cesa-Bianchi et al. combined Bayesian inference with the probabilities output from SVM classifiers in Cesa-Bianchi et al. (2006) for hierarchical classification.", "startOffset": 0, "endOffset": 129}, {"referenceID": 2, "context": "Cesa-Bianchi et al. combined Bayesian inference with the probabilities output from SVM classifiers in Cesa-Bianchi et al. (2006) for hierarchical classification. Similarly, Gopal et al. Gopal et al. (2012) used Bayesian approach (with variational inference) with hierarchical prior for classification problems.", "startOffset": 0, "endOffset": 206}], "year": 2015, "abstractText": "Restricted Boltzmann machines (RBM) and its variants have been widely used on classification problems. In a sense, its success of RBM should be attributed to its strong representation power with hidden variables. Often, classification RBM ignores the interclass relationship or prior knowledge of sharing information among classes. In this paper, we propose a RBM with hierarchical prior for classification problem, by generalizing the classification RBM with sharing information among different classes. Basically, we assume the hierarchical prior over classes, where parameters for nearby nodes are correlated in the hierarchical tree, and further the parameters at each node of the tree to be orthogonal to those at its ancestors. Through the hierarchical prior, our model improves the information sharing between different classes and reduce the redundancy for robust classification. We test our method on several datasets, and show promising results compared to competitive baselines.", "creator": "LaTeX with hyperref package"}}}