{"id": "1406.3650", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2014", "title": "Smoothed Gradients for Stochastic Variational Inference", "abstract": "The field of statistical machine learning has seen a rapid progress in complex hierarchical Bayesian models. In Stochastic Variational Inference (SVI), the inference problem is mapped to an optimization problem involving stochastic gradients. While this scheme was shown to scale up to massive data sets, the intrinsic noise of the stochastic gradients impedes a fast convergence. Inspired by gradient averaging methods from stochastic optimization, we propose a variance reduction scheme tailored to SVI by averaging successively over the sufficient statistics of the local variational parameters. Its simplicity comes at the cost of biased stochastic gradients. We show that we can eliminate large parts of the bias while obtaining the same variance reduction as in simple gradient averaging schemes. We explore the tradeoff between variance and bias based on the example of Latent Dirichlet Allocation.", "histories": [["v1", "Fri, 13 Jun 2014 21:19:09 GMT  (487kb,D)", "https://arxiv.org/abs/1406.3650v1", "8 pages, 6 figures"], ["v2", "Tue, 18 Nov 2014 03:12:37 GMT  (812kb,D)", "http://arxiv.org/abs/1406.3650v2", "Appears in Neural Information Processing Systems, 2014"]], "COMMENTS": "8 pages, 6 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["stephan mandt", "david m blei"], "accepted": true, "id": "1406.3650"}, "pdf": {"name": "1406.3650.pdf", "metadata": {"source": "CRF", "title": "Smoothed Gradients for Stochastic Variational Inference", "authors": ["Stephan Mandt", "David Blei"], "emails": ["smandt@princeton.edu", "david.blei@columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that it is a way and a way, in which it is about the formation of concepts, which is expressed in the way, how they are expressed in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way, how they are in the way they are in the way, how they are in the way they are in the way, how they are in the way they are in the way, how they are in the way they are in the way, how they are in the way they are in the way, how they are in the way they are in the way, how they are in the way they are in the way, how they are in the way they are in the way they are in the way and how they are in the way, how they are in the way they are in the way they are in the way, how they are in the way in the way they are in the way they are in the way, how they are in the way they are in the way they are in the way and they are in the way they are in the way, how they are in the way they are in the way in the way they are in the way in the way and they are in the way they are in the way in the way in the way and they are in the way in the way in the way in the way in the way and they are in the way in the way in the way they are in the way in the way in the way in the way in the way in the way and they in the way in the way in the way in the way in the way"}, {"heading": "2 Smoothed stochastic gradients for SVI", "text": "We begin by testing stochastic variation conclusions for LDA (1, 14), a theme model that will be our current example. We define a multinomic parameter \u03b21: V, 1: K. Each document d is associated with a normalized vector of theme weights. In addition, each word in document d has a theme mapping zdn. This is a vector of binary entries \u03b21, so that zkdn = 1 when the word n in document d is mapped to the theme k, and z k = 0 otherwise.In the generative process, we first draw the themes from a didactic, \u03b2k = Dir."}, {"heading": "3 Empirical study", "text": "We tested SVI for LDA by using the smoothed stochastic gradients on three large corpora (\u03b2 = York): \u2022 882K scientific abstracts from the Arxiv repository by using a vocabulary of 14K words. \u2022 1.7M article from the New York Times by using a vocabulary of 8K words. \u2022 3.6M article from Wikipedia by using a vocabulary of 7.7K words. \u2022 We set the minibatch size to B = 300 and also the number of topics to K = 100 and the hyperparameters \u03b1 = \u03b7 = 0.5. We set the learning rate to \u03c1 = 10 \u2212 3. We compared our results with a decreasing learning value and found the same behavior. For a quantitative test of model suitability, we evaluate the predictive probability of the vocabulary. [1] For this purpose, we separate a test for ihihk from the training set."}, {"heading": "4 Discussion and Conclusions", "text": "In order to reduce the mean square error relative to the full gradient, we have gradually averaged the sufficient SVI statistics over L iteration steps. However, the resulting smoothed gradient is biased, and the performance of the method is determined by the competition between bias and variance. We argued theoretically and empirically that intermediate values of the number of stored sufficient statistics L have the highest probabilities. Convergence of our algorithm is still an open problem, especially because the variation target is not convex. However, to guarantee convergence, we can simply phase out our algorithm and reduce the number of stored gradients to one the closer we get to convergence. At this point, we recover SVI. Recognition values We thank Laurent Charlin, Alp Kucukelbir, Prem Gopolan, Rajespenang Klojath P."}], "references": [{"title": "Stochastic variational inference", "author": ["Matthew D Hoffman", "David M Blei", "Chong Wang", "John Paisley"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Scalable recommendation with Poisson factorization", "author": ["Prem Gopalan", "Jake M Hofman", "David M Blei"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Efficient discovery of overlapping communities in massive networks", "author": ["Prem K Gopalan", "David M Blei"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Mixed membership stochastic blockmodels", "author": ["Edoardo M Airoldi", "David M Blei", "Stephen E Fienberg", "Eric P Xing"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Gaussian processes for big data", "author": ["James Hensman", "Nicolo Fusi", "Neil D Lawrence"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1951}, {"title": "Variance reduction for stochastic gradient optimization", "author": ["Chong Wang", "Xi Chen", "Alex Smola", "Eric Xing"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "Technical report, HAL 00860051,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Yurii Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Online model selection based on the variational Bayes", "author": ["Masa-Aki Sato"], "venue": "Neural Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Larger residuals, less work: Active document scheduling for latent Dirichlet allocation", "author": ["Mirwaes Wahabzada", "Kristian Kersting"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "An incremental gradient (-projection) method with momentum term and adaptive stepsize rule", "author": ["Paul Tseng"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Online learning for latent Dirichlet allocation", "author": ["Matthew Hoffman", "Francis R Bach", "David M Blei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Graphical models, exponential families, and variational inference", "author": ["Martin J Wainwright", "Michael I Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Pattern Recognition and Machine Learning, volume 1", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Latent Dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "SVI has been applied to many types of models, including topic models [1], probabilistic factorization [2], statistical network analysis [3, 4], and Gaussian processes [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "SVI has been applied to many types of models, including topic models [1], probabilistic factorization [2], statistical network analysis [3, 4], and Gaussian processes [5].", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "SVI has been applied to many types of models, including topic models [1], probabilistic factorization [2], statistical network analysis [3, 4], and Gaussian processes [5].", "startOffset": 136, "endOffset": 142}, {"referenceID": 3, "context": "SVI has been applied to many types of models, including topic models [1], probabilistic factorization [2], statistical network analysis [3, 4], and Gaussian processes [5].", "startOffset": 136, "endOffset": 142}, {"referenceID": 4, "context": "SVI has been applied to many types of models, including topic models [1], probabilistic factorization [2], statistical network analysis [3, 4], and Gaussian processes [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 5, "context": "SVI uses stochastic optimization [6] to fit a variational distribution, following easy-to-compute noisy natural gradients that come from repeatedly subsampling from the large data set.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "This is necessary for the conditions of [6] to apply, and guarantees that SVI climbs to a local optimum of the variational objective.", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "Innovations on SVI, such as subsampling from data non-uniformly [2] or using control variates [7, 8], have maintained the unbiasedness of the noisy gradient.", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Innovations on SVI, such as subsampling from data non-uniformly [2] or using control variates [7, 8], have maintained the unbiasedness of the noisy gradient.", "startOffset": 94, "endOffset": 100}, {"referenceID": 7, "context": "Innovations on SVI, such as subsampling from data non-uniformly [2] or using control variates [7, 8], have maintained the unbiasedness of the noisy gradient.", "startOffset": 94, "endOffset": 100}, {"referenceID": 8, "context": "For example, stochastic averaged gradients (SAG) iteratively updates only a subset of terms in the full gradient [9]; averaged gradients (AG) follows the average of the sequence of stochastic gradients [10].", "startOffset": 113, "endOffset": 116}, {"referenceID": 9, "context": "For example, stochastic averaged gradients (SAG) iteratively updates only a subset of terms in the full gradient [9]; averaged gradients (AG) follows the average of the sequence of stochastic gradients [10].", "startOffset": 202, "endOffset": 206}, {"referenceID": 10, "context": "This is an unbiased noisy gradient [11, 1], and we follow it with a step size \u03c1i that decreases across iterations [6].", "startOffset": 35, "endOffset": 42}, {"referenceID": 0, "context": "This is an unbiased noisy gradient [11, 1], and we follow it with a step size \u03c1i that decreases across iterations [6].", "startOffset": 35, "endOffset": 42}, {"referenceID": 5, "context": "This is an unbiased noisy gradient [11, 1], and we follow it with a step size \u03c1i that decreases across iterations [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "[8] and Ref.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] introduce control variates to reduce the gradient\u2019s variance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] develops a method to pre-select documents according to their influence on the global update.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In the stochastic optimization literature, we have already discussed SAG [9] and AG [10].", "startOffset": 73, "endOffset": 76}, {"referenceID": 9, "context": "In the stochastic optimization literature, we have already discussed SAG [9] and AG [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "[13] introduces an exponentially fading momentum term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Latent Dirichlet Allocation and Variational Inference We start by reviewing stochastic variational inference for LDA [1, 14], a topic model that will be our running example.", "startOffset": 117, "endOffset": 124}, {"referenceID": 13, "context": "Latent Dirichlet Allocation and Variational Inference We start by reviewing stochastic variational inference for LDA [1, 14], a topic model that will be our running example.", "startOffset": 117, "endOffset": 124}, {"referenceID": 0, "context": "Following [1], the topics \u03b2 are global parameters, shared among all documents.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "In variational inference [15], we approximate the posterior distribution,", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "The parameters \u03bb, \u03b3 and \u03c6minimize the Kullback-Leibler (KL) divergence between the variational distribution and the posterior [16].", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "[1, 17], the objective to maximize is the evidence lower bound (ELBO), L(q) = Eq[log p(x, \u03b2,\u0398, z)]\u2212 Eq[log q(\u03b2,\u0398, z)].", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[1, 17], the objective to maximize is the evidence lower bound (ELBO), L(q) = Eq[log p(x, \u03b2,\u0398, z)]\u2212 Eq[log q(\u03b2,\u0398, z)].", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "The local parameters are updated as described in [1, 17] .", "startOffset": 49, "endOffset": 56}, {"referenceID": 16, "context": "The local parameters are updated as described in [1, 17] .", "startOffset": 49, "endOffset": 56}, {"referenceID": 0, "context": "In Stochastic Variational Inference for LDA [1, 14], it is approximated by stochastically sampling a \u201dminibatch\u201d Bi \u2282 {1, .", "startOffset": 44, "endOffset": 51}, {"referenceID": 13, "context": "In Stochastic Variational Inference for LDA [1, 14], it is approximated by stochastically sampling a \u201dminibatch\u201d Bi \u2282 {1, .", "startOffset": 44, "endOffset": 51}, {"referenceID": 0, "context": "For arbitrary \u03c1, this update is just stochastic gradient ascent, as a stochastic estimate of the natural gradient of the ELBO [1] is \u011d(\u03bbi, Bi) = (\u03b7 \u2212 \u03bbi) + \u015c(\u03bbi, Bi), (9) This interpretation opens the world of gradient smoothing techniques.", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "For a quantitative test of model fitness, we evaluate the predictive probability over the vocabulary [1].", "startOffset": 101, "endOffset": 104}], "year": 2014, "abstractText": "Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.", "creator": "LaTeX with hyperref package"}}}