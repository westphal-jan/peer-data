{"id": "1406.3676", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2014", "title": "Question Answering with Subgraph Embeddings", "abstract": "This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields state-of-the-art results on a competitive benchmark of the literature.", "histories": [["v1", "Sat, 14 Jun 2014 03:00:23 GMT  (24kb)", "http://arxiv.org/abs/1406.3676v1", null], ["v2", "Wed, 3 Sep 2014 01:02:11 GMT  (152kb,D)", "http://arxiv.org/abs/1406.3676v2", null], ["v3", "Thu, 4 Sep 2014 00:25:35 GMT  (152kb,D)", "http://arxiv.org/abs/1406.3676v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["antoine bordes", "sumit chopra", "jason weston"], "accepted": true, "id": "1406.3676"}, "pdf": {"name": "1406.3676.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Antoine Bordes", "Sumit Chopra", "Jason Weston"], "emails": ["abordes@fb.com", "spchopra@fb.com", "jase@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.36 76v1 [cs.CL] 1 4Ju n20 14"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Task Definition", "text": "The fact is that we will be able to find ourselves in the position we are in."}, {"heading": "3 Embedding Questions and Answers", "text": "The question we ask is not only a question and an answer to a question, but also an answer to a question and a candidate. Learning embeddings is achieved by learning a scoring function S (q, a), so that S generates a high score if the correct answer to the question is q, and a low score otherwise. Note that both q and a are presented as a combination of embeddings of their individual words and / or symbols; therefore learning S essentially involves learning these embeddings. In our model, the form of the scoring function is: S (q, a) = f (q) g (a). Let W be a matrix of Rk \u00d7 N, where k is the dimension of the embedspace, and N is the dictionary we learn."}, {"heading": "3.1 Training and Loss Function", "text": "As in [12], we train our model with a margin-based ranking loss function. Let D = (qi, ai): i = 1,.., | D |} be the training set of questions qi paired with their correct answer ai. The loss function that we minimize is | D | \u2211 i = 1 \u2211 a \u0441A (ai) max {0, m \u2212 S (qi, ai) + S (qi, a)}, (2) where m is the margin (fixed at 0.1). Minimizing equation. (2) learns the embedding of the matrix W, so that the value of a question paired with a correct answer is greater than any wrong answer a par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par. This is achieved by sampling 50% of the time from the totality of the units associated with the totality of the question (i.e. other candidate pairs) and replacing the answer with a random unit."}, {"heading": "3.2 Multitask Training of Embeddings", "text": "Since a large number of questions in our training datasets are synthetically generated, they do not sufficiently cover the range of syntax used in natural language. Therefore, we are also working on the formation of our model multitask with the task of paraphrase prediction. We do this by alternating the formation of S with that of a scoring function Sprp (q1, q2) = f (q1) f (q2), which uses the same embedding matrix W and making the embedding of a question pair (q1, q2) similar to each other when they are paraphrases (i.e., if they belong to the same paraphrase cluster), and otherwise distinguishing them from each other. Training Sprp is similar to that of S, except that negative samples are obtained by sampling a question from another paraphrase cluster. We also multitask the formation of the embedding with the matching of the centers of freebase units to the actual words of their names, so that the embedding should be (the embedding of the word)."}, {"heading": "3.3 Inference", "text": "Once W is trained at the test date for a given question q, the model predicts the answer with the following result: A = argmaxa \"A (q) S (q, a\") (3), where A (q) is the candidate's answer. This candidate set could be the whole KB, but this has both speed and potential precision problems. Instead, we create a candidate set A (q) for each question. We remember that each question contains an identified freebase unit. A (q) is initially populated with all triples from the freebase in which this unit is involved, which allows us to answer simple factual questions whose meaning can be represented by a single tripel. This simple strategy is called C1. Since a system that can answer only such questions would be limited, we add A (q) with examples located in the KB chart on 2 hops from the unit of the question. We do not add all such examples, as this will result in a very large group of candidates as we would select in a subset of q instead."}, {"heading": "4 Experiments", "text": "We compare our system in accuracy (percentage of questions with a correct answer returned by Equation (3) on the WebQuestions test set) with the original model proposed for this dataset in [1] and other recently published systems.2The upper part of Table 3 shows that our approach is just as powerful as the current state of the art of [2] and better than [13], [1] and [6]. [13] Report results that exceed us, but this was achieved with a very large additional amount of training data of 1 billion triples extracted from ClueWeb (to be compared with our 2M).The lower part of Table 3 compares different versions of our model. Our standard approach uses the subgraph representation for answers and C2 as the candidate response set. Replacing C2 with C1 results in a performance drop of more than 5%, as many questions have no answers that are simple triples (not in C1), as the number of candidates is also greater than the number of candidates on the Hop 1 (however)."}, {"heading": "5 Conclusion", "text": "In this paper, an embedding model was presented that learns to perform open quality criteria on the basis of training data that consists of questions paired with their answers and a KB to provide a structure between the answers, and that is able to provide current performance on the competitive benchmark WebQuestions.2 The results of the baselines except [4] were extracted from the original theses. For our experiments, all hyperparameters on the WebQuestions validation set were selected: k was selected from {64, 128, 256}, the learning rate on a scale between 10 \u2212 4 and 10 \u2212 1 and we used a maximum of 100 paths in the subgraph representation."}], "references": [{"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Semantic parsing via paraphrasing", "author": ["J. Berant", "P. Liang"], "venue": "In To appear in Proceedings of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Open question answering with weakly supervised embedding models", "author": ["A. Bordes", "J. Weston", "N. Usunier"], "venue": "arXiv preprint arXiv:1404.4326,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Paraphrase-driven learning for open question answering", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In To appear in Proceedings of KDD,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A survey on question answering technology from an information retrieval perspective", "author": ["O. Kolomiyets", "M.-F. Moens"], "venue": "Information Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["T. Kwiatkowski", "E. Choi", "Y. Artzi", "L. Zettlemoyer"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Entity linking at web scale. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 84\u201388", "author": ["T. Lin", "O. Etzioni"], "venue": "Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. R\u00e9", "S.J. Wright", "F. Niu"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Template-based question answering over rdf data", "author": ["C. Unger", "L. B\u00fchmann", "J. Lehmann", "A.-C. Ngonga Ngomo", "D. Gerber", "P. Cimiano"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["X. Yao", "B. Van Durme"], "venue": "In To appear in Proceedings of ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "These KBs, such as Freebase [3] encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine-grained detection heuristics to identify the exact answer [7,11,13].", "startOffset": 245, "endOffset": 254}, {"referenceID": 10, "context": "Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine-grained detection heuristics to identify the exact answer [7,11,13].", "startOffset": 245, "endOffset": 254}, {"referenceID": 12, "context": "Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine-grained detection heuristics to identify the exact answer [7,11,13].", "startOffset": 245, "endOffset": 254}, {"referenceID": 0, "context": "Interestingly, recent works [1,8,2,6] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.", "startOffset": 28, "endOffset": 37}, {"referenceID": 7, "context": "Interestingly, recent works [1,8,2,6] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.", "startOffset": 28, "endOffset": 37}, {"referenceID": 1, "context": "Interestingly, recent works [1,8,2,6] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.", "startOffset": 28, "endOffset": 37}, {"referenceID": 5, "context": "Interestingly, recent works [1,8,2,6] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.", "startOffset": 28, "endOffset": 37}, {"referenceID": 4, "context": "In contrast, [5] proposed a framework for open QA requiring almost no human annotation.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "[4] introduced an embedding model, which learns low-dimensional vector representations of words and symbols (such as KBs constituents) and can be trained with even less supervision than the system of [5] while being able to achieve better prediction performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] introduced an embedding model, which learns low-dimensional vector representations of words and symbols (such as KBs constituents) and can be trained with even less supervision than the system of [5] while being able to achieve better prediction performance.", "startOffset": 200, "endOffset": 203}, {"referenceID": 4, "context": "However, this approach is only compared with [5] which operates in a simplified setting and has not been applied in more realistic conditions nor evaluated against the best performing methods.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "In this paper, we improve the model of [4] by providing the ability to answer more complicated questions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "Our approach achieves identical performance to the current state-of-the-art on the competitive benchmark WebQuestions [1] without using any lexicon, rules or additional system for part-of-speech tagging, syntactic or dependency parsing during training as most other systems do.", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "We evaluate our system using the WebQuestions benchmark [1] which is built using Freebase as the KB.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "These are similar to those used in [2].", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "Freebase Freebase [3] is a huge and freely available database of general facts; data is organized as triplets (subject, type1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "WebQuestions This dataset, which contains 5,810 question-answer pairs, was introduced in [1] and is used as our evaluation benchmark.", "startOffset": 89, "endOffset": 92}, {"referenceID": 0, "context": "Following [1], we also created questions using ClueWeb extractions provided by [9].", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "Following [1], we also created questions using ClueWeb extractions provided by [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "To overcome this issue, we follow [5] and supplement our training data with an indirect supervision signal made of pairs of question paraphrases collected from the WikiAnswers website.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "On WikiAnswers, users can tag pairs of questions as rephrasings of each other: [5] harvested a set of 2M distinct questions from WikiAnswers, which were grouped into 350k paraphrase clusters.", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "Inspired by [4], our model works by learning low-dimensional vector embeddings of words appearing in questions and of entities and relation types of Freebase,", "startOffset": 12, "endOffset": 15}, {"referenceID": 11, "context": "As in [12], we train our model using a margin-based ranking loss function.", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "Optimization is accomplished using stochastic gradient descent, multi-threaded with Hogwild! [10], with the constraint that the columns wi of W remain within the unit-ball, i.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": ", 2013 [1] 31.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": ", 2014 [4] 31.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": ", 2014 [6] 35.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "0% Bernat and Liang, 2014 [2] 39.", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "Yao and Van Durme, 2014 [13] 36.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "(3)) on the WebQuestions test set with the original model proposed for this dataset in [1] and other systems recently published.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "The upper part of Table 3 indicates that our approach performs as well as the recent state-of-the-art approach of [2] and superior to [13], [1] and [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 12, "context": "The upper part of Table 3 indicates that our approach performs as well as the recent state-of-the-art approach of [2] and superior to [13], [1] and [6].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "The upper part of Table 3 indicates that our approach performs as well as the recent state-of-the-art approach of [2] and superior to [13], [1] and [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "The upper part of Table 3 indicates that our approach performs as well as the recent state-of-the-art approach of [2] and superior to [13], [1] and [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 12, "context": "[13] report results that outperform us but this was achieved using a very large additional training data of 1 billion triples extracted from ClueWeb (to be compared with our 2M).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Finally, we demonstrate that we greatly improve upon the model of [4], which actually corresponds to a setting with the Path representation and C1 as candidate set.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "2 Results of baselines except [4] have been extracted from the original papers.", "startOffset": 30, "endOffset": 33}], "year": 2017, "abstractText": "This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields state-of-the-art results on a competitive benchmark of the literature.", "creator": "LaTeX with hyperref package"}}}