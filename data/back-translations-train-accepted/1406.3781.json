{"id": "1406.3781", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2014", "title": "From Stochastic Mixability to Fast Rates", "abstract": "Empirical risk minimization (ERM) is a fundamental algorithm for statistical learning problems where the data is generated according to some unknown distribution $\\mathsf{P}$ and returns a hypothesis $f$ chosen from a fixed class $\\mathcal{F}$ with small loss $\\ell$. In the parametric setting, depending upon $(\\ell, \\mathcal{F},\\mathsf{P})$ ERM can have slow $(1/\\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as a function of the sample size $n$. There exist several results that give sufficient conditions for fast rates in terms of joint properties of $\\ell$, $\\mathcal{F}$, and $\\mathsf{P}$, such as the margin condition and the Bernstein condition. In the non-statistical prediction with experts setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss $\\ell$ (there being no role there for $\\mathcal{F}$ or $\\mathsf{P}$). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of $(\\ell,\\mathcal{F}, \\mathsf{P})$, and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the generalized moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible.", "histories": [["v1", "Sat, 14 Jun 2014 23:25:05 GMT  (17kb,D)", "https://arxiv.org/abs/1406.3781v1", "13 pages"], ["v2", "Sat, 22 Nov 2014 11:16:28 GMT  (24kb,D)", "http://arxiv.org/abs/1406.3781v2", "21 pages, accepted to NIPS 2014"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nishant a mehta", "robert c williamson"], "accepted": true, "id": "1406.3781"}, "pdf": {"name": "1406.3781.pdf", "metadata": {"source": "CRF", "title": "From Stochastic Mixability to Fast Rates", "authors": ["Nishant A. Mehta", "Robert C. Williamson"], "emails": [], "sections": [{"heading": null, "text": "n) or fast (1 / n) convergence rates of excess risk as a function of sample size n. There are several results that provide sufficient conditions for fast rates in relation to the common properties of, \"F and P, such as the boundary condition and the amber condition. In non-statistical prediction with expert advice, there is an analogous phenomenon of slow and fast rates, and it is fully characterized in relation to the miscibility of loss\" (for F or P there is no role). The concept of stochastic miscibility builds a bridge between these two learning models and in one specific case reduces to classical miscibility. The present paper provides direct evidence of fast rates for ERM in relation to the stochastic miscibility of (, \"F, P), thus providing new insights into the phenomenon of fast rates. The evidence uses an old result from Kempermann to solve the general moment problem. We also show a partial reversal, the fast characterization of the Relestic mixing in relation to the Rchastic."}, {"heading": "1 Introduction", "text": "In fact, it is the case that you are able to play by the rules that you had in the past, and that you are able to play by them."}, {"heading": "2 Stochastic mixability, Crame\u0301r-Chernoff, and ERM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The Setting", "text": "Let (, \"F, P) be a statistical learning problem with\": Y \u00b7 R \u2192 R + a nonnegative loss, F-RX of a compact function class, and P of a probability measurement via X \u00b7 Y for input and output / target space Y. LetZ should be a random variable defined as Z = (X, Y) \u0445 P. We go for all f-F, \"(Y, f (X)) \u2264 V (a.s.) for a certain constant V.A Probability measurement P works on functions and loss-composed functions as: P f = E (X, Y) \u0445 P f (X) P '(\u00b7, f) = E (X, Y) \u0445 P' (Y, f (X))). Similarly, an empirical measurement Pn works on functions in connection with an n-sample z, which includes n-samples (x1, Y) consisting of n iid samples (x1), (xn)."}, {"heading": "2.2 Stochastic mixability", "text": "For \u03b7 > 0 we say that (, \"F, P) \u03b7-stochastically miscible, if for all f-Flog E exp (\u2212 \u03b7Zf) \u2264 0. (1) If \u03b7-stochastic miscibility applies to some \u03b7 > 0, then we say that (,\" F, P) is stochastically miscible. In the course of this thesis it is assumed that the stochastic miscibility condition applies, and we assume that \u03b7-stochastic miscibility is greatest. Condition (1) has a rich history, starting with the basic thesis of Li (1999), which examined the specific case of \u0440 = 1 in density estimation with log losses from the perspective of information geometry. The links that Li showed between this condition and convexity were reinforced by Gr\u00c3 \u00bc nwald (2011, 2012) and Van Erven et al. (2012)."}, {"heading": "2.3 Crame\u0301r-Chernoff", "text": "The high-level strategy used here is to show that the empirical risk mitigation algorithm (ERM) is highly likely not to select a fixed hypotheses function f with excess risk above a constant a > 0. This guarantee will result for each hypothesis from the Crame'r-Chernoff method (Boucheron et al., 2013) by controlling the cumulative generation function (CGF) of \u2212 Zf in a specific way to achieve exponential concentration.This control will be possible because the state of the preliminary stochastic miscibility implies that the CGF of \u2212 Zf absorbs the value 0 at some point in time, which will later be exploited by our key tool Theorem 3. Let Z be a genuinely evaluated random variable."}, {"heading": "2.4 Analysis of ERM", "text": "Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-"}, {"heading": "3 Semi-infinite linear programming and the general moment prob-", "text": "(1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (2) (1) (1) (1) (1)) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (2) (2) (1)) (1) (2) (2) (2) (2) (2) (2) (2) (2) () () () ()) () () () () ()) () () ()) () () ()) () () () () () () () ()) () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () (() () () (() () () () () () () () () () () ((() () () (() () () () () () () () ((() () () () () () (("}, {"heading": "4 Fast rates", "text": "We show how the above results can be used to obtain an exact oracular inequality at a constant rate. We first present a result for finite classes and then present a result for various parametric classes, including classes with logarithmic universal entropy, VC classes and classes with polynomial uniform L1 bracketing number. Theorem 5 (finite classes exact oracle inquality). Let (, \"F, P) be stochastically mixable, where F | = N,\" is a nonnegative loss, and supf-F \"(Y, f (X) \u2264 V a.s. for a constant V a.s. Then for all n classes, with a probability of at least 1 \u2212 P\" (\u00b7, f)."}, {"heading": "5 Characterizing convexity from the perspective of risk minimiza-", "text": "The following raises the question of whether mixability is truly unique. (\", F, P\") We assume that there is no mixability. (\", F\") We assume that there is no mixability. (\", F\") We assume that there is no mixability. (\", F\") We assume that there is no mixability. (\", F\") We assume that there is no mixability. (\", F\") \"It is already known that stochastic mixability guarantees that there is a unique mixability. (\", F \") We assume that there is a mixability. (\", F \") We assume that there is no mixability. (\", F \")\" It is already known that stochastic mixability guarantees that there is a unique mixability. (\"Van Erven et al,\" 2012); this is a simple consequence of Jensen quality."}, {"heading": "6 Weak stochastic mixability", "text": "In some cases it is said that (, \"F, P) -weakly stochastically miscible if this concept is introduced for all \u03b5 > 0, for all f \u00b2, f \u00b2, F \u03b5log E exp (\u2212 \u03b7\u03b5Zf) \u2264 0, (14) with \u03b7\u03b5: = \u03b7\u03b5 1 \u2212 \u0432. This concept was introduced by Van Erven et al. (2012) without a name. Suppose that a fixed function has an excess risk a = \u03b5. Modifying the proof for the finite classes results (theorem 5) with a high probability of no error, provided that all functions in the subclass F \u00b2 n = (\u03b70n) - and thus in cases where \u03b5 = (\u03b70n) \u2212 1 / (2 \u2212 \u0432) - a result of the finite classes (theorem 5) is taken into account (theorem 5) in order to achieve all functions in the subclass F \u00b2, i.e. n = (\u03b70n) \u2212 n / (2 \u2212 \u043c) mixing classes."}, {"heading": "7 Discussion", "text": "We have shown that stochastic miscibility implies fast rates for VC type classes by using a direct argument based on the Crame-r-Chernoff method and sufficient control over the optimal value of a particular instance of the general moment problem. Thus, the approach is able to apply the results presented here for VC type classes to results for non-parametric classes with polynomial metric entropy. Therefore, it was easy to extend the result for finite classes to VC type classes - an important open problem is to extend the results for non-parametric classes with polynomial metric entropy, and beyond that, to achieve rates obtained for these classes under the amber condition. There are still some unanswered questions regarding the link between the amber condition and stochastic miscibility. Van Erven et al showed that for limited losses the stochastic condition implies stochastic miscibility."}, {"heading": "Acknowledgments", "text": "RCW would like to thank Tim van Erven for the initial discussion of the Crame \u0301 r Chernoff Method during his visit to Canberra in 2013 and for his kind permission to continue this work without him as the author, and both authors thank him for further tremendously helpful discovery of a serious flaw in our original evidence of quick prizes for VC-like classes. This work was supported by the Australian Research Council (NAM and RCW) and NICTA (RCW). NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence program."}, {"heading": "A Proof of Theorem 3", "text": "The proof of theorem 3 by Kemperman (1968), when the moment values vector (\u2212 an, 1) in int conv g ([\u2212 1, 1]), the optimal objective value of the problem (6) is equal, we see that (10) corresponds to the inner point. Assuming that the inner point condition is met, each d \u00b2 point returns a lower limit to the optimal value of (6), and hence after negation provides an upper limit to the problem (10). Assuming that the inner point condition is met, each d \u00b2 point returns a lower limit to the optimal value of (6), and hence after negation offers an upper limit to the problem."}, {"heading": "B Hyper-concentrated excess losses", "text": "Lemma 10. Let Z be a random variable with a probability measurement P based on [\u2212 \u03b7 Q, V]. Let us assume that the saloon (\u2212 \u03b7Z) < 1 and EZ \u2212 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 for the Z \u00b2 \u2264 Z with probability 1, the mean of Z \u00b2 e is arbitrarily close to \u00b5, and E exp (\u2212 Z \u00b2) = 1 for the arbitrarily large number of P. Let us first note that Z \u00b2 0 a.s. If not, then there must be a finite number of P > 0 for which E exp (\u2212 Z) = 1. Let us now consider a random variable Z \u00b2 with probability Q \u2212 Z \u00b2, a change of Z (with probability measurement P) constructed in the following way. Let us define A: = [\u00b5, V] and A \u2212: = [\u2212 V, \u2212 z] for the probability."}, {"heading": "C Proof of VC-type results", "text": "It's a simple consequence of the following LP class: \"It's the only way in which we're able to see ourselves.\" \"It's the only way in which we're able.\" \"It's the only way in which we're able.\" \"It's the only way in which we're able.\" \"It's the only way in which we're able.\" \"It's the only way in which we're able.\" \"It's the only way in which we're able.\" \"It's the only way in which we're able.\" \"It's the only way in which we're able.\" \"It's the only way in which we're able.\" It's the only way in which we're able. \"It's the only way in which we're able.\" It's the only way in which we're able. \""}], "references": [{"title": "A stochastic view of optimal regret through minimax duality", "author": ["Jacob Abernethy", "Alekh Agarwal", "Peter L. Bartlett", "Alexander Rakhlin"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory (COLT", "citeRegEx": "Abernethy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2009}, {"title": "Fast learning rates in statistical inference through aggregation", "author": ["Jean-Yves Audibert"], "venue": "The Annals of Statistics,", "citeRegEx": "Audibert.,? \\Q2009\\E", "shortCiteRegEx": "Audibert.", "year": 2009}, {"title": "Empirical minimization", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2006}, {"title": "Local Rademacher complexities", "author": ["Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "Concentration inequalities: A nonasymptotic theory of independence", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Safe learning: bridging the gap between Bayes, MDL and statistical learning theory via empirical convexity", "author": ["Peter Gr\u00fcnwald"], "venue": "In Proceedings of the 24th International Conference on Learning Theory (COLT", "citeRegEx": "Gr\u00fcnwald.,? \\Q2011\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2011}, {"title": "The safe Bayesian", "author": ["Peter Gr\u00fcnwald"], "venue": "In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT", "citeRegEx": "Gr\u00fcnwald.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2012}, {"title": "The weak aggregating algorithm and weak mixability", "author": ["Yuri Kalnishkan", "Michael V. Vyugin"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory (COLT", "citeRegEx": "Kalnishkan and Vyugin.,? \\Q2005\\E", "shortCiteRegEx": "Kalnishkan and Vyugin.", "year": 2005}, {"title": "Kemperman. The general moment problem, a geometric approach", "author": ["H.B. Johannes"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Johannes,? \\Q1968\\E", "shortCiteRegEx": "Johannes", "year": 1968}, {"title": "Interplay between concentration, complexity and geometry in learning theory", "author": ["Guillaume Lecu\u00e9"], "venue": "Ecole dEte\u0301 de Probabilite\u0301s de Saint-Flour XXXVIII-2008,", "citeRegEx": "Lecu\u00e9.,? \\Q2011\\E", "shortCiteRegEx": "Lecu\u00e9.", "year": 2011}, {"title": "Estimation of mixture models", "author": ["Jonathan Qiang Li"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Li.,? \\Q1974\\E", "shortCiteRegEx": "Li.", "year": 1974}, {"title": "Agnostic learning nonconvex function classes", "author": ["2008b. Shahar Mendelson", "Robert C. Williamson"], "venue": null, "citeRegEx": "Mendelson and Williamson.,? \\Q2008\\E", "shortCiteRegEx": "Mendelson and Williamson.", "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "(Vovk, 2001; Kalnishkan and Vyugin, 2005)) have developed a rich convex geometric understanding of mixability, stochastic mixability can be understood as a sort of effective convexity.", "startOffset": 0, "endOffset": 41}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.", "startOffset": 14, "endOffset": 40}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.g. from the work of Audibert (2009), and most recently Van Erven et al.", "startOffset": 14, "endOffset": 378}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.g. from the work of Audibert (2009), and most recently Van Erven et al.\u2019s (2012) injection of the concept of mixability into the statistical learning space in the form of stochastic mixability.", "startOffset": 14, "endOffset": 423}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.g. from the work of Audibert (2009), and most recently Van Erven et al.\u2019s (2012) injection of the concept of mixability into the statistical learning space in the form of stochastic mixability. It is this last connection that will be our departure point for this work. Mixability is a fundamental property of a loss that characterizes when constant regret is possible in the online learning game of prediction with expert advice (Vovk, 1998). Stochastic mixability is a natural adaptation of mixability to the statistical learning setting; in fact, in the special case where the function class consists of all possible functions from the input space to the prediction space, stochastic mixability is equivalent to mixability (Van Erven et al., 2012). Just as Vovk and coworkers (see e.g. (Vovk, 2001; Kalnishkan and Vyugin, 2005)) have developed a rich convex geometric understanding of mixability, stochastic mixability can be understood as a sort of effective convexity. In this work, we study the O( 1 n )-fast rate phenomenon in statistical learning from the perspective of stochastic mixability. Our motivation is that stochastic mixability might characterize fast rates in statistical learning. As a first step, Theorem 5 of this paper establishes via a rather direct argument that stochastic mixability implies an exact oracle inequality (i.e. with leading constant 1) with a fast rate for finite function classes, and Theorem 7 extends this result to VC-type classes. This result can be understood as a new chapter in an evolving narrative that started with Lee et al.\u2019s (1998) seminal paper showing fast rates for agnostic learning with squared loss over convex function classes, and that was continued by Mendelson (2008b) who showed that fast rates are possible for p-losses (y, \u0177) 7\u2192 |y \u2212 \u0177| over effectively convex function classes by passing through a Bernstein condition (defined in (12)).", "startOffset": 14, "endOffset": 1928}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.g. from the work of Audibert (2009), and most recently Van Erven et al.\u2019s (2012) injection of the concept of mixability into the statistical learning space in the form of stochastic mixability. It is this last connection that will be our departure point for this work. Mixability is a fundamental property of a loss that characterizes when constant regret is possible in the online learning game of prediction with expert advice (Vovk, 1998). Stochastic mixability is a natural adaptation of mixability to the statistical learning setting; in fact, in the special case where the function class consists of all possible functions from the input space to the prediction space, stochastic mixability is equivalent to mixability (Van Erven et al., 2012). Just as Vovk and coworkers (see e.g. (Vovk, 2001; Kalnishkan and Vyugin, 2005)) have developed a rich convex geometric understanding of mixability, stochastic mixability can be understood as a sort of effective convexity. In this work, we study the O( 1 n )-fast rate phenomenon in statistical learning from the perspective of stochastic mixability. Our motivation is that stochastic mixability might characterize fast rates in statistical learning. As a first step, Theorem 5 of this paper establishes via a rather direct argument that stochastic mixability implies an exact oracle inequality (i.e. with leading constant 1) with a fast rate for finite function classes, and Theorem 7 extends this result to VC-type classes. This result can be understood as a new chapter in an evolving narrative that started with Lee et al.\u2019s (1998) seminal paper showing fast rates for agnostic learning with squared loss over convex function classes, and that was continued by Mendelson (2008b) who showed that fast rates are possible for p-losses (y, \u0177) 7\u2192 |y \u2212 \u0177| over effectively convex function classes by passing through a Bernstein condition (defined in (12)).", "startOffset": 14, "endOffset": 2075}, {"referenceID": 3, "context": "Much of the recent work in obtaining faster learning rates in agnostic learning has taken place in settings where a Bernstein condition holds, including results based on local Rademacher complexities (Bartlett et al., 2005; Koltchinskii, 2006).", "startOffset": 200, "endOffset": 243}, {"referenceID": 8, "context": "This is precisely the situation at the heart of the works of Mendelson (2008b) and Mendelson and Williamson (2002), which show that having non-unique minimizers is symptomatic of bad geometry of the learning problem.", "startOffset": 83, "endOffset": 115}, {"referenceID": 2, "context": "The Bernstein condition appears to have first been used by Bartlett and Mendelson (2006) in their analysis of empirical risk minimization; this condition is subtly different from the margin condition of Mammen and Tsybakov (1999) and Tsybakov (2004), which has been used to obtain fast rates for classification problems.", "startOffset": 59, "endOffset": 89}, {"referenceID": 2, "context": "The Bernstein condition appears to have first been used by Bartlett and Mendelson (2006) in their analysis of empirical risk minimization; this condition is subtly different from the margin condition of Mammen and Tsybakov (1999) and Tsybakov (2004), which has been used to obtain fast rates for classification problems.", "startOffset": 59, "endOffset": 230}, {"referenceID": 2, "context": "The Bernstein condition appears to have first been used by Bartlett and Mendelson (2006) in their analysis of empirical risk minimization; this condition is subtly different from the margin condition of Mammen and Tsybakov (1999) and Tsybakov (2004), which has been used to obtain fast rates for classification problems.", "startOffset": 59, "endOffset": 250}, {"referenceID": 2, "context": "The Bernstein condition appears to have first been used by Bartlett and Mendelson (2006) in their analysis of empirical risk minimization; this condition is subtly different from the margin condition of Mammen and Tsybakov (1999) and Tsybakov (2004), which has been used to obtain fast rates for classification problems. Lecu\u00e9 (2011) pinpoints that the difference between the two conditions is that the margin condition applies to the excess loss relative to the best predictor (not necessarily in the model class) whereas the Bernstein condition applies to the excess loss relative to the best predictor in the model class.", "startOffset": 59, "endOffset": 334}, {"referenceID": 8, "context": "Condition (1) has a rich history, beginning from the foundational thesis of Li (1999) who studied the special case of \u03b7\u2217 = 1 in density estimation with log loss from the perspective of information geometry.", "startOffset": 76, "endOffset": 86}, {"referenceID": 5, "context": "The connections that Li showed between this condition and convexity were strengthened by Gr\u00fcnwald (2011, 2012) and Van Erven et al. (2012).", "startOffset": 89, "endOffset": 139}, {"referenceID": 4, "context": "For each hypothesis, this guarantee will flow from the Cram\u00e9r-Chernoff method (Boucheron et al., 2013) by controlling the cumulant generating function (CGF) of \u2212Zf in a particular way to yield exponential concentration.", "startOffset": 78, "endOffset": 102}], "year": 2014, "abstractText": "Empirical risk minimization (ERM) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution P and returns a hypothesis f chosen from a fixed class F with small loss `. In the parametric setting, depending upon (`,F ,P) ERM can have slow (1/ \u221a n) or fast (1/n) rates of convergence of the excess risk as a function of the sample size n. There exist several results that give sufficient conditions for fast rates in terms of joint properties of `, F , and P, such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss ` (there being no role there for F or P). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of (`,F ,P), and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible.", "creator": "LaTeX with hyperref package"}}}