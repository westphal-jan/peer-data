{"id": "1406.4905", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "Variational Gaussian Process State-Space Models", "abstract": "State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present extensions to stochastic variational inference and online learning.", "histories": [["v1", "Wed, 18 Jun 2014 22:16:27 GMT  (147kb,D)", "https://arxiv.org/abs/1406.4905v1", null], ["v2", "Mon, 3 Nov 2014 08:17:59 GMT  (248kb,D)", "http://arxiv.org/abs/1406.4905v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.RO cs.SY stat.ML", "authors": ["roger frigola", "yutian chen", "carl e rasmussen"], "accepted": true, "id": "1406.4905"}, "pdf": {"name": "1406.4905.pdf", "metadata": {"source": "CRF", "title": "Variational Gaussian Process State-Space Models", "authors": ["Roger Frigola", "Yutian Chen"], "emails": ["rf342@cam.ac.uk", "yc373@cam.ac.uk", "cer54@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, we have reached the point where we feel we are able to live in a country where most people are able to move and where they are able to flourish."}, {"heading": "2 Gaussian Process State-Space Models", "text": "We consider discrete time nonlinear state-space models constructed with deterministic functions and additive functions to be particularly suitable for the family system. We consider discrete time nonlinear state-space models constructed with deterministic functions and additive functions to be particularly complex functions if this function (1) is very complex, (1a) yt = g (2b) However, the dynamics of the system are defined by the state transition function f (xt) and independent additive noise (process noise).The states xt are latent variables, so that all future variables are conditionally independent of the past, since the current state exists. Observations are linked to the state via another deterministic function g (xt) and independent additive noise (observation noise).The state-space models are stochastic time series y, {y1, yT}. The deterministic functions in (1) may also include external known inputs (such as control signals), but we will ignore this as an emergency."}, {"heading": "3 Variational Inference in GP-SSMs", "text": "Since GP-SSM is a non-parametric model for defining a posterior distribution over f (x) and making probable predictions, it is necessary to first find the smoothing distribution p (x0: T | y1: T). Frigola et al. [8] obtained samples from the smoothing distribution that could be used to define a predictive density via Monte Carlo integration. This approach is expensive as it requires averaging over L state samples of length T. In this section, we present an alternative approach aimed at finding a tractable distribution via the state transition function independent of the length of the time series."}, {"heading": "3.1 Augmenting the Model with Inducing Variables", "text": "As a first step to perform variation conclusions in a GP-SSM, we extend the model by M inducing points u, {ui} Mi = 1. The inducing points are Gaussian in common with the latent function values. In the case of a GP-SSM, the joint probability density p (y, x, f, u) = p (x, f | u) p (u) T-t = 1 p (yt | xt), (5) where (u) = N (u | 0, Ku, u) (6a) p (x, f | u) = p (x0) T-t = 1 p (ft | f1 \u2212 1, x0: t \u2212 1, u) p (xt | ft), (6b) T-t = 1 p (ft, Ku, u) (6a) p (x, f1: t \u2212 1, x0: t \u2212 1, u) = N (f1: T \u2212 K0 \u2212 1, uK \u2212 uu, uu, uu, u = 1 p (ft, Ku-K) in relation to M-K."}, {"heading": "3.2 Evidence Lower Bound of an Augmented GP-SSM", "text": "The variation conclusion [1] is a popular method for approximate Bayesian conclusions based on assumptions about the posterior latent variables leading to a tractable lower limit (sometimes referred to as ELBO). Maximizing this lower limit corresponds to minimizing the Kullback-Leibler divergence between the approximate posterior and the exact. [1] Following the standard variation conclusion methodology, we obtain the evidence lower limit of a GP SSM augmented with inducing point log p (y-Leibler divergence) \u2265 x, f, u q (x, u) log p (u) log p (x0) p (ft-eribler-T = 1 p (ft \u2212 f1: t \u2212 1, u) p (yt | xt) q (xt | ft) q (x, f, u)."}, {"heading": "3.3 Optimal Variational Distribution for u", "text": "The optimal distribution of q (u) can be found by setting the functional derivative of the evidence in relation to q (u) q * (u) q (u) p (u) t = 1 Exp {< logN (xt | At \u2212 1u, Q) > q (x)}, (10) to zero, where < \u00b7 > q (x) denotes an expectation in relation to q (x). The optimal variation distribution q * (u) is conveniently a multivariate Gaussian distribution. If, for the sake of simplicity, we limit ourselves to D = 1 the natural parameters of the optimal distribution, we get 1 = Q \u2212 1 T \u2211 t = 1 < ATt \u2212 1xt > q (xt, xt \u2212 1), 2 = \u2212 1 2 (K \u2212 1uu + Q \u2212 1 T), as sufficient (p = 1 < ATt \u2212 1 > q (xt \u2212 1 < < < < \u2212 11) The mean and the deviation from each of q (xt) and the statistics (1) can be considered sufficient."}, {"heading": "3.4 Optimal Variational Distribution for x", "text": "Similar to q \u0445 (u), we can obtain the optimal form of q (x) q (x) q (x) p (x0) T \u0442t = 1 p (yt | xt) exp {\u2212 1 2 tr (Q \u2212 1 (Bt \u2212 1 + At \u2212 1\u0441A T t \u2212 1)))} N (xt | At \u2212 1\u00b5, Q), (12), where we used q (u | \u00b5, \u03a3) in the second equation. The optimal distribution q (x) corresponds to the smoothing distribution of a parametric state-space auxiliary model. The auxiliary model is simpler than the original one in (3), because the latent states factorize with a Markovian structure. Equation (12) can be interpreted as non-linear state-space models, which represent smoothly state densities, N (xt | 1\u00b5, Q) and a problem with an additional linear structure."}, {"heading": "3.5 Optimizing the Evidence Lower Bound", "text": "Algorithm 1 is a method to maximize the evidence limited by alternative samples from the smoothing distribution. We propose a hybrid variation sampling approach, in which approximate samples from q \u043c (x) are obtained smoother using a Monte Carlo sequential method. However, as discussed in Section 3.4, different smoothing methods could be more appropriate depending on the characteristics of the dynamic system [20]. As an alternative to smoothing the dynamic auxiliary system in (12), forcing a q (x) from a specific family of distributions and optimizing the evidence taking into account its variable parameters, we could, for example, position a Gaussian q (x) with a spar pattern in the covariance matrix assuming zero covariance between non-adjacent states and maximizing the ELBO taking into account the variation parameters."}, {"heading": "3.6 Making Predictions", "text": "One of the most attractive features of our variable approach to learning GP SSMs is that the approximate predictive distribution of state transition function can be calculated cheaply. (f * | x *, y) = x, u p (f * | x *, u) p (x | u, y) p (u | y) p (u * x, u p (f * | x *, u) p (x *, u) p (x) p (x, u) p (x, u) p (x *, u) p (x *, u) p (x *, u) p (x *, u) p (x), u (x) p (x), u (x) p (x, u) p (x) p (x, u) p (x, u) p (x) p (x, u) p (x) p (x, u) p (x) p (x, u (x, u) p (x, u) p (x, u (x), u (x, u) p (x, u), u (x, u (x, u), u (x, u (x, u), u (x, u (x, u), u (x, u (x, u), u (x, u) p (x, u (x, u) p (x, u) p (x, u) p (x, u (x, u) p (x, u) p (x, u) p (x, u (x, u) p (x, u) p (x, u) p (x (x, u) p (x, u) p (x, u) p (x, u (x, u) p (x, u) p (x, u) p (x, u) p (x (x, u) p (x, u (x, u) p (x, u) p (x, u) p (x, u) p (x, u (x, u) p (x, u (x, u) p (x, u) p (x, u) p (x, x, u) p (x, x, u) p (x, x (x, u) p (x, u) p (x"}, {"heading": "4 Stochastic Variational Inference", "text": "The stochastic variation conclusion (SVI) [10] can be easily applied using our lower limit of evidence. If the observed time series is long, it can be expensive to calculate q * (u) or the gradient of Lin in relation to the hyperparameters and inductive inputs. Since both q * (u) and \u2202 L \u2082 / z1: M depend linearly on q (x) for sufficient statistics that include a summation of all elements in the state orbit, we can obtain unbiased estimates of these sufficient statistics by using one or more segments of the sequence that are randomly sampled. However, determining q (x) also requires a time complexity of O (T). In practice, however, q (x) can be estimated by local execution of the smoothing algorithm around these segments. This can be justified by the fact that the smoothing distribution at a certain point in time in a time series context is not largely influenced by future measurements, but by the \u2212 algorithms in the past."}, {"heading": "5 Online Learning", "text": "Our varying approach to learning GP-SSMs naturally also leads to an implementation of online learning. This is of particular interest in the context of dynamic systems, as it is often the case that data arrives sequentially, e.g. a robot learning the dynamics of various objects by interacting with them. Online learning in a Bayesian context consists in applying the Bayes rule sequentially, according to which the buttock becomes t + 1 [2, 15] after observing the data until the time t is the previous one at the time. In our case, this means that the previous p (u) = N (u | 0, Ku, u) is replaced by the approximate rear N (u | \u00b5, \u03a3) obtained in the previous step. The terms for updating the natural parameters of q (u) are replaced by a new mini-batch result:"}, {"heading": "6 Experiments", "text": "The aim of this section is to demonstrate the ability of varying GP SSMs to perform approximate Bayesian learning of nonlinear dynamic systems, in particular: 1) the ability to learn the inherent nonlinear dynamics of a system, 2) the application in cases where the latent states have a higher dimensionality than the observations, and 3) the use of nongaussian probabilities."}, {"heading": "6.1 1D Nonlinear System", "text": "We apply our variational learning method shown above to the one-dimensional nonlinear system, which is described by p (xt + 1 | xt) = N (f (xt), 1) and p (yt | xt) = N (xt, 1), with the transition function xt + 1 if x < 4 and \u2212 4xt + 21 if x \u2265 4. Its pronounced buckling makes it difficult to learn. Our goal is to find a posterior distribution via this function using a GP-SSM with Mate \u0301 rn covariance function. In order to solve the expectations regarding the approximate smoothing distribution q (x), we use a bootstrap particle test with 1000 particles and a delay of 10. In Table 1, we compare our method (Variational GP-SSM) with the PMCMC sampling method from [8], in which 100 samples are taken and 10 samples are burnt in samples. As in [8], the sample showed a very good mixing with 20 particles in the sample ratio of CM to the SSM, we also compare our model with the SSM-GP method."}, {"heading": "6.2 Neural Spike Train Recordings", "text": "We now turn to the use of SSMs to learn a simple model of neuronal activity in the hippocampus of rats. We are using data in neuron cluster 1 (the most active) from the ec013.717 experiment in [14]. In some regions of the time series, the spikes of action potential show a clear pattern in which periods of rapid spikes are followed by periods of very low spikes. We want to model this behavior as an autonomous nonlinear dynamic system (i.e. one that is not driven by external inputs). Many parametric models of nonlinear neuron dynamics have been proposed [11], but our goal here is to learn a model from data without using any biological insights. We are using a GP-SSM with a structure that is the discrete time analog of a second order nonlinear ordinary differential equation: two states, one of which is the derivation of the other."}, {"heading": "7 Discussion and Future Work", "text": "We have derived a tractable variation formula to learn GP-SSMs: an important class of models of nonlinear dynamic systems, which is particularly suitable for applications where a principal parametric model of dynamics is not available. Our approach enables us to learn very meaningful models without the risk of overadaptation. In contrast to earlier approaches [4, 12, 25] we have demonstrated the ability to learn a nonlinear state transition function in a latent space of greater dimensions than the observation space. More importantly, our approach to nonlinear systems yields a tractable posterior, which, unlike those based on samples from the smoothing distribution [8], leads to a computation time for predictions that does not depend on the length of time series. Given the interesting capabilities of variable GP-SSMs, we believe that future work is justified. Specifically, we want to focus on the need for structured dynamics (x) of the problem of dividing the system."}, {"heading": "B Relationship of Variational Approximation with Other Models", "text": "(21) This model can be interpreted as a parametric model in which At \u2212 1u is a deterministic transition function and Bt \u2212 1 is the description of a heteroscedastic process. (21) Note: This process is independent of two timescales. (21) Note: If we are looking for a subordinate approach to section 3 and the following distribution variants, we will (x, u) = q (u) T (x) T (x) T (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x (x) x (x) x (x) x (x) x (x (x) x (x) x (x) x (x) x (x (x) x (x) x (x) x (x (x) x (x) x (x (x) x (x) x (x) x (x x x x x x x x x x x x x x x x (x x x x x x x x x x x x x) x x x x x x x x x x x (x x x x x x x x x x x x x x x x x) x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x) x x x x x x x x (x x x x x x x x x x x x x x x x x x x x x) x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "C Optimization of Hyperparameters", "text": "We optimize the hyperparameters and variation parameters (z1: M) with the slope of the slope. The gradient w.r.t. \u03b8 is calculated as \u2202 L zipzipzipx log p (u) q (u) + \u2202 log p (x0) q (x0) + T \u2211 t = 1 {\u2212 1 2 \u2202 \u03b8 tr (Q \u2212 1 (Bt \u2212 1 + At \u2212 1A T \u2212 1))) q (xt \u2212 1) + \u0445 20s log logN (xt | At \u2212 1\u00b5, Q) q (xt, xt \u2212 1) + xi xi uml log p (yt | xt) q (xt)}. (29) The slope in relation to z1: M is similar to the slope that is replaced by z1: M. In this expression, the optimal settings can be replaced depending on sufficient statistics 1 and 2."}, {"heading": "D Plots from Experiments Section", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "State-space models have been successfully used for more than fifty years in differ-<lb>ent areas of science and engineering. We present a procedure for efficient varia-<lb>tional Bayesian learning of nonlinear state-space models based on sparse Gaussian<lb>processes. The result of learning is a tractable posterior over nonlinear dynamical<lb>systems. In comparison to conventional parametric models, we offer the possi-<lb>bility to straightforwardly trade off model capacity and computational cost whilst<lb>avoiding overfitting. Our main algorithm uses a hybrid inference approach com-<lb>bining variational Bayes and sequential Monte Carlo. We also present stochastic<lb>variational inference and online learning approaches for fast learning with long<lb>time series.", "creator": "LaTeX with hyperref package"}}}