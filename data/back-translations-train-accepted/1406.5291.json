{"id": "1406.5291", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2014", "title": "Generalized Dantzig Selector: Application to the k-support norm", "abstract": "We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS, and non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian width of unit norm ball and suitable set encompassing estimation error. Further, we consider a non-trivial example of the GDS using $k$-support norm. We derive an efficient method to compute the proximal operator for $k$-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the $k$-support norm. The experimental results confirm our theoretical analysis.", "histories": [["v1", "Fri, 20 Jun 2014 07:11:44 GMT  (61kb,D)", "https://arxiv.org/abs/1406.5291v1", null], ["v2", "Tue, 8 Jul 2014 14:53:00 GMT  (61kb,D)", "http://arxiv.org/abs/1406.5291v2", "Added acknowledgements section"], ["v3", "Mon, 2 Feb 2015 17:54:14 GMT  (78kb,D)", "http://arxiv.org/abs/1406.5291v3", "Updates to bound"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["soumyadeep chatterjee", "sheng chen", "arindam banerjee"], "accepted": true, "id": "1406.5291"}, "pdf": {"name": "1406.5291.pdf", "metadata": {"source": "CRF", "title": "Generalized Dantzig Selector: Application to the k-support norm", "authors": ["Soumyadeep Chatterjee", "Sheng Chen"], "emails": ["chatter@cs.umn.edu", "shengc@cs.umn.edu", "banerjee@cs.umn.edu"], "sections": [{"heading": "1 Introduction", "text": "There is an alternative to regularized regression approaches such as Lasso [16, 19] for an economical estimate. While DS does not consider regularized maximum probability approaches, the literature on DS has primarily found clear similarities between the estimates of DS and Lasso. While the normative regression approaches have been generalized to more general norms, such as decomposable norms [11], the literature on DS has primarily focused on the sparse L1 case, with a few notable exceptions that have considered extensions to sparse group-structured norms [8]. In this paper, we look at linear models of the form y = XTB + w, where y-Rn is a series of observations piepiepiepe.X-Rn is a design matrix, and w-Rn is i.i.i.d. For each given standard R (\u00b7), the parameters regarding structural distribution are assumed."}, {"heading": "2 General Optimization and Statistical Recovery Guarantees", "text": "The problem in (1) is a convex program, and a suitable selection of \u03bbp ensures that the realizable amount is not empty. We start the section with an imprecise ADMM framework for solving problems in the form (1) and then present limits on the estimation error that defines the statistical consistency of GDS."}, {"heading": "2.1 General Optimization Framework using Inexact ADMM", "text": "We leave A = XTX, u = XTy, and define the set C\u03bb = {V: R \u00b2 (V). The optimization problem is equivalent to a generally applicable algorithm that replaces the method of multipliers (ADMM). (2) The augmented Lagrangian function for (2) is given asLR (V, z) = R < z, Anel + V > + A, which is the method of multipliers (ADMM). (3) The augmented Lagrangian function for (2) is given asLR (V, z) = R (Z)."}, {"heading": "2.2 Statistical Recovery for Generalized Dantzig Selector", "text": "& & # 8220; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & & # 10; & & & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & & # 10; & # 10; & & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & & & # 10; & & & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; &"}, {"heading": "3.1 Computation of Proximal Operator", "text": "To solve the problem (23), we must apply either the non-linear equation (2) or the non-linear equation (2) (2). (3) The existing methods [1, 10] are not applicable to our scenario, since they calculate the prospective operator for squared k-supporting norm from which the proxICs (2) cannot be calculated directly. (3) In Theorem 2, we show that the proxICs (2) can be calculated effectively, and therefore the algorithm 1 is applicable. Theorem 2 Given p > 0 and x-Rp, if the ProxICs (3) do not exist, then w-Rp = proxICs (x) = x-Raps of the ProxICs (x) = x-Raps in which the ProxICs (x) do exist."}, {"heading": "4 Experimental Results", "text": "On the optimization side, our ADMM framework focuses on its universality, and its efficiency has been demonstrated in [18] for the specific case of the L1 standard. Therefore, we focus on the efficiency of various proximal operators related to the k support standard. On the statistical side, we focus on the behavior and performance of GDS with the k support standard. All experiments are conducted in MATLAB."}, {"heading": "4.1 Efficiency of Proximal Operator", "text": "We tested four proximal operators in connection with the k-base standard, which are our normal proxIC\u03bb (\u00b7) and its accelerated version, prox 12\u03b2 (proxIC\u03bb) (proxIC\u03bb) (proxIC\u03bb) and its accelerated version, prox 12\u03b2 (proxIC\u03bb) (proxIC\u03bb) in [1], and prox\u043c 2 proximal proximal proximal proximal proximal proximal proximal proximal proximal (\u00b7) in [1], and prox\u043c 2 proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proximal proxim"}, {"heading": "4.2 Statistical Recovery on Synthetic Data", "text": "We fixed p = 600, and we investigated how the L2 error came about to get the ROC diagram for k = {1, 10, 50} as shown in Figure 2 (a)."}, {"heading": "5 Conclusions", "text": "In this thesis, we presented the GDS, which generalizes the standard L1 standard Dantzig Selector to estimated values with each standard, so that structural information encoded in the standard can be efficiently used. To solve the GDS, a flexible framework is proposed based on imprecise ADMM, with only one of the conjugated proximal operators having to be efficiently solved. Furthermore, we provide a uniform statistical analytical framework for the GDS, which uses Gaussian widths of certain limited sets to prove consistency. In the non-trivial example of the k support standard, we showed that the proximal operators used in the imprecise ADMM can be calculated more efficiently than previously proposed variants. Our statistical analysis for the k support standard provides the first result of the consistency of this structured standard. Finally, experimental results provided solid support for the theoretical development of the work."}, {"heading": "Acknowledgements", "text": "The research was supported by NSF grant IIS-1029711, NSF grant IIS0916750, IIS-0953274 and CNS-1314560, and NASA grant NNX12AQ39A. A. B. acknowledges the support of IBM and Yahoo."}, {"heading": "A Proof of Theorem 1", "text": "Theorem statement: Suppose the design matrix X consists of i.i.d. Gaussian entries with zero mean variance 1, and we solve the optimization problem (1) with the highest probability. (34) Then, with a probability of at least (1 \u2212 1 exp (\u2212 2n)), we have the answer to the optimization problem (1 \u2212 2) with the highest probability. (34) Then, with the highest probability (1 \u2212 1), the Gaussian width of the intersection of TR (both) and the unit spherical shell Sp \u2212 1,. (44) is the Gaussian width of the unit norm ball, listed L > 0 is the Gaussian width of the unit norm ball, listed L > 0 is the Gaussian width of the GR ('n \u2212 2)."}, {"heading": "B Proof of Theorem 2", "text": "Theorists \"statement: Given that this is a case of a non-linear equation of (54) if s > 0 and root 0 otherwise exist. (55) Then the probable operator w = proxICp (x) is given by the proxicon (x) by the proxicon (x) by the proxicon (x) by the proxicon (x) by the proxicon (x) by the proxicon (x). (54) Leave the non-negative root of (54) if s > 0 and root 0 otherwise exist. (55) Then the probable operator w = proxikon is given by the proxicon (x) by the proxicon (x) by the proxicon (x) by the proxicon (x) by the proxicon (x)."}, {"heading": "C Proof of Theorem 3", "text": "To prove this, we must first specify the following basic data from Theorem 2.Corollary 2 (= > Part 2). < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p: p \u2212 p \u2212 p p \u2212 p p \u2212 p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p >"}, {"heading": "D Proof of Theorem 4", "text": "Theorem statement: For k-supporting norm j, thatG (k) = Gj-supporting norm j, for Gj-supporting norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm j, for Gj-norm y."}], "references": [{"title": "Sparse prediction with the k-support norm", "author": ["Andreas Argyriou", "Rina Foygel", "Nathan Srebro"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["Peter J Bickel", "Ya\u2019acov Ritov", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The Dantzig selector: Statistical estimation when p is much larger than n", "author": ["Emmanuel Candes", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "The convex geometry of linear inverse problems", "author": ["Venkat Chandrasekaran", "Benjamin Recht", "Pablo A Parrilo", "Alan S Willsky"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A generalized dantzig selector with shrinkage", "author": ["Gareth M. James", "Peter Radchenko"], "venue": "tuning. Biometrika,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Dasso: connections between the dantzig selector and lasso", "author": ["Gareth M. James", "Peter Radchenko", "Jinchi Lv"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Probability in Banach Spaces: isoperimetry and processes, volume 23", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "The group dantzig selector", "author": ["Han Liu", "Jian Zhang", "Xiaoye Jiang", "Jun Liu"], "venue": "JMLR Proceedings,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "An alternating direction method for finding dantzig selectors", "author": ["Zhaosong Lu", "Ting Kei Pong", "Yong Zhang"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "New Perspectives on k-Support and Cluster Norms", "author": ["A.M. McDonald", "M. Pontil", "D. Stamos"], "venue": "ArXiv e-prints,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Universal measurement bounds for structured sparse signal recovery", "author": ["Nikhil S Rao", "Ben Recht", "Robert D Nowak"], "venue": "In AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "On sparse reconstruction from fourier and gaussian measurements", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Galois Theory, Third Edition", "author": ["I. Stewart"], "venue": "Chapman Hall/CRC Mathematics Series. Taylor & Francis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Bregman Alternating Direction Method of Multipliers", "author": ["H. Wang", "A. Banerjee"], "venue": "ArXiv e-prints,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "The linearized alternating direction method of multipliers for dantzig selector", "author": ["Xiangfeng Wang", "Xiaoming Yuan"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "On model selection consistency of lasso", "author": ["Peng Zhao", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.", "startOffset": 41, "endOffset": 47}, {"referenceID": 2, "context": "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.", "startOffset": 41, "endOffset": 47}, {"referenceID": 14, "context": "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.", "startOffset": 123, "endOffset": 131}, {"referenceID": 17, "context": "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.", "startOffset": 123, "endOffset": 131}, {"referenceID": 1, "context": "While DS does not consider a regularized maximum likelihood approach, [2] has established clear similarities between the estimates from DS and Lasso.", "startOffset": 70, "endOffset": 73}, {"referenceID": 10, "context": "While norm regularized regression approaches have been generalized to more general norms, such as decomposable norms [11], the literature on DS has primarily focused on the sparse L1 norm case, with a few notable exceptions which have considered extensions to sparse group-structured norms [8].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "While norm regularized regression approaches have been generalized to more general norms, such as decomposable norms [11], the literature on DS has primarily focused on the sparse L1 norm case, with a few notable exceptions which have considered extensions to sparse group-structured norms [8].", "startOffset": 290, "endOffset": 293}, {"referenceID": 2, "context": "If R(\u00b7) is the L1 norm, (1) reduces to standard DS [3].", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "It is instructive to contrast GDS with the recently proposed atomic norm based estimation framework [4] which, unlike GDS, considers constraints based on the L2 norm of the error \u2016y \u2212X\u03b8\u20162, and focuses only on atomic norms.", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "For the L1-norm Dantzig selector, [3] proposed a primal-dual interior point method since the optimization is a linear program.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "DASSO and its generalization proposed in [6, 5] focused on homotopy methods, which provide a piecewise linear solution path through a sequential simplex-like algorithm.", "startOffset": 41, "endOffset": 47}, {"referenceID": 4, "context": "DASSO and its generalization proposed in [6, 5] focused on homotopy methods, which provide a piecewise linear solution path through a sequential simplex-like algorithm.", "startOffset": 41, "endOffset": 47}, {"referenceID": 8, "context": "In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient.", "startOffset": 124, "endOffset": 131}, {"referenceID": 16, "context": "In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient.", "startOffset": 124, "endOffset": 131}, {"referenceID": 16, "context": "In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient.", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "Motivated by such results for DS, we propose a general inexact ADMM [17] framework for GDS where the primal update steps, interestingly, turn out respectively to be proximal updates involving R(\u03b8) and its convex conjugate, the indicator of R\u2217(x) \u2264 \u03bb.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "Interestingly, the bound depends on the Gaussian width of the unit norm ball ofR(\u00b7) as well as the Gaussian width of suitable set where the estimation error belongs [4, 13].", "startOffset": 165, "endOffset": 172}, {"referenceID": 11, "context": "Interestingly, the bound depends on the Gaussian width of the unit norm ball ofR(\u00b7) as well as the Gaussian width of suitable set where the estimation error belongs [4, 13].", "startOffset": 165, "endOffset": 172}, {"referenceID": 0, "context": "As a non-trivial example of the GDS framework, we consider estimation using the recently proposed k-support norm [1, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 9, "context": "As a non-trivial example of the GDS framework, we consider estimation using the recently proposed k-support norm [1, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 0, "context": "Note that existing work [1, 10] on k-support norm has focused on the proximal operator for the square of the k-support norm, which is not directly applicable in our setting.", "startOffset": 24, "endOffset": 31}, {"referenceID": 9, "context": "Note that existing work [1, 10] on k-support norm has focused on the proximal operator for the square of the k-support norm, which is not directly applicable in our setting.", "startOffset": 24, "endOffset": 31}, {"referenceID": 16, "context": "Inspired by [18], we consider a simpler subproblem for the \u03b8-update which minimizes L\u0303R(\u03b8,v, z) = R(\u03b8) + \u3008z,A\u03b8 + v \u2212 u\u3009+ \u03c1 2 (\u2225\u2225A\u03b8k + v \u2212 u\u2225\u22252 2 +", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Remark on convergence: Note that Algorithm 1 is a special case of inexact Bregman ADMM proposed in [17], which matches the case of linearizing quadratic penalty term by using B\u03c6\u03b8(\u03b8,\u03b8k) = 1 2\u2016\u03b8 \u2212 \u03b8k\u20162 as Bregman divergence.", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "In order to converge, the algorithm requires \u03bc2 to be larger than the spectral radius of ATA, and the convergence rate is O(1/T ) according to Theorem 2 in [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "For any set \u03a9 \u2286 Rp, we would measure the size of this set using its Gaussian width [14, 4], which is defined as \u03c9(\u03a9) = Eg [supz\u2208\u03a9\u3008g, z\u3009] , where g is a vector of i.", "startOffset": 83, "endOffset": 90}, {"referenceID": 3, "context": "For any set \u03a9 \u2286 Rp, we would measure the size of this set using its Gaussian width [14, 4], which is defined as \u03c9(\u03a9) = Eg [supz\u2208\u03a9\u3008g, z\u3009] , where g is a vector of i.", "startOffset": 83, "endOffset": 90}, {"referenceID": 16, "context": "Similar updates were used in [18] for L1-norm Dantzig selector.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "It was shown in [4] that the Gaussian width of the set (TL1(\u03b8) \u2229 Sp\u22121) is upper bounded as \u03c9(TL1(\u03b8)\u2229 Sp\u22121)2 \u2264 2s log (p s ) + 54s.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "standard Gaussian entries [3].", "startOffset": 26, "endOffset": 29}, {"referenceID": 10, "context": "Further, [11] has shown that \u03a8R = \u221a s.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "with high probability, which agrees with known results for DS [2, 3].", "startOffset": 62, "endOffset": 68}, {"referenceID": 2, "context": "with high probability, which agrees with known results for DS [2, 3].", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": "In previous work [1, 10], the k-support norm is defined as", "startOffset": 17, "endOffset": 24}, {"referenceID": 9, "context": "In previous work [1, 10], the k-support norm is defined as", "startOffset": 17, "endOffset": 24}, {"referenceID": 0, "context": "It was shown in [1] to behave similarly as the elastic net in the sense that the unit norm ball of the k-support norm is within a constant factor of \u221a 2 of the unit elastic net ball.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "Existing methods [1, 10] are inapplicable to our scenario since they compute the proximal operator for squared k-support norm, from which IC\u03bb (\u00b7) cannot be directly obtained.", "startOffset": 17, "endOffset": 24}, {"referenceID": 9, "context": "Existing methods [1, 10] are inapplicable to our scenario since they compute the proximal operator for squared k-support norm, from which IC\u03bb (\u00b7) cannot be directly obtained.", "startOffset": 17, "endOffset": 24}, {"referenceID": 13, "context": "Remark: The nonlinear equation (24) is quartic, for which we can use general formula to get all the roots [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "Compared with previous proximal operators for squared k-support norm, this complexity is better than that in [1], and roughly the same as the most recent one in [10].", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "Compared with previous proximal operators for squared k-support norm, this complexity is better than that in [1], and roughly the same as the most recent one in [10].", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "We prove these two bounds using the analysis technique for group lasso with overlaps developed in [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "4 Experimental Results On optimization side, our ADMM framework is concentrated on its generality, and its efficiency has been shown in [18] for the special case of L1 norm.", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "1 Efficiency of Proximal Operator We tested four proximal operators related to k-support norm, which are our normal IC\u03bb (\u00b7) and its accelerated version, prox 1 2\u03b2 (\u2016\u00b7\u2016 k )2 (\u00b7) in [1], and prox\u03bb 2 \u2016\u00b7\u2016\u0398 (\u00b7) in [10].", "startOffset": 180, "endOffset": 183}, {"referenceID": 9, "context": "1 Efficiency of Proximal Operator We tested four proximal operators related to k-support norm, which are our normal IC\u03bb (\u00b7) and its accelerated version, prox 1 2\u03b2 (\u2016\u00b7\u2016 k )2 (\u00b7) in [1], and prox\u03bb 2 \u2016\u00b7\u2016\u0398 (\u00b7) in [10].", "startOffset": 209, "endOffset": 213}], "year": 2015, "abstractText": "We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS, and non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian width of unit norm ball and suitable set encompassing estimation error. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.", "creator": "LaTeX with hyperref package"}}}