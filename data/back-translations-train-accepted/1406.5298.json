{"id": "1406.5298", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2014", "title": "Semi-supervised Learning with Deep Generative Models", "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.", "histories": [["v1", "Fri, 20 Jun 2014 07:52:18 GMT  (5762kb,D)", "http://arxiv.org/abs/1406.5298v1", null], ["v2", "Fri, 31 Oct 2014 22:43:31 GMT  (4748kb,D)", "http://arxiv.org/abs/1406.5298v2", "To appear in the proceedings of Neural Information Processing Systems (NIPS) 2014"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["diederik p kingma", "shakir mohamed", "danilo jimenez rezende", "max welling"], "accepted": true, "id": "1406.5298"}, "pdf": {"name": "1406.5298.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Learning with Deep Generative Models", "authors": ["Diederik P. Kingma", "Danilo J. Rezende", "Shakir Mohamed"], "emails": ["M.Welling}@uva.nl", "shakir}@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own, \"he said in an interview with The New York Times, addressing the question of how the situation came about.\" I don't think it's going to come to that, \"he said.\" I don't think it's going to come to that, \"he said.\" But I think it's going to come to that. \""}, {"heading": "2 Deep Generative Models for Semi-supervised Learning", "text": "We are confronted with data that appear as pairs (X, Y) = {(x (1), y (1) = finite number),.., (x), y (N), n), with the nth observation x (n). We are now developing two models that use generative descriptions of the data to improve the classification performance of the alone.Latent feature discriminatory model (M1): A commonly used approach is to construct a model of the data x (n) that provides an embedding or representation of the data, and the use of these features is trained."}, {"heading": "3 Scalable Semi-supervised Variational Inference", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Variational Free Energy Objective", "text": "Given the specification of the two models in the previous section, we can learn conclusions and parameters (using the Limit Probability as an objective function that includes integration via the latent variables). To enable tractability, we instead follow the variation principle of introducing an approximate rear distribution q\u03c6 (z (n) | x (n) with variation parameters (s) and obtain a lower limit on the Limit Probability by using this target for optimization instead. By conditioning the posterior approximation to the datapoint, we avoid variation parameters via datapoint rather than requiring an adjustment to global variation parameters."}, {"heading": "3.2 Efficiency with an Inference Network", "text": "The implication of using a parametric variation distribution is: faster convergence during training and faster inference at test points, since we only need a single pass through the inference mesh (as opposed to VEM, where we repeat the generalized E-step optimization for each test data point). For the model (M1), we choose the inference mesh according to beq\u03c6 (z | x) = N (z | \u00b5 (x), R (x) R (x) >) (8), where R (x) is either a diagonal matrix or a structured covariance consisting of a rank-one matrix with a diagonal correction term, as this allows a principal direction of the correlation x x x, with Wamex x to be taken into account as a linear cost."}, {"heading": "3.3 Free Energy Optimisation", "text": "We use the objective functions (3) and (7) to optimize the parameters of the generative model (11) and the parameters of the variable distributions (11) to enable efficient optimization. We describe the core strategy for the latent-differentiating model (M1), since the same calculations are also used for the conditional generative model (M2). If the previous p (z) is a spherical Gaussian distribution p (z) = N (z), we describe the core strategy for the latent-differentiating model (z), since the same calculation is also used for the conditional generative model (M2). If the previous p (z) is a spherical Gaussian distribution p (z), we can use the core strategy for the latent distribution p (z) = N (z) and the variable distribution qp (z) x, a Gaussian distribution as in (9) is also used, the KL-term can be rewritten into the apparent equation (Loz) and the equation can be analyzed."}, {"heading": "3.4 Computational Complexity", "text": "The general algorithmic complexity of a single common update of the parameters (\u03b8, \u03c6) for the model (M1) using the estimators (14) and (15) is the CM1 = MSCMLP, where M is the minibatch size used for the gradient estimation, S is the number of samples per data point and CMLP is the cost of evaluating the MLPs in the conditional distribution (x | z) and q\u03c6 (z | x) once per data point and z sample. The cost of CMLP is the form O (KD2), where K is the total number of layers and D is the average dimension of the layers of MLPs in the model. Formation of the model (M1) also requires a supervisor classification at the top, which will have its own algorithmic complexity. For example, if it is a neural, it will have a complexity of the form CMLP."}, {"heading": "4 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Benchmark Classification", "text": "We test performance on the standard MNIST digit classification benchmark. In this benchmark, each model is trained on a partially labeled training set with different amounts of labeled instances randomly selected from the complete training set of 50,000 data points. We used a 50-dimensional latent variable z for all models. MLPs, which are part of the Likelihood function, as well as MLPs for the inference network, were constructed with one or two hidden layers, each of which has a reflected linear or softplus (logistic log partition) activation functionality. All parameters were randomly initiated from N, 0.012I by sampling, and we employed a small weight loss equivalent to a Gaussian one before N (0, I). Targets were optimized by using minibatch-based gradient estimates."}, {"heading": "4.2 Conditional Generation", "text": "The conditional generative model can be used to explore the underlying structure of the data, which we demonstrate through two forms of analog reasoning. First, we demonstrate style and content separation by fixing the class label y, and then we vary the latent variables z over a range of Figure 1: Handwriting styles for MNIST by varying the latent variables over a 2D network. Figure 2: Analog reasoning for MNIST. Figure 1 shows four MNIST classes in which a trained model with two latent variables is used, and the latent variables vary from -5 to 5."}, {"heading": "4.3 Image Classification", "text": "We demonstrate the performance of image classification on SVHN and NORB image sets. Since there are no comparative results in the semi-monitored environment, we perform the next neighbor and TSVM classification with RBF cores and compare performance on features generated by our latent-feature discriminatory model with the original features. The results are presented in Tables 2 and 3, and we demonstrate once again the effectiveness of our approach to semi-monitored classification. Table 3: Semi-monitored classification on the NORB dataset of 1000 labels. KNN TSVM M1 (KNN) M1 (TSVM) 78.71 26.00 65.39 18.79 (\u00b1 0.02) (\u00b1 0.06) (\u00b1 0.09) (\u00b1 0.05)"}, {"heading": "5 Discussion and Conclusion", "text": "We have developed new models of semi-supervised learning that allow us to improve the quality of prediction by using information in data density using generative models. We have developed an efficient variational optimization algorithm for approximate Bayesian conclusions in these models, and have shown that they are among the most competitive models currently available for semi-supervised learning. We hope that these results will stimulate the development of even more powerful semi-supervised classification methods based on generative models, of which there is still much scope. Approximate follow-up methods introduced here can easily be extended to the parameters of the model, harnessing the full power of variable learning. Such an extension also provides a principled terrain for model selection. Efficient model selection is particularly important when the amount of data available is not large, as in semi-supervised learning components."}], "references": [{"title": "Archipelago: nonparametric Bayesian semi-supervised learning", "author": ["R.P. Adams", "Z. Ghahramani"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "Adams and Ghahramani,? \\Q2009\\E", "shortCiteRegEx": "Adams and Ghahramani", "year": 2009}, {"title": "Semi-supervised learning using randomized mincuts", "author": ["A. Blum", "J. Lafferty", "M.R. Rwebangira", "R. Reddy"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Blum et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Semi-supervised learning in gigantic image collections", "author": ["R. Fergus", "Y. Weiss", "A. Torralba"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Fergus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fergus et al\\.", "year": 2009}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In Proceeding of the International Conference on Machine Learning (ICML),", "citeRegEx": "Joachims,? \\Q1999\\E", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Semi-supervised learning with trees", "author": ["C. Kemp", "T.L. Griffiths", "S. Stromsten", "J.B. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Kemp et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2003}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "A variational approach to semi-supervised clustering", "author": ["P. Li", "Y. Ying", "C. Campbell"], "venue": "In Proceedings of the European Symposium on Artificial Neural Networks (ESANN),", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "PhD thesis, Massachusetts Institute of Technology", "citeRegEx": "Liang,? \\Q2005\\E", "shortCiteRegEx": "Liang", "year": 2005}, {"title": "Graph-based semi-supervised learning for phone and segment classification", "author": ["Y. Liu", "K. Kirchhoff"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Liu and Kirchhoff,? \\Q2013\\E", "shortCiteRegEx": "Liu and Kirchhoff", "year": 2013}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Fast inference and learning with sparse belief propagation", "author": ["C. Pal", "C. Sutton", "A. McCallum"], "venue": "In Advances in Neural Information Processing Systems (NIPS). Citeseer", "citeRegEx": "Pal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pal et al\\.", "year": 2005}, {"title": "Semi-supervised learning of compact document representations with deep networks", "author": ["M. Ranzato", "M. Szummer"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML),", "citeRegEx": "Ranzato and Szummer,? \\Q2008\\E", "shortCiteRegEx": "Ranzato and Szummer", "year": 2008}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Semi-supervised self-training of object detection models", "author": ["C. Rosenberg", "M. Hebert", "H. Schneiderman"], "venue": "In Proceedings of the Seventh IEEE Workshops on Application of Computer Vision (WACV/MOTION\u201905)", "citeRegEx": "Rosenberg et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rosenberg et al\\.", "year": 2005}, {"title": "Semi-supervised learning improves gene expression-based prediction of cancer recurrence", "author": ["M. Shi", "B. Zhang"], "venue": null, "citeRegEx": "Shi and Zhang,? \\Q2011\\E", "shortCiteRegEx": "Shi and Zhang", "year": 2011}, {"title": "Tangent prop\u2013a formalism for specifying selected invariances in an adaptive network", "author": ["P. Simard", "B. Victorri", "Y. LeCun", "J.S. Denker"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Simard et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1991}, {"title": "Learning stochastic feedforward neural networks", "author": ["Y. Tang", "R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Tang and Salakhutdinov,? \\Q2013\\E", "shortCiteRegEx": "Tang and Salakhutdinov", "year": 2013}, {"title": "A rate distortion approach for semi-supervised conditional random fields", "author": ["Y. Wang", "G. Haffari", "S. Wang", "G. Mori"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical report,", "citeRegEx": "Zhu,? \\Q2006\\E", "shortCiteRegEx": "Zhu", "year": 2006}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J Lafferty"], "venue": "In Proceddings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Such problems are of immense practical interest in a wide range of applications, including image search (Fergus et al., 2009), genomics (Shi and Zhang, 2011), natural language parsing (Liang, 2005), and speech analysis (Liu and Kirchhoff, 2013), where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set.", "startOffset": 104, "endOffset": 125}, {"referenceID": 16, "context": ", 2009), genomics (Shi and Zhang, 2011), natural language parsing (Liang, 2005), and speech analysis (Liu and Kirchhoff, 2013), where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set.", "startOffset": 18, "endOffset": 39}, {"referenceID": 8, "context": ", 2009), genomics (Shi and Zhang, 2011), natural language parsing (Liang, 2005), and speech analysis (Liu and Kirchhoff, 2013), where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set.", "startOffset": 66, "endOffset": 79}, {"referenceID": 9, "context": ", 2009), genomics (Shi and Zhang, 2011), natural language parsing (Liang, 2005), and speech analysis (Liu and Kirchhoff, 2013), where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set.", "startOffset": 101, "endOffset": 126}, {"referenceID": 6, "context": "In this paper we answer this question by developing probabilistic models for inductive and transductive semi-supervised learning by utilising an explicit model of the data density, building upon recent advances in deep generative models and scalable variational inference, namely auto-encoding variational Bayes and stochastic backpropagation (Kingma and Welling, 2014; Rezende et al., 2014).", "startOffset": 343, "endOffset": 391}, {"referenceID": 13, "context": "In this paper we answer this question by developing probabilistic models for inductive and transductive semi-supervised learning by utilising an explicit model of the data density, building upon recent advances in deep generative models and scalable variational inference, namely auto-encoding variational Bayes and stochastic backpropagation (Kingma and Welling, 2014; Rezende et al., 2014).", "startOffset": 343, "endOffset": 391}, {"referenceID": 15, "context": "Amongst existing approaches, the simplest algorithm for semi-supervised learning is based on a self-training scheme (Rosenberg et al., 2005) where the the model is bootstrapped with additional labelled data obtained from its own highly confident predictions; this process being repeated until some termination condition is reached.", "startOffset": 116, "endOffset": 140}, {"referenceID": 4, "context": "Transductive SVMs (TSVM) (Joachims, 1999) extend SVMs with the aim of max-margin classification while ensuring that there are as few unlabelled observations near the margin as possible.", "startOffset": 25, "endOffset": 41}, {"referenceID": 1, "context": "Graph-based methods are amongst the most popular and aim to construct a graph connecting similar observations with label information propagating through the graph from labelled to unlabelled nodes by finding the minimum energy (MAP) configuration (Blum et al., 2004; Zhu et al., 2003).", "startOffset": 247, "endOffset": 284}, {"referenceID": 22, "context": "Graph-based methods are amongst the most popular and aim to construct a graph connecting similar observations with label information propagating through the graph from labelled to unlabelled nodes by finding the minimum energy (MAP) configuration (Blum et al., 2004; Zhu et al., 2003).", "startOffset": 247, "endOffset": 284}, {"referenceID": 3, "context": "Graph-based approaches are sensitive to the graph structure and require eigen-analysis of the graph Laplacian, which limits the scale to which these methods can be applied (though efficient spectral methods are now available (Fergus et al., 2009)).", "startOffset": 225, "endOffset": 246}, {"referenceID": 12, "context": "supervised learning by training feed-forward classifiers with an additional penalty from an autoencoder or other unsupervised embedding of the data (Ranzato and Szummer, 2008; Weston et al., 2012).", "startOffset": 148, "endOffset": 196}, {"referenceID": 20, "context": "supervised learning by training feed-forward classifiers with an additional penalty from an autoencoder or other unsupervised embedding of the data (Ranzato and Szummer, 2008; Weston et al., 2012).", "startOffset": 148, "endOffset": 196}, {"referenceID": 14, "context": "Currently, state-of-the-art results on a semi-supervised version of the MNIST benchmark data set have been achieved by the Manifold Tangent Classifier (MTC) (Rifai et al., 2011), which trains contrastive auto-encoders (CAEs) to learn the manifold on which the data lies, followed by an instance of TangentProp (Simard et al.", "startOffset": 157, "endOffset": 177}, {"referenceID": 17, "context": ", 2011), which trains contrastive auto-encoders (CAEs) to learn the manifold on which the data lies, followed by an instance of TangentProp (Simard et al., 1991) to train a classifier that is approximately invariant to local perturbations along the manifold.", "startOffset": 140, "endOffset": 161}, {"referenceID": 21, "context": "Existing generative approaches based on models such as Gaussian mixture or hidden Markov models (Zhu, 2006), have not been very successful due to the limited capacity and the need for many states to perform well.", "startOffset": 96, "endOffset": 107}, {"referenceID": 5, "context": "More recent solutions have used non-parametric density models, either based on trees (Kemp et al., 2003) or Gaussian processes (Adams and Ghahramani, 2009), but accurate inference and the scalability of these approaches is still lacking.", "startOffset": 85, "endOffset": 104}, {"referenceID": 0, "context": ", 2003) or Gaussian processes (Adams and Ghahramani, 2009), but accurate inference and the scalability of these approaches is still lacking.", "startOffset": 30, "endOffset": 58}, {"referenceID": 7, "context": "Variational approximations for semi-supervised clustering have also been explored previously (Li et al., 2009; Wang et al., 2009).", "startOffset": 93, "endOffset": 129}, {"referenceID": 19, "context": "Variational approximations for semi-supervised clustering have also been explored previously (Li et al., 2009; Wang et al., 2009).", "startOffset": 93, "endOffset": 129}, {"referenceID": 6, "context": "In both cases, exact inference will be intractable, but we exploit recent advances in variational inference (Kingma and Welling, 2014; Rezende et al., 2014) to efficiently obtain accurate posterior distributions for latent variables as well as to perform efficient parameter learning.", "startOffset": 108, "endOffset": 156}, {"referenceID": 13, "context": "In both cases, exact inference will be intractable, but we exploit recent advances in variational inference (Kingma and Welling, 2014; Rezende et al., 2014) to efficiently obtain accurate posterior distributions for latent variables as well as to perform efficient parameter learning.", "startOffset": 108, "endOffset": 156}, {"referenceID": 6, "context": "This optimisation can be done jointly, without resort to the variational EM algorithm, using the stochastic backpropagation technique introduced by Kingma and Welling (2014) and Rezende et al.", "startOffset": 148, "endOffset": 174}, {"referenceID": 6, "context": "This optimisation can be done jointly, without resort to the variational EM algorithm, using the stochastic backpropagation technique introduced by Kingma and Welling (2014) and Rezende et al. (2014), which we discuss when developing optimisation algorithms for these loss functions in section 3.", "startOffset": 148, "endOffset": 200}, {"referenceID": 6, "context": "the stochastic backpropagation technique described by Kingma and Welling (2014) and Rezende et al.", "startOffset": 54, "endOffset": 80}, {"referenceID": 6, "context": "the stochastic backpropagation technique described by Kingma and Welling (2014) and Rezende et al. (2014) to allow for efficient optimisation.", "startOffset": 54, "endOffset": 106}, {"referenceID": 2, "context": "During optimization we use the estimated gradients in conjunction with standard stochastic gradientbased optimization methods such as SGD, RMSprop or AdaGrad (Duchi et al., 2010).", "startOffset": 158, "endOffset": 178}, {"referenceID": 2, "context": "The objectives were optimized using minibatch-based gradient estimates together with AdaGrad (Duchi et al., 2010), with a learning rate selected from the set {0.", "startOffset": 93, "endOffset": 113}, {"referenceID": 14, "context": "Some of the best results currently are obtained by the manifold tangent classifier (MTC) (Rifai et al., 2011).", "startOffset": 89, "endOffset": 109}, {"referenceID": 19, "context": "We compare to a broad range of existing solutions in semi-supervised learning, in particular to classification using nearest neighbours (NN), support vector machines on the labelled set (SVM), the transductive SVM (TSVM), the Embedded neural networks Weston et al. (2012), and contractive auto-encoders (CAE).", "startOffset": 251, "endOffset": 272}, {"referenceID": 14, "context": "Some of the best results currently are obtained by the manifold tangent classifier (MTC) (Rifai et al., 2011). Results in the table for these methods are reproduced from Rifai et al. (2011). The latent-feature discriminative model (M1) performs better than other models based on simple embeddings of the data, demonstrating the effectiveness of the latent space in providing robust features that allow for easier classification.", "startOffset": 90, "endOffset": 190}, {"referenceID": 10, "context": "We also show a similar visualisation for the street view house numbers (SVHN) data set (Netzer et al., 2011), which consists of more than 70,000 images of house numbers, in figure 3 (top).", "startOffset": 87, "endOffset": 108}, {"referenceID": 18, "context": "The model used in this way also provides an alternative model to the stochastic feed-forward networks (SFNN) described by Tang and Salakhutdinov (2013). The performance of our model significantly improves on SFNN, since instead of an inefficient Monte Carlo EM algorithm relying on importance sampling, we are able to perform efficient joint inference that is easy to scale.", "startOffset": 122, "endOffset": 152}, {"referenceID": 11, "context": "For instance we could combine our method with the truncation algorithm suggested by Pal et al. (2005). The extension of our model to multi-label classification problems that is essential for image-tagging is also possible, but requires similar approximations to reduce the number of likelihood-evaluations per class.", "startOffset": 84, "endOffset": 102}, {"referenceID": 1, "context": "It would be desirable to have a single principled loss function similar to (Blum et al., 2004) or (Zhu et al.", "startOffset": 75, "endOffset": 94}, {"referenceID": 22, "context": ", 2004) or (Zhu et al., 2003).", "startOffset": 11, "endOffset": 29}], "year": 2014, "abstractText": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.", "creator": "LaTeX with hyperref package"}}}