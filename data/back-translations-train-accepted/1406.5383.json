{"id": "1406.5383", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2014", "title": "Noise-Adaptive Margin-Based Active Learning and Lower Bounds under Tsybakov Noise Condition", "abstract": "We present and analyze an adaptive margin-based algorithm that actively learns the optimal linear separator for multi-dimensional data. The algorithm has the capacity of adapting to unknown level of label noise in the underlying distribution, making it suitable for model selection under the active learning setting. Compared to other alternative agnostic active learning algorithms, our proposed method is much simpler and achieves the optimal convergence rate in query budget T and data dimension d, if logarithm factors are ignored. Furthermore, our algorithm can handle classification loss functions other than the 0-1 loss, such as hinge and logistic loss, and hence is computationally feasible.", "histories": [["v1", "Fri, 20 Jun 2014 13:42:30 GMT  (20kb,D)", "http://arxiv.org/abs/1406.5383v1", null], ["v2", "Fri, 6 Mar 2015 20:14:19 GMT  (130kb,D)", "http://arxiv.org/abs/1406.5383v2", null], ["v3", "Mon, 23 Nov 2015 23:09:47 GMT  (140kb,D)", "http://arxiv.org/abs/1406.5383v3", "16 pages, 2 figures. An abridged version to appear in Thirtieth AAAI Conference on Artificial Intelligence (AAAI), which is held in Phoenix, AZ USA in 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yining wang", "aarti singh"], "accepted": true, "id": "1406.5383"}, "pdf": {"name": "1406.5383.pdf", "metadata": {"source": "CRF", "title": "Noise-adaptive Margin-based Active Learning for Multi-dimensional Data", "authors": ["Yining Wang", "Aarti Singh"], "emails": ["ynwang.yining@gmail.com", "aartisingh@cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we look at the pool-based active learning environment, under which algorithms have access to a pool of blank examples drawn by the underlying margin, and have the ability to request labels of specific examples. It is hoped that by arranging label queries to most informative examples in a feedback-driven manner, we might be able to achieve significant improvements in sample complexity via passive learning algorithms. For example, that the problem of learning linear separators could be achieved, an exponential improvement in sample complexity, with the labels consistent with the underlying linear classifier. Although there are significant improvements under the realizable case, it is often the case that data labels are corrupted by noise noise, a popular assumption that characterizes noise in the underlying distribution."}, {"heading": "2 Pool-based active learning for multi-dimensional Data", "text": "In this section, we will present the multidimensional pool-based active learning problem discussed in this paper, introduce a non-adaptive active learning algorithm introduced in [2], present our proposed margin-based active learning algorithm, which is tuning-free as it adapts to label noise, and analyze the convergence rate of our proposed algorithm. Throughout the paper, we will use ln (x) to denote the natural logarithm of x and log (x) to denote log2 (x)."}, {"heading": "2.1 Problem setup and notations", "text": "In the pool-based active learning environment, an algorithm is given a pool of training data with labels that are not disclosed. < The algorithm then actively selects data points in the training pool to request their labels. This selection is feedback-driven and could be based on previously observed, labeled and / or unlabeled data points. In many cases, the active learning frame can significantly reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12]. We assume that the data points (x, y) are drawn from an unknown underlying distribution PX \u00b7 Y, with X being the instance space and Y the label space. Furthermore, the instances x are drawn in an i.d. manner in which we assume that X Rd is the standard ball in Rd and Y = {+ 1, \u2212 1}. We also assume that the underlying distribution is a uniform PX distribution across the learning ball in general terms, which is a unified distribution in Rd."}, {"heading": "2.2 A non-adaptive margin-based active learning algorithm", "text": "Before introducing our algorithm, we first present a non-adaptive margin-based algorithm that actively learns a classification hyper-level for multi-dimensional data. [2] The algorithm is derived from [2] and its pseudo-code is converted into algorithm 1.Algorithm 1A non-adaptive margin-based active learning algorithm for the unrealizable case 1: function NONADAPTIVEACTIVELEARNING (, \u03b4, s, {mk, bk, rk, k, k} k, Z +) 2: Initialize random w by selecting random w, w, w, 2 = 1. 3: Draw m1 examples from PXY and insert them into a workgroup W. 4: for k = 1 to s do 5: Find w, k, k, k, k, k, k, b) that finds the results from the work."}, {"heading": "2.3 Surrogate loss functions", "text": "Finding a linear classifier that minimizes the empirical classification error of 0 / 1 might be difficult because the 0 / 1 loss function is neither convex nor differentiable. Instead, people often try to minimize a replacement loss function on the training dataset. Popular choices are hinge loss, logistical loss, and exponential loss. In this paper, we consider replacement loss functions that fulfill the following two properties: (A1) There is a passive learning algorithm that achieves an excess loss without assuming TNC limited by an excess error, i.e. probability > 1 \u2212 g, err (w) \u2212 err (w) \u2212 err (w) \u2212 err (w). For some 0 < < < 1, which depends on an excess loss, n = T / E, i.e. w and w (w) minimizers of empirical and real environmental loss."}, {"heading": "2.4 An adaptive margin-based active learning algorithm", "text": "In this section, we present a margin-based active learning algorithm that adapts to different noise level parameters not included in the TNC condition. (1) The pseudo code of the proposed algorithm is represented in algorithm 2. (1) Algorithm 2 admits 4 parameters: d is the dimension of the instance space X; T is the sampling budget (i.e., the maximum number of allowed label requests is a confidence parameter; r (0, 1 / 2) is the shinkage rate for each iteration in the algorithm, and smaller r allows us to adjust to smaller alpha values. The basic idea of the algorithm is to divide T requests into e-labels, using the optimal passive learning procedure within each iteration and reducing the scope of searching for the best classifications."}, {"heading": "3 Proof of the main theorem", "text": "In this section, we first present a convergence rate analysis for the optimal passive learning procedure in Lemma 1 and 2. < < / p > p > p > p > p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p () p (0) p (0) p) p (0) p) p (0) p) p (0) p) p (0) p) p (0) p \"p (0) p) p\" p (0) p \"p (0) p) p\" p \"p (0) p) p) p () p) p () p) p (0 (0) p) p) p) p (0) p) p (0) p (0) p) p (0) p) p (0) p) p (0) p) p (0) p) p (0) p) p) p (0) p) p) p (0) p) p (0) p) p) p (0) p) p) p) p (0) p) p) p) p (0 (0) p) p) p) p) p) p) p) p) p (0 (0 (0) p) p) p) p) p) p) p (0 p) p) p) p) p) p) p) p) p) p) p (0 (0 (0 p) p) p) p) p) p) p (0 (0 p) p) p) p) p) p p p) p (0 p p) p p) p p p) p p p p) p (0 p) p p) p (0 p p) p) p) p) p) p) p p) p (p) p (p) p) p (p) p) p) p) p) p (p) p) p (p) p) p (p) p) p) p) p) p) p ("}, {"heading": "4 Discussion and remarks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Lower bounds on noisy adaptive learning:", "text": "Theorem 3 (Theorem 4.3, [13]) Suppose the underlying distribution PXY meets the condition of the TNC in Equation (1) with \u03b1 (0, 1). For each algorithm A that achieves an excessive error rate with probability 1 \u2212 \u03b4 for sufficiently small, g > 0, the number of queries executed should be at least T = \u0432 (\u2212 2\u03b1 (d + log (1 / \u03b4))). (9)"}, {"heading": "4.2 Comparison of adaptive active learning algorithms:", "text": "In [12] another noise-resistant adaptive learning algorithm is introduced, originally proposed in [9] and based on the concept of the discrepancy coefficient introduced in [11]; the algorithm adapts to the different sound level \u03b1 and achieves an excessive error rate of O ((\u03b8 (d log T + log (1 / \u03b4) T) 1 2\u03b1) (10) with probability 1 \u2212 \u03b4, where d is the underlying dimensionality, T is the sample query budget and \u03b8 the discrepancy coefficient. In our scenario, where X is the origin-centred unit sphere in Rd for d > 2, the hypothesis of class C contains all linear separators whose decision surface traverses the origin and PX is the uniform distribution, the dicongruence coefficient is satisfactory [11]."}, {"heading": "4.3 Connection to adaptive convex optimization:", "text": "Our proposed algorithm is inspired by an adaptive algorithm for stochastic convex optimization introduced in [14]. A function f is uniformly called convex on a closed convex set Q if there is \u03c1 \u2265 2 and \u00b5 \u2265 0, so that for all x, y, Q and \u03b1 [0, 1], f (\u03b1x + (1 \u2212 \u03b1) y) \u2264 \u03b1f (x) + (1 \u2212 \u03b1) f (y) \u2212 1 2 \u00b5\u03b1 (1 \u2212 \u03b1) x \u2212 y \u03c1. (11) Furthermore, if we say that function f is strongly convex. In [14] an adaptive stochastic optimization algorithm for uniformly and strongly convex functions was presented, the algorithm adapts to unknown convexity parameters \u03c1 and \u00b5 in Equation (11). [15] establishes a link between multidimensional stochastic convex-active optimization and oneditional active learning."}, {"heading": "4.4 Future work:", "text": "Finally, we mention two directions of future work: First, given query buddy T, our proposed algorithm adapts to an unknown level of label noise. It is an interesting problem whether there are adaptive active learning algorithms when the target error rate is given instead of query buddy T. Furthermore, algorithm 2 fails at \u03b1 = 0. Therefore, another possible direction for future work would be to design active learning algorithms that adapt to \u03b1 = 0 while maintaining the exponential improvement in convergence rate observed in this case in previous active learning research [2, 3].2The relationship between \u03b1 and \u03c1 can be explicitly established by noting \u03b1 = 1 \u2212 1 / \u03c1."}], "references": [{"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Margin based active learning", "author": ["M.-F. Balcan", "A. Broder", "T. Zhang"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["M.-F. Balcan", "P. Long"], "venue": "In COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Convexity, classification, and risk bounds", "author": ["P. Bartlett", "M. Jordan", "J. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Upper and lower error bounds for active learning", "author": ["R. Castro", "R. Nowak"], "venue": "In The 44th Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Analysis of perceptron-based active learning", "author": ["S. Dasgupta", "A. Kalai", "C. Monteleoni"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "The Annals of Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Primal-dual subgradient methods for minimizing uniformly convex functions", "author": ["A. Juditsky", "Y. Nesterov"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Algorithmic connections between active learning and stochastic convex optimization", "author": ["A. Ramdas", "A. Singh"], "venue": "In ALT,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Optimal rates for stochastic convex optimization under tsybakov noise condition", "author": ["A. Ramdas", "A. Singh"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Optimal rates for stochastic convex optimization under tsybakov noise condition", "author": ["A. Ramdas", "A. Singh"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Are loss functions all the same", "author": ["L. Rosasco", "E. De Vito", "A. Caponnetto", "M. Piana", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Active learning is an increasingly popular setting in machine learning that makes use of both labeled and unlabeled data [1, 6, 8].", "startOffset": 121, "endOffset": 130}, {"referenceID": 5, "context": "Active learning is an increasingly popular setting in machine learning that makes use of both labeled and unlabeled data [1, 6, 8].", "startOffset": 121, "endOffset": 130}, {"referenceID": 7, "context": "Active learning is an increasingly popular setting in machine learning that makes use of both labeled and unlabeled data [1, 6, 8].", "startOffset": 121, "endOffset": 130}, {"referenceID": 1, "context": "For instance, in the problem of learning linear separators, an exponential improvement in sample complexity could be achieved under the realizable case, where the labels are consistent with the underlying linear classifier [2, 3].", "startOffset": 223, "endOffset": 229}, {"referenceID": 2, "context": "For instance, in the problem of learning linear separators, an exponential improvement in sample complexity could be achieved under the realizable case, where the labels are consistent with the underlying linear classifier [2, 3].", "startOffset": 223, "endOffset": 229}, {"referenceID": 4, "context": "Unlike the realizable case, it is known that for most cases of the TNC condition only a polynomial achievement in sample complexity could be achieved [5, 13].", "startOffset": 150, "endOffset": 157}, {"referenceID": 4, "context": "Recently there has been substantial development of active learning algorithms for the non-realizable case with the TNC condition [5, 2, 3].", "startOffset": 129, "endOffset": 138}, {"referenceID": 1, "context": "Recently there has been substantial development of active learning algorithms for the non-realizable case with the TNC condition [5, 2, 3].", "startOffset": 129, "endOffset": 138}, {"referenceID": 2, "context": "Recently there has been substantial development of active learning algorithms for the non-realizable case with the TNC condition [5, 2, 3].", "startOffset": 129, "endOffset": 138}, {"referenceID": 0, "context": "Although there exist some general-purpose agnostic active learning algorithms that can adapt to some level of noise [1, 12, 9], such algorithms are often quite complicated, requiring computing some estimated bounds on classification error.", "startOffset": 116, "endOffset": 126}, {"referenceID": 11, "context": "Although there exist some general-purpose agnostic active learning algorithms that can adapt to some level of noise [1, 12, 9], such algorithms are often quite complicated, requiring computing some estimated bounds on classification error.", "startOffset": 116, "endOffset": 126}, {"referenceID": 8, "context": "Although there exist some general-purpose agnostic active learning algorithms that can adapt to some level of noise [1, 12, 9], such algorithms are often quite complicated, requiring computing some estimated bounds on classification error.", "startOffset": 116, "endOffset": 126}, {"referenceID": 12, "context": "The algorithm is inspired by recent work of adaptive algorithms for stochastic convex optimization [14] and one-dimensional active threshold learning [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "The algorithm is inspired by recent work of adaptive algorithms for stochastic convex optimization [14] and one-dimensional active threshold learning [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "Building on previous work [14, 15], we extend the method to multi-dimensional data and relax the constraint imposed on the TNC noise level parameter \u03b1 (earlier required to be \u2265 2).", "startOffset": 26, "endOffset": 34}, {"referenceID": 13, "context": "Building on previous work [14, 15], we extend the method to multi-dimensional data and relax the constraint imposed on the TNC noise level parameter \u03b1 (earlier required to be \u2265 2).", "startOffset": 26, "endOffset": 34}, {"referenceID": 13, "context": "Given the connection between convex optimization and active learning described in [15, 16], we conjecture that our techniques could also lead to a stochastic convex optimization algorithm that adapts to a broader class of objective functions.", "startOffset": 82, "endOffset": 90}, {"referenceID": 14, "context": "Given the connection between convex optimization and active learning described in [15, 16], we conjecture that our techniques could also lead to a stochastic convex optimization algorithm that adapts to a broader class of objective functions.", "startOffset": 82, "endOffset": 90}, {"referenceID": 1, "context": "In fact, it can be viewed as a modification of the non-adaptive margin-based algorithm introduced in [2] and does not require computing complex quantities such as upper and lower classification error bounds.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "In this section, we set up the multi-dimensional pool-based active learning problem considered in this paper and introduce a non-adaptive active learning algorithm presented in [2].", "startOffset": 177, "endOffset": 180}, {"referenceID": 0, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 5, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 7, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 4, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 11, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 6, "context": "We also assume that the underlying distribution PX is a uniform distribution over the unit ball in R, which is a setting commonly studied in the active learning literature [7, 8, 10].", "startOffset": 172, "endOffset": 182}, {"referenceID": 7, "context": "We also assume that the underlying distribution PX is a uniform distribution over the unit ball in R, which is a setting commonly studied in the active learning literature [7, 8, 10].", "startOffset": 172, "endOffset": 182}, {"referenceID": 9, "context": "We also assume that the underlying distribution PX is a uniform distribution over the unit ball in R, which is a setting commonly studied in the active learning literature [7, 8, 10].", "startOffset": 172, "endOffset": 182}, {"referenceID": 4, "context": "Various forms of the TNC condition for the one-dimensional and multi-dimensional active learning are explored in [5, 15, 2] and have been increasingly popular in the active learning literature.", "startOffset": 113, "endOffset": 123}, {"referenceID": 13, "context": "Various forms of the TNC condition for the one-dimensional and multi-dimensional active learning are explored in [5, 15, 2] and have been increasingly popular in the active learning literature.", "startOffset": 113, "endOffset": 123}, {"referenceID": 1, "context": "Various forms of the TNC condition for the one-dimensional and multi-dimensional active learning are explored in [5, 15, 2] and have been increasingly popular in the active learning literature.", "startOffset": 113, "endOffset": 123}, {"referenceID": 1, "context": "The algorithm is derived in [2] and its pseudocode is presented in Algorithm 1.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "The following theorem from [2] characterizes the convergence rate of Algorithm 1 and shows how the parameters (e.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Theorem 1 (Theorem 3, [2]).", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "Thm 8 in [2]) as well as hinge and logistic loss (c.", "startOffset": 9, "endOffset": 12}, {"referenceID": 16, "context": "[18]) with \u2032 =", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "This is true for some surrogate losses including hinge loss, logistic loss, etc [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "It remains an open problem whether there exists a tuning-free active learning algorithm when a target error rate instead of query budget T is given [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 1, "context": "Their proofs are partly inspired by the analysis in [2].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "Since \u03b8(\u0175k\u22121, \u0175k), \u03b8(\u0175k\u22121, w\u2217 k) \u2264 \u03b2k\u22121, by Lemma 7 in [2], we have Pr[(\u0175k\u22121 \u00b7 x)(\u0175k \u00b7 x) < 0, x \u2208 S2] \u2264 sin\u03b2k\u22121 C cos\u03b2k\u22121 ,", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Lemma 4 in [2] tells us that Pr[x \u2208 S1] \u2264 bk\u22121 \u221a d 2 \u221a \u03c0 .", "startOffset": 11, "endOffset": 14}, {"referenceID": 11, "context": "In [12] another noise-robust adaptive learning algorithm is introduced.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "The algorithm is originally proposed in [9] and is based on the concept of disagreement coefficient introduced in [11].", "startOffset": 40, "endOffset": 43}, {"referenceID": 10, "context": "The algorithm is originally proposed in [9] and is based on the concept of disagreement coefficient introduced in [11].", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "Under our scenario where X is the origin-centered unit ball in R for d > 2, the hypothesis class C contains all linear separators whose decision surface passes through the origin and PX is the uniform distribution, the diagreement coefficient \u03b8 satisfies [11] \u03c0 4 \u221a d \u2264 \u03b8 \u2264 \u03c0 \u221a d.", "startOffset": 255, "endOffset": 259}, {"referenceID": 2, "context": "Such improvements show the advantage of margin-based active learning and were also observed in [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 12, "context": "Our proposed algorithm is inspired by an adaptive algorithm for stochastic convex optimization presented in [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "A function f is called uniformly convex on a closed convex set Q if there exists \u03c1 \u2265 2 and \u03bc \u2265 0 such that for all x, y \u2208 Q and \u03b1 \u2208 [0, 1], f(\u03b1x+ (1\u2212 \u03b1)y) \u2264 \u03b1f(x) + (1\u2212 \u03b1)f(y)\u2212 1 2 \u03bc\u03b1(1\u2212 \u03b1)\u2016x\u2212 y\u2016.", "startOffset": 132, "endOffset": 138}, {"referenceID": 12, "context": "In [14] an adaptive stochastic optimization algorithm for uniformly and strongly convex functions was presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [15] a connection between multi-dimensional stochastic convex optimization and onedimensional active learning was established.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "(11) are closely related, and the exponents \u03b1 and \u03c1 are tied together in [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "In this paper, we extend the algorithms presented in [14, 15] to build an adaptive margin-based active learning for multi-dimensional data.", "startOffset": 53, "endOffset": 61}, {"referenceID": 13, "context": "In this paper, we extend the algorithms presented in [14, 15] to build an adaptive margin-based active learning for multi-dimensional data.", "startOffset": 53, "endOffset": 61}, {"referenceID": 1, "context": "Therefore, another possible direction of future work would be to design active learning algorithms that adapts to \u03b1 = 0 while still retaining the exponential improvement on convergence rate for this case, which is observed in previous active learning research [2, 3].", "startOffset": 260, "endOffset": 266}, {"referenceID": 2, "context": "Therefore, another possible direction of future work would be to design active learning algorithms that adapts to \u03b1 = 0 while still retaining the exponential improvement on convergence rate for this case, which is observed in previous active learning research [2, 3].", "startOffset": 260, "endOffset": 266}], "year": 2017, "abstractText": "We present and analyze an adaptive margin-based algorithm that actively learns the optimal linear separator for multi-dimensional data. The algorithm has the capacity of adapting to unknown level of label noise in the underlying distribution, making it suitable for model selection under the active learning setting. Compared to other alternative agnostic active learning algorithms, our proposed method is much simpler and achieves the optimal convergence rate in query budget T and data dimension d, if logarithm factors are ignored. Furthermore, our algorithm can handle classification loss functions other than the 0-1 loss, such as hinge and logistic loss, and hence is computationally feasible.", "creator": "LaTeX with hyperref package"}}}