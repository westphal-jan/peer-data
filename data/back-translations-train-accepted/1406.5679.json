{"id": "1406.5679", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2014", "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "abstract": "We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.", "histories": [["v1", "Sun, 22 Jun 2014 06:22:50 GMT  (5227kb,D)", "http://arxiv.org/abs/1406.5679v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["andrej karpathy", "armand joulin", "fei-fei li"], "accepted": true, "id": "1406.5679"}, "pdf": {"name": "1406.5679.pdf", "metadata": {"source": "CRF", "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "authors": ["Andrej Karpathy", "Armand Joulin", "Li Fei-Fei"], "emails": ["karpathy@cs.stanford.edu", "ajoulin@cs.stanford.edu", "feifeili@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The ability to associate natural language descriptions with images is of considerable value. Describing the image content is useful for automated captioning, and vice versa, the ability to retrieve images based on natural language queries has direct image search applications. Specifically, in this work we are interested in training a model based on a series of images and associated natural language descriptions so that we can later evaluate a fixed set of retained sentences based on an image query, and vice versa. This task is difficult because it requires a detailed understanding of the content of images, sentences, and their intermodal correspondence. Consider an example sentence query, such as \"A dog with a tennis ball swims in murky water\" (Figure 1). To successfully retrieve an appropriate image, we must accurately identify all the entities, attributes, and relationships that are present in the sentence, and ground them to a complex visual scenery.Our primary contribution consists of both a neural formulation for a strong spatial sentence, and a structured one in the target sentence."}, {"heading": "2 Related Work", "text": "There is a growing number of works linking images and sentences to each other. Some approaches focus on describing the contents of images that have been formulated either as a task of assigning images to a fixed set of people [5, 6], or as a task of automatically generating new labels [7, 8, 9, 10, 11, 12]. Closely related to our approach are methods that allow bi-rectional mapping between the two modalities. Socher and Fei-Fei [13] and Hodosh et al. [3] use Kernel Canonical Correlation Analysis to align images and sets, but theirar Xiv 6.56 79v1 [cs.CV] 2 2Ju n20 method is not easily scalable as it relies on the composition of cores on square images and sets."}, {"heading": "3 Proposed Model", "text": "Our task is to retrieve relevant images that describe their contents (Figure 2). Given this set of matches, we train the weights of a neural network to achieve a high score when a compatible picture-set pair is fed through the network, and a low score. Once the training is complete, all training data is discarded and the network evaluated using a series of test images and sets. The evaluation will evaluate all picture-set pairs, sort images / sets to reduce the score, and record the position of a basic truth in the list.Fragment embeddings Our core finding is that images are complex structures consisting of multiple interacting units to which the sentences form explicit references. We capture this intuition directly in our model by splitting both images and sets into fragments."}, {"heading": "3.1 Dependency Tree Relations as Sentence Fragments", "text": "Using the example in Figure 2, for example, we would like to identify the entities (dog, child) and characterize their attributes (black, young) and their pair-by-pair interactions (hunting). Inspired by previous work [5, 21], we find that a dependency tree of a sentence provides a rich set of dependency relationships that can serve this purpose more effectively than single words or bigrams. We discard the tree structure in favor of a simpler model and interpret each relationship (edge) as an individual sentence fragment (Figure 2, right shows 5 example dependency relationships). Therefore, we represent each word using a vector with 1-of-k coding w with a dictionary of 400,000 words and represent each dependency triplet (R, w1, w2) as an individual sentence fragment (Figure 2, right shows 5 example dependency relationships)."}, {"heading": "3.2 Object Detections as Image Fragments", "text": "Inspired by previous work [7], our modeling assumption is that the subject of most sentence descriptions are attributes of objects and their context in a scene. Of course, this motivates the use of objects and the global context as fragments of an image. In particular, we follow Girshick et al. [26] and recognize objects in each image with a Region Convolutional Neural Network (RCNN). CNN is pre-trained on ImageNet [36] and fine-tuned to the 200 classes of the ImageNet Detection Challenge [37]. We use the best 19 detected locations and the entire image as image fragments and calculate the embedding vectors based on the pixels Ib within each bounding box as follows: v = Wm [CNN\u03b8c (Ib)], (2) where CNN (Ib) considers the image as image fragments and calculates the embedding vectors based on the pixels Ib in each bounding box as follows."}, {"heading": "3.3 Objective Function", "text": "We are now ready to formulate the objective function. Let's remember that we get a set of N-images and corresponding sentences. In the previous sections, we described parameterized functions that assign each sentence and each image to a set of fragment vectors {s} or {v}, respectively. In Figure 2, our model interprets the inner product vTi sj between an image fragment vi and a sentence fragment sj as a similarity value. The similarity values for each image-sentence pair are in turn calculated as a fixed function of their paired fragment values. Intuitively, several matching fragments lead to a high image-sentence score. This is motivated by two criteria when designing the objective function: First, the image-sentence similarities should correspond to the basic truth. That is, corresponding picture-sentence pairs should have a higher score than all other image-sentence pairs. This is forced by the global ranking goal."}, {"heading": "3.3.1 Fragment Alignment Objective", "text": "The fragment Alignment Objective encodes the intuition that if a sentence contains a pair of columns (e.g., \"blue ball,\" Figure 3), at least one of the fields in the corresponding image should have a high score with that fragment, while all other fields in all other images that have no mention of the \"blue ball\" should have a low score. Finally, other images may contain the visual concept described, but its mention may be omitted from the associated sentence descriptions. Nevertheless, in many cases, the assumption is still fulfilled and can be used to formulate a cost function. Consider a (incomplete) fragment Alignment Objective, which requires a dense alignment between each corresponding image and the sentence fragments: C0."}, {"heading": "3.3.2 Global Ranking Objective", "text": "Remember that the Global Ranking Objective ensures that the calculated image-sentence similarities are consistent with the basic truth comments. First, we define the image-sentence value as being the average threshold of their pair-by-pair fragment ratings: Skl = 1 | gk | (| gl | + n) that it is the average threshold of their pair-by-pair fragment ratings. (6) Here, gk is the amount of image fragments in picture k and gl is the amount of sentence fragments in sentence l. Both k, l range from 1,.. In practice, we have found that it was helpful to add a smooth term n, because in the mi-SVM lens values greater than 0 are considered correct alignments and values smaller than 0 are considered as wrong sentence fragments in sentence l (i.e. wrong members of a positive bag). In practice, we have found that it was helpful to add a smooth term n, as otherwise short sentences may have found an advantage on Valin = that works well."}, {"heading": "3.4 Optimization", "text": "We use Stochastic Gradient Descent (SGD) with mini-batches of 100, a pulse of 0.9 and do 15 epochs through the training data. The learning rate is cross-validated and annealed for the last two epochs by a fraction of \u00d7 0.1. Since both Multiple Instance Learning and CNN Finetuning benefit from a good initialization, we perform the first 10 epochs with the fragment alignment target C0 and the CNN weights Bhc. After 10 epochs we switch to the full MIL target CF and start fine tuning CNN. The word embedding matrix We are fixed due to overfit concerns. Our implementation runs at about 1 second per batch on a standard CPU workstation."}, {"heading": "4 Experiments", "text": "We evaluate our picture set retrieval performance on Pascal1K [2], Flickr8K [3], and Flickr30K [4] datasets, each containing 1,000, 8,000, and 30,000 images, and each image is commented with Amazon Mechanical Turk with 5 independent sets. Sentence Data Preprocessing. We have not used any explicit filter, spell check, or normalization of any of the sentences for the sake of simplicity. We use Stanford's CoreNLP parser to calculate the dependency trees for each sentence. As there are many possible relationships (as many as hundreds), due to excessive concerns and practical considerations, we remove all relationship types that occur in each dataset less than 1% of the time. In practice, this reduces the number of relationships from 136 to 16 in Pascal1K, 170 to 17 in Flickr8K, and from 212 to 21 in Flickr30K. Additionally, all words that are not found in our dictionary of 400,000 dictionaries are compared to 33]."}, {"heading": "4.1 Comparison Methods", "text": "SDT-RNN. Socher et al. [21] embed a full-screen CNN representation in the sentence representation of a semantic dependency tree recursive neural network (SDT-RNN). Its loss is in line with our global ranking goal. We requested the source code of Socher et al. [21] and replaced the Flickr8K and Flick30K datasets. To better understand the benefits of using our evidence CNN and for a fairer comparison, we also train their method with our CNN features. Since we have multiple objects per image, we use the average representations of all objects with detection security above a (cross-validated) threshold. We refer to the exact method of Socher et al. [21] with a single full-screen CNN as \"Socher et al.,\" and to their method with our combined CNN features as \"SDTRNN.\" We were not able to prevent SDT-RNN on patches of several days because we evaluate the SEK in an average of 30K."}, {"heading": "4.2 Quantitative Evaluation", "text": "The quantitative results for Pascal1K, Flickr8K and Flickr30K are the same in Tables 1, 2 and 3 respectively. Our model outperforms previous methods. Our complete method consistently and significantly outperforms previous methods on Flickr8K (Table 2) and Flickr30K (Table 3) records. OnPascal1K (Table 1) the SDT RNN appears to be competitive in image search. Fragment and global targets are complementary. As seen in Tables 2 and 3, both targets perform well, but there is a noticeable improvement when the two are combined, suggesting that the targets bring complementary information to the cost function. Note that the Global Target consistently works slightly better, possibly because it directly minimizes the evaluation criteria (ranking costs), while the Alignment Objective fragment only works indirectly."}, {"heading": "4.3 Qualitative Experiments", "text": "Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Sets -Sets -Sets -Sets -Sets -Sets -Sets -Sets -Samples -Samples -Samples -Samples -Sets -Sets -Sets -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples-detectors -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -Samples -"}, {"heading": "5 Conclusions", "text": "Our neural network model learns a multimodal embedding space for fragments of images and sentences and reasons for their latent, intermodal alignment. Thinking at a finer level of image and sentence fragments allowed us to formulate a new fragment alignment objective that complements a more traditional concept of the global ranking objective. We have shown that our model significantly improves the query performance of image set queries compared to previous work. Our model also provides interpretable predictions. In future work, we hope to expand the model to support counting, thinking about spatial positions of objects and going beyond sacks of fragments."}], "references": [{"title": "Generating typed dependency parses from phrase structure parses", "author": ["M.C. De Marneffe", "B. MacCartney", "Manning", "C.D"], "venue": "Proceedings of LREC. Volume", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Framing image description as a ranking task: data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "I2t: Image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu"], "venue": "Proceedings of the IEEE", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "H. Daum\u00e9 III", "Y. Aloimonos"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "III Daum\u00e9"], "venue": "EACL", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Learning the visual interpretation of sentences", "author": ["C.L. Zitnick", "D. Parikh", "L. Vanderwende"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "In: NIPS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R.S. Zemel", "R. Salakhutdinov"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": "ICCV", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. De Freitas", "D.M. Blei", "M.I. Jordan"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Devise: A deep visualsemantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T Mikolov"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "TACL", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le"], "venue": "In: Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1311.2901", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks. In: ICLR", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Neural probabilistic language models. In: Innovations in Machine Learning", "author": ["Y. Bengio", "H. Schwenk", "J.S. Sen\u00e9cal", "F. Morin", "J.L. Gauvain"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "ICML", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "J. Krause", "A. Berg", "L. Fei-Fei"], "venue": "http://image-net.org/challenges/LSVRC/2013/", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "Wang", "J.Z.: Miles"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Multiple instance learning with generalized support vector machines", "author": ["S. Andrews", "T. Hofmann", "I. Tsochantaridis"], "venue": "AAAI/IAAI", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2002}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "http://caffe.berkeleyvision.org/", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Unlike previous work that embeds images and sentences, our model breaks down and embeds fragments of images (objects) and fragments of sentences (dependency tree relations [1]) in a common embedding space and explicitly reasons about their latent, inter-modal correspondences.", "startOffset": 172, "endOffset": 175}, {"referenceID": 1, "context": "In particular, we report dramatic improvements over state of the art methods on image-sentence retrieval tasks on Pascal1K [2], Flickr8K [3] and Flickr30K [4] datasets.", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "In particular, we report dramatic improvements over state of the art methods on image-sentence retrieval tasks on Pascal1K [2], Flickr8K [3] and Flickr30K [4] datasets.", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "In particular, we report dramatic improvements over state of the art methods on image-sentence retrieval tasks on Pascal1K [2], Flickr8K [3] and Flickr30K [4] datasets.", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "Some approaches focus on describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people [5, 6], or as a task of automatically generating novel captions [7, 8, 9, 10, 11, 12].", "startOffset": 152, "endOffset": 158}, {"referenceID": 5, "context": "Some approaches focus on describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people [5, 6], or as a task of automatically generating novel captions [7, 8, 9, 10, 11, 12].", "startOffset": 152, "endOffset": 158}, {"referenceID": 6, "context": "Some approaches focus on describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people [5, 6], or as a task of automatically generating novel captions [7, 8, 9, 10, 11, 12].", "startOffset": 216, "endOffset": 237}, {"referenceID": 7, "context": "Some approaches focus on describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people [5, 6], or as a task of automatically generating novel captions [7, 8, 9, 10, 11, 12].", "startOffset": 216, "endOffset": 237}, {"referenceID": 8, "context": "Some approaches focus on describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people [5, 6], or as a task of automatically generating novel captions [7, 8, 9, 10, 11, 12].", "startOffset": 216, "endOffset": 237}, {"referenceID": 9, "context": "Some approaches focus on describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people [5, 6], or as a task of automatically generating novel captions [7, 8, 9, 10, 11, 12].", "startOffset": 216, "endOffset": 237}, {"referenceID": 10, "context": "Some approaches focus on describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people [5, 6], or as a task of automatically generating novel captions [7, 8, 9, 10, 11, 12].", "startOffset": 216, "endOffset": 237}, {"referenceID": 11, "context": "Some approaches focus on describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people [5, 6], or as a task of automatically generating novel captions [7, 8, 9, 10, 11, 12].", "startOffset": 216, "endOffset": 237}, {"referenceID": 12, "context": "Socher and Fei-Fei [13] and Hodosh et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "[3] use Kernel Canonical Correlation Analysis to align images and sentences, but their", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "In sentences, fragments consist of typed dependency tree [1] relations.", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "[5] learn a common meaning space, but their method is limited to representing both images and sentences with a single triplet of (object, action, scene).", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] use a Conditional Random Field to reason about complex relationships of cartoon scenes and their natural language descriptions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Several probabilistic models for representing joint multimodal probability distributions over images and sentences have been developed, using Deep Boltzmann Machines [15], log-bilinear models [16], and topic models [17, 18].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "Several probabilistic models for representing joint multimodal probability distributions over images and sentences have been developed, using Deep Boltzmann Machines [15], log-bilinear models [16], and topic models [17, 18].", "startOffset": 192, "endOffset": 196}, {"referenceID": 16, "context": "Several probabilistic models for representing joint multimodal probability distributions over images and sentences have been developed, using Deep Boltzmann Machines [15], log-bilinear models [16], and topic models [17, 18].", "startOffset": 215, "endOffset": 223}, {"referenceID": 17, "context": "Several probabilistic models for representing joint multimodal probability distributions over images and sentences have been developed, using Deep Boltzmann Machines [15], log-bilinear models [16], and topic models [17, 18].", "startOffset": 215, "endOffset": 223}, {"referenceID": 18, "context": "[19] described an autoencoder that learns audio-video representations through a shared bottleneck layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20], who introduced a visual semantic embedding model that learns to map images and words to a common semantic embedding with a ranking cost.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] described a Dependency Tree Recursive Neural Network that puts entire sentences into correspondence with visual data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In Computer Vision, Convolutional Neural Networks (CNNs) [22] have recently been shown to learn powerful image representations that support state of the art image classification [23, 24, 25] and object detection [26, 27].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "In Computer Vision, Convolutional Neural Networks (CNNs) [22] have recently been shown to learn powerful image representations that support state of the art image classification [23, 24, 25] and object detection [26, 27].", "startOffset": 178, "endOffset": 190}, {"referenceID": 23, "context": "In Computer Vision, Convolutional Neural Networks (CNNs) [22] have recently been shown to learn powerful image representations that support state of the art image classification [23, 24, 25] and object detection [26, 27].", "startOffset": 178, "endOffset": 190}, {"referenceID": 24, "context": "In Computer Vision, Convolutional Neural Networks (CNNs) [22] have recently been shown to learn powerful image representations that support state of the art image classification [23, 24, 25] and object detection [26, 27].", "startOffset": 178, "endOffset": 190}, {"referenceID": 25, "context": "In Computer Vision, Convolutional Neural Networks (CNNs) [22] have recently been shown to learn powerful image representations that support state of the art image classification [23, 24, 25] and object detection [26, 27].", "startOffset": 212, "endOffset": 220}, {"referenceID": 26, "context": "In Computer Vision, Convolutional Neural Networks (CNNs) [22] have recently been shown to learn powerful image representations that support state of the art image classification [23, 24, 25] and object detection [26, 27].", "startOffset": 212, "endOffset": 220}, {"referenceID": 27, "context": "In language domain, several neural network models have been proposed to learn word/n-gram representations [28, 29, 30, 31, 32, 33], sentence representations [34] and paragraph/document representations [35].", "startOffset": 106, "endOffset": 130}, {"referenceID": 28, "context": "In language domain, several neural network models have been proposed to learn word/n-gram representations [28, 29, 30, 31, 32, 33], sentence representations [34] and paragraph/document representations [35].", "startOffset": 106, "endOffset": 130}, {"referenceID": 29, "context": "In language domain, several neural network models have been proposed to learn word/n-gram representations [28, 29, 30, 31, 32, 33], sentence representations [34] and paragraph/document representations [35].", "startOffset": 106, "endOffset": 130}, {"referenceID": 30, "context": "In language domain, several neural network models have been proposed to learn word/n-gram representations [28, 29, 30, 31, 32, 33], sentence representations [34] and paragraph/document representations [35].", "startOffset": 106, "endOffset": 130}, {"referenceID": 31, "context": "In language domain, several neural network models have been proposed to learn word/n-gram representations [28, 29, 30, 31, 32, 33], sentence representations [34] and paragraph/document representations [35].", "startOffset": 106, "endOffset": 130}, {"referenceID": 32, "context": "In language domain, several neural network models have been proposed to learn word/n-gram representations [28, 29, 30, 31, 32, 33], sentence representations [34] and paragraph/document representations [35].", "startOffset": 106, "endOffset": 130}, {"referenceID": 33, "context": "In language domain, several neural network models have been proposed to learn word/n-gram representations [28, 29, 30, 31, 32, 33], sentence representations [34] and paragraph/document representations [35].", "startOffset": 157, "endOffset": 161}, {"referenceID": 34, "context": "In language domain, several neural network models have been proposed to learn word/n-gram representations [28, 29, 30, 31, 32, 33], sentence representations [34] and paragraph/document representations [35].", "startOffset": 201, "endOffset": 205}, {"referenceID": 0, "context": "In particular, we propose to detect objects as image fragments and use sentence dependency tree relations [1] as sentence fragments (Figure 2).", "startOffset": 106, "endOffset": 109}, {"referenceID": 19, "context": "In previous related work [20, 21], Neural Networks embed images and sentences into a common space and the parameters are trained such that true image-sentence pairs have an inner product (interpreted as a score) higher than false image-sentence pairs by a margin.", "startOffset": 25, "endOffset": 33}, {"referenceID": 20, "context": "In previous related work [20, 21], Neural Networks embed images and sentences into a common space and the parameters are trained such that true image-sentence pairs have an inner product (interpreted as a score) higher than false image-sentence pairs by a margin.", "startOffset": 25, "endOffset": 33}, {"referenceID": 4, "context": "Inspired by previous work [5, 21] we observe that a dependency tree of a sentence provides a rich set of typed relationships that can serve this purpose more effectively than individual words or bigrams.", "startOffset": 26, "endOffset": 33}, {"referenceID": 20, "context": "Inspired by previous work [5, 21] we observe that a dependency tree of a sentence provides a rich set of typed relationships that can serve this purpose more effectively than individual words or bigrams.", "startOffset": 26, "endOffset": 33}, {"referenceID": 32, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Inspired by prior work [7], as a modeling assumption we observe that the subject of most sentence descriptions are attributes of objects and their context in a scene.", "startOffset": 23, "endOffset": 26}, {"referenceID": 25, "context": "[26] and detect objects in every image with a Region Convolutional Neural Network (RCNN).", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "The CNN is pre-trained on ImageNet [36] and finetuned on the 200 classes of the ImageNet Detection Challenge [37].", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "The CNN is pre-trained on ImageNet [36] and finetuned on the 200 classes of the ImageNet Detection Challenge [37].", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "It contains approximately 60 million parameters \u03b8c and closely resembles the architecture of Krizhevsky et al [24].", "startOffset": 110, "endOffset": 114}, {"referenceID": 37, "context": "We now describe a Multiple Instance Learning (MIL) [38] extension of the objective C0 that attempts to infer the latent alignment between fragments in corresponding image-sentence pairs.", "startOffset": 51, "endOffset": 55}, {"referenceID": 38, "context": "Our precise formulation is inspired by the mi-SVM [39], which is a simple and natural extension of a Support Vector Machine to a Multiple Instance Learning setting.", "startOffset": 50, "endOffset": 54}, {"referenceID": 38, "context": "This objective cannot be solved efficiently [39] but a commonly used heuristic is to set yij = sign(v T i sj).", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "We evaluate our image-sentence retrieval performance on Pascal1K [2], Flickr8K [3] and Flickr30K [4] datasets.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "We evaluate our image-sentence retrieval performance on Pascal1K [2], Flickr8K [3] and Flickr30K [4] datasets.", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "We evaluate our image-sentence retrieval performance on Pascal1K [2], Flickr8K [3] and Flickr30K [4] datasets.", "startOffset": 97, "endOffset": 100}, {"referenceID": 32, "context": "Additionally, all words that are not found in our dictionary of 400,000 words [33] are discarded.", "startOffset": 78, "endOffset": 82}, {"referenceID": 39, "context": "We use the Caffe [40] implementation of the ImageNet Detection RCNN model [26] to detect objects in all images.", "startOffset": 17, "endOffset": 21}, {"referenceID": 25, "context": "We use the Caffe [40] implementation of the ImageNet Detection RCNN model [26] to detect objects in all images.", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "[21] 23.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] 21.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "9 DeViSE [20] 17.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "5 SDT-RNN [21] 25.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "[21] 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "0 29 DeViSE [20] 4.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "6 29 SDT-RNN [21] 6.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "[3] 8.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The starred evaluation criterion (*) in [3] is slightly different since it discards 4,000 out of 5,000 test sentences.", "startOffset": 40, "endOffset": 43}, {"referenceID": 20, "context": "[21] and use 800 images for training, 100 for validation and 100 for testing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "For Flickr datasets we use 1,000 images for validation, 1,000 for testing and the rest for training (consistent with [3]).", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "[3] we closely follow their evaluation protocol and only keep a subset of N sentences out of total 5N (we use the first sentence out of every group of 5).", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[21] embed a fullframe CNN representation with the sentence representation from a Semantic Dependency Tree Recursive Neural Network (SDT-RNN).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] and substituted the Flickr8K and Flick30K datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] with a single fullframe CNN as \u201cSocher et al\u201d, and to their method with our combined CNN features as \u201cSDTRNN\u201d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "The DeViSE [20] source code is not publicly available but their approach is a special case of our method with the following modifications: we use the average (L2-normalized) word vectors as a sentence fragment, the average CNN activation of all objects above a detection threshold (as discussed in case of SDT-RNN) as an image fragment and only use the global ranking objective.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "0 500 DeViSE [20] 4.", "startOffset": 13, "endOffset": 17}], "year": 2014, "abstractText": "We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred intermodal fragment alignment is explicit.", "creator": "LaTeX with hyperref package"}}}