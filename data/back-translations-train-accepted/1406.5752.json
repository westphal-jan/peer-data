{"id": "1406.5752", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2014", "title": "Divide-and-Conquer Learning by Anchoring a Conical Hull", "abstract": "We reduce a broad class of machine learning problems, usually addressed by EM or sampling, to the problem of finding the $k$ extremal rays spanning the conical hull of a data point set. These $k$ \"anchors\" lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the $k$ anchors, we propose a novel divide-and-conquer learning scheme \"DCA\" that distributes the problem to $\\mathcal O(k\\log k)$ same-type sub-problems on different low-D random hyperplanes, each can be solved by any solver. For the 2D sub-problem, we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine for other methods to check whether a point is covered in a conical hull, which improves algorithm design in multiple dimensions and brings significant speedup to learning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering, then show its competitive performance and scalability over other methods on rich datasets.", "histories": [["v1", "Sun, 22 Jun 2014 19:16:20 GMT  (4847kb,D)", "http://arxiv.org/abs/1406.5752v1", "26 pages, long version, in updating"]], "COMMENTS": "26 pages, long version, in updating", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tianyi zhou", "jeff a bilmes", "carlos guestrin"], "accepted": true, "id": "1406.5752"}, "pdf": {"name": "1406.5752.pdf", "metadata": {"source": "CRF", "title": "Divide-and-Conquer Learning by Anchoring a Conical Hull", "authors": ["Tianyi Zhou", "Jeff Bilmes", "Carlos Guestrin"], "emails": ["guestrin}@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "In practice, however, sample moments usually suffer from large variables that can be easily categorized into space. [18] In addition, the results of matrix factorization [27, 35] are three algorithms commonly used to produce maximum probability (or maximum one posteriori (MAP)) estimates of models with latent variables / factors, and therefore are used in a wide range of applications such as clustering, theme modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis. However, their learning procedures are based on alternating optimizations / updates between parameters and latent variables suffering from local optimisation. Their quality strongly depends on the initialization and use of a large number of iterations for correct processing. [34] The method of moments [32, 7, 23] on the other hand, solves m equations by putting the first m moments of observation x Rp in relation to the estimators, resulting in a consistent global solution."}, {"heading": "2 General Separability Assumption and Minimum Conical Hull Problem", "text": "This means that each point can be represented as a convex combination of specific sub-areas that define the convex contour and the convex form of the convex. The dividing line can be defined both geometrically and algebraically. Convexia is a non-convexic convexia. The convexia of convexen is a convexic form of convexia. The convexia of convex is a convex form of convexia. The convexia of convex is a form of convex."}, {"heading": "2.1 General Separability Assumption and General Minimum Conical Hull Problem", "text": "By generalizing the separation assumption, we get a general minimum problem that can lead to a reduction in general learning processes, apart from NMF, i.e., latent variability and matrix, to find a set of \"anchor points.\" First, we all do not need to be negative, and we only need the points in XA and XS that we can pick from another group that is not necessary to set X itself. Second, we need to make sure that Connection (XA) is definitively generated, we only need the points in XA that we can pick from another group."}, {"heading": "3 Minimum Conical Hull Problem for General Learning Models", "text": "In this section we discuss how to reduce the learning of general models such as matrix factorization and latent variable model to the (general) minimum conical fuselage problem. Table 1 gives five examples to show how this general technique can be applied to specific models."}, {"heading": "3.1 Matrix Factorization", "text": "In addition to NMF, we consider more general matrix factorization models (MF), which work with negative characteristics and can specify a complicated structure of F. The MF X = FW is a deterministic latent variable model in which F and W are deterministic latent factors. By assigning a probability p (Xi, j | Fi, (WT) j) and the priors p (F) and p (W), its optimization model can be derived from maximum probability or MAP estimation. The resulting object is usually a loss function \"(\u00b7) of X \u2212 FW plus regulation terms for F and W, i.e. min\" (X, FW) + RF (F) + RW (W). Similar to separable NMF, the minimization of the target of general MF can be reduced to a minimum of conical envelope problems, selecting the subset A with X = FXA. In this setting, RW (W) can be reduced from k = 1 row w (W), with W (W) resulting in a row w = W (W)."}, {"heading": "3.2 Example: Subspace Clustering", "text": "Subspace clustering (SC) assumes that the data points in each cluster exist in a low-dimensional subspace. Subspaces defining different clusters are assumed to be distinguishable, e.g. with large principle angles between them. [36] SC surpasses traditional cluster methods in various tasks such as motion segmentation. Most existing subspace cluster methods [16, 36] rely on spectral clustering in the sparse representations of data points achieved by finding a sparse C with diag (C) = 0 in Model X = CX. This usually requires a series of time-consuming laser type optimizations. Then (spectral) clustering in lines of C guarantees that can lead to a reliable estimate of k subspaces. [36] Under the general assumption of separability, the SC model can be reduced to a minimum."}, {"heading": "3.3 Latent Variable Model", "text": "Unlike deterministic MF, we can build a system of equations from the moments of the probability variables, and then formulate it as a general minimal conical problem rather than solving it directly. If we leave the generalization model h-p (h; \u03b1) and x-p (x-h; \u03b8), where h is a latent variable, then x-x stands for observation, and {\u03b1, \u03b8} are parameters. In a variety of graphical models such as GMMs and HMMs, we must model conditional independence between groups of characteristics, also known as multiview assumptions. W.l.above. We assume that x is composed of three groups of characteristics."}, {"heading": "3.4 Example: Multi-view Mixture Model", "text": "We focus essentially on the way in which we learn (. \"), how we learn (.\"). \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\" \".\" \".\" \"\". \"\" \".\" \"\". \"\" \".\" \"\". \"\" \".\" \"\". \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\". \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\" \".\" \".\" \"\". \"\". \"\". \"\". \".\" \".\". \"\". \".\". \".\". \"\". \".\" \".\". \"\". \".\". \"\". \".\" \".\". \".\" \".\". \".\". \".\" \".\". \"\". \".\". \"\". \".\". \"\". \".\". \"\" \".\". \".\". \".\" \".\". \".\" \"\". \".\". \".\". \"\". \".\" \".\". \".\". \"\". \".\". \".\". \".\" \".\". \".\" \".\". \".\" \".\". \".\" \".\". \"\". \".\". \"\" \"\". \".\". \".\". \"\" \".\". \".\". \".\". \"\". \"\". \".\". \".\" \"\". \".\". \"\" \".\". \"\" \".\". \"\" \"\" \"\". \".\". \"\". \".\" \"\" \".\". \".\" \".\". \".\" \".\" \".\". \"\" \"\". \".\". \"\". \".\". \".\". \".\". \".\". \"\". \".\". \"\" \".\". \".\". \""}, {"heading": "3.5 Example: Hidden Markov Model", "text": "Hidden Markov model (HMM) is a latent variable model widely used to analyze sequential data and time series. HMM can be illustrated as a Markov chain of hidden states {ht} t \u00b2 [T] (ht = ei if the state is i), each ht generated the observation xt at time. Markov chain property implies that [t] j 6 = t, xt \u00b2 xj | ht, so multi-view assumption holds. The generalization process is h1 \u0445 \u03c0 \u0445 k \u2212 1, ht \u0445 Tht \u2212 1 and xt \u0445 p (xt | p, O), where T \u0445 Rk \u00b7 k is the transition matrix such that Ti, j = p (ht = ei = ej), and O \u00b2 Tht \u2212 k is the emission matrix such that E (xt | ht)."}, {"heading": "3.6 Example: Kalman Filter", "text": "If we have compared the discrete latent state h in HMM to a general latent problem, we can obtain a linear dynamic system (LDS) that is used both in filtering and smoothing time series. (D) If we apply the distribution p (wt) and p (vt) as a symmetric system, we have linearity E (ht \u2212 1) = Tht \u2212 1 and E (xt \u2212 ht) = hTt OT, which are analogous to HMM. Similarly, the multi-view or conditional independence automatically holds for such a Markov model, and it is also reasonable to assume that the goal of learning is to be appreciated."}, {"heading": "3.7 Example: Latent Dirichlet Allocation", "text": "The latent dirichlet allocation (LDA) is a latent variable model widely applied to word bag functions for text and vision data to extract semantic topics whose effectiveness has been proven in cluster and classification tasks. It generates the jth word xj = word components in a document by first forming a topic ratio h \u00b2 dir (\u03b1) (dirichlet distribution with parameters \u03b1 = {\u03b1i} i [k]) for the document, then a topic zj \u00b2 h from {ei} i [k] with associated probability \u03b2 = zTj \u00b2 OT over p words in vocabularies, and drawing word xj \u00b2 \u03b2 finite. The subject probability matrix O stores the conditional probabilities Oi, j = p (x = ei = ej). Given h, different words in a document are generated independently, so that the multiview assumption J is 6 t = xt."}, {"heading": "4 Algorithms for Minimum Conical Hull Problem", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Divide-and-Conquer Anchoring (DCA) for General Minimum Conical Hull Problem", "text": "In this section, we will focus on a novel distributed learning scheme covering several dimensions in the development of algorithms for X = FYA. The first DCA algorithm was proposed in [39] for separable NMF only, but this paper will broadly extend the idea to a much richer class of problems and algorithm designs. DCA's main findings stem from two observations on the geometry of the convex cone. First, the projection of a conical hull to a lower D hyperplane as an optimization algorithm."}, {"heading": "4.2 Distributing Conical Hull Problem to Sub-problems in Low Dimensions", "text": "It is only a matter of time until there is a final solution. (A) It is only a matter of time until there is a final solution. (A) It is only a matter of time until there is a final solution. (A) It is only a matter of time until there is a final solution. (A) It is only a matter of time until there is a final solution. (A) It is only a matter of time until there is a final solution. (A) It is only a matter of time until there is a final solution. (A) It is a matter of time until there is a final solution. (A) It is only a matter of time until there is a final solution. (A) It is only a matter of time until there is a minimum problem. (A) It is a matter of time until there is a minimum problem. (A) It is a question of time until there is a minimum problem. (A) It is a question of time until there is a minimum problem."}, {"heading": "4.3 Anchoring on 2D Plane", "text": "DCA provides a fast unified distributed learning scheme that can use all the minimal conical hull problem solvers as a subroutine to solve the partial problems. Although there are multiple solvers for X = FXA for NMF, most of them depend on expensive iterative algorithms derived from optimization or greedy tracking. Furthermore, an algorithm addressing the general model X = FYA is rarely known. Although DCA can call any solver for the partial problem on any low-D hyperplane, an ultrafast solver for the 2D partial problem always shows high accuracy in locating anchors when embedded in the general model X = FYA. Its motivation stems from the geometry of the conical hull on a 2D plane, which is a special case of a d-dim hyperplane H in the partial problem of DCA. It leads to a non-iterative algorithm for A = MCH (X, Y-D level)."}, {"heading": "4.4 DCA as Subroutine of Other Methods", "text": "In many algorithms that find conical hull or other problems, it is tested whether a point Xi is covered by X in the (minimal) conical hull of Y, or equivalent if Xi-cone (Y) dominates the calculation per step. In previous work, the test must calculate the conical combination coefficients by solving a linear programming. DCA provides a much faster off-the-shelf subroutine that can be easily invoked by other methods [5] to achieve a significant acceleration. In particular, the tallest partial problem in DCA proves to be a test when covering the 2D projection Xi\u03a6 in cone (Y), which requires that Computeti = 1 (Xi\u0445 < Y \u0445) + 1 (Xi\u0435 > maxY \u0445) is calculated. (38) The conquest step in DCA becomes {Xi-cone (Y) if T = 1 t i = 0Xi 6 cone (Y), Y > 1 (T = 39)."}, {"heading": "4.5 Examples", "text": "In this section, we present five examples of using the general conical fuselage problem in \u00a7 3 and DCA in algorithm 1 to develop scalable novel learning algorithms for five popular latent variable and matrix factorization models. Although locating anchor set A plays an important role in all algorithms, each of them in practice requires additional pre- / post-processing steps, which are highlighted below."}, {"heading": "4.5.1 DCA for Multi-view Mixture Model", "text": "Given X and Y in (18) or (19) of paragraph 3.4, the application of DCA (X, Y, k, M) in algorithm 1 to X and Y in the GMM is able to find the anchor set A w.h.p, and thus O-1 = XA, 1, O-2 = XA, 2 and O-3 = XA, 3. Therefore, DCA learns GMM by assigning k-real data instances to the mean vectors of the k components. This is reasonable, because we can usually find a real data instance that comes close enough to the true mean in each cluster if n is large enough. This results in a more interpretable GMM, because the center of each cluster is no longer an artificial mean, but a representative real data instance."}, {"heading": "4.5.2 DCA for Hidden Markov Model", "text": "GivenX and Y in (18) or (19) from paragraph 3.4 and the following notations in paragraph 3.5, we can immediately restore the transition matrix T by solving the linear equation OT = XA, 3 with simple constraints on the columns of T. Since T is a small k \u00b7 k matrix, there are many standard solvers that can reach T quickly."}, {"heading": "4.5.3 DCA for Latent Dirichlet Allocation", "text": "Given X in (27) of \u00a7 3.7, after we have developed A = DCA (X = X, k, M), we can develop our special procedure so that it comes to a solution of the system of linear equations (1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = = 1 = 1 = = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = + 1 = 1 = 1 = 1 = + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = + 1 = 1 = 1 = 1 = 1 = 1 = 1 = + 1 = + 1 = 1 = + 1 = + 1 = + 1 = 1 = + 1 = 1 = 1 = 1 = + 1 = 1 = 1 = 1 = 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = + 1 = 1 = + 1 = 1 = 1 = + 1 = 1 = + 1 = + 1 = + 1 = + + 1 = + 1 = + 1 = 1 = + 1 = + 1 = + 1 = + + 1 = + 1 = + 1 = + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +"}, {"heading": "4.5.4 DCA for Non-negative Matrix Factorization", "text": "The conical hull problem for NMF uses the X = FXA model. DCA for NMF was proposed in [39]."}, {"heading": "4.5.5 DCA for Subspace Clustering", "text": "Compared to existing SC algorithms, which rely on expensive lasso-like optimizations and spectral clusters that require SVD, a slightly modified DCA is able to provide a much more efficient algorithm. Specifically, we change each partial problem from A = DCA (X, X, K = K = 1 ki, M) to a divisible SC from X\u03a6 on a low-D hyperplane that can be solved by any available divisible SC solver. A simple but effective problem is to add the two points with the maximum and minimum angle in each cluster. The reason for using the mean shift is 1) it is fast and provides a highly reliable cluster result based on 1D values; and 2) the number of clusters can be determined automatically."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 DCA for Non-negative Matrix Factorization on Synthetic Data", "text": "The experimental comparison results are shown in Figure 2. Greedy SPA, XRAY, and SFO algorithms achieve the best accuracy and the smallest recovery error when the noise level is above 0.2, but XRAY and SFO are the slowest of the two. SPA is slightly faster, but still much slower than DCA. DCA with varying numbers of partial problems exhibits slightly less accuracy and greater errors than greedy algorithms, but the difference is acceptable. Given its significant acceleration, DCA offers an advantageous compromise. LP test [5] has the exact solution warranty, but is not robust to noise and too slow in speed. Therefore, DCA offers a much faster and more practical NMF algorithm with comparable performance to the best."}, {"heading": "5.2 DCA for Gaussian Mixture Models on Synthetic Dataset", "text": "We thoroughly evaluate DCA-GMM using synthetic data generated with varying levels of variance and noise levels, and the higher these two levels, the more difficult the cluster task is. Results are reported in Figure 3, the detailed process that generates data and evaluation metrics, as stated in the caption. On all metrics, DCA-GMM exhibits phase transition characteristics, i.e. the algorithm will be overwhelmingly successful under a curve of noise and variance level. This property confirms the robustness of DCA-GMM compared to data noise and variance within a cluster. Furthermore, increasing the number of sub-problems (layers from bottom to top) shows that the accuracy of DCA-GMM will soon be saturated to a value close to 1, suggesting that a small number of sub-problems in DCA-GMM will be sufficient to produce a promising cluster result, which in practice is highly preferred DCA-DCA-GMM."}, {"heading": "5.4 DCA for Hidden Markov Model on Stock Price and Motion Capture Data", "text": "The experimental comparative results for stock price modeling and motion segmentation are shown in Figure 5 and Figure 6, respectively. In the first method, the DCA always achieves a slightly lower but comparable probability compared to the Baum-Welch method (EM), while the spectral method performs worse and more unstable. DCA has a significant speed advantage over other methods and is therefore preferable in practice.In the second, we evaluate the probability and predictability of both the training and the test set, so that the regulation induced by the separability assumption results in the highest test accuracy and fastest speed of the DCA. Note that the time cost of the Baum-Welch method does not increase with the number of training samples at constant velocity. This is because the method uses an adaptive stop criterion, i.e. the optimization is stopped if the probability of the training data increases too slowly. Since we do not randomly select the observations in a sequence for the training due to the sequence of the CA and due to the inherent nature of the sequence, the data cannot be observed."}, {"heading": "5.5 DCA for Latent Dirichlet Allocation on Text Dataset", "text": "The experimental comparison results for the subject modeling are shown in Figure 7. Compared to both traditional EM and sampling methods, DCA achieves not only the least perplexity (highest probability) of the test set and the highest speed, but also the most stable performance in increasing the number of subjects. Furthermore, the \"anchor word\" achieved by DCA provides more interpretable topics than other methods."}, {"heading": "5.6 DCA for Subspace Clustering on Synthetic Dataset", "text": "The results are shown in Figure 8, the detailed method of generating data and evaluation metrics, in the caption. On all metrics, DCA subspace clustering has a phase transition characteristic, i.e. the algorithm will be overwhelmingly successful under a noise level and maximum voltage angle curve. This characteristic confirms the robustness of DCA subspace clusters, which focus on data noise and overlap between clusters. Furthermore, increasing the number of sub-problems (layers from bottom to top) results in overwhelming success below a noise level and maximum voltage angle curve. This characteristic confirms the robustness of DCA subspace clusters, which focus on a value close to 1, suggesting that a small number of sub-problems is sufficient to produce a promising result (layers from bottom to top)."}, {"heading": "5.7 DCA for Subspace Clustering on Image and Motion Capture Dataset", "text": "The experimental comparison results for clustering on object image data COIL-100 are shown in Figure 9. DCA provides a much more practical algorithm in speed (DCA subspace clustering) that can achieve comparable mutual information, but is more than 1000 times faster than the state-of-the-art SC algorithms [11, 16, 28, 36]. We also apply DCA subspace clustering to a sequence of motion capture data that cannot be analyzed by existing subspace clustering methods due to their high computing complexity. DCA subspace clustering aims to find multiple anchor frames for each cluster, so that they can reconstruct most frames in the same cluster as their conical combinations. According to the results shown in Figure 9, the anchor frames in each of the 8 detected clusters will summarize some kind of movement in critical positions. Thus, DCA provides a more interpretable subspace clustering results than those normally provided by other subspace clustering methods."}, {"heading": "6 Conclusion", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "6.1 Future Works and Discussions", "text": "Although we can both apply the general minimum of conical problem formulations to the general learning models, it is possible to solve the problem with detailed examples for the individual models, there are several interesting and important extended topics for our method. \u2022 To break the linearity assumptions, we need to take a closer look at the distributions between the individual countries."}], "references": [{"title": "A spectral algorithm for latent dirichlet allocation", "author": ["A. Anandkumar", "D.P. Foster", "D. Hsu", "S. Kakade", "Y. Liu"], "venue": "NIPS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "COLT,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Supplemental material", "author": ["A. Anonymous"], "venue": "Submitted to NIPS,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D.M. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "ICML,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Computing a nonnegative matrix factorization - provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "STOC,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical inference for probabilistic functions of finite state Markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "Annals of Mathematical Statistics, 37:1554\u20131563,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1966}, {"title": "Polynomial learning of distribution families", "author": ["M. Belkin", "K. Sinha"], "venue": "FOCS,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Maching Learning Research (JMLR), 3:993\u20131022,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics, 9:717\u2013772,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Full reconstruction of markov models on evolutionary trees: Identifiability and consistency", "author": ["J.T. Chang"], "venue": "Mathematical Biosciences, 137(1):51\u201373,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Spectral curvature clustering (scc)", "author": ["G. Chen", "G. Lerman"], "venue": "International Journal of Computer Vision (IJCV), 81(3):317\u2013330,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Coherent matrix completion", "author": ["Y. Chen", "S. Bhojanapalli", "S. Sanghavi", "R. Ward"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Mean shift, mode seeking, and clustering", "author": ["Y. Cheng"], "venue": "IEEE Transactions on Pattern Analysis and Maching Intelligence (TPAMI), 17(8):790\u2013799,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B, 39(1):1\u201338,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1977}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts", "author": ["D. Donoho", "V. Stodden"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Elsevier,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 6(6):721\u2013741,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1984}, {"title": "Fast and robust recursive algorithmsfor separable nonnegative matrix factorization", "author": ["N. Gillis", "S.A. Vavasis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 36(4):698\u2013714,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "COLT,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Effective split-merge monte carlo methods for nonparametric models of sequential data", "author": ["M.C. Hughes", "E.B. Fox", "E.B. Sudderth"], "venue": "NIPS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Conference in modern analysis and probability (New Haven, Conn., 1982), volume 26 of Contemporary Mathematics, pages 189\u2013206.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1984}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["A.T. Kalai", "A. Moitra", "G. Valiant"], "venue": "STOC,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "COLT,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Random conic pursuit for semidefinite programming", "author": ["A. Kleiner", "A. Rahimi", "M.I. Jordan"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast conical hull algorithms for near-separable nonnegative matrix factorization", "author": ["A. Kumar", "V. Sindhwani", "P. Kambadur"], "venue": "ICML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401:788\u2013791,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "ICML,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory (TIT), 28(2):129\u2013 137,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1982}, {"title": "Lecture Notes: Introduction to Linear Optimization", "author": ["A. Nemirovski"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y. Ng", "Michael I. Jordan", "Yair Weiss"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}, {"title": "Contributions to the mathematical theory of evolution", "author": ["K. Pearson"], "venue": "Philosophical Transactions of the Royal Society of London. A, 185:71\u2013110,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1894}, {"title": "Fast collapsed gibbs sampling for latent dirichlet allocation", "author": ["I. Porteous", "D. Newman", "A. Ihler", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "SIGKDD, pages 569\u2013577,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Mixture Densities, Maximum Likelihood and the Em Algorithm", "author": ["R.A. Redner", "H.F. Walker"], "venue": "SIAM Review, 26(2):195\u2013239,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1984}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "NIPS,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust subspace clustering", "author": ["M. Soltanolkotabi", "E. Elhamifar", "E.J. Cand\u00e8s"], "venue": "arXiv:1301.2603,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical Analysis of Finite Mixture Distributions", "author": ["D.M. Titterington", "A.F.M. Smith", "U.E. Makov"], "venue": "Wiley, New York,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1985}, {"title": "An analysis of the greedy algorithm for the submodular set covering problem", "author": ["Laurence A. Wolsey"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1982}, {"title": "Divide-and-conquer anchoring for near-separable nonnegative matrix factorization and completion in high dimensions", "author": ["T. Zhou", "W. Bian", "D. Tao"], "venue": "ICDM,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "Expectation-maximization (EM) [14], sampling methods [18], and matrix factorization [27, 35] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "Expectation-maximization (EM) [14], sampling methods [18], and matrix factorization [27, 35] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.", "startOffset": 53, "endOffset": 57}, {"referenceID": 26, "context": "Expectation-maximization (EM) [14], sampling methods [18], and matrix factorization [27, 35] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.", "startOffset": 84, "endOffset": 92}, {"referenceID": 34, "context": "Expectation-maximization (EM) [14], sampling methods [18], and matrix factorization [27, 35] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.", "startOffset": 84, "endOffset": 92}, {"referenceID": 33, "context": "Hence, their quality greatly depends on initialization and on using a large number of iterations for proper convergence [34].", "startOffset": 120, "endOffset": 124}, {"referenceID": 31, "context": "The method of moments [32, 7, 23], in contrast, solves m equations by relating the first m moments of observation x \u2208 R to the m model parameters, and thus yields a consistent estimator with a global solution.", "startOffset": 22, "endOffset": 33}, {"referenceID": 6, "context": "The method of moments [32, 7, 23], in contrast, solves m equations by relating the first m moments of observation x \u2208 R to the m model parameters, and thus yields a consistent estimator with a global solution.", "startOffset": 22, "endOffset": 33}, {"referenceID": 22, "context": "The method of moments [32, 7, 23], in contrast, solves m equations by relating the first m moments of observation x \u2208 R to the m model parameters, and thus yields a consistent estimator with a global solution.", "startOffset": 22, "endOffset": 33}, {"referenceID": 9, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 33, "endOffset": 48}, {"referenceID": 23, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 33, "endOffset": 48}, {"referenceID": 19, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 33, "endOffset": 48}, {"referenceID": 0, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 33, "endOffset": 48}, {"referenceID": 1, "context": "Although recent spectral methods [10, 24, 20, 1] reduces m to 2 or 3 when estimating O(p) m parameters [2] by relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale, the variance of sample moments is still sensitive to large p or data noise, which may result in poor estimation.", "startOffset": 103, "endOffset": 106}, {"referenceID": 14, "context": "This property was introduced in [15] to establish the uniqueness of non-negative matrix factorization (NMF) under simplex constraints, and was later [26, 19] extended to non-negative constraints.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "This property was introduced in [15] to establish the uniqueness of non-negative matrix factorization (NMF) under simplex constraints, and was later [26, 19] extended to non-negative constraints.", "startOffset": 149, "endOffset": 157}, {"referenceID": 18, "context": "This property was introduced in [15] to establish the uniqueness of non-negative matrix factorization (NMF) under simplex constraints, and was later [26, 19] extended to non-negative constraints.", "startOffset": 149, "endOffset": 157}, {"referenceID": 36, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 182, "endOffset": 185}, {"referenceID": 26, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 191, "endOffset": 195}, {"referenceID": 15, "context": "We apply both the conical hull anchoring model and DCA to five learning models: Gaussian mixture models (GMM) [37], hidden Markov models (HMM) [6], latent Dirichlet allocation (LDA) [8], NMF [27], and subspace clustering (SC) [16].", "startOffset": 226, "endOffset": 230}, {"referenceID": 14, "context": "2 General Separability Assumption and Minimum Conical Hull Problem The original separability property [15] is defined on the convex hull of a set of data points, namely that each point can be represented as a convex combination of certain subsets of vertices that define the convex hull.", "startOffset": 102, "endOffset": 106}, {"referenceID": 25, "context": "Later works on separable NMF [26, 19] extend it to the conical hull case, which replaced convex with conical combinations.", "startOffset": 29, "endOffset": 37}, {"referenceID": 18, "context": "Later works on separable NMF [26, 19] extend it to the conical hull case, which replaced convex with conical combinations.", "startOffset": 29, "endOffset": 37}, {"referenceID": 29, "context": "According to basic rules [30], a finitely generated and pointed cone C possesses a finite and unique set of extreme rays R, and C = cone(R) is the conical hull generated by these extreme rays R.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "3 from [5]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "4 in [5]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "It is easy to verify that XA obtained by Arora\u2019s exact algorithm [5] is a feasible solution fulfilling the constraint in (8).", "startOffset": 65, "endOffset": 68}, {"referenceID": 37, "context": "Lastly, we see how Equation (3) can be seen as a submodular cover problem [38].", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "It can be verified that the constraint in Definition 5 equals to the constraint to F in Definition 2, and that f(A) = cover(XA\u222aI), where XI is any set of linearly independent row vectors, is submodular [17].", "startOffset": 202, "endOffset": 206}, {"referenceID": 2, "context": "When X = Y and S = R + , it degenerates to the original separability assumption given in [3].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "We generalize the minimum conical hull problem from [3].", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "When X = Y , it degenerates to the original minimum conical hull problem defined in [3].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "3, by following the analysis of the separability assumption in [3],we can prove that A is unique and identifiable given X .", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "More details are given in [3].", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Model X in conical hull problem Y in conical hull problem k in conical hull problem NMF data matrixX \u2208 Rn\u00d7p + Y := X # of factors SC data matrixX \u2208 Rn\u00d7p Y := X # of basis from all clusters GMM [vec[X 1 X2]; vec[X T 1 Diag(X3\u03b7t)X2]t\u2208[q]]/n [vec(Xt,1 \u2297Xt,2)]t\u2208[n] # of components/clusters HMM [vec[X 2 X3]; vec[X T 2 Diag(X1\u03b7t)X3]t\u2208[q]]/n [vec(Xt,2 \u2297Xt,3)]t\u2208[n] # of hidden states LDA word-word co-occurrence matrixX \u2208 Rp\u00d7p + Y := X # of topics Algo Each sub-problem in DCA Post-processing afterA := \u22c3 i \u00c3 i Interpretation of anchors indexed byA NMF \u00c3 = MCH(X\u03a6, X\u03a6), can be solved by (37) solving F inX = FXA basisXA are real data points SC \u00c3 =anchors of clusters achieved by meanshift( \u0302 (X\u03a6)\u03c6) clustering anchorsXA cluster i is a cone cone(XAi ) GMM \u00c3 = MCH(X\u03a6, Y\u03a6), can be solved by (37) N/A centers [XA,i]i\u2208[3] from real data HMM \u00c3 = MCH(X\u03a6, Y\u03a6), can be solved by (37) solving T inOT = XA,3 emission matrixO = XA,2 LDA \u00c3 = MCH(X\u03a6, X\u03a6), can be solved by (37) col-normalize {F : X = FXA} anchor word for topic i (topic prob.", "startOffset": 809, "endOffset": 812}, {"referenceID": 35, "context": ", with large principle angles between each other [36].", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "Most existing subspace clustering methods [16, 36], relies on spectral clustering to the sparse representations of data points, which are obtained by finding a sparse C with diag(C) = 0 in model X = CX .", "startOffset": 42, "endOffset": 50}, {"referenceID": 35, "context": "Most existing subspace clustering methods [16, 36], relies on spectral clustering to the sparse representations of data points, which are obtained by finding a sparse C with diag(C) = 0 in model X = CX .", "startOffset": 42, "endOffset": 50}, {"referenceID": 35, "context": "Then (spectral) clustering to rows of C guarantees to provide clustering labels which can lead to reliable estimation of the k subspaces [36].", "startOffset": 137, "endOffset": 141}, {"referenceID": 2, "context": ", we assume that x is composed of three groups(views) of features {xi}i\u2208[3] such that \u2200i 6= j, xi \u22a5 xj |h.", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "We will mainly focus on the models in which {\u03b1, \u03b8} can be exactly recovered from conditional mean vectors {Oi}i\u2208[3] and E(h\u2297 h)1, because they cover most popular models such as GMM and HMM in real applications.", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "Therefore, learning {Oi}i\u2208[3] is reduced to selecting k rank-one matrices from {Xt,i \u2297 Xt,j}t\u2208[n] indexed by A, and defining the extreme rays of a conical hull covering the q matrices {Y }t\u2208[q].", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "However, thanks to the matrix completion research [9, 12], when k min{pi, pj}, we can retain merely m = O(max{pi, pj}k log(pi + pj)) pipj entries in the vectorization of each matrix from {Y }t\u2208[q] and {Xt,i \u2297Xt,j}t\u2208[n] in (17).", "startOffset": 50, "endOffset": 57}, {"referenceID": 11, "context": "However, thanks to the matrix completion research [9, 12], when k min{pi, pj}, we can retain merely m = O(max{pi, pj}k log(pi + pj)) pipj entries in the vectorization of each matrix from {Y }t\u2208[q] and {Xt,i \u2297Xt,j}t\u2208[n] in (17).", "startOffset": 50, "endOffset": 57}, {"referenceID": 3, "context": "This leads to a natural assumption of \u201canchor word\u201d for LDA [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Under multi-view assumption x = {xi}i\u2208[3] with \u2200i 6= j, xi \u22a5 xj |h, MM generates an observation x by firstly drawing a label indicator h \u223c \u03c3 \u2208 \u2206k\u22121 from {ei}i\u2208[k] and then drawing features of different views xi \u223c p(xi| \u2211k j=1 hj\u03b8 j i ) independently.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": ", for each triple of observations {x1, x2, x3} and the corresponding hidden states {h1, h2, h3}, let h1 \u223c \u03c0, by integrating out h1 and h2, we have [2] \uf8f4\uf8f2\uf8f4\uf8f3 E(x2|h2) = h2 O \u2192 h2 O 2 , h2 \u223c T\u03c0, E(x1|h2) = \u2211 h1 E(x1|h1) \u00b7 p(h2|h1)p(h1) p(h2) = h T 2 [ ODiag(\u03c0)TTDiag((T\u03c0)\u22121) ]T \u2192 h2 O 1 , E(x3|h2) = \u2211 h3 E(x3|h3)p(h3|h2) = h2 [OT ] \u2192 h2 O 3 .", "startOffset": 147, "endOffset": 150}, {"referenceID": 38, "context": "The first DCA algorithm was proposed in [39] only for separable NMF, but this paper will largely extend the idea to much richer class of problems and algorithm designs.", "startOffset": 40, "endOffset": 44}, {"referenceID": 38, "context": "For the special case of NMF when X = FXA, the above result was proved in [39].", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "Define d-dim (d \u2264 p) hyperplanes {Hi}i\u2208[4] such that A3A2 \u22a5 H1, A \u2032 1A \u2032 2 \u22a5 H2, B\u2032 1A2 \u22a5 H3, B\u2032 1C \u2032 1 \u22a5 H4, let \u03b1 = \u01241H2 be the angle between hyperplanes H1 and H2, \u03b2 = \u01243H4 be the angle between H3 and H4.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "By applying Chernoff bound to random variable \u011d(Ai) \u2212 \u011d(Bj) (randomness is due to random hyperplane H), for any \u03b4 \u2208 [0, 1], Ai and Bj , we have", "startOffset": 116, "endOffset": 122}, {"referenceID": 21, "context": "Remarks: It is worth noting that although DCA uses random projection to reduce the problem size, it is different from the random projection methods based on Johnson-Lindenstrauss (JL) Lemma [22] or its variants.", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "DCA provides a much faster off-the-shelf subroutine which can be easily invoked by other methods [5] to gain a significant acceleration.", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "Recently, we surprisingly discover that the above procedure equals to a Bayes learning algorithm proposed in [4], whose major idea is to solve an NMF under simplex constraint by a greedy pursuit typed algorithm.", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "Comparing to the specific greedy algorithm developed for LDA in [4], our method provides a unified scheme that can reduce more general models besides LDA to a conical hull problem, and the proposed DCA leads to a significantly efficient algorithm.", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "This is because 1) Limited by the simplex constraint in NMF, the Bayes learning method decomposes the row-normalized X in (27), thus it needs to additionally compute p(w2 = j|w1 = i) and p(w1 = i) for normalization, and \u2211 i p(z = et|x = ei)p(x = ei) after NMF; and 2)The greedy algorithm in [4] finds the anchors of convex hull is slower than DCA using parallel and randomized strategy.", "startOffset": 291, "endOffset": 294}, {"referenceID": 3, "context": "The equivalence between [4] and our method can be established by the following theorem.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "In order to make the comparison clear, we map all the notations in our method to those used in [4] such that Q \u2190 E(x1 \u2297 x2), V \u2190 A, A \u2190 O , vt denotes the index of the anchor word for topic t, \u0100t,i \u2190 At,i/At,vt , wi = j \u21d4 xi = ej , zi = j \u21d4 zi = ej .", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Then we use the notations in [4] throughout the theorem and its proof below.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Solving the conical hull problem Q = \u0100QV proposed in this paper for LDA equals to Bayes learning of p(z1 = t|w1 = i) by NMF proposed in [4], i.", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "Since A can be uniquely recovered as row-normalized \u0100, and the above reasoning is reversible (due to all the equalities), the equivalence between conical hull problem and the Bayes learning [4] given in (40) holds.", "startOffset": 190, "endOffset": 193}, {"referenceID": 38, "context": "DCA for NMF has been proposed in [39].", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "A simple but effective one is to sample \u03a6 \u2208 Rp\u00d72, project X to an 2D plane, apply mean shift clustering algorithm [13] to the n-array of angles \u0302 (Xi\u03a6)\u03c6, and add to \u00c3 the two points with the maximal and minimal angle in each cluster.", "startOffset": 114, "endOffset": 118}, {"referenceID": 30, "context": "Thus we can build a graph Laplacian from similarity matrixG \u2208 RK\u00d7K such thatGi,j = #(XA(i)\u03a6 andXA(j)\u03a6 in the same cluster), and spectral clustering [31] is able to give us the k clusters of anchors {Ai}i\u2208[k].", "startOffset": 148, "endOffset": 152}, {"referenceID": 4, "context": "LP-test [5] has the exact solution guarantee, but it is not robust to noise, and too slow in speed.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "LP-test is the backward removal algorithm from [5].", "startOffset": 47, "endOffset": 50}, {"referenceID": 28, "context": "Baselines: K-means [29], EM algorithm, spectral method.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "DCA provides a much more practical algorithm in speed that can achieve comparable mutual information but more than 1000 times speedup than the state-of-the-art SC algorithms [11, 16, 28, 36].", "startOffset": 174, "endOffset": 190}, {"referenceID": 15, "context": "DCA provides a much more practical algorithm in speed that can achieve comparable mutual information but more than 1000 times speedup than the state-of-the-art SC algorithms [11, 16, 28, 36].", "startOffset": 174, "endOffset": 190}, {"referenceID": 27, "context": "DCA provides a much more practical algorithm in speed that can achieve comparable mutual information but more than 1000 times speedup than the state-of-the-art SC algorithms [11, 16, 28, 36].", "startOffset": 174, "endOffset": 190}, {"referenceID": 35, "context": "DCA provides a much more practical algorithm in speed that can achieve comparable mutual information but more than 1000 times speedup than the state-of-the-art SC algorithms [11, 16, 28, 36].", "startOffset": 174, "endOffset": 190}, {"referenceID": 20, "context": "The motion for each frame is manually labeled by the authors of [21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 32, "context": "Baselines: EM algorithm for variational method, Gibbs sampling [33], spectral method.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "\u2022 In order to break the linearity assumption E(x|h) = hO and generalize distribution p(x|h) to even non-parametric forms, we can consider to embed the joint distribution of {xi}i\u2208[3] into a reproducing kernel Hilbert space (RKHS).", "startOffset": 179, "endOffset": 182}, {"referenceID": 1, "context": "A very related work [2] has shown this possibility for linear Bayesian networks.", "startOffset": 20, "endOffset": 23}, {"referenceID": 24, "context": "The essential idea is to represent X by a weighted sum of k rank-one matrices [25], each of which is generated by a real data point.", "startOffset": 78, "endOffset": 82}], "year": 2014, "abstractText": "We reduce a broad class of machine learning problems, usually addressed by EM or sampling, to the problem of finding the k extremal rays spanning the conical hull of a data point set. These k \u201canchors\u201d lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the k anchors, we propose a novel divide-and-conquer learning scheme \u201cDCA\u201d that distributes the problem to O(k log k) same-type sub-problems on different low-D random hyperplanes, each can be solved by any solver. For the 2D sub-problem, we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine for other methods to check whether a point is covered in a conical hull, which improves algorithm design in multiple dimensions and brings significant speedup to learning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering, then show its competitive performance and scalability over other methods on rich datasets.", "creator": "LaTeX with hyperref package"}}}