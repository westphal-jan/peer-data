{"id": "1406.6247", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2014", "title": "Recurrent Models of Visual Attention", "abstract": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.", "histories": [["v1", "Tue, 24 Jun 2014 14:16:56 GMT  (982kb,D)", "http://arxiv.org/abs/1406.6247v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["volodymyr mnih", "nicolas heess", "alex graves", "koray kavukcuoglu"], "accepted": true, "id": "1406.6247"}, "pdf": {"name": "1406.6247.pdf", "metadata": {"source": "CRF", "title": "Recurrent Models of Visual Attention", "authors": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that it is a purely mental game, in which it is a matter of placing people at the centre of attention, in order to put them in a position to move, in a position to move, in a position to put themselves in a world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Previous Work", "text": "Computer-related limitations have attracted a lot of attention in the computer visualization literature. For example, much work has been done on object recognition to reduce the cost of the widespread paradigm of sliding windows by focusing primarily on reducing the number of windows for which the full classifier is evaluated, for example by classification cascades (e.g. [7, 24]), by removing image regions from consideration by an industry-bound and bound approach to classifier output (e.g. [13]), or by proposing candidate windows that are likely to contain objects (e.g. [1, 23]). Although such approaches can achieve substantial accelerations, and some of them can be combined with an add-on to CNN classifiers [8], they remain firmly rooted in the window classification design for object recognition and use only past information to influence future processing of the image in a very limited way."}, {"heading": "3 The Recurrent Attention Model (RAM)", "text": "In this paper, we consider the attention problem to be the sequential decision-making process of a targeted agent interacting with a visual environment. At any given time, the agent observes the environment only through a bandwidth-limited sensor, i.e., he never fully senses the environment. He can only extract information in a local region or in a narrow frequency band. However, the agent can actively control how he uses his sensor resources (e.g., selecting the sensor location), and the agent can also influence the true state of the environment by performing actions. As the environment is only partially observed, the agent must integrate information over time to determine how he will act most effectively and how he will use his sensor. At each step, the agent receives a scalar reward (which depends on the actions the agent has performed and can be delayed), and the agent's goal is to maximize the total sum of such rewards. This formulation includes tasks as transferring the computer game to images as varied as the static and random control state of the object, or the controller state."}, {"heading": "3.1 Model", "text": "The agent is built around a recurrent neural network, as shown in Fig. 1. In each time step, it processes the sensor data, integrates information over time, and chooses how to proceed and how to use its sensor in the next step: Sensor: In each step, the agent receives a (partial) observation of the environment in the form of an image. The agent does not have full access to this image, but can extract information from its bandwidth-limited sensor, for example by focusing the sensor on a region or a frequency band of interest. In this paper, we assume that the bandwidth-limited environment extracts a sensor that creates a retina-like representation of the image (xt \u2212 1). It encodes the region around l with high resolution, but uses a progressively lower resolution for pixels farther from l, resulting in a vector of much lower dimensionality than the original image."}, {"heading": "3.2 Training", "text": "The parameters of our agent are given by the parameters of the gaze network, the core network (fig. 1C =), and the action network \u03b8 = {\u03b8g, \u03b8h, \u03b8a}, and we learn these in order to maximize the total reward that the agent can expect when interacting with the environment. Formally, the agent's policy, possibly in combination with the dynamics of the environment (e.g. for the game), is a distribution of possible interaction sequences s1: N and we aim to maximize the reward under this distribution: J (BA) = Ep (s1: T; p = 1 rt] = Ep (s1: T) = Ep (s1: T), where p (s1: T; p) depends on the policyMaximizing J accurately is not trivial, as it involves an expectation of the high-dimensional interaction sequences, which in turn may involve unknown dynamics."}, {"heading": "4 Experiments", "text": "First, we describe the design decisions common to all of our experiments: Retinal and location encodings: The retina encoding \u03c1 (x, l) extracts k-square spots centered at position l, where the first spot is gw \u00b7 gw pixels in size and each successive spot is twice as wide as the previous one; the k spots are then all enlarged and concatenated to gw \u00b7 gw. Glimpse spots l were encoded as real weighted (x, y) coordinates2, where (0, 0) is the center of the image x and (\u2212 1, \u2212 1) is the upper left corner of the x. Glimpse network is: The glimpse network fg (x, l) had two fully connected layers. LetLinear (x) denotes a linear transformation of the vector x, i.e. Linear (x) = Wx + b for a weight matrix and LinW was the default of the vector (max and 12h) and (12h)."}, {"heading": "4.1 Image Classification", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4.2 Dynamic Environments", "text": "An appealing feature of the recurring attention model is that it can be applied to videos or interactive problems with visual input as well as to static image tasks. We are testing the ability of our approach to learn a control policy in a dynamic visual environment, while perceiving the environment through a bandwidth-limited retina by training it to play a simple game. The game is played on a 24 by 24 binary pixel screen and includes two objects: a single pixel representing a ball that falls from the top of the screen as it bounces off the sides of the screen, and a two-pixel paddle positioned at the bottom of the screen with the goal of catching the ball. If the falling pixel reaches the bottom of the screen, the agent receives either a reward of 1 when the paddle intersects with the ball, and a reward of 0 otherwise. The game then begins from the front to restart the game to find the point of focus, where we are just tracing the recurring attention."}, {"heading": "5 Discussion", "text": "This work introduced a novel visual attention model, formulated as a single recursive neural network that takes a viewing window as an input and uses the internal state of the network to select the next location to focus on, and to generate control signals in a dynamic environment. Although the model is indistinguishable, the proposed uniform architecture can be controlled end-to-end from pixel input to action using a method of political gradient. The model has several appealing properties. First, our experiments show that both the number of parameters and the amount of computational memory can be controlled independently of the size of the input images. Second, the model is able to ignore clutter present in an image by centering its retina on the relevant regions. Our experiments show that RAM is a revolutionary architecture with a comparable number of parameters on a confusing object classification task."}], "references": [{"title": "What is an object", "author": ["Bogdan Alexe", "Thomas Deselaers", "Vittorio Ferrari"], "venue": "In CVPR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Searching for objects driven by context", "author": ["Bogdan Alexe", "Nicolas Heess", "Yee Whye Teh", "Vittorio Ferrari"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Optimal scanning for faster object detection", "author": ["Nicholas J. Butko", "Javier R. Movellan"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "I-pomdp: An infomax model of eye movement", "author": ["N.J. Butko", "J.R. Movellan"], "venue": "In Proceedings of the 7th IEEE International Conference on Development and Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Cascade object detection with deformable part models", "author": ["Pedro F. Felzenszwalb", "Ross B. Girshick", "David A. McAllester"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross B. Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "CoRR, abs/1311.2524,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Eye movements in natural behavior", "author": ["Mary Hayhoe", "Dana Ballard"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "A model of saliency-based visual attention for rapid scene analysis", "author": ["L. Itti", "C. Koch", "E. Niebur"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Beyond sliding windows: Object localization by efficient subwindow search", "author": ["Christoph H. Lampert", "Matthew B. Blaschko", "Thomas Hofmann"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Hugo Larochelle", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Action from still image dataset and inverse optimal control to learn task specific visual scanpaths", "author": ["Stefan Mathe", "Cristian Sminchisescu"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Q-learning of sequential attention for visual object recognition from informative local descriptors", "author": ["Lucas Paletta", "Gerald Fritz", "Christin Seifert"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "On Learning Where To Look", "author": ["M. Ranzato"], "venue": "ArXiv e-prints,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "The dynamic representation of scenes", "author": ["Ronald A. Rensink"], "venue": "Visual Cognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "CoRR, abs/1312.6229,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Evolving a roving eye for go", "author": ["Kenneth O. Stanley", "Risto Miikkulainen"], "venue": "In GECCO,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David Mcallester", "Satinder Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search", "author": ["Antonio Torralba", "Aude Oliva", "Monica S Castelhano", "John M Henderson"], "venue": "Psychol Rev,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Segmentation as Selective Search for Object Recognition", "author": ["K E A van de Sande", "J.R.R. Uijlings", "T Gevers", "A.W.M. Smeulders"], "venue": "In ICCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["Paul A. Viola", "Michael J. Jones"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Solving deep memory pomdps with recurrent policy gradients", "author": ["Daan Wierstra", "Alexander Foerster", "Jan Peters", "Juergen Schmidhuber"], "venue": "In ICANN", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1992}], "referenceMentions": [{"referenceID": 7, "context": "Neural network-based architectures have recently had great success in significantly advancing the state of the art on challenging image classification and object detection datasets [8, 12, 19].", "startOffset": 181, "endOffset": 192}, {"referenceID": 11, "context": "Neural network-based architectures have recently had great success in significantly advancing the state of the art on challenging image classification and object detection datasets [8, 12, 19].", "startOffset": 181, "endOffset": 192}, {"referenceID": 18, "context": "Neural network-based architectures have recently had great success in significantly advancing the state of the art on challenging image classification and object detection datasets [8, 12, 19].", "startOffset": 181, "endOffset": 192}, {"referenceID": 11, "context": "The large convolutional neural networks typically used currently take days to train on multiple GPUs even though the input images are downsampled to reduce computation [12].", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "In the case of object detection processing a single image at test time currently takes seconds when running on a single GPU [8, 19] as these approaches effectively follow the classical sliding window paradigm from the computer vision literature where a classifier, trained to detect an object in a tightly cropped bounding box, is applied independently to thousands of candidate windows from the test image at different positions and scales.", "startOffset": 124, "endOffset": 131}, {"referenceID": 18, "context": "In the case of object detection processing a single image at test time currently takes seconds when running on a single GPU [8, 19] as these approaches effectively follow the classical sliding window paradigm from the computer vision literature where a classifier, trained to detect an object in a tightly cropped bounding box, is applied independently to thousands of candidate windows from the test image at different positions and scales.", "startOffset": 124, "endOffset": 131}, {"referenceID": 17, "context": "Instead humans focus attention selectively on parts of the visual space to acquire information when and where it is needed, and combine information from different fixations over time to build up an internal representation of the scene [18], guiding future eye movements and decision making.", "startOffset": 235, "endOffset": 239}, {"referenceID": 10, "context": "in the form of saliency; [11]) play an important role, the locations on which humans fixate have also been shown to be strongly task specific (see [9] for a review and also e.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "in the form of saliency; [11]) play an important role, the locations on which humans fixate have also been shown to be strongly task specific (see [9] for a review and also e.", "startOffset": 147, "endOffset": 150}, {"referenceID": 14, "context": "[15, 22]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[15, 22]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "[7, 24]), removing image regions from consideration via a branch and bound approach on the classifier output (e.", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "[7, 24]), removing image regions from consideration via a branch and bound approach on the classifier output (e.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[13]), or by proposing candidate windows that are likely to contain objects (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1, 23]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 22, "context": "[1, 23]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "Even though substantial speedups may be obtained with such approaches, and some of these can be combined with or used as an add-on to CNN classifiers [8], they remain firmly rooted in the window classifier design for object detection and only exploit past information to inform future processing of the image in a very limited way.", "startOffset": 150, "endOffset": 153}, {"referenceID": 10, "context": "[11]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Saliency detectors indeed capture some of the properties of human eye movements, but they typically do not to integrate information across fixations, their saliency computations are mostly hardwired, and they are based on low-level image properties only, usually ignoring other factors such as semantic content of a scene and task demands (but see [22]).", "startOffset": 348, "endOffset": 352}, {"referenceID": 1, "context": "[2, 4, 6, 14, 16, 17, 20] have embraced vision as a sequential decision task as we do here.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "[2, 4, 6, 14, 16, 17, 20] have embraced vision as a sequential decision task as we do here.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "[2, 4, 6, 14, 16, 17, 20] have embraced vision as a sequential decision task as we do here.", "startOffset": 0, "endOffset": 25}, {"referenceID": 13, "context": "[2, 4, 6, 14, 16, 17, 20] have embraced vision as a sequential decision task as we do here.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "[2, 4, 6, 14, 16, 17, 20] have embraced vision as a sequential decision task as we do here.", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "[2, 4, 6, 14, 16, 17, 20] have embraced vision as a sequential decision task as we do here.", "startOffset": 0, "endOffset": 25}, {"referenceID": 19, "context": "[2, 4, 6, 14, 16, 17, 20] have embraced vision as a sequential decision task as we do here.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "[4] employs the learned Bayesian observer model from [5] to the task of object detection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] employs the learned Bayesian observer model from [5] to the task of object detection.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "The learning framework of [5] is related to ours as they also employ a policy gradient formulation (cf.", "startOffset": 26, "endOffset": 29}, {"referenceID": 5, "context": "Our work is perhaps the most similar to the other attempts to implement attentional processing in a deep learning framework [6, 14, 17].", "startOffset": 124, "endOffset": 135}, {"referenceID": 13, "context": "Our work is perhaps the most similar to the other attempts to implement attentional processing in a deep learning framework [6, 14, 17].", "startOffset": 124, "endOffset": 135}, {"referenceID": 16, "context": "Our work is perhaps the most similar to the other attempts to implement attentional processing in a deep learning framework [6, 14, 17].", "startOffset": 124, "endOffset": 135}, {"referenceID": 13, "context": "We will refer to this low-resolution representation as a glimpse [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "Viewing the problem as a POMDP, however, allows us to bring techniques from the RL literature to bear: As shown by Williams [26] a sample approximation to the gradient is given by", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "But this is just the gradient of the RNN that defines our agent evaluated at time step t and can be computed by standard backpropagation [25].", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "It is natural to select bt = E\u03c0 [Rt] [21], and this form of baseline known as the value function in the reinforcement learning literature.", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "The experiment done on a dynamic environment used a core of LSTM units [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 2, "context": "Hyperparameters such as the learning rate and the variance of the location policy were selected using random search [3].", "startOffset": 116, "endOffset": 119}], "year": 2014, "abstractText": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.", "creator": "LaTeX with hyperref package"}}}