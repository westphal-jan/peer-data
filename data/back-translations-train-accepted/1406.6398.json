{"id": "1406.6398", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2014", "title": "Incremental Clustering: The Case for Extra Clusters", "abstract": "The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.", "histories": [["v1", "Tue, 24 Jun 2014 21:41:03 GMT  (80kb,D)", "http://arxiv.org/abs/1406.6398v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["margareta ackerman", "sanjoy dasgupta"], "accepted": true, "id": "1406.6398"}, "pdf": {"name": "1406.6398.pdf", "metadata": {"source": "CRF", "title": "Incremental Clustering: The Case for Extra Clusters", "authors": ["Margareta Ackerman", "Sanjoy Dasgupta"], "emails": ["maackerman@ucsd.edu", "dasgupta@eng.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Definitions", "text": "We consider a space X equipped with a symmetrical distance function d: X \u00b7 X \u2192 R + satisfactory d (x, x) = 0. An example is X = Rp with d (x, x) = x \u2212 x \u00b2 2. It is assumed that a cluster algorithm can call d (\u00b7, \u00b7) on each pair x, x \u00b2 x. A cluster formation (or, partition) of X is a group of clusters C = {C1,..., Ck} so that Ci-Cj = can take into account for all i 6 = j and X = \u0441ki = 1Ci. A k cluster formation is a cluster formation with k clusters. Write x-C y if x, y are both in any cluster Cj; and x-6 x-C y otherwise. This is an equivalence relationship. Definition 2.1. An incremental cluster algorithm has the following structure: for n = 1,"}, {"heading": "2.1 Examples of incremental clustering algorithms", "text": "s algorithm [14, 15]: Algorithm 2.2. Sequential k averages. Set T = (t1,., tk) on the first k data points Initializes the number of points n1, n2,..., nk \u2032 Repeat: Record the next example, x If ti is the nearest center to x: Increment ni Replace ti by ti + (1 / ni) (x \u2212 ti) This method, and many variants thereof, has been extensively studied in the literature on selforganizing maps [13]. It attempts to find centers T that optimize the k mean cost function: Cost (T) = specified data xmin."}, {"heading": "2.2 The target clustering", "text": "Unlike supervised learning tasks, which typically come with a unique, correct classification, clustering is ambiguous. An approach to clustering disambiguity is to identify an objective function such as k-means and then define the cluster task in such a way that the partition is found at minimal cost. Although there are situations for which this approach is well suited, many cluster applications are not inherently suitable for a specific objective function. Therefore, objective functions play an essential role in deriving cluster methods, but they do not circumvent the ambiguous nature of clustering. Target clusters are the partitions that a particular user is looking for in a data set. This notion has been used by Balcan et al. [7] to investigate what constraints of cluster structure make them efficiently identifiable in a batch environment. In this paper, we look at families of target clusters that meet different characteristics and ask whether incremental algorithms can identify such clusters."}, {"heading": "3 A basic limitation of incremental clustering", "text": "We begin by studying the limitations of incremental clustering as compared to the batch setting. One of the most basic types of cluster structure is what we call \"nice clustering,\" for brevity's sake. Definition 3.1 (\"nice clustering\") is a nice, if for all x, z \"X\" (y, x) < d, x \"every time x\" x \"and x.\" C. \"See Figure 2 for an observation 3.2.\" If we select a point from each cluster of a nice cluster from a nice C, the resulting set induced C. (Moreover, we are the minimum property under which this holds.) A nice cluster. \""}, {"heading": "4 A more restricted class of clusterings", "text": "The discovery that beautiful clusters cannot be detected by any incremental method, although they can easily be detected in a batch setting, points to the considerable limitations of incremental algorithms. Next, we ask if there is a well-behaved subclass of beautiful clusters that can be detected by incremental methods. Following [8, 2, 5, 1], we consider clusters in which the maximum cluster diameter is smaller than the minimum sequence separation between clusters. However, Definition 4.1 (Perfect Clustering) means that clustering C of (X, d) is perfect when d (x, y) < d (w, z) whenever x \u00b2 C, w \u00b2 C. Each perfect cluster is beautiful. But, unlike beautiful clusters, perfect clusters are unique: Lemma 4.2. For each (X, d) and k, there is at most a perfect cluster formation of (X, d)."}, {"heading": "5 Incremental clustering with extra clusters", "text": "It is not as if there would be some kind of clustering, if there would be some kind of clustering, if there would be such a kind of clustering, if there would be such a kind of clustering, if there would be such a kind of clustering, if there would be such a kind of clustering, if there would be such a kind of clustering. We also show that this is a tight.It is the utility of additional clusters for sequential k-means. We show that for a random arrangement of the data, and with additional centers, these algorithms can recover (a minor variant of) nice clustering. We also show that the random arrangement of the data is necessary. We show that additional centers, and with additional centers, these algorithms can recover (a minor variant of) nice clustering. We also show that the random arrangement is necessary for such results."}, {"heading": "5.3 A broader class of clusterings", "text": "We conclude by considering a substantial generalization of convenience that can be detected by incremental methods when additional centers are admitted. Definition 5.8 (Core). For each cluster formation C = {C1,.., Ck} of (X, d), the core of a cluster is the maximum subset of Coi-Ci, so d (x, z) < d (x, y) for all x-Ci, z-Coi and y 6-Ci. In a nice cluster formation, the core of a cluster is the entire cluster. We now only require that each core contains a significant fraction of points, and we show that the following simple sampling routine will find a refinement of the target cluster, even if the points are arranged adversarially. Algorithm 5.9. Algorithm subsample.Set T to the first 'elements for t =' + 1, '+ 2,.: Get a new point with probability of any element: Any Remove."}], "references": [{"title": "Clusterability: A theoretical study", "author": ["M. Ackerman", "S. Ben-David"], "venue": "Proceedings of AISTATS- 09, JMLR: W&CP, 5(1-8):53", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Weighted clustering", "author": ["M. Ackerman", "S. Ben-David", "S. Branzei", "D. Loker"], "venue": "Proc. 26th AAAI Conference on Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Characterization of linkage-based clustering", "author": ["M. Ackerman", "S. Ben-David", "D. Loker"], "venue": "COLT", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards property-based classification of clustering paradigms", "author": ["M. Ackerman", "S. Ben-David", "D. Loker"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Clustering oligarchies", "author": ["M. Ackerman", "S. Ben-David", "D. Loker", "S. Sabato"], "venue": "Proceedings of AISTATS-09, JMLR: W&CP, 31(6674)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust hierarchical clustering", "author": ["M.-F. Balcan", "P. Gupta"], "venue": "COLT, pages 282\u2013294", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["M.F. Balcan", "A. Blum", "S. Vempala"], "venue": "Proceedings of the 40th annual ACM symposium on Theory of Computing, pages 671\u2013680. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Clusterability detection and initial seed selection in large datasets", "author": ["S. Epter", "M. Krishnamoorthy", "M. Zaki"], "venue": "The International Conference on Knowledge Discovery in Databases, volume 7", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Consistency of single linkage for high-density clusters", "author": ["J.A. Hartigan"], "venue": "Journal of the American Statistical Association, 76(374):388\u2013394", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1981}, {"title": "Mathematical taxonomy", "author": ["N. Jardine", "R. Sibson"], "venue": "London", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1971}, {"title": "An impossibility theorem for clustering", "author": ["J. Kleinberg"], "venue": "Proceedings of International Conferences on Advances in Neural Information Processing Systems, pages 463\u2013470", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "The Art of Computer Programming: Seminumerical Algorithms, volume", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1981}, {"title": "Self-organizing maps", "author": ["T. Kohonen"], "venue": "Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Transactions on Information Theory, 28(2):129\u2013137", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "Proceedings of Fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281\u2013297. University of California Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1967}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["J.H. Ward"], "venue": "Journal of the American Statistical Association, 58:236\u2013244", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1963}, {"title": "A uniqueness theorem for clustering", "author": ["R.B. Zadeh", "S. Ben-David"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 639\u2013646. AUAI Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "Until now, differences in the input-output behaviour of clustering methods have only been studied in the batch setting [10, 11, 7, 4, 3, 5, 2, 17].", "startOffset": 119, "endOffset": 146}, {"referenceID": 10, "context": "Until now, differences in the input-output behaviour of clustering methods have only been studied in the batch setting [10, 11, 7, 4, 3, 5, 2, 17].", "startOffset": 119, "endOffset": 146}, {"referenceID": 6, "context": "Until now, differences in the input-output behaviour of clustering methods have only been studied in the batch setting [10, 11, 7, 4, 3, 5, 2, 17].", "startOffset": 119, "endOffset": 146}, {"referenceID": 3, "context": "Until now, differences in the input-output behaviour of clustering methods have only been studied in the batch setting [10, 11, 7, 4, 3, 5, 2, 17].", "startOffset": 119, "endOffset": 146}, {"referenceID": 2, "context": "Until now, differences in the input-output behaviour of clustering methods have only been studied in the batch setting [10, 11, 7, 4, 3, 5, 2, 17].", "startOffset": 119, "endOffset": 146}, {"referenceID": 4, "context": "Until now, differences in the input-output behaviour of clustering methods have only been studied in the batch setting [10, 11, 7, 4, 3, 5, 2, 17].", "startOffset": 119, "endOffset": 146}, {"referenceID": 1, "context": "Until now, differences in the input-output behaviour of clustering methods have only been studied in the batch setting [10, 11, 7, 4, 3, 5, 2, 17].", "startOffset": 119, "endOffset": 146}, {"referenceID": 16, "context": "Until now, differences in the input-output behaviour of clustering methods have only been studied in the batch setting [10, 11, 7, 4, 3, 5, 2, 17].", "startOffset": 119, "endOffset": 146}, {"referenceID": 0, "context": "To qualify the type of cluster structure present in data, a number of notions of clusterability have been proposed (for a detailed discussion, see [1] and [7]).", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "To qualify the type of cluster structure present in data, a number of notions of clusterability have been proposed (for a detailed discussion, see [1] and [7]).", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "[7], requires that every element be closer to data in its own cluster than to other points.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "It was shown by [7] that such clusterings are readily detected offline by classical batch algorithms.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "[8] requires that the minimum separation between clusters be larger than the maximum cluster diameter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "It is an incremental variant of Lloyd\u2019s algorithm [14, 15]: Algorithm 2.", "startOffset": 50, "endOffset": 58}, {"referenceID": 14, "context": "It is an incremental variant of Lloyd\u2019s algorithm [14, 15]: Algorithm 2.", "startOffset": 50, "endOffset": 58}, {"referenceID": 12, "context": "This method, and many variants of it, have been studied intensively in the literature on selforganizing maps [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "Another family of clustering algorithms with incremental variants are agglomerative procedures [10] like single-linkage [9].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "Another family of clustering algorithms with incremental variants are agglomerative procedures [10] like single-linkage [9].", "startOffset": 120, "endOffset": 123}, {"referenceID": 15, "context": "For instance, Ward\u2019s method of average linkage [16] is geared towards the k-means cost function.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "[7] to study what constraints on cluster structure make them efficiently identifiable in a batch setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] under the name \u201cstrict separation,\u201d this notion has since been applied in [2], [1], and [6], to name a few.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[7] under the name \u201cstrict separation,\u201d this notion has since been applied in [2], [1], and [6], to name a few.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "[7] under the name \u201cstrict separation,\u201d this notion has since been applied in [2], [1], and [6], to name a few.", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "[7] under the name \u201cstrict separation,\u201d this notion has since been applied in [2], [1], and [6], to name a few.", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "Indeed, in the batch setting, a unique nice k-clustering can be recovered by single-linkage [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": ", x \u2032 M \u2208 X such that \u2022 All interpoint distances are in the range [1, 2].", "startOffset": 66, "endOffset": 72}, {"referenceID": 1, "context": ", x \u2032 M \u2208 X such that \u2022 All interpoint distances are in the range [1, 2].", "startOffset": 66, "endOffset": 72}, {"referenceID": 7, "context": "Following [8, 2, 5, 1], among others, we consider clusterings in which the maximum cluster diameter is smaller than the minimum inter-cluster separation.", "startOffset": 10, "endOffset": 22}, {"referenceID": 1, "context": "Following [8, 2, 5, 1], among others, we consider clusterings in which the maximum cluster diameter is smaller than the minimum inter-cluster separation.", "startOffset": 10, "endOffset": 22}, {"referenceID": 4, "context": "Following [8, 2, 5, 1], among others, we consider clusterings in which the maximum cluster diameter is smaller than the minimum inter-cluster separation.", "startOffset": 10, "endOffset": 22}, {"referenceID": 0, "context": "Following [8, 2, 5, 1], among others, we consider clusterings in which the maximum cluster diameter is smaller than the minimum inter-cluster separation.", "startOffset": 10, "endOffset": 22}, {"referenceID": 6, "context": "Earlier work [7] has shown that that any nice clustering corresponds to a pruning of the tree obtained by single linkage on the points.", "startOffset": 13, "endOffset": 16}, {"referenceID": 11, "context": "It is well-known (see, for instance, [12]) that at any time t, the set T consists of ` elements chosen at random without replacement from {x1, .", "startOffset": 37, "endOffset": 41}], "year": 2014, "abstractText": "The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.", "creator": "LaTeX with hyperref package"}}}