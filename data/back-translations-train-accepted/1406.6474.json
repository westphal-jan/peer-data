{"id": "1406.6474", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2014", "title": "On the Convergence Rate of Decomposable Submodular Function Minimization", "abstract": "Submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision. However, minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of \"simple\" submodular functions. Empirically, this algorithm performs extremely well, but no theoretical analysis was given. In this paper, we show that the algorithm converges linearly, and we provide upper and lower bounds on the rate of convergence. Our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory.", "histories": [["v1", "Wed, 25 Jun 2014 06:52:33 GMT  (43kb)", "https://arxiv.org/abs/1406.6474v1", "19 pages, 3 figures"], ["v2", "Fri, 27 Jun 2014 18:12:03 GMT  (43kb)", "http://arxiv.org/abs/1406.6474v2", "19 pages, 3 figures"], ["v3", "Wed, 5 Nov 2014 07:19:00 GMT  (599kb)", "http://arxiv.org/abs/1406.6474v3", "17 pages, 3 figures"]], "COMMENTS": "19 pages, 3 figures", "reviews": [], "SUBJECTS": "math.OC cs.DM cs.DS cs.LG cs.NA", "authors": ["robert nishihara", "stefanie jegelka", "michael i jordan"], "accepted": true, "id": "1406.6474"}, "pdf": {"name": "1406.6474.pdf", "metadata": {"source": "CRF", "title": "On the Convergence Rate of Decomposable Submodular Function Minimization", "authors": ["Robert Nishihara", "Stefanie Jegelka", "Michael I. Jordan"], "emails": ["rkn@eecs.berkeley.edu", "stefje@eecs.berkeley.edu", "jordan@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.64 74v3 [m. ath. OC] 5 N"}, {"heading": "1 Introduction", "text": "It is about the question of whether and to what extent it is about a way in which people are able to play by the rules. (...) It is about the way in which people in the world play by the rules. (...) It is about the way in which people play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...)"}, {"heading": "1.1 Background", "text": "(F) F (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D) D (F) D (F) D (F) D (F) D (F) D (F) D) D (F) D (F) D (F) D (F) D (F) D) D (F) D (F) D) D (F) D (F) D) D (F) D (F) D) D (F) D) D (F) D (F) D) D (F) D (F) D (F) D) D (F) D (F) D (F) D (F) D) D (F) D (F) D) D (F) D (F) D) D (F) D (F) D) D (F) D (F) D) D (F) D) D (F) D (F) D (F) D) D (F) D (F) D) D (F) D (F) D (F) D) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F) D (F (F) D (F) D (F) D (F (F) D (F (F (F) D) D) D (F (F) D (F) D) D (F (F (F) D (F (F) (F) D (F) (F) D (F) (F) D (F) (F) D (F) (F (F (F) (F) D (F) (F) D (F) (F) D (F (F) (F) D (F) D (F ("}, {"heading": "2 Algorithm and Idea of Analysis", "text": "\"We will provide a detailed analysis of convergence between the two areas where we have the rate for all k and for some constants C1 and C2.\" \"We will adjust the rate for all k and for some constants C2 and C2.\" \"We will adjust the rate for all k and for some constants C2 and C2.\" \"We will adjust the rate for C2.\" \"We will adjust the rate for all k and for some constants C2 and C2.\" \"We will adjust the rate for C1 and C2.\" \"We will adjust the rate for C2.\" \"We will adjust the rate for all k and for some constants C1 and C2.\" \"We will adjust the rate C2.\" \"We will adjust the rate C2.\" \"We will adjust the rate for all k.\" We will specify the rate for all k and for some constants C2. \""}, {"heading": "3 The Upper Bound", "text": "We first derive an upper limit for the convergence rate of AP between polyhedra A and B. The results in this section are documented in Appendix A."}, {"heading": "3.1 A Condition for Linear Convergence", "text": "We start with a condition under which AP converges linearly between two closed convex sentences P and Q. This result is similar to that of Bauschke and Borwein [3, Corollary 3.14], but the rate we achieve is twice as high and is based on slightly weaker assumptions. We need a few definitions of Bauschke and Borwein [3]. Leave d (K1, K2) = inf (P, Q) \u2212 k2: k1 K1 (Q, P) = d (Q, P)}, (4) and leave v = Q \u2212 P (P, P) the sentences of \"tightest points\" asE = {p, P) = d (P, Q)} H = {q\\ d (Q, P)}, (4) and leave v = qQ \u2212 P 0 (see Figure 1) and the sentences of \"tightest points\" asE (see Figure 1). Note that H = E = + v, v and qP."}, {"heading": "3.2 Relating \u03ba\u2217 to the Angles Between Faces of the Polyhedra", "text": "In this section, we look at the case of the polyhedra P and Q, limiting ourselves to the angles between the pairs of their faces. In Lemma 3, we show that there is no decrease in the order of points P and Q \u00b2 generated by AP between P and Q. We treat the points p, for which Q (p) = 1 separately, because these are the points from which AP converges between P and Q \u00b2 in one step. This problem allows us to bind AP to p and limits at a later stage in the resulting order by initializing AP. Lemma 3. For each p \u00b2 P\\ E, it is now either the other way around (p) = 1 or 1 < for each q \u00b2 Q \u00b2 p). Similarly, for each q \u00b2 Q \u00b2 E, it is either the other way around (q) = 1 or 1 < for each p \u00b2 P \u00b2 q)."}, {"heading": "3.3 Angles Between Subspaces and Singular Values", "text": "To do this, we must first refer the human perspective to the individual values of certain matrices in Lemma 1, although this result is implicit in the characterization of the main angles between the sub-ranges indicated in [27, Section 1]. Ideas linking the angles between sub-ranges and eigenranges are also implicit in the characterization of the main angles between the sub-ranges and with the same number of columns by Diaconis et al. [14].Lemma 6, implicitly include S and T in the characterization of the main angles between the sub-ranges and with the same number of columns. If all the individual values of ST are equal to one, then cF (null (S), null (T)) = 0. Others, cF (null (S), null (T) is equal to the largest single value of ST."}, {"heading": "3.4 Bounding the Relevant Eigenvalues", "text": "To do this, we consider the matrix in the sense of the symmetrical normalized laplacian of a weighted graph. Let G be the graph whose vertices are indexed by (r, m) with 1 \u2264 r \u2264 R and 1 \u2264 m \u2264. Let the edge be weighted between vertices (r1, m1) and (r2, m2). We can assume that G is connected (the analysis in this case generally summarizes the analysis).The symmetrical normalized laplacian L of this chart is closely related to our interest rate matrix, (ST) = I \u2212 R \u2212 1R L. (7) This results in the largest eigenvalue of (ST) Nendice (ST) (ST 2), which results from the smallest eigenvalue of 2 (L)."}, {"heading": "4 A Lower Bound", "text": "To investigate the density of theorem 12, we construct a \"bad\" submodular function and decomposition that leads to a slow rate. Appendix B gives the formal details. Our example is an extended intersection function on a cycle: for each x, y and V we define Gxy as the intersection function of a single edge (x, y), Gxy = {1 if | A \u00b2 {x, y} | = 1 0 otherwise. Let us take N as even and R \u2265 2 and define the submodular function F lb = F lb1 + \u00b7 + F lbR, where F lb1 = G12 + G34 + \u00b7 + G (N \u2212 1) N F lb2 = G45 + \u00b7 \u00b7 \u00b7 \u00b7 + GN1 and F lbr = 0 for all r \u2265 3. The optimal solution to the problem of best approximation is the all nulos vector.Lemma 13. The cosine of the Friedrichs angle between A and Blaff (the verb \u00b2 is between A and B \u00b2)."}, {"heading": "5 Convergence of the Primal Objective", "text": "In this section we show that this result also implies the linear convergence of the object in problem (P3) and of the original discrete object in problem (P1). The proofs can be found in Appendix C. Define the matrix in which S is the matrix defined in Equation (5). Multiplication by area multiplication forms a vector (w1,.., wR) to \u2212 quot wr, where wr. RN for each r. set xk = \u2212 R1 / 2S, where S is the matrix defined in Equation (5). Multiplication by area multiplication forms a vector (w1,., wR) to \u2212 quot wr, where wr. RN for each r. set xk = quot."}, {"heading": "6 Discussion", "text": "In this paper, we analyze the projection methods for parallel SFM and give upper and lower limits for the linear rate of convergence, which means that the number of iterations required for accuracy of convergence is logarithmic in 1 / 3, not linear as in previous work [35]. Our rate is consistent across all submodular functions. Furthermore, our demonstration highlights how the number R of components and the facial structure of B affects the convergence rate. These insights can serve as guidelines for working with projection algorithms and tools in the analysis of special cases. For example, reducing R is often possible. Any collection of Fr that have disjoint support, such as the intersection functions corresponding to the rows or columns of a grid graph, can be summarized without affecting the projection sharders.Our analysis also shows the effects of additional properties of F. For example, we assume that F is separable, that is S. (S + S) (S = V) for some"}, {"heading": "A Upper Bound Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Theorem 2", "text": "To prove this theorem, we need the fact that the projection cards are clearly not expansive, that is, for a closed convex, not empty subset C'RD we have a closed convex, not empty subset C'RD. Let us now assume that there is a closed convex, not empty subset C'RD. Let us assume that there is a closed, convex, not empty subset C'RD. Let us assume that v = Qe and that v = Qe \u2212 e and that we have a closed convex, not empty subset C'RD. Let us assume that v = Qe \u2212 e and that we have a closed group."}, {"heading": "A.2 Connection Between \u03ba and cF in the Subspace Case", "text": "In this section we present a simple problem that establishes a relationship between \u0430 and cF in the case of sub-spaces U and V. We will apply this problem in several subsequent proofs. Lemma 17. Let U and V be subspaces and assume that u-cF (U, V) and u-cF (U, V) are 6 = 0. Then (a) that u-cF is u-cF (U, V) 2) \u2212 1 / 2 (u) = (1 \u2212 cF (U, V) 2) \u2212 1 / 2 if and only if and when they are. Evidence. Part (a) follows from the definition of cF. In fact, cF (U, V) results from part (U, V)."}, {"heading": "A.3 Proof of Lemma 3", "text": "Suffice it to prove the statement for p-P \u2212 E, where [p, e \u2212 E] denotes the line segment between p and e (which is contained in P by convectivity). See Figure 2 for a graph. If q-E, then it is the other way round. Thus, we can assume that q / E, which also implies d (p, E) > 0 and d (p, E) > 0. We have (p) = d (p, E) d (p, E) d (p, Q \u00b2) \u2264 p \u2212 e \u00b2 p \u2212 e \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p (q, E) > 0. (8) The first inequality applies because d (p, E) \u2264 p \u2212 e \u00b2 p \u2212 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p (q, E)."}, {"heading": "A.4 Proof of Proposition 4", "text": "Let's say that p & P & E (the case Q & E is the same), and let's say (the case Q & E is the same) that p & P & E (the case Q & E is the same), and let's (the case Q & Q is the same), and let's (the case Q / Q is the same), and let's (the case Q / Q is the same), the statement is obvious, so we can assume that p & P (the case Q) and Q (the case Q) is each a face of Q + 1. Either dim (Pj + 1), dim (Pj + 1) and dim (the case Q + 1) that there is a face of Q + 1."}, {"heading": "A.5 Proof of Lemma 6", "text": "We havecF (zero (S), zero (T) = cF (range (S '), range (T's), range (S's), range (T's), where the first equality uses the fact that zero (W) = range (W's) for the matrices W, and the second equality uses the fact that cF (U's, V's) = cF (U, V) for the sub-spaces U and V [6, fact 2,3]. Let S's and T's have the dimensions D's J and D's K, and let X and Y be the sub-spaces spanned by the columns of S's and T's. Suppose that J \u2264 1 \u2264 K's are the individual values of ST and Y's with the corresponding left-singular vectors U1,."}, {"heading": "A.6 Cheeger\u2019s Inequality", "text": "s inequality. G is a weighted, connected graph with vertex offset VG and edge weights (wij) i, j, j, VG. If we define the weighted degree of a vertex i as \u03b4i = \u2211 j 6 = i wij, we define the volume of a subset of vertices as the sum of their weighted degrees, vol (U) = \u0445 i, j, Uc wij. If we define the magnitude of the intersection between U and its supplement U c as the sum of the weights of the edges between U and U c, | E (U, U c) | = \u2211 i, j, Uc wij. The Cheeger constant is defined as ashG = min = 7 = U (VG | E (U, U c) as the sum of the weights of the edges between U and U c."}, {"heading": "A.7 Proof of Lemma 10", "text": "Evidence. We havemin (vol. U), vol (U c) \u2264 1 2 vol (VG) = 12 \u2211 (r, m) \u2211 (r, m) 6 = (r, m) | Arm-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-"}, {"heading": "B Results for the Lower Bound", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Some Helpful Results", "text": "In Lemma 19 we show how AP can be initialized between the subranges U and V near the origin to achieve exactly the worst-case rate of convergence. In Corollary 20 we then show that we can initialize AP between U and V to achieve the same worst-case rate of convergence. In Lemma 19 U and V become subranges with U 6, V and V 6 U. Then there is a nonzero u0, U, so that when we initialize AP at u0, the resulting sequences {uk} k 0 and {vk} k 0, 0, uk = cF (U, V) 2k, property (U, V) 2k, v0 and U."}, {"heading": "B.2 Proof of Lemma 13", "text": "Note that we can write A = {(s1, \u2212 s1,.. \u00b7 sN 2, \u2212 sN 2, \u2212 tN 2, t1, \u2212 t1,. \u2212 tN 2, 0,. \u00b7., 0,. \u00b7., 0,.., 0). () We can write aff (Blb) as the zero space of the matrix Tlb, 1 = Tlb, 1 Tlb, 2IN..... (IN, where the N \u00b7 N identity matrix IN repeats twice and where Tlb, 1 and Tlb, 2 the zero space of the matrix Tlb, 1 = 1 \u221a 2 1. 1. 1. 1. 1 Tlb, 2 = 1. (). (1 1 1.) We remember that we can define A as the zero space of the matrix S (5)."}, {"heading": "B.3 Lower Bound Illustration", "text": "The proof for theorem 14 shows that there is a certain number of a0-A, so that if we initialize AP between A and Blb at a0, we create a sequence {ak} k \u2265 0 satisfyingd (ak, E) = (1 \u2212 1R (1 \u2212 cos (2\u03c0N))) kd (a0, E), where E = A-Blb is the optimal set. In Figure 3, we plot the theoretical boundary in red and in blue the successive ratios d (ak + 1, E) / d (ak, E) for five gradients of AP between A and Blb with random initializations. If we had initialized AP at a0, the successive ratios would correspond exactly to 1 \u2212 1R (1 \u2212 cos (2\u03c0N). The graph of these ratios would coincide with the red line in Figure 3.Figure 3 and illustrate that the empirical behavior of AP between A and Blb often resembles the worst case, even if the initialization is random."}, {"heading": "C Results for Convergence of the Primal and Discrete Problems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Proof of Proposition 15", "text": "Let's assume first that s-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b"}, {"heading": "C.2 Proof of Theorem 16", "text": "Proposition 21: Let (w, s) and RN \u00b7 B (F) be a pair of primary-dual candidates for the minimization of 12, w \u00b2 and f (w), with a duality gap of 12, w \u00b2 + f (w) + 12, s \u00b2 2. If A is the suplevel row of w with the smallest value of F, then thenF (A) \u2212 s \u2212 (V) is. Use this result in our setting, remember that Ak is by definition the amount of the form {n \u00b2 V | (xk) n \u00b2 for any constant c with the smallest value of F (xk) n \u00b2). Let (w \u00b2, s \u00b2) and maxi \u00b2 (F) be a primary-dual optimal combination for the left version of problem (P3)."}, {"heading": "C.3 Running times", "text": "Theorem 16 implies that the number of iterations required for an accuracy of Q values is most2N2R2 log (\u221a 6FmaxNR1 / 2 \u0445 b0 \u2212 b *). (15) Each iteration involves minimizing each individual Fr. For comparison, the number of iterations required in Stobbe and Krause [35] is 24 \u221a NRFmax. The dependence of this algorithm on N and R is better, but its dependence on Fmax / \u0439 is worse. For example, to get the exact discrete solution, we need < minS, T | F (S) \u2212 F (T) |. This is a function evaluated for integers (in this case, the lower rate may be desirable), but can otherwise be very small. The constant Fmax may be of order O (N) in general (or even greater if the function becomes very negative).For empirical comparisons, we refer to the [Fmax-25] value of the QM is not constant."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Submodular functions describe a variety of discrete problems in machine learn-<lb>ing, signal processing, and computer vision. However, minimizing submodular<lb>functions poses a number of algorithmic challenges. Recent work introduced an<lb>easy-to-use, parallelizable algorithm for minimizing submodular functions that<lb>decompose as the sum of \u201csimple\u201d submodular functions. Empirically, this al-<lb>gorithm performs extremely well, but no theoretical analysis was given. In this<lb>paper, we show that the algorithm converges linearly, and we provide upper and<lb>lower bounds on the rate of convergence. Our proof relies on the geometry of<lb>submodular polyhedra and draws on results from spectral graph theory.", "creator": "LaTeX with hyperref package"}}}