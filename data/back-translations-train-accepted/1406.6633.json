{"id": "1406.6633", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2014", "title": "Active Learning and Best-Response Dynamics", "abstract": "We examine an important setting for engineered systems in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-defined game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. By using techniques from game theory and empirical processes, we prove positive (and negative) results on the denoising power of several natural dynamics. We then show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.", "histories": [["v1", "Wed, 25 Jun 2014 16:34:35 GMT  (628kb,D)", "http://arxiv.org/abs/1406.6633v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["maria-florina balcan", "christopher berlind", "avrim blum", "emma cohen", "kaushik patnaik", "le song"], "accepted": true, "id": "1406.6633"}, "pdf": {"name": "1406.6633.pdf", "metadata": {"source": "CRF", "title": "Active Learning and Best-Response Dynamics", "authors": ["Maria-Florina Balcan", "Chris Berlind", "Avrim Blum", "Emma Cohen", "Kaushik Patnaik", "Le Song"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the effectiveness of active learning is capable of rapidly degrading as noise rates are high. [5] In this work, we introduce ourselves and analyze a novel setting in which the laboratory information is held by high-level agents (such as sensors or micro-robots); we show how to quickly degrade the dynamics between agents with simple play. [6] This allows us to harness the power of active learning (such as the ability to cross borders between countries); we show how to compare the dynamics between agents with simple methods that we can quickly denounce the system; this allows us to harness the power of active learning (especially the recent advances in the agnostic field of active learning), leading to efficient learning."}, {"heading": "1.1 Related Work", "text": "There has been significant work in the field of active learning (see [10, 14] and references to it), but it is known that active learning can only offer significant benefits in low-noise scenarios [5]. There has also been extensive work to analyze the performance of simple dynamics in consensus games [6, 8, 13, 12, 3, 2]. However, this work has focused on achieving some balances or states of low social cost, whereas we are primarily interested in approaching a particular desired configuration which, as we show below, represents an approximate balance."}, {"heading": "2 Setup", "text": "We assume that we have a large number of N agents (e.g. sensors), which are evenly distributed in a geometric area, which we consider to be a sphere of unity in Rd for the sake of concreteness. There is an unknown linear separator, so that in the initial state each sensor on the positive side of this separator is positive regardless of the probability \u2265 1 \u2212 \u03b7 and everyone on the negative side is negative regardless of the probability \u2265 1 \u2212 \u03b7. The quantity \u03b7 < 1 / 2 is the noise rate."}, {"heading": "2.1 The basic sensor consensus game", "text": "The sensors will denote themselves by regarding themselves as players in a particular consensual game and performing a simple dynamic in the game that leads to a certain state of equilibrium. Specifically, the game is defined as follows and is parameterized by a communication radius r that should be thought of as small. Consider a graph in which the sensors are vertices, and any two sensors in the distance r are connected to each other by an edge. Each sensor is in one of two states, positive or negative. The payout that a sensor receives is its correlation with its neighbors: the fraction of neighbors in the same state as it minus the fraction in the opposite state. Thus, if a sensor is in the same state as all its neighbors, then its payout is likely if it is in the opposite state of all its neighbors, then its payout is \u2212 1, and if sensors are in a uniformly random state, then the expected payout is 0."}, {"heading": "3 Analysis of the denoising dynamics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Simultaneous-move dynamics", "text": "We will start by providing a positive theoretical guarantee for simultaneous motion dynamics with high probability. We will use the following standard concentration limit: Theorem 1 (Amber, 1924) Let X = \u2211 Ni = 1Xi be a sum of independent random variables, so that | Xi \u2212 E [Xi] | \u2264 M applies to all i. Then, for any t > 0, P [X \u2212 E [X] > t] \u2264 exp (\u2212 t2 2 (Var [X] + Mt / 3). Theorem 2 If N (r / 2) d (1 2 \u2212 \u03b7) 2 ln (1) l (r / 2) d (1) d (1 \u2212 2) exp (1), then with probability 1 \u2212 III). Each sensor at the distance r from the separator has the correct designation. Note that since a bandwidth of 2r via a linear separator implies the probability massO (r). Theorem 2 implies a high probability of updating with a sensory constant."}, {"heading": "3.2 A negative result for arbitrary-order asynchronous dynamics", "text": "We compare the above positive result with a negative result for asynchronous movements of any order. In particular, we show that for all d \u2265 1, for sufficiently large N, there is a high probability of an update order that causes all sensors to become negative. Theorem 3 For an absolute constant c > 0, if r \u2264 1 / 2 and sensors start with noise rate \u03b7, and N \u2265 16 (cr) d\u03c62 (ln8 (cr) d\u03c62 + ln1\u03b4), where \u03c6 = \u03c6 = min (\u03b7, 12 \u2212 \u03b7), then there is a probability of at least 1 \u2212 \u03b4 having a sequence of agents, so that asynchronous updates in this sequence cause all points to have the same label.PROOF SKETCH: Consider the case d = 1 and a target function x > 0. Any underdistance of [\u2212 1, 1] the width r has a probability mass r / 2, and let m = rN / 2 reach the expected number of points > within a half-time period, both of which are high."}, {"heading": "3.3 Random order dynamics", "text": "While there is a high probability that there will be no erroneous updates between the sensors within the first 3N / 4 updates, we now show that we can get positive theoretical guarantees for random order. (The high idea of the analysis is to divide the sensors into three groups: those located within the distance of the destination separator, those located within the distance between r and 2r from the destination separator, and then the rest.) Those located within the distance between r and 2r from the separator may also update incorrectly (due to the \"corruption\" of the neighbors in the distance < r from the separator that was previously updated incorrectly), but we will show that this most likely only happens in the last 1 / 4 of the order."}, {"heading": "4 Query efficient polynomial time active learning algorithm", "text": "Recently, Awasthi et al. [1], the first polynomial-time-active learning algorithm capable of learning linear separators, gave errors about the uniform distribution in the presence of agnostic noise of the rate O (). Moreover, the algorithm is so with optimal query complexity of O (d log 1 /). This algorithm is ideal for our setting because (a) the sensors are uniformly distributed, and (b) the result of the best response error dynamics is noise that is low but potentially strongly coupled (hence that it corresponds to the low noise agnostic model). In our experiments (Section 5), we show that this algorithm, in combination with the dynamics of the best response, achieves a small error from a small number of queries, exceeding active and passive learning algorithms without the most responsive denoizing steps, as well as the passive learning algorithms with respective intuitive algorithms. [1] Here we briefly describe the space behind the respective intuition.Here]"}, {"heading": "5 Experiments", "text": "In our experiments, we try to determine whether our overall algorithm of best-response dynamics combined with active learning is effective in denouncing the sensors and reaching the target limit. Experiments were conducted on synthetic data and the target limit was determined as a randomly chosen linear separator by the origin. To simulate noise scenarios, we corrupted the true sensor labels using two different methods: 1) flip the sensor labels with probability, and 2) flip the randomly chosen sensor labels and all their neighbors around bags with which we corrupt the true sensor labels."}, {"heading": "5.1 Results", "text": "Here we report the results for N = 10000 and r = 0.1. Results for experiments with other values of the parameters are included in Appendix C. Each reported value is an average of over 50 independent tests. Denoising effectiveness. Figure 2 (left) shows that the proportion of sensors with incorrect labels is more difficult to denode after applying 100 rounds of synchronous denoising updates at different initial error rates. In this case, the final noise rate increases with the initial noise rate, but is almost always lower than the initial level of the noises. Synchronous vs. asynchronous updates. To compare synchronous and asynchronous updates, we record the noise rate as a function of the number of update rounds in Figure 2 (right)."}, {"heading": "6 Discussion", "text": "We show, through theoretical analysis and experiments with synthetic data, that the dynamics of the best on-site response can clearly denounce a highly noisy sensor network without destroying the underlying signal, enabling quick learning from a small number of label queries. Another way to look at this result is that the cost function that is really important to us is that a sensor receives a price of 1 because it has a label that matches the true cost function, but unfortunately it cannot measure this directly; therefore, we give each sensor a \"proxy object\" that it can measure whether it agrees with its neighbors. Our positive theoretical guarantees show that the update after this proxy will be well in accordance with the true cost function and can be applied to both synchronous and asynchronous updates in order. This is also confirmed in the experiments that our negative result in Section 3.2 is an adversarial ordermal dynamics, which is an entire system that can move from the left to the right of the updated dynamics in a general order."}, {"heading": "Acknowledgments", "text": "This work was partially supported by ONR grants N00014-09-1-0751, NSF grants CCF-0953192, CCF1101283 and CCF-1101215 and AFOSR grants FA9550-09-1-0538."}, {"heading": "A Arbitrary Order and Conservative Best Response Dynamics", "text": "Given the negative outcome of Section 3.2, the basic aspirations of separation dynamics, the majority of neighbors on both sides of the separator, we are not appropriate if no assumptions can be made about the order in which the sensors perform their updates. (The idea of this dynamic is that sensors only change their state if they are sure they are not on the wrong side of the separator.) This dynamic is not as practical as regular best-response dynamics because it requires much more calculation on the part of the sensors. Nevertheless, it shows that positive results are actually achievable for arbitrary update commands. (In this dynamic, the sensors are as follows: 1. If we have a majority of neighbors on both sides of the separator for all linear dividers through the position of the sensors, then flip to positive.)"}, {"heading": "B Additional Proofs", "text": "Proof of theory 3: Suppose the label is given by characters (w \u00b7 x). We show that if sensors are updated in increasing order from w \u00b7 x (from the most negative to the most positive), then most likely all sensors will be updated to negative labels. Consider what we see when we update the sensor to x. Suppose we do not yet have a positive label (since we have a positive label), all points x \u00b2 with w \u00b7 x \u00b2 \u00b2 \u00b2 are based on the difference between the number of negative and positive points in the neighborhood of x, which we call the sum of the (N \u2212 1) independent variables i. The expected labels of the nearby points depend on the location of x, so we apply Amber's theorem 2 to the difference between the negative and positive points in the neighborhood of x."}, {"heading": "C Additional Experimental Results", "text": "All subsequent experiments were conducted with an initial noise rate of 7.4% to 1.6%, which helps with both active noise and active noise, and the results were averaged over 20 attempts with 50 iterations. (The results of the results of theorem 2 in Section 3.1 for synchronous updates are expected to improve sensor learning performance, which in turn should improve the generalization error rate after generalization.) Figures 5, 6 and 7 show the generalization error after generalization after generalization after generalization after generalization after generalization. (Figures 5, 6 and 7 show the generalization error after generalization after generalization after generalization.) For a budget of 30 labels on random noise, the noise rate after generalization is 12.0% with 1000 sensors after 1.7% with 25,000 sensors, and with this improvement we see a corresponding decrease after generalization from 1.6% to 1.6%."}], "references": [{"title": "The power of localization for efficiently learning linear separators with noise", "author": ["P. Awasthi", "M.F. Balcan", "P. Long"], "venue": "STOC,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "The price of uncertainty", "author": ["M.-F. Balcan", "A. Blum", "Y. Mansour"], "venue": "EC,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Circumventing the price of anarchy: Leading dynamics to good behavior", "author": ["M.-F. Balcan", "A. Blum", "Y. Mansour"], "venue": "SICOMP,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical active learning algorithms", "author": ["M.F. Balcan", "V. Feldman"], "venue": "NIPS,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Importance weighted active learning", "author": ["A. Beygelzimer", "S. Dasgupta", "J. Langford"], "venue": "ICML,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "The statistical mechanics of strategic interaction", "author": ["L. Blume"], "venue": "Games and Economic Behavior, 5:387\u2013 424,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": "OUP Oxford,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning, local interaction, and coordination", "author": ["G. Ellison"], "venue": "Econometrica, 61:1047\u20131071,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "A statistical theory of active learning", "author": ["S. Hanneke"], "venue": "Foundations and Trends in Machine Learning, pages 1\u2013212,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association, 58(301):13\u201330, March", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1963}, {"title": "Maximizing the spread of influence through a social network", "author": ["D. Kempe", "J. Kleinberg", "E. Tardos"], "venue": "Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201903, pages 137\u2013146. ACM,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Contagion", "author": ["S. Morris"], "venue": "The Review of Economic Studies, 67(1):57\u201378,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Active Learning", "author": ["B. Settles"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Mathematical Theories of Interaction with Oracles", "author": ["L. Yang"], "venue": "PhD thesis, CMU Dept. of Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "In large part this is because the effectiveness of active learning is known to quickly degrade as noise rates become high [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "However, because of the high noise rate, learning this function directly would require a very large number of queries to be made (for noise rate \u03b7, one would necessarily require \u03a9( 1 (1/2\u2212\u03b7)2 ) queries [4]).", "startOffset": 202, "endOffset": 205}, {"referenceID": 0, "context": "This then nicely meshes with recent advances in agnostic active learning [1], allowing for the center to learn a good approximation to the target function from a small number of queries to the agents.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "In particular, in addition to proving theoretical guarantees on the denoising power of game-theoretic agent dynamics, we also show experimentally that a version of the agnostic active learning algorithm of [1], when combined with these dynamics, indeed is able to achieve low error from a small number of queries, outperforming active and passive learning algorithms without the best-response denoising step, as well as outperforming passive learning algorithms with denoising.", "startOffset": 206, "endOffset": 209}, {"referenceID": 8, "context": ", see [10, 14] and references therein), yet it is known active learning can provide significant benefits in low noise scenarios only [5].", "startOffset": 6, "endOffset": 14}, {"referenceID": 12, "context": ", see [10, 14] and references therein), yet it is known active learning can provide significant benefits in low noise scenarios only [5].", "startOffset": 6, "endOffset": 14}, {"referenceID": 4, "context": ", see [10, 14] and references therein), yet it is known active learning can provide significant benefits in low noise scenarios only [5].", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "There has also been extensive work analyzing the performance of simple dynamics in consensus games [6, 8, 13, 12, 3, 2].", "startOffset": 99, "endOffset": 119}, {"referenceID": 7, "context": "There has also been extensive work analyzing the performance of simple dynamics in consensus games [6, 8, 13, 12, 3, 2].", "startOffset": 99, "endOffset": 119}, {"referenceID": 11, "context": "There has also been extensive work analyzing the performance of simple dynamics in consensus games [6, 8, 13, 12, 3, 2].", "startOffset": 99, "endOffset": 119}, {"referenceID": 10, "context": "There has also been extensive work analyzing the performance of simple dynamics in consensus games [6, 8, 13, 12, 3, 2].", "startOffset": 99, "endOffset": 119}, {"referenceID": 2, "context": "There has also been extensive work analyzing the performance of simple dynamics in consensus games [6, 8, 13, 12, 3, 2].", "startOffset": 99, "endOffset": 119}, {"referenceID": 1, "context": "There has also been extensive work analyzing the performance of simple dynamics in consensus games [6, 8, 13, 12, 3, 2].", "startOffset": 99, "endOffset": 119}, {"referenceID": 7, "context": "Since the probability mass of the r-ball around x is at least (r/2)d (see discussion in proof of Theorem 2), so long as N \u2212 1 \u2265 (2/r)d \u00b7 max[8, 4 2 ] ln( \u03b4 ), with probability 1 \u2212 \u03b4 2N , point x will have mx \u2265 2 2 ln( \u03b4 ) neighbors (by Chernoff bounds), each of which is at least as likely to be on x\u2019s side of the target as on the other side.", "startOffset": 140, "endOffset": 149}, {"referenceID": 3, "context": "Since the probability mass of the r-ball around x is at least (r/2)d (see discussion in proof of Theorem 2), so long as N \u2212 1 \u2265 (2/r)d \u00b7 max[8, 4 2 ] ln( \u03b4 ), with probability 1 \u2212 \u03b4 2N , point x will have mx \u2265 2 2 ln( \u03b4 ) neighbors (by Chernoff bounds), each of which is at least as likely to be on x\u2019s side of the target as on the other side.", "startOffset": 140, "endOffset": 149}, {"referenceID": 1, "context": "Since the probability mass of the r-ball around x is at least (r/2)d (see discussion in proof of Theorem 2), so long as N \u2212 1 \u2265 (2/r)d \u00b7 max[8, 4 2 ] ln( \u03b4 ), with probability 1 \u2212 \u03b4 2N , point x will have mx \u2265 2 2 ln( \u03b4 ) neighbors (by Chernoff bounds), each of which is at least as likely to be on x\u2019s side of the target as on the other side.", "startOffset": 140, "endOffset": 149}, {"referenceID": 0, "context": "We can now combine this result with the efficient agnostic active learning algorithm of [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 13, "context": "In particular, applying the most recent analysis of [9, 15] of the algorithm of [1], we get the following bound on the number of queries needed to efficiently learn to accuracy 1\u2212 with probability 1\u2212 \u03b4.", "startOffset": 52, "endOffset": 59}, {"referenceID": 0, "context": "In particular, applying the most recent analysis of [9, 15] of the algorithm of [1], we get the following bound on the number of queries needed to efficiently learn to accuracy 1\u2212 with probability 1\u2212 \u03b4.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "Corollary 1 There exists constant c1 > 0 such that for r \u2264 /(c1 \u221a d), and N satisfying the bound of Theorem 2, if sensors are each initially in agreement with the target linear separator independently with probability at least 1 \u2212 \u03b7, then one round of best-response dynamics is sufficient such that the agnostic active learning algorithm of [1] will efficiently learn to error using only O(d log 1/ ) queries to sensors.", "startOffset": 341, "endOffset": 344}, {"referenceID": 9, "context": "Proof: First, the guarantee on mx follows immediately from the fact that the probability mass of the ball around each sensor x is at least (r/2)d, so for appropriate c4 the expected value of mx is at least max[8, 2c1 \u03b32 ] ln(4N/\u03b4), and then applying Hoeffding bounds [11, 7] and the union bound.", "startOffset": 267, "endOffset": 274}, {"referenceID": 6, "context": "Proof: First, the guarantee on mx follows immediately from the fact that the probability mass of the ball around each sensor x is at least (r/2)d, so for appropriate c4 the expected value of mx is at least max[8, 2c1 \u03b32 ] ln(4N/\u03b4), and then applying Hoeffding bounds [11, 7] and the union bound.", "startOffset": 267, "endOffset": 274}, {"referenceID": 9, "context": "However, we use two facts: (1) by Lemma 2, we can set c4 so that with high probability the total contribution of neighbors that have already updated is at most \u03b38mx in the incorrect direction (since the outside-neighbors will have updated correctly, by induction), and (2) by standard concentration inequalities [11, 7], with high probability at least 18mx neighbors of x have not yet updated.", "startOffset": 312, "endOffset": 319}, {"referenceID": 6, "context": "However, we use two facts: (1) by Lemma 2, we can set c4 so that with high probability the total contribution of neighbors that have already updated is at most \u03b38mx in the incorrect direction (since the outside-neighbors will have updated correctly, by induction), and (2) by standard concentration inequalities [11, 7], with high probability at least 18mx neighbors of x have not yet updated.", "startOffset": 312, "endOffset": 319}, {"referenceID": 0, "context": "[1] gave the first polynomial-time active learning algorithm able to learn linear separators to error over the uniform distribution in the presence of agnostic noise of rate O( ).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Here, we briefly describe the algorithm of [1] and the intuition behind it.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "[1, 9, 15] show that by setting the parameters appropriately (in particular, bk = \u0398(1/2k) and rk = \u0398(1/2k)), the algorithm will achieve error using only k = O(log 1/ ) rounds, with O(d) label requests per round.", "startOffset": 0, "endOffset": 10}, {"referenceID": 13, "context": "[1, 9, 15] show that by setting the parameters appropriately (in particular, bk = \u0398(1/2k) and rk = \u0398(1/2k)), the algorithm will achieve error using only k = O(log 1/ ) rounds, with O(d) label requests per round.", "startOffset": 0, "endOffset": 10}, {"referenceID": 0, "context": "[1] described in Section 4 to decide which sensors to query and obtain a linear separator.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "For the active algorithm, we used parameters asymptotically matching those given in Awasthi et al [1] for a uniform distribution.", "startOffset": 98, "endOffset": 101}], "year": 2014, "abstractText": "We examine an important setting for engineered systems in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-defined game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. By using techniques from game theory and empirical processes, we prove positive (and negative) results on the denoising power of several natural dynamics. We then show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.", "creator": "LaTeX with hyperref package"}}}