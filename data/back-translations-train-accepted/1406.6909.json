{"id": "1406.6909", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2014", "title": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks", "abstract": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).", "histories": [["v1", "Thu, 26 Jun 2014 15:07:14 GMT  (667kb,D)", "http://arxiv.org/abs/1406.6909v1", null], ["v2", "Fri, 19 Jun 2015 11:43:36 GMT  (7431kb,D)", "http://arxiv.org/abs/1406.6909v2", "PAMI submission. Includes matching experiments as inarXiv:1405.5769v1. Also includes new network architectures, experiments on Caltech-256, experiment on combining Exemplar-CNN with clustering"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["alexey dosovitskiy", "philipp fischer", "jost tobias springenberg", "martin riedmiller", "thomas brox"], "accepted": true, "id": "1406.6909"}, "pdf": {"name": "1406.6909.pdf", "metadata": {"source": "CRF", "title": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks", "authors": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller"], "emails": ["dosovits@cs.uni-freiburg.de", "springj@cs.uni-freiburg.de", "riedmiller@cs.uni-freiburg.de", "brox@cs.uni-freiburg.de"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to fight, to fight, to fight, to fight, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "1.1 Related Work", "text": "In this context, it should be noted that the measures we have mentioned are not only measures, but also measures that are taken by politics. [...] In this context, I would like to point out that the measures that we have taken are not measures, but measures that are taken by politics. [...] It is not that these measures are taken by politics. [...] It is not that they are taken by politics. [...] It is not that they are taken by politics. [...] It is rather that they are taken by politics. [...] It is not that they are taken by politics. [...] It is as if they are taken by politics, as if they are taken by the economy, and as if they are taken by the economy. [...]"}, {"heading": "2 Creating Surrogate Training Data", "text": "The input to the training sequence is a series of blank images that come from approximately the same distribution as the images to which we will later apply the learned properties. We are interested in sample N-shaped image fields of 32 x 32 pixels from different images at different positions and scales that form the initial image set X = {x1,.. xN}. We are interested in image fields that contain objects or parts of objects, so sample only from regions with significant progressions. We define a family of transformations {T\u03b1 | \u03b1 \u00b2 A} that are parameterized by vectors \u03b1 \u00b2 A, where A is the set of all possible parameter vectors. Each transformation T\u03b1 is a composition of elementary transformations from the following list: \u2022 Translation: vertical or horizontal transformation within a distance within 0.2 of the image size."}, {"heading": "3 Learning Algorithm", "text": "Considering the amount of transformed image fields, we declare each of these groups a class by assigning the label i to class Sxi. Next, we train a CNN to distinguish between these replacement classes. Formally, we minimize the following loss function: L (X) = [X] = [X] = [I, Txi], (1) where l (i, Txi) is the loss on the transformed sample Txi with (replacement) label i. We use a CNN with a softmax output layer and optimize the multinomial probability of negative protocols of network output, hence in our case (i, Txi) = M (ei, f (Txi))) = \u2212 < y, log f > = \u2212 k yk log fk, (2) where f (\u00b7) is the function that calculates the values of the output layer of CNN taking into account the input data, and ei is the standard ith base vector."}, {"heading": "3.1 Formal Analysis", "text": "We define the random vector of the transformation parameters by g (x) the vector of activations of the second to last level of the network when we present the input field x, by W the matrix of weights of the last network layer, by defining the softmax activation functionsoftmax (z) = x) the last activation of the softmax level before applying the softmax level and by f (x) the objective function (3) with the loss (2) the form xi-X level. [\u2212 < ei (T\u03b1xi) > + log-exp (z) = exp (z) exp (4) the objective function (3) takes the form xi-X level."}, {"heading": "3.2 Conceptual Comparison to Previous Unsupervised Learning Methods", "text": "Suppose we want to learn unsupervised a feature representation that is useful for a recognition task, such as classification. Assigning input images x to a feature representation g (x) should then meet two requirements: (1) There must be at least one feature that is similar for images of the same category y (invariance); (2) there must be at least one feature that is sufficiently different for images of different categories (ability to distinguish). That is, if a representation is learned from which a given sample can be perfectly reconstructed, then the representation is expected to include information about the category of the sample (ability to distinguish) x. In addition, the learned representation should be transferred to variations in the samples that are irrelevant for the classification."}, {"heading": "4 Experiments", "text": "In order to compare our discriminatory approach with previous methods of uncontrolled feature learning, we report on the classification results of the STL-10 [22], CIFAR-10 [23] and Caltech-101 [24] datasets. Furthermore, we evaluate the influence of augmentation parameters on classification performance and examine the invariance display properties of the network."}, {"heading": "4.1 Experimental Setup", "text": "The data sets we are testing are in the number of classes (10 for CIFAR and STL, 101 for Caltech) and in the number of samples per class. STL is particularly suitable for unattended learning as it contains a large set of unattended samples. In all experiments (except for the data transfer in the supplement) we have found replacement data from the unattended subset of STL-10."}, {"heading": "4.2 Classification Results", "text": "In Table 1, we compare our method with several unattended learning methods, including the current state of the art on each dataset. We do not compare our method with supervised methods that use class labels to represent learning functions. In addition, we show the dimensionality of the feature vectors generated by each method before final pooling. The small network was trained on 8000 replacement classes of 150 samples each and the large class of 16000 classes of 100 samples each. Features extracted from the larger network match or exceed the best previous result on all datasets, despite the fact that the dimensionality of the feature vector is smaller than most other approaches and that the networks are trained on the STL-10 dataset without labeling (i.e. they are used in a transfer learning mode when applied to CIFAR-10 and Caltech 101 datasets). The performance increase is particularly pronounced when only a few marked samples are available for SVM training (i.e. they are used in a transfer learning mode when applied to CIFAR and Caltech 101 datasets)."}, {"heading": "4.3 Detailed Analysis", "text": "In addition to the general classification results, our approach examined the effects of three design options and validated the invariance characteristics of the acquired traits. Additional experiments on the influence of the data set from which the \"seed fields\" are extracted can be found in the supplementary material. We used the \"small\" network in all the experiments described below."}, {"heading": "4.3.1 Number of Surrogate Classes", "text": "We varied the number N of the surrogate classes between 50 and 32000. As a health test, we also tried the classification with random filters. The results are in Fig. 3.Obviously, the classification accuracy increases with the number of surrogate classes until it reaches an optimum for about 8000 surrogate classes, after which it has not changed or even decreased. This is to be expected: the greater the number of surrogate classes, the more likely it is to take very similar or even identical samples, which are difficult or impossible to distinguish. However, few such cases do not affect the classification performance, but once such collisions dominate the set of surrogate labels, the discriminatory loss is no longer acceptable. The classification problem becomes too difficult and the adaptation of the network to the surrogate task is no longer successful. In order to verify the validity of this explanation, we also draw in Fig. 3 the classification error on the validation set (taken from the surrogate data), which is calculated according to the network training."}, {"heading": "4.3.2 Number of Samples per Surrogate Class", "text": "Fig. 4 shows classification accuracy when the number K of training samples per surrogate class varies approximately between 1 and 300. Performance improves with more samples per surrogate class and is saturated at approximately 100 samples, suggesting that this amount is sufficient to approximate the formal target of Equivalent (3), so the number of samples does not change significantly in the optimization problem. On the other hand, if the number of samples is too small, there is insufficient data to learn the desired inventory display characteristics. 4.3.3 Types of Transformations We did not significantly vary the transformations used to create the surrogate data to analyze their impact on final classification performance. The number of \"seed fields\" has been fixed, and the result is shown in Fig. 5. 0 \"corresponds to the application of random compositions of all elementary transformations: scaling, rotation, transformation, color variation, color variation, color variation and color variation."}, {"heading": "4.3.4 Invariance Properties of the Learned Representation", "text": "In a final experiment, we analyzed the extent to which the representation learned by the network has an invariant effect on the transformations applied during the training. We sampled 500 images from the STL-10 test set and applied a series of transformations (translation, rotation, contrast, color) to each image. However, to avoid empty regions beyond the image boundaries when applying spatial transformations, we cropped the central 64 x 64 pixel sub-patch from each 96 x 96 pixel image. We then applied two measurements of inventory to these patches. First, as an explicit measure of inventory, we calculated the normalized euclidean distance between normalized feature vectors of the original image patch and the transformed image [11] (see the supplementary material for details)."}, {"heading": "5 Discussion", "text": "We have proposed a way to use a discriminatory target for unsupervised feature learning by training a CNN without class labeling. The core idea is to generate a set of surrogate labels through data augmentation. The features learned by the network result in a great improvement in classification accuracy compared to features obtained using previous unsupervised methods. These results strongly suggest that a discriminatory target is superior to goals previously used for unsupervised feature learning. A potential deficiency of the proposed method is that it does not scale arbitrarily large data sets in its current state. Two likely reasons for this are that (1) as the number of surrogate classes increases, many of them become similar, which contradicts the discriminatory target, and (2) the surrogate task we are using is relatively simple and does not allow the network to learn inventory on complex variations, such as 3D point-of-view changes that might suggest a more viable approach to the proposed variations."}, {"heading": "Acknowledgements", "text": "We acknowledge the funding provided by the ERC Starting Grant VideoLearn (279401); the work was also partially supported by the Excellence Cluster BrainLinks-BrainTools funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)."}, {"heading": "1 Formal Analysis", "text": "In this section we present the evidence for the formal analysis of section 3.1 of our thesis. Sentence 1 The function Z (x) = log-sum-exp (x) is given by the span (1) proof SinceZ (x) = log-exp (x) - 1 = log-exp (x) 2 (1Tu) 2 (1Tu) diag (u) \u2212 uuuT) (8) we have to prove the convexity of the log-sum-exp function (x). Hessian convexity 2 of this function is given as log-2Z (x) = 1 (1Tu) diag (u) \u2212 uuuT) (9) with u = exp (x) and 1 x-sum-exp function. To show the convexexexity, we have to prove that zT-2Z (x) z-\u03b1uk."}, {"heading": "2 Details on Training Procedure", "text": "Here we describe in detail which network architectures we have tried and explain the process of the network training."}, {"heading": "2.1 Network Architecture", "text": "We tested various network architectures in combination with our training procedure. They are coded as follows: NcF stands for a Convolutionary Layer with N filters of size F \u00d7 F pixels, Nf stands for a fully connected layer with N neurons. For example, 64c5-64c5-128f denotes a network with two Convolutionary Layers with 64 filters, each containing 5 \u00d7 5 pixels, followed by a fully connected layer with 128 neurons. The last specified layer is always followed by a Softmax layer, which serves as network output. We used 2 \u00d7 2 max pooling on the outputs of the first and second Convolutionary Layers. As stated in the paper, in our experiments we used a 64c5-64c5-64c5-128f architecture to evaluate the influence of various components of the augmentation process (we refer to this architecture as the \"small\" network). A large network encoded as 64c5-64c5-64c5-64c5-12625f architecture was kept as complete in our experiments."}, {"heading": "2.2 Training the Networks", "text": "We adopted the usual practice of training the stochastic gradient drop network with a fixed pulse of 0.9. We started with a learning rate of 0.01 and gradually decreased the learning rate during the training. That is, until there was no improvement in validation errors, we then decreased the learning rate by a factor of 3 and repeated the process until convergence."}, {"heading": "3 Experiments", "text": "We report here on two further experiments that examine the influence of different aspects of the algorithm on the quality of the learned characteristics. Details of how we measure the invariance of character representations are described in Section 4.3.4 of the paper."}, {"heading": "3.1 Influence of the Network Architecture on Classification Performance", "text": "We are conducting an additional experiment to evaluate the impact of network architecture on classification performance; the results of this experiment are shown in Table 2. All networks have been trained with a surrogate attraction set that includes either 8000 classes of 150 samples each or 16000 classes of 100 samples each (for larger networks) We vary the number of layers, layer sizes and filter sizes. Classification accuracy generally improves when the network size indicates that our classification problem extends well to relatively large networks without overhauling them."}, {"heading": "3.2 Influence of the Dataset", "text": "We applied our feature-learning algorithm to images from three datasets - STL-10 unlabeled dataset, CIFAR-10, and Caltech-101 - and evaluated the performance of the learned feature representations in classification tasks on these datasets. We used the \"small\" network (64c5-64c5128f) for this experiment. We show first-layer filters we learned from the three datasets in Figure 7. Notice how the filters differ qualitatively depending on which dataset they were trained on. Classification results are shown in Table 3. The best classification results for each dataset are obtained by training the patches extracted from the dataset itself. However, the difference is not drastic, suggesting that the features learned can be generalized to other datasets."}, {"heading": "3.3 Details of computing the measure of invariance", "text": "We now explain in detail and motivate the calculation of the normalized Euclidean distance, which is used as a measure of the invariance of the paper. First, we calculate feature vectors of all image fields and their transformed versions. Next, we normalize each feature vector to the unit of the Euclidean norm and calculate the Euclidean distances between each original patch and all its transformed versions. For each transformation and size, we calculate these distances on average across all patches. Finally, we divide the resulting curves by their maximum values (typically it is the value for the maximum size of the transformation).The normalizations are performed to compensate for possibly different scales of different characteristics. Normalizing feature vectors to unit length ensures that the values for different characteristics are in the same range. The final normalization of the curves by the maximum value allows to balance different variations of different characteristics: as an extreme, a constant characteristic would be considered completely invariant, which is certainly not desirable without normalization."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1106\u20131114,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation, 1(4):541\u2013551,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Simple sparsification improves sparse denoising autoencoders in denoising highly corrupted images", "author": ["K. Cho"], "venue": "ICML. JMLR Workshop and Conference Proceedings,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Descriptor matching with convolutional neural networks: a comparison to SIFT", "author": ["P. Fischer", "A. Dosovitskiy", "T. Brox"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning convolutional feature hierachies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "ICML, pages 1096\u20131103,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "A generative process for contractive auto-encoders", "author": ["S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning of invariant features via simulated fixations in video", "author": ["W.Y. Zou", "A.Y. Ng", "S. Zhu", "K. Yu"], "venue": "NIPS, pages 3212\u20133220,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning invariant representations with local transformations", "author": ["K. Sohn", "H. Lee"], "venue": "ICML,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Direct modeling of complex invariances for visual object features", "author": ["K.Y. Hui"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Tangent prop - a formalism for specifying selected invariances in an adaptive network", "author": ["P. Simard", "B. Victorri", "Y. LeCun", "J.S. Denker"], "venue": "NIPS,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "Improving generalization performance using double backpropagation", "author": ["H. Drucker", "Y. LeCun"], "venue": "IEEE Transactions on Neural Networks, 3(6):991\u2013997,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "Semi supervised logistic regression", "author": ["M.-R. Amini", "P. Gallinari"], "venue": "ECAI, pages 390\u2013394,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Entropy regularization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "Semi-Supervised Learning, pages 151\u2013168.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks", "author": ["A. Ahmed", "K. Yu", "W. Xu", "Y. Gong", "E. Xing"], "venue": "ECCV (3), pages 69\u201382,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, 12:2493\u20132537,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout training as adaptive regularization", "author": ["S. Wager", "S. Wang", "P. Liang"], "venue": "NIPS.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y.N. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "NIPS.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "AISTATS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master\u2019s thesis, Department of Computer Science, University of Toronto,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "CVPR WGMBV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "http://caffe. berkeleyvision.org/,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "NIPS, pages 2528\u20132536,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised Feature Learning for RGB-D Based Object Recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "ISER, June", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Ask the locals: multi-way local pooling for image recognition", "author": ["Y. Boureau", "N. Le Roux", "F. Bach", "J. Ponce", "Y. LeCun"], "venue": "ICCV\u201911. IEEE,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Multipath sparse coding using hierarchical matching pursuit", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "CVPR, pages 660\u2013667,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional neural networks (CNNs) trained via backpropagation were recently shown to perform well on image classification tasks with millions of training images and thousands of categories [1, 2].", "startOffset": 192, "endOffset": 198}, {"referenceID": 1, "context": "Convolutional neural networks (CNNs) trained via backpropagation were recently shown to perform well on image classification tasks with millions of training images and thousands of categories [1, 2].", "startOffset": 192, "endOffset": 198}, {"referenceID": 2, "context": "While CNNs have been known to yield good results on supervised image classification tasks such as MNIST for a long time [3], the feature representation learned by the recent networks achieves stateof-the-art performance not only on the classification task for which the network was trained, but also on various other visual recognition tasks, for example: classification on Caltech-101 [2, 4], Caltech256 [2], Caltech-UCSD birds dataset [4], SUN-397 scene recognition database [4]; detection on PASCAL VOC dataset [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "While CNNs have been known to yield good results on supervised image classification tasks such as MNIST for a long time [3], the feature representation learned by the recent networks achieves stateof-the-art performance not only on the classification task for which the network was trained, but also on various other visual recognition tasks, for example: classification on Caltech-101 [2, 4], Caltech256 [2], Caltech-UCSD birds dataset [4], SUN-397 scene recognition database [4]; detection on PASCAL VOC dataset [5].", "startOffset": 386, "endOffset": 392}, {"referenceID": 3, "context": "While CNNs have been known to yield good results on supervised image classification tasks such as MNIST for a long time [3], the feature representation learned by the recent networks achieves stateof-the-art performance not only on the classification task for which the network was trained, but also on various other visual recognition tasks, for example: classification on Caltech-101 [2, 4], Caltech256 [2], Caltech-UCSD birds dataset [4], SUN-397 scene recognition database [4]; detection on PASCAL VOC dataset [5].", "startOffset": 386, "endOffset": 392}, {"referenceID": 1, "context": "While CNNs have been known to yield good results on supervised image classification tasks such as MNIST for a long time [3], the feature representation learned by the recent networks achieves stateof-the-art performance not only on the classification task for which the network was trained, but also on various other visual recognition tasks, for example: classification on Caltech-101 [2, 4], Caltech256 [2], Caltech-UCSD birds dataset [4], SUN-397 scene recognition database [4]; detection on PASCAL VOC dataset [5].", "startOffset": 405, "endOffset": 408}, {"referenceID": 3, "context": "While CNNs have been known to yield good results on supervised image classification tasks such as MNIST for a long time [3], the feature representation learned by the recent networks achieves stateof-the-art performance not only on the classification task for which the network was trained, but also on various other visual recognition tasks, for example: classification on Caltech-101 [2, 4], Caltech256 [2], Caltech-UCSD birds dataset [4], SUN-397 scene recognition database [4]; detection on PASCAL VOC dataset [5].", "startOffset": 437, "endOffset": 440}, {"referenceID": 3, "context": "While CNNs have been known to yield good results on supervised image classification tasks such as MNIST for a long time [3], the feature representation learned by the recent networks achieves stateof-the-art performance not only on the classification task for which the network was trained, but also on various other visual recognition tasks, for example: classification on Caltech-101 [2, 4], Caltech256 [2], Caltech-UCSD birds dataset [4], SUN-397 scene recognition database [4]; detection on PASCAL VOC dataset [5].", "startOffset": 477, "endOffset": 480}, {"referenceID": 4, "context": "While CNNs have been known to yield good results on supervised image classification tasks such as MNIST for a long time [3], the feature representation learned by the recent networks achieves stateof-the-art performance not only on the classification task for which the network was trained, but also on various other visual recognition tasks, for example: classification on Caltech-101 [2, 4], Caltech256 [2], Caltech-UCSD birds dataset [4], SUN-397 scene recognition database [4]; detection on PASCAL VOC dataset [5].", "startOffset": 514, "endOffset": 517}, {"referenceID": 0, "context": "[1] was only possible due to massive efforts on manually annotating millions of images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For example, unsupervised feature learning is known to be beneficial for image restoration [6] and recent results show that it outperforms supervised feature learning also on descriptor matching [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "For example, unsupervised feature learning is known to be beneficial for image restoration [6] and recent results show that it outperforms supervised feature learning also on descriptor matching [7].", "startOffset": 195, "endOffset": 198}, {"referenceID": 2, "context": "[3], Kavukcuoglu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], Krizhevsky et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], Zeiler and Fergus [2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1], Zeiler and Fergus [2]).", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "Denoising autoencoders [9], for example, learn features that are robust to noise by trying to reconstruct data from randomly perturbed input samples.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "Similarly, contractive autoencoders [10] penalize the sensitivity of the feature representation to small changes in the input.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "[11] learn invariant features from video by enforcing a temporal slowness constraint on the feature representation learned by a linear autoencoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Sohn and Lee [12] and Hui [13] learn features invariant to local image transformations.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "Sohn and Lee [12] and Hui [13] learn features invariant to local image transformations.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "The research most similar to ours is early work on tangent propagation [14] (and the related double backpropagation [15]) which aims to learn invariance to small predefined transformations in a neural network by directly penalizing the derivative of the output with respect to the magnitude of the transformations.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "The research most similar to ours is early work on tangent propagation [14] (and the related double backpropagation [15]) which aims to learn invariance to small predefined transformations in a neural network by directly penalizing the derivative of the output with respect to the magnitude of the transformations.", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "This work is also loosely related to the use of unlabeled data for regularizing supervised algorithms, for example self-training [16] or entropy regularization [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "This work is also loosely related to the use of unlabeled data for regularizing supervised algorithms, for example self-training [16] or entropy regularization [17].", "startOffset": 160, "endOffset": 164}, {"referenceID": 17, "context": "[18], Collobert et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For each initial patch xi \u2208 X we sample K \u2208 [1, 300] random parameter vectors {\u03b1 i , .", "startOffset": 44, "endOffset": 52}, {"referenceID": 19, "context": "[20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] for a recent discussion).", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": ", by enforcing sparsity [8] or robustness to noise [9].", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": ", by enforcing sparsity [8] or robustness to noise [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 21, "context": "To compare our discriminative approach to previous unsupervised feature learning methods, we report classification results on the STL-10 [22], CIFAR-10 [23] and Caltech-101 [24] datasets.", "startOffset": 137, "endOffset": 141}, {"referenceID": 22, "context": "To compare our discriminative approach to previous unsupervised feature learning methods, we report classification results on the STL-10 [22], CIFAR-10 [23] and Caltech-101 [24] datasets.", "startOffset": 152, "endOffset": 156}, {"referenceID": 23, "context": "To compare our discriminative approach to previous unsupervised feature learning methods, we report classification results on the STL-10 [22], CIFAR-10 [23] and Caltech-101 [24] datasets.", "startOffset": 173, "endOffset": 177}, {"referenceID": 24, "context": "Dropout [25] was applied to the fully connected layers.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "We trained the networks using an implementation based on Caffe [26].", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "To each feature map, we applied the pooling method that is commonly used on the respective dataset: 1) 4-quadrant max-pooling, resulting in 4 values per feature map, which is the standard procedure on STL-10 and CIFAR-10 [27, 11, 28, 13]; 2) 3-layer spatial pyramid, i.", "startOffset": 221, "endOffset": 237}, {"referenceID": 10, "context": "To each feature map, we applied the pooling method that is commonly used on the respective dataset: 1) 4-quadrant max-pooling, resulting in 4 values per feature map, which is the standard procedure on STL-10 and CIFAR-10 [27, 11, 28, 13]; 2) 3-layer spatial pyramid, i.", "startOffset": 221, "endOffset": 237}, {"referenceID": 27, "context": "To each feature map, we applied the pooling method that is commonly used on the respective dataset: 1) 4-quadrant max-pooling, resulting in 4 values per feature map, which is the standard procedure on STL-10 and CIFAR-10 [27, 11, 28, 13]; 2) 3-layer spatial pyramid, i.", "startOffset": 221, "endOffset": 237}, {"referenceID": 12, "context": "To each feature map, we applied the pooling method that is commonly used on the respective dataset: 1) 4-quadrant max-pooling, resulting in 4 values per feature map, which is the standard procedure on STL-10 and CIFAR-10 [27, 11, 28, 13]; 2) 3-layer spatial pyramid, i.", "startOffset": 221, "endOffset": 237}, {"referenceID": 28, "context": "max-pooling over the whole image as well as within 4 quadrants and within the cells of a 4 \u00d7 4 grid, resulting in 1 + 4 + 16 = 21 values per feature map, which is the standard on Caltech-101 [29, 11, 30].", "startOffset": 191, "endOffset": 203}, {"referenceID": 10, "context": "max-pooling over the whole image as well as within 4 quadrants and within the cells of a 4 \u00d7 4 grid, resulting in 1 + 4 + 16 = 21 values per feature map, which is the standard on Caltech-101 [29, 11, 30].", "startOffset": 191, "endOffset": 203}, {"referenceID": 29, "context": "max-pooling over the whole image as well as within 4 quadrants and within the cells of a 4 \u00d7 4 grid, resulting in 1 + 4 + 16 = 21 values per feature map, which is the standard on Caltech-101 [29, 11, 30].", "startOffset": 191, "endOffset": 203}, {"referenceID": 26, "context": "Algorithm STL-10 CIFAR-10(400) CIFAR-10 Caltech-101 #features Convolutional K-means Network [27] 60.", "startOffset": 92, "endOffset": 96}, {"referenceID": 28, "context": "0 \u2014 8000 Multi-way local pooling [29] \u2014 \u2014 \u2014 77.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "6 1024\u00d7 64 Slowness on videos [11] 61.", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "6 556 Hierarchical Matching Pursuit (HMP) [28] 64.", "startOffset": 42, "endOffset": 46}, {"referenceID": 29, "context": "5\u00b1 1 \u2014 \u2014 \u2014 1000 Multipath HMP [30] \u2014 \u2014 \u2014 82.", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "5 5000 View-Invariant K-means [13] 63.", "startOffset": 30, "endOffset": 34}, {"referenceID": 26, "context": "This is in agreement with previous evidence that with increasing feature vector dimensionality and number of labeled samples, training an SVM typically becomes less dependent on the quality of the extracted features [27, 13].", "startOffset": 216, "endOffset": 224}, {"referenceID": 12, "context": "This is in agreement with previous evidence that with increasing feature vector dimensionality and number of labeled samples, training an SVM typically becomes less dependent on the quality of the extracted features [27, 13].", "startOffset": 216, "endOffset": 224}, {"referenceID": 10, "context": "First, as an explicit measure of invariance, we calculated the normalized Euclidean distance between normalized feature vectors of the original image patch and the transformed one [11] (see the supplementary material for details).", "startOffset": 180, "endOffset": 184}], "year": 2014, "abstractText": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled \u2019seed\u2019 image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).", "creator": "LaTeX with hyperref package"}}}