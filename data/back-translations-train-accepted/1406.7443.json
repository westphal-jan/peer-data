{"id": "1406.7443", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2014", "title": "Efficient Learning in Large-Scale Combinatorial Semi-Bandits", "abstract": "In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose a novel learning algorithm called Randomized Combinatorial Maximization (RCM). RCM is motivated by Thompson sampling, and is computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that RCM is provably statistically efficient in the coherent Gaussian case, by developing a Bayes regret bound that is independent of the problem scale (number of items) and sublinear in time. We also evaluate RCM on a variety of real-world problems with thousands of items. Our experimental results demonstrate that RCM learns two orders of magnitude faster than the best baseline.", "histories": [["v1", "Sat, 28 Jun 2014 21:50:56 GMT  (51kb,D)", "https://arxiv.org/abs/1406.7443v1", null], ["v2", "Tue, 14 Apr 2015 06:38:56 GMT  (163kb,D)", "http://arxiv.org/abs/1406.7443v2", null], ["v3", "Mon, 8 Jun 2015 06:35:54 GMT  (130kb,D)", "http://arxiv.org/abs/1406.7443v3", null], ["v4", "Tue, 31 Jan 2017 05:32:13 GMT  (152kb,D)", "http://arxiv.org/abs/1406.7443v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["zheng wen", "branislav kveton", "azin ashkan"], "accepted": true, "id": "1406.7443"}, "pdf": {"name": "1406.7443.pdf", "metadata": {"source": "META", "title": "Efficient Learning in Large-Scale Combinatorial Semi-Bandits", "authors": ["Zheng Wen", "Branislav Kveton", "Azin Ashkan"], "emails": ["ZHENGWEN@YAHOO-INC.COM", "KVETON@ADOBE.COM", "AZIN.ASHKAN@TECHNICOLOR.COM"], "sections": [{"heading": "1. Introduction", "text": "In practice, the optimized modular functionality is often unknown and needs to be learned over and over again, while the solution of the problem (MST), the shortest path and the maximum number of learning processes of bipartite matching methods that can be considered in practice as instances of this problem, was recently formulated as a combinatorial contribution adapted to the feedback model (Audibert et al., 2014). Since then, many combinatorial bandit / semi-bandit algorithms have been proposed: for stochastic setting (Gai et al., 2012; Chen et al., 2013; Kveton al., 2015b) for stochastic setting."}, {"heading": "2. Combinatorial Optimization", "text": "Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-Items-ms-Items-Items-ms-Items-Items-Items-ms-Items-Items-ms-Items-Items-Items-Items-Items-Items-Items-ms-Items-Items-Items-Items-ms"}, {"heading": "3. Combinatorial Semi-Bandits with Linear Generalization", "text": "Many real-world problems are combinatorial. In recommendation systems, for example, the user is typically recommended K items from L. The value of an item, such as the expected rating of a movie, is never fully known and needs to be refined while it is repeatedly recommended to the pool of users. Recommendation problems are known to be highly structured. In particular, it is known that the user item matrix is typically low (Koren et al., 2009) and that the value of an item can be written as a linear combination of its position in latent space. In this thesis, we propose a learning algorithm for combinatorial optimization that uses this structure. In particular, we assume that the weight of each item is a linear function of its properties, and then we learn the parameters of this model, common to all items."}, {"heading": "3.1. Combinatorial Semi-Bandits", "text": "We formalize our learning problem as a combinatorial semi-bandit. A combinatorial semi-bandit is a triple (E, A, P), where E and A are defined in section 2 and P is a probability distribution over the weights w, RL of the items in the ground set E. We assume that the weights w are drawn by P. The mean weight is denoted in section 2 and P is a probability distribution over the weights w, RL of the items in the ground set E. We assume that a subset of items A, RE can be drawn if and only if A, RA is drawn. The return of the arms A is f (A, w) (Equation (1)), the sum of the weights of all items in A. After the arms A are drawn, we observe the individual return of each arm, w (e): e, RL of the items in sequence A and RL in sequence A. This feedback model is known as a semi-bandit."}, {"heading": "3.2. Linear Generalization", "text": "As we discussed in Section 1, many demonstrably efficient algorithms have been developed for various combinatorial semi-bandits of the form (3). (Chen et al., 2013; Gai et al., 2012; Kveton et al., 2014a; Russo & Van Roy, 2014) However, since learning parameters exist and these algorithms do not take into account the generalization of items, the derived upper limit for the expected cumulative regret and / or cumulative regret of these algorithms will be at least O (\u221a L). In addition, Audibert et al. (2014) have derived a generalization of items having lower limits for adversarial combinatorial semi-bandits, while Kveton et al. (2014a) has derived an asymptotic elementary analysis (L log (n) / \u0432) showing gap-dependent lower limits for combinatorial semi-bandits."}, {"heading": "3.3. Performance Metrics", "text": "Let us remember that the learning algorithm in episode t defines the expected cumulative regret of the learning algorithm in n episodes asR (n) = 1E [Rt | w], (4) if the expectation is above random weights and possible randomization in the learning algorithm. If necessary, we refer to R (n) as R (n; w) to emphasize the dependence on w (n). On the other hand, if w (n) is randomly generated or the agent has a previous belief in w (n), then by Russo & Van Roy (2013), the cumulative regret of the learning algorithm is highlighted in n episodes n (n; w)."}, {"heading": "4. Learning Algorithms", "text": "In this section we propose two learning algorithms for combinatorial semi-bandits: Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear Algorithm 2 Combinatorial Linear Thompson Sampling Input: Combinatorial Structure (E, A), Generalization Matrix (RL), Algorithm Parameters (N), \u03c3 > 0, Oracle ORACLEInitialize (E, A) Choose Set At (E) and Combinatorial Structure (E, A)..., n doSample (N) Compute At (E, A) Choose Set At (E, E), and CombLinUCT (E), E (E), E (E), E (At Compute), T + 1 and T + 1 based on algorithm 1end CombUCB (CombLinUCB), each motivated by Thomsons Sampling and LinUCB."}, {"heading": "4.1. Combinatorial Linear Thompson Sampling", "text": "The psuedocode of CombLinTS is contained in algorithm 2, where (E, A) is the combinatorial structure, (E) the generalization matrix, (ORACLE) is a combinatorial optimization algorithm, and (E, A) are two algorithm parameters that control the learning rate. Specifically, this is an invert regularization parameter, and (smaller) a covariance matrix that moves closer to 0. Therefore, a \u03bb that is too small leads to insufficient exploration and significantly reduces the performance of CombLinTS. On the other hand, it controls the reduction rate of the covariance matrix. \u2212 In particular, one that is too slow to learn, while a \u03c3 that is too small quickly leads the algorithm to a suboptimal coefficient vector. In each episode t, algorithm consists of t, algorithm 2 of three steps. First, it randomly samples a coefficient vector from a Gaussian distribution."}, {"heading": "4.2. Combinatorial Linear UCB", "text": "The pseudo code of CombLinUCB is given in algorithm 3, where E, A, \u03a6 and ORACLE are defined as in algorithm 2, and \u03bb, \u03c3 and c are three algorithm parameters. Similarly, \u03bb is an inverse regulation parameter, \u03c3 controls the reduction rate of the covariance matrix and c controls the degree of optimism (exploration). If c is too small, the algorithm could converge to a suboptimal coefficient vector due to insufficient exploration; on the other hand, a too large c leads to excessive exploration and slow learning.In each episode t, algorithm 3 also consists of three steps. Firstly, it calculates an upper confidence limit (UCB) w-t (e) for each e-E. Secondly, it calculates At on the basis of w-t and the predefined oracle. Finally, it updates the solution of T + 1 and T + 1 based on the algorithm (1)."}, {"heading": "5. Regret Bounds", "text": "In this section, we present a regret about CombLinTS and a regret about CombLinUCB. We will also briefly discuss how these limits are derived, as well as their density, leaving the detailed evidence to the appendices. Without loss of generality, during this section, we assume that these limits are independent of each other. (1) We assume that Bayes Regret on CombLinTSWe have the following upper limit applied to RBayes (n) if CombLinTS is applied to a related Gaussian case with the correct parameters. (1) We assume that (1) the upper limit applied to CombLinTSWe have the following upper limit applied to RBayes (n). (3) The noises are generally sampled by N (0, 2), and (4) the upper limit applied to Bayes."}, {"heading": "6. Experiments", "text": "In this section, we evaluate CombLinTS based on three problems. The first problem is synthetic, but the last two problems are constructed based on real-world data sets. As we discussed in Section 1, we evaluate CombLinTS only because Thompson's scanning algorithms usually outperform UCB-like algorithms in practice. On the other hand, our experimental results in the synthetic problem show that CombLinTS is both scalable and robust for selecting algorithm parameters, and they also suggest that the Bayes limits derived in Theorem 1 are likely to be narrow. On the other hand, our experimental results in the last two problems show the value of linear generalization in the real world: In domain-specific but imperfect linear generalization (i.e., agnostic learning), CombLinTS can significantly exceed the status-of-the-art learning algorithms that do not generalize linearly, but in all three problems cumulatively exceed the expected OREL (i.e., the OREL problem)."}, {"heading": "6.1. Longest Path", "text": "We first evaluate CombLinTS on a synthetic problem. Specifically, we experiment with a stochastic longest path problem on a (m + 1) \u00b7 (m + 1) square grid 4. The points in floor group E are the edges in the grid, L = 2 (m + 1) overall regrettable. Practical sample A are all paths in the grid from the upper left corner to the lower right corner following the instructions of the edges. The length of these paths is K = 2 meters. In this problem, we focus on coherent Gaussian cases and random samples of the linear generalization of the matrix, which weakens the dependence on a particular choice of the bayvares. Our experiments are parameterized by a sextuple (m, d, true, true, what we do), defining m, d, and evil, and evil."}, {"heading": "6.2. Online Advertising", "text": "In the second experiment, we evaluate CombLinTS based on an advertising problem. Our goal is to identify 100 people who are most likely to accept an advertising offer, subject to the targeting constraint that exactly half of them are female. Specifically, the Ground Set E 33k includes representative persons from the Adult Dataset (Asuncion & Newman, 2007), which was collected in the 1994 U.S. Census. A workable solution A is any subset of E with | A | = 100 and the satisfaction of the above targeting constraint. We assume that the person e accepts an advertising offer with a probability. (E) = 0.15 Income is at least 50k 0.05 otherwise, and people accept offers independently of each other. The characteristics in the generalization matrix are the age involved in 7 groups; gender; whether the person works more than 40 hours a week; and the length of training in years. All of these characteristics can be compared based on the Linet Dataset problems with three."}, {"heading": "6.3. Artist Recommendation", "text": "In the last experiment, we evaluated CombLinTS based on a problem with the recommendation of K = 10 musicians, who are most likely to be selected by the average user of a music recommendation website. Specifically, the Ground Set E includes artists from the Last.fm music recommendation dataset (Cantador et al., 2011). The dataset contains tagging and music artist listening information from a group of 2k users of the Last.fm online music system6. The tagging part includes the tag assignments of all artists provided by users. For each user, the artists she has listened to and the number of listening events are also available in the dataset. 6http: / / www.lastfm.com We select E as a group of artists who have been listened to by at least two users and had at least one tag assignment among the 20 most popular, and | E | E | E episodes of listening events are also available in the dataset."}, {"heading": "7. Conclusion", "text": "We have proposed two learning algorithms, CombLinTS and CombLinUCB, for stochastic combinatorial semibandites with linear generalization. The most important contribution of this work is twofold. First, we have set linear limits of regret for these two algorithms under reasonable assumptions, with L being the number of items. Second, we have also evaluated CombLinTS on a variety of issues. Results of the experiment in the first problem show that CombLinTS is scalable and robust, and the experiment demonstrates the value of using linear generalization in real-world environments in the other two problems. It is worth noting that our results can easily be tended to the contextual combinatorial semibandites with linear generalization."}, {"heading": "A. Proof for Theorem 1", "text": "To prove theorem 1, we must first prove the following theorem: Theorem 3: If (1) w = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p p = p p = p p = p p = p p = p p = p = p = p = p = p = p = p p = p = p = p = p = p = p = h = h = p = h = p = p = p h = p = p = p = p = h = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "B. Proof for Theorem 2", "text": "We start by defining an alternative formula for equality and equality of the sexes, we start by defining an alternative formula for equality and equality of the sexes."}, {"heading": "C. Technical Lemma", "text": "In this section, we derive the equation (37). We first prove the following technical problem: Lemma 8. For every single measure, every positive measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure that is taken. We now prove equation (43) that we take it, every measure, every measure that is taken, every measure, every measure that is taken, every measure, every measure that is taken, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure, every measure that follows directly from the identity of the Woodbury matrix (matrix reversal dilemma)."}, {"heading": "D. A Variant of Theorem 2 for Approximation Algorithms", "text": "By appropriately redefining the realized regret, we can prove a variant of theorem 2 in which we can be an approximation algorithm. (1) Specifically, we define the (scaled) difference of ORACLE iff (A), w), max A (A), max A (A), max A (A), w), we define the (scaled) difference of ORACLE iff (A), w), w), max A (A), f (A), w (A), w), w), w (46) Then we define the (scaled) realized difference of ORACLE iff (A), wt), w), w) max A (A (A), w) max A (A), w), w), w), w (46)."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Abbasi-Yadkori", "Yasin", "P\u00e1l", "D\u00e1vid", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["Agrawal", "Shipra", "Goyal", "Navin"], "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Agrawal", "Shipra", "Goyal", "Navin"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Agrawal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Regret in online combinatorial optimization", "author": ["Audibert", "Jean-Yves", "Bubeck", "Sebastien", "Lugosi", "Gabor"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2014}, {"title": "Using confidence bounds for exploitationexploration trade-offs", "author": ["Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Auer and Peter.,? \\Q2002\\E", "shortCiteRegEx": "Auer and Peter.", "year": 2002}, {"title": "Second workshop on information heterogeneity and fusion in recommender systems (hetrec", "author": ["Cantador", "Iv\u00e1n", "Brusilovsky", "Peter", "Kuflik", "Tsvi"], "venue": "In Proceedings That is,", "citeRegEx": "Cantador et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cantador et al\\.", "year": 2011}, {"title": "Combinatorial bandits", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Lugosi", "G\u00e1bor"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "An empirical evaluation of Thompson sampling", "author": ["Chapelle", "Olivier", "Li", "Lihong"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Combinatorial multi-armed bandit: General framework and applications", "author": ["Chen", "Wei", "Wang", "Yajun", "Yuan", "Yang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Dani", "Varsha", "Hayes", "Thomas", "Kakade", "Sham"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory, pp", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Adaptive submodular maximization in bandit setting", "author": ["Gabillon", "Victor", "Kveton", "Branislav", "Wen", "Zheng", "Eriksson", "Brian", "S. Muthukrishnan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gabillon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2013}, {"title": "Large-scale optimistic adaptive submodularity", "author": ["Gabillon", "Victor", "Kveton", "Branislav", "Wen", "Zheng", "Eriksson", "Brian", "S. Muthukrishnan"], "venue": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Gabillon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2014}, {"title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations", "author": ["Gai", "Yi", "Krishnamachari", "Bhaskar", "Jain", "Rahul"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "Gai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gai et al\\.", "year": 2012}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": "IEEE Computer,", "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Matroid bandits: Fast combinatorial optimization with learning", "author": ["Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Eydgahi", "Hoda", "Eriksson", "Brian"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Kveton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2014}, {"title": "Learning to act greedily: Polymatroid semi-bandits", "author": ["Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Eydgahi", "Hoda", "Valko", "Michal"], "venue": "CoRR, abs/1405.7752,", "citeRegEx": "Kveton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2014}, {"title": "Combinatorial Optimization", "author": ["Papadimitriou", "Christos", "Steiglitz", "Kenneth"], "venue": "Dover Publications, Mineola, NY,", "citeRegEx": "Papadimitriou et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 1998}, {"title": "Learning to optimize via posterior sampling", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "CoRR, abs/1301.2609,", "citeRegEx": "Russo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2013}, {"title": "An informationtheoretic analysis of thompson", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "sampling. CoRR,", "citeRegEx": "Russo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2014}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Efficient exploration and value function generalization in deterministic systems", "author": ["Wen", "Zheng", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2013}, {"title": "Sequential Bayesian search", "author": ["Wen", "Zheng", "Kveton", "Branislav", "Eriksson", "Brian", "Bhamidipati", "Sandilya"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Wen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2013}, {"title": "Linear submodular bandits and their application to diversified retrieval", "author": ["Yue", "Yisong", "Guestrin", "Carlos"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "This class of learning problems was recently formulated as a combinatorial bandit/semi-bandit, depending on the feedback model (Audibert et al., 2014).", "startOffset": 127, "endOffset": 150}, {"referenceID": 12, "context": "Since then, many combinatorial bandit/semi-bandit algorithms have been proposed: for the stochastic setting (Gai et al., 2012; Chen et al., 2013; Russo & Van Roy, 2014; Kveton et al., 2015b); for the adversarial setting (Cesa-Bianchi & Lugosi, 2012; Audibert et al.", "startOffset": 108, "endOffset": 190}, {"referenceID": 8, "context": "Since then, many combinatorial bandit/semi-bandit algorithms have been proposed: for the stochastic setting (Gai et al., 2012; Chen et al., 2013; Russo & Van Roy, 2014; Kveton et al., 2015b); for the adversarial setting (Cesa-Bianchi & Lugosi, 2012; Audibert et al.", "startOffset": 108, "endOffset": 190}, {"referenceID": 3, "context": ", 2015b); for the adversarial setting (Cesa-Bianchi & Lugosi, 2012; Audibert et al., 2014; Neu & Bart\u00f3k, 2013); and for subclasses of combinatorial problems, matroid and polymatroid bandits (Kveton et al.", "startOffset": 38, "endOffset": 110}, {"referenceID": 20, "context": ", 2014a;b), submodular maximization (Wen et al., 2013; Gabillon et al., 2013), and cascading bandits (Kveton et al.", "startOffset": 36, "endOffset": 77}, {"referenceID": 10, "context": ", 2014a;b), submodular maximization (Wen et al., 2013; Gabillon et al., 2013), and cascading bandits (Kveton et al.", "startOffset": 36, "endOffset": 77}, {"referenceID": 19, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al.", "startOffset": 89, "endOffset": 151}, {"referenceID": 16, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al.", "startOffset": 71, "endOffset": 192}, {"referenceID": 9, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al. (2008); AbbasiYadkori et al.", "startOffset": 193, "endOffset": 212}, {"referenceID": 9, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al. (2008); AbbasiYadkori et al. (2011)) , to combinatorial semi-bandits with linear generalization.", "startOffset": 193, "endOffset": 241}, {"referenceID": 9, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al. (2008); AbbasiYadkori et al. (2011)) , to combinatorial semi-bandits with linear generalization. In this paper, we propose two learning algorithms, Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB), based on Thompson sampling and LinUCB. Both CombLinTS and CombLinUCB are computationally efficient, as long as the offline version of the combinatorial problem can be solved efficiently. The first major contribution of the paper is that we establish a Bayes regret bound on CombLinTS and a regret bound on CombLinUCB, under reasonable assumptions. Both bounds are L-independent, and sublinear in time. The second major contribution of the paper is that we evaluate CombLinTS on a variety of problems with thousands of items, and two of these problems are based on real-world datasets. We only evaluate CombLinTS since recent literature (Chapelle & Li, 2011) suggests that Thompson sampling algorithms usually outperform UCB-like algorithms in practice. Our experimental results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines. It is worth mentioning that our derived L-independent regret bounds also hold in cases with L = \u221e. Moreover, as we will discuss in Section 7, our proposed algorithms and their analyses can be easily extended to the contextual combinatorial semibandits. Finally, we briefly review some relevant papers. Gabillon et al. (2014) and Yue & Guestrin (2011) focus on submodular maximization with linear generalization.", "startOffset": 193, "endOffset": 1693}, {"referenceID": 9, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al. (2008); AbbasiYadkori et al. (2011)) , to combinatorial semi-bandits with linear generalization. In this paper, we propose two learning algorithms, Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB), based on Thompson sampling and LinUCB. Both CombLinTS and CombLinUCB are computationally efficient, as long as the offline version of the combinatorial problem can be solved efficiently. The first major contribution of the paper is that we establish a Bayes regret bound on CombLinTS and a regret bound on CombLinUCB, under reasonable assumptions. Both bounds are L-independent, and sublinear in time. The second major contribution of the paper is that we evaluate CombLinTS on a variety of problems with thousands of items, and two of these problems are based on real-world datasets. We only evaluate CombLinTS since recent literature (Chapelle & Li, 2011) suggests that Thompson sampling algorithms usually outperform UCB-like algorithms in practice. Our experimental results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines. It is worth mentioning that our derived L-independent regret bounds also hold in cases with L = \u221e. Moreover, as we will discuss in Section 7, our proposed algorithms and their analyses can be easily extended to the contextual combinatorial semibandits. Finally, we briefly review some relevant papers. Gabillon et al. (2014) and Yue & Guestrin (2011) focus on submodular maximization with linear generalization.", "startOffset": 193, "endOffset": 1719}, {"referenceID": 8, "context": "Similarly to Chen et al. (2013), in this paper, we allow the agent to use any approximation / randomized algorithm ORACLE to solve (2), and denote its solution as A\u2217 = ORACLE(E,A,w).", "startOffset": 13, "endOffset": 32}, {"referenceID": 13, "context": "In particular, it is well known that the user-item matrix is typically low-rank (Koren et al., 2009) and that the value of an item can be written as a linear combination of its position in the latent space.", "startOffset": 80, "endOffset": 100}, {"referenceID": 3, "context": "This feedback model is known as semi-bandit (Audibert et al., 2014).", "startOffset": 44, "endOffset": 67}, {"referenceID": 8, "context": "Linear Generalization As we have discussed in Section 1, many provably efficient algorithms have been developed for various combinatorial semi-bandits of form (3) (Chen et al., 2013; Gai et al., 2012; Kveton et al., 2014a; Russo & Van Roy, 2014).", "startOffset": 163, "endOffset": 245}, {"referenceID": 12, "context": "Linear Generalization As we have discussed in Section 1, many provably efficient algorithms have been developed for various combinatorial semi-bandits of form (3) (Chen et al., 2013; Gai et al., 2012; Kveton et al., 2014a; Russo & Van Roy, 2014).", "startOffset": 163, "endOffset": 245}, {"referenceID": 9, "context": "Like existing literature on linear bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011), the analysis in this paper focuses on coherent learning cases.", "startOffset": 43, "endOffset": 91}, {"referenceID": 0, "context": "Like existing literature on linear bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011), the analysis in this paper focuses on coherent learning cases.", "startOffset": 43, "endOffset": 91}, {"referenceID": 2, "context": "Furthermore, Audibert et al. (2014) has derived an \u03a9( \u221a LKn) lower bound on adversarial combinatorial semi-bandits, while Kveton et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 2, "context": "Furthermore, Audibert et al. (2014) has derived an \u03a9( \u221a LKn) lower bound on adversarial combinatorial semi-bandits, while Kveton et al. (2014a) has derived an asymptotic \u03a9(L log(n)/\u2206) gap-dependent lower bound on stochastic combinatorial semi-bandits, where \u2206 is the \u201cgap\u201d.", "startOffset": 13, "endOffset": 144}, {"referenceID": 2, "context": "Furthermore, Audibert et al. (2014) has derived an \u03a9( \u221a LKn) lower bound on adversarial combinatorial semi-bandits, while Kveton et al. (2014a) has derived an asymptotic \u03a9(L log(n)/\u2206) gap-dependent lower bound on stochastic combinatorial semi-bandits, where \u2206 is the \u201cgap\u201d. However, in many modern combinatorial semi-bandit problems, L tends to be enormous. Thus, an O( \u221a L) regret is unacceptably large in these problems. On the other hand, in many practical problems, there exists a generalization model based on which the weight of one item can be (approximately) inferred based on the weights of other items. By exploiting such generalization models, an o( \u221a L) or even an L-independent cumulative regret might be achieved. In this paper, we assume that there is a (possibly imperfect) linear generalization model across the items. Specifically, we assume that the agent knows a generalization matrix \u03a6 \u2208 RL\u00d7d s.t. w\u0304 either lies in or is \u201cclose\u201d to the subspace span [\u03a6]. We use \u03c6e to denote the transpose of the e-th row of \u03a6, and refer to it as the feature vector of item e. Without loss of generality, we assume that rank [\u03a6] = d. Similar to Wen & Van Roy (2013), we distinguish between the coherent learning cases, in which w\u0304 \u2208 span [\u03a6], and the agnostic learning cases, in which w\u0304 / \u2208 span [\u03a6].", "startOffset": 13, "endOffset": 1171}, {"referenceID": 9, "context": "We now outline the proof of Theorem 1, which is motivated by Russo & Van Roy (2013) and Dani et al. (2008). LetHt denote the \u201chistory\u201d (i.", "startOffset": 88, "endOffset": 107}, {"referenceID": 9, "context": "It is well-known that the O( \u221a d) factor is due to linear generalization (Dani et al., 2008; Abbasi-Yadkori et al., 2011), and as is discussed in the appendix (see Remark 1), the extra O( \u221a K) factor is Audibert et al.", "startOffset": 73, "endOffset": 121}, {"referenceID": 0, "context": "It is well-known that the O( \u221a d) factor is due to linear generalization (Dani et al., 2008; Abbasi-Yadkori et al., 2011), and as is discussed in the appendix (see Remark 1), the extra O( \u221a K) factor is Audibert et al.", "startOffset": 73, "endOffset": 121}, {"referenceID": 2, "context": "no generalization), Russo & Van Roy (2014) provides an O( \u221a LK log(L/K)n) upper bound on RBayes(n) when Thompson sampling is applied, and Audibert et al. (2014) provides an \u03a9( \u221a LKn) lower bound2.", "startOffset": 138, "endOffset": 161}, {"referenceID": 0, "context": ", 2008; Abbasi-Yadkori et al., 2011), and as is discussed in the appendix (see Remark 1), the extra O( \u221a K) factor is Audibert et al. (2014) focuses on the adversarial setting but the lower bound is stochastic.", "startOffset": 8, "endOffset": 141}, {"referenceID": 0, "context": "We first construct a confidence set G of \u03b8\u2217 based on the \u201cself normalized bound\u201d developed in Abbasi-Yadkori et al. (2011). Then we decompose the regret over the high-probability \u201cgood\u201d event G and the low-probability \u201cbad\u201d event \u1e20, where \u1e20 is the complement of G.", "startOffset": 94, "endOffset": 123}, {"referenceID": 0, "context": "We first construct a confidence set G of \u03b8\u2217 based on the \u201cself normalized bound\u201d developed in Abbasi-Yadkori et al. (2011). Then we decompose the regret over the high-probability \u201cgood\u201d event G and the low-probability \u201cbad\u201d event \u1e20, where \u1e20 is the complement of G. Finally, we bound the term associated with the event G based on the same worst-case bound on \u2211n t=1 \u2211 e\u2208At \u221a \u03c6e \u03a3t\u03c6e used in the analysis for CombLinTS (see Lemma 4 in Appendix A), and bound the term associated with the event \u1e20 based on a naive bound. Please refer to Appendix B for the detailed proof of Theorem 2. Notice that if we choose \u03bb = \u03c3 = 1, \u03b4 = 1/(nK), and c as the lower bound specified in Inequality (9), then the regret bound derived in Theorem 2 is also \u00d5(Kd \u221a n). Compared with the lower bound derived in Audibert et al. (2014), this bound is at most \u00d5( \u221a Kd) larger.", "startOffset": 94, "endOffset": 809}, {"referenceID": 5, "context": "fm music recommendation dataset (Cantador et al., 2011).", "startOffset": 32, "endOffset": 55}, {"referenceID": 9, "context": "We now outline the proof of Theorem 3, which is based on (Russo & Van Roy, 2013; Dani et al., 2008).", "startOffset": 57, "endOffset": 99}, {"referenceID": 9, "context": "Our analysis is motivated by the analysis in (Dani et al., 2008).", "startOffset": 45, "endOffset": 64}, {"referenceID": 0, "context": "We first construct a confidence set of \u03b8\u2217 based on the \u201cself normalized bound\u201d developed in (Abbasi-Yadkori et al., 2011).", "startOffset": 92, "endOffset": 121}, {"referenceID": 0, "context": "As we will see later, we define Vt and \u03bet to use the \u201cself normalized bound\u201d developed in (Abbasi-Yadkori et al., 2011) (see Theorem 1 of (Abbasi-Yadkori et al.", "startOffset": 90, "endOffset": 119}, {"referenceID": 0, "context": ", 2011) (see Theorem 1 of (Abbasi-Yadkori et al., 2011)).", "startOffset": 26, "endOffset": 55}, {"referenceID": 0, "context": "We now provide a high probability bound on \u2016\u03bet\u2016V \u22121 t , based on the \u201cself normalized bound\u201d proposed in (Abbasi-Yadkori et al., 2011).", "startOffset": 105, "endOffset": 134}, {"referenceID": 0, "context": "From Theorem 1 of (Abbasi-Yadkori et al., 2011), we know for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4,", "startOffset": 18, "endOffset": 47}], "year": 2017, "abstractText": "A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines.", "creator": "LaTeX with hyperref package"}}}