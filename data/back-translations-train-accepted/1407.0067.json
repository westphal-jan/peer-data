{"id": "1407.0067", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2014", "title": "Rates of Convergence for Nearest Neighbor Classification", "abstract": "Nearest neighbor methods are a popular class of nonparametric estimators with several desirable properties, such as adaptivity to different distance scales in different regions of space. Prior work on convergence rates for nearest neighbor classification has not fully reflected these subtle properties. We analyze the behavior of these estimators in metric spaces and provide finite-sample, distribution-dependent rates of convergence under minimal assumptions. As a by-product, we are able to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known. We illustrate our upper and lower bounds by introducing smoothness classes that are customized for nearest neighbor classification.", "histories": [["v1", "Mon, 30 Jun 2014 22:00:57 GMT  (36kb,D)", "https://arxiv.org/abs/1407.0067v1", null], ["v2", "Wed, 2 Jul 2014 00:44:29 GMT  (37kb,D)", "http://arxiv.org/abs/1407.0067v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.ST stat.ML stat.TH", "authors": ["kamalika chaudhuri", "sanjoy dasgupta"], "accepted": true, "id": "1407.0067"}, "pdf": {"name": "1407.0067.pdf", "metadata": {"source": "CRF", "title": "Rates of Convergence for Nearest Neighbor Classification", "authors": ["Kamalika Chaudhuri", "Sanjoy Dasgupta"], "emails": ["kamalika@cs.ucsd.edu", "dasgupta@cs.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper we deal with binary predictions in metric spaces. A classification problem is defined by a metric space (X, \u03c1) from which instances are drawn, a space of possible labels Y = {0, 1}, and a distribution P overX \u00d7 Y. The goal is to find a function that minimizes the probability of errors in pairs (X, Y) drawn from P; this error rate is the risk R (h) = P (h) 6 = Y). The best such function is easy to specify: if we determine the marginal distribution of X and the conditional probability of P (x) = P (Y = x)."}, {"heading": "1.1 Previous work on rates of convergence", "text": "The earliest convergence rates for the closest neighbors were non-distributed. Cover [3] examined the 1-NN classifier in case X = R, assuming class-related densities with uniformly delimited third derivatives. He showed that ERn converges at a convergence rate of O (1 / n2). Wagner [18] and later Fritz [8] also investigated 1-NN, but in higher dimension X = Rd. The latter achieved an asymptotic convergence rate for Rn under the milder assumption of non-atomic \u00b5 and lower semi-continuous class-related densities. The non-distributional results are of some value, but cannot accurately characterize which properties of a distribution most influence the performance of the next neighbor classification. Recent work has examined various approaches to achieve distributional boundaries."}, {"heading": "1.2 Some illustrative examples", "text": "Let us now look at a few examples to get a sense of which properties of a distribution most critically affect the convergence rate of the nearest neighbor. In any case, let us examine the k-NN classification. Let us first consider a finite instance space X. To obtain a sufficiently high convergence rate, let us proceed more generally and observe that for every number of points n, the nearest neighbors of x will be x itself, which immediately results in a margin of error. However, this kind of reasoning results in an asymptotic convergence rate. To obtain a finite sample rate, let us go further and observe that for every number of points n, the nearest neighbors of x will be within a sphere B = B (x, r), whose probability limit below \u00b5 is approximately k / n. The quality of the prediction can be judged by how many sample variants vary within that sphere. To be a little more precise, let us make sure that subjects have a distance of x (B) = (1 / \u00b5) and a distance of B (x)."}, {"heading": "1.3 Results of this paper", "text": "Returning to our earlier setting of pairs (X, Y), where X records values in a metric space (X, \u03c1) and has a distribution \u00b5, while Y (0, 1) has a conditional probability function \u03b7 (x) = Pr (Y = 1 | X = x). We obtain convergence rates for k-NN by trying to specify the intuitions discussed above, which results in a slightly different analysis style than was used in previous work. For all positive integers k \u2264 n, we define an idea of an effective limit for k-NN by sample size n. At the moment, we denote this by An, k, X. \u2022 We show that with a high probability of using the training data, the misclassification rate of the k-NN classifier (in relation to the Bayes optimal classification) is limited above this limit of \u00b5 (An, k) plus a small additional limit that can be made arbitrarily small. \u2022 We identify a general condition that we have a lower probability that we grow to the actual limit and that we have a greater one of the n (x)."}, {"heading": "2 Definitions and results", "text": "The designation of an instance X = x is Y = (((X1, Y1),..., (Xn, Yn) and a query point x \u00b2 X we use the notation X (i) (x) to denote the i-th nearest neighbor of x in the training set, and Y (i) (x) to denote its label. Distances are calculated in relation to the given metric point, and bonds are broken by favoring points earlier in the sequence. K-NN \u00b7 classical \u00b7 Y \u00b7 n (classical) is calculated with the probability (1 x) (classical (1 x), with the probability (1 x) with which we (n) calculate."}, {"heading": "2.1 Definitions", "text": "We begin with some x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "2.2 A general bound on the misclassification error", "text": "We start with a general upper limit for the misclassification rate of the k-NN classifier. We will specialize it later on various interesting situations. All the evidence will appear in the appendix. Theorem 1. Select any 0 < \u03b4 < 1 and positive integers k < n. Let gn, k classify the k-NN classifier based on n training points and g (x) the Bayes optimal classifier. Probably at least 1 \u2212 \u03b4 is higher than the selection of training data PrX (gn, k (X) 6 = g (X)) \u2264 \u03b4 + \u00b5 (Yankee p, \u2206), with ep = k n \u00b7 1 \u2212 ympe (4 / k) ln (2 / \u043c) and \u0445 = min (12, \u221a 1k ln2\u043c).Convergence results for the nearest neighbor have traditionally yielded the excess risks Rn, k \u2212 Rn, k = Pr (6 = gn) thus."}, {"heading": "2.3 Universal consistency", "text": "A number of results, starting with [15], have shown that kn-NN is highly consistent (Rn = Rn, kn \u2192 R \u043d almost certainly) when X is an end-dimensional Euclidean space and \u00b5 is a Borel measure. A consequence of Theorem 1 is that this phenomenon is of a much more general nature. Indeed, strong consistency holds true in any metric measuring space (X, \u03c1, \u00b5) for which the Lebesgue differentiation law applies: i.e. in voids where for any limited measurable f, lim r \u2193 01\u00b5 (B (x, r)); in any metric space B (x, r) f d\u00b5 = f (x) (3) for almost all (\u00b5-a.e.) x-X. For more details on this differentiation property see [6, 2,9,8] and [10, 1,13]."}, {"heading": "2.4 A lower bound", "text": "Next, we give a counterpart to Theorem 1 that falls short of the expected error probability of gn, k. For all positive integers k < n, we define the highly error-prone quantity En, k = E + n, k, where E + n, k = {x-supp (\u00b5) | \u03b7 (x) > 12, \u03b7 (B (x, r)) \u2264 1 2 + 1 \u221a kfor all rk / n (x) \u2264 r \u2264 r (k + \u221a k + 1) / n (x)} E \u2212 n, k = {x-supp (\u00b5) | \u03b7 (x) < 12, \u03b7 (B (x, r)) \u2265 1 \u2212 1 \u221a k for all rk / n (x) \u2264 r (k + \u221a k + 1) / n (x)}. (Recall the definition (1) of \u03b7 (A) for sets A).) We will see that this region is comparable to the effective limit X / k that applies to any schooling."}, {"heading": "2.5 Smooth measures", "text": "For the purposes of the nearest neighbor, it makes sense to define a term of smoothness in relation to the boundary distribution to instances. We use a variant of the holder continuity: for \u03b1, L > 0, we say that the conditional probability function \u03b7 (\u03b1, L) -smooth in metric dimensional space (X, \u03c1, \u00b5), if for all x, x \u00b2 -smooth condition X, x \u00b2 -smooth condition. This is expressed so as to resemble the standard smoothing conditions, but what we really need is the weaker assertion that for all x-supp (\u00b5) and all r > 0, | p \u00b2 (B (x, r) \u2212 p \u00b2 -smooth condition (x, x \u2032) -smooth condition (Bo) -smooth condition (x, r)."}, {"heading": "2.6 Margin bounds", "text": "One achievement of statistical theory over the last two decades has been margin limits, which result in fast convergence rates for many classifiers (\u03b2 = \u03b2 = \u03b2) when the underlying data distribution P (given by \u00b5 and \u03b7) meets a margin condition, which roughly stipulates that each step moves gracefully from 1 / 2 close to the margin limit. Following [13, 16, 1], for each \u03b2 \u2265 0, we say that P meets the margin condition if there is a constant C > 0 of such margin excess (x). (x) \u2212 12 percent of the margin condition. (\u03b1) \u2264 1 percent implies a larger margin difference. We now get limits on the margin condition and excess risk of k-NN under smoothness and margin conditions. Theorem 7."}, {"heading": "Appendix: Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.7 A tie-breaking mechanism", "text": "In some situations, such as in individual instance rooms, the probability is higher than zero that two or more of the training points are equidistant from the query point. In practice, we break ties by a simple rule, preferring points that occur earlier in the sequence. To accurately reflect this in the analysis, we use the following mechanism: For each training point X, we independently and uniformly extract a value Z [0, 1]. When breaking equilibrium, points with a lower Z value are preferred. We use the notation X \u2032 = (X, Z). X \u00b7 [0, 1] to refer to the advanced training points that are extracted from the product measure."}, {"heading": "2.8 Proof of Theorem 1", "text": "Theorem 1 is based on the following basic observation. Lemma 8. Let us assume gn, k that the k-NN classifier is based on training data (X \"1, Y1),.., (X\" n, Yn). Select all xo \"X\" and all 0 \"p\" (1, 0 \"\u2264 1 / 2. Let B\" (xo \"p\"), (xo \"), Z (k\" n \"), (k\" 1 \"), 6\" g \"), (xo\"), (xo \"), (xo\"), (xo \"p\"), (xo \"), (xo\"), (xo \"), (xo\"), (xo \"), (xo\"), (xo \"), (xo\"), (xo \"), (xo\"), (xo \"), (xo\")."}, {"heading": "2.9 Proof of Theorem 2", "text": "Remember that we have Rn = PrX (gn, kn (X) 6 = > Result (2): Rn \u2212 R \u0445 \u2264 PrX (\u03b7 (X) 6 = 1 / 2 and gn, kn (X) 6 = g (X) > Result (2). The definition of Rn \u2212 n (x) is the decision limit, we then have the following sequence of Theorem 1. Korollar11. Let us (n) be any sequence of positive realities, and (kn) any sequence of positive integers. For each n, we define (pn) and (p) as in Theorem 1. ThenPrn \u2212 R > Answer (Rn >). Let (n) any sequence of positive realities, and (kn) any sequence of positive integers."}, {"heading": "2.10 Proof of Theorem 3", "text": "For positive integers n and 0 \u2264 p \u2264 1, then bin (n, p) k denotes the (binomial) distribution of the sum of n independent Bernoulli (p) random variables. We will use bin (n, p; k) to indicate the probability that this sum is \u2265 k; and likewise bin (n, p; \u2264 k). It is generally known that the binomial distribution can be approximated by a normal distribution, scaled appropriately. Slud [14] has results of this form that will be useful to us. Lemma 16. Select any 0 < p \u2264 1 / 2 and any non-negative integer. \"(a) [14, p 404, item (v) If\" \u2264 np, then bin (n, p; \u2265 \") results of this form that will be useful to us. Lemma 16. Select any 0 < p \u2264 1 and any non-negative integer.\""}, {"heading": "2.11 Proofs of Lemmas 4 and 5", "text": "It is immediately clear that if \u03b7 (\u03b1, L) -smooth in (X, \u03c1, \u00b5), then for all x-Supp (\u00b5) and all r > 0, | \u03b7 (B (x, r) \u2212 \u03b7 (x) | \u2264 L\u00b5 (Bo (x, r))) \u03b1 (5) all x-Supp (\u00b5) and all p \u2265 0. For r \u2264 rp (x) we have \u00b5 (Bo (x, r) \u2264 p and thus for (5), | \u03b7 (B (x, r) \u2212 \u03b7 (x) | \u2264 Lp\u03b1.Consequently, such an x lies in the effective interior X + p, \u0445. A similar result applies for x with \u03b7 (x) > 1 / 2 + \u2206 + Lp\u03b1 then \u03b7 (B (x, r) > 1 / 2 + \u0430, whenever r \u2264 rp (x). Therefore, such an x lies in the effective interior X + p (x)."}, {"heading": "2.12 Proof of Lemma 6", "text": "Suppose \u03b7 meets the \u03b1-holder condition, so that for a certain constant we have C > 0, | \u03b7 (x) \u2212 \u03b7 (x \u2032) | \u2264 C \u00b2 x \u2212 x \u00b2 \u03b1Hwhenever x, x \u00b2 X. For each x-soup (\u00b5) and r > 0 we then have | \u03b7 (x) \u2212 \u03b7 (B (x, r)) | \u2264 Cr\u03b1H. If \u00b5 has a density lower by \u00b5min and B (x, r) \u00b2 X, we also have \u00b5 (Bo (x, r) \u2265 \u00b5minvdrd, where vd is the volume of the standard sphere in Rd. The problem arises from the combination of these two inequalities."}, {"heading": "2.13 Proof of Theorem 7", "text": "Suppose that \u03b7 (\u03b1, L) -smooth is in (X, \u03c1, \u00b5), i.e. that it (B (x, r) \u2212 p (Bo (x, r)) \u2212 p (all x) \u2212 p (all x) \u2212 supp (\u00b5) and all r > 0, and also that it fulfills the \u03b2 margin condition (with constant C), under which for each t 0, \u00b5 (\u2264) and for each other x (x) \u2212 p (12), those under (x) and (7), for each other x). (7) Proof theorem 7 (a) theorem p, as specified in Theorem 1. It follows from this theorem and from Lemma 4 that under (6) and (7), for each other x), with probability. (0, with at least 1 \u2212 p) Proof Theorem 7 (a) Proof Theorem 7 (gn)."}, {"heading": "2.14 Zero Bayes Risk", "text": "An interesting case is when there is no inherent uncertainty in the conditional probability distribution p (y | x). Formally, moreover, for all x in sample space X, except for those in a subset X0 of the measurement variable zero, \u03b7 (x) either 0 or 1. In this case, the omniscient Bayes classifier carries a risk R \u043a = 0; however, a classifier based on a finite sample that is not aware of the true procedure will cause a non-zero error in the classification. An interesting quantity to consider in this case is the effective rest of the classes as a whole: X + p = {x \u00b2 Supp (\u00b5) | p Profinitative probability (x, p \u00b2), p \u00b2 Profinitative probability (B (x, r)) = 1 for all r \u2264 rp (x). X \u2212 p = {x \u00b2 Profinitative probability is equal (x)."}, {"heading": "2.15 Additional technical lemmas", "text": "any-any-any-any-any-any-any-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arbitrary-arary-"}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Nearest neighbor methods are a popular class of nonparametric estimators with several<lb>desirable properties, such as adaptivity to different distance scales in different regions of<lb>space. Prior work on convergence rates for nearest neighbor classification has not fully<lb>reflected these subtle properties. We analyze the behavior of these estimators in metric<lb>spaces and provide finite-sample, distribution-dependent rates of convergence under min-<lb>imal assumptions. As a by-product, we are able to establish the universal consistency of<lb>nearest neighbor in a broader range of data spaces than was previously known. We illus-<lb>trate our upper and lower bounds by introducing smoothness classes that are customized<lb>for nearest neighbor classification.", "creator": "LaTeX with hyperref package"}}}