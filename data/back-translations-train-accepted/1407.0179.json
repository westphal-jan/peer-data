{"id": "1407.0179", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2014", "title": "Mind the Nuisance: Gaussian Process Classification using Privileged Noise", "abstract": "The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian Process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC sigmoid likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.", "histories": [["v1", "Tue, 1 Jul 2014 10:44:49 GMT  (644kb,D)", "http://arxiv.org/abs/1407.0179v1", "14 pages with figures"]], "COMMENTS": "14 pages with figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["daniel hern\u00e1ndez-lobato", "viktoriia sharmanska", "kristian kersting", "christoph h lampert", "novi quadrianto"], "accepted": true, "id": "1407.0179"}, "pdf": {"name": "1407.0179.pdf", "metadata": {"source": "CRF", "title": "Mind the Nuisance: Gaussian Process Classification using Privileged Noise", "authors": ["Daniel Hern\u00e1ndez-Lobato", "Viktoriia Sharmanska", "Kristian Kersting", "Christoph Lampert", "Novi Quadrianto"], "emails": ["N.Quadrianto@sussex.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 GPC+: Gaussian Process Classification with Privi-", "text": "leged NoiseFor the self-consistency of the paper, we first review the GPC model [13] with particular emphasis on the noise-corrupted latent Gaussian process view. We then show how we treat privileged information in this latent process as heteroscedastic noise. The elegant aspect of this view is the intuition of how privileged noise is able to distinguish simple from hard samples and recalibrate our uncertainty in the original space."}, {"heading": "2.1 Gaussian process classifier with noisy latent process", "text": "There are a number of N starting points or samples D = (x1, y1) in which noise (xN, yN) plays an essential role. (xn, yN) There is an essential function (xn) in which noise (xn) plays an essential role. (xn, yn) There is an essential function (xn) in which noise (xn) plays an essential role. (xn) There is an essential function (xn) in which noise (xn) plays an essential role. (xn) We assume the following form of the perception function for f (x1). (xn) >: Pr (y), X = (x1) >. (xn) > We assume that noise (xn) plays an essential role."}, {"heading": "2.2 Privileged information is in the Nuisance Function", "text": "In the realm of learning with privileges (LUPI), the paradigm (2), in addition to the input data points (x1,., xN) and associated outputs (x1,.,., yN) that we received as additional information, must do so while ensuring that the function does not use the privileged data directly as input, as they simply are not available in the test phase. We achieve this, of course, by treating the privileged information as heteroscedastic (input-dependent) noise in the latent process.Our model of classification with privileged noise is then as follows: Pr (yn = 1, f) = I [f], where xn (xn) is the privileged noise in the latent process.Our model with privileged noise is then as follows: Likelihood Model: Pr (yn = 1, f) = I [f]."}, {"heading": "2.3 Posterior and Prediction on Test Data", "text": ", f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f), f, f, f, f, f, f, f, f, f, f, f, f, f (), f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f (), f, f, f, f), f, f (), f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f,"}, {"heading": "3 Expectation Propagation with Numerical Quadrature", "text": "Unfortunately, as with most interesting Bayesian models, the conclusion in the GPC + model is very challenging, and even in the homoscedastic case study the predictive density and marginal probability is not analytically comprehensible. Therefore, in this paper we adapt Minka's Expectation Propagation (EP) [15] with numerical squaring for approximate distribution. Please note that EP is the preferred method for approximate inference with GPCs in terms of accuracy and computational costs [16, 17]. Consider the common distribution of f, g, and y. Namely, Pr (X, X), f, g, p, p, p) Pr (f) Pr (g), where Pr (g) and Pr (g) are Gaussian processes and the probability statement is Pr (y, X, g)."}, {"heading": "4 Experiments", "text": "To achieve this goal, we looked at three types of binary classification tasks that correspond to different privileged information, using two real data sets: Attributes Discovery and Animals with Attributes. We detail these experiments successively in the following sections. Methods We compared our proposed GPC + method with the well-established method based on SVM, SVM +. For reference, we also adapt standard GP and SVM classifiers when learning on the original Rd space (GPC and SVM baselines). For all four methods, we used a quadratic exponential kernel with amplitude parameters and smoothing parameters. To ensure simplicity, we set procedural error = 1.0 in all cases. For GPC and GPC + we used type II maximum probability for evaluating the hyperparameters. There are two hyperparameters in GPC (smoothing parameters and deviation parameter and SVM + two and SVM + two)."}, {"heading": "4.1 Attribute Discovery Dataset [19]", "text": "The dataset was collected from a shopping site that aggregates product data from a variety of e-commerce sources and includes both images and related textual descriptions. The images and related texts are grouped into 4 broad purchasing categories: bags, earrings, ties and shoes. We used 1800 samples from this dataset. We generated 6 binary classification tasks for each pair of the 4 classes with 200 samples for training, 200 samples for validation and the rest of the samples for testing predictable performance. Neural networks to texts as privileged information We used images as the original domain and texts as the privileged domain. This setting was also explored in [6]. However, we used a different dataset than textual descriptions of the images used in [6] and included duplicates. In addition, we extracted more advanced text properties instead of simple termini-frequency (TF) characteristics."}, {"heading": "4.2 Animals with Attributes (AwA) Dataset [23]", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "5 Conclusions", "text": "We presented the first treatment of learning with privileged information settings in the Gaussian Process Classification (GPC +), called GPC +. Privileged information penetrates into the latent noise layer of GPC +, resulting in a data-dependent modulation of the sigmoid propensity of GP probability. As our experimental results show, this is an effective method of using privileged information that manifests itself in significantly improved classification accuracies. In fact, to our knowledge, this is the first time that a heteroscedastic noise term has been used to improve the GPC. Furthermore, we have shown that recent advances in continuous word-vector neural network imaging [22] and deep convolutionary networks for imaging [24] are privileged information. For future work, we plan to extend the GPC + to multicassituation and accelerate the calculation by developing a square-free propagation method, similar [26]."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "The learning with privileged information setting has recently attracted a lot of atten-<lb>tion within the machine learning community, as it allows the integration of additional<lb>knowledge into the training process of a classifier, even when this comes in the form of<lb>a data modality that is not available at test time. Here, we show that privileged infor-<lb>mation can naturally be treated as noise in the latent function of a Gaussian Process<lb>classifier (GPC). That is, in contrast to the standard GPC setting, the latent func-<lb>tion is not just a nuisance but a feature: it becomes a natural measure of confidence<lb>about the training data by modulating the slope of the GPC sigmoid likelihood function.<lb>Extensive experiments on public datasets show that the proposed GPC method using<lb>privileged noise, called GPC+, improves over a standard GPC without privileged knowl-<lb>edge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover,<lb>we show that advanced neural networks and deep learning methods can be compressed<lb>as privileged information.", "creator": "LaTeX with hyperref package"}}}