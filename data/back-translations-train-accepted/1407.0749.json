{"id": "1407.0749", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2014", "title": "Projecting Ising Model Parameters for Fast Mixing", "abstract": "Inference in general Ising models is difficult, due to high treewidth making tree-based algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We find that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.", "histories": [["v1", "Thu, 3 Jul 2014 00:19:08 GMT  (1976kb)", "https://arxiv.org/abs/1407.0749v1", "Advances in Neural Information Processing Systems 2013"], ["v2", "Wed, 8 Oct 2014 06:30:20 GMT  (1976kb)", "http://arxiv.org/abs/1407.0749v2", "Advances in Neural Information Processing Systems 2013"]], "COMMENTS": "Advances in Neural Information Processing Systems 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["justin domke", "xianghang liu"], "accepted": true, "id": "1407.0749"}, "pdf": {"name": "1407.0749.pdf", "metadata": {"source": "CRF", "title": "Projecting Ising Model Parameters for Fast Mixing", "authors": ["Justin Domke"], "emails": ["justin.domke@nicta.com.au", "xianghang.liu@nicta.com.au"], "sections": [{"heading": null, "text": "ar Xiv: 140 7.07 49v2 [cs.LG] 8 October 2Inference in general Output of models is difficult because the tree width makes tree-based algorithms insoluble. In addition, in strong interactions it can take exponentially long for Gibbs sampling to converge to stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be mixed quickly among multiple divergences. We find that Gibbs sampling is more accurate based on the projected parameters than with the original parameters when the interaction strength is strong and when there is limited time available for sampling."}, {"heading": "1 Introduction", "text": "To accomplish this, an approximation is often made on the basis of a tractable model. For example, given some intractable distributions q, center inference [14], attempts are made to minimize KL (p | | q) via p \u00b2 TRACT, where TRACT is the set of fully factored distributions. Similarly, structured intermediate field minimizes KL divergence, but allows TRACT to compose distributions that obey any tree [16] or a non-overlapping cluster structure [20]. In different ways, the loopic belief propagation [21] and tree-weighted belief propagation [19] can also be the divergence of distributions that obey any tree [16] or represent a non-overlapping structure [20]."}, {"heading": "2 Background", "text": "The literature on mixing times in Markov-Wang algorithms is extensive, including a recent textbook [7] principle that is unique in each case. [10] The presentation in the rest of this section is based on that of Dyer et al. [4]. Given a distribution p (x), one will often wish to take samples from it. [10] In this paper, we consider the classic Gibbs sampling method [5], in which one begins with a certain configuration x and repeatedly selects a node i from p (xi).Gibbs methods can be shown using a distribution converted from p. It is common to use more complex methods such as Block Gibbs sampling, the Swendsen-Wang algorithm [18], or Tree sampling [7]."}, {"heading": "3 Mixing Time Bounds", "text": "For variables xi-1, + 1}, an Ising model of the form (x) = exp-i, j\u03b2ijxixj + \u2211 i\u03b1ixi-A (\u03b2, \u03b1), where \u03b2ij is the interaction strength between variables i and j, \u03b1i is the \"field\" for variable i and A ensures normalization, can be considered a member of the exponential family p (x) = exp (\u03b8 \u00b7 f (x) \u2212 A (\u03b8), where f (x) = {xixj-i (i, j)} and \u03b8 contains both \u03b2 and \u03b1. Lemma 5. In an Ising model, the dependency matrix is limited by Rij \u2264 tanh | \u03b2ij | \u03b2ij | \u03b2ij-haes [8], proving this in the case of constants \u03b2 and zero field, but simple modifications of the evidence can provide this result."}, {"heading": "4 Projection", "text": "In this section, we imagine that we have a number of parameters that do not necessarily come closest to rapid mixing, and would like to obtain another set of parameters that are as close to the ground as possible, but guaranteed to be mixed quickly. This section derives a projection in the Euclidean norm, while section 5 will build on it to take into account other divergence measurements. If A has a singular value indicating decomposition A = UPS T and decomposition M, then B = arg minB | 2 \u2264 c | F as threshold can be reached, the densest matrix B with a maximum spectral norm, decomposition A = UPS T, and decomposition F denoting the Frobenius norm, then B = arg minB | 2 \u2264 c | F as threshold. Theorem 6. If A = US \u2032 V T, where S \u2032 ii = min (Sii, c 2).We like to denounce this projection."}, {"heading": "5 Divergences", "text": "Again, we would like to find a parameter vector that is close to a given vector \u03b8, but is guaranteed to be mixed quickly, but with several notions of \"proximity\" that vary in terms of accuracy and computational ease. Formally, if we specify a set of parameters that we can guarantee for fast mixing, and D (\u03b8, \u03c8) represents a divergence between the quality of the approximation and the ease of calculating the projection in equation, we would like to find a solution. (5) As we will see, when selecting D, there seems to be something of a trade-off between the quality of the approximation and the simplicity of calculating the projection in equation. In this section, we will work with the generic exponential family representation p (x; junction) = Exp (\u03b8 \u00b7 f (x) \u2212 A (IS)))). We use \u00b5 to denote the mean value of f. By a standard result, this is similar to the gradient of A, i.e.: Celsius = (Exfilfactor = Exp)."}, {"heading": "5.1 Euclidean Distance", "text": "The simplest divergence is simply the l2 distance between the parameter vectors, D (\u03b8, \u0443) = | | \u03b8 \u2212 \u043d | | 2. For the Ising model, theorem 7 represents a method for calculating the projected lineage strategy for calculating the projection in equation 5 under more general deviations. Specifically, we will do this by iteration. Specifically, we will do this by iteration. In other cases, this will be done simply by iteration."}, {"heading": "5.2 KL-Divergence", "text": "Perhaps the most natural divergence would be the \"integrative\" KL divergence D (\u03b8, \u0443) = KL (\u03b8 | | \u0443) = \u2211 xp (x; \u03b8) log p (x; \u03b8) p (x; \u0443). (6) This has the \"zero avoidance\" property [12], which will tend to assign a certain probability to all configurations to which \u03b8 assigns an unequal probability. It is easy to prove that the derivative isdD (\u03b8, \u0443) d\u0443 = \u00b5 (\u043a) \u2212 \u00b5 (\u03b8), (7) where microconstellations have a non-zero probability. Unfortunately, this requires conclusions both with respect to the parameters vector approaches and with respect to them. Since this divergence will be forced during optimization, one could approach the micro range by random sampling. However, it is assumed that this is a slow mixing, which makes the calculation of the microparameter of the toy more difficult."}, {"heading": "5.3 Piecewise KL-Divergences", "text": "Inspired by the piece probability [17] and the probability approximations based on tree mixtures [15], we are looking for tractable approximations of KL divergence based on tractable subgraphs. Our motivation is the following: If both groups define the same distribution, then when a certain set of edges is removed from both, they should continue to have the same distribution1. Thus, in a certain diagram T, we define the \"projection\" onto the tree by setting all edge parameters to zero, if not part of T. Then, in a set of diagrams T, we calculate the piecemeal KL divergence isD (\u03b8, \u0432) = max TKL (T) | | instead (T). Calculating the derivation of this divergence is not difficult - simply calculate the KL divergence for each tree and use the gradient as in Equation 7. There is some flexibility in the selection of the graph, with each T being selected with the simplest branch."}, {"heading": "5.4 Reversed KL-Divergence", "text": "We also consider the zero-forcing divergence KL divergence D (\u03b8, \u0432) = KL (\u043d | | \u03b8) = \u2211 xp (x; preservation) p (x; \u03b8).Theorem 8. Divergence D (\u03b8, \u0432) = KL (\u043a | | \u03b8) has the gradation degree D (\u03b8, \u0432) = \u2211 xp (x; \u0432) (\u044b \u2212 \u03b8) \u00b7 f (f (x) \u2212 \u00b5 (\u0432))).However, the use of this divergence is worse than the zero-avoidance divergence KL divergence. For example, since the parameters quantitatively do not represent a significant probability in configurations where the transition works by using samples for reweighting in order to estimate expectations regarding the development might have a high deviation. Furthermore, it may not be convex-forcex exvex. Nevertheless, it works well in practice where the indicator is often closely related to the indicator."}, {"heading": "6 Experiments", "text": "Our experimental evaluation follows that of Hazan and Shashua [9] in evaluating the accuracy of the methods using the Ising model in different configurations. In the experiments, we approach randomly generated issuing models with fast-mixing distributions using the projection algorithms described above. However, then the margins of the fast-mixing approximate distribution are compared with those of the target distributions by running a Gibbs chain each. We calculate the mean absolute distance of the marginals as a measure of accuracy, using the marginals calculated using the exact junction-tree algorithm. We evaluate the projection under the Euclidean distance (Section 5.1), the piecewise divergence (Section 5.3), and the zero mandatory KL divergence KL (Section 5.4), and the TRI divergence KL (Section 5.4)."}, {"heading": "6.1 Configurations", "text": "Two types of graph topologies are used: two-dimensional 8 \u00d7 8 grids and random graphs with 10 nodes. Each edge is independent with the probability of pe pe pe pe pe pe pe pe {0,3, 0,5, 0,7}. Node parameters \u03b8i are uniformly drawn from unif (\u2212 dn, dn) and we fix the field strength to dn = 1,0. Edge parameters \u03b8ij are uniformly drawn from unif (\u2212 de, de) or unif (0, de) to obtain mixed or attractive interactions in each case. We generate diagrams with different interaction strength de = 0, 0,5,.., 4. All results are averaged over 50 random trialities. To calculate piecemeal deviations, it remains to specify the set of subgraphs T. It can be any traceable subgraph of the original distribution. For grids, an easy choice is to use the horizontal and vertical chains as subgraphs."}, {"heading": "7 Discussion", "text": "In this paper, we have proposed a new concept of \"tractability,\" not insisting that a graph has a fast algorithm for exact inferences, but only that it complies with parameter-space conditions that ensure that Gibbs sampling converges quickly to the stationary distribution. In the case of Ising models, we use a simple condition that can guarantee rapid mixing, namely that the spectral norm of the matrix of interaction strengths is less than one.7 In the face of an intractable set of parameters, we consider the use of this approximate family by \"projecting\" the intractable distribution onto it among multiple divergences. First, we consider the Euclidean distance from parameters and derive a dual algorithm to solve the projection based on an iterative threshold decomposition."}], "references": [{"title": "A limited memory algorithm for bound constrained optimization", "author": ["Richard H. Byrd", "Peihuang Lu", "Jorge Nocedal", "Ciyou Zhu"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Markov chain monte carlo convergence diagnostics: A comparative review", "author": ["Mary Kathryn Cowles", "Bradley P. Carlin"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Ergodic mirror descent", "author": ["John C. Duchi", "Alekh Agarwal", "Mikael Johansson", "Michael I. Jordan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Matrix norms and rapid mixing for spin systems", "author": ["Martin E. Dyer", "Leslie Ann Goldberg", "Mark Jerrum"], "venue": "Ann. Appl. Probab.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["Stuart Geman", "Donald Geman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1984}, {"title": "Approximate inference using planar graph decomposition", "author": ["Amir Globerson", "Tommi Jaakkola"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "From fields to trees", "author": ["Firas Hamze", "Nando de Freitas"], "venue": "In UAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A simple condition implying rapid mixing of single-site dynamics on spin systems", "author": ["Thomas P. Hayes"], "venue": "In FOCS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Convergent message-passing algorithms for inference over general graphs with convex free energies", "author": ["Tamir Hazan", "Amnon Shashua"], "venue": "In UAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Markov chains and mixing times", "author": ["David A. Levin", "Yuval Peres", "Elizabeth L. Wilmer"], "venue": "American Mathematical Society,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Critical Ising on the square lattice mixes in polynomial time", "author": ["Eyal Lubetzky", "Allan Sly"], "venue": "Commun. Math. Phys.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Divergence measures and message passing", "author": ["Thomas Minka"], "venue": "Technical report,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Can extra updates delay mixing", "author": ["Yuval Peres", "Peter Winkler"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A mean field theory learning algorithm for neural networks", "author": ["C. Peterson", "J.R. Anderson"], "venue": "Complex Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1987}, {"title": "Spanning Tree Approximations for Conditional Random Fields", "author": ["Patrick Pletscher", "Cheng S. Ong", "Joachim M. Buhmann"], "venue": "In AISTATS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Exploiting tractable substructures in intractable networks", "author": ["Lawrence K. Saul", "Michael I. Jordan"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Piecewise training for structured prediction", "author": ["Charles Sutton", "Andrew Mccallum"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Nonuniversal critical dynamics in monte carlo simulations", "author": ["Robert H. Swendsen", "Jian-Sheng Wang"], "venue": "Phys. Rev. Lett.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1987}, {"title": "A new class of upper bounds on the log partition function", "author": ["Martin Wainwright", "Tommi Jaakkola", "Alan Willsky"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "A generalized mean field algorithm for variational inference in exponential families", "author": ["Eric P. Xing", "Michael I. Jordan", "Stuart Russell"], "venue": "In UAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["Jonathan Yedidia", "William Freeman", "Yair Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}], "referenceMentions": [{"referenceID": 13, "context": "For example, given some intractable distribution q, mean-field inference [14] attempts to minimize KL(p||q) over p \u2208 TRACT, where TRACT is the set of fully-factorized distributions.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "Similarly, structured meanfield minimizes the KL-divergence, but allows TRACT to be the set of distributions that obey some tree [16] or a non-overlapping clustered [20] structure.", "startOffset": 129, "endOffset": 133}, {"referenceID": 19, "context": "Similarly, structured meanfield minimizes the KL-divergence, but allows TRACT to be the set of distributions that obey some tree [16] or a non-overlapping clustered [20] structure.", "startOffset": 165, "endOffset": 169}, {"referenceID": 20, "context": "In different ways, loopy belief propagation [21] and tree-reweighted belief propagation [19] also make use of tree-based approximations, while Globerson and Jaakkola [6] provide an approximate inference method based on exact inference in planar graphs with zero field.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "In different ways, loopy belief propagation [21] and tree-reweighted belief propagation [19] also make use of tree-based approximations, while Globerson and Jaakkola [6] provide an approximate inference method based on exact inference in planar graphs with zero field.", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "In different ways, loopy belief propagation [21] and tree-reweighted belief propagation [19] also make use of tree-based approximations, while Globerson and Jaakkola [6] provide an approximate inference method based on exact inference in planar graphs with zero field.", "startOffset": 166, "endOffset": 169}, {"referenceID": 9, "context": "The literature on mixing times in Markov chains is extensive, including a recent textbook [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In this paper, we consider the classic Gibbs sampling method [5], where one starts with some configuration x, and repeatedly picks a node i, and samples xi from p(xi|x\u2212i).", "startOffset": 61, "endOffset": 64}, {"referenceID": 17, "context": "It is common to use more sophisticated methods such as block Gibbs sampling, the Swendsen-Wang algorithm [18], or tree sampling [7].", "startOffset": 105, "endOffset": 109}, {"referenceID": 6, "context": "It is common to use more sophisticated methods such as block Gibbs sampling, the Swendsen-Wang algorithm [18], or tree sampling [7].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "Here, we focus on the univariate case for simplicity and because fast mixing of univariate Gibbs is sufficient for fast mixing of some other methods [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 10, "context": "For example, for the two-dimensional Ising model with zero field and uniform interactions, it is known that mixing time is polynomial (in the size of the grid) when the interaction strengths are below a threshold \u03b2c, and exponential for stronger interactions [11].", "startOffset": 259, "endOffset": 263}, {"referenceID": 7, "context": "The main result we will use is the following [8].", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "A similar result holds in the case of systematic scan updates [4, 8].", "startOffset": 62, "endOffset": 68}, {"referenceID": 7, "context": "A similar result holds in the case of systematic scan updates [4, 8].", "startOffset": 62, "endOffset": 68}, {"referenceID": 7, "context": "Hayes [8] proves this for the case of constant \u03b2 and zero-field, but simple modifications to the proof can give this result.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "To accomplish the maximization of g over M and \u039b, we use LBFGS-B [1], with bound constraints used to enforce that M \u2265 0.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Since the gradients estimated at each time-step are dependent, this can be seen as an instance of Ergodic Mirror Descent [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 11, "context": "This has the \u201czero-avoiding\u201d property [12] that \u03c8 will tend to assign some probability to all configurations that \u03b8 assigns nonzero probability to.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "Inspired by the piecewise likelihood [17] and likelihood approximations based on mixtures of trees [15], we seek tractable approximations of the KL-divergence based on tractable subgraphs.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "Inspired by the piecewise likelihood [17] and likelihood approximations based on mixtures of trees [15], we seek tractable approximations of the KL-divergence based on tractable subgraphs.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Our experimental evaluation follows that of Hazan and Shashua [9] in evaluating the accuracy of the methods using the Ising model in various configurations.", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "However, this can take exponentially long and convergence is generally hard to diagnose [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "(This algorithm can be seen as an instance of Ergodic Mirror Descent [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "4407, obtained using more advanced techniques than the spectral norm [11].", "startOffset": 69, "endOffset": 73}], "year": 2014, "abstractText": "Inference in general Ising models is difficult, due to high treewidth making treebased algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We find that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.", "creator": "LaTeX with hyperref package"}}}