{"id": "1407.0754", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2014", "title": "Structured Learning via Logistic Regression", "abstract": "A successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages, and iterate between updates to each. This paper observes that if the inference problem is \"smoothed\" through the addition of entropy terms, for fixed messages, the learning objective reduces to a traditional (non-structured) logistic regression problem with respect to parameters. In these logistic regression problems, each training example has a bias term determined by the current set of messages. Based on this insight, the structured energy function can be extended from linear factors to any function class where an \"oracle\" exists to minimize a logistic loss.", "histories": [["v1", "Thu, 3 Jul 2014 00:48:34 GMT  (20206kb)", "http://arxiv.org/abs/1407.0754v1", "Advances in Neural Information Processing Systems 2013"]], "COMMENTS": "Advances in Neural Information Processing Systems 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["justin domke"], "accepted": true, "id": "1407.0754"}, "pdf": {"name": "1407.0754.pdf", "metadata": {"source": "CRF", "title": "Structured Learning via Logistic Regression", "authors": ["Justin Domke"], "emails": ["justin.domke@nicta.com.au"], "sections": [{"heading": null, "text": "ar Xiv: 140 7.07 54v1 [cs.LG] 3 Jul 201 4"}, {"heading": "1 Introduction", "text": "The structured learning problem is to find a function F (x, y) to fix a problem from inputs x to outputs as y \u043a = argmaxy F (x, y). F is chosen to optimize a loss function defined on these outputs. A major challenge is that assessing the loss for a given function F requires the solution of inference optimization to find the highest-value results for each copy, which is generally NP-hard. A standard solution for this is to write the loss function by means of an LP loosening of the inference problem, which means an upper limit for the actual loss. The learning problem can then be formulated as a common optimization of parameters and inference variables that can be solved, e.g. by alternating message updates of inference variables with gradient descent updates of parameters [16, 9].Previous work has largely focused on linear energy functions (F, y)."}, {"heading": "2 Structured Prediction", "text": "The structured prediction problem can be written in such a way that it searches for a function that predicts an output y from an input x. The maximum occurs over all configurations of the discrete vector y. Furthermore, it is assumed that the structured learning problem breaks down into a sum of functions using subsets of variables y\u03b1 as\u03a6 (x, y) = \u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "3 Loss Functions", "text": "Considering a dataset (x1, y1), (xN, yN), we must select the energyF to minimize empirical risk. [22] l0 (x k, yk; F) (xk, y) \u2212 F (xk, yk) + any loss function. [22] l0 (x k, yk; F) = max y (xk, yk) + any loss function l (yk, y), (3) where a standard selection (yk, y) is a certain level of discrepancy. [22] l0 (x k, yk; F) = max y (xk, yk) + a minor loss (yk, y), (\u03b1) where there is (yk, y) a certain level of discrepancy. We assume that it is a function via \u03b1, y.e."}, {"heading": "4 Overview", "text": "Now the learning problem is to select the functions f\u03b1 that compose F in order to minimize R as defined in Eq.2. The biggest challenge is that the assessment of R (F) requires conclusions. Specifically, if we define A (\u03b8) = max. \u00b5-MTB \u00b7 \u00b5-H (\u00b5\u03b1), then we have this saddle point problem. Inspired by previous work [16, 9], our solution (Section 5) is to introduce a vector of \"messages\" to write A into the dual formA (\u03b8) = min-A section, which leads to formulating learning as a common minimization date F-k [\u2212 F (xk, yk) + A section."}, {"heading": "5 Inference", "text": "To assess the loss, it is necessary to maximize in Eq = \u03b2 \u03b2 \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 k \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s s s \u00b2 s s \u00b2 s s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s s s \u00b2 s \u00b2 s s s s \u00b2 s s \u00b2 s s \u00b2 s s s s \u00b2 s s \u00b2 s s \u00b2 s s s \u00b2 s s s \u00b2 s s s \u00b2 s s s s \u00b2 s s s \u00b2 s s s \u00b2 s s s s \u00b2 s s s \u00b2 s s s s \u00b2 s s s s s \u00b2 s s s s s s \u00b2 s s \u00b2 s s s s \u00b2 s s s s s \u00b2 s s s s s \u00b2 s s s s s s \u00b2 s s s s s \u00b2 s s s \u00b2 s s s s s \u00b2 s s s s s \u00b2 s s s s \u00b2 s s s \u00b2 s s s s s s s s s \u00b2 s s s s s s s \u00b2 s s s s s s \u00b2 s s s s s s s s \u00b2 s s s \u00b2 s \u00b2 s s s s s s s s s s s s s s \u00b2 s s s s s s s s \u00b2 s s s s s s s s \u00b2 s s s s s s s s s s s \u00b2 s s s s s s s s s s s s s."}, {"heading": "6 Logistic Regression", "text": "Logistic regression is traditionally understood as the definition of a conditional distribution p (y | x; W) = exp ((Wx) y) / Z (x), where W is a matrix mapping the input characteristics x to a vector of the margins Wx. It is easy to show that the maximum conditional training problem maxW \u2211 k log p (y k | xk; W) Tomax W \u2211 k [(Wxk) yk \u2212 log \u2211 yexp (Wxk) y] corresponds to. Here, we generalize this in two ways. First, we take the mapping of characteristics x to the margin for labeling y as the y th component of Wx, and assume it as f (x, y) for a set of functions f in a set of functions F. (This is reduced to the linear case if f (x, y) = (Wx) y = (Bx) y \u2212 Secondly, let us assume that there is a predetermined \"biector\" -vector associated with each of the vectors."}, {"heading": "7 Training", "text": "Remember that the learning problem is to select the functions f\u03b1-F\u03b1 to determine the empirical risk R (F) = max (F) = max (F) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f (f) x (f) x (f) x (f) x (f (f) x (f) x (f) x (f (f) x (f) x (f (f) x (f) x (f (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f (f (f) x (f (f) x (f) x (f) x (f) x (f) x (f (f) x (f (f)"}, {"heading": "8 Experiments", "text": "These experiments look at three different function classes: linear, increased decision trees and multi-layer classification values. To maximize Eq. 11 among linear functions f (x, y) = (Wx) y, we simply calculate the gradient relative to W and use batch L-BFGS. For multi-layer perception, we fit the function f (x, y) = (Wx) y using stochastic gradient lineage with momentum2 to mini-batches of size 1000, using a step size of.25 for universal classifiers and.05 for pairs. Increased decision trees use stochastic gradients that increase logistic loss."}, {"heading": "9 Discussion", "text": "This work observes that in the structured learning environment, the optimization in terms of energy can be formulated as a logistical regression for each factor, \"biased\" by the current news. Thus, it is possible to use any functional class in which an \"oracle\" exists to optimize a logistical loss. The ability to define new energy functions for individual problems could have practical implications."}, {"heading": "Appendix for paper: Structured Learning via Logistic Regression", "text": "Theorem 5. The difference between l and l1 is limited by l1 (x, y, F) \u2264 l (x, y, F) \u2264 l1 (x, y, F) + l1 (x, y, F)."}], "references": [{"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Discriminative models for multi-class object layout", "author": ["Chaitanya Desai", "Deva Ramanan", "Charless C. Fowlkes"], "venue": "International Journal of Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Training conditional random fields via gradient tree boosting", "author": ["Thomas G. Dietterich", "Adam Ashenfelter", "Yaroslav Bulatov"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["Justin Domke"], "venue": "PAMI, 35(10):2454\u20132467,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Training structural svms when exact inference is intractable", "author": ["Thomas Finley", "Thorsten Joachims"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Stochastic gradient boosting", "author": ["Jerome H. Friedman"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Multi-class segmentation with relative location", "author": ["Stephen Gould", "Jim Rodgers", "David Cohen", "Gal Elidan", "Daphne Koller"], "venue": "prior. IJCV,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Efficient learning of structured predictors in general graphical models", "author": ["Tamir Hazan", "Raquel Urtasun"], "venue": "CoRR, abs/1210.2346,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Multiscale conditional random fields for image labeling", "author": ["Xuming He", "Richard S. Zemel", "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Convexity arguments for efficient minimization of the bethe and kikuchi free energies", "author": ["Tom Heskes"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Discriminative fields for modeling spatial dependencies in natural images", "author": ["Sanjiv Kumar", "Martial Hebert"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Associative hierarchical CRFs for object class image segmentation", "author": ["Lubor Ladicky", "Christopher Russell", "Pushmeet Kohli", "Philip H.S. Torr"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Polyhedral outer approximations with application to natural language parsing", "author": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Eric P. Xing"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Convergence rate analysis of MAP coordinate minimization algorithms", "author": ["Ofer Meshi", "Tommi Jaakkola", "Amir Globerson"], "venue": "In NIPS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Learning efficiently with approximate inference via dual losses", "author": ["Ofer Meshi", "David Sontag", "Tommi Jaakkola", "Amir Globerson"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "On parameter learning in CRF-based approaches to object class image segmentation", "author": ["Sebastian Nowozin", "Peter V. Gehler", "Christoph H. Lampert"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Decision tree fields", "author": ["Sebastian Nowozin", "Carsten Rother", "Shai Bagon", "Toby Sharp", "Bangpeng Yao", "Pushmeet Kohli"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Object class segmentation using random forests", "author": ["Florian Schroff", "Antonio Criminisi", "Andrew Zisserman"], "venue": "In BMVC,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture", "author": ["Jamie Shotton", "John M. Winn", "Carsten Rother", "Antonio Criminisi"], "venue": "layout, and context. IJCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Indoor scene segmentation using a structured light sensor", "author": ["Nathan Silberman", "Rob Fergus"], "venue": "In ICCV Workshops,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Max-margin markov networks", "author": ["Benjamin Taskar", "Carlos Guestrin", "Daphne Koller"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Scene segmentation with crfs learned from partially labeled images", "author": ["Jakob J. Verbeek", "Bill Triggs"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "The layout consistent random field for recognizing and segmenting partially occluded objects", "author": ["John M. Winn", "Jamie Shotton"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}], "referenceMentions": [{"referenceID": 15, "context": ", by alternating message-passing updates to inference variables with gradient descent updates to parameters [16, 9].", "startOffset": 108, "endOffset": 115}, {"referenceID": 8, "context": ", by alternating message-passing updates to inference variables with gradient descent updates to parameters [16, 9].", "startOffset": 108, "endOffset": 115}, {"referenceID": 22, "context": "While linear weights are often useful in practice [23, 16, 9, 3, 17, 12, 5], it is also common to make use of non-linear classifiers.", "startOffset": 50, "endOffset": 75}, {"referenceID": 15, "context": "While linear weights are often useful in practice [23, 16, 9, 3, 17, 12, 5], it is also common to make use of non-linear classifiers.", "startOffset": 50, "endOffset": 75}, {"referenceID": 8, "context": "While linear weights are often useful in practice [23, 16, 9, 3, 17, 12, 5], it is also common to make use of non-linear classifiers.", "startOffset": 50, "endOffset": 75}, {"referenceID": 2, "context": "While linear weights are often useful in practice [23, 16, 9, 3, 17, 12, 5], it is also common to make use of non-linear classifiers.", "startOffset": 50, "endOffset": 75}, {"referenceID": 16, "context": "While linear weights are often useful in practice [23, 16, 9, 3, 17, 12, 5], it is also common to make use of non-linear classifiers.", "startOffset": 50, "endOffset": 75}, {"referenceID": 11, "context": "While linear weights are often useful in practice [23, 16, 9, 3, 17, 12, 5], it is also common to make use of non-linear classifiers.", "startOffset": 50, "endOffset": 75}, {"referenceID": 4, "context": "While linear weights are often useful in practice [23, 16, 9, 3, 17, 12, 5], it is also common to make use of non-linear classifiers.", "startOffset": 50, "endOffset": 75}, {"referenceID": 19, "context": "ensembles of trees [20, 8, 25, 13, 24, 18, 19] or multi-layer perceptrons [10, 21]) to predict each variable independently.", "startOffset": 19, "endOffset": 46}, {"referenceID": 7, "context": "ensembles of trees [20, 8, 25, 13, 24, 18, 19] or multi-layer perceptrons [10, 21]) to predict each variable independently.", "startOffset": 19, "endOffset": 46}, {"referenceID": 12, "context": "ensembles of trees [20, 8, 25, 13, 24, 18, 19] or multi-layer perceptrons [10, 21]) to predict each variable independently.", "startOffset": 19, "endOffset": 46}, {"referenceID": 23, "context": "ensembles of trees [20, 8, 25, 13, 24, 18, 19] or multi-layer perceptrons [10, 21]) to predict each variable independently.", "startOffset": 19, "endOffset": 46}, {"referenceID": 17, "context": "ensembles of trees [20, 8, 25, 13, 24, 18, 19] or multi-layer perceptrons [10, 21]) to predict each variable independently.", "startOffset": 19, "endOffset": 46}, {"referenceID": 18, "context": "ensembles of trees [20, 8, 25, 13, 24, 18, 19] or multi-layer perceptrons [10, 21]) to predict each variable independently.", "startOffset": 19, "endOffset": 46}, {"referenceID": 9, "context": "ensembles of trees [20, 8, 25, 13, 24, 18, 19] or multi-layer perceptrons [10, 21]) to predict each variable independently.", "startOffset": 74, "endOffset": 82}, {"referenceID": 20, "context": "ensembles of trees [20, 8, 25, 13, 24, 18, 19] or multi-layer perceptrons [10, 21]) to predict each variable independently.", "startOffset": 74, "endOffset": 82}, {"referenceID": 19, "context": "Linear edge interaction weights are then learned, with unary classifiers either held fixed [20, 8, 25, 13, 24, 10] or used essentially as \u201cfeatures\u201d with linear weights readjusted [18].", "startOffset": 91, "endOffset": 114}, {"referenceID": 7, "context": "Linear edge interaction weights are then learned, with unary classifiers either held fixed [20, 8, 25, 13, 24, 10] or used essentially as \u201cfeatures\u201d with linear weights readjusted [18].", "startOffset": 91, "endOffset": 114}, {"referenceID": 12, "context": "Linear edge interaction weights are then learned, with unary classifiers either held fixed [20, 8, 25, 13, 24, 10] or used essentially as \u201cfeatures\u201d with linear weights readjusted [18].", "startOffset": 91, "endOffset": 114}, {"referenceID": 23, "context": "Linear edge interaction weights are then learned, with unary classifiers either held fixed [20, 8, 25, 13, 24, 10] or used essentially as \u201cfeatures\u201d with linear weights readjusted [18].", "startOffset": 91, "endOffset": 114}, {"referenceID": 9, "context": "Linear edge interaction weights are then learned, with unary classifiers either held fixed [20, 8, 25, 13, 24, 10] or used essentially as \u201cfeatures\u201d with linear weights readjusted [18].", "startOffset": 91, "endOffset": 114}, {"referenceID": 17, "context": "Linear edge interaction weights are then learned, with unary classifiers either held fixed [20, 8, 25, 13, 24, 10] or used essentially as \u201cfeatures\u201d with linear weights readjusted [18].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "Here, following previous work [15], we add entropy smoothing to the LP-relaxation of the inference problem.", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "Absent computational concerns, a standard choice would be the slackrescaled loss [22] l0(x , y;F ) = max y F (x, y)\u2212 F (x, y) + \u2206(y, y), (3) where \u2206(y, y) is some measure of discrepancy.", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "If this inference problem must be solved approximately, there is strong motivation [6] for using relaxations of the maximization in Eq.", "startOffset": 83, "endOffset": 86}, {"referenceID": 15, "context": "A common solution [16, 14, 6] is to use a linear relaxation1 l1(x , y;F ) = max \u03bc\u2208M F (x, \u03bc)\u2212 F (x, y) + \u2206(y, \u03bc), (4) where the local polytope M is defined as the set of local pseudomarginals that are normalized, and agree when marginalized over other neighboring regions, M = {\u03bc|\u03bc\u03b1\u03b2(y\u03b2) = \u03bc\u03b2(y\u03b2)\u2200\u03b2 \u2282 \u03b1, \u2211", "startOffset": 18, "endOffset": 29}, {"referenceID": 13, "context": "A common solution [16, 14, 6] is to use a linear relaxation1 l1(x , y;F ) = max \u03bc\u2208M F (x, \u03bc)\u2212 F (x, y) + \u2206(y, \u03bc), (4) where the local polytope M is defined as the set of local pseudomarginals that are normalized, and agree when marginalized over other neighboring regions, M = {\u03bc|\u03bc\u03b1\u03b2(y\u03b2) = \u03bc\u03b2(y\u03b2)\u2200\u03b2 \u2282 \u03b1, \u2211", "startOffset": 18, "endOffset": 29}, {"referenceID": 5, "context": "A common solution [16, 14, 6] is to use a linear relaxation1 l1(x , y;F ) = max \u03bc\u2208M F (x, \u03bc)\u2212 F (x, y) + \u2206(y, \u03bc), (4) where the local polytope M is defined as the set of local pseudomarginals that are normalized, and agree when marginalized over other neighboring regions, M = {\u03bc|\u03bc\u03b1\u03b2(y\u03b2) = \u03bc\u03b2(y\u03b2)\u2200\u03b2 \u2282 \u03b1, \u2211", "startOffset": 18, "endOffset": 29}, {"referenceID": 5, "context": "It is easy to show that l1 \u2265 l0, since the two would be equivalent if \u03bc were restricted to binary values, and hence the maximization in l1 takes place over a larger set [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 14, "context": "[15] who show that local message-passing can have a guaranteed convergence rate, and by Hazan and Urtasun [9] who use it for learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[15] who show that local message-passing can have a guaranteed convergence rate, and by Hazan and Urtasun [9] who use it for learning.", "startOffset": 106, "endOffset": 109}, {"referenceID": 14, "context": "A similar result was previously given [15] bounding the difference of the objective obtained by inference with and without entropy smoothing.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "Inspired by previous work [16, 9], our solution (Section 5) is to introduce a vector of \u201cmessages\u201d \u03bb to write A in the dual form A(\u03b8) = min \u03bb A(\u03bb, \u03b8), which leads to phrasing learning as the joint minimization min F min {\u03bbk} \u2211", "startOffset": 26, "endOffset": 33}, {"referenceID": 8, "context": "Inspired by previous work [16, 9], our solution (Section 5) is to introduce a vector of \u201cmessages\u201d \u03bb to write A in the dual form A(\u03b8) = min \u03bb A(\u03bb, \u03b8), which leads to phrasing learning as the joint minimization min F min {\u03bbk} \u2211", "startOffset": 26, "endOffset": 33}, {"referenceID": 10, "context": "Standard Lagrangian duality theory gives the following dual representation for A(\u03b8) in terms of \u201cmessages\u201d \u03bb\u03b1(x\u03b2) from a region \u03b1 to a subregion \u03b2 \u2282 \u03b1, a variant of the representation of Heskes [11].", "startOffset": 194, "endOffset": 198}, {"referenceID": 10, "context": "It can be shown [11, 15] that the update is \u03bb\u03b1(y\u03bd) \u2190 \u03bb\u03b1(y\u03bd) + \u01eb 1 +N\u03bd (log\u03bc\u03bd(y\u03bd) + \u2211", "startOffset": 16, "endOffset": 24}, {"referenceID": 14, "context": "It can be shown [11, 15] that the update is \u03bb\u03b1(y\u03bd) \u2190 \u03bb\u03b1(y\u03bd) + \u01eb 1 +N\u03bd (log\u03bc\u03bd(y\u03bd) + \u2211", "startOffset": 16, "endOffset": 24}, {"referenceID": 14, "context": "[15] show that with greedy or randomized selection of blocks to update, O( \u03b4 ) iterations are sufficient to converge within error \u03b4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Boosted decision trees use stochastic gradient boosting [7]: the gradient of the logistic loss is computed for each exemplar, and a regression tree is induced to fit this (one tree for each class).", "startOffset": 56, "endOffset": 59}], "year": 2014, "abstractText": "A successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages, and iterate between updates to each. This paper observes that if the inference problem is \u201csmoothed\u201d through the addition of entropy terms, for fixed messages, the learning objective reduces to a traditional (non-structured) logistic regression problem with respect to parameters. In these logistic regression problems, each training example has a bias term determined by the current set of messages. Based on this insight, the structured energy function can be extended from linear factors to any function class where an \u201coracle\u201d exists to minimize a logistic loss.", "creator": "LaTeX with hyperref package"}}}