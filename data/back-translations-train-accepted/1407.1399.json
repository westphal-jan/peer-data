{"id": "1407.1399", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2014", "title": "Generalized Higher-Order Tensor Decomposition via Parallel ADMM", "abstract": "Higher-order tensors are becoming prevalent in many scientific areas such as computer vision, social network analysis, data mining and neuroscience. Traditional tensor decomposition approaches face three major challenges: model selecting, gross corruptions and computational efficiency. To address these problems, we first propose a parallel trace norm regularized tensor decomposition method, and formulate it as a convex optimization problem. This method does not require the rank of each mode to be specified beforehand, and can automatically determine the number of factors in each mode through our optimization scheme. By considering the low-rank structure of the observed tensor, we analyze the equivalent relationship of the trace norm between a low-rank tensor and its core tensor. Then, we cast a non-convex tensor decomposition model into a weighted combination of multiple much smaller-scale matrix trace norm minimization. Finally, we develop two parallel alternating direction methods of multipliers (ADMM) to solve our problems. Experimental results verify that our regularized formulation is effective, and our methods are robust to noise or outliers.", "histories": [["v1", "Sat, 5 Jul 2014 11:58:30 GMT  (106kb)", "http://arxiv.org/abs/1407.1399v1", "9 pages, 5 figures, AAAI 2014"]], "COMMENTS": "9 pages, 5 figures, AAAI 2014", "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["fanhua shang", "yuanyuan liu", "james cheng"], "accepted": true, "id": "1407.1399"}, "pdf": {"name": "1407.1399.pdf", "metadata": {"source": "CRF", "title": "Generalized Higher-Order Tensor Decomposition via Parallel ADMM", "authors": ["Fanhua Shang", "Yuanyuan Liu", "James Cheng"], "emails": ["jcheng}@cse.cuhk.edu.hk;", "yyliu@se.cuhk.edu.hk"], "sections": [{"heading": null, "text": "ar Xiv: 140 7,13 99v1 [cs.NA] 5 Jul 201 4"}, {"heading": "Introduction", "text": "In fact, it is in such a way that it is about a way in which it is about a way in which people blame themselves and others. (...) In fact, it is in such a way that it is about a way in which people blame themselves and others. (...) It is as if it is about a way in which people blame themselves and themselves. (...) It is as if it is about a way in which people care about themselves and themselves about themselves. (...) It is as if it is about a way in which it is about themselves and about themselves. (...) It is as if it is about a way in which it is about, in which it is about. (...) It is as if it is about a way in which it is about, in which it is about a way in which it is about itself, in which it is about itself. (...) It is as if it is about a way in which it is about a way in which it is about itself and in which it is about itself."}, {"heading": "Notations and Related Work", "text": "First we present the notations, and further details can be seen in Kolda and Bader (2009). \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "Tensor Decomposition", "text": "We will review two popular models for the decomposition of tensors, i.e., the decomposition of Tucker and the decomposition of CANDECOMP (CP). It is known that the decomposition of CP with the minimum tensor rank is a hard problem, and there is no simple algorithm for calculating tensors of higher order (Hillar and Lim, 2013). The decomposition of tensor T into a core tensor multiplied with each mode as follows: T = G \u00b7 1 \u00b7 2 \u00b7 N UN, (1) where the decomposition of tensor T is."}, {"heading": "Convex Tensor Decomposition Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Convex Tensor Decomposition Model", "text": "Considering a tensor T, our goal is to find a low-ranking X tensor in order to minimize the Frobenius standard of its difference as follows: min X1, 2, X, T, 2F. (3) In contrast to the matrix case, the low-ranking tensor estimation problem (3) is generally difficult to solve (Narita et al., 2012). Following the progress in tensor completion, we throw it into a (weighted) track standard minimization problem: min XN, n = 1, X (n), 2, X \u2212 T, 2F, (4), where \"X, n\" tr denotes the track standard of the unfolded matrix X (n), i.e. the sum of its individual values, \u03bb > 0 is a regulation parameter. To handle the unbalanced target tensor, we set the preselected weights briefly."}, {"heading": "Parallel Optimization Algorithm", "text": "Due to the interdependent matrix normal terms, the proposed tensor decomposition model (4) is very difficult to solve. Therefore, we insert some additional variables Mn into the model (4) and formulate it in the following equivalent form: min X, {Mn} N = 1% Mn, (n) 2% Mn, (n) 2% Mn, (n) X \u2212 T, Mn = X, N (5) In the following, we will design an alternative method of multipliers (ADMM) for solving the problem (5). The algorithms decompose the original problem into smaller partial problems and solve them individually in each iteration. The parallel ADMM for the problem (5) is minimized by minimizing the augmented lagrange function L\u00b5 in relation to (X, {Mn}) and then updating the multipliers."}, {"heading": "Non-Convex Tensor Decomposition Method", "text": "Several researchers (Keshavan et al., 2010; Wen et al., 2012) have therefore presented some matrix rank estimation strategies to calculate some good values (r1, r2,.., rN) for the N rank of the tensor involved. Therefore, we only use relatively large integers (R1, R2,.., RN) so that Rn \u2265 rn, n = 1, \u00b7 \u00b7 \u00b7, N.Theorem 2 Let X = RI1 \u00b7 I2 \u00b7 \u00b7 \u00b7 \u00b7 IN with N rank = (r1, r2, \u00b7 \u00b7, rN) and G R1 \u00b7 \u00b7 \u00b7 \u00b7 RN \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 X = G \u00b7 \u00b7 \u00b7 \u00b7 N UN and UTn Un = IRn, n = 1, 2 \u00b7, N, then: X (n) \u00b7 sR (n) \u00b7 tensizes of the original, tr = 1, 2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 ssizes, n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 In theorem \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 2 \u00b7, the material that replaces the \u00b7 sizes, the tensizes, tensizes, tensizes \u00b7 sizes, n, tensizes \u00b7 sizes, n, tensizes \u00b7 sizes, n."}, {"heading": "Generalized HOOI Model with Trace Norm Penalties", "text": "According to the above analysis, our trace standard regulated HOOI model is formulated as follows: min G, {Un} N = IRn, n = 1, 2, \u00b7 \u00b7, N. (11) The Core Tensor Trace Standard regulated HOOI model (11), also known as generalized HOOI, can reduce the SVD computing load of large unfolded matrices involved in the convex Tucker decomposition problem (4). In addition, we use the trace standard regularization term in (11) to promote the robustness of ranking selection, while the original Tucker decomposition method is usually sensitive to the given ranks (R1, R2, \u00b7 \u00b7 \u00b7 n RN) (Liu et al., 2009)."}, {"heading": "Parallel Optimization Procedure", "text": "In this part, we will also propose a parallel ADMM problem to solve the problem (12).Updating {Gk + 1, Uk + 11, \u00b7 \u00b7, U k + 1 N}: The optimization problem related to G + 1, \u00b7 \u00b7, UN) is formulated as follows: min G, {Un + 1, # 2, # 1, # 2, # 2, # 2, # 2, # 2, # 2, # 2, # 2, # 2, # 2, # 2, # 2, # 2, # 3, # 2, # 3, # 4, # 4, # 5, # 5, # 5, # 5, # 6, # 6, # 6, # 6, 6, #, 6, 6, #, 6, 6, #, 6, 6, #, 6, 6 #, 6, 6 #, 6, 6, 6 #, 6, 6, 6 #, 6, 6, 6 #, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,"}, {"heading": "Complexity Analysis", "text": "We will now discuss the time complexity of our NCTD algorithm. Problem (11) uses up the main running time of our NCTD algorithm by executing SVD for the proximal operator and some multiplications. Thus, the time complexity of executing the proximal operator in (20) is O1: = O (\u2211 R2n\u0451j 6 = nRj) and O3: = O (\u2211 R2nIn). Thus, the total time complexity of our NCTD method is O (T (O1 + O2 + O3)), where T is the number of iterations. Algorithm 2 Solution problem (11) via parallel ADMM input: T, the tensor order (R1, R2, \u00b7 \u00b7 \u00b7 Update, RN + O3), where T is the number of iterations."}, {"heading": "Experimental Results", "text": "In this section, we evaluate both the effectiveness and efficiency of our methods for solving tensor decomposition problems using synthetic and real data. All experiments were conducted on an Intel (R) Core (TM) i5-4570 (3.20 GHz) PC with Windows 7 and 8 GB of RAM."}, {"heading": "Synthetic Data", "text": "The tensor data followed the Tucker model, i.e., T = G \u00b7 1 U1 \u00b7 \u00b7 \u00b7 N UN, where the core tensor G \u00b7 r \u00b7 \u00b7 r and factor matrices Un were generated with standard Gaussian entries i.i.d. The order of tensors varied from three to four, and rank r was set at {5, 10, 20}. Finally, we dissected the input tensor matrices Un were generated with i.i.i.d. standard Gaussian methods and the order of tensor algorithms including HOSVD (Vannieuwenhoven et al., 2012), HOOI (Lathauwer et al., 2000b), Mixture (Tomioka.) and ADM (Gandy."}, {"heading": "MRI Data", "text": "This part compares our CTD and NCTD methods, HOSVD and HOOI, based on 181 x 217 x 181 brain MRI data used in (Liu et al., 2009). This data set is roughly low: for the three gradients, the number of individual values greater than 1% of the largest is 17, 21 and 17, respectively. Figure 3 shows the average relative errors and runtimes of ten independent studies for each setting of the given order. Results show that our CTD and NCTD methods consistently have much lower relative errors than those of HOSVD and HOOI. Furthermore, our NCTD method is generally faster than the other methods."}, {"heading": "Conclusions", "text": "In this paper, we first proposed a convex tensor decomposition method, which, through our optimization scheme, can automatically determine the number of factors in each mode. In addition, taking into account the low structure of input sensors, we analyzed the equivalence relationship of the trace standard between a low tensor and its core tensor. We then used the non-convex tensor decomposition model in a weighted combination of several, much smaller, matrix trace standards minimizations. Finally, we developed two efficient parallel ADMM algorithms to solve the proposed problems. Convincing experimental results show that our regulated formulation is reasonable and our methods are robust against interference or outliers. In addition, our tensor decomposition methods can overcome some tensor restoration problems, such as tensor completion, and a low and sparse tensor decomposition."}, {"heading": "Acknowledgements", "text": "The first and third authors are supported by the CUHK Direct Grant no. 4055017 and Microsoft Research Asia Grant 6903555. The first and third authors are supported by the CUHK Direct Grant no. 4055017 and Microsoft Research Asia Grant no. 6903555."}, {"heading": "Convergence Behaviors of Our Algorithms", "text": "We also study the convergence behavior of our CTD and NCTD algorithms using the synthetic tendon data of size 200 x 200 x 200 with the given ranks, R1 = R2 = R3 = 20, as shown in Figure 1, where the ordinate is the residual quantity of max {\u0432 Gk (n) \u2212 G k n \u00b2 F, n = 1, \u00b7 \u00b7 \u00b7, N} or \u0441 X k \u2212 T \u00b2 F or the relative variation of X k, and the abscess indicates the number of iterations. In addition, we also provide the convergence results of HOOI. We can observe that the relative change or residual quantity of CTD and NCTD algorithms decreases very quickly and converges very quickly within 50 iterations, especially the relative change or residual quantity of CTD and NCTD decreases much faster than HOOI."}, {"heading": "Rank Estimation", "text": "In this part, we evaluate the ability of our CTD method to estimate the tensor ranges, as in Figure 2. As in the experimental section, we randomly have T of size 200 \u00b7 200 \u00b7 200 with tensor ranges r1 = r2 = r3 = r = 10 or 20. From Figure 2, we see that our CTD method can accurately estimate the rank of each tensor deployment."}, {"heading": "Robustness Against Outliers", "text": "Figure 3 shows the phase transition plots of HOSVD, HOOI, CTD and NCTD on the third order tensor data with rank r = 10, where the specified tensor Rn, n = 1, 2, 3 varied from 10 to 50 with increment 4 and the error parity ratio from 0 to 0.05 with increment 0.005. We created a sparse tensor whose unequal entries in the range [\u2212 1, 1] are independently and evenly distributed and whose support is randomly selected. 50 independent studies were conducted for each setting. Figure 3 shows that our methods significantly outperform CTD and NCTD HOSVD and HOOI."}, {"heading": "APPENDIX A", "text": "The detection of theorem 2: Before providing the proof of theorem 2, we will first present some properties of matrices and tensors in the following list. (UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUand UUUUUUUUUUUUUUUUUUUUUUUUUand))), and /.........................................................................................................................................................."}, {"heading": "APPENDIX B", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Theorem 3:", "text": "The optimization problem (13) in relation to G is written with (21) min GJ (G) = N \u2211 n = 1\u00b5k2 \u0445 G (n) \u2212 Gk n + Y k n / \u00b5 k \u0445 2F + \u03bb2 \u0445 T \u2212 G \u00b7 1 Uk 1 \u00b7 \u00b7 \u00b7 N U k N \u0445 2 F. The problem (21) above is a smooth convex optimization problem, so that we can obtain the derivation of function J as follows: (22) None G = \u03bb (G \u2212 T \u00b7 1 (U k 1) T \u00b7 \u00b7 N (U k N) T) + N \u2211 n = 1\u00b5k \u00b7 K \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 K)) = (N\u00b5k + \u03bb) G \u00b7 K \u00b7 K \u00b7 \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K (Gkn \u2212 Y \u00b7 K), (N \u00b7 K = 1 \u00b7 M (N \u00b7 K), (N \u00b7 K = 1 \u00b7 M \u00b7 K), (N \u00b7 K \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 K))))) = (N \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K), (N \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K), (N \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 \u00b7 K \u00b7 \u00b7 \u00b7 K \u00b7 \u00b7 K \u00b7 \u00b7 K \u00b7 K \u00b7"}, {"heading": "APPENDIX C", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Theorem 4:", "text": "Leave (25) H (G, {Un}) = N \u00b2 n = 1\u00b5k \u00b2 G (n) \u2212 G k n + Y k \u00b2 2F + \u03bb \u00b2 T \u2212 G \u00b7 1 U1 \u00b7 \u00b7 \u00b7 N UN \u00b2 2 F, then the closed solution of (25) with respect to G can be obtained by (14), and it can be rewritten asG \u00b2 = 1\u03bb + N\u00b5k (\u03bbM + \u00b5kN).This results in (26) H (G \u00b2, {Un}) = N \u00b2 n = 1\u00b5k \u00b2 G \u00b2 (n) \u2212 G \u00b2 n + Y k \u00b2 n / p \u00b2 k \u00b2 2F + \u03bb T \u00b2 2 F + \u043c G \u00b2 2F < T, G \u00b2 1 U1 \u00b7 \u00b7 \u00b7 N UN >, = 2\u043ah (U1, U2,.."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Higher-order tensors are becoming prevalent in many<lb>scientific areas such as computer vision, social network<lb>analysis, data mining and neuroscience. Traditional ten-<lb>sor decomposition approaches face three major chal-<lb>lenges: model selecting, gross corruptions and compu-<lb>tational efficiency. To address these problems, we first<lb>propose a parallel trace norm regularized tensor de-<lb>composition method, and formulate it as a convex op-<lb>timization problem. This method does not require the<lb>rank of each mode to be specified beforehand, and can<lb>automatically determine the number of factors in each<lb>mode through our optimization scheme. By considering<lb>the low-rank structure of the observed tensor, we ana-<lb>lyze the equivalent relationship of the trace norm be-<lb>tween a low-rank tensor and its core tensor. Then, we<lb>cast a non-convex tensor decomposition model into a<lb>weighted combination of multiple much smaller-scale<lb>matrix trace norm minimization. Finally, we develop<lb>two parallel alternating direction methods of multipli-<lb>ers (ADMM) to solve our problems. Experimental re-<lb>sults verify that our regularized formulation is effective,<lb>and our methods are robust to noise or outliers. Introduction<lb>The term tensor used in the context of this paper refers to<lb>a multi-dimensional array, also known as a multi-way or<lb>multi-mode array. For example, if X \u2208 R123 , then<lb>we say X is a third-order tensor, where order is the num-<lb>ber of ways or modes of the tensor. Thus, vectors and ma-<lb>trices are first-order and second-order tensors, respectively.<lb>Higher-order tensors arise in a wide variety of application ar-<lb>eas, such as machine learning (Tomioka and Suzuki, 2013;<lb>Signoretto et al., 2014), computer vision (Liu et al., 2009),<lb>data mining (Yilmaz et al., 2011; Morup, 2011; Narita et<lb>al., 2012; Liu et al., 2014), numerical linear algebra (Lath-<lb>auwer et al., 2000a; 2000b), and so on. Especially, with<lb>the rapid development of modern computer technology in<lb>recent years, tensors are becoming increasingly ubiquitous<lb>such as multi-channel images and videos, and have become<lb>increasingly popular (Kolda and Bader, 2009). When work-<lb>ing with high-order tensor data, various new computational Copyright c<lb>\u00a9 2014, Association for the Advancement of Artificial<lb>Intelligence (www.aaai.org). All rights reserved.<lb>challenges arise due to the exponential increase in time and<lb>memory space complexity when the number of orders in-<lb>creases. This is called the curse of dimensionality. In prac-<lb>tice, the underlying tensor is often low-rank, even though<lb>the actual data may not be due to noise or arbitrary errors.<lb>Essentially, the major component contained in the given ten-<lb>sor is often governed by a relatively small number of latent<lb>factors.<lb>One standard tool to alleviate the curse is tensor de-<lb>composition. Decomposition of high-order tensors into<lb>a small number of factors has been one of the main<lb>tasks in multi-way data analysis, and commonly takes two<lb>forms: Tucker decomposition (Tucker, 1966) and CANDE-<lb>COMP/PARAFAC (CP) (Harshman, 1970) decomposition.<lb>There are extensive studies in the literature for finding the<lb>Tucker decomposition and the CP decomposition for higher-<lb>order tensors (Kolda and Bader, 2009). In those tensor de-<lb>composition methods, their goal is to (approximately) re-<lb>construct the input tensor as a sum of simpler components<lb>with the hope that these simpler components would reveal<lb>the latent structure of the data. However, existing tensor de-<lb>composition methods face three major challenges: rank se-<lb>lection, outliers and gross corruptions, and computational<lb>efficiency. Since the Tucker and CP decomposition meth-<lb>ods are based on least-squares approximation, they are also<lb>very sensitive to outliers and gross corruptions (Goldfarb<lb>and Qin, 2014). In addition, the performance of those meth-<lb>ods is usually sensitive to the given ranks of the involved ten-<lb>sor (Liu et al., 2009). To address the problems, we propose<lb>two robust and parallel higher-order tensor decomposition<lb>methods with trace norm regularization.<lb>Recently, much attention has been drawn to the low-rank<lb>tensor recovery problem, which arises in a number of real-<lb>word applications, such as 3D image recovery, video in-<lb>painting, hyperspectral data recovery, and face reconstruc-<lb>tion. Compared with matrix-based analysis methods, tensor-<lb>based multi-linear data analysis has shown that tensor mod-<lb>els are capable of taking full advantage of the high-order<lb>structure to provide better understanding and more preci-<lb>sion. The key idea of low-rank tensor completion and recov-<lb>ery methods is to employ matrix trace norm minimization<lb>(also known as the nuclear norm, which is the convex surro-<lb>gate of the rank of the involved matrix). In addition, there are<lb>some theoretical developments that guarantee reconstruction of a low-rank tensor from partial measurements or grossly<lb>corrupted observations via solving the trace norm minimiza-<lb>tion problem under some reasonable conditions (Tomioka et<lb>al., 2011; Shi et al., 2013). Motivated by the recent progress<lb>in tensor completion and recovery, one goal of this paper<lb>is to extend the trace norm regularization to robust higher-<lb>order tensor decomposition.<lb>Different from existing tensor decomposition methods,<lb>we first propose a parallel trace norm regularized tensor de-<lb>composition method, which can automatically determine the<lb>number of factors in each mode through our optimization<lb>scheme. In other words, this method does not require the<lb>rank of each mode to be specified beforehand. In addition,<lb>by considering the low-rank structure of the observed tensor<lb>and further improving the scalability of our convex method,<lb>we analyze the equivalent relationship of the trace norm be-<lb>tween a low-rank tensor and its core tensor. Then, we cast<lb>the non-convex trace norm regularized higher-order orthog-<lb>onal iteration model into a weighted combination of multiple<lb>much-smaller-scale matrix trace norm minimization. More-<lb>over, we design two parallel alternating direction methods of<lb>multipliers (ADMM) to solve the proposed problems, which<lb>are shown to be fast, insensitive to initialization and robust<lb>to noise and/or outliers with extensive experiments. Notations and Related Work<lb>We first introduce the notations, and more details can be seen<lb>in Kolda and Bader (2009). An N th-order tensor is denoted<lb>by a calligraphic letter, e.g., T \u2208 R12N , and its<lb>entries are denoted by ti1\u00b7\u00b7\u00b7in\u00b7\u00b7\u00b7iN , where in \u2208 {1, \u00b7 \u00b7 \u00b7 ,<lb>In}<lb>for 1 \u2264 n \u2264 N . Fibers are the higher-order analogue of<lb>matrix rows and columns. The mode-n fibers are vectors<lb>ti1\u00b7\u00b7\u00b7in\u22121in+1\u00b7\u00b7\u00b7iN that are obtained by fixing the values of<lb>{i1, \u00b7 \u00b7 \u00b7 ,<lb>iN}\\in.<lb>The mode-n unfolding, also known as matricization, of an<lb>N th-order tensor T is denoted by T(n) \u2208 Rnj 6=nIj and<lb>arranges the mode-n fibers to be the columns of the result-<lb>ing matrix T(n) such that the mode-n fiber becomes the row<lb>index and all other (N \u2212 1) modes become the column in-<lb>dices. The tensor element (i1, i2, \u00b7 \u00b7 \u00b7 , iN) is mapped to the<lb>matrix element (in, j), where", "creator": "LaTeX with hyperref package"}}}