{"id": "1407.2538", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jul-2014", "title": "Learning Deep Structured Models", "abstract": "In recent years the performance of deep learning algorithms has been demonstrated in a variety of application domains. The goal of this paper is to enrich deep learning to be able to predict a set of random variables while taking into account their dependencies. Towards this goal, we propose an efficient algorithm that is able to learn structured models with non-linear functions. We demonstrate the effectiveness of our algorithm in the tasks of predicting words as well as codes from noisy images, and show that by jointly learning multilayer perceptrons and pairwise features, significant gains in performance can be obtained.", "histories": [["v1", "Wed, 9 Jul 2014 15:54:27 GMT  (380kb,D)", "https://arxiv.org/abs/1407.2538v1", "10 pages including reference"], ["v2", "Fri, 19 Dec 2014 21:50:10 GMT  (776kb,D)", "http://arxiv.org/abs/1407.2538v2", "11 pages including reference"], ["v3", "Mon, 27 Apr 2015 21:11:32 GMT  (4321kb,D)", "http://arxiv.org/abs/1407.2538v3", "11 pages including reference"]], "COMMENTS": "10 pages including reference", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["liang-chieh chen", "alexander g schwing", "alan l yuille", "raquel urtasun"], "accepted": true, "id": "1407.2538"}, "pdf": {"name": "1407.2538.pdf", "metadata": {"source": "CRF", "title": "LEARNING DEEP STRUCTURED MODELS", "authors": ["Liang-Chieh Chen", "Alexander G. Schwing", "Alan L. Yuille", "Raquel Urtasun"], "emails": ["lcchen@cs.ucla.edu", "aschwing@cs.toronto.edu", "yuille@stat.ucla.edu", "urtasun@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "A variety of variants have been proposed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and have been shown to be highly successful in a variety of applications, including object recognition, speech recognition, and natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2014). Recently, however, the results of state-of-the-art work in many computer vision tasks are being achieved that exceed competing methods by a large margin (Krizhevsky et al al al., 2013; Girshick et al.).Deep neural networks can be even more powerful when combined with models for mapping the dependent order."}, {"heading": "2 LEARNING DEEP STRUCTURED MODELS", "text": "In this section, we examine how to learn \"deep properties\" that take into account the dependencies between the estimated output variables. Let's let y-Y be the set of random variables y = (y1,.., yN) that we want to predict. In this thesis, we assume that the space of valid configurations is a product space, i.e. the input data x-X and the parameters w-RA of function F (x, y; w) must be discrete the computational capability of each variable yi, i.e., Yi-Yi |}. Given the input data x-X and the parameters w-RA of function F (x, y; w): X-Y \u00d7 RA \u2192 R, the inference boils down to the highest point configuration y = arg maxy F (x, y; w)."}, {"heading": "2.1 LEARNING VIA GRADIENT DESCENT", "text": "During the learning process, we are interested in finding the parameters w of the model by maximizing the data probability, i.e. by minimizing the negative log probability. (1) Note that this corresponds to maximizing the cross entropy between a target distribution p (x, y), tg (y, y) - D (x, y; w). (1) Note that this corresponds to maximizing the cross entropy between a target distribution p (x, y), tg (y) - D (x, y) - D (x, y) by putting all their mass on the truth label, and the model distribution p (x, y) (y) - the difference between the cross entropy (1) and the cross entropy p (x, y) is equally represented by max, y, y and y distributions p (x, y)."}, {"heading": "2.2 APPROXIMATE LEARNING", "text": "It is therefore not possible for us to calculate the exact gradients of the costs in Eq. (2) and we have to rely on an approximate solution (x, w). (F, y) E (x, y) E (x, y) E (x, y) E (x, y) E (x, y) E (x, y) E (x, y) E (x, y) E (x, y) E (x) E (x) E (x, y) E (x) E (x, y) E (x) E (x) E (x) E (x) E (x))) E (x) E (x) E (x)."}, {"heading": "2.3 EFFICIENT APPROXIMATE LEARNING BY BLENDING LEARNING AND INFERENCE", "text": "In this section, we propose a more efficient algorithm based on the principle of blending learning (i.e., parameter updates) (i.e., we are interested in writing only a single message before updating the parameters. Note that a simple reduction in the number of iterations is generally not an option, since the beliefs obtained are b (x, y), r (x), r (x), e (x), e (x), e (x), e (x), e (x), e (x), e (x), e (x), e (x), e (x), e (x), e (x), e (x), e (x), e (e), e (x), e (e), x (e), x (e), e (x, e, x (e), e (e), x (e), e (e, x (e), e (e, x (e), x (e), e (e, x (e), x (e), e (e, x (e), x (e), e (e, x (e), x (e), x (e, x (e), x (e, x (e), x (e), x (e, x (e), x (e, x (e), x (e, x (e), x (e, x (e), x (e), x (e, x (e), x (e, x (e), x (e), x (x (e), x (e), x (e, x (e), x (e, x (e), x (x (e), x (x (e), x (e), x (e, x (x), x (x (e), x (x (e), x (e), x (x (e), x (x (e), x (x (x (x), e), x (x), x (x (x (x (x (x), e), e), x (x (x (x (x (x), e), e), x (x ("}, {"heading": "2.4 IMPLEMENTATION DETAILS", "text": "We implemented the general algorithm shown in Fig. 2 in C + + as a library for Linux, Windows, and OS X platforms. It supports the use of the GPU for forward and reverse gears, both with standard linear algebra packages and with manually tuned GPU cores. In addition to the standard gradient descent, we allow the specification of mini-batches, moments, and different regularizers such as the 2-standard and \u221e-standard. Between iterations, the increment can be reduced either on the basis of the negative log probability or the specified validation performance. Unlike the available deep learning packages, our function F is specified using a general calculation tree. Therefore, we support an arbitrarily nested function structure consisting of data, parameters, and function prototypes (conversion, affination aka fully connected, drop-out, local response normalization, pooling, max, and fictified units)."}, {"heading": "3 EXPERIMENTAL EVALUATION", "text": "We demonstrate the performance of our model on two tasks: text recognition and image classification. We examine four strategies to learn the model parameters. \"Unary only\" refers to the training of only simple classifiers and ignores the structure of the graphical model, i.e. paired weights are equal to 0. \"JointTrain\" randomly initializes all weights and trains them together. \"PwTrain\" uses piece-by-piece training by first training the simple potentials and then fixing them when learning the paired potentials. \"PreTrainJoint\" pre-trains the unaries, but in a second step optimizes paired weights and simple weights together."}, {"heading": "3.1 WORD RECOGNITION: WORD50", "text": "We are the first ones that we are able to put ourselves in the world, in which we are able to put ourselves in the world, in which we are able to put ourselves in the world, in which we are able to put ourselves in the world, in which we live in the world, in which we live, in which we live, in the world in which we live, in the world in which we live, in the world in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in the world, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which"}, {"heading": "3.2 IMAGE CLASSIFICATION: FLICKR", "text": "We have achieved the importance of blend learning and the conclusion in relation to this goal, we have the use of the Flickr datasets, which consist of 10, 000 training sessions and 10, 000 test images from Flickr. The task is to predict which of 38 possible brand names should be associated with each image. The graphical model has 38 random variables, each indicating the presence / absence of a particular brand name. We define the non-linear patterns for the 8-layer network architecture of Krizhevsky et al. (2013)"}, {"heading": "4 DISCUSSION & CONCLUSION", "text": "In fact, it is the case that most of them are able to survive themselves if they are unable to abide by the rules. (...) Most of them are able to put themselves in a position. (...) Most of them are not able to put themselves in a position. (...) Most of them are not able to put themselves in a position. (...) Most of them are not able to put themselves in a position to put themselves in a position. (...) Most of them are not able to put themselves in a position. (...) Most of them are not able to put themselves in a position. (...) Most of them are not able to put themselves in a position to put themselves in a position. (...) Most of them are not able to put themselves in a position to put themselves in a position. (...) Most of them are not able to be able to be able to move. (...)"}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank NVIDIA Corporation for donating GPUs used in this research, which was partially funded by ONR-N00014-14-1-0232."}], "references": [{"title": "Greedy Layer-Wise Training of Deep Networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In Proc. NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. Bengio", "Y. LeCun"], "venue": "In Proc. CVPR,", "citeRegEx": "Bottou et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 1997}, {"title": "Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters", "author": ["J.S. Bridle"], "venue": "In Proc. NIPS,", "citeRegEx": "Bridle,? \\Q1990\\E", "shortCiteRegEx": "Bridle", "year": 1990}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Character recognition in natural images", "author": ["T.E. de Campos", "B.R. Babu", "M. Varma"], "venue": "In Proc. VISAPP,", "citeRegEx": "Campos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Campos et al\\.", "year": 2009}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": "In Proc. CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Large-Scale Object Classification using Label Relation Graphs", "author": ["J. Deng", "N. Ding", "Y. Jia", "A. Frome", "K. Murphy", "S. Bengio", "Y. Li", "H. Neven", "H. Adam"], "venue": "In Proc. ECCV,", "citeRegEx": "Deng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2014}, {"title": "Neural conditional random fields", "author": ["Do", "T.-M.-T", "T. Artieres"], "venue": "In Proc. AISTATS,", "citeRegEx": "Do et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Do et al\\.", "year": 2010}, {"title": "Structured Learning via Logistic Regression", "author": ["J. Domke"], "venue": "In Proc. NIPS,", "citeRegEx": "Domke,? \\Q2013\\E", "shortCiteRegEx": "Domke", "year": 2013}, {"title": "Understanding Deep Architectures using a Recursive Convolutional Network", "author": ["D. Eigen", "J. Rolfe", "R. Fergus", "Y. LeCun"], "venue": "In Proc. ICLR,", "citeRegEx": "Eigen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eigen et al\\.", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proc. CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "In Proc. ECCV,", "citeRegEx": "Hariharan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2014}, {"title": "A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction", "author": ["T. Hazan", "R. Urtasun"], "venue": "In Proc. NIPS,", "citeRegEx": "Hazan and Urtasun,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Urtasun", "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Boltzmann Machines: Constraint Satisfaction Networks that Learn", "author": ["G.E. Hinton", "T.J. Sejnowski", "D.H. Ackley"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Hinton et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1984}, {"title": "Caffe: An Open Source Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia"], "venue": "http://caffe. berkeleyvision.org/,", "citeRegEx": "Jia,? \\Q2013\\E", "shortCiteRegEx": "Jia", "year": 2013}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proc. NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2013}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proc. ICML,", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proc. ICML,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "High Order Regularization for Semi-Supervised Learning of Structured Output Problems", "author": ["Y. Li", "R. Zemel"], "venue": "In Proc. ICML,", "citeRegEx": "Li and Zemel,? \\Q2014\\E", "shortCiteRegEx": "Li and Zemel", "year": 2014}, {"title": "A conditional neural fields model for protein", "author": ["J. Ma", "J. Peng", "S. Wang", "J. Xu"], "venue": "threading. Bioinformatics,", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "Convergent Message Passing Algorithms: a unifying view", "author": ["T. Meltzer", "A. Globerson", "Y. Weiss"], "venue": "In Proc. UAI,", "citeRegEx": "Meltzer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meltzer et al\\.", "year": 2009}, {"title": "Learning Efficiently with Approximate inference via Dual Losses", "author": ["O. Meshi", "D. Sontag", "T. Jaakkola", "A. Globerson"], "venue": "In Proc. ICML,", "citeRegEx": "Meshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2010}, {"title": "Conditional random fields for integrating local discriminative classifiers", "author": ["J. Morris", "E. Fosler-Lussier"], "venue": "IEEE Trans. Audio, Speech, and Language Processing,", "citeRegEx": "Morris and Fosler.Lussier,? \\Q2008\\E", "shortCiteRegEx": "Morris and Fosler.Lussier", "year": 2008}, {"title": "Decision tree fields", "author": ["S. Nowozin", "C. Rother", "S. Bagon", "T. Sharp", "B. Yao", "P. Kohli"], "venue": "In Proc. ICCV,", "citeRegEx": "Nowozin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2011}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Backpropagation training for multilayer conditional random field based phone recognition", "author": ["R. Prabhavalkar", "E. Fosler-Lussier"], "venue": "In Proc. ICASSP,", "citeRegEx": "Prabhavalkar and Fosler.Lussier,? \\Q2010\\E", "shortCiteRegEx": "Prabhavalkar and Fosler.Lussier", "year": 2010}, {"title": "An Efficient Learning Procedure for Deep Boltzmann Machines", "author": ["R.R. Salakhutdinov", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2012}, {"title": "Inference and Learning Algorithms with Applications to 3D Indoor Scene Understanding", "author": ["A.G. Schwing"], "venue": "PhD thesis, ETH Zurich,", "citeRegEx": "Schwing,? \\Q2013\\E", "shortCiteRegEx": "Schwing", "year": 2013}, {"title": "Convolutional-Recursive Deep Learning for 3D Object Classification", "author": ["R. Socher", "B. Huval", "B. Bhat", "C.D. Manning", "A.Y. Ng"], "venue": "In Proc. NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation", "author": ["J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "In Proc. NIPS,", "citeRegEx": "Tompson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tompson et al\\.", "year": 2014}, {"title": "Graphical Models, Exponential Families and Variational Inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "A new class of upper bounds on the log partition function", "author": ["M.J. Wainwright", "T. Jaakkola", "A.S. Willsky"], "venue": null, "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies", "author": ["Y. Weiss", "C. Yanover", "T. Meltzer"], "venue": "In Proc. UAI,", "citeRegEx": "Weiss et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2007}, {"title": "Fractional belief propagation", "author": ["W. Wiegerinck", "T. Heskes"], "venue": "In Proc. NIPS,", "citeRegEx": "Wiegerinck and Heskes,? \\Q2003\\E", "shortCiteRegEx": "Wiegerinck and Heskes", "year": 2003}, {"title": "Tell me what you see and I will show you where it is", "author": ["J. Xu", "A.G. Schwing", "R. Urtasun"], "venue": "In Proc. CVPR,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Trans. Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In Proc. ECCV,", "citeRegEx": "Zeiler and Fergus,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "A multiplicity of variants have been proposed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al.", "startOffset": 46, "endOffset": 191}, {"referenceID": 19, "context": "A multiplicity of variants have been proposed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al.", "startOffset": 46, "endOffset": 191}, {"referenceID": 0, "context": "A multiplicity of variants have been proposed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al.", "startOffset": 46, "endOffset": 191}, {"referenceID": 20, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 31, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 15, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 17, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 9, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 17, "context": "Recently, state-of-the-art results have been achieved in many computer vision tasks, outperforming competitive methods by a large margin (Krizhevsky et al., 2013; Girshick et al., 2014).", "startOffset": 137, "endOffset": 185}, {"referenceID": 10, "context": "Recently, state-of-the-art results have been achieved in many computer vision tasks, outperforming competitive methods by a large margin (Krizhevsky et al., 2013; Girshick et al., 2014).", "startOffset": 137, "endOffset": 185}, {"referenceID": 32, "context": "In pose estimation, more accurate predictions can be obtained when encoding the spatial relationships between joint locations (Tompson et al., 2014).", "startOffset": 126, "endOffset": 148}, {"referenceID": 26, "context": "Existing approaches often rely on a two-step process (Nowozin et al., 2011; Xu et al., 2014) where a non-linear classifier that employs deep features is trained first, and its output is used to generate potentials for the structured predictor.", "startOffset": 53, "endOffset": 92}, {"referenceID": 37, "context": "Existing approaches often rely on a two-step process (Nowozin et al., 2011; Xu et al., 2014) where a non-linear classifier that employs deep features is trained first, and its output is used to generate potentials for the structured predictor.", "startOffset": 53, "endOffset": 92}, {"referenceID": 0, "context": ", 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014). Recently, state-of-the-art results have been achieved in many computer vision tasks, outperforming competitive methods by a large margin (Krizhevsky et al., 2013; Girshick et al., 2014). Deep neural networks can, however, be even more powerful when combined with graphical models in order to capture the statistical dependencies between the variables of interest. For example, Deng et al. (2014) exploit mutual exclusion, overlapping and subsumption properties of class labels in order to better predict in large scale classification tasks.", "startOffset": 38, "endOffset": 762}, {"referenceID": 11, "context": ", independently learned segmentation and detection features (Hariharan et al., 2014) might be focusing on predicting the same examples correctly.", "startOffset": 60, "endOffset": 84}, {"referenceID": 27, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 38, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 34, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 35, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 23, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 30, "context": "We refer the reader to Schwing (2013) for a derivation in the log-linear setting.", "startOffset": 23, "endOffset": 38}, {"referenceID": 4, "context": "We then generated writing variations of each word as follows: we took the lower case characters from the Chars74K dataset (de Campos et al., 2009), and inserted them in random background image patches (similar to Larochelle et al. (2007)) by alpha matting, i.", "startOffset": 126, "endOffset": 238}, {"referenceID": 17, "context": "We define the non-linear unaries fr(x, yi;wu) using the 8-layer deep-net architecture from Krizhevsky et al. (2013) followed by a 76-dimensional top layer.", "startOffset": 91, "endOffset": 116}, {"referenceID": 5, "context": "We initialized the deep-net parameters using a model pre-trained on ImageNet (Deng et al., 2009).", "startOffset": 77, "endOffset": 96}, {"referenceID": 3, "context": "Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities.", "startOffset": 14, "endOffset": 163}, {"referenceID": 22, "context": "Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities.", "startOffset": 14, "endOffset": 163}, {"referenceID": 24, "context": "The blending strategy, which was previously employed for learning log-linear models by (Meshi et al., 2010; Hazan & Urtasun, 2010), amounts to converting the maximization task into a minimization problem using its dual.", "startOffset": 87, "endOffset": 130}, {"referenceID": 1, "context": "One of the earliest works by Bridle (1990) jointly optimizes a system consisting of multilayer perceptrons and hidden Markov models for speech recognition.", "startOffset": 29, "endOffset": 43}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition.", "startOffset": 33, "endOffset": 54}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing = 0.", "startOffset": 33, "endOffset": 738}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing = 0. More recently, Li & Zemel (2014) use a hinge loss to learn the unary term defined as a neural net, but keep the pairwise potentials fixed (i.", "startOffset": 33, "endOffset": 878}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing = 0. More recently, Li & Zemel (2014) use a hinge loss to learn the unary term defined as a neural net, but keep the pairwise potentials fixed (i.e., no joint training). Domke (2013) considers non-linear structured prediction and decomposes the learning problem into a subset of logistic regressors, which require the parameter updates to be run till convergence before updating the messages.", "startOffset": 33, "endOffset": 1023}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing = 0. More recently, Li & Zemel (2014) use a hinge loss to learn the unary term defined as a neural net, but keep the pairwise potentials fixed (i.e., no joint training). Domke (2013) considers non-linear structured prediction and decomposes the learning problem into a subset of logistic regressors, which require the parameter updates to be run till convergence before updating the messages. Tompson et al. (2014) also jointly train convolutional neural networks and a graphical model for pose estimation.", "startOffset": 33, "endOffset": 1255}], "year": 2015, "abstractText": "Many problems in real-world applications involve predicting several random variables which are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such relationships. The goal of this paper is to combine MRFs with deep learning algorithms to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as multi-class classification of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.", "creator": "LaTeX with hyperref package"}}}