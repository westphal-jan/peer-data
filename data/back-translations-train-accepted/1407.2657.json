{"id": "1407.2657", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2014", "title": "Beyond Disagreement-Based Agnostic Active Learning", "abstract": "We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are {\\em{disagreement-based active learning}}, which has a high label requirement, and {\\em{margin-based active learning}}, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems.", "histories": [["v1", "Thu, 10 Jul 2014 00:34:16 GMT  (26kb)", "https://arxiv.org/abs/1407.2657v1", null], ["v2", "Fri, 11 Jul 2014 23:35:49 GMT  (26kb)", "http://arxiv.org/abs/1407.2657v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["chicheng zhang", "kamalika chaudhuri"], "accepted": true, "id": "1407.2657"}, "pdf": {"name": "1407.2657.pdf", "metadata": {"source": "CRF", "title": "Beyond Disagreement-based Agnostic Active Learning", "authors": ["Chicheng Zhang"], "emails": ["chz038@eng.ucsd.edu", "kamalika@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 7,26 57v2 [cs.LWe study agnostic active learning, where the goal is to learn a classifier in a given hypotheses class interactively with as few label queries as possible, without making assumptions about the true function that the labels produce. Key algorithms for this problem are non-consensual active learning, which has a high label requirement, and margin-based active learning, which applies only to relatively limited environments. A major challenge is to find an algorithm that achieves better label complexity, is consistent in an agnostic environment, and applies to general classification problems.In this paper, we provide such an algorithm. Our solution is based on two novel contributions - a reduction of consistent active learning to trust-rated prediction with guaranteed errors and a novel confidence-rating predictor."}, {"heading": "1 Introduction", "text": "This year, it's so far that it's going to be able to put itself at the top, \"he said.\" It's not so far yet that we're going to be able to do what we need to do, \"he said.\" But it's too early to do it, \"he said."}, {"heading": "2 Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The Setting", "text": "We study active learning for binary classification (h).Examples belong to an instance space X, and their labels lie in a uniform space Y = {\u2212 1}; examples are derived from an underlying data distribution D to X \u00b7 Y. We use DX to denote the marginal distribution from D to X, and DY | X to denote the conditional distribution from Y to X. Our algorithm has access to examples through two oracles - an example oracle U, which defines an unmarked example X to X, and a laboracle O, which returns the label of an input X to X-X-X. Given a class H hypothesis of the VC dimensions d, the error of a data distribution via X \u00d7 Y is defined as error (h) = P (x, y)."}, {"heading": "2.2 Main Algorithm", "text": "Our active learning algorithm runs in the epochs in which the target of the epoch k = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K + 1, by querying a new set of examples whose labels should be questioned. We make this decision using a confidence rating predictor P = P with candidate hypothesis set V = Vk and error guarantee. Whenever P abstains, we ask the label of the example. The number of labels mk queried is set so that it is sufficient to achieve an excess generalization error."}, {"heading": "2.3 Confidence-Rated Predictor", "text": "Our active learning algorithm uses a confidence-rated predictor with guaranteed errors to make its label query decisions. In this section, we provide a new type of confidence-rated predictor with guaranteed errors. This predictor has optimal coverage in the feasible case and may be of independent interest. The predictor P receives as input a quantity V H of hypotheses (which probably contain the true risk minimizer), a guarantee of error, and a set of U of unspecified examples. We consider a soft prediction algorithm to be confidence; for each example in U, the predictor P gives three probabilities that seem to add up to 1 - the probability of predicting 1, \u2212 1, and 0. This output is limited to the expected discrepancy between the \u00b1 1 designated examples 3, where the expectation of the random decisions made by Passigned P and assigned by the random decisions is made."}, {"heading": "3 Performance Guarantees", "text": "An essential characteristic of any active learning algorithm is consistency - that it adapts to the actual risk minimization (sufficiently shown examples). We observe that our algorithm is consistent, provided we use any confidence-rated predictor P with guaranteed errors as a subroutine. (The consistency of our algorithm is a sequence of lemmas 3 and 6 and is shown in Theorem 3 (consistency). (Suppose we run algorithm 1 with input example oracle U, labeling oracle O, consistency class H, consistency class P, target surplus error and target consistency. (Then with probability) is the classifier h returned by algorithm 1. \u2212 errD (h) \u2212 errD (h). (h). (D). We are now establishing a label complexity tied to our algorithms."}, {"heading": "3.1 Tsybakov Noise Conditions", "text": "An important part of learning from sound data is learning under the Tsybakov noise conditions [Tsy04]. Definition 1. (Tsybakov noise state) Let us know about other factors. \u2212 Definition 1. (Tsybakov noise state) Let us know more about it. \u2212 Definition 1. (Tsybakov noise state) Let us know more about it. \u2212 Definition 1. (Tsybakov noise state) Let us know more (C0 > 0 if for all h noise conditions). \u2212 Definition 5. (C0, h) -Tsybakov noise state. (errD) \u2212 errD (h noise state. (D)) 1. Definition shows the performance guarantees achieved by algorithm 1 under the Tsybakov noise conditions. \u2212 Theorem 5. Definition (C0, h noise state) \u2212 Tsybakov (D). \u2212 Noise state. \u2212 (D) \u2212 (D). \u2212 Noise state."}, {"heading": "3.2 Case Study: Linear Classification under the Log-concave Distribution", "text": "We now consider linear classifiers in relation to the log concave data distribution on Rd > constant. (ln) In this case, the discrepancy coefficient 1 (r) \u2264 O (n) depends, which is much smaller than it is. (n) This leads to the following label complexity limit 1 (r, p) r \u2264 O (ln (r / p))) (see Lemma 14 in the appendix), which is much smaller as long as it / r is not too small.) This leads to the following label complexity limit. (1) Supplement DX is isotropic and log concavity on R d, and H is the set of homogeneous linear classifiers on Rd. Then algorithm 1 with input example oracel U, label oracel O, hypothesis class H, trust predictor P of algorithm 3, target exceedance error and target reliability satisfies the following properties."}, {"heading": "4 Related Work", "text": "In fact, we are able to assert ourselves, we are able to adapt, we are able to adapt to the needs of the people and we are able to adapt to the needs of the people."}, {"heading": "A Additional Notation and Concentration Lemmas", "text": "We start with an additional notation used in the following proofs. Remember that k (k) for all k (n) for all k (n) for all k (n) for all k (n) for all k (n) for all k (n) for all k (n), for all k (n), for all k (n), for all k (n), for all k (n), for all k (n), for all k (n), for all k (n), for all k (n), for all k (n), for all k (n), for all h (n), for all h (n), for all h (n), for all h (n), for all h (n), for all h (n), for all h (n), for all h (h), for all h (n)."}, {"heading": "B Proofs related to the properties of Algorithm 2", "text": "We first determine some properties of algorithm 2. \u2212 jjh The inputs to algorithm 2 are a set V of hypotheses of dimension d, an example distribution, a target surplus, and a target trust. \u2212 jh We define eventE 2 (for all j = 1, 2,..: equations (4) - (7) hold for example Sj with n = nj and 0 (for all). \u2212 jh (for all j = 1, 2,.): equations (of Lemma 4). \u2212 jh The proof for (1), define jmax as the smallest integer j (for all). \u2212 jmax We define jmax 0 (for all). \u2212 jmax (for all). \u2212 jmax (for all)."}, {"heading": "C Remaining Proofs from Section 2", "text": "Proof. (Of Lemm h) Proof. (Of Lemm h) Proof. (Of Lemm h) Proof. (Of Lemm h) Proof. (For k = EU) Proof. (For k = EU) Proof. (For k = EU) Proof. (For k = EU) Proof. (For k = EU) Proof. (For k = 1, clearly h (D) Proof. (D) Proof. (Of Lemma 2) Proof. (Of Lemma 2) Proof. (D) Proof. (EU) Proof. (EU) Proof. (EU) Proof. (EU) Proof. (D) Proof. (D) Proof. (EU) Proof. (D) Proof. (EU) Proof. (D) Proof. (EU) Proof. (D) (We.) (We.) (D) Proof. (Of Lemm h) Proof. (Of Lemm h) Proof. (Of Lemm h) Proof. (Of EU) Proof. (Of Lemm h) Proof. (For k = EU) Proof. (For k = EU) Proof. (For k = EU) Proof. (For k = EU) Proof. (D) Proof."}, {"heading": "D Proofs from Section 3", "text": "Proof. (of Theorem 4) (1) In the realizable case, it is assumed that the event Er (1) happens. (From Equation (15) of Lemma 10, while we execute algorithm 3, it follows that the second inequality follows from the fact that Vk (h) BD (h) BD (h), and the third inequality follows from Lemma 18 and denseness assumption. (Thus, there is c3 > 0 so that in round k, mk = (d) the second inequality follows from the fact that the second inequality follows from the fact that Vk (h) BD (h), and the third inequality follows from Lemma 18 and denseness assumption."}, {"heading": "E A Suboptimal Alternative to Algorithm 2", "text": "4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 2: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: 4: 4: 4: algorithm 4: algorithm 4: 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4: algorithm 4:"}, {"heading": "F Proofs of Concentration Lemmas", "text": "Proof. (from Lemma 9) We start with the observation that: errU k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c c = c = c = c = c = c = c = c = c = c c = c c c = c c c = c c = c c = c c = c c = c c = c = c = c c = c = c = c = c = c = c = c = c = c = c, c = c = c = c = c, c = c = c = c = c, c = c = c = c, c = c = c = c, c = c = c = c, c = c = c = c = c = c, c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c"}, {"heading": "G Detailed Derivation of Label Complexity Bounds", "text": "G.1 AgnosticProposition 1 (k) k (k) k k k k k k k k k (k) k) k (k) k) k (k) k (k) k) k (k) k) k (k) k) k (k) k (k) k) k (k) k (k) k (k) k (k) k (c) k (c) k (c) k (c) k) k (c) k) k (k) k) k (c) k (k) k (c) k) k (k) k) k (k) k (k) k (c) k (c) k (c) k (k) k) k (k) k (c) k) k (k) k) k (k) k) k k k k k k k k k k k k k k k k k k k k k k k k k k k (k) k c) k (c) k) k (c) k) k) k (k) k) k) k) k (c) k) k) k) k (k) k) k) k (c) k) k (c) k) k (c) k) k (c) k (k) k) k (k) k) k (k) k) k) k) k) k (c) k) k) k (k) k) k) k) k) k (c) k) k) k (k) k) k) k) k (k) k) k) k (k) k) k (k) k) k (k) k) k) k (k) k) k (k) k) k (k) k) k) k (k) k) k (k) k) k (k) k) k (k) k k) k k) k k (k) k) k k k k (k) k k k k k) k k k k k k (k) k (k) k) k k (k) k k k k) k k k k k k) k k k k k k (k) k) k k k k (k) k) k k k k k k) k) k k k k (k) k) k k k"}], "references": [{"title": "The power of localization for efficiently learning linear separators with noise", "author": ["P. Awasthi", "M-F. Balcan", "P.M. Long"], "venue": "In STOC,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Balcan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2009}, {"title": "Margin based active learning", "author": ["M.-F. Balcan", "A.Z. Broder", "T. Zhang"], "venue": "In COLT,", "citeRegEx": "Balcan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2007}, {"title": "Importance weighted active learning", "author": ["A. Beygelzimer", "S. Dasgupta", "J. Langford"], "venue": "In ICML,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Agnostic active learning without constraints", "author": ["A. Beygelzimer", "D. Hsu", "J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2010}, {"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["M.-F. Balcan", "P.M. Long"], "venue": "In COLT,", "citeRegEx": "Balcan and Long.,? \\Q2013\\E", "shortCiteRegEx": "Balcan and Long.", "year": 2013}, {"title": "Improving generalization with active learning", "author": ["D.A. Cohn", "L.E. Atlas", "R.E. Ladner"], "venue": "Machine Learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "Dasgupta.,? \\Q2004\\E", "shortCiteRegEx": "Dasgupta.", "year": 2004}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "Dasgupta.,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta.", "year": 2005}, {"title": "Two faces of active learning", "author": ["S. Dasgupta"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Dasgupta.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta.", "year": 2011}, {"title": "Hierarchical sampling for active learning", "author": ["S. Dasgupta", "D. Hsu"], "venue": "In ICML,", "citeRegEx": "Dasgupta and Hsu.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Hsu.", "year": 2008}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In NIPS,", "citeRegEx": "Dasgupta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2007}, {"title": "On the foundations of noise-free selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": null, "citeRegEx": "El.Yaniv and Wiener.,? \\Q2010\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2010}, {"title": "Agnostic selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "In NIPS,", "citeRegEx": "El.Yaniv and Wiener.,? \\Q2011\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2011}, {"title": "Active learning via perfect selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": null, "citeRegEx": "El.Yaniv and Wiener.,? \\Q2012\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2012}, {"title": "Generalization bounds for averaged classifiers", "author": ["Y. Freund", "Y. Mansour", "R.E. Schapire"], "venue": "The Ann. of Stat.,", "citeRegEx": "Freund et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2004}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "In ICML,", "citeRegEx": "Hanneke.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke.", "year": 2007}, {"title": "Adaptive rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "In COLT,", "citeRegEx": "Hanneke.,? \\Q2009\\E", "shortCiteRegEx": "Hanneke.", "year": 2009}, {"title": "A statistical theory of active learning", "author": ["S. Hanneke"], "venue": null, "citeRegEx": "Hanneke.,? \\Q2013\\E", "shortCiteRegEx": "Hanneke.", "year": 2013}, {"title": "Algorithms for Active Learning", "author": ["D. Hsu"], "venue": "PhD thesis, UC San Diego,", "citeRegEx": "Hsu.,? \\Q2010\\E", "shortCiteRegEx": "Hsu.", "year": 2010}, {"title": "Surrogate losses in passive and active learning", "author": ["S. Hanneke", "L. Yang"], "venue": "CoRR, abs/1207.3772,", "citeRegEx": "Hanneke and Yang.,? \\Q2012\\E", "shortCiteRegEx": "Hanneke and Yang.", "year": 2012}, {"title": "Active learning in the non-realizable case", "author": ["M. K\u00e4\u00e4ri\u00e4inen"], "venue": "In ALT,", "citeRegEx": "K\u00e4\u00e4ri\u00e4inen.,? \\Q2006\\E", "shortCiteRegEx": "K\u00e4\u00e4ri\u00e4inen.", "year": 2006}, {"title": "Rademacher complexities and bounding the excess risk in active learning", "author": ["V. Koltchinskii"], "venue": null, "citeRegEx": "Koltchinskii.,? \\Q2010\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2010}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["L. Li", "M.L. Littman", "T.J. Walsh"], "venue": "In ICML,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Noisy bayesian active learning", "author": ["M. Naghshvar", "T. Javidi", "K. Chaudhuri"], "venue": "In Allerton,", "citeRegEx": "Naghshvar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Naghshvar et al\\.", "year": 2013}, {"title": "The geometry of generalized binary search", "author": ["R.D. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nowak.,? \\Q2011\\E", "shortCiteRegEx": "Nowak.", "year": 2011}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "Technical report, University of Wisconsin-Madison,", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "A tutorial on conformal prediction", "author": ["G. Shafer", "V. Vovk"], "venue": null, "citeRegEx": "Shafer and Vovk.,? \\Q2008\\E", "shortCiteRegEx": "Shafer and Vovk.", "year": 2008}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["A.B. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "Tsybakov.,? \\Q2004\\E", "shortCiteRegEx": "Tsybakov.", "year": 2004}, {"title": "Plal: Cluster-based active learning", "author": ["R. Urner", "S. Wulff", "S. Ben-David"], "venue": "In COLT,", "citeRegEx": "Urner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urner et al\\.", "year": 2013}], "referenceMentions": [], "year": 2014, "abstractText": "We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are disagreement-based active learning, which has a high label requirement, and margin-based active learning, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions \u2013 a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor.", "creator": "LaTeX with hyperref package"}}}