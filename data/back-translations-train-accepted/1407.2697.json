{"id": "1407.2697", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2014", "title": "A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation", "abstract": "A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lov\\'asz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.", "histories": [["v1", "Thu, 10 Jul 2014 05:45:17 GMT  (287kb,D)", "http://arxiv.org/abs/1407.2697v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["aaron defazio", "tib\u00e9rio s caetano"], "accepted": true, "id": "1407.2697"}, "pdf": {"name": "1407.2697.pdf", "metadata": {"source": "CRF", "title": "A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation", "authors": ["Aaron J. Defazio", "Tiberio S. Caetano"], "emails": ["aaron.defazio@anu.edu.au", "tiberio.caetano@nicta.com.au"], "sections": [{"heading": null, "text": "IntroductionStructure learning for graphical models is a problem that comes in many contexts. In applied statistics, undirected graphical models can be used as a tool to understand the underlying conditional independence relationships between variables in a dataset. For example, in bioinfomatics, Gaussian graphical models are adapted to data resulting from micro-array experiments, where the adapted graph can be interpreted as a gene expression network [9]. In the context of Gaussian models, the structural learning problem is known as covariance selection [8]. The most common approach is the application of thrift, which leads regulation to the goal of maximum probability. There is a significant body of literature, more than 30 papers, on various methods for optimizing L1 regulated covariance selection (see the recent review by Scheinberg and Ma [17])."}, {"heading": "1 Combinatorial Objective", "text": "Consider an undirected graph with edge set E and node set V, where n > is the number of selected nodes. We refer to the degree of node v as dE (v) and the complete graph with n nodes asKn. We deal with the placement of priors on the degree distributions of graphs such as (V, E). By degree distribution, however, we mean the bag of degrees {dE (v) | v \u00b2 V}. A natural prior degree distribution can be formed from the family of exponential random functions [21]. Exponential random diagrams (ERG) we assign a probability to each n-node graph by using an exponential family model. The probability of each graph depends on a small set of sufficient statistics, in our case we consider only the degree statistics. An ERG distribution with degree parameterization takes the form: (p, V)."}, {"heading": "2 Submodularity", "text": "A specified function F: 2E \u2192 R on E is a non-decreasing submodular function if the following conditions apply to all A-B-E and x-E-B: F (A-Z) \u2212 F (A) \u2265 F (B-Z) \u2212 F (B) (submodularity) and F (A) \u2264 F (B). (non-decreasing) The first condition can be interpreted as a decreasing yield condition; adding x to a setA increases F (A) by more than adding it to a larger group B if B now contains a set of conditions that can be set to h, so that F is submodular. Denote as a tractable condition if h is not decreasing, concave, and h (0) = 0. For tractable h, F is a non-decreasing submodular function."}, {"heading": "3 Optimization", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "3.1 Alternating direction method of multipliers", "text": "The alternating direction method of the multipliers (ADMM, Boyd et al. [6]) is an approach to optimizing our goal, which has a number of advantages over the basic proximal method. Let U be the matrix of dual variables for the decoupled problem: minimizeX g (X) + \u03b1\u0432 (Y), s.t. X = Y.Following the presentation of the algorithm in Boyd et al. [6] the values Y (l) and U (l) from the iteration l, with U (0) = 0n and Y (0) = In the ADMM updates for the iteration l + 1 show: X (l + 1) = arg min X [< X, C > \u2212 log detX + 2 | X \u2212 Y (l) (l) + U (l), that Qm (l) + 1)."}, {"heading": "3.2 Proximal operator using dual decomposition", "text": "The only difference is that we have made the symmetry constraint explicit, where the dual formulation is treated as separate variables; the dual variable matrix V corresponds to the Lagrange multipliers of the symmetry configuration that we store; the dual matrix V corresponds to the Lagrange multipliers of the symmetry configuration that we store in an antisymmetric matrix; the dual matrix V corresponds to the Lagrange multipliers of the symmetry configuration that we store in an antisymmetric matrix; and the dual matrix V corresponds to the Lagrange multipliers of the symmetry configuration that we store in an antisymmetric matrix."}, {"heading": "4 Alternative degree priors", "text": "Among the constraints on h set out in Proposal 1, several other choices seem reasonable; the previous scale-free approach may be somewhat smoothed out by the addition of a linear term, Givingh, \u03b2 (i) = log (i +) + \u03b2i, algorithm 2 Dual decomposition subproblem (solveSubproblem). Input: Initialize vectors z, v: Disjoint-set datastructure with set membership function \u03b3 w = z \u2212 v # w yields the sort order u = 0n build: sorted-to-original position function \u00b5 under descending absolute value order of w, with the diagonals for k = 0 to n \u2212 1 do j = \u00b5 (k) uj = | \u2212 pending (h + 1) \u2212 h (k) \u2212 h (j)).value = uj r = k, while r > 1 and \u03b3 (r) ltg (r)).value dojoin: the sets featuring it (r).it (ERD) (ERD) is the first one (ERD)."}, {"heading": "5 Related Work", "text": "The problem of covariance selection has recently been addressed by Liu and Ihler [14] using a reweighted L1 regularization, which minimizes the following goal: f (X) = < X, C > \u2212 log detX + \u03b1 \u2211 v \u00b2 V log + \u03b2 \u00b2 v | Xvv |. The regularizer is split into an off-diagonal term designed to promote edge parameter economy and a more traditional diagonal term. Essentially, they use \"X \u00b2 v \u00b2\" as a continuous counterpart to the degree of node V. The main difficulty in this goal is the log term, which makes f highly non-convex. This is in contrast to our approach, where we essentially start with the same combinatorial precursor, but use an alternative, convex relaxation.The reweighted L1 [7] aspect refers to the method of optimization applied."}, {"heading": "6 Experiments", "text": "In fact, most of them are only a very small group that is able to move."}, {"heading": "Acknowledgements", "text": "NICTA is funded by the Australian Government, represented by the Department of Broadband, Communications and the Digital Economy, and the Australian Research Council through the ICT Centre of Excellence programme."}], "references": [{"title": "Convex analysis and optimization with submodular functions: a tutorial", "author": ["Francis Bach"], "venue": "Technical report,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Structured sparsity-inducing norms through submodular functions", "author": ["Francis Bach"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Optimization with sparsityinducing penalties", "author": ["Francis Bach", "Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Emergence of scaling in random networks", "author": ["Albert-Laszlo Barabasi", "Reka Albert"], "venue": "Science, 286:509\u2013", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "A sequential algorithm for generating random graphs", "author": ["Moshen Bayati", "Jeong Han Kim", "Amin Saberi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, 3", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Enhancing sparsity by reweighted l1 minimization", "author": ["Emmanuel J. Candes", "Michael B. Wakin", "Stephen P. Boyd"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Covariance selection", "author": ["A.P. Dempster"], "venue": "Biometrics, 28:157\u2013175", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1972}, {"title": "Sparse graphical models for exploring gene expression data", "author": ["Adrian Dobra", "Chris Hans", "Beatrix Jones", "Joseph R Nevins", "Mike West"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski", "Francis Bach"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Structured sparse principal component analysis", "author": ["Rodolphe Jenatton", "Guillaume Obozinski", "Francis Bach"], "venue": "AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Learning scale free networks by reweighted l1 regularization", "author": ["Qiang Liu", "Alexander Ihler"], "venue": "AISTATS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Smooth optimization approach for sparse covariance selection", "author": ["Zhaosong Lu"], "venue": "SIAM J. Optim.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Exploring biological network structure using exponential random graph models", "author": ["Zachary M. Saul", "Vladimir Filkov"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Optimization for Machine Learning, chapter 17. optimization methods for sparse inverse covariance selection", "author": ["Katya Scheinberg", "Shiqian Ma"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Sparse inverse covariance selection via alternating linearization methods", "author": ["Katya Scheinbert", "Shiqian Ma", "Donald Goldfarb"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Markov chain monte carlo estimation of exponential random graph models", "author": ["T. Snijders"], "venue": "Journal of Social Structure", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "New specifications for exponential random graph models", "author": ["Tom A.B. Snijders", "Philippa E. Pattison", "Mark S. Handcock"], "venue": "Technical report, University of Washington,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Exponential random graphs", "author": ["Alan Terry"], "venue": "Master\u2019s thesis, University of York,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 8, "context": "For example, in bioinfomatics Gaussian graphical models are fitted to data resulting from micro-array experiments, where the fitted graph can be interpreted as a gene expression network [9].", "startOffset": 186, "endOffset": 189}, {"referenceID": 7, "context": "In the context of Gaussian models, the structure learning problem is known as covariance selection [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 15, "context": "There is a significant body of literature, more than 30 papers by our count, on various methods of optimizing the L1 regularized covariance selection objective alone (see the recent review by Scheinberg and Ma [17]).", "startOffset": 210, "endOffset": 214}, {"referenceID": 20, "context": "Examples include group sparsity [22], where parameters are linked so that they are regularized in groups.", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "More complex sparsity patterns, such as region shape constraints in the case of pixels in an image [13], or hierarchical constraints [12] have also been explored.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "More complex sparsity patterns, such as region shape constraints in the case of pixels in an image [13], or hierarchical constraints [12] have also been explored.", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Recent work has offered an approach to deal with this problem which results in a non-convex formulation [14].", "startOffset": 104, "endOffset": 108}, {"referenceID": 1, "context": "We show that scale-free networks can be induced by enforcing submodular priors on the network\u2019s degree distribution, and then using their convex envelope (the Lov\u00e1sz extension) as a convex relaxation [2].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "We outline a few options for solving the optimisation problem via proximal operators [3], in particular an efficient dual decomposition method.", "startOffset": 85, "endOffset": 88}, {"referenceID": 12, "context": "Experiments on both synthetic data produced by scale-free network models and a real bioinformatics dataset suggest that the convex relaxation is not weak: we can infer scale-free networks with similar or superior accuracy than in [14].", "startOffset": 230, "endOffset": 234}, {"referenceID": 19, "context": "A natural prior on degree distributions can be formed from the family of exponential random graphs [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "[20] and has been widely adopted.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "A concave transformation of a modular function is submodular [1], and the sum of submodular functions is submodular.", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "As far as we are aware, this is a novel way of mathematically modelling the \u2018preferential attachment\u2019 rule [4] that gives rise to scale-free networks: through non-decreasing submodular functions on the degree distribution.", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "For tractable h, we have by construction that F satisfies the conditions of Proposition 1 in [2], so that the convex envelope of F (Supp(X)) on the L\u221e ball is precisely the Lov\u00e1sz extension evaluated on |X|.", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "The Lov\u00e1sz extension for our function is easy to determine as it is a sum of \u201cfunctions of cardinality\u201d which are considered in [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "Below is the result from [2] adapted to our problem.", "startOffset": 25, "endOffset": 28}, {"referenceID": 13, "context": "In fact X can be shown to be in a strictly smaller cone, X\u2217 aI , for a derivable from C [15].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "This restricted domain is useful as g(X) has Lipschitz continuous gradients over X aI but not over all positive definite matrices [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 1, "context": "Bach [2] suggests two approaches to optimizing functions involving submodular relaxation priors; a subgradient approach and a proximal approach.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "An alternative is the use of proximal methods [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "For non-decreasing submodular relaxations, the proximal operator can be evaluated by solving a submodular minimization on a related (not necessarily non-decreasing) submodular function [2].", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "Bach [2] considers several example problems where the proximal operator can be evaluated using fast graph cut methods.", "startOffset": 5, "endOffset": 8}, {"referenceID": 16, "context": "For solving our optimisation problem, instead of using the standard proximal method (sometimes known as ISTA), which involves a gradient step followed by the proximal operator, we propose to use the alternating direction method of multipliers (ADMM), which has shown good results when applied to the standard L1 regularized covariance selection problem [18].", "startOffset": 353, "endOffset": 357}, {"referenceID": 5, "context": "[6]) is one approach to optimizing our objective that has a number of advantages over the basic proximal method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6], given the values Y (l) and U (l) from iteration l, with U (0) = 0n and Y (0) = In the ADMM updates for iteration l + 1 are: X = arg min X [ \u3008X,C\u3009 \u2212 log detX + \u03c1 2 ||X \u2212 Y (l) + U ||2 ]", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "the X update) actually has a simple solution [18] that can be computed by taking an eigenvalue decompositionQ\u039bQ = \u03c1(Y \u2212U)\u2212C, where \u039b = diag(\u03bb1, .", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "[18] show that convergence is guaranteed if additional cone restrictions are placed on the minimization with respect to X , and small enough step sizes are used.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Approaches based on Markov chain Monte Carlo techniques have been applied widely to ERG models [19] and are therefore applicable to our model.", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "The covariance selection problem has recently been addressed by Liu and Ihler [14] using reweighted L1 regularization.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "The reweighted L1 [7] aspect refers to the method of optimization applied.", "startOffset": 18, "endOffset": 21}, {"referenceID": 12, "context": "We performed a comparison against the reweighted L1 method of Liu and Ihler [14], and a standard L1 regularized method, both implemented using ADMM for optimization.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "Although Liu and Ihler [14] use the glasso [10] method for the inner loop, ADMM will give identical results, and is usually faster [18].", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "Although Liu and Ihler [14] use the glasso [10] method for the inner loop, ADMM will give identical results, and is usually faster [18].", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "Although Liu and Ihler [14] use the glasso [10] method for the inner loop, ADMM will give identical results, and is usually faster [18].", "startOffset": 131, "endOffset": 135}, {"referenceID": 3, "context": "Graphs with 60 nodes were generated using both the Barabasi-Albert model [4] and a predefined degree distribution model sampled using the method from Bayati et al.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "[5] implemented in the NetworkX software package.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "We can see from the plots that our method with the square-root weighting presents results superior to those from Liu and Ihler [14] for these datasets.", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "This is encouraging particularly since our formulation is convex while the one from Liu and Ihler [14] isn\u2019t.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "Many biological networks are conjectured to be scale-free, and additionally ERG modelling techniques are known to produce good results on biological networks [16].", "startOffset": 158, "endOffset": 162}, {"referenceID": 1, "context": "The MNP algorithm is a submodular minimization method that can be adapted for computing the proximal operator [2].", "startOffset": 110, "endOffset": 113}], "year": 2014, "abstractText": "A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lov\u00e1sz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.", "creator": "LaTeX with hyperref package"}}}