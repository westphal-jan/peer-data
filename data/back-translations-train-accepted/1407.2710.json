{"id": "1407.2710", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2014", "title": "Finito: A faster, permutable incremental gradient method for big data problems", "abstract": "Recent advances in optimization theory have shown that smooth strongly convex finite sums can be minimized faster than by treating them as a black box \"batch\" problem. In this work we introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical results showing state of the art performance.", "histories": [["v1", "Thu, 10 Jul 2014 07:01:31 GMT  (570kb,D)", "http://arxiv.org/abs/1407.2710v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["aaron defazio", "justin domke", "tib\u00e9rio s caetano"], "accepted": true, "id": "1407.2710"}, "pdf": {"name": "1407.2710.pdf", "metadata": {"source": "META", "title": "Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems", "authors": ["Aaron J. Defazio", "Tib\u00e9rio S. Caetano", "Justin Domke"], "emails": ["AARON.DEFAZIO@ANU.EDU.AU", "TIBERIO.CAETANO@NICTA.COM.AU", "JUSTIN.DOMKE@NICTA.COM.AU"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "2. Algorithm", "text": "We consider differentiable convex functions of the form f (w) = 1n n \u2211 i = 1 fi (w).We assume that each fi has continuous Lipschitz gradients with constant L and is strongly convex with constant s. If we allow n = 1, practically all smooth, strongly convex problems are contained. Instead, we limit ourselves to problems that meet the big data condition.Big data condition: Functions of the above form meet the big data condition with constant \u03b2 ifn \u2265 \u03b2L sTypical values of \u03b2 are 1-8. In plain language, we consider problems where the data set is in the same order as the conditional number (L / s) of the problem."}, {"heading": "2.1. Additional Notation", "text": "We set the superscripts up with (k) to specify the value of the scripted size in iteration k. We omit the n superscripts with sums, and the subscripts with i with the implication that indexing starts at 1. If we use separate arguments for each fi, we call it \u03c6i. Let us leave? (k) the average?? (k) = 1n? n? n i? (k) i. Our increment length constant, which depends on \u03b2, is called \u03b1. We use bracket notation for point products < \u00b7, \u00b7 >."}, {"heading": "2.2. The Finito algorithm", "text": "We start with a table of the known number (0) i values and a table of the known gradients f \"i (0) i,\" for each i. \"We will update these two tables in the course of the algorithm. However, the step for the iteration k\" is as follows: 1. Update w \"with the step: w\" (k) = \"p\" (k) - 1 \"p\" i \"(k).2.\" The step for the iteration k \"is as follows: 1. Sentence w\" (k) = \"p\" (k) j \"(k) j\" (k) in the table and leave the other variables equal (k) i \"p.\" 4. \"Calculate and store f\" j \"j\" (k) in the table. Our main theoretical result is proof of the convergence of this method."}, {"heading": "3. Randomness is key", "text": "In fact, it is not as if we are emphasising the importance of the peripheral issue."}, {"heading": "4. Proximal variant", "text": "We will now consider composite problems of the formf (w) = 1n \u2211 i fi (w) + \u03bbr (w), where r is convex but not necessarily smooth or strongly convex. Such problems are often solved using proximal algorithms, especially if the proximal operator for r: proxr\u03bb (z) = argminx 12% x \u2212 z \u00b2 2 + \u03bbr (x) has a closed solution. An example would be the use of the L1 regulation. We will now describe the finite update for this setting. First, it should be noted that if we set w in the finite method, it can be interpreted as minimizing the quantity: B (x) = 1n \u2211 i fi (ancii) + 1 n \u2211 i < f \u2032 i (\u03c6i), x \u2212 approximant i (inspi), x \u2212 proximal i (inspi), x \u2212 proximal (inspi), where the convergence (sp) for Finito method can be defined as the minimization of the quantity."}, {"heading": "5. Convergence proof", "text": "We begin by specifying two simple lemmas. All expectations below are about the choice of index j at step k. \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). \u2212 w (1). (1). (1). (1). (1). (1). (1). (1). (2). (1). (2). (1). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2).). (2). (2). (2). (2). (2). (2).).). (2)."}, {"heading": "6. Lower complexity bounds and exploiting problem structure", "text": "The theory for the class of smooth, strongly convex problems with Lipschitz continuous courses under first order optimization methods (known as S1,1s, L) is well developed. These results require the technical condition that the dimensionality of the input space Rm is much greater than the number of iterations that we will perform. For the sake of simplicity, we assume that this is the case in the following discussions. It is known that problems exist in S1,1s, L for which the iteration convergence rate is limited, while this rate is achieved to a small constant factor by several methods, mainly by Nesterov's accelerated descent method (Nesterov 1988, Nesterov 1998)."}, {"heading": "6.1. Oracle class", "text": "We now describe the (stochastic) oracle class FS1,1s, L, n (Rm), for which SAG and Finito are the most naturally fit. Function class: f (w) = 1n \u2211 n = 1 fi (w), with fi S1,1s, L (R m).Oracle: Each query takes a point x Rm and returns j, fj (w) and f \u2212 j (w), with j selected, uniformly at random. Accuracy: Find w so that E (k) \u2212 w 2]. The most important choice made in formulating this definition is to place the random choice in the oracle. This limits the methods that are quite permitted, an alternative case in which the index j is entered into the oracle in addition to x is also interesting. Let us suppose that the method has access to a source of true random indices, we call this class DS1,1s, L (Rm)."}, {"heading": "7. Experiments", "text": "In this section we compare Finito, SAG, SDCA and LBFGS. We only look at problems where the regulator is large enough so that the big data condition holds, as this is the case that our theory supports. In practice, however, our method can be used with smaller step sizes in the more general case, much like SAG. Since we do not exactly know the Lipschitz constant for these problems, the SAG method has been used for a variety of step sizes, with the fastest rate of convergence plotted, and the best step size for SAG is usually not what the theory suggests. Schmidt et al. (2013) propose to use 1L instead of the theoretical rate 116L. For Finito, we find that the use of \u03b1 = 2 is the fastest rate of convergence if the big data condition holds for each \u03b2 > 1. This is the step that our theory proposes, if we do not improve 2. Instead, the reduction to 1 = the improvement in our experiments."}, {"heading": "8. Related work", "text": "Traditional incremental gradient methods (Bertsekas, 2010) have the same form as SGD, but are applied to finite sums. Essentially, they are the non-online analogue of SGD. Applying SGD to strongly convex problems does not lead to linear convergence, and in practice it is slower than the linear convergence methods we discuss in the rest of this paragraph. In addition to the methods that fall under the term \"classical incremental gradient method,\" the methods SAG and MISO (Mairal, 2013) are also related. MISO falls into the class of top limit minimization methods, such as EM and classical gradient descent. MISO is essentially the finite method, but with increments n smaller. When using these larger step sizes, the method is no longer an upper limit minimization method, but rather a fast step method that can be considered as a MISO."}, {"heading": "9. Conclusion", "text": "We have presented a new method for minimizing finite sums of smooth, strongly convex functions when the sum contains a sufficiently large number of terms. In addition, we are developing a theory for the lower limits of complexity of this class and show the empirical performance of our method."}], "references": [{"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey", "author": ["Bertsekas", "Dimitri P"], "venue": "Technical report,", "citeRegEx": "Bertsekas and P.,? \\Q2010\\E", "shortCiteRegEx": "Bertsekas and P.", "year": 2010}, {"title": "Optimization with first-order surrogate functions", "author": ["Mairal", "Julien"], "venue": null, "citeRegEx": "Mairal and Julien.,? \\Q2013\\E", "shortCiteRegEx": "Mairal and Julien.", "year": 2013}, {"title": "Stochastic Optimization: Algorithms and Applications, chapter Convergence Rate of Incremental Subgradient Algorithms", "author": ["Nedic", "Angelia", "Bertsekas", "Dimitri"], "venue": "Kluwer Academic,", "citeRegEx": "Nedic et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nedic et al\\.", "year": 2000}, {"title": "On an approach to the construction of optimal methods of minimization of smooth convex functions", "author": ["Nesterov", "Yu"], "venue": "Ekonomika i Mateaticheskie Metody,", "citeRegEx": "Nesterov and Yu.,? \\Q1988\\E", "shortCiteRegEx": "Nesterov and Yu.", "year": 1988}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Nesterov", "Yu"], "venue": "Technical report, CORE,", "citeRegEx": "Nesterov and Yu.,? \\Q2010\\E", "shortCiteRegEx": "Nesterov and Yu.", "year": 2010}, {"title": "Beneath the valley of the noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences", "author": ["Recht", "Benjamin", "Re", "Christopher"], "venue": "Technical report, University of Wisconsin-Madison,", "citeRegEx": "Recht et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2012}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["Richtarik", "Peter", "Takac", "Martin"], "venue": "Technical report, University of Edinburgh,", "citeRegEx": "Richtarik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Richtarik et al\\.", "year": 2011}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Schmidt", "Mark", "Roux", "Nicolas Le", "Bach", "Francis"], "venue": "Technical report,", "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": null, "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "The recently developed SAG algorithm (Schmidt et al., 2013) has shown that even with this simple form of structure, as long as we have sufficiently many data points we are able to do significantly better than black-box optimization techniques in expectation for smooth strongly convex problems.", "startOffset": 37, "endOffset": 59}, {"referenceID": 7, "context": "Schmidt et al. (2013) suggest using 1 L instead of the theoretical rate 1 16L .", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "A similar trick is suggested by Schmidt et al. (2013) for SAG.", "startOffset": 32, "endOffset": 54}], "year": 2014, "abstractText": "Recent advances in optimization theory have shown that smooth strongly convex finite sums can be minimized faster than by treating them as a black box \u201dbatch\u201d problem. In this work we introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical results showing state of the art performance.", "creator": "LaTeX with hyperref package"}}}