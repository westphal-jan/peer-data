{"id": "1407.5158", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2014", "title": "Tight convex relaxations for sparse matrix factorization", "abstract": "Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual $\\ell_1$-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.", "histories": [["v1", "Sat, 19 Jul 2014 07:04:08 GMT  (204kb)", "http://arxiv.org/abs/1407.5158v1", null], ["v2", "Thu, 4 Dec 2014 11:19:07 GMT  (576kb)", "http://arxiv.org/abs/1407.5158v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST stat.TH", "authors": ["emile richard", "guillaume obozinski", "jean-philippe vert"], "accepted": true, "id": "1407.5158"}, "pdf": {"name": "1407.5158.pdf", "metadata": {"source": "CRF", "title": "Tight convex relaxations for sparse matrix factorization", "authors": ["Emile Richard", "Guillaume Obozinski", "Jean-Philippe Vert"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 7.51 58v1 [st at.M L] 1"}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "1.1 Contributions and organization of the paper", "text": "This year it is more than ever before."}, {"heading": "1.2 Notations", "text": "For all integer numbers 1 \u2264 k \u2264 p, [1, p] = {1,.., p} the quantity of integer numbers from 1 to p and Gpk denotes the quantity of sub-sets of k indices in [1, p]. For a vector w-W-W-W-W-W-0, the number of non-zero coefficients in w-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W"}, {"heading": "2 Tight convex relaxations of sparse factorization constraints", "text": "In this section, we propose two new matrix standards that allow us to formulate different matrix components. (...) We begin by defining the (k, q) precedence of a matrix in Section 2.1, a useful generalization of precedence that also quantifies the thriftiness of a matrix factorization. (...) We then present two atomic norms that are defined as narrowly convex, and the (k, q) precedence in Section 2.2: the (k, q) precedence of the matrix precedence order via the operator norm, and the (k, q) precedence order norm, obtained through a similar construction with additional constraints of factors. (...) In Section 2.3, we will convert these matrix norms to vector norms, in particular by linking the (k, q) trace standards for matrices with the support of the standard."}, {"heading": "2.3 Equivalent nuclear norms built upon vector norms", "text": "In this section, we show that the (k, q) trace standard (definition 4) and the (k, q) CUT standard (definition 8), which we define as atomic norms generated by specific atomic sets, can alternatively be considered as instances of nuclear norms considered by Jameson (1987). To this end, it is useful to recall the general definition of nuclear norms and the characterization of the corresponding dual norms as formulated in Jameson (1987, Propositions 1,9 and 1,11): Proposition 9 (nuclear norm) Let us list any vector norms on Rm1 and Rm2 (Z)."}, {"heading": "3 Learning matrices with sparse factors", "text": "In this section, we briefly discuss how the (k, q) trace standard and (k, q) CUT standard can be used to address various problems affecting the estimation of sparse low-level matrices."}, {"heading": "3.1 Denoising", "text": "A natural convex formula to restore the noiseless matrix is to be solved: min Z1, min Z1, min Z1, min Z- X, x 2Fro, q (Z), q (Z), (15), \u03bb being a parameter to be tuned. Note that at \u03bb \u2192 0 you simply get a soft (k, q) SVD of X."}, {"heading": "3.2 Bilinear regression", "text": "More generally, in view of some empirical risks L (Z), it is natural to consider formulations of the form min Z L (Z) + \u03bb\u0430k, q (Z) in order to learn matrices that are a priori assumed to have a low (k, q) rank. A particular example is bilinear regression, in which, at two inputs x-Rm1 and x-Rm2, one considers a noisy version of y = x-Zx as output. Suppose Z has a low (k, q) rank meaning that the noiseless response is a sum of a small number of terms, each containing only a small number of characteristics from one of the input vectors. To make such a model based on observations (xi, x-i, yi) i = 1,..., n, one can consider the following convex formulation: min Zn \u00b2 i = 1 (x i Zx \u00b2 i, yi), q (Z), (16), where there is a problem where q = 1 is a function in which evar = 1."}, {"heading": "3.3 Subspace clustering.", "text": "Clustering in subspace assumes that the data can be bundled in such a way that the dots in each cluster belong to a low-dimensional space. If we have a design matrix X-Rn-p with each line corresponding to an observation, then the previous assumption is that if X-j-j-j-j-j-p is a matrix formed from the rows of cluster j, there is a low-level matrix Z-j-Rnj-nj, so that Z-j-j-j-j-j-matrix exists. This means that there is a block diagonal matrix Z-X-X with low-level diagonal blocks. This idea, recently exploited by Wang et al. (2013), implies that Z is a sum of sparse matrices of low rank; and this property is still valid when clustering is unknown."}, {"heading": "3.4 Sparse PCA", "text": "It is not as if it were an unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen,"}, {"heading": "3.5 NP-hard convex problems", "text": "Although the (k, q) -K-Norm and the problems associated with it with respect to the (k, c, c) -D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-"}, {"heading": "4.2 Performance through the statistical dimension", "text": "These results are essentially based on the fact that if the tangent Cone4 of the regularizer is thinner at a point of interest Z, then the regularizer is more efficient in solving problems of denosis, demixing and compressed capture of Z. The efficiency gain can essentially be quantified by appropriate measurements of the width of the tangent cone, such as the Gaussian width of its intersection with a unit of Euclidean sphere (Chandrasekaran et al., 2012), which normalizes the standard."}, {"heading": "4.2.1 The statistical dimension and its properties", "text": "(Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z. (Z). (Z). (Z.). (Z. (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z. (Z). (Z). (Z). (Z.). (Z). (Z.). (Z.). (Z. (Z.). (Z. (Z.). (Z.). (Z). (Z.). (Z. (Z. (Z.).). (Z. (Z. (Z. (Z.). (Z. (Z.).). (Z.). (Z. (Z. (Z.). (Z). (Z. (Z. (Z. (Z.). (Z.). (Z.). (Z. (Z. (Z. (Z.). (Z.). (Z. (Z.). (Z. (Z.). (Z. (Z.). (Z.). (Z. (Z.). (Z. (Z.). (Z"}, {"heading": "4.2.2 Some cone inclusions and their consequences", "text": "Before assessing and comparing the statistical dimensions of these standards, which require more technical evidence, let us first show, using simple geometric arguments, that for a number of matrices the tangents cones of the various standards are actually nested, which will allow us to derive a deterministic improvement in performance when one standard is used as a regulator instead of another, which should be contrasted with the kind of guarantees derived from bounds of the statistical dimension, which are typically statements with a very high probability. The results in this section are listed in Appendix C. Proposition 17 The standards that are considered satisfactory are the following equations and inequalities:"}, {"heading": "As a consequence, for any A \u2208 A\u0303k,q, the statistical dimensions of the different norms satisfy:", "text": "(32) As examined in Section 4.2.1, statistical dimensions provide estimates for the performance of the various standards in different contexts. Plugging (32) in these results shows that the estimation of an atom in Section 4.2.1 using intersection k, q is at least as good as the use of any convex combination of intersection 1 and trace standards. Note that the various statements in Section 4.2.1 provide upper limits on the performance of the various standards, with guarantees given that are either probable or expected. In fact, the inclusion of the tangent cones (31) and even more so the tangential inclusion of the standard spheres in Section 4.2.1 imply much stronger results, as it may also lead to some deterministic statements, such as the following: Corollary 19 (Improvement in Exact Restoration)."}, {"heading": "4.2.3 Bounds on the statistical dimensions", "text": "The results presented in Section 4.2.2 refer only to a very specific group of matrices (A \u00b2 q = q), and not to the relative achievements of the various standards. (A \u00b2 q = q = q q q). (A \u00b2 q = q = q q q). (A \u00b2 q). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A. (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A).). (A). (A). (A).). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A. (A). (A). (A). (A). (A. (A. (A). (A. (A). (A. (A). (A). (A). (A). (A). (A). (A. (A. (A.). (A. (A). (A.).). (A. (A.). (A.).). (A."}, {"heading": "4.2.4 The vector case", "text": "Norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-norm-"}, {"heading": "5 Algorithms", "text": "As can be seen in section 3, many problems, which are sparse low-level matrix estimates, can be formulated as optimization problems of the form: min Z-Rm1 \u00b7 m2L (Z) + \u03bb-k, q (Z). (37) Unfortunately, this problem, although convex, can be mathematically difficult (Section 3.5). In this section, we present a work set algorithm to approximately solve such problems in practice, if L is differentiable."}, {"heading": "5.1 A working set algorithm", "text": "In view of a set of S-Gm1k \u00b7 Gm2q of the pairs of row and column components, we are forced to consider the optimization problem: min (Z-Z) (I, J-Z) (I, J) (I, J) (I, J) (I) (I, J) (S) (I, J) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S (S) (S) (S) (S (S) (S (S) (S) (S (S (S) (S (S) (S (S (S) (S (S) (S (S (S) (S (S (S) (S (S) (S (S (S (S) (S (S) (S (S (S) (S (S (S) (S (S (S) (S (S) (S (S (S) (S (S (S) (S (S (S (S) (S (S (S (S (S) (S) (S (S (S) (S (S (S) (S (S (S (S) (S) (S (S (S) (S (S) (S) (S (S (S) (S (S (S) (S (S) (S) (S) (S (S (S (S) (S) (S (S"}, {"heading": "5.2 Finding new active components", "text": "In fact, this is an 'unbalanced' solution which relates to the 'unbalanced' ('unbalanced') ('unbalanced') ('unbalanced') and 'unbalanced' ('unbalanced') ('unbalanced') ('unbalanced' () ('unbalanced' () ('unbalanced') ('unbalanced' () ('unbalanced') () ('unbalanced' () () () ('unbalanced' () () () () ('unbalanced' () () () () ('unbalanced' () () () () ()"}, {"heading": "5.3 Computational cost", "text": "If m1, m2 are large, the solution of PS involves minimizing trace standards for matrices of the size k \u00d7 q, which, if k and q are small compared to m1 and m2, have low computing costs. The bottleneck for providing computational complexity of the algorithm is the (k, q) -linRank-1 step. Yuan and Zhang (2013) demonstrated that the problem can be solved in linear time under certain conditions. If the conditions persist at each step of the gradient, the total cost of an iteration can be factored into the cost of evaluating the gradient and evaluating thin SVDs: O (k2q). The evaluation of the gradient has costs dependent on the risk function L. These costs for common applications are O (m1m2). So, assuming the RIP conditions required by Yuan and Zhang (2013), the cost of the algorithm 2 is dominated by matrix vector multiplications, so that a general solution (m1) is very accurate."}, {"heading": "6 Numerical experiments", "text": "In this section, we report on experimental results for assessing the performance of sparse low-level matrix estimates using various techniques. In Section 6.1, we begin with simulations aimed at validating the theoretical results on the statistical dimension of \u0394k, q and assessing how they are generalized to matrices with (k, q) ranks greater than 1. In Section 6.2, we compare several techniques for sparse PCA based on simulated data."}, {"heading": "6.1 Empirical estimates of the statistical dimension.", "text": "In fact, it is such that it is a matter of a way, in which it is a matter of a way, in which people are in a world, in which people are in a world, in which people are in a world, in which people are in a world, in a world, in which people are in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in which people are in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in"}, {"heading": "6.2 Comparison of algorithms for sparse PCA", "text": "In this section we compare the performance of different algorithms in estimating a sparsely factored covariance matrix, which we call \"true.\" The observed sample consists of n random vectors, which are i.e. generated according to N (0, \u03a3 + \u03c32Idp), where (k, k) -rank (\u0440) = 3. The matrix is formed by adding 3 blocks of rank 1 \u2212 quential = a1a 1 + a2a 2 + a3a 3, all of which have the same thrift."}, {"heading": "7 Conclusion", "text": "In this paper, we proposed two new convex penalties, the (k, q) trace standard and the (k, q) CUT standard, which are specifically designed for estimating low-level matrices with sparse factors. Our motivation to propose such convex formulations for sparse low-level matrix inference was twofold: First, it enabled us to consider algorithmic schemes that are better understood when formulating a problem as convex rather than convex optimization problem, although the complexity of solving the problem remains exactly superpolynomial. Second, the use of convex geometry, sample complexity and statistical guarantees allowed us to provide, and in particular, to show that the proposed estimators have a much better statistical dimension than convex combinations of the V1 and the spurnorms. We observed that the improvement only exists for matrices, sample complexity and statistical guarantees, and in this case, the proposed estimators have a much better statistical dimension than convex combinations of the V1 and the spurnorms."}, {"heading": "Acknowledgments", "text": "We would like to thank Francis Bach for the interesting discussions in connection with this work, which was supported by the European Research Council (SMAC-ERC-280032)."}, {"heading": "A Proofs of results in Sections 2 and 3.", "text": "To prove the first assertion, the matrix of matrix (2, 2) -SVD of matrix Z (2, 2) -SVD of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2, 2, 2, 2, R2, (2, 2, 2, 2, 2, 2, 2, R2, (2, 2, 2, 2) -R3 of matrix Z (2, 2, 2) -R3 of matrix Z (2, 2, 2) -R3 of matrix Z (2, 2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2) -R3 of matrix Z (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2 (2, 2, 2, 2, 2, 2, 2, 2) -R3 (2, 2, 2, 2, 2, 2, 2, 2, 2, 2 (2, 2, 2, 2) -R3 (2, 2, 2) -SVD of matrix."}, {"heading": "B Proofs of results in Section 4.1", "text": "Proof [Lemma 14] We prove a more general result than Lemma 14. Let: Rm1 \u00b7 m2 \u2022 R be any matrix norm, and X: Rm1 \u00b7 m2 \u2022 Rn be a linear map. We assume that Xi (i = 1,., n) the i-th design matrix defined by X (Z) i = < Z, Xi \u2212 Z (Z) n random map Z (Z = 1,., n) the i-th design matrix defined by X (Z) i (Z) i-th design matrix (Z) i = < Z, Xi \u2212 Z (Z) n random map Z (Z)."}, {"heading": "C Some cone inclusions (Proofs of results in Section 4.2.2)", "text": "Let's start with a simple result that is useful to prove the inclusions of the tangent. Let's start with a simple result to prove the inclusions of the tangent. Let's start with a simple result that is useful to prove the inclusions of the tangent. Let's start with a simple result that is useful to prove the inclusions of the tangent."}, {"heading": "D Upper bound on the statistical dimension of \u2126k,q (proof of Proposition 23)", "text": "(A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D (A) D) D (A) D (A) D) D (A) D (A) D) D (A) D (A) D) D (A) D (A) D (A) D (A) D (A) D (A) D) D (A) D (A) D) (A) D (A) D) (D) (D) (D) (D) (A) D) (D) (D) (A) (A) D) (A) D (A) (A) D) (A) D (A) (A) D) (A) D (A) D) (A) (A) D) (A) D) (A) (A) D) (A) D) (A) (A) D) (A) D) (A) (A) D) (A) (A) D) (A) (A) D) (A) (A) D) (A) (A) D) (D) (D) (D) (A) (D) (D) (D) (D) (D) (A) (D) (D) (D) (D) (D) (D) (D) (A) (D) (D) (D) (D) (A) (D) (D) (D) (A) (D) (D) (D) (A) (D) (A) (D) (A) (D) (A) (D) (A) (D) (A) (D) (A) (A) (D) (D) (A) (A) (A) (D) (A) (D) (A) (D) (D) (A) (A) (D"}, {"heading": "E Lower bound on the statistical dimension of \u0393\u00b5 (Proof of Proposition 24)", "text": "Let us start with a technical problem: let us start with a solution that we are not satisfied with. (...) Let us start with a solution. (...) Let us start with a solution. (...) Let us start with a solution. (...) Let us start with a solution. (...) Let us start with a solution. (...) Let us start with a solution. (...) Let us start with a solution. (...) Let us start with a solution. (...) Let us start with a solution. (...) Let us find a solution. (...) Let us find a solution. (...) Let us find a solution. (...) Let us find a solution. (...) Let us find a solution. (...) Let us find a solution. (...) Let us find a solution. (...) Let us find a solution."}, {"heading": "F Bounds on the statistical dimension in the vector case (proofs of", "text": "(v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v v (v) v (v) v (v) v (v) v (v) v v (v) v v (v) v v v (v) v v v v v (v) v v v (v) v (v) v (v) v v v (v) v v v (v) v v (v) v (v) v v (v) v (v) v (v) v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v) v (v) v (v) v (v) v (v) v) v (v) v (v) v (v) v (v) v) v (v) v (v) v (v) v (v) v) v (v) v (v) v (v) v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v (v) v (v) v (v) v v v v (v) v (v) v (v v (v) v v (v) v v (v v) v (v v v v) v (v v v v (v) v (v) v v v (v) v (v) v (v) v (v) v v v (v) v (v) v v (v) v (v) v v v (v) v (v) v (v) v v (v) v v (v) v v v v (v) v (v) v (v) v v v v (v) v v v v"}], "references": [{"title": "Living on the edge: Phase transitions in convex programs with random data", "author": ["D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp"], "venue": "Technical Report 1303.6672,", "citeRegEx": "Amelunxen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Amelunxen et al\\.", "year": 2013}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["A.A. Amini", "M.J. Wainwright"], "venue": "Ann. Stat.,", "citeRegEx": "Amini and Wainwright.,? \\Q2009\\E", "shortCiteRegEx": "Amini and Wainwright.", "year": 2009}, {"title": "Sparse prediction with the k-support norm", "author": ["A. Argyriou", "R. Foygel", "N. Srebro"], "venue": "Adv. Neural. Inform. Process Syst.,", "citeRegEx": "Argyriou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2012}, {"title": "Convex relaxations of structured matrix factorizations", "author": ["F. Bach"], "venue": "Technical Report 1309.3117,", "citeRegEx": "Bach.,? \\Q2013\\E", "shortCiteRegEx": "Bach.", "year": 2013}, {"title": "Convex sparse matrix factorizations", "author": ["F. Bach", "J. Mairal", "J. Ponce"], "venue": "Technical Report 0812.1869,", "citeRegEx": "Bach et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2008}, {"title": "Structured sparsity through convex optimization", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Stat. Sci.,", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Q. Berthet", "P. Rigollet"], "venue": "COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "PhaseLift: Exact and stable signal recovery from magnitude measurements via convex programming", "author": ["E.J. Cand\u00e8s", "T. Strohmer", "V. Voroninski"], "venue": "Comm. Pure Appl. Math.,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2013}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["V. Chandrasekaran", "M.I. Jordan"], "venue": "Proc. Natl. Acad. Sci. USA,", "citeRegEx": "Chandrasekaran and Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran and Jordan.", "year": 2013}, {"title": "The convex geometry of linear inverse problems", "author": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"], "venue": "Found. Comput. Math.,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2012}, {"title": "On bounds for the normal integral", "author": ["J.T. Chu"], "venue": "Biometrika, 42(1/2):263\u2013265,", "citeRegEx": "Chu.,? \\Q1954\\E", "shortCiteRegEx": "Chu.", "year": 1954}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L. El Ghaoui"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "Local operator theory, random matrices and Banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "Handbook of the Geometry of Banach Spaces,", "citeRegEx": "Davidson and Szarek.,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek.", "year": 2001}, {"title": "Geometry of Cuts and Metrics, volume 15 of Algorithms and Combinatorics", "author": ["M.M. Deza", "M. Laurent"], "venue": null, "citeRegEx": "Deza and Laurent.,? \\Q1997\\E", "shortCiteRegEx": "Deza and Laurent.", "year": 1997}, {"title": "Finding approximately rank-one submatrices with the nuclear norm and l1 norms", "author": ["X.V. Doan", "S.A. Vavasis"], "venue": "SIAM J. Optimiz.,", "citeRegEx": "Doan and Vavasis.,? \\Q2013\\E", "shortCiteRegEx": "Doan and Vavasis.", "year": 2013}, {"title": "Corrupted sensing: Novel guarantees for separating structured signals", "author": ["R. Foygel", "L. Mackey"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Foygel and Mackey.,? \\Q2014\\E", "shortCiteRegEx": "Foygel and Mackey.", "year": 2014}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J.-P. Vert"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Summing and Nuclear Norms in Banach Space Theory. Number 8 in London Mathematical Society Student Texts", "author": ["G.J.O. Jameson"], "venue": null, "citeRegEx": "Jameson.,? \\Q1987\\E", "shortCiteRegEx": "Jameson.", "year": 1987}, {"title": "Large cliques elude the Metropolis process", "author": ["M. Jerrum"], "venue": "Random Struct. Alg.,", "citeRegEx": "Jerrum.,? \\Q1992\\E", "shortCiteRegEx": "Jerrum.", "year": 1992}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M. Journ\u00e9e", "Y. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "Nuclear norm penalization and optimal rates for noisy matrix completion", "author": ["V. Koltchinskii", "K. Lounici", "A.B. Tsybakov"], "venue": "Ann. Stat.,", "citeRegEx": "Koltchinskii et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii et al\\.", "year": 2011}, {"title": "Do semidefinite relaxations really solve sparse PCA", "author": ["R. Krauthgamer", "B. Nadler", "D. Vilenchik"], "venue": "Technical Report 1306:3690,", "citeRegEx": "Krauthgamer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krauthgamer et al\\.", "year": 2013}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "Adv. Neural. Inform. Process Syst.,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Conditional gradient algorithms for rank-one matrix approximations with a sparsity constraint", "author": ["R. Luss", "M. Teboulle"], "venue": "SIAM Rev.,", "citeRegEx": "Luss and Teboulle.,? \\Q2013\\E", "shortCiteRegEx": "Luss and Teboulle.", "year": 2013}, {"title": "Deflation methods for sparse PCA", "author": ["L.W. Mackey"], "venue": "Adv. Neural. Inform. Process Syst.,", "citeRegEx": "Mackey.,? \\Q2009\\E", "shortCiteRegEx": "Mackey.", "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Spectral bounds for sparse PCA: Exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "Sh. Avidan"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Moghaddam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2006}, {"title": "Sparse regression as a sparse eigenvalue problem", "author": ["B. Moghaddam", "A. Gruber", "Y. Weiss", "S. Avidan"], "venue": "In Information Theory and Applications Workshop,", "citeRegEx": "Moghaddam et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2008}, {"title": "A unified framework for highdimensional analysis of M-estimators", "author": ["S. N Negahban", "P. Ravikumar", "M. J Wainwright", "B. Yu"], "venue": "Statistical Science,", "citeRegEx": "Negahban et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2012}, {"title": "Sharp mse bounds for proximal denoising", "author": ["S. Oymak", "B. Hassibi"], "venue": "Technical Report 1305.2714,", "citeRegEx": "Oymak and Hassibi.,? \\Q2013\\E", "shortCiteRegEx": "Oymak and Hassibi.", "year": 2013}, {"title": "Simultaneously structured models with application to sparse and low-rank matrices", "author": ["S. Oymak", "A. Jalali", "M. Fazel", "Y.C. Eldar", "B. Hassibi"], "venue": "Technical Report 1212.3753,", "citeRegEx": "Oymak et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Oymak et al\\.", "year": 2012}, {"title": "The squared-error of generalized LASSO: A precise analysis", "author": ["S. Oymak", "C. Thrampoulidis", "B. Hassibi"], "venue": "In 51st Annual Allerton Conference on Communication,", "citeRegEx": "Oymak et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oymak et al\\.", "year": 2013}, {"title": "Estimation of simultaneously sparse and low-rank matrices", "author": ["E. Richard", "P.-A. Savalle", "N. Vayatis"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Richard et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Richard et al\\.", "year": 2012}, {"title": "Intersecting singularities for multi-structured estimation", "author": ["E. Richard", "F. Bach", "J.-P. Vert"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "Richard et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richard et al\\.", "year": 2013}, {"title": "Link prediction in graphs with autoregressive features", "author": ["E. Richard", "S. Ga\u00efffas", "N. Vayatis"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Richard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Richard et al\\.", "year": 2014}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1997\\E", "shortCiteRegEx": "Rockafellar.", "year": 1997}, {"title": "A coordinate gradient descent method for nonsmooth separable minimization", "author": ["P. Tseng", "S. Yun"], "venue": "Math. Program.,", "citeRegEx": "Tseng and Yun.,? \\Q2009\\E", "shortCiteRegEx": "Tseng and Yun.", "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": null, "citeRegEx": "Vershynin.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin.", "year": 2012}, {"title": "Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting", "author": ["M.J. Wainwright"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Wainwright.,? \\Q2009\\E", "shortCiteRegEx": "Wainwright.", "year": 2009}, {"title": "Provable subspace clustering: When LRR meets SSC", "author": ["Y.-X. Wang", "H. Xu", "C. Leng"], "venue": "Adv. Neural. Inform. Process Syst.,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Characterization of the subdifferential of some matrix norms", "author": ["G.A. Watson"], "venue": "Lin. Alg. Appl.,", "citeRegEx": "Watson.,? \\Q1992\\E", "shortCiteRegEx": "Watson.", "year": 1992}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "R. Tibshirani", "T. Hastie"], "venue": "URL http://dx.doi.org/10.1093/biostatistics/kxp008", "citeRegEx": "Witten et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2009}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["X.-T. Yuan", "T. Zhang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Yuan and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Yuan and Zhang.", "year": 2013}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "J. Comput. Graph. Stat.,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}, {"title": "offers the possibility of constraining the estimator to lie in a cone C. In our case, C = Rm1\u00d7m2 , given the definition of \u03b3 we therefore have \u03b3 \u2264 2. The result follows from applying the theorem with \u03b82 = \u03b6(a", "author": ["Oymak"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "q,m1/k,m2/q \u2265 M then n0 is large enough to ensure 1 \u2212 c1 exp(\u2212c2n0) > 4 exp (\u221232/17). Then, according to Lemma 38, solving (28) with the norm \u0393\u03bc fails to recover A = ab with probability at least 4 exp (\u221232/17)", "author": ["M Take"], "venue": "On the other hand, Amelunxen et al", "citeRegEx": "Take,? \\Q2013\\E", "shortCiteRegEx": "Take", "year": 2013}, {"title": "Annex C), for (61) we used the fact that for a standard normal random variable G", "author": ["Chandrasekaran"], "venue": null, "citeRegEx": "Chandrasekaran,? \\Q2012\\E", "shortCiteRegEx": "Chandrasekaran", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "We compute slow rates and an upper bound on the statistical dimension Amelunxen et al. (2013) of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l1-norm, trace norm and their combinations.", "startOffset": 70, "endOffset": 94}, {"referenceID": 35, "context": "A range of machine learning problems such as link prediction in graphs containing community structure (Richard et al., 2014), phase retrieval (Cand\u00e8s et al.", "startOffset": 102, "endOffset": 124}, {"referenceID": 7, "context": ", 2014), phase retrieval (Cand\u00e8s et al., 2013), subspace clustering (Wang et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 40, "context": ", 2013), subspace clustering (Wang et al., 2013) or dictionary learning for sparse coding (Mairal et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 26, "context": ", 2013) or dictionary learning for sparse coding (Mairal et al., 2010) amount to solve sparse matrix factorization problems, i.", "startOffset": 49, "endOffset": 70}, {"referenceID": 44, "context": "Landmark applications of sparse matrix factorization are sparse principal components analysis (SPCA, d\u2019Aspremont et al., 2007; Zou et al., 2006) or sparse canonical correlation analysis (SCCA, Witten et al.", "startOffset": 94, "endOffset": 144}, {"referenceID": 27, "context": "From a computational point of view, however, sparse matrix factorization is challenging since it typically leads to non-convex, NP-hard problems (Moghaddam et al., 2006).", "startOffset": 145, "endOffset": 169}, {"referenceID": 6, "context": "For instance, Berthet and Rigollet (2013) noted that solving sparse PCA with a single component is equivalent to the planted clique", "startOffset": 14, "endOffset": 42}, {"referenceID": 19, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix.", "startOffset": 8, "endOffset": 22}, {"referenceID": 23, "context": "A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010).", "startOffset": 158, "endOffset": 197}, {"referenceID": 26, "context": "A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010).", "startOffset": 158, "endOffset": 197}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008).", "startOffset": 102, "endOffset": 162}, {"referenceID": 25, "context": "Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix.", "startOffset": 143, "endOffset": 157}, {"referenceID": 18, "context": "(2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations.", "startOffset": 196, "endOffset": 211}, {"referenceID": 15, "context": "Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014).", "startOffset": 204, "endOffset": 282}, {"referenceID": 31, "context": "Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014).", "startOffset": 204, "endOffset": 282}, {"referenceID": 1, "context": "(2013) prove that the SDP relaxations fail at finding the sparse principal component outside the favorable regime where a simple diagonal thresholding algorithm (Amini and Wainwright, 2009) works.", "startOffset": 161, "endOffset": 189}, {"referenceID": 11, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journ\u00e9e et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component.", "startOffset": 9, "endOffset": 648}, {"referenceID": 11, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journ\u00e9e et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component.", "startOffset": 9, "endOffset": 674}, {"referenceID": 11, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journ\u00e9e et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component.", "startOffset": 9, "endOffset": 697}, {"referenceID": 11, "context": "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journ\u00e9e et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component. These algorithms perform well empirically and have been proved to be efficient theoretically under mild conditions by Yuan and Zhang (2013). Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al.", "startOffset": 9, "endOffset": 889}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations.", "startOffset": 103, "endOffset": 497}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations.", "startOffset": 103, "endOffset": 621}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d\u2019Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al.", "startOffset": 103, "endOffset": 1387}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d\u2019Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al.", "startOffset": 103, "endOffset": 1408}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d\u2019Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al. (2012) showed that for compressed sensing applications, no convex combination of the two norms improves over each norm taken alone.", "startOffset": 103, "endOffset": 1429}, {"referenceID": 1, "context": "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d\u2019Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d\u2019Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al. (2012) showed that for compressed sensing applications, no convex combination of the two norms improves over each norm taken alone. Krauthgamer et al. (2013) prove that the SDP relaxations fail at finding the sparse principal component outside the favorable regime where a simple diagonal thresholding algorithm (Amini and Wainwright, 2009) works.", "startOffset": 103, "endOffset": 1580}, {"referenceID": 9, "context": "2 two new atomic norms for matrices (Chandrasekaran et al., 2012).", "startOffset": 36, "endOffset": 65}, {"referenceID": 17, "context": "3 an equivalent characterization of the norms as nuclear norms, in the sense of Jameson (1987), highlighting in particular a link to the k-support norm of Argyriou et al.", "startOffset": 80, "endOffset": 95}, {"referenceID": 2, "context": "3 an equivalent characterization of the norms as nuclear norms, in the sense of Jameson (1987), highlighting in particular a link to the k-support norm of Argyriou et al. (2012). \u2022 Using these norms to estimate sparse low-rank matrices (Section 3).", "startOffset": 155, "endOffset": 178}, {"referenceID": 2, "context": "3 we relate these matrix norms to vector norms using the concept of nuclear norms, establishing in particular a connection of the (k, q)-trace norm for matrices with the k-support norm of Argyriou et al. (2012), and the (k, q)-CUT norm to the vector k-norm, defined as the sum of the k largest components in absolute value of a vector (Bhatia, 1997, Exercise II.", "startOffset": 188, "endOffset": 211}, {"referenceID": 44, "context": "For example, the standard rank-1 SPCA problem consists in finding the symmetric matrix with (k, k)-rank equal to 1 and providing the best approximation of the sample covariance matrix (Zou et al., 2006).", "startOffset": 184, "endOffset": 202}, {"referenceID": 9, "context": "They are both instances of the atomic norms introduced by Chandrasekaran et al. (2012), which we first review.", "startOffset": 58, "endOffset": 87}, {"referenceID": 9, "context": "Chandrasekaran et al. (2012) show that the atomic norm induced by A is indeed a norm, which can be rewritten as \u2016x\u2016A = inf { \u2211", "startOffset": 0, "endOffset": 29}, {"referenceID": 36, "context": "see Rockafellar (1997), p.", "startOffset": 4, "endOffset": 23}, {"referenceID": 14, "context": "Our choice of terminology is motivated by the following relation of our norm to the CUT-polytope: in the case k = m1 and q = m2, the unit ball of \u03a9\u0303k,q coincides (up to a scaling factor of \u221a m1m2) with the polytope known as the CUT polytope of the complete graph on n vertices (Deza and Laurent, 1997), defined by CUT = conv {ab , a \u2208 {\u00b11}1 , b \u2208 {\u00b11}2} .", "startOffset": 277, "endOffset": 301}, {"referenceID": 18, "context": "3 Equivalent nuclear norms built upon vector norms In this section we show that the (k, q)-trace norm (Definition 4) and the (k, q)-CUT norm (Definition 8), which we defined as atomic norms induced by specific atom sets, can alternatively be seen as instances of nuclear norms considered by Jameson (1987). For that purpose it is useful to recall the general definition of nuclear norms and the characterization of the corresponding dual norms as formulated in Jameson (1987, Propositions 1.", "startOffset": 291, "endOffset": 306}, {"referenceID": 2, "context": "The (k, q)-trace norm is the nuclear norm induced by \u03b8k on R m1 and \u03b8q on Rm2 , where for any j \u2265 1, \u03b8j is the j-support norm introduced by Argyriou et al. (2012). 2.", "startOffset": 140, "endOffset": 163}, {"referenceID": 2, "context": "For the sake of completeness, let us recall the closed-form expression of the k-support norm \u03b8k shown by Argyriou et al. (2012). For any vector w \u2208 Rp, let w\u0304 \u2208 Rp be the vector obtained by sorting the entries of w by decreasing order of absolute values.", "startOffset": 105, "endOffset": 128}, {"referenceID": 5, "context": "It is however known since Jameson (1987) (see also Bach, 2013; Bach et al., 2012) that the nuclear norm induced by vector l1-norm is simply the l1 of the matrix which fails to induce low rank (except in the very sparse case).", "startOffset": 41, "endOffset": 81}, {"referenceID": 15, "context": "It is however known since Jameson (1987) (see also Bach, 2013; Bach et al.", "startOffset": 26, "endOffset": 41}, {"referenceID": 3, "context": "It is however known since Jameson (1987) (see also Bach, 2013; Bach et al., 2012) that the nuclear norm induced by vector l1-norm is simply the l1 of the matrix which fails to induce low rank (except in the very sparse case). However Bach et al. (2012) proposed nuclear norms associated with vectors norms that are similar to the elastic net penalty.", "startOffset": 51, "endOffset": 253}, {"referenceID": 7, "context": "Quadratic regression combined with additional constraints on Z is closely related to phase retrieval (Cand\u00e8s et al., 2013).", "startOffset": 101, "endOffset": 122}, {"referenceID": 40, "context": "This idea, exploited recently by Wang et al. (2013) implies that Z is a sum of low rank sparse matrices; and this property still holds if the clustering is unknown.", "startOffset": 33, "endOffset": 52}, {"referenceID": 11, "context": "4 Sparse PCA In sparse PCA (d\u2019Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix \u03a3\u0302n by a low-rank matrix with sparse factors.", "startOffset": 27, "endOffset": 92}, {"referenceID": 42, "context": "4 Sparse PCA In sparse PCA (d\u2019Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix \u03a3\u0302n by a low-rank matrix with sparse factors.", "startOffset": 27, "endOffset": 92}, {"referenceID": 44, "context": "4 Sparse PCA In sparse PCA (d\u2019Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix \u03a3\u0302n by a low-rank matrix with sparse factors.", "startOffset": 27, "endOffset": 92}, {"referenceID": 25, "context": "In contrast to sequential approaches that estimate the principal components one by one (Mackey, 2009), this formulation requires to find simultaneously a set of factors which are complementary to one another in order to explain as much variance as possible.", "startOffset": 87, "endOffset": 101}, {"referenceID": 28, "context": "which it is known to be NP-hard (Moghaddam et al., 2008).", "startOffset": 32, "endOffset": 56}, {"referenceID": 29, "context": "Obviously the comparison of upper bounds is not enough to conclude to the superiority of (k, q)-trace norm and, admittedly, the problem of denoising considered here is a special instance of linear regression in which the design matrix is the identity, and, since this is a case in which the design is trivially incoherent, it is possible to obtain fast rates for decomposable norms such as the l1 or trace norm (Negahban et al., 2012); however, slow rates are still valid in the presence of an incoherent design, or when the signal to recover is only weakly sparse, which is not the case for the fast rates.", "startOffset": 411, "endOffset": 434}, {"referenceID": 0, "context": "We present in the next section more involved results, based on lower and upper bounds on the so-called statistical dimension of the different norms (Amelunxen et al., 2013), a measure which is closely related to Gaussian widths.", "startOffset": 148, "endOffset": 172}, {"referenceID": 9, "context": "The gain in efficiency can be quantified by appropriate measures of width of the tangent cone such as the Gaussian width of its intersection with a unit Euclidean ball (Chandrasekaran et al., 2012), or the closely related concept of statistical dimension of the cone, proposed by Amelunxen et al.", "startOffset": 168, "endOffset": 197}, {"referenceID": 31, "context": "In particular, we will consider the norms \u03a9k,q, \u03a9\u0303k,q and linear combinations of the l1 and trace norms, which have been used in the literature to infer sparse lowrank matrices (Oymak et al., 2012; Richard et al., 2012).", "startOffset": 177, "endOffset": 219}, {"referenceID": 33, "context": "In particular, we will consider the norms \u03a9k,q, \u03a9\u0303k,q and linear combinations of the l1 and trace norms, which have been used in the literature to infer sparse lowrank matrices (Oymak et al., 2012; Richard et al., 2012).", "startOffset": 177, "endOffset": 219}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al.", "startOffset": 117, "endOffset": 141}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al.", "startOffset": 117, "endOffset": 171}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al.", "startOffset": 117, "endOffset": 197}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al. (2013) to quantify the statistical power of a convex nonsmooth regularizer used as a constraint or penalty.", "startOffset": 117, "endOffset": 218}, {"referenceID": 0, "context": "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al. (2013) to quantify the statistical power of a convex nonsmooth regularizer used as a constraint or penalty. These results rely essentially on the fact that if the tangent cone of the regularizer at a point of interest Z is thiner, then the regularizer is more efficient at solving problems of denoising, demixing and compressed sensing of Z. The gain in efficiency can be quantified by appropriate measures of width of the tangent cone such as the Gaussian width of its intersection with a unit Euclidean ball (Chandrasekaran et al., 2012), or the closely related concept of statistical dimension of the cone, proposed by Amelunxen et al. (2013). In this section, we study the statistical dimensions induced by different matrix norms in order to compare their theoretical properties for exact or approximate recovery of sparse low-rank matrices.", "startOffset": 117, "endOffset": 857}, {"referenceID": 9, "context": "14 in Chandrasekaran et al. (2012) and from the upper bound log (m k ) \u2264 k(1 + log(m/k)), that Proposition 21 For any A \u2208 \u00c3k,q, we have S(A, \u03a9\u0303k,q) \u2264 16(k + q) + 9 ( k log m1 k + q log m2 q ) .", "startOffset": 6, "endOffset": 35}, {"referenceID": 39, "context": "In the vector case, for example, it is known that the recovery of a sparse vector \u03b2 with support I0 depends on its smallest coefficient \u03b2min = mini\u2208I0 \u03b2 2 i (Wainwright, 2009).", "startOffset": 157, "endOffset": 175}, {"referenceID": 8, "context": "This result is actually stated informally for the special case of k = q = \u221a m = with m = m1 = m2 in the context of a discussion of the planted clique problem in Chandrasekaran and Jordan (2013).", "startOffset": 161, "endOffset": 194}, {"referenceID": 31, "context": "Finally, regarding the combination \u0393\u03bc of the l1 norm and of the trace norm, Oymak et al. (2012) has shown that it does not improve rates up to constants over the best of the two norms.", "startOffset": 76, "endOffset": 96}, {"referenceID": 31, "context": "In that case, we see that, as stated by Oymak et al. (2012), \u0393\u03bc does not bring any improvement over the l1 and trace norms taken imdividually, and in particular has a worse statistical dimension than \u03a9k,q and \u03a9\u0303k,q.", "startOffset": 40, "endOffset": 60}, {"referenceID": 9, "context": "In the lasso case (k = 1), we recover the standard bound (Chandrasekaran et al., 2012):", "startOffset": 57, "endOffset": 86}, {"referenceID": 3, "context": "Working set algorithms (Bach et al., 2011, Chap. 6) are typically useful to speed up algorithm for sparsity inducing regularizer; they have been used notably in the case of the overlapping group Lasso of Jacob et al. (2009) which is also naturally formulated via latent components.", "startOffset": 24, "endOffset": 224}, {"referenceID": 41, "context": "From the characterization of the subdifferential of the trace norm (Watson, 1992), writing Z = U \u03a3V (IJ) the SVD of Z, this is equivalent to, for all (I, J) in S, either Z 6=0 and \u2207L(Z)IJ = \u2212\u03bb ( U V (IJ) \u22a4 +A )", "startOffset": 67, "endOffset": 81}, {"referenceID": 37, "context": "Problem (PS) is solved easily using the approximate block coordinate descent of Tseng and Yun (2009) (see", "startOffset": 80, "endOffset": 101}, {"referenceID": 20, "context": "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journ\u00e9e et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.", "startOffset": 133, "endOffset": 202}, {"referenceID": 24, "context": "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journ\u00e9e et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.", "startOffset": 133, "endOffset": 202}, {"referenceID": 43, "context": "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journ\u00e9e et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.", "startOffset": 133, "endOffset": 202}, {"referenceID": 20, "context": "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journ\u00e9e et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.", "startOffset": 134, "endOffset": 260}, {"referenceID": 43, "context": "It has been proved by Yuan and Zhang (2013) that under some conditions the problem can be solved in linear time.", "startOffset": 22, "endOffset": 44}, {"referenceID": 43, "context": "It has been proved by Yuan and Zhang (2013) that under some conditions the problem can be solved in linear time. If the conditions hold at every step of gradient, the overall cost of an iteration can be cast into the cost of evaluating the gradient and the evaluation of thin SVDs: O(k2q). Evaluating the gradient has a cost dependent on the risk function L. This cost for usual applications is O(m1m2). So assuming the RIP conditions required by Yuan and Zhang (2013) hold, the cost of Algorithm 2 is dominated by matrix-vector multiplications so of the order O(m1m2).", "startOffset": 22, "endOffset": 469}, {"referenceID": 30, "context": "Fro \u03c32 is a good estimate of the statistical dimension, since Oymak and Hassibi (2013) show that", "startOffset": 62, "endOffset": 87}], "year": 2017, "abstractText": "Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension Amelunxen et al. (2013) of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.", "creator": "LaTeX with hyperref package"}}}