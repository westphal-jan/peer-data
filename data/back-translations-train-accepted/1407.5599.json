{"id": "1407.5599", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2014", "title": "Scalable Kernel Methods via Doubly Stochastic Gradients", "abstract": "The general perception is that kernel methods are not scalable, and neural nets are the methods of choice for nonlinear learning problems. Or have we simply not tried hard enough for kernel methods? Here we propose an approach that scales up kernel methods using a novel concept called \"doubly stochastic functional gradients\". Our approach relies on the fact that many kernel methods can be expressed as convex optimization problems, and we solve the problems by making two unbiased stochastic approximations to the functional gradient, one using random training points and another using random functions associated with the kernel, and then descending using this noisy functional gradient. We show that a function produced by this procedure after $t$ iterations converges to the optimal function in the reproducing kernel Hilbert space in rate $O(1/t)$, and achieves a generalization performance of $O(1/\\sqrt{t})$. This doubly stochasticity also allows us to avoid keeping the support vectors and to implement the algorithm in a small memory footprint, which is linear in number of iterations and independent of data dimension. Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show that our method can achieve competitive performance to neural nets in datasets such as 8 million handwritten digits from MNIST, 2.3 million energy materials from MolecularSpace, and 1 million photos from ImageNet.", "histories": [["v1", "Mon, 21 Jul 2014 19:05:47 GMT  (297kb,D)", "https://arxiv.org/abs/1407.5599v1", "22 pages, 11 figures"], ["v2", "Tue, 5 Aug 2014 17:58:57 GMT  (297kb,D)", "http://arxiv.org/abs/1407.5599v2", "22 pages, 11 figures"], ["v3", "Tue, 23 Sep 2014 15:39:03 GMT  (345kb,D)", "http://arxiv.org/abs/1407.5599v3", "24 pages, 14 figures"], ["v4", "Thu, 10 Sep 2015 16:40:45 GMT  (1737kb,D)", "http://arxiv.org/abs/1407.5599v4", "32 pages, 22 figures"]], "COMMENTS": "22 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["bo dai", "bo xie 0002", "niao he", "yingyu liang", "anant raj", "maria-florina balcan", "le song"], "accepted": true, "id": "1407.5599"}, "pdf": {"name": "1407.5599.pdf", "metadata": {"source": "CRF", "title": "Scalable Kernel Methods via Doubly Stochastic Gradients", "authors": ["Bo Dai", "Bo Xie", "Niao He", "Yingyu Liang", "Anant Raj", "Maria-Florina Balcan", "Le Song"], "emails": ["araj34}@gatech.edu,", "lsong@cc.gatech.edu", "nhe6@gatech.edu", "ninamf@cs.cmu.edu"], "sections": [{"heading": null, "text": "We show that our method can compete with neural networks in datasets such as 2.3 million energy materials from MolecularSpace, 8 million handwritten digits from MNIST, and 1 million photos from ImageNet using folding functions."}, {"heading": "1 Introduction", "text": "The general perception is that kernel methods are not scalable. When it comes to large-scale nonlinear learning problems, the methods of choice so far are neural networks in which theoretical understanding remains incomplete. Are kernel methods really not scalable? Or is it simply because we have not tried hard enough while neural networks have exploited complex design of feature architectures, sample virtual generation for dealing with invariance, stochastic gradient derivation for efficient training, and GPUs for further acceleration? A bottleneck in the scaling of kernel methods is the storage and calculation of the kernel matrix that is normally dense. Storing the matrix requires O (n2) space, and the calculation takes O (n2d) operations where n is the number of data points and d the dimension. There have been many large attempts to scale the methods, including efforts from numerical analysis and optimization."}, {"heading": "2 Duality between Kernels and Random Processes", "text": "The kernel methods take their name from the use of kernel functions, k (x, x > function): X \u00b7 X 7 \u2192 R, which are symmetrically positively defined (PD), which means that for all n > 1, and x1,.., xn \u00b2 X, and c1,.., cn \u00b2 R, we have n i, j = 1 cicjk (xi, xj) > 0. There is a fascinating duality between kernel and stochastic processes that will play a crucial role in our later algorithm design. More specifically, theorem 1 (e.g., [17]), if k (x, x \u00b2) is a PD kernel, then there is a set of P based and random features that will play a crucial role in our later algorithm design. More specifically, theorem 1 (e.g., [17], if k (x, x \u00b2) gives a set of P based on P and random features."}, {"heading": "3 Doubly Stochastic Functional Gradients", "text": "Many core methods can be written as convex optimizations about functions in the RKHS and are solved with the functional gradient methods (13, 15). Inspired by this previous work, we will present a new concept called \"double stochastic functional gradients\" to solve the problem of scalability. Let l (u, y) and the associated RKHS H, many kernel methods try to find a function f (u, y) that solves the optimization problem f (f)."}, {"heading": "4 Doubly Stochastic Kernel Machines", "text": "The first key intuition behind our algorithm is based on the property of the stochastic gradient. We assume that the stochastic gradient is unbiased, so the convergence of the algorithm is guaranteed [16]. In our algorithm, we will exploit this property and introduce two sources of random number generators, of which the sequences of apparently random samples can in fact be determined entirely from a baseline value (a seed). Although these random samples are not the \"true\" sample in the purest sense of the word, they are sufficient for our task in practice."}, {"heading": "5 Theoretical Guarantees", "text": "In this section we will show that, both in expectation and with high probability, our algorithm can estimate the optimal function in the RKHS with the RKHS with the RKHS rate O (1 / t) and achieve a generalization of O (1 / p). The analysis for our algorithm has a new twist compared to previous analyses of stochastic gradient parentage algorithms, since the sequence of estimators, ft + 1, does not exist outside the RKHS convergence to optimal function outside the RKHS. We make the following standard assumptions for later references: A. There is an optimal solution that we have designated as f. \"to solve the problem of our interest (1). B. Loss function (u): R.\""}, {"heading": "6 Computation, Memory and Statistics Trade-off", "text": "In fact, most of them are able to survive on their own if they are not able to play by the rules."}, {"heading": "7 Experiments", "text": "O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \"O\" O \""}, {"heading": "7.1 Kernel Ridge Regression", "text": "In this section, we compare our approach with alternative algorithms for kernel regression on 2D synthetic datasets. Data is generated by y = cos (0.5p x 2) exp (\u2212 0.1p x 2) + 0.1e, where x [\u2212 5, 5] 2 and e \u0445 N (0, 1). We use the Gaussian RBF kernel with a kernel bandwidth chosen to be 0.1 times the median of the paired distances between data points (median trick). Regularization parameter is set to 10 \u2212 6. Batch size and function block are set to 210. Results are presented in Figure 4 (1). We justify our proof of the convergence rate in Figure 4 (2). The blue dashed line is a convergence rate of 1 / t as a guide."}, {"heading": "7.2 Gaussian Processes Regression", "text": "As we have introduced in Section (4), the mean and variance of the rear Gaussian process for the regression problem can be formulated as solutions to some convex optimization problems. We conduct experiments with synthetic datasets for justification. Since the task is to calculate the rear mean, we evaluate the performance by comparing the solutions with the rear mean and the variance, which are called fgp and \u03c3 2 gp, obtained by closed form (9). We select 211 data from the same model in the previous section for training and 210 data for testing, so that the closed form of the rear mean is tractable. We use the Gaussian RBF kernel with the core bandwidth chosen by a median trick. The noise level \u03c32 is set to 0.1. The stack size is set to 64 and the feature block to 512. We compare the double chastic algorithm with the NORMA analogous results in both the ORMA and the ORMA analogous algorithm."}, {"heading": "7.3 Kernel Support Vector Machine", "text": "We evaluate our algorithm to solve the SVM problem against three datasets (3) - (5) compared to other multiple algorithms listed in Table 2, using the criteria SC1 and SC2.Adult. We use the Gaussian RBF kernel with the bandwidth obtained by median tricks. Regularization parameters are set to 1 / 100, while our algorithm achieves the number of training samples. We set the batch size to 26 and the block to 25. After going through the entire dataset, the best error rate of NORMA and k-SDCA is reached, which is 15%, while our algorithm achieves comparable results. Performance is illustrated in Figure 6. (1) All algorithms work similarly in Figure 6 (4)."}, {"heading": "7.4 Classification Comparisons to Convolution Neural Networks", "text": "We compare our algorithm with the state-of-the-art neural network. In these experiments, the block size is set to O (104). Compared to the number of samples, O (108), this block size is also reasonable. In this experiment, we compare with a variant of LeNet-5 [32], where all Tanh units are replaced by linear units. We also use more convolution filters and a larger fully connected layer. In particular, the first two convolution layers have 16 and 32 filters, respectively, and the fully connected layer contains 128 neurons. We use kernel logistic regression for the task. We extract characteristics from the last max pooling layer with the dimension 1568 and use Gaussian RBF layer with the bandwidth we have trained to four times the mean distance."}, {"heading": "7.5 Regression Comparisons to Neural Networks", "text": "We test our neural network kernel regression algorithm proposed in [30] on two large-scale real-world regression datasets, (9) and (10) in Table 4. To the best of our knowledge, this is the first comparison between kernel regression and neural network on the MolecularSpace.2The specification is at https: / / code.google.com / p / cuda-convennet / QuantumMachine. In this experiment, we use the same binary representations based on random coulomb matrices as in [30]. First, we create a series of randomly sorted coulomb matrices for each molecule. And then we break every dimension of the coulomb matrix apart from steps and convert them into binary predicates. The predictions are made by making the average of all predictions on different coulomb matrices of the same molecule."}, {"heading": "8 Discussion", "text": "Our work contributes to making core methods scalable for large-scale datasets. In particular, by introducing artificial randomness associated with cores in addition to random data samples, we propose double stochastic functional gradients for kernel machines that make the kernel machines efficient in both computation and memory requirements. Our algorithm successfully reduces the memory requirements of kernel machines from O (dn) to O (n). Meanwhile, we also show that our algorithm achieves the optimal convergence rate, O (1 / t), for strongly convex stochastic optimization. We compare our algorithm to state-of-the-art neural networks as well as some other competing algorithms for kernel methods on several large datasets for both classification and regression problems. With our efficient algorithm, kernel methods could be comparable to sophisticated neural networks."}, {"heading": "Acknowledgement", "text": "M.B. is partially supported by the NSF scholarship CCF-1101283, the AFOSR scholarship FA9550-09-1-0538, a Microsoft Faculty Fellowship and a Raytheon Faculty Fellowship. L.S. is partially supported by the NSF IIS-1116886, NSF / NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983 and a Raytheon Faculty Fellowship."}, {"heading": "A Convergence Rate", "text": "First, we provide specific boundaries and detailed proofs for the two error terms found in Theorem 4 and Theorem 5.A.1."}, {"heading": "B L\u221e distance, L2 distance, and generalization bound", "text": "The result is that the average solution f + 1 (+ 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 1) t (= 2) t (= 2) t (+ 2) t (= 2) t (+ 2) t (+ 2) t (+ 2) t (+ 2) t (= 2) t (2) t (2) t (2) t (2) t (x) t (x) t (x) t (x) t (f) t (x) t (x) t (x) t (x)."}, {"heading": "C Suboptimality", "text": "For comprehensive purposes, we also provide the O (1 / t) for suboptimal properties = = \u03b2\u03b2\u03b2\u03b21 = \u03b2\u03b21 = \u03b21 = \u03b2\u03b22 = \u03b22 (f) 6 Q (ln (t) + 1) t: \u2212 1 t: \u2212 1 t: (4) t: (4) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (1) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (1) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2) t: (2"}, {"heading": "D Doubly Stochastic Gradient Algorithm for Posterior Variance", "text": "The operator in Gaussian Process RegressionAs shows in Section 4 that the estimation of the variance of the probable distribution of the Gaussian process for the regression problem can be defined as the estimation of the operator A, which is defined in (10). First, we show that the operator A is the solution to the following optimization problem. The gradient of R (A) with respect to A (A) is no more than 1 (A) - Ak (x, \u00b7 2n) - 2H + 2 (A) 2HSwhere the Hilbert Schmidt standard of the operator is. The gradient of R (A) with respect to A is no more than 1 (A) - Ak (x, \u00b7 2n) - k (x, \u00b7 2n) - K (x, \u00b7 2n) - K (x, \u00b7 2n) - X (x, \u00b7 2HSwith respect to the operator's Hilbert-Schmidt standard."}], "references": [{"title": "Sparse greedy matrix approximation for machine learning", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Using the Nystrom method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg"], "venue": "JMLR, 2:243\u2013264,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "On the nystr om method for approximating a gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M. Mahoney"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "On the impact of kernel approximation on learning accuracy", "author": ["Corinna Cortes", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Fastfood \u2014 computing hilbert space expansions in loglinear time", "author": ["Q.V. Le", "T. Sarlos", "A.J. Smola"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Randomized nonlinear component analysis", "author": ["David Lopez-Paz", "Suvrit Sra", "A.J. Smola", "Zoubin Ghahramani", "Bernhard Schlkopf"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["John C. Platt"], "venue": "Technical Report MSR-TR-98-14, Microsoft Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods \u2014 Support Vector Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A.J. Smola", "R.C. Williamson"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "A modified finite Newton method for fast solution of large scale linear SVMs", "author": ["S.S. Keerthi", "D. DeCoste"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Kernel conjugate gradient for fast kernel machines", "author": ["N. Ratliff", "J. Bagnell"], "venue": "In IJCAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. on Optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Integral representation of pd functions", "author": ["A. Devinatz"], "venue": "Trans. AMS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1953}, {"title": "Kernels, associated structures, and generalizations", "author": ["M. Hein", "O. Bousquet"], "venue": "Technical Report 127, Max Planck Institute for Biological Cybernetics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Learning with Kernels", "author": ["Bernhard Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["N. Pham", "R. Pagh"], "venue": "In KDD. ACM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Predicting time series with support vector machines", "author": ["K.-R. M\u00fcller", "A.J. Smola", "G. R\u00e4tsch", "B. Sch\u00f6lkopf", "J. Kohlmorgen", "V. Vapnik"], "venue": "Artificial Neural Networks ICANN\u201997,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J. Platt", "J. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization", "author": ["X.L. Nguyen", "M. Wainwright", "M. Jordan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Relative novelty detection", "author": ["Alex J Smola", "Le Song", "Choon H Teo"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Training invariant support vector machines with selective sampling", "author": ["G. Loosli", "S. Canu", "L. Bottou"], "venue": "Large Scale Kernel Machines,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Learning invariant representations of molecules for atomization energy prediction", "author": ["Gr\u00e9goire Montavon", "Katja Hansen", "Siamac Fazli", "Matthias Rupp", "Franziska Biegler", "Andreas Ziehe", "Alexandre Tkatchenko", "Anatole von Lilienfeld", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Random feature maps for dot product kernels", "author": ["Purushottam Kar", "Harish Karnick"], "venue": "editors, AISTATS-12,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Random laplace feature maps for semigroup kernels on histograms", "author": ["Jiyan Yang", "Vikas Sindhwani", "Quanfu Fan", "Haim Avron", "Michael W. Mahoney"], "venue": "In CVPR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Stochastic block mirror descent methods for nonsmooth and stochastic optimization", "author": ["Cong D. Dang", "Guanghui Lan"], "venue": "Technical report, University of Florida,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yurii Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Learning optimally sparse support vector machines", "author": ["Andrew Cotter", "Shai Shalev-Shwartz", "Nati Srebro"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Least squares revisited: Scalable approaches for multi-class prediction", "author": ["A. Agarwal", "S. Kakade", "N. Karampatziakis", "L. Song", "G. Valiant"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "On data preconditioning for regularized loss minimization", "author": ["Tianbao Yang", "Rong Jin", "Shenghuo Zhu"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Quasi-monte carlo feature maps for shift-invariant kernels", "author": ["Jiyan Yang", "Vikas Sindhwani", "Haim Avron", "Michael W. Mahoney"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Learning by stretching deep networks", "author": ["Gaurav Pandey", "Ambedkar Dukkipati"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Kernel methods for deep learning", "author": ["Youngmin Cho", "Lawrence K. Saul"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Many works, such as Greedy basis selection techniques [1], Nystr\u00f6m approximation [2] and incomplete Cholesky decomposition [3], all followed this strategy.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Many works, such as Greedy basis selection techniques [1], Nystr\u00f6m approximation [2] and incomplete Cholesky decomposition [3], all followed this strategy.", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "Many works, such as Greedy basis selection techniques [1], Nystr\u00f6m approximation [2] and incomplete Cholesky decomposition [3], all followed this strategy.", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "In fact, without further assumption on the regularity of the kernel matrix, the generalization ability after low-rank approximation is typically of the order O(1/ \u221a r + 1/ \u221a n) [4, 5], which implies that the rank needs to be nearly linear in the number of data points! Thus, in order for kernel methods to achieve the best generalization ability, the low-rank approximation based approaches quickly become impractical for big datasets due to their O(n + nd) preprocessing time and O(n) memory requirement.", "startOffset": 177, "endOffset": 183}, {"referenceID": 4, "context": "In fact, without further assumption on the regularity of the kernel matrix, the generalization ability after low-rank approximation is typically of the order O(1/ \u221a r + 1/ \u221a n) [4, 5], which implies that the rank needs to be nearly linear in the number of data points! Thus, in order for kernel methods to achieve the best generalization ability, the low-rank approximation based approaches quickly become impractical for big datasets due to their O(n + nd) preprocessing time and O(n) memory requirement.", "startOffset": 177, "endOffset": 183}, {"referenceID": 5, "context": "Random feature approximation is another popular approach for scaling up kernel methods [6, 7].", "startOffset": 87, "endOffset": 93}, {"referenceID": 6, "context": "Random feature approximation is another popular approach for scaling up kernel methods [6, 7].", "startOffset": 87, "endOffset": 93}, {"referenceID": 7, "context": "Similar to low-rank kernel matrix approximation approach, the generalization ability of random feature approach is of the order O(1/ \u221a r+1/ \u221a n) [8, 9], which implies that the number of random features also needs to be O(n).", "startOffset": 145, "endOffset": 151}, {"referenceID": 8, "context": "Similar to low-rank kernel matrix approximation approach, the generalization ability of random feature approach is of the order O(1/ \u221a r+1/ \u221a n) [8, 9], which implies that the number of random features also needs to be O(n).", "startOffset": 145, "endOffset": 151}, {"referenceID": 9, "context": ", [10, 11, 12]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": ", [10, 11, 12]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 11, "context": ", [10, 11, 12]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": ", [13, 15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [13, 15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 15, "context": "The key intuitions behind our algorithm originate from (i) the property of stochastic gradient descent algorithm that as long as the stochastic gradient is unbiased, the convergence of the algorithm is guaranteed [16]; and", "startOffset": 213, "endOffset": 217}, {"referenceID": 15, "context": "More specifically, both in expectation and with high probability, our algorithm can estimate the optimal function in the RKHS in the rate of O(1/t), which are indeed optimal [16], and achieve a generalization bound of O(1/ \u221a t).", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": ",[17]; [18]) If k(x, x\u2032) is a PD kernel, then there exists a set \u03a9, a measure P on \u03a9, and random feature \u03c6\u03c9(x) : X 7\u2192 R from L2(\u03a9,P), such that k(x, x\u2032) = \u222b \u03a9 \u03c6\u03c9(x)\u03c6\u03c9(x \u2032) dP(\u03c9).", "startOffset": 1, "endOffset": 5}, {"referenceID": 17, "context": ",[17]; [18]) If k(x, x\u2032) is a PD kernel, then there exists a set \u03a9, a measure P on \u03a9, and random feature \u03c6\u03c9(x) : X 7\u2192 R from L2(\u03a9,P), such that k(x, x\u2032) = \u222b \u03a9 \u03c6\u03c9(x)\u03c6\u03c9(x \u2032) dP(\u03c9).", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "For Gaussian RBF kernel, k(x \u2212 x\u2032) = exp(\u2212\u2016x \u2212 x\u2032\u20162/2\u03c32), this yields a Gaussian distribution P(\u03c9) with density proportional to exp(\u2212\u03c3\u2016\u03c9\u2016/2); for the Laplace kernel, this yields a Cauchy distribution; and for the Martern kernel, this yields the convolutions of the unit ball [20].", "startOffset": 275, "endOffset": 279}, {"referenceID": 19, "context": "Similar representation where the explicit form of \u03c6\u03c9(x) and P(\u03c9) are known can also be derived for rotation invariant kernel, k(x, x\u2032) = k(\u3008x, x\u2032\u3009), using Fourier transformation on sphere [20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "For polynomial kernels, k(x, x\u2032) = (\u3008x, x\u2032\u3009+ c), a random tensor sketching approach can also be used [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "Explicit random features have been designed for many other kernels, such as dot product kernel [33], additive/multiplicative class of homogeneous kernels [34], e.", "startOffset": 95, "endOffset": 99}, {"referenceID": 33, "context": "Explicit random features have been designed for many other kernels, such as dot product kernel [33], additive/multiplicative class of homogeneous kernels [34], e.", "startOffset": 154, "endOffset": 158}, {"referenceID": 34, "context": ", Hellinger\u2019s, \u03c7, Jensen-Shannon\u2019s and Intersection kernel, as well as kernels on Abelian semigroups [35].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": ", [19]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": "The former random features define a hierachical kernel [45], and the latter random features induce a linear combination of multiple kernels.", "startOffset": 55, "endOffset": 59}, {"referenceID": 33, "context": "It is worth to note that the Hellinger\u2019s, \u03c7, Jensen-Shannon\u2019s and Intersection kernels in [34] are special cases of multiple kernels combination.", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "Many kernel methods can be written as convex optimizations over functions in the RKHS and solved using the functional gradient methods [13, 15].", "startOffset": 135, "endOffset": 143}, {"referenceID": 14, "context": "Many kernel methods can be written as convex optimizations over functions in the RKHS and solved using the functional gradient methods [13, 15].", "startOffset": 135, "endOffset": 143}, {"referenceID": 5, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 3, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 5, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 2, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 2, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 1, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 0, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 2, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 3, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 2, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 4, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 3, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 4, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 15, "context": "The first key intuition behind our algorithm originates from the property of stochastic gradient descent algorithm that as long as the stochastic gradient is unbiased, the convergence of the algorithm is guaranteed [16].", "startOffset": 215, "endOffset": 219}, {"referenceID": 13, "context": "Remark: [14] used squared hinge loss, l(u, y) = 12 max{0, 1\u2212 uy} , in `2-SVM.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "Huber\u2019s loss is used for robust regression [22] where", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "The loss function l(u, \u03c4) = max{0, \u03c4\u2212u} [23] is proposed for novelty detection.", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": ", Ep [ r( q p ) ] , where r : R \u2192 R is a convex function with r(1) = 0, [24] proposed a nonparametric estimator for the logarithm of the density ratio, log q p , which is the solution of following convex optimization, argmin f\u2208H Eq[exp(f)] + Ep[r(\u2212 exp(f))] + \u03bd 2 \u2016f\u2016H (8) where r\u2217 denotes the Fenchel-Legendre dual of r, r(\u03c4) := sup\u03c7 \u03c7\u03c4 \u2212 r\u2217(\u03c7).", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "[24] proposed another convex optimization based on rKL(\u03c4) whose solution is a nonparametric estimator for the density ratio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] designed rnv(\u03c4) = max(0, \u03c1 \u2212 log \u03c4) for novelty detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "reference in [16]) shows that to obtain -accuracy solution, the number of iterations needed for the stochastic approximation is \u03a9(1/ ) for strongly convex case and \u03a9(1/ ) for general convex case.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "For example, techniques for reducing variance of SGD proposed in [37], mini-batch and preconditioning [41, 42] can be used to reduce the constant factors in the bound significantly.", "startOffset": 65, "endOffset": 69}, {"referenceID": 39, "context": "For example, techniques for reducing variance of SGD proposed in [37], mini-batch and preconditioning [41, 42] can be used to reduce the constant factors in the bound significantly.", "startOffset": 102, "endOffset": 110}, {"referenceID": 40, "context": "For example, techniques for reducing variance of SGD proposed in [37], mini-batch and preconditioning [41, 42] can be used to reduce the constant factors in the bound significantly.", "startOffset": 102, "endOffset": 110}, {"referenceID": 3, "context": "Thus the required number of random feature (or ranks), r, will be of the order O(n) = O(1/ ) [4, 5, 8, 9].", "startOffset": 93, "endOffset": 105}, {"referenceID": 4, "context": "Thus the required number of random feature (or ranks), r, will be of the order O(n) = O(1/ ) [4, 5, 8, 9].", "startOffset": 93, "endOffset": 105}, {"referenceID": 7, "context": "Thus the required number of random feature (or ranks), r, will be of the order O(n) = O(1/ ) [4, 5, 8, 9].", "startOffset": 93, "endOffset": 105}, {"referenceID": 8, "context": "Thus the required number of random feature (or ranks), r, will be of the order O(n) = O(1/ ) [4, 5, 8, 9].", "startOffset": 93, "endOffset": 105}, {"referenceID": 12, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 158, "endOffset": 162}, {"referenceID": 25, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 508, "endOffset": 512}, {"referenceID": 36, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 553, "endOffset": 557}, {"referenceID": 37, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 602, "endOffset": 606}, {"referenceID": 38, "context": ", hinge-loss, there are algorithms proposed to achieve better memory saving with extra training cost, such as support vector reduction technique [40].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "3M 2850 [0, 13] no guarantees.", "startOffset": 8, "endOffset": 15}, {"referenceID": 29, "context": "For datasets (9) and (10), we compare with the neural net described in [30] and use exactly the same input.", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "In this experiment, we compare to a variant of LeNet-5 [32], where all tanh units are replaced with rectified linear units.", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "In this experiment, we compare to a neural net with two convolution layers (after contrast normalization and max-pooling layers) and two local layers that achieves 11% test error on CIFAR 10 [28].", "startOffset": 191, "endOffset": 195}, {"referenceID": 28, "context": "The jointly-trained neural net is Alex-net [29].", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "5 Regression Comparisons to Neural Networks We test our algorithm for kernel ridge regression with neural network proposed in [30] on two large-scale real-world regression datasets, (9) and (10) in Table 4.", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "In this experiment, we use the same binary representations converted based on random Coulomb matrices as in [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "We use the same feature representation as for \u201cQuantumMachine\u201d dataset [30].", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "It is straightforward to replace the sampling strategy for random features with Fastfood [7] which enjoys the efficient computational cost, or Quasi-Monte Carlo sampling [43], data-dependent sampling [47] which enjoys faster convergence rate with fewer generated features.", "startOffset": 89, "endOffset": 92}, {"referenceID": 41, "context": "It is straightforward to replace the sampling strategy for random features with Fastfood [7] which enjoys the efficient computational cost, or Quasi-Monte Carlo sampling [43], data-dependent sampling [47] which enjoys faster convergence rate with fewer generated features.", "startOffset": 170, "endOffset": 174}], "year": 2015, "abstractText": "The general perception is that kernel methods are not scalable, and neural nets are the methods of choice for large-scale nonlinear learning problems. Or have we simply not tried hard enough for kernel methods? Here we propose an approach that scales up kernel methods using a novel concept called \u201cdoubly stochastic functional gradients\u201d. Our approach relies on the fact that many kernel methods can be expressed as convex optimization problems, and we solve the problems by making two unbiased stochastic approximations to the functional gradient, one using random training points and another using random features associated with the kernel, and then descending using this noisy functional gradient. Our algorithm is simple, does not need to commit to a preset number of random features, and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting. We show that a function learned by this procedure after t iterations converges to the optimal function in the reproducing kernel Hilbert space in rate O(1/t), and achieves a generalization performance of O(1/ \u221a t). Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show that our method can achieve competitive performance to neural nets in datasets such as 2.3 million energy materials from MolecularSpace, 8 million handwritten digits from MNIST, and 1 million photos from ImageNet using convolution features.", "creator": "LaTeX with hyperref package"}}}