{"id": "1408.0055", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2014", "title": "Thurstonian Boltzmann Machines: Learning from Multiple Inequalities", "abstract": "We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture that can naturally incorporate a wide range of data inputs at the same time. Our motivation rests in the Thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables, and in the observation that each realisation of a discrete type imposes certain inequalities on those variables. Thus learning and inference in TBM reduce to making sense of a set of inequalities. Our proposed TBM naturally supports the following types: Gaussian, intervals, censored, binary, categorical, muticategorical, ordinal, (in)-complete rank with and without ties. We demonstrate the versatility and capacity of the proposed model on three applications of very different natures; namely handwritten digit recognition, collaborative filtering and complex social survey analysis.", "histories": [["v1", "Fri, 1 Aug 2014 00:32:32 GMT  (903kb,D)", "http://arxiv.org/abs/1408.0055v1", "Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&amp;CP volume 28"]], "COMMENTS": "Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&amp;CP volume 28", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["truyen tran 0001", "dinh q phung", "svetha venkatesh"], "accepted": true, "id": "1408.0055"}, "pdf": {"name": "1408.0055.pdf", "metadata": {"source": "META", "title": "Thurstonian Boltzmann Machines: Learning from Multiple Inequalities", "authors": ["Truyen Tran", "Dinh Phung", "Svetha Venkatesh"], "emails": ["truyen.tran@deakin.edu.au", "dinh.phung@deakin.edu.au", "svetha.venkatesh@deakin.edu.au"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2 Gaussian RBM", "text": "s distribution P (x, h) = 1Z exp {\u2212 E (x, h)} (1), where Z = \u2211 h'exp {\u2212 E (x, h)} dx is the normalizing constant and E (x, h) is the state energy. Energy is decomposed asE (x, h) = 1 Z exp {\u2212 i (x2i 2 \u2212 (i + Wi \u00b7 h) xi (x, h) \u2212 P (x, h)."}, {"heading": "3 Thurstonian Boltzmann Machines", "text": "We now generalize Gaussian RBM into the Thurston Boltzmann Machine (TBM). Use e to denote an observed proof for x. Standard proofs are the point assignment of x to a specific real value vector, i.e. x = e. Generalized proofs can be expressed by means of inequality limitations: b \u2264 Ax \u2264 c (5) for a transformation matrix A-RM \u00b7 N and vectors b, c-RM, where \u2264 are elementary inequalities. Thus, a proof can be fully realized by specifying the three < A, b, c >, for example, for the point assignment < A = I, b = e, c = e >, where I am the identity matrix. Below we will detail other useful popular realizations of these quantities."}, {"heading": "3.1 Boxed Constraints", "text": "This refers to the case where input variables are independently limited, i.e. A = I, and therefore we only need to specify the pair < b, c >.censored observations, referring to a situation where we only know the continuous observation from a certain point, i.e. b = e and c = + \u221e. If the measurements are inaccurate, it may be better, for example, to specify the range of possible observations with greater reliability than a single point, i.e. b = e thresholds and c = e + \u03b4 for some pairs (e, g)."}, {"heading": "3.2 Inequality Constraints", "text": "This refers to the situation in which out of a disordered set of categories, we consider only one category at a time = 1. This can be formulated as follows: Each category is associated with a \"utility variable.\" Category l is observed (i.e., ei = m) when it has the greatest benefit, i.e., xil \u2265 6 = l xim. Thus, xil is the upper limit for all other commodities. On the other hand, maxm 6 = l xim is the lower threshold for xil. This indicates an EM-like procedure: (i) fix xil (or treat it as a threshold) and learn the model under the intervals xim xil for all m 6 = l, and (ii) fix all categories except l, learn the model under the intervals xil-xil observations xim. This provides an alternative to multinomial logit treatment in [26].To illustrate the point, let us assume that there are only four variables, 1, 2 we are observed."}, {"heading": "4 Inference Under Linear Constraints", "text": "Under the TBM, the MCMC-based conclusion without evidence is simple: We alternate between P (h | x) and P (x | h), which is efficient because of the factorisations in equations (3,4). Conclusions on evidence of inequality e are, however, much more involved, except in the limiting case of point assignments. Name the limited range of x = {x | b \u2264 Ax \u2264 c}, which is defined by the evidence e. Now, we must specify the limited distribution P (x, h | e) and select from it a random sample defined on the basis of B (e). The sample P (h | x) remains unchanged, and in the following, we will focus on the sample of P (x | h, e)."}, {"heading": "4.1 Inference under Boxed Constraints", "text": "With boxed constraints (section 3.1) we still enjoy the factorisation P (x | h, e) = \u0441i P (xi | h, e) due to the conditional independence. Furthermore, we have P (xi | h, e) = P (xi | h) \u03a6 (ci | h) \u2212 \u03a6 (bi | h), where \u03a6 (\u00b7 | h) is the normal cumulative distribution function of P (xi | h, e). Now P (xi | h, e) is an abbreviated normal distribution from which we can derive the simple repulsion method or more advanced methods such as those in [23]."}, {"heading": "4.2 Inference under Inequality Constraints", "text": "For general inequality constraints (section 3.2), the input variables are mutually dependent due to the linear transformation A. However, we can specify the conditional distribution P (xi | x \u00ac i, h, e) (here x \u00ac i = x\\ xi) by realizing that the input variables are conditioned by other variables. This indicates a Gibbs method by meandering through x1, x2,..., xN. In a certain notation misuse, b-mi = (bm-j 6 = iAmjxj) / Ami and c-mi = (cm-j 6 = iAmyxj) / Ami can be summarized by asxi-Mm = (bm-j 6 = iAmyxj) / Ami and c-mi = (cm-j 6 = iAmyxj) / Ami."}, {"heading": "4.3 Estimating the Binary Posteriors", "text": "In contrast to the standard RBMs described above, the latent variables here are coupled by the unknown Gaussians and therefore there are no exact solutions, unless the proofs are point allocations. The MCMC-based techniques described above offer an approximate estimate by averaging the samples {h (s)} S = 1. In the case of boxed limitations, midfield offers an alternative approach that can be numerically faster. In particular, the center field actualizations are recursive: Qk \u2190 1 + exp {\u2212 \u03b3k \u2212 iWiki} \u00b5i VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV"}, {"heading": "4.4 Estimating Probability of Evidence Generation", "text": "Considering the hidden states h, we want to estimate the probability that hidden states provide a specific proof eP (e | h) = \u03a6 (e) P (x | h) dxFor hidden constraints, an analytical solution is available, since the Gaussian variables are decoupled, i.e. P (ei | h) = \u03a6 (ci \u2212 \u00b5i) \u2212 \u03a6 (bi \u2212 \u00b5i), where \u00b5i = \u03b1i + \u2211 kWikhk. However, for general inequality constraints, these variables are coupled by the inequalities. The general strategy is to prick from P (x | h) and calculate the proportion of samples falling within the restricted range D (e). For certain classes of inequalities, we can approximate the gauze the Gau\u00df by means of appropriate distributions, of which integration has the closed form. In particular, these inequalities, imposed by the categorical and ranking evidence, can be addressed by the use of extreme value distributions."}, {"heading": "5 Stochastic Gradient Learning with Persistent Markov", "text": "ChainsLearning is based on maximizing the evidence probability of L = logP (e) = logP (e) = logP (h, x) dxwo P (h, x) is defined in Equation (1). Leave Z (e) = \u2211 h'ig (e) exp {\u2212 E (x, h)} dx, then L = logZ (e) \u2212 logZ. The gradient w.r.t. of the mapping parameter is Forward WikL = EP (xi, hk | e) [xihk] \u2212 EP (xi, hk) [xihk] (8) The derivation is left to the supplement."}, {"heading": "5.1 Estimating Data Statistics", "text": "The data-dependent statistics EP (xi, hk | e) [xihk] and the data-independent statistics EP (xi, hk) [xihk] are generally not traceable, and therefore approximations are required.Data-dependent statistics. Under the limitations of the box, the method of the middle field (Section 4.3) can be applied as follows: EP (xi, hk | e) [xihk]. This would allow us to explore the space of the data-dependent distribution P (x, h | e) by alternating between P (h | x) and P (x | h, e), using techniques described in Section 4.Data instance. Data-independent statistics are not suitable for exploring the entire state space because they tend to fit into one mode. A practical solution is based on the idea that a particular chain of divergences (which we maintain after the set-up chain) where the most recent data-independent statistics are discarded."}, {"heading": "5.2 Learning the Box Boundaries", "text": "The gradient of the log probability at the lower limits is: \"biL = 1Z (e)\" \"h\" bi \"(e)\" exp \"{\u2212 E (x, h)\" dx = \"h\" (e) \"P (x, h | e)\" (x, h | e) \"P (x, h | e)\" (x, h | e), \"where\" h (s) \"s = 1\" samples are collected during the MCMC procedure, which proceeds according to the data-dependent distribution P (x, h | e)."}, {"heading": "6 Applications", "text": "In this section, we will describe TBM applications in three realistic areas, namely handwritten number recognition, collaborative filtering, and global survey analysis. Before going into detail, let's first address the most important implementation issues (see supplement for more details).However, one observed difficulty in training TBM is that the hidden samples can get stuck in one of the two goals, and therefore no progress can be made. Reasons for this could be the large mapping parameters or the unlimited nature of the underlying Gaussian variables that can saturate the hidden units. We can control the standard of mapping parameters either by using the standard \"2-standard regulation\" or by recalculating the standard of the parameter vector for each hidden unit."}, {"heading": "6.1 Probit RBM for Handwritten Digits", "text": "We use the name Probit RBM to indicate the special case of the TBM in which the observations are binary (i.e., boxed constraints, see Section 3.1).The threshold \u03b8i for each visible unit i is chosen so that below the mean of zero the probability of generating a binary proof corresponding to empirical probability, i.e. we do not need to update the thresholds any further. We report here on the result of the midpoint method for calculating data-dependent statistics averaged over a random stack of 500 frames. For the data-independent statistics, 500 persistent chains are executed in parallel to the samples collected after all 5 Gibbs steps. The spareness level of reductions is set to 0.3 and the sensitivity weight to 0.5. Once the model is learned, the mean intensity of classically used 3% is estimated (i.e., boxed constraints, see Section 3.1)."}, {"heading": "6.2 Rank Evidences for Collaborative Filtering", "text": "In recent years, the number of those who are able to reform has multiplied, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "6.3 Mixed Evidences for World Attitude Analysis", "text": "Finally, we show the TBM on mixed evidence. The data comes from the field of survey analysis, which consists largely of several questions of different nature, such as basic facts (e.g. age and gender) and opinions (e.g. binary decisions, individual decisions, multiple decisions, ordinal judgments, preferences and rankings).The standard approach to dealing with such heterogeneities is to perform the so-called \"encoding,\" which converts types into some numerical representations (e.g. ordinal scales into stars, rankings into several pairs of comparisons) so that standard processing tools can deal with them. However, this coding process disrupts the structure of the data and thus significant information is lost. Thus, our TBM provides a scalable and generic machine to process the data in its original format and then convert the mixed types into a more homogeneous way that can be intergenerated."}, {"heading": "7 Related Work", "text": "Latent multivariate Gaussian variables have been extensively studied in statistical analysis, first to model correlated binary data [1, 4], then for a variety of data types such as ordered categories [15], disordered categories [43], and the mix of types [6]. Learning with the underlying Gaussian model is notoriously difficult for large-scale translations: independent sampling costs due to the need to reverse the covariance matrix [39, 14], while MCMC techniques such as Gibbs sampling can be very slow when the graph is dense and the interactions between variables are strong. This can be partially overcome by adding another layer of latent variables, as in the factor analysis [39, 14] and the probable principle component analysis [33]. The main difference from our TBM models is that these models are guided by continuous factors, while our models are not governed by Gausal factors such as BB41."}, {"heading": "8 Discussion and Conclusion", "text": "Since the underlying variables of the TBM are Gaussian, various extensions can be made without much difficulty. For example, direct correlations between variables, regardless of their types, can be easily modeled by introducing the non-identifying covariance matrix [22]. This is clearly a good choice for image modeling, as nearby pixels are strongly correlated. Another situation is when the input units are associated with their own attributes. That is, we cannot model the role of rows and columns by a linear combination of attributes in the middle structure of the Gaussian. The additive nature of the mean structure allows the natural extension of matrix modeling (e.g. see [37, 35]. That is, we do not distinguish the role of rows and columns, and can therefore be modeled with their own hidden units (the row parameters and columns are different)."}, {"heading": "A Supplementary Material", "text": "A.1 Conclusion"}, {"heading": "A.1.1 Estimating the Partition Function", "text": "For convenience, we parameterise the distribution as the following (xi) = following (xi) = following (p) = following (p) = following (p). The model potential is then the product of all local potentials (x, h) = previous (xi, hk) = previous (hk) = previous (hk)] (10) The partition function can then be used as a product of all local potentials (x, h) = previous (h) = previous (h) [previous (h). We now proceed with the calculation of the distribution (h): previous (h) = previous (h)."}, {"heading": "A.1.2 Estimating Posteriors using Mean-field", "text": "Recall that we want to make an estimate for the evidence e of the following constraints: P (h = Q = Q = Q = Q = Q (e) (h) (h, x | e). Suppose the evidence can be expressed in terms of constraints leading to the following factorization P (x | e, h) = P (xi | e, h). This factorization is crucial because it ensures that there are no deterministic constraints under {xi} ni = 1, which are the conditions under which variation methods such as midfields would work well. This is because midfield solutions generally will not satisfy deterministic constraints, and therefore the probability cannot be cerebral that the midfield approximation Q (h, x) Qi (h, x | e) Qi (h) Qi (h, x) Qi (h) Qi (h) Qi (h), x) Qi (k) (hi)."}, {"heading": "A.1.3 Seeking Modes and Generating Representative Samples", "text": "Once the model is learned, samples can be easily generated by first sampling the underlying Gaussian RBM and then collecting the true samples that meet the inequalities of interest. For example, if the Gaussian value generated for a visible unit is greater than the threshold, we have an active sample. Likewise, for rank samples, we only have to rank the collected Gaussian values. However, these may suffer from poor mixing if we use standard Gibbs samples, i.e. the Markov chain can get stuck in some energy traps. To jump out of the trap, we suggest raising the temperature periodically to a certain level (e.g. 10) and then slowly cooling down to the original temperature (which is 1). In our experiment, cooling is provided as a result of T instead."}, {"heading": "A.2 Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.2.1 Gradient of the Likelihood", "text": "The log probability of a proof is L = logP (e) = logZ (e) = logZ (e) = logZ (e) = \"h\" (e) exp {\u2212 E (x, h)} dx. The gradient of logZ (e) w.r.t. the mapping parameter Wik reads \"Wik logZ (e) = \u2212 1 Z (e).h\" exp {\u2212 E (x, h)} dx. The gradient of logZ (e) w.r.t. the mapping parameter Wik reads \"Wik logZ (e) = \u2212 1 Z (e).h\" exp \"(e) {\u2212 E (x, h) \u0445 WikE\" (x, h).h \".\" p \".p\". \"p.\" p. \"p.\" dx (16).p (e) where we have the constant Z \u2212 1 (e) in the sum of. \"k\" and the fact that. \"k\" p. \"and.\" p."}, {"heading": "A.2.2 Regularising the Markov Chains", "text": "An undesirable feature of the MCMC chains we have used so far in learning is the tendency for the binary hidden states to hang, i.e., after a certain point, they do not reverse their assignments during the learning process. We suspect that this phenomenon could be due to the saturation effect inherent in the posterior factor: P (hk = 1 | x) = 1 + exp (\u2212 \u03b3k \u2212 \u2211 iWikxi), i.e., once the accumulated value of a node (\u03b3k + \u2211 iWikxi) is too high or too low, it is very difficult to reverse it. Fortunately, there is a known technique for regulating the chain: We force that there should be only a fraction of active nodes at a given time, where there should be a fraction of active nodes, the latter (0, 1) being too high or too low. One way is to maximize the following objective function L2 logk + k k k k + k k k = kk (kk = P hp) where the latter (x x hp) x x is hp (1)."}, {"heading": "A.2.3 Online Estimation of Posteriors", "text": "For tasks such as data completion (e.g. collaborative filtering) we need the rear P (h | e) for the predictive phase. One possibility is to recalculate the Markov chain or the interpretation field from scratch. Here, we propose a simple method to get an approximation directly from the training phase at no additional cost. The idea is to update the estimated rear h exponentially smoothing with each learning step t: h (t), (t), (t \u2212 1), h (t), (t) for a smoothing factor \u03b7 (0, 1) and initial h (0), where h (t) k = P (h 1 k | x (t), e) and x (t) the sampled Gausses are weighted exponentially in due time."}, {"heading": "A.2.4 Monitoring the Learning Progress", "text": "It is often of practical importance to track the learning progress, either on the basis of the reconstruction errors or on the basis of the data probability. The data probability can be estimated as P (e) \u2248 1 S \u0445 s = 1 \u0435\u0441\u0442 (e) P (x | h (s)) dxwhere h (s) are those samples that are collected in the course of the learning progress in the data-independent phase, and the integration can be done with the technique described in the main text."}, {"heading": "A.3 Extreme Value Distributions", "text": "Extreme value distributions are a class of distributions of extreme parameters [11]."}, {"heading": "A.3.1 Gumbel Distribution for Categorical Choices", "text": "Let's start with the Gumbel density functionP (x) = 1\u03c3 exp = > p = > p (x \u2212 \u00b5 + e \u2212 x \u2212 p), where \u00b5 is the mode (location) and \u03c3 is the scale parameter.Using the approximation of Laplace (e.g. by Taylor's expansion of (x \u2212 \u00b5 + e \u2212 x \u2212 p) using the second order polynomial (x), we get the standard Gaussian distribution. \u2212 l We now turn to the categorical model using Gumbel variables. We maintain one variable per category that plays the role of utility for the category. xxxxm, let's assume that all utilities have the same scale parameters."}, {"heading": "A.3.2 Gumbel Distribution for Rank", "text": "We now extend the case of categorical evidence to the precedence of evidence. Again, we maintain one Gaussian variable per category. Without losing generality, we assume for a given rank \u03c0 that we must ensure that x1 > x2 >... > xD. This corresponds to [x1 > max l > 1 xl], [x2 > maxl > 2 xl], [xD \u2212 1 > xD] The probability for this is essentially P ({em = m} Dm = 1) = P (e1 = 1). m > 2 P (el = m | {ed = l} m \u2212 1l = 1) In other words, this provides a step-by-step method for ranking the categories: First, we select the best category, the second best category from the remaining categories, and so on (see also [7]). The probability of picking the best category from a subset is already given in Annex A.3.1: P (e1 = 1 / 3 l = 1 \u00b5p = 1 \u00b5p = 1 \u00b5p / m)."}, {"heading": "A.4 Global Attitude: Sample Questions", "text": "\u2022 Q4 (Ordinary): [...] how would you describe the current economic situation in (survey country) - \"very good, something good, something bad or very bad\"? \u2022 Q11a (Binary): How do you think that people in other countries of the world think about China? - \"Do I like, do I dislike\"? \u2022 Q35,35a (Category Ranking): Which of the following, if any, harms the environment most / second most {India, Germany, China, Brazil, Japan, USA, Russia, Others}? \u2022 Q76 (Continuous): How old were you on your last birthday? \u2022 Q85 (Category): What is your current employment situation?"}, {"heading": "A.5 Other Supporting Materials", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.5.1 Laplace Approximation", "text": "Laplace approximation is the technique that uses a Gaussian distribution to approximate a different distribution. Let's assume the univariate case where the original density distribution has the FormP (x) \u0445 exp {\u2212 f (x)}. First, we find the mode \u00b5 of P (x) or equivalent the minimizer of f (x) as it exists. Then, we apply Taylor's expansion f (x) \u2248 f (\u00b5) + f \u00b2 (\u00b5) (x \u2212 \u00b5) 22Gaussian approximation has the FormP \u0445 (x) \u0445 exp {\u2212 f \u2032 (\u00b5) (\u00b5) (\u00b5) (x \u2212 \u00b5) 22}, where 1 / f \u2032 (\u00b5) is the new variance."}, {"heading": "A.5.2 Some Properties of the Truncated Normal Distribution", "text": "For a normal distribution P (x | \u00b5, \u03c3) of the mean \u00b5 and the standard deviation \u03c3, which is cut off from both sides, i.e. \u03b1 < x < \u03b2, the new density value P [\u03b1, \u03b2] (x |, \u00b5, \u03c3) = Q (x *) \u03c3 [\u03a6 (\u03b2 *) \u2212 \u03a6 (\u03b1 *)] results, whereby the probability density function and the cumulative distribution of the standard normal distribution are always the same. In particular, we are interested in the mean value of the distribution quantity P [\u03b1, \u03b2]: \u00b5 = \u00b5i + \u03c3 Q (\u03b2) \u2212 Q (\u03b2 *) \u0445 (\u03b2 * *). Some special cases: \u2022 If \u03b1 = \u03b2, this distribution is reduced to the delta of the dirac. \u2022 If we have a one-sided truncation from top to bottom (\u03b2 *), then we get a truncation from bottom to bottom (\u03b2 *)."}], "references": [{"title": "Multi-variate probit analysis", "author": ["JR Ashford", "RR Sowden"], "venue": "Biometrics, pages 535\u2013 546,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1970}, {"title": "Thurstonian-based analyses: past, present, and future utilities", "author": ["U. B\u00f6ckenholt"], "venue": "Psychometrika, 71(4):615\u2013629,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Expected reciprocal rank for graded relevance", "author": ["O. Chapelle", "D. Metlzer", "Y. Zhang", "P. Grinspan"], "venue": "CIKM, pages 621\u2013630. ACM,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Analysis of multivariate probit models", "author": ["S. Chib", "E. Greenberg"], "venue": "Biometrika, 85(2):347\u2013361,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "The nonnegative Boltzmann machine", "author": ["O.B. Downs", "D.J.C. MacKay", "D.D. Lee"], "venue": "Advances in Neural Information Processing Systems, 12:428\u2013434,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Bayesian latent variable models for mixed discrete outcomes", "author": ["D.B. Dunson", "A.H. Herring"], "venue": "Biostatistics, 6(1):11,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Multistage ranking models", "author": ["M.A. Fligner", "J.S. Verducci"], "venue": "Journal of the American Statistical Association, 83(403):892\u2013901,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Unsupervised learning of distributions on binary vectors using two layer networks", "author": ["Y. Freund", "D. Haussler"], "venue": "Advances in Neural Information Processing Systems, pages 912\u2013919,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "The rate adapting Poisson model for information retrieval and object recognition", "author": ["P.V. Gehler", "A.D. Holub", "M. Welling"], "venue": "Proceedings of the ICML, pages 337\u2013344,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient simulation from the multivariate normal and student-t distributions subject to linear constraints and the evaluation of constraint probabilities", "author": ["J. Geweke"], "venue": "Computing science and statistics: Proceedings of the 23rd symposium on the interface, pages 571\u2013578,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1991}, {"title": "Statistical of extremes", "author": ["EJ Gumbel"], "venue": "Columbia University Press, New York,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1958}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS), 20(4):446,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Variational bounds for mixed-data factor analysis", "author": ["E. Khan", "B. Marlin", "K. Murphy"], "venue": "Proc. of Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric Bayesian modeling for multivariate ordinal data", "author": ["A. Kottas", "P. M\u00fcller", "F. Quintana"], "venue": "Journal of Computational and Graphical Statistics, 14(3):610\u2013 625,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning a generative model of images by factoring appearance and shape", "author": ["N. Le Roux", "N. Heess", "J. Shotton", "J. Winn"], "venue": "Neural Computation, 23(3):593\u2013650,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Individual choice behavior", "author": ["R.D. Luce"], "venue": "Wiley New York,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1959}, {"title": "Conditional logit analysis of qualitative choice behavior", "author": ["D. McFadden"], "venue": "Frontiers in Econometrics, pages 105\u2013142,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1973}, {"title": "Annealed importance sampling", "author": ["R.M. Neal"], "venue": "Statistics and Computing, 11(2):125\u2013139,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "ICML,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "The analysis of permutations", "author": ["R.L. Plackett"], "venue": "Applied Statistics, pages 193\u2013202,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1975}, {"title": "Modeling pixel means and covariances using factorized third-order Boltzmann machines", "author": ["M.A. Ranzato", "G.E. Hinton"], "venue": "CVPR, pages 2551\u20132558. IEEE,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Simulation of truncated normal variables", "author": ["C.P. Robert"], "venue": "Statistics and computing, 5(2):121\u2013125,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Deep Boltzmann Machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Proceedings of 20th AISTATS, volume 5, pages 448\u2013455,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Replicated softmax: an undirected topic model", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 22,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Restricted Boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th ICML, pages 791\u2013798,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "List-wise learning to rank with matrix factorization for collaborative filtering", "author": ["Y. Shi", "M. Larson", "A. Hanjalic"], "venue": "ACM RecSys, pages 269\u2013272. ACM,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "Parallel distributed processing: Explorations in the microstructure of cognition, 1:194\u2013281,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1986}, {"title": "Multimodal learning with deep Boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "NIPS, pages 2231\u20132239,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Models for distributions on permutations", "author": ["H. Stern"], "venue": "Journal of the American Statistical Association, 85(410):558\u2013564,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1990}, {"title": "A law of comparative judgment", "author": ["L.L. Thurstone"], "venue": "Psychological review, 34(4):273,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1927}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "Proceedings of the 25th ICML, pages 1064\u20131071,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society: Series B, 61(3):611\u2013622,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1999}, {"title": "Mixed-variate restricted Boltzmann machines", "author": ["T. Tran", "D.Q. Phung", "S. Venkatesh"], "venue": "Proc. of 3rd Asian Conference on Machine Learning (ACML), Taoyuan, Taiwan,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Cumulative restricted Boltzmann machines for ordinal matrix data analysis", "author": ["T. Tran", "D.Q. Phung", "S. Venkatesh"], "venue": "Proc. of 4th Asian Conference on Machine Learning (ACML), Singapore,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic models over ordered partitions with applications in document ranking and collaborative filtering", "author": ["T. Truyen", "D.Q Phung", "S. Venkatesh"], "venue": "Proc. of SIAM Conference on Data Mining (SDM), Mesa, Arizona, USA,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Ordinal Boltzmann machines for collaborative filtering", "author": ["T.T. Truyen", "D.Q. Phung", "S. Venkatesh"], "venue": "Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI), Montreal, Canada, June", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Factor analysis with (mixed) observed and latent variables in the exponential family", "author": ["M. Wedel", "W.A. Kamakura"], "venue": "Psychometrika, 66(4):515\u2013530,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2001}, {"title": "Mining associated text and images with dual-wing harmoniums", "author": ["E. Xing", "R. Yan", "A.G. Hauptmann"], "venue": "Proceedings of the 21st UAI,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "Boltzmann machines with bounded continuous random variables", "author": ["M. Yasuda", "K. Tanaka"], "venue": "Interdisciplinary Information Sciences, 13(1):25\u201331,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Parametric inference for imperfectly observed Gibbsian fields", "author": ["L. Younes"], "venue": "Probability Theory and Related Fields, 82(4):625\u2013645,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1989}, {"title": "Bayesian analysis of multivariate nominal measures using multivariate multinomial probit models", "author": ["X. Zhang", "W.J. Boscardin", "T.R. Belin"], "venue": "Computational statistics & data analysis, 52(7):3697\u20133708,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 11, "context": "1 Introduction Restricted Boltzmann machines (RBMs) have proved to be a versatile tool for a wide variety of machine learning tasks and as a building block for deep architectures [12, 24, 28].", "startOffset": 179, "endOffset": 191}, {"referenceID": 23, "context": "1 Introduction Restricted Boltzmann machines (RBMs) have proved to be a versatile tool for a wide variety of machine learning tasks and as a building block for deep architectures [12, 24, 28].", "startOffset": 179, "endOffset": 191}, {"referenceID": 27, "context": "1 Introduction Restricted Boltzmann machines (RBMs) have proved to be a versatile tool for a wide variety of machine learning tasks and as a building block for deep architectures [12, 24, 28].", "startOffset": 179, "endOffset": 191}, {"referenceID": 11, "context": "Recent extensions to other data types result in type-dependent models: the Gaussian for continuous inputs [12], Beta for bounded continuous inputs [16], Poisson for count data [9], multinomial for unordered categories [25], and ordinal models for ordered categories [37, 35].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "Recent extensions to other data types result in type-dependent models: the Gaussian for continuous inputs [12], Beta for bounded continuous inputs [16], Poisson for count data [9], multinomial for unordered categories [25], and ordinal models for ordered categories [37, 35].", "startOffset": 147, "endOffset": 151}, {"referenceID": 8, "context": "Recent extensions to other data types result in type-dependent models: the Gaussian for continuous inputs [12], Beta for bounded continuous inputs [16], Poisson for count data [9], multinomial for unordered categories [25], and ordinal models for ordered categories [37, 35].", "startOffset": 176, "endOffset": 179}, {"referenceID": 24, "context": "Recent extensions to other data types result in type-dependent models: the Gaussian for continuous inputs [12], Beta for bounded continuous inputs [16], Poisson for count data [9], multinomial for unordered categories [25], and ordinal models for ordered categories [37, 35].", "startOffset": 218, "endOffset": 222}, {"referenceID": 36, "context": "Recent extensions to other data types result in type-dependent models: the Gaussian for continuous inputs [12], Beta for bounded continuous inputs [16], Poisson for count data [9], multinomial for unordered categories [25], and ordinal models for ordered categories [37, 35].", "startOffset": 266, "endOffset": 274}, {"referenceID": 34, "context": "Recent extensions to other data types result in type-dependent models: the Gaussian for continuous inputs [12], Beta for bounded continuous inputs [16], Poisson for count data [9], multinomial for unordered categories [25], and ordinal models for ordered categories [37, 35].", "startOffset": 266, "endOffset": 274}, {"referenceID": 19, "context": "The work of [20, 29, 40] combines continuous (e.", "startOffset": 12, "endOffset": 24}, {"referenceID": 28, "context": "The work of [20, 29, 40] combines continuous (e.", "startOffset": 12, "endOffset": 24}, {"referenceID": 39, "context": "The work of [20, 29, 40] combines continuous (e.", "startOffset": 12, "endOffset": 24}, {"referenceID": 33, "context": "The work of [34] extends the idea further to incorporate ordinal and rank data.", "startOffset": 12, "endOffset": 16}, {"referenceID": 30, "context": "one or several underlying continuous variables, in the spirit of Thurstonian models [31], and (ii) evidences be expressed in the form of one or several inequalities of these underlying variables.", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "In psychology and economics, for example, it gives much better interpretation on why a particular choice is made given the perceived utilities [2].", "startOffset": 143, "endOffset": 146}, {"referenceID": 27, "context": "Thus, this model offers an alternative to standard binary RBMs of [28, 8].", "startOffset": 66, "endOffset": 73}, {"referenceID": 7, "context": "Thus, this model offers an alternative to standard binary RBMs of [28, 8].", "startOffset": 66, "endOffset": 73}, {"referenceID": 36, "context": "This offers an alternative to the ordinal RBMs of [37].", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "This offers an alternative to the multinomial logit treatment in [26].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "Now P (xi | h, e) is a truncated normal distribution, from which we can sample using the simple rejection method, or more advanced methods such as those in [23].", "startOffset": 156, "endOffset": 160}, {"referenceID": 9, "context": "For more sophisticated Gibbs procedures, we refer to the work in [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 41, "context": "In particular, we maintain one persistent Markov chain [42, 32] per data instance and estimate the statistics after a very short run.", "startOffset": 55, "endOffset": 63}, {"referenceID": 31, "context": "In particular, we maintain one persistent Markov chain [42, 32] per data instance and estimate the statistics after a very short run.", "startOffset": 55, "endOffset": 63}, {"referenceID": 41, "context": "If it is not the case, then we can maintain a moderate set of parallel chains and collect the samples after a short run at every updating step [42, 32].", "startOffset": 143, "endOffset": 151}, {"referenceID": 31, "context": "If it is not the case, then we can maintain a moderate set of parallel chains and collect the samples after a short run at every updating step [42, 32].", "startOffset": 143, "endOffset": 151}, {"referenceID": 26, "context": "MF [27] and PMOP [36].", "startOffset": 3, "endOffset": 7}, {"referenceID": 35, "context": "MF [27] and PMOP [36].", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "Two ranking metrics from the information retrieval literature are used: the ERR [3] and the NDCG@T [13].", "startOffset": 80, "endOffset": 83}, {"referenceID": 12, "context": "Two ranking metrics from the information retrieval literature are used: the ERR [3] and the NDCG@T [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 37, "context": "Figure 4 shows the 2D distribution of respondents from 24 countries obtained by feeding the posteriors to the t-SNE [38] (here no explicit information of countries is used).", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "7 Related Work Latent multivariate Gaussian variables have been widely studied in statistical analysis, initially to model correlated binary data [1, 4] then now used for a variety of data types such as ordered categories [15], unordered categories [43], and the mixture of types [6].", "startOffset": 146, "endOffset": 152}, {"referenceID": 3, "context": "7 Related Work Latent multivariate Gaussian variables have been widely studied in statistical analysis, initially to model correlated binary data [1, 4] then now used for a variety of data types such as ordered categories [15], unordered categories [43], and the mixture of types [6].", "startOffset": 146, "endOffset": 152}, {"referenceID": 14, "context": "7 Related Work Latent multivariate Gaussian variables have been widely studied in statistical analysis, initially to model correlated binary data [1, 4] then now used for a variety of data types such as ordered categories [15], unordered categories [43], and the mixture of types [6].", "startOffset": 222, "endOffset": 226}, {"referenceID": 42, "context": "7 Related Work Latent multivariate Gaussian variables have been widely studied in statistical analysis, initially to model correlated binary data [1, 4] then now used for a variety of data types such as ordered categories [15], unordered categories [43], and the mixture of types [6].", "startOffset": 249, "endOffset": 253}, {"referenceID": 5, "context": "7 Related Work Latent multivariate Gaussian variables have been widely studied in statistical analysis, initially to model correlated binary data [1, 4] then now used for a variety of data types such as ordered categories [15], unordered categories [43], and the mixture of types [6].", "startOffset": 280, "endOffset": 283}, {"referenceID": 38, "context": "This can be partly overcome by adding one more layer of latent variables as in factor analysis [39, 14] and probabilistic principle component analysis [33].", "startOffset": 95, "endOffset": 103}, {"referenceID": 13, "context": "This can be partly overcome by adding one more layer of latent variables as in factor analysis [39, 14] and probabilistic principle component analysis [33].", "startOffset": 95, "endOffset": 103}, {"referenceID": 32, "context": "This can be partly overcome by adding one more layer of latent variables as in factor analysis [39, 14] and probabilistic principle component analysis [33].", "startOffset": 151, "endOffset": 155}, {"referenceID": 11, "context": "Gaussian RBMs have been used for modelling continuous data such as visual features [12], where the evidences are the value assignments, and thus a limiting case of our evidence system.", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "Some restrictions to the continuous Boltzmann machines have been studied: In [5], Gaussian variables are assumed to be non-negative, and in [41], continuous variables are bounded.", "startOffset": 77, "endOffset": 80}, {"referenceID": 40, "context": "Some restrictions to the continuous Boltzmann machines have been studied: In [5], Gaussian variables are assumed to be non-negative, and in [41], continuous variables are bounded.", "startOffset": 140, "endOffset": 144}, {"referenceID": 34, "context": "GRBMs that handle ordinal evidences have been studied in [35], which is an instance of the boxed-constraints in our TBM.", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "For example, direct correlations among variables, regardless of their types, can be readily modelled by introducing the non-identity covariance matrix [22].", "startOffset": 151, "endOffset": 155}, {"referenceID": 36, "context": ", see [37, 35]).", "startOffset": 6, "endOffset": 14}, {"referenceID": 34, "context": ", see [37, 35]).", "startOffset": 6, "endOffset": 14}, {"referenceID": 18, "context": "Now we apply the Annealed Importance Sampling (AIS) [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "3 Extreme Value Distributions Extreme value distributions are a class of distributions of extremal measurements [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 17, "context": "The existing literature [18] asserts that the probability of choosing the m-th category is", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "In words, this offers a stagewise process to rank categories: first we pick the best category, the pick the second best from the remaining categories and so on (see also [7]).", "startOffset": 170, "endOffset": 173}, {"referenceID": 16, "context": "P (e1 = 1) = e1 \u2211 l\u22651 e \u03bcl/\u03c3 P ( el = m | {ed = l} l=1 ) = em \u2211 l\u2265m e \u03bcl/\u03c3 This gives us the Plackett-Luce model [17, 21] as mentioned in [30].", "startOffset": 113, "endOffset": 121}, {"referenceID": 20, "context": "P (e1 = 1) = e1 \u2211 l\u22651 e \u03bcl/\u03c3 P ( el = m | {ed = l} l=1 ) = em \u2211 l\u2265m e \u03bcl/\u03c3 This gives us the Plackett-Luce model [17, 21] as mentioned in [30].", "startOffset": 113, "endOffset": 121}, {"referenceID": 29, "context": "P (e1 = 1) = e1 \u2211 l\u22651 e \u03bcl/\u03c3 P ( el = m | {ed = l} l=1 ) = em \u2211 l\u2265m e \u03bcl/\u03c3 This gives us the Plackett-Luce model [17, 21] as mentioned in [30].", "startOffset": 138, "endOffset": 142}], "year": 2014, "abstractText": "We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture that can naturally incorporate a wide range of data inputs at the same time. Our motivation rests in the Thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables, and in the observation that each realisation of a discrete type imposes certain inequalities on those variables. Thus learning and inference in TBM reduce to making sense of a set of inequalities. Our proposed TBM naturally supports the following types: Gaussian, intervals, censored, binary, categorical, muticategorical, ordinal, (in)-complete rank with and without ties. We demonstrate the versatility and capacity of the proposed model on three applications of very different natures; namely handwritten digit recognition, collaborative filtering and complex social survey analysis.", "creator": "LaTeX with hyperref package"}}}