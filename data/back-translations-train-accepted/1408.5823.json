{"id": "1408.5823", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2014", "title": "Improved Distributed Principal Component Analysis", "abstract": "We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve problems such as $k$-means clustering and low rank approximation. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for $k$-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as a general transformation from a constant success probability subspace embedding to a high success probability subspace embedding with a dimension and sparsity independent of the success probability, may be of independent interest.", "histories": [["v1", "Mon, 25 Aug 2014 16:24:43 GMT  (292kb,D)", "http://arxiv.org/abs/1408.5823v1", "23 pages"], ["v2", "Mon, 1 Sep 2014 01:32:30 GMT  (292kb,D)", "http://arxiv.org/abs/1408.5823v2", "23 pages"], ["v3", "Thu, 13 Nov 2014 14:51:43 GMT  (312kb,D)", "http://arxiv.org/abs/1408.5823v3", "23 pages"], ["v4", "Sun, 21 Dec 2014 01:13:00 GMT  (346kb,D)", "http://arxiv.org/abs/1408.5823v4", null], ["v5", "Tue, 23 Dec 2014 03:17:19 GMT  (346kb,D)", "http://arxiv.org/abs/1408.5823v5", null]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yingyu liang", "maria-florina balcan", "vandana kanchanapally", "david p woodruff"], "accepted": true, "id": "1408.5823"}, "pdf": {"name": "1408.5823.pdf", "metadata": {"source": "CRF", "title": "Improved Distributed Principal Component Analysis", "authors": ["Maria-Florina Balcan", "Vandana Kanchanapally", "Yingyu Liang", "David Woodruff"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that it is a matter of a way in which people must put themselves and themselves at the centre. (...) Indeed, it is such that people are able to understand the world. (...) It is not so that they are able to see the world. (...) It is such that they cannot understand the world. (...) It is such that they do not understand the world. (...) It is such that they cannot understand the world. (...) It is such that they cannot understand the world. (...) It is such that they cannot understand the world. (...) It is such that they do not understand the world. (...) \"(...)\" () \"(\") \"(...\") \"(\") \"(...\") (\") (\") (\") (\") ((...) (\") (\") ((\") ((...) (\") (\") (() (()) (()) ((()) (())) ((())) (()) (()) ()) (()) () () () () ()) () () () () ()) ()) () () () () ()) () () ()) () () () ()) () () ()) () () () () () () () ()) () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() () () (() () () (() (() () (() () () (() (() () () (() () () (() () () (() (() (() () () ((() () () ((() () () (() (() () (((() () (() () () ((("}, {"heading": "2 Preliminaries", "text": "The global data collection P-Rn \u00b7 d is then a concatenation of the local data matrix, i.e. P > s] and n = 1 ni. Let pi denote thei th line by P. Thier assumes that the data points are centered to have zero mean, i.e., P > s] and n = 1 ni. Let pi denote thei th line by P. Throughs the paper, we are centered to have zero mean, i.e., Uncentered data needs a rank-one modification to the algorithms, whose communication and calculation costs are dominated by those in the other steps."}, {"heading": "3 Tradeoff between Communication and Solution Quality", "text": "In the global phase, the coordination data matrix (Vi (t1)) > to form a matrix Y (t1), and the first 1) -Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- Vectoring- VectoringVecing- VectoringVecing- Vectoring- Vectoring- Vectoring- VectoringVecing- Vectoring- Vectoring- Vectoring- VectoringVecing- Vectoring- Vectoring- VectoringVecing- Vectoring- Vectoring- VecingVectoring- VecingVectoring- VectoringVecing- Vectoring- Vecing- VectoringVecingVectoring- VectoringVecing- VectoringVectoring- Vecing- VectoringVectoring- VecingVecing- VectoringVectoring- VectoringVectoring- Vecing- VecingVectoringVectoringVectoring- VecingVectoring- VecingVectoring- VecingVectoring- Vectoring- Vecing- VectoringVectoring- Vectoring- Vectoring- VectoringVectoring- VectoringVectoring- V"}, {"heading": "4 Fast Distributed PCA", "text": "The newest, fastest and most optimized data (Meng and Mahatma, 2013) and their optimizations (Meng and Mahatma, 2013) are able to significantly improve the time of distributed PCA algorithms by limiting themselves to a certain number of data, while relying on a limited number of data necessary for all vectors and their predisposition. (D) It is not only a random subspace, but also a random subspace for its local data. (D) It is a random subspace that we select for its local data. (D) Then they trace algorithms disPCA to the embedded data. (D) It is the work of (Sarlo) s, 2006).The most recent, fast subspace beds (Clarkson and Woodruff, 2013) and their optimizations (Meng and Mahatma, 2013)."}, {"heading": "5 Experiments", "text": "We have written to the flags that we will be able to go in search of the solution, \"he told the Deutsche Presse-Agentur in an interview with the\" S\u00fcddeutsche Zeitung \"(Saturday).\" We have written to the flags, \"he said,\" but we have written to the flags. \"(\" The flags \"),\" The flags \"(\" The flags \"),\" The flags \"(\" The flags \"),\" The flags \"(\" The flags \"),\" The flags \"(\" The flags \"),\" The flags (\"The flags\"), \"The flags (\" The flags \"),\" (\"The flags\"), \"(\" The flags \"),\" (The flags), \"(The flags),\" (The flags), \"(The flags),\" (The flags), \"(The flags),\" (The flags), \"(The flags),\" (The flags), \"(The flags)."}, {"heading": "A Guarantees for Distributed PCA", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Lemma 1", "text": "First of all, we prove a generalization of Lemma 1.Lemma 7. Let A-Rn-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D"}, {"heading": "A.2 Proof of Theorem 2", "text": "Theorem 2. Suppose the algorithm disPCA takes the parameters t1 \u2265 r + > d4r / e \u2212 1 and t2 = r, and outputs V (r), then the communication becomes O (srd) (V (r)) > 2F \u2264 (1 +) min X d2 (P, LX) where the minimization is via d \u00b7 r orthonormal matrices X. The communication is O (srd) words.Proof. Leave P (t) i (t) i (t) i) i (P, LX) >, and let P (p) conceive of P (i.First we show that P (srd) is the proxy of P for the optimization of d2 (P, LX).According to the Pythagorean theorem, Lemorem, for each oral matrix of size d \u00b7 r, d2 (P, LX) \u2212 d2 (P) \u2212 d2 are the proxy of P for the optimization of d2 (LP)."}, {"heading": "Furthermore, \u03c4(A, r, ) = O( r ).", "text": "Evidence. Note that the limit on t is only used in the test (2) for which t > \u03c4 (A, r,) suffice. \u03c4 (A, r,) = O (r) follows by definition.Theorem 9. Suppose algorithm disPCA takes the parameters t1 \u2265 maxi \u03c4 (Pi, r,) and t2 = r and outputs V (r). Then it takes the parameters P \u2212 PV (r) (V (r) > 2F \u2264 (1 +) min X d2 (P, LX), where minimization via ormal matrices X \u2212 Rd \u00b7 r. The total communication costs are O (sdmaxi \u0445 (Pi, r,)) word.The comparative data (Pi, r,) are typically much lower than O (r /) in practice. This provides an explanation for the fact that t1 is much smaller than O (r /) in many practical cases can still lead to a good solution."}, {"heading": "B.1 Proof of Lemma 4", "text": "Let us first introduce some intermediate variables for our analysis. - Let us imagine we perform two extrapolations: first project Pi to P = Pi = Pi (t) (Vi (t)) >, then project P + i to Pi = P (t) (V (t)) > where t = t1 = t2. Let us leave P + d8k / e \u2212 1 in algorithm disPCA for k + and P \u2212 X (0, 1). \u2212 P (V (t) and P = P1... P = P1... Ps Lemma 4. Let us leave t1 = t2 P + d8k / e \u2212 1 in algorithm disPCA for k + and P (0, 1). Then for each d \u00b7 k matrix X with lemonormal X."}, {"heading": "B.2 Proof of Theorem 3", "text": "The following weak triangular relationship is useful for our analysis. \u2212 Face 1: \u2212 Fact 1: \u2212 Fact 1: \u2212 Fact 1: \u2212 Fact 1: \u2212 Fact 1: \u2212 Fact 1: \u2212 Fact 1: \u2212 Fact 1: \u2212 Face 1: \u2212 Face 1: \u2212 Face 1: \u2212 Face 2: \u2212 Face 1: \u2212 Face 2: \u2212 Face 2: \u2212 Face 2: \u2212 Face 2: \u2212 Face 2: Face 2: Face 2 \u2212 Face 2: \u2212 Face 2: Face 2 \u2212 Face 2: \u2212 Face 2 \u2212 Face 2: \u2212 Face 2: \u2212 Face 2 \u2212 Face 2: \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2: \u2212 Face 2 \u2212 Face 2: \u2212 Face 2 \u2212 Face 2: \u2212 Face 2 \u2212 Face 2 \u2212 Face 2: \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2: Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2: Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2: Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 1: Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 1: Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 1: Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 1: Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2: Face 2 \u2212 Face 2 \u2212 Face 2 \u2212 Face 2 \u2212"}, {"heading": "C Fast Distributed PCA", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Proofs for Subspace Embedding", "text": "The construction of the embedding matrix H is represented in algorithm 4 (A). Note: The embedding matrix H does not need to be explicitly constructed (A). \u2212 Note: The embedding matrix H does not need to be explicitly designated (A). \u2212 Note: The algorithm matrix H does not need to be explicitly designated (A). \u2212 Note: The algorithm matrix H does not need to be explicitly designated (A). \u2212 Note: The algorithm matrix H does not need to be explicitly designated (A). \u2212 Note: The algorithm matrix P must be at least 99 / 100, - HAy-2 for all vectors y-Rd. \u2212 Note: The HA matrix P can be calculated in time (A). \u2212 Note: The algorithm matrix P must be at least 99 / 100, - HAy-2 for all vectors."}, {"heading": "C.2 Proofs for Randomized SVD", "text": "The details of the randomized SVD are presented in algorithm 6, which has been reformulated in our notations. We have the following analogy to Lemma 1. Lemma 15. Let A-R'x-d-Matrix be ('> d) Let A-R'x-d-Matrix be (' > d) Let A-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T"}, {"heading": "C.3 Proof of Theorem 6", "text": "Let T have a diagonal block matrix with H1, H2,., Hs on the diagonal., Then algorithm 3 is just to run algorithm disPCA on TP to get the main components. Remember that the goal is to show that PVV > is a good proxy for the original data P in terms of \"2 error matching problems.\" It is enough to show that P + P + P, P + P, P, PVV > is a good proxy for the original data P in terms of \"2 error matching problems.\" That these properties are obtained approximately when they replace exact SVD with randomized SVD in algorithm disPCA (Lemma 16). Then we can show that P, TP, X, X, X, X, X, TP, TP, TP, TP, TP, TP, TP, TP, X, X, X, X, X, X, X, X, X, X, X."}], "references": [{"title": "Principal component analysis for distributed data sets with updating", "author": ["Zheng-Jian Bai", "Raymond H Chan", "Franklin T Luk"], "venue": "In Proceedings of the International Conference on Advanced Parallel Processing Technologies,", "citeRegEx": "Bai et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2005}, {"title": "Distributed k-means and k-median clustering on general communication topologies", "author": ["Maria-Florina Balcan", "Steven Ehrlich", "Yingyu Liang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Balcan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2013}, {"title": "Stochastic dimensionality reduction for k-means clustering", "author": ["Christos Boutsidis", "Anastasios Zouzias", "Michael W. Mahoney", "Petros Drineas"], "venue": "CoRR, abs/1110.2897,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Spanner: Googles globallydistributed database", "author": ["James C Corbett", "Jeffrey Dean", "Michael Epstein", "Andrew Fikes", "Christopher Frost", "JJ Furman", "Sanjay Ghemawat", "Andrey Gubarev", "Christopher Heiser", "Peter Hochschild"], "venue": "In Proceedings of the USENIX Symposium on Operating Systems Design and Implementation,", "citeRegEx": "Corbett et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Corbett et al\\.", "year": 2012}, {"title": "A unified framework for approximating and clustering data", "author": ["Dan Feldman", "Michael Langberg"], "venue": "In Proceedings of the Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Feldman and Langberg.,? \\Q2011\\E", "shortCiteRegEx": "Feldman and Langberg.", "year": 2011}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "author": ["Dan Feldman", "Melanie Schmidt", "Christian Sohler"], "venue": "In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Feldman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2013}, {"title": "Relative errors for deterministic low-rank matrix approximations", "author": ["Mina Ghashami", "Jeff M. Phillips"], "venue": "CoRR, abs/1307.7454,", "citeRegEx": "Ghashami and Phillips.,? \\Q2013\\E", "shortCiteRegEx": "Ghashami and Phillips.", "year": 2013}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp"], "venue": "SIAM review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Nimble algorithms for cloud computing", "author": ["Ravindran Kannan", "Santosh Vempala", "David Woodruff"], "venue": "arXiv preprint arXiv:1304.3162,", "citeRegEx": "Kannan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2013}, {"title": "Combining structured and unstructured randomness in large scale pca", "author": ["Nikos Karampatziakis", "Paul Mineiro"], "venue": "CoRR, abs/1310.6304,", "citeRegEx": "Karampatziakis and Mineiro.,? \\Q2013\\E", "shortCiteRegEx": "Karampatziakis and Mineiro.", "year": 2013}, {"title": "Distributed principal component analysis for wireless sensor", "author": ["Yann-A\u00ebl Le Borgne", "Sylvain Raybaud", "Gianluca Bontempi"], "venue": "networks. Sensors,", "citeRegEx": "Borgne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Borgne et al\\.", "year": 2008}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Daniel D. Lee", "H. Sebastian Seung"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Lee and Seung.,? \\Q2001\\E", "shortCiteRegEx": "Lee and Seung.", "year": 2001}, {"title": "Consensus-based distributed principal component analysis in wireless sensor networks", "author": ["Sergio Valcarcel Macua", "Pavle Belanovic", "Santiago Zazo"], "venue": "In Proceedings of the IEEE International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),", "citeRegEx": "Macua et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Macua et al\\.", "year": 2010}, {"title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression", "author": ["Xiangrui Meng", "Michael W Mahoney"], "venue": "In Proceedings of the Annual ACM symposium on Symposium on theory of computing,", "citeRegEx": "Meng and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Meng and Mahoney.", "year": 2013}, {"title": "Characterizing web-based video sharing workloads", "author": ["Siddharth Mitra", "Mayank Agrawal", "Amit Yadav", "Niklas Carlsson", "Derek Eager", "Anirban Mahanti"], "venue": "ACM Transactions on the Web,", "citeRegEx": "Mitra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2011}, {"title": "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L Nguy\u00ean"], "venue": "arXiv preprint arXiv:1211.1002,", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2012\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2012}, {"title": "Adaptive filters for continuous queries over distributed data streams", "author": ["Chris Olston", "Jing Jiang", "Jennifer Widom"], "venue": "In Proceedings of the ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Olston et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Olston et al\\.", "year": 2003}, {"title": "Principal component analysis for dimension reduction in massive distributed data sets", "author": ["Yongming Qu", "George Ostrouchov", "Nagiza Samatova", "Al Geist"], "venue": "In Proceedings of IEEE International Conference on Data Mining,", "citeRegEx": "Qu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2002}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tam\u00e1s Sarl\u00f3s"], "venue": "In FOCS,", "citeRegEx": "Sarl\u00f3s.,? \\Q2006\\E", "shortCiteRegEx": "Sarl\u00f3s.", "year": 2006}, {"title": "The algorithm runs in time O(qt`d+ t2(`+ d))", "author": ["\u2016AX\u2016F"], "venue": "(Halko et al.,", "citeRegEx": ".,? \\Q2011\\E", "shortCiteRegEx": ".", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "Since data is often partitioned across multiple servers (Olston et al., 2003; Corbett et al., 2012; Mitra et al., 2011), there is an increased interest in computing on it in the distributed model.", "startOffset": 56, "endOffset": 119}, {"referenceID": 4, "context": "Since data is often partitioned across multiple servers (Olston et al., 2003; Corbett et al., 2012; Mitra et al., 2011), there is an increased interest in computing on it in the distributed model.", "startOffset": 56, "endOffset": 119}, {"referenceID": 15, "context": "Since data is often partitioned across multiple servers (Olston et al., 2003; Corbett et al., 2012; Mitra et al., 2011), there is an increased interest in computing on it in the distributed model.", "startOffset": 56, "endOffset": 119}, {"referenceID": 12, "context": "Moreover, it can serve as a preprocessing step to reduce the data dimension in various machine learning tasks, such as k-means, Non-Negative Matrix Factorization (NNMF) (Lee and Seung, 2001) and Latent Dirichlet Allocation (LDA) (Blei et al.", "startOffset": 169, "endOffset": 190}, {"referenceID": 6, "context": "A beautiful property of the coresets developed in (Feldman et al., 2013) is that for approximate PCA their size also only depends linearly on the dimension d, whereas previous coresets depended quadratically on d (Feldman and Langberg, 2011).", "startOffset": 50, "endOffset": 72}, {"referenceID": 5, "context": ", 2013) is that for approximate PCA their size also only depends linearly on the dimension d, whereas previous coresets depended quadratically on d (Feldman and Langberg, 2011).", "startOffset": 148, "endOffset": 176}, {"referenceID": 4, "context": ", 2003; Corbett et al., 2012; Mitra et al., 2011), there is an increased interest in computing on it in the distributed model. A basic tool for distributed data analysis is Principal Component Analysis (PCA). The goal of PCA is to find an r-dimensional (affine) subspace that captures as much of the variance of the data as possible. Hence, it can reveal lowdimensional structure in very high dimensional data. Moreover, it can serve as a preprocessing step to reduce the data dimension in various machine learning tasks, such as k-means, Non-Negative Matrix Factorization (NNMF) (Lee and Seung, 2001) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). In the distributed model, approximate PCA was used by Feldman et al. (2013) for solving a number of shape fitting problems such as k-means clustering, where the approximation PCA solution is computed based on a summary of the data called coreset.", "startOffset": 8, "endOffset": 737}, {"referenceID": 6, "context": "For approximate distributed PCA, the following protocol is implicit in (Feldman et al., 2013): each server i computes its top O(r/ ) principal components Yi of Pi and sends them to the coordinator.", "startOffset": 71, "endOffset": 93}, {"referenceID": 6, "context": "5 in (Feldman et al., 2013), which shows that given a data matrix P, if we project the rows onto the space spanned by the top O(k/ 2) principal components, and solve the k-means problem in this subspace, we obtain a (1 + )approximation.", "startOffset": 5, "endOffset": 27}, {"referenceID": 1, "context": "One feature of this approach is that by using the distributed k-means algorithm in (Balcan et al., 2013) on the projected data, the coordinator can sample points from the servers proportional to their local k-means cost solutions, which reduces the communication roughly by a factor of s in the k-means step, which would come from each server sending their local k-means coreset to the coordinator.", "startOffset": 83, "endOffset": 104}, {"referenceID": 2, "context": "For example, if we want a 1 + approximation factor, we can set d\u2032 = O(log n/ 2) by a Johnson-Lindenstrauss transform; if we want a larger 2 + approximation factor, we can set d\u2032 = O(k/ 2) using (Boutsidis et al., 2011).", "startOffset": 194, "endOffset": 218}, {"referenceID": 19, "context": "to instead have each server first sample an oblivious subspace embedding (OSE) (Sarl\u00f3s, 2006; Clarkson and Woodruff, 2013; Nelson and Nguy\u00ean, 2012; Meng and Mahoney, 2013) matrix Hi, and instead run the algorithm on the point set defined by the rows of HiPi.", "startOffset": 79, "endOffset": 171}, {"referenceID": 3, "context": "to instead have each server first sample an oblivious subspace embedding (OSE) (Sarl\u00f3s, 2006; Clarkson and Woodruff, 2013; Nelson and Nguy\u00ean, 2012; Meng and Mahoney, 2013) matrix Hi, and instead run the algorithm on the point set defined by the rows of HiPi.", "startOffset": 79, "endOffset": 171}, {"referenceID": 16, "context": "to instead have each server first sample an oblivious subspace embedding (OSE) (Sarl\u00f3s, 2006; Clarkson and Woodruff, 2013; Nelson and Nguy\u00ean, 2012; Meng and Mahoney, 2013) matrix Hi, and instead run the algorithm on the point set defined by the rows of HiPi.", "startOffset": 79, "endOffset": 171}, {"referenceID": 14, "context": "to instead have each server first sample an oblivious subspace embedding (OSE) (Sarl\u00f3s, 2006; Clarkson and Woodruff, 2013; Nelson and Nguy\u00ean, 2012; Meng and Mahoney, 2013) matrix Hi, and instead run the algorithm on the point set defined by the rows of HiPi.", "startOffset": 79, "endOffset": 171}, {"referenceID": 16, "context": "This number of rows can be further reducted to O(d log d/ 2) if one is willing to spend O(nnz(Pi) log d/ ) time (Nelson and Nguy\u00ean, 2012).", "startOffset": 112, "endOffset": 137}, {"referenceID": 18, "context": "Related Work A number of algorithms for approximate distributed PCA have been proposed (Qu et al., 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication.", "startOffset": 87, "endOffset": 188}, {"referenceID": 0, "context": "Related Work A number of algorithms for approximate distributed PCA have been proposed (Qu et al., 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication.", "startOffset": 87, "endOffset": 188}, {"referenceID": 13, "context": "Related Work A number of algorithms for approximate distributed PCA have been proposed (Qu et al., 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication.", "startOffset": 87, "endOffset": 188}, {"referenceID": 6, "context": "Related Work A number of algorithms for approximate distributed PCA have been proposed (Qu et al., 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication.", "startOffset": 87, "endOffset": 188}, {"referenceID": 6, "context": "Most closely related to our work is (Feldman et al., 2013), which observes that the top singular vectors of the local point set can be viewed as its summary and the union of the local summaries can be viewed as a summary of the global data, i.", "startOffset": 36, "endOffset": 58}, {"referenceID": 9, "context": "In (Kannan et al., 2013) the authors study algorithms in the arbitrary partition model in which each server holds a matrix Pi and P = \u2211s i=1 Pi.", "startOffset": 3, "endOffset": 24}, {"referenceID": 9, "context": "Moreover, our k-means algorithms are more general, in the sense that they do not make a well-separability assumption, and more efficient in that the communication of (Kannan et al., 2013) is O(sd2) + s(k/ )O(1) words as opposed to our O(sdk/ 2) + sk + (k/ )O(1).", "startOffset": 166, "endOffset": 187}, {"referenceID": 7, "context": "Other related work includes the recent (Ghashami and Phillips, 2013) (see also the references therein), who give a deterministic streaming algorithm for low rank approximation in which each point of P is seen one at a time and uses O(dk/ ) words of communication.", "startOffset": 39, "endOffset": 68}, {"referenceID": 0, "context": ", 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication. Qu et al. (2002) proposed an algorithm but provided no analysis on the tradeoff between communication and approximation.", "startOffset": 8, "endOffset": 191}, {"referenceID": 10, "context": "Speeding up large scale PCA using different versions of subspace embeddings was also considered in (Karampatziakis and Mineiro, 2013), though not in a distributed setting and not for `2-error shape fitting problems.", "startOffset": 99, "endOffset": 133}, {"referenceID": 18, "context": "Algorithm disPCA for distributed PCA is suggested in (Qu et al., 2002; Feldman et al., 2013), which consists of a local stage and a global stage.", "startOffset": 53, "endOffset": 92}, {"referenceID": 6, "context": "Algorithm disPCA for distributed PCA is suggested in (Qu et al., 2002; Feldman et al., 2013), which consists of a local stage and a global stage.", "startOffset": 53, "endOffset": 92}, {"referenceID": 6, "context": "These lead to the following theorem, which is implicit in (Feldman et al., 2013), stating that the algorithm can produce a (1 + )-approximation for the distributed PCA problem.", "startOffset": 58, "endOffset": 80}, {"referenceID": 1, "context": "2: Run the distributed k-means clustering algorithm in (Balcan et al., 2013) on {PiEE}i=1, usingA\u03b1 as a subroutine, to get k centers L.", "startOffset": 55, "endOffset": 76}, {"referenceID": 1, "context": "As a concrete example, we can use original data (d\u2032 = d), then run Algorithm disPCA, and finally run the distributed clustering algorithm in (Balcan et al., 2013) which uses any non-distributed \u03b1-approximation algorithm as a subroutine and computes a (1 + )\u03b1-approximate solution.", "startOffset": 141, "endOffset": 162}, {"referenceID": 19, "context": "The work of (Sarl\u00f3s, 2006) pioneered subspace embeddings.", "startOffset": 12, "endOffset": 26}, {"referenceID": 3, "context": "The recent fast sparse subspace embeddings (Clarkson and Woodruff, 2013) and its optimizations (Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) are particularly suitable for large scale sparse data sets, since their running time is linear in the number of non-zero entries in the data matrix, and they also preserve the sparsity of the data.", "startOffset": 43, "endOffset": 72}, {"referenceID": 14, "context": "The recent fast sparse subspace embeddings (Clarkson and Woodruff, 2013) and its optimizations (Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) are particularly suitable for large scale sparse data sets, since their running time is linear in the number of non-zero entries in the data matrix, and they also preserve the sparsity of the data.", "startOffset": 95, "endOffset": 144}, {"referenceID": 16, "context": "The recent fast sparse subspace embeddings (Clarkson and Woodruff, 2013) and its optimizations (Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) are particularly suitable for large scale sparse data sets, since their running time is linear in the number of non-zero entries in the data matrix, and they also preserve the sparsity of the data.", "startOffset": 95, "endOffset": 144}, {"referenceID": 8, "context": "Here we show that the randomized SVD algorithm from (Halko et al., 2011) can be applied to speed up the computation without compromising the quality of the solution much.", "startOffset": 52, "endOffset": 72}, {"referenceID": 1, "context": "For k-means, we run the algorithm in (Balcan et al., 2013) (with Lloyd\u2019s method as a subroutine) on the projected data to get a solution.", "startOffset": 37, "endOffset": 58}, {"referenceID": 6, "context": "1 in (Feldman et al., 2013).", "startOffset": 5, "endOffset": 27}, {"referenceID": 6, "context": "The proof follows that in (Feldman et al., 2013), with slight modification for the distributed setting.", "startOffset": 26, "endOffset": 48}, {"referenceID": 3, "context": "Algorithm 4 Fast Sparse Subspace Embedding (Clarkson and Woodruff, 2013) Input: parameters n, ` \u2208 N+.", "startOffset": 43, "endOffset": 72}, {"referenceID": 3, "context": "(Clarkson and Woodruff, 2013; Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) Suppose n > d and ` = O( 2 2 ).", "startOffset": 0, "endOffset": 78}, {"referenceID": 14, "context": "(Clarkson and Woodruff, 2013; Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) Suppose n > d and ` = O( 2 2 ).", "startOffset": 0, "endOffset": 78}, {"referenceID": 16, "context": "(Clarkson and Woodruff, 2013; Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) Suppose n > d and ` = O( 2 2 ).", "startOffset": 0, "endOffset": 78}, {"referenceID": 8, "context": "Algorithm 6 Randomized SVD (Halko et al., 2011) Input: matrix A \u2208 R`\u00d7d; parameters t, q \u2208 N+.", "startOffset": 27, "endOffset": 47}, {"referenceID": 8, "context": "4 in (Halko et al., 2011), with probability at least 1\u2212 3e\u2212t, we have \u2016A\u2212 \u00c2\u2016S \u2264 2\u03c3t+1(A).", "startOffset": 5, "endOffset": 25}], "year": 2017, "abstractText": "We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve problems such as k-means clustering and low rank approximation. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for k-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as a general transformation from a constant success probability subspace embedding to a high success probability subspace embedding with a dimension and sparsity independent of the success probability, may be of independent interest.", "creator": "LaTeX with hyperref package"}}}