{"id": "1408.6179", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2014", "title": "Evaluating Neural Word Representations in Tensor-Based Compositional Settings", "abstract": "We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts, in a number of compositional tasks. We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the larger-scale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks.", "histories": [["v1", "Tue, 26 Aug 2014 16:28:21 GMT  (30kb)", "http://arxiv.org/abs/1408.6179v1", "To be published in EMNLP 2014"]], "COMMENTS": "To be published in EMNLP 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dmitrijs milajevs", "dimitri kartsaklis", "mehrnoosh sadrzadeh", "matthew purver"], "accepted": true, "id": "1408.6179"}, "pdf": {"name": "1408.6179.pdf", "metadata": {"source": "CRF", "title": "Evaluating Neural Word Representations in Tensor-Based Compositional Settings", "authors": ["Dmitrijs Milajevs", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver"], "emails": ["d.milajevs@qmul.ac.uk", "m.sadrzadeh@qmul.ac.uk", "m.purver@qmul.ac.uk", "dimitri.kartsaklis@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 140 8.61 79v1 [cs.CL] 2 6A ug2 01"}, {"heading": "1 Introduction", "text": "Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distribution semantics community and have demonstrated state-of-the-art performance in many natural language processing tasks. While they have been compared to co-occurence-based models in simple similarity tasks (Levy et al., 2014; Baroni et al., 2014), we are only aware of one work that attempts to compare the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared to a neural autoencoder. The purpose of this paper is to provide a more complete picture regarding the potential of neo-ral word embeddings in compositional tasks, and meaningful comparisons with the traditional distribution approach."}, {"heading": "2 Meaning representation", "text": "There are several approaches to the representation of word, phrase and sentence meaning. Since natural languages are very creative and it is very rare to see the same sentence twice, any practical approach dealing with large text segments must be compositional and construct the meaning of phrases and sentences from their components. Therefore, the ideal method would not only express the similarity in meaning between these components, but also between the results of their composition, and do so in a way that matches linguistic texts and generalizations. Formal semantics Formal approaches to the semantics of natural language have long been based on the classical idea of compositivity - that the meaning of a sentence is a function of the meanings of its parts (Frege, 1892). In compositional typlogic approaches, predicate-based structures that represent phrases and sentences that represent phrases and sentences are built from their components by \u03b2reduction within the framework of the John Lambda Calculation Frame (for example, we can create a sleep frame in 1970)."}, {"heading": "3 Compositional models", "text": "It is not that we are looking for another country, but it is that we are looking for another country. It is as if we are looking for another country. It is as if we are looking for another country. It is as if we are looking for another country. It is as if we are looking for another country. It is as if we are looking for another country. It is as if we are looking for another country. It is as if we are looking for another country. It is as if we are looking for another country."}, {"heading": "4 Semantic word spaces", "text": "The vector space is only part of the attention we have received in the scientific community. \"(POS SUBST, VERB, ADJ and ADV in the BNC XML Distribution2). (POS SUBST, ADJ and ADV in the BNC XML Distribution2). The vector space is only part of the attention we have received in the scientific community. (POS SUBST, ADJ and ADV in the BNC XML Distribution2). (POS SUBST, ADJ and ADV in the BNC XML Distribution2). (POS SUBNC, ADJ and ADV in the BNC XML Distribution2). (POS SUBNC, ADJ and ADV in the BNC XML Distribution2)."}, {"heading": "5 Experiments", "text": "Our experiments examine the use of the vector spaces described above, together with the compositional operators described in Section 3, in a series of tasks, all of which require semantic composition: verbal clarification, sentence similarity, transcription and marking of dialog acts."}, {"heading": "5.1 Disambiguation", "text": "We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh 350 (2011a) 5. This dataset consists of ambiguous transitive verbs along with their arguments, landmark verbs identifying one of the verb senses, and human judgments indicating how similar is the unambiguous meaning of the verb in the given context to5This and the sentence similarity dataset are available at http: / / www.cs.ox.ac.uk / activities / compdistmeaning / one of the landmarks. This is similar to the intransitive dataset described in (Mitchell and Lapata, 2008). Consider the sentence \"System meets specification\"; meets is the ambiguous transitive verb, and system and specification are its arguments in this context. Possible landmarks for we fulfill the satisfaction and the visit; for this sentence, the human judgments show that the disambiguous meaning of the verb is more similar to the cosmos and less similar to the task."}, {"heading": "5.2 Sentence similarity", "text": "In this experiment, we used the transitive sentence similarity data set described in Kartsaklis and Sadrzadeh (2014). The data set consists of transitive sentence pairs and a human similarity judgement6. The task is to estimate a measure of similarity between two sentences. As in the disambiguation task, we first put word vectors together to obtain sentence vectors, and then calculate cosinal similarity thereof. We calculate the human judgments for identical sentence pairs in order to calculate a correlation with cosinal scales.Table 4 shows the results. Here, too, the most powerful vector space is KS14, but this time with an addition: The Spearman correlation value with averaged human judgments is 0.732. Addition was the mean for the other vector spaces in order to also achieve peak performance: GS11 and NWE achieved 0.682 and 0.689 points. None of the models in tense-based composition exceeded the addition of the multiplier vector spaces of the SWE with the other SWE-11 and SWE-11."}, {"heading": "5.3 Paraphrasing", "text": "In this experiment we evaluate our vector spaces on a mainstream paraphrase detection task. 6The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgments, while the previous dataset uses the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010).Specifically, we obtain classification results on the Microsoft Research Paraphrase Corpus Paraphrase Corpus (Dolan et al., 2005) works in the following way: We construct vectors for the sentences of each pair; if the cosmic similarity between the two proposition vectors exceeds a certain threshold, the pair is classified as the paraphrase Corpus Paraphrase Corpus Paraphrase Paraphrase Paraphrase Corpus Paraphrase, otherwise not the paraphrase of the paraphrase."}, {"heading": "5.4 Dialogue act tagging", "text": "As our last experiment, we evaluate the word spaces on a dialogue act marking task (Stolcke et al., 2000) via the conciliation committee (Godfrey et al., 1992). Switchboard is a collection of approximately 2500 dialogues via a telephone line of 500 speakers from the USA on predefined topics. 8The experiment pipeline follows (Milajevs and Purver, 2014). The input statements are pre-processed in such a way that the parts of the interrupted utterances are linked to each other (Webb et al., 2005). Disturbance markers and commas are removed from the statement texts. For GS11 and KS14, the statement tokens are marked with POS and lemmatized; for NWE, we test the vectors in both a lemmatized and an un-lemmatized version of the corpus.9 We share the training and the test statements as provided by Stolcke et al. (2000)."}, {"heading": "6 Discussion", "text": "While Baroni et al. (2014) concludes that \"context-dependent predictive models achieve a thorough and resounding victory over their number-based counterparts,\" this seems to contradict, at least at first glance, the more conservative conclusions of Levy et al. (2014) that \"analogy utilization is not limited to neural word embeddings [.] a similar amount of relative similarities can be derived from traditional distributional word representations\" and the results of Blacoe and Lapata (2012) that \"flat approaches are just as good as computer-intensive alternatives\" to phrase equality and paraphrase recognition tasks.It seems clear that neural word embeddings have an advantage when used in tasks for which they were trained; our most important questions are whether they jettison co-precombinations; and what approaches are."}, {"heading": "7 Conclusion", "text": "In this paper, we compared the performance of two co-occurrences based on semantic spaces with vectors learned from a neural network in compositional environments; we performed two small tasks (word disambiguation and sentence similarity) and two large tasks (paraphrase recognition and dialogue tagging); in small tasks where sentence structures are predefined and relatively limited, NWE delivers better or similar results on number-based vectors; however, tensor-based composition does not always surpass simple compositional operators, but in most cases it delivers results within the same range. In large tasks, neural vectors are more successful than occurrence-based alternatives; however, this study does not show whether this is due to their neural nature, or just because they are trained on a larger amount of data."}, {"heading": "Acknowledgements", "text": "We thank the three anonymous reviewers for their fruitful comments. EPSRC grant support EP / F042728 / 1 is gratefully acknowledged by Milajevs, Kartsaklis and Sadrzadeh. Purver is partially supported by ConCreTe: The ConCreTe project recognises the financial support of the Future and Emerging Technologies (FET) programme within the Seventh Framework Programme of the European Commission under FET grant number 611733."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "NLTK: the natural language toolkit", "author": ["Steven Bird."], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions, pages 69\u201372. Association for Computational Linguistics.", "citeRegEx": "Bird.,? 2006", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "First-order inference and the interpretation of questions and answers", "author": ["Johan Bos", "Malte Gabsdil."], "venue": "Proceedings of Gotelog, pages 43\u201350.", "citeRegEx": "Bos and Gabsdil.,? 2000", "shortCiteRegEx": "Bos and Gabsdil.", "year": 2000}, {"title": "Wide-coverage semantic analysis with boxer", "author": ["Johan Bos."], "venue": "Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277\u2013286. College Publi-", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Commutative Algebra: Chapters 1-7", "author": ["N. Bourbaki."], "venue": "Srpinger Verlag, Berlin/New York.", "citeRegEx": "Bourbaki.,? 1989", "shortCiteRegEx": "Bourbaki.", "year": 1989}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "CoRR, abs/1003.4394.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Minimal recursion semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A Sag."], "venue": "Research on Language and Computation, 3(2-3):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Microsoft research paraphrase corpus", "author": ["Bill Dolan", "Chris Brockett", "Chris Quirk."], "venue": "Retrieved May, 29:2013.", "citeRegEx": "Dolan et al\\.,? 2005", "shortCiteRegEx": "Dolan et al\\.", "year": 2005}, {"title": "Introducing and evaluating ukWaC, a very large web-derived corpus of English", "author": ["Adriano Ferraresi", "Eros Zanchetta", "Marco Baroni", "Silvia Bernardini."], "venue": "Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47\u201354.", "citeRegEx": "Ferraresi et al\\.,? 2008", "shortCiteRegEx": "Ferraresi et al\\.", "year": 2008}, {"title": "On sense and reference", "author": ["Gottlob Frege."], "venue": "Ludlow (1997), pages 563\u2013584.", "citeRegEx": "Frege.,? 1892", "shortCiteRegEx": "Frege.", "year": 1892}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["John J Godfrey", "Edward C Holliman", "Jane McDaniel."], "venue": "Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on, volume 1,", "citeRegEx": "Godfrey et al\\.,? 1992", "shortCiteRegEx": "Godfrey et al\\.", "year": 1992}, {"title": "word2vec Explained: deriving Mikolov et al.\u2019s negativesampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": "arXiv preprint arXiv:1402.3722", "citeRegEx": "Goldberg and Levy.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy.", "year": 2014}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394\u20131404.", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011a", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Experimenting with transitive verbs in a DisCoCat", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 62\u201366, Edinburgh, UK, July. As-", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011b", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Distributional structure", "author": ["Z.S. Harris."], "venue": "Word.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Recurrent convolutional neural networks for discourse compositionality", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 119\u2013126, Sofia, Bulgaria, August. Asso-", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Prior disambiguation of word tensors for constructing sentence vectors", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNL), pages 1590\u20131601, Seat-", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2013", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2013}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL), Kyoto, Japan, June.", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2014", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2014}, {"title": "A unified sentence space for categorical distributional-compositional semantics: Theory and experiments", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman."], "venue": "Proceedings of COLING 2012: Posters, pages 549\u2013558, Mumbai, India,", "citeRegEx": "Kartsaklis et al\\.,? 2012", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "A systematic study of semantic vector space model parameters", "author": ["Douwe Kiela", "Stephen Clark."], "venue": "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 21\u201330, Gothenburg, Sweden, April.", "citeRegEx": "Kiela and Clark.,? 2014", "shortCiteRegEx": "Kiela and Clark.", "year": 2014}, {"title": "A Solution to Plato\u2019s Problem: The Latent Semantic Analysis Theory of Acquision, Induction, and Representation of Knowledge", "author": ["T. Landauer", "S. Dumais."], "venue": "Psychological Review.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Claws4: the tagging of the british national corpus", "author": ["Geoffrey Leech", "Roger Garside", "Michael Bryant."], "venue": "Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 622\u2013 628. Association for Computational Linguistics.", "citeRegEx": "Leech et al\\.,? 1994", "shortCiteRegEx": "Leech et al\\.", "year": 1994}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland, USA, June.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["Nitin Madnani", "Joel Tetreault", "Martin Chodorow."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of", "citeRegEx": "Madnani et al\\.,? 2012", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava."], "venue": "AAAI, volume 6, pages 775\u2013780.", "citeRegEx": "Mihalcea et al\\.,? 2006", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of NAACLHLT, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Investigating the contribution of distributional semantic information for dialogue act classification", "author": ["Dmitrijs Milajevs", "Matthew Purver."], "venue": "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC),", "citeRegEx": "Milajevs and Purver.,? 2014", "shortCiteRegEx": "Milajevs and Purver.", "year": 2014}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of ACL-08: HLT, pages 236\u2013244. Association for Computational Linguistics.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131439.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Universal grammar", "author": ["Richard Montague."], "venue": "Theoria, 36(3):373\u2013398.", "citeRegEx": "Montague.,? 1970", "shortCiteRegEx": "Montague.", "year": 1970}, {"title": "Improving distributional semantic vectors through context selection and normalisation", "author": ["Tamara Polajnar", "Stephen Clark."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230\u2013", "citeRegEx": "Polajnar and Clark.,? 2014", "shortCiteRegEx": "Polajnar and Clark.", "year": 2014}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u2013 50, Valletta, Malta, May. ELRA. http://is.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Ambiguity resolution in natural language learning", "author": ["Hinrich Sch\u00fctze."], "venue": "csli. Stanford, CA, 4:12\u201336.", "citeRegEx": "Sch\u00fctze.,? 1997", "shortCiteRegEx": "Sch\u00fctze.", "year": 1997}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Dialogue act modeling for automatic tagging and recognition", "author": ["Andreas Stolcke", "Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Carol Van Ess-Dykema", "Rachel Martin", "Marie Meteer"], "venue": null, "citeRegEx": "Stolcke et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stolcke et al\\.", "year": 2000}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Similarity of semantic relations", "author": ["Peter D Turney."], "venue": "Computational Linguistics, 32(3):379\u2013416.", "citeRegEx": "Turney.,? 2006", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "Dialogue act classification based on intra-utterance features", "author": ["Nick Webb", "Mark Hepple", "Yorick Wilks."], "venue": "Proceedings of the AAAI Workshop on Spoken Language Understanding. Citeseer.", "citeRegEx": "Webb et al\\.,? 2005", "shortCiteRegEx": "Webb et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks.", "startOffset": 23, "endOffset": 95}, {"referenceID": 9, "context": "Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks.", "startOffset": 23, "endOffset": 95}, {"referenceID": 30, "context": "Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks.", "startOffset": 23, "endOffset": 95}, {"referenceID": 27, "context": "While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive", "startOffset": 107, "endOffset": 147}, {"referenceID": 1, "context": "While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive", "startOffset": 107, "endOffset": 147}, {"referenceID": 4, "context": ", 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive", "startOffset": 114, "endOffset": 139}, {"referenceID": 39, "context": "We are especially interested in investigating the performance of neural word vectors in compositional models involving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others).", "startOffset": 308, "endOffset": 329}, {"referenceID": 19, "context": "(2012), Kalchbrenner and Blunsom (2013) and many others).", "startOffset": 8, "endOffset": 40}, {"referenceID": 34, "context": "We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks.", "startOffset": 113, "endOffset": 140}, {"referenceID": 41, "context": "(Stolcke et al., 2000)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks.", "startOffset": 194, "endOffset": 215}, {"referenceID": 8, "context": "In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures.", "startOffset": 194, "endOffset": 479}, {"referenceID": 8, "context": "In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures.", "startOffset": 194, "endOffset": 558}, {"referenceID": 8, "context": "In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.", "startOffset": 194, "endOffset": 768}, {"referenceID": 30, "context": "In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below.", "startOffset": 68, "endOffset": 91}, {"referenceID": 13, "context": "Formal semantics Formal approaches to the semantics of natural language have long built upon the classical idea of compositionality \u2013 that the meaning of a sentence is a function of the meanings of its parts (Frege, 1892).", "startOffset": 208, "endOffset": 221}, {"referenceID": 36, "context": "In compositional type-logical approaches, predicateargument structures representing phrases and sentences are built from their constituent parts by \u03b2reduction within the lambda calculus framework (Montague, 1970): for example, given a representation of John as john \u2032 and sleeps as \u03bbx.", "startOffset": 196, "endOffset": 212}, {"referenceID": 6, "context": "(Bos, 2008)).", "startOffset": 0, "endOffset": 11}, {"referenceID": 18, "context": "Co-occurrence-based word representations One way to produce such vectorial representations is to directly exploit Harris (1954)\u2019s intuition that semantically similar words tend to appear in similar contexts.", "startOffset": 114, "endOffset": 128}, {"referenceID": 24, "context": ", 2010) and below for discussion, and (Kiela and Clark, 2014) for a detailed comparison.", "startOffset": 38, "endOffset": 61}, {"referenceID": 26, "context": "Table 1: Word co-occurrence frequencies extracted from the BNC (Leech et al., 1994).", "startOffset": 63, "endOffset": 83}, {"referenceID": 31, "context": "the probability of observing the surrounding words in some context (Mikolov et al., 2013b):", "startOffset": 67, "endOffset": 90}, {"referenceID": 32, "context": "Additionally, neural vectors have also proven successful in other tasks (Mikolov et al., 2013c), since they seem to encode not only attributional similarity (the degree to which similar words are close to each other), but also relational similarity (Turney, 2006).", "startOffset": 72, "endOffset": 95}, {"referenceID": 43, "context": ", 2013c), since they seem to encode not only attributional similarity (the degree to which similar words are close to each other), but also relational similarity (Turney, 2006).", "startOffset": 162, "endOffset": 176}, {"referenceID": 34, "context": "The first approach combines word vectors by vector addition or point-wise multiplication (Mitchell and Lapata, 2008)\u2014as this is independent of word order, it cannot capture the difference between the two sentences \u201cdogs chase cats\u201d and \u201ccats chase dogs\u201d.", "startOffset": 89, "endOffset": 116}, {"referenceID": 40, "context": "autoencoders (Socher et al., 2012) or convolutional filters (Kalchbrenner et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 20, "context": ", 2012) or convolutional filters (Kalchbrenner et al., 2014).", "startOffset": 33, "endOffset": 60}, {"referenceID": 16, "context": "and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al.", "startOffset": 22, "endOffset": 57}, {"referenceID": 16, "context": "and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012).", "startOffset": 22, "endOffset": 87}, {"referenceID": 34, "context": "see (Mitchell and Lapata, 2008).", "startOffset": 4, "endOffset": 31}, {"referenceID": 7, "context": "Using a general duality theorem from multi-linear algebra (Bourbaki, 1989), it follows that tensors are in one-one correspondence with multi-linear maps, that is we have:", "startOffset": 58, "endOffset": 74}, {"referenceID": 21, "context": "Kartsaklis and Sadrzadeh (2013). Since the creation and manipulation of tensors of order higher than 2 is difficult, one can work with simplified versions of tensors, faithful to their underlying mathematical basis; these have found intuitive interpretations,", "startOffset": 0, "endOffset": 32}, {"referenceID": 16, "context": "see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014).", "startOffset": 4, "endOffset": 39}, {"referenceID": 16, "context": "see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014). In such cases, \u22c6 becomes a combination of a range of operations such", "startOffset": 4, "endOffset": 72}, {"referenceID": 16, "context": "These models are referred to by relational (Grefenstette and Sadrzadeh, 2011a); they are generalisations of predicate semantics of transitive verbs, from pairs of individuals to pairs of vectors.", "startOffset": 43, "endOffset": 78}, {"referenceID": 17, "context": "These models are referred to by Kronecker, which is the term sometimes used to denote the outer product of tensors (Grefenstette and Sadrzadeh, 2011b).", "startOffset": 115, "endOffset": 150}, {"referenceID": 23, "context": "The models of the last five lines of the table use the so-called Frobenius operators from categorical compositional distributional semantics (Kartsaklis et al., 2012) to expand the relational matrices of verbs from order 2 to order 3.", "startOffset": 141, "endOffset": 166}, {"referenceID": 22, "context": "Add, Frobenius-Mult, and Frobenius-Outer (Kartsaklis and Sadrzadeh, 2014).", "startOffset": 41, "endOffset": 73}, {"referenceID": 24, "context": "Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies).", "startOffset": 121, "endOffset": 170}, {"referenceID": 37, "context": "Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies).", "startOffset": 121, "endOffset": 170}, {"referenceID": 34, "context": "Addition w1w2 \u00b7 \u00b7 \u00b7wn \u2212\u2192 w1 +\u2212\u2192 w2 + \u00b7 \u00b7 \u00b7+\u2212\u2192 wn Mitchell and Lapata (2008) Multiplication w1w2 \u00b7 \u00b7 \u00b7wn \u2212\u2192 w1 \u2299\u2212\u2192 w2 \u2299 \u00b7 \u00b7 \u00b7 \u2299 \u2212\u2192 wn Mitchell and Lapata (2008)", "startOffset": 49, "endOffset": 76}, {"referenceID": 34, "context": "Addition w1w2 \u00b7 \u00b7 \u00b7wn \u2212\u2192 w1 +\u2212\u2192 w2 + \u00b7 \u00b7 \u00b7+\u2212\u2192 wn Mitchell and Lapata (2008) Multiplication w1w2 \u00b7 \u00b7 \u00b7wn \u2212\u2192 w1 \u2299\u2212\u2192 w2 \u2299 \u00b7 \u00b7 \u00b7 \u2299 \u2212\u2192 wn Mitchell and Lapata (2008)", "startOffset": 49, "endOffset": 160}, {"referenceID": 16, "context": "Relational Sbj Verb Obj Verb \u2299 ( \u2212\u2192 Sbj \u2297 \u2212\u2192 Obj) Grefenstette and Sadrzadeh (2011a)", "startOffset": 50, "endOffset": 85}, {"referenceID": 16, "context": "Kronecker Sbj Verb Obj \u1e7cerb \u2299 ( \u2212\u2192 Sbj \u2297 \u2212\u2192 Obj) Grefenstette and Sadrzadeh (2011b)", "startOffset": 49, "endOffset": 84}, {"referenceID": 23, "context": "Copy object Sbj Verb Obj \u2212\u2192 Sbj \u2299 (Verb \u00d7 \u2212\u2192 Obj) Kartsaklis et al. (2012)", "startOffset": 50, "endOffset": 75}, {"referenceID": 23, "context": "Copy subject Sbj Verb Obj \u2212\u2192 Obj \u2299 (Verb T \u00d7 \u2212\u2192 Sbj) Kartsaklis et al. (2012)", "startOffset": 53, "endOffset": 78}, {"referenceID": 21, "context": "Sbj Verb Obj ( \u2212\u2192 Sbj \u2299 (Verb \u00d7 \u2212\u2192 Obj)) + ( \u2212\u2192 Obj \u2299 (Verb T \u00d7 \u2212\u2192 Sbj)) Kartsaklis and Sadrzadeh (2014)", "startOffset": 73, "endOffset": 105}, {"referenceID": 21, "context": "Sbj Verb Obj ( \u2212\u2192 Sbj \u2299 (Verb \u00d7 \u2212\u2192 Obj))\u2299 ( \u2212\u2192 Obj \u2299 (Verb T \u00d7 \u2212\u2192 Sbj)) Kartsaklis and Sadrzadeh (2014)", "startOffset": 72, "endOffset": 104}, {"referenceID": 21, "context": "outer Sbj Verb Obj ( \u2212\u2192 Sbj \u2299 (Verb \u00d7 \u2212\u2192 Obj))\u2297 ( \u2212\u2192 Obj \u2299 (Verb T \u00d7 \u2212\u2192 Sbj)) Kartsaklis and Sadrzadeh (2014)", "startOffset": 78, "endOffset": 110}, {"referenceID": 26, "context": "In this vector space, the co-occurrence counts are extracted from the British National Corpus (BNC) (Leech et al., 1994).", "startOffset": 100, "endOffset": 120}, {"referenceID": 27, "context": "PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014)", "startOffset": 56, "endOffset": 75}, {"referenceID": 16, "context": "Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al.", "startOffset": 0, "endOffset": 35}, {"referenceID": 16, "context": "Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al.", "startOffset": 0, "endOffset": 63}, {"referenceID": 16, "context": "Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details.", "startOffset": 0, "endOffset": 83}, {"referenceID": 24, "context": "and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previ-", "startOffset": 34, "endOffset": 57}, {"referenceID": 12, "context": "KS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content).", "startOffset": 71, "endOffset": 95}, {"referenceID": 0, "context": "SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014).", "startOffset": 73, "endOffset": 134}, {"referenceID": 22, "context": "SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014).", "startOffset": 73, "endOffset": 134}, {"referenceID": 30, "context": "Neural word embeddings (NWE) For our neural setting, we used the skip-gram model of Mikolov et al. (2013b) trained with negative sampling.", "startOffset": 84, "endOffset": 107}, {"referenceID": 38, "context": "Furthermore, the gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010) was used for accessing the vectors.", "startOffset": 32, "endOffset": 57}, {"referenceID": 31, "context": "More details about the negative sampling approach can be found in (Mikolov et al., 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting.", "startOffset": 66, "endOffset": 89}, {"referenceID": 15, "context": ", 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting.", "startOffset": 22, "endOffset": 47}, {"referenceID": 16, "context": "We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5.", "startOffset": 63, "endOffset": 98}, {"referenceID": 34, "context": "This is similar to the intransitive dataset described in (Mitchell and Lapata, 2008).", "startOffset": 57, "endOffset": 84}, {"referenceID": 17, "context": "28 (Grefenstette and Sadrzadeh, 2011b).", "startOffset": 3, "endOffset": 38}, {"referenceID": 21, "context": "In this experiment we use the transitive sentence similarity dataset described in Kartsaklis and Sadrzadeh (2014). The dataset consists of transitive sentence pairs and a human similarity judgement6.", "startOffset": 82, "endOffset": 114}, {"referenceID": 21, "context": "The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010).", "startOffset": 59, "endOffset": 91}, {"referenceID": 22, "context": "The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010).", "startOffset": 131, "endOffset": 163}, {"referenceID": 35, "context": "The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010).", "startOffset": 283, "endOffset": 310}, {"referenceID": 11, "context": "Specifically, we get classification results on the Microsoft Research Paraphrase Corpus paraphrase corpus (Dolan et al., 2005) working in the following way: we construct vectors for the sentences of each pair; if the cosine similarity between the two sentence vectors exceeds a certain threshold, the pair is classified as a paraphrase, otherwise as not a paraphrase.", "startOffset": 106, "endOffset": 126}, {"referenceID": 28, "context": "84 F-score7) by the time of this writing has been obtained using 8 machine translation metrics and three constituent classifiers (Madnani et al., 2012).", "startOffset": 129, "endOffset": 151}, {"referenceID": 41, "context": "As our last experiment, we evaluate the word spaces on a dialogue act tagging task (Stolcke et al., 2000) over the Switchboard corpus (Godfrey et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 14, "context": ", 2000) over the Switchboard corpus (Godfrey et al., 1992).", "startOffset": 36, "endOffset": 58}, {"referenceID": 33, "context": "The experiment pipeline follows (Milajevs and Purver, 2014).", "startOffset": 32, "endOffset": 59}, {"referenceID": 44, "context": "The input utterances are preprocessed so that the parts of interrupted utterances are concatenated (Webb et al., 2005).", "startOffset": 99, "endOffset": 118}, {"referenceID": 41, "context": "9 We split the training and testing utterances as suggested by Stolcke et al. (2000). Utterance vectors are then obtained as in the previous experiments; they are", "startOffset": 63, "endOffset": 85}, {"referenceID": 3, "context": "html We use WordNetLemmatizer of the NLTK library (Bird, 2006).", "startOffset": 50, "endOffset": 62}, {"referenceID": 33, "context": "similar results to (Milajevs and Purver, 2014)\u2014although note that the dimensionality of our NWE vectors is 10 times lower than theirs.", "startOffset": 19, "endOffset": 46}, {"referenceID": 33, "context": "Multiplicative NWE outperformed the corresponding model in (Milajevs and Purver, 2014).", "startOffset": 59, "endOffset": 86}, {"referenceID": 1, "context": "While Baroni et al. (2014) conclude that \u201ccontext-predicting models obtain a thorough and resounding victory against their count-based counterparts\u201d, this seems to contradict, at least at the first consideration, the more conservative conclusion of Levy et al.", "startOffset": 6, "endOffset": 27}, {"referenceID": 1, "context": "While Baroni et al. (2014) conclude that \u201ccontext-predicting models obtain a thorough and resounding victory against their count-based counterparts\u201d, this seems to contradict, at least at the first consideration, the more conservative conclusion of Levy et al. (2014) that", "startOffset": 6, "endOffset": 268}, {"referenceID": 4, "context": "tributional word representations\u201d and the findings of Blacoe and Lapata (2012) that \u201cshallow approaches are as good as more computationally intensive alternatives\u201d on phrase similarity and paraphrase detection tasks.", "startOffset": 54, "endOffset": 79}, {"referenceID": 29, "context": "For the paraphrase task, we take the global vector-based similarity reported in (Mihalcea et al., 2006): 0.", "startOffset": 80, "endOffset": 103}, {"referenceID": 33, "context": "For the dialogue act tagging task the baseline is the accuracy of the bag-of-unigrams model in (Milajevs and Purver, 2014): 0.", "startOffset": 95, "endOffset": 122}, {"referenceID": 20, "context": "Kalchbrenner et al. (2014)).", "startOffset": 0, "endOffset": 27}], "year": 2014, "abstractText": "We provide a comparative study between neural word representations and traditional vector spaces based on cooccurrence counts, in a number of compositional tasks. We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}