{"id": "1408.6181", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2014", "title": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning", "abstract": "This paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition. In contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models, in our work we use a robust model trained with linear regression. The results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is model-independent.", "histories": [["v1", "Tue, 26 Aug 2014 16:43:30 GMT  (21kb)", "http://arxiv.org/abs/1408.6181v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dimitri kartsaklis", "nal kalchbrenner", "mehrnoosh sadrzadeh"], "accepted": true, "id": "1408.6181"}, "pdf": {"name": "1408.6181.pdf", "metadata": {"source": "CRF", "title": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning", "authors": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh"], "emails": ["dimitri.kartsaklis@cs.ox.ac.uk", "nkalch@cs.ox.ac.uk", "mehrnoosh.sadrzadeh@qmul.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 140 8.61 81v1 [cs.CL] 2 6A ug2 01"}, {"heading": "1 Introduction", "text": "The provision of compositivity in the distribution models of meaning, in which a word is presented as a vector of interaction, has attracted the attention of researchers; until it counts with every other word in the vocabulary, there is a solution to the fact that no text corpus, regardless of its size, is able to provide reliable coexistence statistics for anything but very short text constitutions. By compiling the vectors for the words within a sentence, we are still able to create a vector representation for that sentence that is very useful in a variety of natural language processing tasks, such as paraphrase recognition, sentiment analysis, or machine translation. Hence, given a sentence w1w2. wn, a compositional distribution model provides a function f like this: \u2212 w1, \u2212 w2,."}, {"heading": "2 Composition in distributional models", "text": "Compositional distribution models of meaning vary in their complexity, from simple elementary operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al., 2011; Socher et al., 2012; Kalchbrenner and Blunsom, 2013a). Tensor-based models formalized by Coecke et al. (2010) include a third class of models that lie somewhere between these two extremes. From this point of view, relational words such as verbs and adjectives are represented by multilinear maps (tensors of different orders) based on a number of arguments. An adjective, for example, is a linear map f: N (where N is our basic vector space for nouns) that takes a noun and adjectives as input and returns a modified version of them. \u2212 Since each map of this type of function is represented by a matrix, we can now see in the car, the carrex, and the N."}, {"heading": "3 Disambiguation and composition", "text": "The idea of separating disambiguations from composition first emerged in a paper by Reddy et al. (2011), in which the authors show that the introduction of an explicit disambiguation step is advantageous before the simple elementary composition for nouns and sadrzadeh. (2013) These experiments were extended to tensor models following the categorical framework of Coecke et al. (2010), where all \"unambiguous\" models, in turn, perform better than their \"ambiguous\" versions. (However, in this latest work, one of the dimensions of tensor models was expanded according to the categorical framework of Coecke et al. (2010)."}, {"heading": "4 Creating tensors for verbs", "text": "The essence of any tensor-based compositional model is the way in which we produce our sentence production concentrate, i.e. verbs. In this work, we apply a method suggested by Baroni and Zamparelli (2010) to form adjective matrices that can be generally applied to any arbitrary relation word. To create a matrix for, say, the intransitive verb \"play,\" we first collect all instances of the verb that occur in a subject's training corpus, and then generate non-compositional holistic vectors for these elementary sentences that follow exactly the same methodology as if they were words. We now have a data set with instances of the form < \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 subji, \u2212 subji play > (e.g. the vector of \"kids\" paired with the holistic vector of \"kids\" playing, \"and so on) that relates to verbs."}, {"heading": "5 Experimental setting", "text": "Our basic vector space is trained by the ukWaC vector vector vector vector vector (Ferraresi et al., 2008), where we originally used the 2,000 words containing the highest frequency (but without a list of stop words and the 50 most common words containing content, since they have a low information content). We created vectors for all words containing at least 100 occurrences in the corpus. As a context, we considered a 5-word window of each side of the target word, while we used local mutual information as a weighting scheme (i.e. point-by-point information multiplied by raw counting).This initial semantic space reached a score of 0.77 Spearman's vector (and 0.71 Pearsons r) on the well-known benchmark data set of Rubenstein and Goodenough (1965) vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector 300-dimensional vector space. To shorten the time of the regression trainting, our vector space was expanded to the 300-dimensional vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector (1965)."}, {"heading": "6 Supervised disambiguation", "text": "In fact, most of them will be able to move to another world, where they can move to another world, where they can find their way to another world."}, {"heading": "7 Unsupervised disambiguation", "text": "In this section, we will briefly outline how the process of creating tensors for different meanings of a verb can be automated, and we will test this idea on a generic phrase that is similar. First, we will use a vector that represents the surrounding context by recognizing the vectors of every other word in the corpus. Then, we will apply hierarchical agglomerations to capture these contexts. We hope that different groups of contexts will match by representing the surrounding contexts in which the word \"different\" is used. We hope that different groups of contexts under which the word was used in the corpus will correspond."}, {"heading": "8 Conclusion and future work", "text": "This work complements existing evidence from previous research that the introduction of an explicit disambiguation step before composition improves the quality of the compositional representations produced, and the use of a robust regression model refutes the hypothesis that the proposed methodology is only helpful for relatively \"weak\" compositional approaches. In terms of future work, it would be interesting to see how an earlier disambiguation step can similarly affect deep learning compositional settings (Socher et al., 2012) and (Kalchbrenner and Blunsom, 2013b)."}, {"heading": "Acknowledgements", "text": "We would like to thank the three anonymous reviewers for their fruitful comments. D. Kartsaklis and M. Sadrzadeh gratefully acknowledge the support of the EPSRC scholarship EP / F042728 / 1."}], "references": [{"title": "Nouns are Vectors, Adjectives are Matrices", "author": ["M. Baroni", "R. Zamparelli."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "A Dendrite Method for Cluster Analysis", "author": ["T. Cali\u0144ski", "J. Harabasz."], "venue": "Communications in StatisticsTheory and Methods, 3(1):1\u201327.", "citeRegEx": "Cali\u0144ski and Harabasz.,? 1974", "shortCiteRegEx": "Cali\u0144ski and Harabasz.", "year": 1974}, {"title": "Mathematical Foundations for Distributed Compositional Model of Meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark."], "venue": "Lambek Festschrift. Linguistic Analysis, 36:345\u2013384.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Introducing and evaluating ukWaC, a very large web-derived corpus of English", "author": ["Adriano Ferraresi", "Eros Zanchetta", "Marco Baroni", "Silvia Bernardini."], "venue": "Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47\u201354.", "citeRegEx": "Ferraresi et al\\.,? 2008", "shortCiteRegEx": "Ferraresi et al\\.", "year": 2008}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["Edward Grefenstette", "Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni."], "venue": "Proceedings of the 10th International Conference on Computational", "citeRegEx": "Grefenstette et al\\.,? 2013", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Recurrent convolutional neural networks for discourse compositionality", "author": ["N. Kalchbrenner", "P. Blunsom."], "venue": "Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria, August.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013a", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA, October. Association for Computational Lin-", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013b", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Prior disambiguation of word tensors for constructing sentence vectors", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA, October.", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2013", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2013}, {"title": "Separating Disambiguation from Composition in Distributional Semantics", "author": ["D. Kartsaklis", "M. Sadrzadeh", "S. Pulman."], "venue": "Proceedings of 17th Conference on Computational Natural Language Learning (CoNLL-2013), Sofia, Bulgaria, August.", "citeRegEx": "Kartsaklis et al\\.,? 2013", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2013}, {"title": "Compositional operators in distributional semantics", "author": ["Dimitri Kartsaklis."], "venue": "Springer Science Reviews, April. DOI: 10.1007/s40362-014-0017-z.", "citeRegEx": "Kartsaklis.,? 2014", "shortCiteRegEx": "Kartsaklis.", "year": 2014}, {"title": "Vector-based Models of Semantic Composition", "author": ["J. Mitchell", "M. Lapata."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 236\u2013244.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131439.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Dynamic and static prototype vectors for semantic composition", "author": ["Siva Reddy", "Ioannis Klapaftis", "Diana McCarthy", "Suresh Manandhar."], "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing, pages 705\u2013713.", "citeRegEx": "Reddy et al\\.,? 2011", "shortCiteRegEx": "Reddy et al\\.", "year": 2011}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J Mooney."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Reisinger and Mooney.,? 2010", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Contextual Correlates of Synonymy", "author": ["H. Rubenstein", "J.B. Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "The Oxford Junior Dictionary", "author": ["R. Sansome", "D. Reid", "A. Spooner."], "venue": "Oxford University Press.", "citeRegEx": "Sansome et al\\.,? 2000", "shortCiteRegEx": "Sansome et al\\.", "year": 2000}, {"title": "Automatic Word Sense Discrimination", "author": ["H. Sch\u00fctze."], "venue": "Computational Linguistics, 24:97\u2013123.", "citeRegEx": "Sch\u00fctze.,? 1998", "shortCiteRegEx": "Sch\u00fctze.", "year": 1998}, {"title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning."], "venue": "Advances in Neural Information Processing Systems, 24.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "Ng. A."], "venue": "Conference on Empirical Methods in Natural Language Processing 2012.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "Recent experimental evidence (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013) suggests that for a number of compositional models the introduction of a disambiguation step prior to the actual compositional process results in better composite representations.", "startOffset": 29, "endOffset": 106}, {"referenceID": 8, "context": "Recent experimental evidence (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013) suggests that for a number of compositional models the introduction of a disambiguation step prior to the actual compositional process results in better composite representations.", "startOffset": 29, "endOffset": 106}, {"referenceID": 7, "context": "Recent experimental evidence (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013) suggests that for a number of compositional models the introduction of a disambiguation step prior to the actual compositional process results in better composite representations.", "startOffset": 29, "endOffset": 106}, {"referenceID": 7, "context": "An exception to this is the work of Kartsaklis and Sadrzadeh (2013), who apply Eq.", "startOffset": 36, "endOffset": 68}, {"referenceID": 10, "context": "Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al.", "startOffset": 159, "endOffset": 186}, {"referenceID": 17, "context": "Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al., 2011; Socher et al., 2012; Kalchbrenner and Blunsom, 2013a).", "startOffset": 240, "endOffset": 315}, {"referenceID": 18, "context": "Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al., 2011; Socher et al., 2012; Kalchbrenner and Blunsom, 2013a).", "startOffset": 240, "endOffset": 315}, {"referenceID": 5, "context": "Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al., 2011; Socher et al., 2012; Kalchbrenner and Blunsom, 2013a).", "startOffset": 240, "endOffset": 315}, {"referenceID": 2, "context": "Tensor-based models, formalized by Coecke et al. (2010), comprise a third class of models lying somewhere in between these two extremes.", "startOffset": 35, "endOffset": 56}, {"referenceID": 9, "context": "A concise introduction to compositional distributional models can be found in (Kartsaklis, 2014).", "startOffset": 78, "endOffset": 96}, {"referenceID": 7, "context": "Finally, in (Kartsaklis and Sadrzadeh, 2013) these experiments were extended to include tensor-based models following the categorical framework of Coecke et al.", "startOffset": 12, "endOffset": 44}, {"referenceID": 8, "context": "The idea of separating disambiguation from composition first appears in a work of Reddy et al. (2011), where the authors show that the introduction of an explicit disambiguation step prior to simple element-wise composition is beneficial for noun-noun compounds.", "startOffset": 82, "endOffset": 102}, {"referenceID": 6, "context": "Subsequent work by Kartsaklis et al. (2013) reports very similar findings for verb-object structures, again on additive and multiplicative models.", "startOffset": 19, "endOffset": 44}, {"referenceID": 2, "context": "Finally, in (Kartsaklis and Sadrzadeh, 2013) these experiments were extended to include tensor-based models following the categorical framework of Coecke et al. (2010), where again all \u201cunambiguous\u201d models present superior performance compared to their \u201cambiguous\u201d versions.", "startOffset": 147, "endOffset": 168}, {"referenceID": 0, "context": "In this paper we adopt a method proposed by Baroni and Zamparelli (2010) for building adjective matrices, which can be generally applied to any relational word.", "startOffset": 44, "endOffset": 73}, {"referenceID": 4, "context": "In principle, our method is directly applicable to tensors of higher order, following a multi-step process similar to that of Grefenstette et al. (2013) who create order3 tensors for transitive verbs using similar means.", "startOffset": 126, "endOffset": 153}, {"referenceID": 3, "context": "Our basic vector space is trained from the ukWaC corpus (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content).", "startOffset": 56, "endOffset": 80}, {"referenceID": 3, "context": "Our basic vector space is trained from the ukWaC corpus (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content). We created vectors for all content words with at least 100 occurrences in the corpus. As context we considered a 5-word window from either side of the target word, while as our weighting scheme we used local mutual information (i.e. point-wise mutual information multiplied by raw counts). This initial semantic space achieved a score of 0.77 Spearman\u2019s \u03c1 (and 0.71 Pearson\u2019s r) on the well-known benchmark dataset of Rubenstein and Goodenough (1965). In order to reduce the time of regression training, our vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD).", "startOffset": 57, "endOffset": 739}, {"referenceID": 15, "context": "In more detail, the creation of the dataset was done in the following way: First, all verb entries with more than one definition in the Oxford Junior Dictionary (Sansome et al., 2000) were collected into a list.", "startOffset": 161, "endOffset": 183}, {"referenceID": 16, "context": "First, we use unsupervised learning in order to detect the latent senses of each verb in the corpus, following a procedure first described by Sch\u00fctze (1998). For every occurrence of the verb, we create a vector representing the surrounding context by averaging the vectors of every other word in the same sentence.", "startOffset": 142, "endOffset": 157}, {"referenceID": 1, "context": "Since HAC returns a dendrogram embedding all possible groupings, we measure the quality of each partitioning by using the variance ratio criterion (Cali\u0144ski and Harabasz, 1974) and we select the partitioning that achieves the best score (so the number of senses varies from verb to verb).", "startOffset": 147, "endOffset": 176}, {"referenceID": 11, "context": "We test this system on a verb phase similarity task introduced in (Mitchell and Lapata, 2010).", "startOffset": 66, "endOffset": 93}, {"referenceID": 13, "context": "In general, our approach is quite close to the multiprototype models of Reisinger and Mooney (2010). Model Spearman\u2019s \u03c1", "startOffset": 72, "endOffset": 100}, {"referenceID": 18, "context": "As for future work, an interesting direction would be to see how a prior disambiguation step can affect deep learning compositional settings similar to (Socher et al., 2012) and (Kalchbrenner and Blunsom, 2013b).", "startOffset": 152, "endOffset": 173}, {"referenceID": 6, "context": ", 2012) and (Kalchbrenner and Blunsom, 2013b).", "startOffset": 12, "endOffset": 45}], "year": 2014, "abstractText": "This paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition. In contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models, in our work we use a robust model trained with linear regression. The results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is modelindependent.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}