{"id": "1409.1320", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2014", "title": "Marginal Structured SVM with Hidden Variables", "abstract": "In this work, we propose the marginal structured SVM (MSSVM) for structured prediction with hidden variables. MSSVM properly accounts for the uncertainty of hidden variables, and can significantly outperform the previously proposed latent structured SVM (LSSVM; Yu &amp; Joachims (2009)) and other state-of-art methods, especially when that uncertainty is large. Our method also results in a smoother objective function, making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs. We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets. Furthermore, we propose a unified framework that includes both our and several other existing methods as special cases, and provides insights into the comparison of different models in practice.", "histories": [["v1", "Thu, 4 Sep 2014 05:06:34 GMT  (381kb,D)", "https://arxiv.org/abs/1409.1320v1", "Accepted by the 31st International Conference on Machine Learning (ICML 2014). 12 pages version with supplement"], ["v2", "Fri, 5 Sep 2014 21:13:36 GMT  (379kb,D)", "http://arxiv.org/abs/1409.1320v2", "Accepted by the 31st International Conference on Machine Learning (ICML 2014). 12 pages version with supplement"]], "COMMENTS": "Accepted by the 31st International Conference on Machine Learning (ICML 2014). 12 pages version with supplement", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["wei ping", "qiang liu", "alexander t ihler"], "accepted": true, "id": "1409.1320"}, "pdf": {"name": "1409.1320.pdf", "metadata": {"source": "META", "title": "Marginal Structured SVM with Hidden Variables", "authors": ["Wei Ping", "Qiang Liu", "Alexander Ihler"], "emails": ["WPING@ICS.UCI.EDU", "QLIU1@ICS.UCI.EDU", "IHLER@ICS.UCI.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. Related Work", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3. Structured Prediction with Hidden Variables", "text": "In this section, we check the background of the structured prediction with hidden variables. Suppose we have structured input-output pairs (x, y), where X, Y are the spaces of the input and output variables. In many applications, this input-output relationship is not only characterized by (x, y), but also depends on some unobserved hidden or latent variables h, H. Suppose (x, y, h) follows a conditional model, p (y, h; w) = 1 Z (x; w) exp [wT\u03c6 (x, y, h)], (1), where?? (x, y, h): X \u00d7 Y \u00d7 H \u2192 RD is a set of features that describe the relationships between the (x, y, h), and w? RD are the corresponding weights, or model parameters. The function Z (x; w) is the normalization constant, or prediction function, Z (w)."}, {"heading": "4. Marginal Structured SVM", "text": "In this section, we derive our main method, the marginal structured SVM (MSSVM), which minimizes an upper limit of the empirical risk function. Suppose we have a set of training instances S = {(x1, y1), \u00b7 \u00b7 \u00b7, (xn, yn)} (X \u00b7 Y) n. Risk is measured by a user-specific empirical loss function (yi, y, i), which quantifies the difference between an estimator y and the correct output yi. Normally, it is difficult to minimize the loss function accurately because it is typically non-convex and discontinuous with w (e.g., hammering loss). Instead, we assume upper limits to overcome them. Suppose that y (w) the marginal MAP prediction is defined on instance xi as in (2)."}, {"heading": "5. A Unified Framework", "text": "In this section, we compare our framework with a range of existing methods and present a more general framework that includes all these methods as special cases. First, it should be noted that the objective function of LSSVM (Yu & Joachims, 2009) 1 \u2192 2 + 2 + C n + 1 max y max h (yi, y) + w T\u03c6 (xi, y, h) \u2212 C n 0 + (xi, yi, h)]. (4) Our goal in (3) is similar to (4) except that the maximum operator of h should be replaced by the log sum exp function, the soTable 1 \u2212 n sequential comparisons within our unified framework. Model h \u2192 0 + (maxh) h = 1 (h) y \u2192 \u2192 (maxy) LSSVM MSSVM y y y y = 1 (smy y) N / A HCRF y y = h (0, 1) -extension modelled operator."}, {"heading": "6. Training Algorithms", "text": "In this section we present two optimization algorithms to minimize objective function in (3), including a sub-gradient descent algorithm (SGD) and a concaveconvex algorithm (CCCP). An empirical comparison of these two algorithms is performed in the experiments in Section 7."}, {"heading": "6.1. Sub-gradient Descent (SGD)", "text": "According to Danskin's theorem, the subgradient of the MSSVM target (3) can be similarly achieved by using algorithms. (3) is: \"wM = w + C n = p (max.)\" (xi, yi, yi) \"(xi, yi)\" (xi, yi) \"(xi, yi, h)\" (6) \"where y = arg max y y\" (yi, y) + log \"h\" (xi, h) \"exp\" (xi, h) \"(xi, h)\" (7) is the loss augmented margmented MAP prediction that can be approximated by mixed product propagation as described in Liu & Ihler (2013). \"The Ep (h | xi, y) and Ep (h)\" i, \"yi\" (xi, yi) denote the expectation of the distributions p (h | xi, y, yi) and (h)."}, {"heading": "6.2. CCCP Training Algorithm", "text": "The concave-convex algorithm (CCCP) (Yuille & Rangarajan, 2003) is a general non-convex optimization algorithm with broad application in machine learning, based on the idea of rewriting the non-convex objective function into the sum of a convex function and a concave function (or equivalent of a difference between two convex functions) and transforming the non-convex optimization problem into a sequence of convex subproblems by linearising the convex part. CCCP offers a simple solution to our problem, since the objective functions of all the methods we have discussed - in (3), (4) and (5) - are, of course, differences from two convex functions. For example, the MSSVM target in (3) can of course be written as follows: f (w) = f + (w) - f \u2212 f \u2212 f \u2212 w \u2212 w, where the convex in the concurve is excursive."}, {"heading": "7. Experiments", "text": "In this section, we compare our MSSVM with other state-of-art methods on both simulated and real data sets. We show that MSSVM performs significantly better than LSSVM by modelling latent variables (ModLat) (Kumar et al., 2012), the minimal entropy (M3E) model (Miller et al., 2012) and loss-based learning (ModLat) (Kumar et al., 2012), especially when uncertainty about hidden variables is high."}, {"heading": "7.1. Simulated Data", "text": "We simulate both training and test data from a paired Markov field (MRF =) via graph G = (V, E) with discrete random variables, the values in {0, 1, 2, 3} n, given by, p (x, y, h), p (xi, yj), p (xi, yj), p (xi, hk), p (xi, hk), p (hk), p (xi, hk), p (xi, yj), p (xi, yj), p (xi, yj), p (xi, hk), p (xi, hk), p (xi, hk), p (xi), p), p (hk), p (yj), p (yj), yj), yj), hk)] where the graph structure G is either a \"hidden chain\" (40 nodes) or a 2D mesh (size 6 x = 72 nodes."}, {"heading": "7.2. Image Segmentation", "text": "In this section, we evaluate our MSSVM method based on the task of segmenting poorly described images. Our settings are motivated by the experiments in (Schwing et al., 2012). We proceed from a ground-level image of 20 x 40 pixels, as shown in Figure 4 (a), where each pixel i has a caption yi, using values in {1, \u00b7 \u00b7 \u00b7, 5}. The observed image x is obtained by adding Gaussian noise, N (0, 5), on the ground-level image as Figure 4 (b).We use the 2D grid model as shown in Figure 1 (b), with local features \u03c6 (yi, xi) = eyi xi and paired features \u03c6 (yi, yj) = eyi eyj, as defined in Figure 4 (b). We use the 2D grid model as shown in Figure 1 (b)."}, {"heading": "7.3. Object Categorization", "text": "We use the dataset from Microsoft Research Cambridge (Winn et al., 2005), which consists of 240 images with 213 x 320 pixels and their captions at the sub-pixel level. Missing captions can correspond to ambiguous regions, undefined categories or object boundaries, etc. The result is a 20 x 31 grid model as shown in Figure 1 (b). The local features of each patch are encoded using texture and color descriptors. For the texture, we calculate the 128-dimensional SIFT descriptors of the patch and vector, quantify them in a 500-word codebook that is learned by aggregating all the patches."}, {"heading": "8. Conclusion", "text": "We have proposed a novel structured SVM method for structured prediction with hidden variables. We demonstrate that our MSSVM consistently outperforms state-of-the-art methods in both simulated and real datasets, especially when the uncertainty of hidden variables is high. Compared to the popular LSSVM, the objective function of our MSSVM is easier to optimize due to the smoothness of its objective function. We also offer a unified framework that encompasses our method as well as a range of previous methods as special cases. Acknowledgements. This work was sponsored in part by NSF grants IIS-1065618 and IIS-1254071, and in part by the United States Air Force under contract number FA875014-C-0011 under the DARPA PAML program."}], "references": [{"title": "Deterministic annealing for semi-supervised structured output learning", "author": ["P. Dhillon", "Keerthi", "S. Sathiya", "K. Bellare", "O. Chapelle", "S. Sundararajan"], "venue": "In Proceedings of AISTAT, pp", "citeRegEx": "Dhillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2012}, {"title": "Introduction to statistical relational learning", "author": ["L. Getoor", "B. Taskar"], "venue": "The MIT press,", "citeRegEx": "Getoor and Taskar,? \\Q2007\\E", "shortCiteRegEx": "Getoor and Taskar", "year": 2007}, {"title": "A primal-dual message-passing algorithm for approximated large scale structured prediction", "author": ["T. Hazan", "R. Urtasun"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Hazan and Urtasun,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Urtasun", "year": 2010}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "The MIT press,", "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Modeling latent variable uncertainty for loss-based learning", "author": ["P. Kumar", "B. Packer", "D. Koller"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proceedings of ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Efficient marginal likelihood optimization in blind deconvolution", "author": ["A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Levin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2011}, {"title": "Protein\u2013protein interaction site prediction based on conditional random", "author": ["M.H. Li", "L. Lin", "X.L. Wang", "T. Liu"], "venue": "field. Bioinformatics,", "citeRegEx": "Li et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "Variational algorithms for marginal map", "author": ["Q. Liu", "A. Ihler"], "venue": "JMLR, 14:3165\u20133200,", "citeRegEx": "Liu and Ihler,? \\Q2013\\E", "shortCiteRegEx": "Liu and Ihler", "year": 2013}, {"title": "Max-margin min-entropy models", "author": ["K. Miller", "P. Kumar", "B. Packer", "D. Goodman", "D. Koller"], "venue": "In Proceedings of AISTATS, pp", "citeRegEx": "Miller et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2012}, {"title": "Improving NLP through marginalization of hidden syntactic structure", "author": ["J. Naradowsky", "S. Riedel", "D. Smith"], "venue": "In Proceeding of EMNLP,", "citeRegEx": "Naradowsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Naradowsky et al\\.", "year": 2012}, {"title": "Structured prediction and learning in computer vision", "author": ["S. Nowozin", "C. Lampert"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Nowozin and Lampert,? \\Q2011\\E", "shortCiteRegEx": "Nowozin and Lampert", "year": 2011}, {"title": "Conditional random fields for object recognition", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": "In Proceedings of NIPS, pp", "citeRegEx": "Quattoni et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2004}, {"title": "Hidden conditional random fields", "author": ["A. Quattoni", "S. Wang", "L. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE Transactions on PAMI,", "citeRegEx": "Quattoni et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2007}, {"title": "Online) Subgradient methods for structured prediction", "author": ["N. Ratliff", "J.A. Bagnell", "M. Zinkevich"], "venue": "In Proceedings of AISTATS, pp", "citeRegEx": "Ratliff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "Unified expectation maximization", "author": ["R. Samdani", "M.W. Chang", "D. Roth"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Samdani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Samdani et al\\.", "year": 2012}, {"title": "RNA secondary structural alignment with conditional random fields", "author": ["K. Sato", "Y. Sakakibara"], "venue": null, "citeRegEx": "Sato and Sakakibara,? \\Q2005\\E", "shortCiteRegEx": "Sato and Sakakibara", "year": 2005}, {"title": "Efficient structured prediction with latent variables for general graphical models", "author": ["A. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun"], "venue": "In Proceedings of ICML,", "citeRegEx": "Schwing et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schwing et al\\.", "year": 2012}, {"title": "Max-margin Markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, 6:1453\u20131484,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Scene segmentation with CRFs learned from partially labeled images", "author": ["J. Verbeek", "B. Triggs"], "venue": "In Proceedings of NIPS, pp", "citeRegEx": "Verbeek and Triggs,? \\Q2007\\E", "shortCiteRegEx": "Verbeek and Triggs", "year": 2007}, {"title": "Losssensitive training of probabilistic conditional random fields", "author": ["M. Volkovs", "H. Larochelle", "R.S. Zemel"], "venue": "Technical report,", "citeRegEx": "Volkovs et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Volkovs et al\\.", "year": 2011}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Hidden conditional random fields for gesture recognition", "author": ["Wang", "S.B", "A. Quattoni", "L. Morency", "D. Demirdjian"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "Max-margin hidden conditional random fields for human action recognition", "author": ["Y. Wang", "G. Mori"], "venue": "In Proceedings of CVPR, pp", "citeRegEx": "Wang and Mori,? \\Q2009\\E", "shortCiteRegEx": "Wang and Mori", "year": 2009}, {"title": "Object categorization by learned universal visual dictionary", "author": ["J. Winn", "A. Criminisi", "T. Minka"], "venue": "In Proceedings of ICCV, pp", "citeRegEx": "Winn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Winn et al\\.", "year": 2005}, {"title": "Hyperlink prediction in hypernetworks using latent social features", "author": ["Y. Xu", "D. Rockmore", "A. Kleinbaum"], "venue": "In Discovery Science,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Multi-level structured models for document-level sentiment classification", "author": ["A. Yessenalina", "Y. Yue", "C. Cardie"], "venue": "In Proceedings of EMNLP, pp", "citeRegEx": "Yessenalina et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2010}, {"title": "Learning structural SVMs with latent variables", "author": ["C. Yu", "T. Joachims"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Yu and Joachims,? \\Q2009\\E", "shortCiteRegEx": "Yu and Joachims", "year": 2009}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Computation,", "citeRegEx": "Yuille and Rangarajan,? \\Q2003\\E", "shortCiteRegEx": "Yuille and Rangarajan", "year": 2003}, {"title": "Latent hierarchical structural learning for object detection", "author": ["L. Zhu", "Y. Chen", "A. Yuille", "W. Freeman"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Zhu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets.", "startOffset": 95, "endOffset": 118}, {"referenceID": 5, "context": "Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) and structured SVMs (SSVMs) (Taskar et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 18, "context": ", 2001) and structured SVMs (SSVMs) (Taskar et al., 2003; Tsochantaridis et al., 2005) are standard tools for structured prediction in many important domains, such as computer vision (Nowozin & Lampert, 2011), natural language processing (Getoor & Taskar, 2007) and computational biology (e.", "startOffset": 36, "endOffset": 86}, {"referenceID": 19, "context": ", 2001) and structured SVMs (SSVMs) (Taskar et al., 2003; Tsochantaridis et al., 2005) are standard tools for structured prediction in many important domains, such as computer vision (Nowozin & Lampert, 2011), natural language processing (Getoor & Taskar, 2007) and computational biology (e.", "startOffset": 36, "endOffset": 86}, {"referenceID": 10, "context": "Examples also arise in natural language processing, such as semantic role labeling, where the semantic predictions are inherently coupled with latent syntactic relations (Naradowsky et al., 2012).", "startOffset": 170, "endOffset": 195}, {"referenceID": 13, "context": "Perhaps the most notable of these are hidden conditional random fields (HCRFs) (Quattoni et al., 2007) and latent structured SVMs (LSSVMs) (Yu & Joachims, 2009), which are derived from conditional random fields and structured SVMs, respectively.", "startOffset": 79, "endOffset": 102}, {"referenceID": 12, "context": "Related Work HCRFs naturally extend CRFs to include hidden variables, and have found numerous applications in areas such as object recognition (Quattoni et al., 2004) and gesture recognition (Wang et al.", "startOffset": 143, "endOffset": 166}, {"referenceID": 23, "context": ", 2004) and gesture recognition (Wang et al., 2006).", "startOffset": 32, "endOffset": 51}, {"referenceID": 30, "context": "Alternatively, the LSSVM (Yu & Joachims, 2009) is an extension of structured SVM that handles hidden variables, with wide application in areas like object detection (Zhu et al., 2010), human action recognition (Wang & Mori, 2009), document-level sentiment classification (Yessenalina et al.", "startOffset": 165, "endOffset": 183}, {"referenceID": 27, "context": ", 2010), human action recognition (Wang & Mori, 2009), document-level sentiment classification (Yessenalina et al., 2010) and link prediction (Xu et al.", "startOffset": 95, "endOffset": 121}, {"referenceID": 26, "context": ", 2010) and link prediction (Xu et al., 2013).", "startOffset": 28, "endOffset": 45}, {"referenceID": 6, "context": "In many domains, marginal MAP can provide significant improvement over joint MAP estimation, which jointly optimizes hidden and output variables; recent examples include blind deconvolution in computer vision (Fergus et al., 2006; Levin et al., 2011) and relation extraction and semantic role labeling in natural language processing (Naradowsky et al.", "startOffset": 209, "endOffset": 250}, {"referenceID": 10, "context": ", 2011) and relation extraction and semantic role labeling in natural language processing (Naradowsky et al., 2012).", "startOffset": 90, "endOffset": 115}, {"referenceID": 14, "context": "Sub-gradient decent (SGD) (Ratliff et al., 2007) and the concave-convex procedure (CCCP)(Yuille & Rangarajan, 2003) are two popular training algorithms for structured prediction problems.", "startOffset": 26, "endOffset": 48}, {"referenceID": 8, "context": "Related Work HCRFs naturally extend CRFs to include hidden variables, and have found numerous applications in areas such as object recognition (Quattoni et al., 2004) and gesture recognition (Wang et al., 2006). HCRFs have the same pros and cons as general CRFs; in particular, they perform well when the model assumptions hold and when there are enough training instances, but may otherwise perform badly. Alternatively, the LSSVM (Yu & Joachims, 2009) is an extension of structured SVM that handles hidden variables, with wide application in areas like object detection (Zhu et al., 2010), human action recognition (Wang & Mori, 2009), document-level sentiment classification (Yessenalina et al., 2010) and link prediction (Xu et al., 2013). However, LSSVM relies on a joint MAP procedure, and may not perform well when a non-trivial uncertainty exists in the hidden variables. Recently, Schwing et al. (2012) proposed an -extension framework for discriminative graphical models with hidden variables that includes both HCRFs and LSSVM as special cases.", "startOffset": 144, "endOffset": 912}, {"referenceID": 7, "context": "For example, Miller et al. (2012) proposed a max margin min-entropy (M3E) model that minimizes an uncertainty measure on hidden variables while performing max-margin learning.", "startOffset": 13, "endOffset": 34}, {"referenceID": 4, "context": "In another work, Kumar et al. (2012) proposes a learning procedure that encourages agreement between two separate models \u2013 one for predicting outputs and another for representing the uncertainty over the hidden variables.", "startOffset": 17, "endOffset": 37}, {"referenceID": 4, "context": "In another work, Kumar et al. (2012) proposes a learning procedure that encourages agreement between two separate models \u2013 one for predicting outputs and another for representing the uncertainty over the hidden variables. They model the uncertainty of hidden variable during training, and rely on a joint MAP procedure during prediction. Our proposed method builds on recent work for marginal MAP inference (Koller & Friedman, 2009; Liu & Ihler, 2013), which averages over the hidden variables (or variables that are not of direct interest), and then optimizes over the output variables (or variables of direct interest). In many domains, marginal MAP can provide significant improvement over joint MAP estimation, which jointly optimizes hidden and output variables; recent examples include blind deconvolution in computer vision (Fergus et al., 2006; Levin et al., 2011) and relation extraction and semantic role labeling in natural language processing (Naradowsky et al., 2012). Unfortunately, marginal MAP tasks on graphical models are notoriously difficult; marginal MAP can be NP-hard even when the underlying graphical model is tree-structured (Koller & Friedman, 2009). Recently, Liu & Ihler (2013) proposed efficient variational algorithms that approximately solve marginal MAP.", "startOffset": 17, "endOffset": 1207}, {"referenceID": 17, "context": "Similar temperature-based approaches have been used both in structured prediction (Hazan & Urtasun, 2010; Schwing et al., 2012) and in other problems, such as semi-supervised learning (Samdani et al.", "startOffset": 82, "endOffset": 127}, {"referenceID": 15, "context": ", 2012) and in other problems, such as semi-supervised learning (Samdani et al., 2012; Dhillon et al., 2012).", "startOffset": 64, "endOffset": 108}, {"referenceID": 0, "context": ", 2012) and in other problems, such as semi-supervised learning (Samdani et al., 2012; Dhillon et al., 2012).", "startOffset": 64, "endOffset": 108}, {"referenceID": 20, "context": "If we set y = h = 1, we obtain the loss-augmented likelihood objective in Volkovs et al. (2011), and further reduces to the standard likelihood objective of HCRFs if we assume \u2206(yi, y) \u2261 0.", "startOffset": 74, "endOffset": 96}, {"referenceID": 17, "context": "Our framework also generalizes the -extension model by Schwing et al. (2012), which corresponds to the restriction that y = h.", "startOffset": 55, "endOffset": 77}, {"referenceID": 9, "context": "We demonstrate that the MSSVM significantly outperforms LSSVM, max-margin min-entropy (M3E) model (Miller et al., 2012) and loss-based learning by modeling latent variable(ModLat) (Kumar et al.", "startOffset": 98, "endOffset": 119}, {"referenceID": 4, "context": ", 2012) and loss-based learning by modeling latent variable(ModLat) (Kumar et al., 2012), especially when the uncertainty over hidden variables is high.", "startOffset": 68, "endOffset": 88}, {"referenceID": 9, "context": "For comparison, we also evaluate the performance of M3E (Miller et al., 2012) and Table 3.", "startOffset": 56, "endOffset": 77}, {"referenceID": 4, "context": "ModLat (Kumar et al., 2012).", "startOffset": 7, "endOffset": 27}, {"referenceID": 17, "context": "Our settings are motivated by the experiments in (Schwing et al., 2012).", "startOffset": 49, "endOffset": 71}, {"referenceID": 17, "context": "Our settings are motivated by the experiments in (Schwing et al., 2012). We assume a ground truth image of 20\u00d740 pixels as shown in Figure 4 (a), where each pixel i has a label yi taking values in {1, \u00b7 \u00b7 \u00b7 , 5}. The observed image x is obtained by adding Gaussian noise, N(0, 5), on the ground truth image as Figure 4 (b). We use the 2D-grid model as in Figure 1 (b), with local features \u03c6(yi, xi) = eyi \u2297 xi and pairwise features \u03c6(yi, yj) = eyi \u2297 eyj \u2208 R5\u00d75 as defined in Nowozin & Lampert (2011), where eyi is the unit normal vector with entry one on dimension yi and \u2297 is the outer product.", "startOffset": 50, "endOffset": 500}, {"referenceID": 25, "context": "We use the Microsoft Research Cambridge data set (Winn et al., 2005), consisting of 240 images with 213\u00d7320 pixels and their partial pixel-level labelings.", "startOffset": 49, "endOffset": 68}, {"referenceID": 25, "context": "We use the Microsoft Research Cambridge data set (Winn et al., 2005), consisting of 240 images with 213\u00d7320 pixels and their partial pixel-level labelings. The missing labels may correspond to ambiguous regions, undefined categories or object boundaries, etc. Modeled on the approach outlined in Verbeek & Triggs (2007), we use 20 \u00d7 20 pixel patches with centers at 10 pixel intervals and treat each patch as a node in our model.", "startOffset": 50, "endOffset": 320}], "year": 2014, "abstractText": "In this work, we propose the marginal structured SVM (MSSVM) for structured prediction with hidden variables. MSSVM properly accounts for the uncertainty of hidden variables, and can significantly outperform the previously proposed latent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art methods, especially when that uncertainty is large. Our method also results in a smoother objective function, making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs. We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets. Furthermore, we propose a unified framework that includes both our and several other existing methods as special cases, and provides insights into the comparison of different models in practice.", "creator": "LaTeX with hyperref package"}}}