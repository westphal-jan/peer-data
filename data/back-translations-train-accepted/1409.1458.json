{"id": "1409.1458", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2014", "title": "Communication-Efficient Distributed Dual Coordinate Ascent", "abstract": "Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, CoCoA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to state-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoA converges to the same .001-accurate solution quality on average 25x as quickly.", "histories": [["v1", "Thu, 4 Sep 2014 14:59:35 GMT  (552kb)", "http://arxiv.org/abs/1409.1458v1", null], ["v2", "Mon, 29 Sep 2014 16:07:32 GMT  (187kb)", "http://arxiv.org/abs/1409.1458v2", "NIPS 2014 version, including proofs. Published in Advances in Neural Information Processing Systems 27 (NIPS 2014)"]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["martin jaggi", "virginia smith", "martin tak\u00e1c", "jonathan terhorst", "sanjay krishnan", "thomas hofmann", "michael i jordan"], "accepted": true, "id": "1409.1458"}, "pdf": {"name": "1409.1458.pdf", "metadata": {"source": "META", "title": "Communication-Efficient Distributed Dual Coordinate Ascent", "authors": ["Martin Jaggi"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 9.14 58v1 [cs.LG] 4 S"}, {"heading": "1 Introduction", "text": "This year it has come to the point where one is able to see oneself in the first half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the first half of the 20th century, in the second half of the 20th century, in the first half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the first half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century."}, {"heading": "2 Setup", "text": "A large class of machine learning and signal processing methods can be represented as minimizing a convex loss function of linear predictors with a convex regularization term. (1) 2 We plan to release our code to share it with Spark's open source machine learning library, MLlib. Here, the data training problems are real evaluated vectors xi-Rd; the loss functions i, i = 1,..., n are convex and may depend on labels yi-R; and \u03bb > 0 is the regulation parameter. Based on the setup of [SSZ13], we assume that the regularizer is the most convenient 2 standard. Examples of this problem class include support vector machines as well as regulated linear and logistic regression."}, {"heading": "3 Method Description", "text": "The COCOA framework, as presented in algorithm 1, assumes that the data (xi, yi) ni = 1 for a regulated loss minimization problem of form (1) are distributed across K-Worker machines. We associate with the data their corresponding dual variables (\u03b1i) ni = 1, which are equally distributed between the workers. The core idea is to use the dual variables to efficiently merge the parallel updates from the different workers without much conflict, taking advantage of the fact that they are all distributed on disjoint sets of dual variable.Algorithm 1: COCOA: Communication-Efficient Distributed Dual Coordinate Ascent Input: T-1, Scaling Parameter 1 \u2264 \u03b2K: = 1). Data: {xi, yi} ni = 1 distributed over K machines Initialize (0) [k]."}, {"heading": "4 Convergence Analysis", "text": "In view of the dual problem (2), we define the local sub-optimality on each coordinate block as follows: \u03b5D, k (\u03b1): = max \u03b1 (k),.,.,.,.,.,.,.,.,.,.,.,...,..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 Related Work", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "6 Experiments", "text": "In this context, it should be noted that this is one of the greatest challenges in the history of the European Union."}, {"heading": "7 Conclusion", "text": "We have presented a communication-efficient framework for distributed algorithms for double coordinate ascent that can be used to solve large-scale, regulated loss minimization problems, which is critical in situations where data sets need to be distributed across multiple machines and where communication between nodes is costly. We have demonstrated that the proposed algorithm is competitive with real, widely distributed data sets, and have presented the first theoretical analysis of this algorithm that achieves competitive convergence rates without making additional assumptions about the data itself. It remains open to obtaining improved convergence rates for more aggressive updates equivalent to \u03b2K > 1 that are appropriate for the use of the \"safe\" update techniques of [TBRS13] and the associated expected divisible approximations of [RT14, RT12] applied here to K instead of n blocks. Furthermore, it remains open to showing convergence rates for SD in the same way as described here."}, {"heading": "A Proof of Theorem 2", "text": "In the following, for a given vector \u03b1 = max. Rnk, we write \u03b1 < [k] > \"Rn\" for its zero-value version, which corresponds to \u03b1 [k] on the k-th coordinate block and is zero everywhere. Theorem '2. Let's assume that algorithm 1 is smooth for the outer iteration of K machines (with the LOCALDUAL METHOD procedure with local geometric improvement, and let's leave \u03b2K: = 1. Let's assume that the loss functions of i (1 / 3) -then the following geometric convergence rate applies to the global (double) objective: E [D \u2212 \u2212 \u2212 (T)). (1 \u2212 (1 \u2212) 1Kill. (1Kill.) \"k\" Let's assume that the loss functions of i (1 / 3) -smooth. Then the following geometric concentrical rate applies to the double global (E \u2212 \u2212 -D) target."}, {"heading": "B Proof of Proposition 1", "text": "The core concept of our new analysis method is the following basic dual structure on each local coordinate block. With this approach, we will show below that all steps of LOCALAL METHOD can be interpreted as ascension steps on the global dual objective function D (\u03b1), with coordinate changes limited to the k-th block. In other words, each set of Ik consists of the individual coordinate blocks (these with the dual variables) and the datapoints A (k) and A (k) available at the k-th level."}, {"heading": "C Proof of Lemma 3", "text": "3. If K = 1 then \u03c3min = 0. If we start from K \u2265 1, then we have 0 \u2264 \u03c3min \u2264 n, if the data points between different workers are orthogonal, i.e. (ATA) i, j = 0 \u0445 i, j so that i and j do not belong to the same part, then \u03c3min = 0.Proof. if K = 1 then \u03b1 [K]. For a non-trivial case, if K > 1 does not belong to the same part, we have min (7) = max.Proof. If K = 1 = A [k]."}], "references": [{"title": "Distributed Delayed Stochastic Optimization", "author": ["Alekh Agarwal", "John C Duchi"], "venue": "In NIPS,", "citeRegEx": "Agarwal and Duchi.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal and Duchi.", "year": 2011}, {"title": "Distributed Learning, Communication Complexity and Privacy", "author": ["Maria-Florina Balcan", "Avrim Blum", "Shai Fine", "Yishay Mansour"], "venue": null, "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "Parallel Coordinate Descent for L1-Regularized Loss Minimization", "author": ["Joseph K Bradley", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin"], "venue": "In ICML,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Large-Scale Machine Learning with Stochastic Gradient Descent", "author": ["L\u00e9on Bottou"], "venue": "editors, COMPSTAT\u20192010 - Proceedings of the 19th International Conference on Computational Statistics,", "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Optimal Distributed Online Prediction", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "Using Mini-Batches. JMLR,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Estimation, Optimization, and Parallelism when Data is Sparse", "author": ["John C Duchi", "Michael I Jordan", "H Brendan Mcmahan"], "venue": "In NIPS,", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Fast Distributed Coordinate Descent for NonStrongly Convex Losses", "author": ["Olivier Fercoq", "Zheng Qu", "Peter Richt\u00e1rik"], "venue": null, "citeRegEx": "Fercoq et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fercoq et al\\.", "year": 2014}, {"title": "A Dual Coordinate Descent Method for Large-scale Linear SVM", "author": ["Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin", "S Sathiya Keerthi", "S Sundararajan"], "venue": "In the 25th International Conference on Machine Learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "An Asynchronous Parallel Stochastic Coordinate Descent Algorithm", "author": ["Ji Liu", "Stephen J Wright", "Christopher R\u00e9", "Victor Bittorf", "Srikrishna Sridhar"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models", "author": ["Gideon Mann", "Ryan McDonald", "Mehryar Mohri", "Nathan Silberman", "Daniel D Walker"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2009}, {"title": "Distributed Block Coordinate Descent for Minimizing Partially Separable Functions", "author": ["Jakub Marecek", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "Marecek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marecek et al\\.", "year": 2014}, {"title": "Efficient parallel coordinate descent algorithm for convex optimization problems with separable constraints: Application to distributed MPC", "author": ["Ion Necoara", "Dragos Clipici"], "venue": "Journal of Process Control,", "citeRegEx": "Necoara and Clipici.,? \\Q2013\\E", "shortCiteRegEx": "Necoara and Clipici.", "year": 2013}, {"title": "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["Feng Niu", "Benjamin Recht", "Christopher R\u00e9", "Stephen J Wright"], "venue": "In NIPS,", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "A Stochastic Approximation Method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Parallel Coordinate Descent Methods for Big Data Optimization", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2012}, {"title": "Distributed Coordinate Descent Method for Learning with Big Data", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "maths.ed.ac.uk,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2013}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2014\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2014}, {"title": "Pegasos: Primal Estimated Sub-Gradient Solver for SVM", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": null, "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2013}, {"title": "Communication-Efficient Distributed Optimization using an Approximate Newton-type Method", "author": ["Ohad Shamir", "Nathan Srebro", "Tong Zhang"], "venue": null, "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms", "author": ["J Tsitsiklis", "D Bertsekas", "M Athans"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1986}, {"title": "Mini-Batch Primal and Dual Methods for SVMs", "author": ["Martin Tak\u00e1\u010d", "Avleen Bijral", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": "In ICML,", "citeRegEx": "Tak\u00e1\u010d et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tak\u00e1\u010d et al\\.", "year": 2013}, {"title": "Primal-Dual Parallel Coordinate Descent for Machine Learning Optimization", "author": ["Martin Tak\u00e1\u010d", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": null, "citeRegEx": "Tak\u00e1\u010d et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tak\u00e1\u010d et al\\.", "year": 2014}, {"title": "Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent", "author": ["Tianbao Yang"], "venue": "In NIPS,", "citeRegEx": "Yang.,? \\Q2013\\E", "shortCiteRegEx": "Yang.", "year": 2013}, {"title": "Large linear classification when data cannot fit in memory", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin"], "venue": "In the 16th ACM SIGKDD international conference,", "citeRegEx": "Yu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2010}, {"title": "Large Linear Classification When Data Cannot Fit in Memory", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "On Theoretical Analysis of Distributed Stochastic Dual Coordinate Ascent", "author": ["Tianbao Yang", "Shenghuo Zhu", "Rong Jin", "Yuanqing Lin"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing", "author": ["Matei Zaharia", "Mosharaf Chowdhury", "Tathagata Das", "Ankur Dave", "Murphy McCauley", "Michael J Franklin", "Scott Shenker", "Ion Stoica"], "venue": null, "citeRegEx": "Zaharia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2012}, {"title": "Communication-Efficient Algorithms for Statistical Optimization", "author": ["Yuchen Zhang", "John C Duchi", "Martin J Wainwright"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Parallelized Stochastic Gradient Descent", "author": ["Martin A Zinkevich", "Markus Weimer", "Alex J Smola", "Lihong Li"], "venue": "NIPS 2010: Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [], "year": 2017, "abstractText": "Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, COCOA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to stateof-the-art mini-batch versions of SGD and SDCA algorithms, COCOA converges to the same .001-accurate solution quality on average 25\u00d7 as quickly.", "creator": "LaTeX with hyperref package"}}}