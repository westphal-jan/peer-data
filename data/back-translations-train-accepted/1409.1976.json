{"id": "1409.1976", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2014", "title": "A Reduction of the Elastic Net to Support Vector Machines with an Application to GPU Computing", "abstract": "The past years have witnessed many dedicated open-source projects that built and maintain implementations of Support Vector Machines (SVM), parallelized for GPU, multi-core CPUs and distributed systems. Up to this point, no comparable effort has been made to parallelize the Elastic Net, despite its popularity in many high impact applications, including genetics, neuroscience and systems biology. The first contribution in this paper is of theoretical nature. We establish a tight link between two seemingly different algorithms and prove that Elastic Net regression can be reduced to SVM with squared hinge loss classification. Our second contribution is to derive a practical algorithm based on this reduction. The reduction enables us to utilize prior efforts in speeding up and parallelizing SVMs to obtain a highly optimized and parallel solver for the Elastic Net and Lasso. With a simple wrapper, consisting of only 11 lines of MATLAB code, we obtain an Elastic Net implementation that naturally utilizes GPU and multi-core CPUs. We demonstrate on twelve real world data sets, that our algorithm yields identical results as the popular (and highly optimized) glmnet implementation but is one or several orders of magnitude faster.", "histories": [["v1", "Sat, 6 Sep 2014 03:12:39 GMT  (226kb)", "http://arxiv.org/abs/1409.1976v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["quan zhou", "wenlin chen", "shiji song", "jacob r gardner", "kilian q weinberger", "yixin chen"], "accepted": true, "id": "1409.1976"}, "pdf": {"name": "1409.1976.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["zhouq10@mails.tsinghua.edu.cn", "wenlinchen@wustl.edu", "shijis@mail.tsinghua.edu.cn", "gardner.jake@wustl.edu", "kilian@wustl.edu", "ychen25@wustl.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.19 76v1 [st at"}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to assert themselves, that they are able to assert themselves."}, {"heading": "2 Notation and Background", "text": "In this case it is the case that in the last two years we only briefly check a part of Elastic Net and SVM.Elastic Net and SVM.Elastic Net. In the regression scenario, which we have provided with a data set, we are ni = 1, where each xi-Rp and the labels are really evaluated, i.e. yi _ R _ R _ 1, y1 _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _ E _"}, {"heading": "3 The Reduction of Elastic Net to SVM", "text": "In this section, we derive the equivalence between Elastic Mesh and SVM and reduce problem (1) to a specific instance of the SVM optimization problem (3). We start with the Elastic Mesh Week as specified in (1). First, we divide the lens and the constraint by t and the replacement in a rescaled weight vector, \u03b2: = 1t \u03b2.This step allows us to completely absorb and rewrite the constant t into the lens (1) asmin \u03b2 and the constraint by t and the replacement in a rescaled weight vector, \u03b2 and 2 s.t. To simplify the L1 constraint, we follow [24] and divide ourselves into two sets of non-negative variables representing positive components."}, {"heading": "4 Related Work", "text": "The elasticity of the mesh has been used in many machine learning applications, but little effort has been made to solve the problem of the elastic mesh. The elasticity of the algorithm has become the dominant strategy for optimization, but the elasticity of the mesh is the Glmnet package developed by Friedman. The elasticity of the elasticity is usually written in the language of the format, and the elasticity of the elasticity is highly optimized. As far as we know, it is the fastest solution for the elastic mesh. The elasticity of the elasticity is the result that the elasticity of the algorithm is extremely hard to parallelise the elasticity of the algorithm."}, {"heading": "5 Experimental Results and Conclusion", "text": "This year we have to deal with a series of projects that deal with the question of whether and to what extent the people of the United States, Europe and the whole world will be able to represent their interests and interests. (...) We have it in our hand to represent their interests and interests. (...) We have it in our hand to represent their interests and interests. (...) We have it in our hand to represent their interests and interests. (...) We have it not easy. (...) We do not have it easy. (...) We do not have it easy. (...) We do not have it easy. (...) We do not have it easy. (...) We do not have it easy. (...) We have it easy. (...). \"(...).\" (It. \"(It.).\" (It.). \"(It.\" (It.). \"(It.).\" (It. \"(It.).\" (It. \"(It.).\" (It. \"(It.).\" (It. \"It.\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" It. \"(It.).\" (It. \"(It.\" It. \"(It.).\" (It. \"(It.\" It. \"It.\" (It.). \"(It.\" It. \"(It.).\" (It. \"(It.\" It. \"(It.).\" (It. \"It.\" (It. \").\" (It. (It.). \"(It.\" (It. \"). (It.\" It. \"(It.\"). \"(It. (It.\" It. \").\" (It. (It. (It. \"). (It.\"). (It. (It. (It.). (It. (It. (It.). (It. (It. \"). (It.\"). (It. (It. (It.). (It. (It. \"). (It. (It. (It. (It.). (It. (It. (It.). (It. (It."}, {"heading": "5.1 Discussion", "text": "The use of algorithmic reduction to achieve parallelization and improved scalability has several highly compelling advantages: 1. No new learning algorithm needs to be implemented and manually optimized (except for the small transformation code); 2. The burden of maintaining the code is reduced to the only highly optimized (SVM) algorithm; 3. The implementation is very reliable from the outset, since almost all execution time is spent in a well-established and tested implementation; 4. Finally, the target algorithm can be much more naturally devoted to parallelization (e.g. CUBLAS for NVIDIA GPUs http: / / tinyurl.com / cublas.) The square SVM formulation can be solved almost entirely with large matrix operations that are already parallelized (and maintained) by high-performance experts using BLAS libraries (e.g. CUBLAS for NVIUs GUSPUS.com: http: / lel.com /)."}], "references": [{"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMP- STAT\u20192010, pages 177\u2013186. Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["J. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin"], "venue": "ICML, pages 321\u2013328,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Libsvm: a library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Psvm: Parallelizing support vector machines on distributed computers", "author": ["E. Chang", "K. Zhu", "H. Wang", "H. Bai", "J. Li", "Z Qiu", "H Cui"], "venue": "NIPS, pages 257\u2013264,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Training a support vector machine in the primal", "author": ["O. Chapelle"], "venue": "Neural Computation, 19(5):1155\u20131178,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "A gpu-tailored approach for training kernelized svms", "author": ["A. Cotter", "N. Srebro", "J. Keshet"], "venue": "SIGKDD, pages 805\u2013813,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["R. Fan", "K. Chang", "C. Hsieh", "X. Wang", "C. Lin"], "venue": "The Journal of Machine Learning Research, 9:1871\u20131874,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of statistical software, 33(1):1,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Gene expression prediction by soft integration and the elastic netbest performance of the dream3 gene expression challenge", "author": ["M. Gustafsson", "M. H\u00f6rnquist"], "venue": "PLoS One, 5(2):e9134,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "The elements of statistical learning, volume 2", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Beating sgd: Learning svms in sublinear time", "author": ["E. Hazan", "T. Koren", "N. Srebro"], "venue": "NIPS, pages 1233\u20131241,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["C. Hsieh", "K. Chang", "C. Lin", "S.a Keerthi", "S. Sundararajan"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "An equivalence between the lasso and support vector machines", "author": ["M. Jaggi"], "venue": "arXiv preprint arXiv:1303.1152,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217\u2013226. ACM,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "An interior-point method for large-scale l 1-regularized least squares", "author": ["S. Kim", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky"], "venue": "Selected Topics in Signal Processing, IEEE Journal of, 1(4):606\u2013617,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS, pages 1097\u20131105,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Relating reinforcement learning performance to classification performance", "author": ["J. Langford", "B. Zadrozny"], "venue": "ICML, pages 473\u2013480,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR, volume 2, pages 2169\u20132178,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Distributed support vector machines", "author": ["A. Navia-V\u00e1zquez", "D. Gutierrez-Gonzalez", "E. Parrado-Hern\u00e1ndez", "J.J. Navarro-Abellan"], "venue": "Neural Networks, IEEE Transactions on, 17(4):1091\u20131097,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine learning classifiers and fmri: a tutorial overview", "author": ["F. Pereira", "T. Mitchell", "M. Botvinick"], "venue": "Neuroimage, 45(1):S199\u2013S209,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines. technical report msr-tr-98-14", "author": ["J. Platt"], "venue": "Microsoft Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Least squares optimization with l1-norm regularization", "author": ["M. Schmidt"], "venue": "CS542B Project Report,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Parallel support vector machines in practice", "author": ["S. Tyree", "J. Gardner", "K.Q. Weinberger", "K. Agrawal", "J. Tran"], "venue": "arXiv preprint arXiv:1404.1066,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Z. Xu", "K.Q. Weinberger", "O. Chapelle"], "venue": "ICML, pages 1175\u20131182,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301\u2013320,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "1 Introduction The Elastic Net [28] and Lasso as a special case [12] are arguably two of the most celebrated and widely used feature selection algorithms of the past decade.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "1 Introduction The Elastic Net [28] and Lasso as a special case [12] are arguably two of the most celebrated and widely used feature selection algorithms of the past decade.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "For example in fMRI classification [22] one can easily obtain data sets with p> 1, 000, 000 voxels.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Similarly in genetics [11] genome-wide predictions often have millions of features.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "For example, deep (convolutional) neural networks can naturally take advantage of multiple GPUs [18]; support vector machines (SVM) have been ported to GPUs [8, 26], multi-core CPUs [7, 26] and even distributed systems [6, 21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "For example, deep (convolutional) neural networks can naturally take advantage of multiple GPUs [18]; support vector machines (SVM) have been ported to GPUs [8, 26], multi-core CPUs [7, 26] and even distributed systems [6, 21].", "startOffset": 157, "endOffset": 164}, {"referenceID": 23, "context": "For example, deep (convolutional) neural networks can naturally take advantage of multiple GPUs [18]; support vector machines (SVM) have been ported to GPUs [8, 26], multi-core CPUs [7, 26] and even distributed systems [6, 21].", "startOffset": 157, "endOffset": 164}, {"referenceID": 5, "context": "For example, deep (convolutional) neural networks can naturally take advantage of multiple GPUs [18]; support vector machines (SVM) have been ported to GPUs [8, 26], multi-core CPUs [7, 26] and even distributed systems [6, 21].", "startOffset": 182, "endOffset": 189}, {"referenceID": 23, "context": "For example, deep (convolutional) neural networks can naturally take advantage of multiple GPUs [18]; support vector machines (SVM) have been ported to GPUs [8, 26], multi-core CPUs [7, 26] and even distributed systems [6, 21].", "startOffset": 182, "endOffset": 189}, {"referenceID": 4, "context": "For example, deep (convolutional) neural networks can naturally take advantage of multiple GPUs [18]; support vector machines (SVM) have been ported to GPUs [8, 26], multi-core CPUs [7, 26] and even distributed systems [6, 21].", "startOffset": 219, "endOffset": 226}, {"referenceID": 19, "context": "For example, deep (convolutional) neural networks can naturally take advantage of multiple GPUs [18]; support vector machines (SVM) have been ported to GPUs [8, 26], multi-core CPUs [7, 26] and even distributed systems [6, 21].", "startOffset": 219, "endOffset": 226}, {"referenceID": 7, "context": "Although not originally parallelized, liblinear [9] utilizes clever dual coordinate ascent updates to drastically speed up linear SVMs.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "One of the most popular parallel implementations of Lasso may be Shotgun [4], which however in our experiments does often not outperform the (admittedly highly optimized) single-core Elastic Net implementation glmnet by Friedman [10].", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "One of the most popular parallel implementations of Lasso may be Shotgun [4], which however in our experiments does often not outperform the (admittedly highly optimized) single-core Elastic Net implementation glmnet by Friedman [10].", "startOffset": 229, "endOffset": 233}, {"referenceID": 6, "context": "For example, the GT-SVM implementation of kernel-SVM for GPUs uses handwritten CUDA kernels [8] to truly utilize the computing power of modern graphics cards.", "startOffset": 92, "endOffset": 95}, {"referenceID": 13, "context": "Instead of proposing a new hand designed parallel implementation of the core algorithm, we take inspiration from recent work on machine learning reductions [15, 19] and we reduce the Elastic Net to the squared hinge-loss SVM (without a bias term).", "startOffset": 156, "endOffset": 164}, {"referenceID": 17, "context": "Instead of proposing a new hand designed parallel implementation of the core algorithm, we take inspiration from recent work on machine learning reductions [15, 19] and we reduce the Elastic Net to the squared hinge-loss SVM (without a bias term).", "startOffset": 156, "endOffset": 164}, {"referenceID": 4, "context": "The resulting algorithm, which we refer to as Support Vector Elastic Net (SVEN), naturally takes advantage of the vast existing work on parallel SVMs, immediately providing highly efficient Elastic Net and Lasso implementations on GPUs, multi-core CPUs and distributed systems [6, 8, 9, 21, 26].", "startOffset": 277, "endOffset": 294}, {"referenceID": 6, "context": "The resulting algorithm, which we refer to as Support Vector Elastic Net (SVEN), naturally takes advantage of the vast existing work on parallel SVMs, immediately providing highly efficient Elastic Net and Lasso implementations on GPUs, multi-core CPUs and distributed systems [6, 8, 9, 21, 26].", "startOffset": 277, "endOffset": 294}, {"referenceID": 7, "context": "The resulting algorithm, which we refer to as Support Vector Elastic Net (SVEN), naturally takes advantage of the vast existing work on parallel SVMs, immediately providing highly efficient Elastic Net and Lasso implementations on GPUs, multi-core CPUs and distributed systems [6, 8, 9, 21, 26].", "startOffset": 277, "endOffset": 294}, {"referenceID": 19, "context": "The resulting algorithm, which we refer to as Support Vector Elastic Net (SVEN), naturally takes advantage of the vast existing work on parallel SVMs, immediately providing highly efficient Elastic Net and Lasso implementations on GPUs, multi-core CPUs and distributed systems [6, 8, 9, 21, 26].", "startOffset": 277, "endOffset": 294}, {"referenceID": 23, "context": "The resulting algorithm, which we refer to as Support Vector Elastic Net (SVEN), naturally takes advantage of the vast existing work on parallel SVMs, immediately providing highly efficient Elastic Net and Lasso implementations on GPUs, multi-core CPUs and distributed systems [6, 8, 9, 21, 26].", "startOffset": 277, "endOffset": 294}, {"referenceID": 25, "context": "As in [28], we assume throughout that the response vector is centered and all features are normalized.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "The Elastic Net [28] learns a (sparse) linear model to predict yi from xi by minimizing the squared loss with L2-regularization and an L1-norm constraint, min \u03b2\u2208Rp \u2016X\u03b2 \u2212 y\u20162 + \u03bb2\u2016\u03b2\u2016 2 2 such that |\u03b2|1 \u2264 t, (1) where \u03b2 = [\u03b21, .", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "In the case where \u03bb2=0, the Elastic Net reduces to the Lasso [12] as a special case.", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "Without replicating the derivation [14, 25], we state the dual problem of (2) as: min \u03b1i\u22650 \u2016\u1e90\u03b1\u20162 + 1 2C m", "startOffset": 35, "endOffset": 43}, {"referenceID": 5, "context": "Both formulations of the SVM can be solved particularly efficiently on modern hardware with Newton\u2019s Method [7, 9], which offloads the majority of the computation onto matrix operations and therefore can be vectorized and parallelized to achieve near peak computing performance [26].", "startOffset": 108, "endOffset": 114}, {"referenceID": 7, "context": "Both formulations of the SVM can be solved particularly efficiently on modern hardware with Newton\u2019s Method [7, 9], which offloads the majority of the computation onto matrix operations and therefore can be vectorized and parallelized to achieve near peak computing performance [26].", "startOffset": 108, "endOffset": 114}, {"referenceID": 23, "context": "Both formulations of the SVM can be solved particularly efficiently on modern hardware with Newton\u2019s Method [7, 9], which offloads the majority of the computation onto matrix operations and therefore can be vectorized and parallelized to achieve near peak computing performance [26].", "startOffset": 278, "endOffset": 282}, {"referenceID": 22, "context": "(4) To simplify the L1 constraint, we follow [24] and split \u03b2 into two sets of non-negative variables, representing positive components \u03b2 \u2265 0 and negative components \u03b2 \u2265 0, i.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "Barring the (uninteresting) case with extremely large t \u226b 0, the L1-norm constraint in (1) will always be tight [3], i.", "startOffset": 112, "endOffset": 115}, {"referenceID": 10, "context": "(If t is extremely large, (1) is equivalent to ridge regression [12], which typically yields completely dense (non-sparse) solutions.", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "Many solvers [2, 7, 9, 26] have been developed for the linear SVM problem (2).", "startOffset": 13, "endOffset": 26}, {"referenceID": 5, "context": "Many solvers [2, 7, 9, 26] have been developed for the linear SVM problem (2).", "startOffset": 13, "endOffset": 26}, {"referenceID": 7, "context": "Many solvers [2, 7, 9, 26] have been developed for the linear SVM problem (2).", "startOffset": 13, "endOffset": 26}, {"referenceID": 23, "context": "Many solvers [2, 7, 9, 26] have been developed for the linear SVM problem (2).", "startOffset": 13, "endOffset": 26}, {"referenceID": 7, "context": "liblinear [9]) and for others it is trivial to remove [7].", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "liblinear [9]) and for others it is trivial to remove [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "In our experiments we use an SVM implementation based on Chapelle\u2019s original exact linear SVM implementation [7] (which can solve the dual and primal formulation respectively).", "startOffset": 109, "endOffset": 112}, {"referenceID": 23, "context": "[26], the individual Newton steps can be parallelized trivially by using This is not well defined if |\u03b1\u2217|1=0, which is the degenerate case when the SVM selects no support vectors, \u03b1=0, and which is not meaningful without bias term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "If \u03bb2\u21920 the Elastic Net becomes LASSO [12], which has previously been shown to be equivalent to the hard-margin SVM [15].", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "If \u03bb2\u21920 the Elastic Net becomes LASSO [12], which has previously been shown to be equivalent to the hard-margin SVM [15].", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "Chapelle\u2019s MATLAB implementation can scale in the worst case either O(n) (primal mode) or O(p) (dual mode) [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 14, "context": "SVM implementations with other running times can easily be adapted to our setting, for example [16] would allow training in time O(np) and recent work even suggests solvers with sub-linear time complexity [13] (although the solution might be insufficiently exact for our purposes in practice).", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "SVM implementations with other running times can easily be adapted to our setting, for example [16] would allow training in time O(np) and recent work even suggests solvers with sub-linear time complexity [13] (although the solution might be insufficiently exact for our purposes in practice).", "startOffset": 205, "endOffset": 209}, {"referenceID": 8, "context": "The state-of-the-art single-core implementation for solving the Elastic Net problem is the glmnet package developed by Friedman [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 2, "context": "The Shotgun algorithm proposed by [4] is among the first to parallelize coordinate descent for Lasso.", "startOffset": 34, "endOffset": 37}, {"referenceID": 15, "context": "The L1 LS algorithm proposed by [17] transforms the Lasso to its dual form directly and uses a log-barrier interior point method for optimization.", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "On the SVM side, one of the most popular and user-friendly implementations is the libsvm library [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 21, "context": "However, it is optimized to solve kernel SVM problems using sequential minimal optimization (SMO) [23], which is not efficient for the specific case of linear SVM.", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": "The liblinear library [9] is specially tailored for linear SVMs, including the squared hinge loss version.", "startOffset": 22, "endOffset": 25}, {"referenceID": 23, "context": "However, we did find that on modern multi-core platforms (with and without GPU acceleration) algorithms that actively seek updates through matrix operations [26] tend to be substantially faster (in both settings, p\u226bn and n\u226bp).", "startOffset": 157, "endOffset": 161}, {"referenceID": 13, "context": "Our work is inspired by a recent theoretical contribution, Jaggi 2013 [15], which reveals the close relation between Lasso and hard-margin SVMs.", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "For comparison, we have a single-threaded CPU baseline method: glmnet [10], a popular and highly optimized Elastic Net software package.", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "[4] parallelizes coordinate gradient descent.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "To illustrate the equivalence, Figure 1 shows the regularization path of SVEN (GPU) and glmnet on the prostate cancer data used in [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 18, "context": "5 Scene15, a scene recognition data set [20, 27] where we use the binary class 6 and 7 for feature selection; PEMS [1], a dataset that describes the occupancy rate, between 0 and 1, of different car lanes of San Francisco bay area freeways.", "startOffset": 40, "endOffset": 48}, {"referenceID": 24, "context": "5 Scene15, a scene recognition data set [20, 27] where we use the binary class 6 and 7 for feature selection; PEMS [1], a dataset that describes the occupancy rate, between 0 and 1, of different car lanes of San Francisco bay area freeways.", "startOffset": 40, "endOffset": 48}], "year": 2014, "abstractText": "The past years have witnessed many dedicated open-source projects that built and maintain implementations of Support Vector Machines (SVM), parallelized for GPU, multi-core CPUs and distributed systems. Up to this point, no comparable effort has been made to parallelize the Elastic Net, despite its popularity in many high impact applications, including genetics, neuroscience and systems biology. The first contribution in this paper is of theoretical nature. We establish a tight link between two seemingly different algorithms and prove that Elastic Net regression can be reduced to SVM with squared hinge loss classification. Our second contribution is to derive a practical algorithm based on this reduction. The reduction enables us to utilize prior efforts in speeding up and parallelizing SVMs to obtain a highly optimized and parallel solver for the Elastic Net and Lasso. With a simple wrapper, consisting of only 11 lines of MATLAB code, we obtain an Elastic Net implementation that naturally utilizes GPU and multi-core CPUs. We demonstrate on twelve real world data sets, that our algorithm yields identical results as the popular (and highly optimized) glmnet implementation but is one or several orders of magnitude faster.", "creator": "LaTeX with hyperref package"}}}