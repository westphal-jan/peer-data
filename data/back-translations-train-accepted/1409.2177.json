{"id": "1409.2177", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2014", "title": "The Large Margin Mechanism for Differentially Private Maximization", "abstract": "A basic problem in the design of privacy-preserving algorithms is the private maximization problem: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine-learning.", "histories": [["v1", "Sun, 7 Sep 2014 23:51:00 GMT  (22kb)", "http://arxiv.org/abs/1409.2177v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IT math.IT math.ST stat.TH", "authors": ["kamalika chaudhuri", "daniel j hsu", "shuang song"], "accepted": true, "id": "1409.2177"}, "pdf": {"name": "1409.2177.pdf", "metadata": {"source": "CRF", "title": "The Large Margin Mechanism for Differentially Private Maximization", "authors": ["Kamalika Chaudhuri", "Daniel Hsu", "Shuang Song"], "emails": ["kamalika@cs.ucsd.edu,", "djhsu@cs.columbia.edu,", "shs037@eng.ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.21 77v1 [cs.LG] 7 Previous algorithms for this problem are either range-dependent - i.e. their usefulness decreases with the size of the universe - or only applicable to very limited functional classes. This work provides the first universal, range-independent algorithm for private maximization, which guarantees an approximately differentiated privacy. Its applicability is demonstrated in two basic tasks in data mining and machine learning."}, {"heading": "1 Introduction", "text": "An algorithm for processing sensitive data forces a differentiated privacy by ensuring that the likelihood of an outcome does not change significantly when an individual changes their privacy. Privacy is typically guaranteed by adding noise either to the sensitive data or to the output of an algorithm that processes the sensitive data. For many machine learning tasks, this leads to a corresponding deterioration in accuracy or usefulness. Thus, a key challenge in differential private learning is to design algorithms with better goals between privacy and usefulness for a variety of statistics and utilities. In this paper, we examine the private maximization problem, a fundamental problem that arises as we design privacy-preserving algorithms for a number of statistical and machine learning applications."}, {"heading": "2 Background", "text": "This section deals with different privacy issues and presents the problem of private maximization."}, {"heading": "2.1 Definitions of Differential Privacy and Private Maximization", "text": "For the rest of the work, we consider randomized algorithms A: Xn \u2192 \u0445 (S), which use as input data sets the data sets of n individuals and output values in a range S. Two datasets D, D \u00b2, Xn are called neighbors if they differ in the input of a single person. A function \u03c6: Xn \u2192 R is L-Lipschitz if the data differs from other individuals. A randomized algorithmA: Xn \u00b2 -L for all neighbors D, D \u00b2, X.The following definitions of (approximate) differential privacy come from [17] and [20].Definition 1 (differential privacy) refers to the value of laptop data."}, {"heading": "2.2 Previous Algorithms for Private Maximization", "text": "The default algorithm for private maximization is the exponential mechanism [28]. Given a privacy parameter \u03b1 > 0, the exponential mechanism randomly draws an item i-U with a probability pi-en\u03b1f (i, D) / 2; this guarantees alpha-differential privacy. While the exponential mechanism is widely used due to its generality, a major limitation is its range dependence - i.e., its usefulness decreases with the universe size K. To be more precise, consider the following example in which X: = [K] andf (i, D): = 1n Greater deterioration of its range (n): Dj \u2265 i) | (1) is the j-th entry in the dataset D). If D = (1, 1,"}, {"heading": "3 The Large Margin Mechanism", "text": "We now have our new algorithm for private maximisation, the so-called large margin mechanism, together with its privacy and utility guarantees."}, {"heading": "3.1 Margins", "text": "We first introduce the term margin on which our algorithm is based. Given an instance of the private maximization problem and a positive integer (1), we propose that f (K + 1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin (1) -margin) -margin (1) -margin (1) -margin) (1) -margin (1) (1) -margin) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1 (1) (1) (1) (1) (1 (1) (1) (1 (1) (1) (1) (1 (1) (1) (1 (1) (1) (1) (1 (1) (1) (1 (1) (1 (1) (1) (1) (1) (1 (1) (1) (1) (1 (1) (1 (1) (1) (1) (1) (1) (1 (1) (1) (1 (1 (1) (1) (1 (1) (1 (1) (1) (1 (1) (1) (1 (1) (1) (1"}, {"heading": "3.2 Algorithm", "text": "The lower limit in Theorem 1 suggests the following algorithm. First, privately determine a pair (, \u03b3), with as small as possible and \u03b3 = () / (\u03b1n), so that D meets the (, \u03b3) margin condition. Then, run the exponential mechanism on the specified UU of objects with the highest f (, D) values. This sounds fairly natural and simple, but a knee-jerk reaction to this approach is that the specified UU margin itself depends on the sensitive dataset D, and it can have a high sensitivity in the sense that belonging to many objects can change in U. Thus, the differentiated private calculation of Uproblematic appears. It turns out that we do not have to guarantee the privacy of the specified U @ - @ dataset, but rather only of a valid (, \u03b3) pair. This is mainly due to the fact that the Margin fulfills (D) condition."}, {"heading": "3.3 Privacy and Utility Guarantees", "text": "Theorem 2 (Privacy Guarantee). lmm (\u03b1, \u03b4, \u00b7) fulfills (\u03b1, \u03b4) approximate differential privacy. The proof for theorem 2 is in Appendix A. The following theorem, proven in Appendix B, provides a warranty on the usefulness of lmm.Theorem 3 (Warranty of Use). Choose any course of action (0, 1). Suppose D-Xn meets the (, \u03b3) boundary condition except for what happened."}, {"heading": "4 Illustrative Applications", "text": "We now describe applications from lmm to problems in the fields of data mining and machine learning."}, {"heading": "4.1 Private Frequent Itemset Mining", "text": "Frequent Itemset Mining (FIM) is the following popular data mining problem: In view of users \"purchase lists (say, for an online grocery store), the goal is to find the sets of items that are most frequently purchased together. [5] The work of [5] provides the first differentiated private algorithms for FIM. However, since these algorithms are based on the exponential mechanism and the maximum laplace mechanism, their performance deteriorates with the total number of possible items. Subsequent algorithms take advantage of other characteristics of items or avoid directly finding the most common items [8, 15, 27, 34]. Let me be the set of items that can be purchased \u2212 and let B be the maximum length of a user's purchase list. Let U 2I be the family of items of interest. For convenience, let U: = (I r) - i.e. all items of size - and let us consider the problem of user selection (approximately)."}, {"heading": "4.2 Private PAC Learning", "text": "We are looking at private PAC learning with a Class H finite hypothesis with a limited UK dimension d [25]. Here, the Dataset D consists of n designated training examples drawn from a fixed distribution. The error rate in the literature is the probability that there is a random example from the same distribution. The goal is to return a hypothesis that h with an error is as low as possible. The standard method, which has been well studied in the literature, simply gives the minimizer h of the empirical error rate e rr (h, D) calculated on the training data D, but this does not guarantee differentiated privacy. Instead, the work of [25] uses the exponential mechanism to choose a hypothesis hEM."}, {"heading": "5 Additional Related Work", "text": "There has been a great deal of differential privacy work over the last decade for a wide range of statistical and machine learning tasks [1, 6, 13, 21, 24, 30, 33]; for overviews see [18] and [29]. Specifically, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypotheses testing [32]. A distinction between pure and approximate differential privacy has been demonstrated in several previous papers [3, 19, 31]. The first approximate differentiation between private algorithms that achieve separation is the Propose Test Release (PTR) framework [19]. Given a function, PTR determines an upper limit between their local sensitivity at the input data set and spatial deviation by a search method."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we have introduced the first generic and range-independent algorithm for near-differentiated private maximization, which automatically adapts to the large margin properties available in the sensitive dataset and relies on worst-case guarantees when such features are absent. We have demonstrated the applicability of the algorithm in two fundamental issues of data mining and machine learning; in future work, we plan to explore other applications where range independence is a significant advantage. Recognition. We thank an anonymous reviewer for proposing the simpler variant of lmm based on the exponential mechanism. (The original version of lmm used a maximum truncated exponential mechanism that provides the same guarantees up to constant factors.) This work was partially supported by the NIH under U54 HL108460 and the NSF under IIS 1253942."}, {"heading": "A Privacy Analysis", "text": "In this section we present the proof of theory 2. We rely on the composition results for approximate differential privacy to analyze the three parts of algorithm 1: \u2022 differential privacy of release m after step 3 \u2212 \u2212 \u2212 \u2212 r (except step 12). \u2212 r (approximate differential privacy of release I after step 15. We do this explicitly by combining these parts into algorithm 2 (M), algorithm 3 (S) and algorithm 4 (A), so that we can write algorithm 1 as follows (according to the definitions of T (r) and t (r): 1. m: = M (\u03b1 / 3, D).2., T (2),., T (K \u2212 1),."}, {"heading": "B Utility Analysis", "text": "The proof of theorem 3. On the basis of tail boundaries for the Laplace distribution it results that with probability at least 1 \u2212 \u03b7 / 2, Z \u2265 \u2212 3 \u03b1 ln 3 \u03b7, G \u2264 6 \u03b1 ln 3 \u03b7, Z\u0439 \u2264 12 \u03b1 ln 3 \u03b7. In this case, the assumption that D fulfils the boundary condition implies that (f (1) (D) + Z / n) \u2212 f (3 + 1) (D) > (Z3 + G) / n + T (3), so that the while loop ends with 0. Furthermore, the probability distribution p in step 14 of the algorithm 1 assigns a probability mass to the set of items i withf (i, D) \u2264 f (1) (D) \u2212 6 ln (2 / n) n\u03b1."}, {"heading": "C Proofs of Lower Bounds", "text": "The proof for theorem 1: We construct the private maximization problem as follows: Let domain X: = 2U (subsets of items), and define f: U \u2212 \u2212 n \u00b2 (i, D): = 1nn \u00b2 s = 11 {i \u00b2 Ds}. In other words, the function f (i, \u00b7) is the fraction of items containing items. It is easy to see that f \u2212 n) -n \u00b2 -Lipschitz is for all i \u00b2 -U. Let m: = min {n / 2, Protocol ((((\u2212 1) / 2) / \u03b1 \u00b2. We define a collection of items D1, D2,., DXn with the following properties: 1. For each i, the first n / 2 entries of Di are equal to Itefs: = = {1, 2,."}], "references": [{"title": "Private empirical risk minimization, revisited", "author": ["Raef Bassily", "Adam Smith", "Abhradeep Thakurta"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Bounds on the sample complexity for private learning and private data release", "author": ["Amos Beimel", "Shiva Prasad Kasiviswanathan", "Kobbi Nissim"], "venue": "In Theory of Cryptography,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Private learning and sanitization: Pure vs. approximate differential privacy", "author": ["Amos Beimel", "Kobbi Nissim", "Uri Stemmer"], "venue": "In RANDOM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Characterizing the sample complexity of private learners", "author": ["Amos Beimel", "Kobbi Nissim", "Uri Stemmer"], "venue": "In ITCS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Discovering frequent patterns in sensitive data", "author": ["Raghav Bhaskar", "Srivatsan Laxman", "Adam Smith", "Abhradeep Thakurta"], "venue": "In KDD,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Practical privacy: the SuLQ framework", "author": ["A. Blum", "C. Dwork", "F. McSherry", "K. Nissim"], "venue": "In PODS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "A learning theory approach to noninteractive database privacy", "author": ["Avrim Blum", "Katrina Ligett", "Aaron Roth"], "venue": "Journal of the ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Mining frequent patterns with differential privacy", "author": ["Luca Bonomi", "Li Xiong"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Fingerprinting codes and the price of approximate differential privacy", "author": ["Mark Bun", "Jonathan Ullman", "Salil Vadhan"], "venue": "In STOC,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sample complexity bounds for differentially private learning", "author": ["Kamalika Chaudhuri", "Daniel Hsu"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Convergence rates for differentially private statistical estimation", "author": ["Kamalika Chaudhuri", "Daniel Hsu"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A stability-based validation procedure for differentially private machine learning", "author": ["Kamalika Chaudhuri", "Staal A Vinterbo"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Differentially private empirical risk minimization", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Near-optimal differentially private principal components", "author": ["Kamalika Chaudhuri", "Anand D. Sarwate", "Kaushik Sinha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Publishing set-valued data via differential privacy", "author": ["Rui Chen", "Noman Mohammed", "Benjamin CM Fung", "Bipin C Desai", "Li Xiong"], "venue": "In VLDB,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Lower bounds in differential privacy", "author": ["Anindya De"], "venue": "In Ronald Cramer, editor, Theory of Cryptography,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "In Theory of Cryptography,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Differential privacy: A survey of results", "author": ["Cynthia Dwork"], "venue": "In Theory and Applications of Models of Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Differential privacy and robust statistics", "author": ["Cynthia Dwork", "Jing Lei"], "venue": "In Proceedings of the 41st annual ACM symposium on Theory of computing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["Cynthia Dwork", "Krishnaram Kenthapadi", "Frank McSherry", "Ilya Mironov", "Moni Naor"], "venue": "In Advances in Cryptology-EUROCRYPT", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Data mining with differential privacy", "author": ["A. Friedman", "A. Schuster"], "venue": "In KDD,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "A multiplicative weights mechanism for privacy-preserving data analysis", "author": ["Moritz Hardt", "Guy N Rothblum"], "venue": "In FOCS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "On the geometry of differential privacy", "author": ["Moritz Hardt", "Kunal Talwar"], "venue": "In Proceedings of the 42nd ACM symposium on Theory of computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Differentially private online learning", "author": ["Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta"], "venue": "In COLT,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "What can we learn privately", "author": ["Shiva Prasad Kasiviswanathan", "Homin K Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "SIAM Journal on Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Computable shell decomposition bounds", "author": ["John Langford", "David McAllester"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Privbasis: frequent itemset mining with differential privacy", "author": ["Ninghui Li", "Wahbeh Qardaji", "Dong Su", "Jianneng Cao"], "venue": "In VLDB,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Mechanism design via differential privacy", "author": ["Frank McSherry", "Kunal Talwar"], "venue": "In FOCS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Signal processing and machine learning with differential privacy: Algorithms and challenges for continuous data", "author": ["A.D. Sarwate", "K. Chaudhuri"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Privacy-preserving statistical estimation with optimal convergence rates", "author": ["Adam Smith"], "venue": "In STOC,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Differentially private feature selection via stability arguments, and the robustness of the lasso", "author": ["Adam Smith", "Abhradeep Thakurta"], "venue": "In COLT,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Privacy-preserving data sharing for genome-wide association studies", "author": ["Caroline Uhler", "Aleksandra B. Slavkovic", "Stephen E. Fienberg"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "A statistical framework for differential privacy", "author": ["Larry Wasserman", "Shuheng Zhou"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "Differential privacy [17] is a cryptographically-motivated definition of privacy that has recently gained significant attention in the data mining and machine learning communities.", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).", "startOffset": 106, "endOffset": 110}, {"referenceID": 20, "context": "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).", "startOffset": 184, "endOffset": 188}, {"referenceID": 4, "context": "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).", "startOffset": 260, "endOffset": 263}, {"referenceID": 11, "context": "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).", "startOffset": 321, "endOffset": 325}, {"referenceID": 31, "context": "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).", "startOffset": 404, "endOffset": 408}, {"referenceID": 27, "context": "The most common algorithms for this problem are the exponential mechanism [28], and a computationally efficient alternative from [5], which we call the max-of-Laplaces mechanism.", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "The most common algorithms for this problem are the exponential mechanism [28], and a computationally efficient alternative from [5], which we call the max-of-Laplaces mechanism.", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "This problem has also been addressed by recent algorithms of [3, 31], who provide algorithms that are range-independent and satisfy approximate differential privacy, a relaxed version of differential privacy.", "startOffset": 61, "endOffset": 68}, {"referenceID": 30, "context": "This problem has also been addressed by recent algorithms of [3, 31], who provide algorithms that are range-independent and satisfy approximate differential privacy, a relaxed version of differential privacy.", "startOffset": 61, "endOffset": 68}, {"referenceID": 30, "context": "For example, the algorithm from [31] provides a range-independent result only when there is a single clear maximizer i such that f(i, D) is greater than the second highest value by some margin; the algorithm from [3] also has restrictive conditions that limit its applicability (see Section 2.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "For example, the algorithm from [31] provides a range-independent result only when there is a single clear maximizer i such that f(i, D) is greater than the second highest value by some margin; the algorithm from [3] also has restrictive conditions that limit its applicability (see Section 2.", "startOffset": 213, "endOffset": 216}, {"referenceID": 25, "context": "For the second problem, our algorithm achieves tight sample complexity bounds for private PAC learning analogous to the shell bounds of [26] for non-private learning.", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "The following definitions of (approximate) differential privacy are from [17] and [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "The following definitions of (approximate) differential privacy are from [17] and [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "Smaller values of the privacy parameters \u03b1 > 0 and \u03b4 \u2208 [0, 1] imply stronger guarantees of privacy.", "startOffset": 55, "endOffset": 61}, {"referenceID": 16, "context": ") Note that this problem is different from private release of the maximum value of f(\u00b7, D); a solution for the latter is easily obtained by adding Laplace noise with standard deviation O(1/(\u03b1n)) to maxi\u2208U f(i,D) [17].", "startOffset": 212, "endOffset": 216}, {"referenceID": 27, "context": "2 Previous Algorithms for Private Maximization The standard algorithm for private maximization is the exponential mechanism [28].", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "Another general purpose algorithm is the max-of-Laplaces mechanism from [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.", "startOffset": 65, "endOffset": 83}, {"referenceID": 6, "context": "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.", "startOffset": 65, "endOffset": 83}, {"referenceID": 9, "context": "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.", "startOffset": 65, "endOffset": 83}, {"referenceID": 10, "context": "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.", "startOffset": 65, "endOffset": 83}, {"referenceID": 22, "context": "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.", "startOffset": 65, "endOffset": 83}, {"referenceID": 30, "context": "The approximate differentially private algorithm from [31] applies in the case where there is a single clear maximizer whose value is much larger than that of the rest.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "[3] provides an approximate differentially private algorithm that applies when f satisfies a condition called l-bounded growth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "In fact, our notion is more closely related to the shell decomposition bounds of [26], which we discuss in Section 4.", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "Moreover, we can find such a valid (l, \u03b3) pair using a differentially private search procedure based on the sparse vector technique [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "The work of [5] provides the first differentially private algorithms for FIM.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "Subsequent algorithms exploit other properties of itemsets or avoid directly finding the most frequent itemset [8, 15, 27, 34].", "startOffset": 111, "endOffset": 126}, {"referenceID": 14, "context": "Subsequent algorithms exploit other properties of itemsets or avoid directly finding the most frequent itemset [8, 15, 27, 34].", "startOffset": 111, "endOffset": 126}, {"referenceID": 26, "context": "Subsequent algorithms exploit other properties of itemsets or avoid directly finding the most frequent itemset [8, 15, 27, 34].", "startOffset": 111, "endOffset": 126}, {"referenceID": 30, "context": "Finally, unlike [31], our algorithm does not require a gap between the top two itemset frequencies.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "2 Private PAC Learning We now consider private PAC learning with a finite hypothesis class H with bounded VC dimension d [25].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "The work of [25] instead uses the exponential mechanism to select a hypothesis hEM \u2208 H.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "The dependence on log |H| is improved to d log |\u03a3| by [7] when the data entries come from a finite set \u03a3.", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "The subsequent work of [4] introduces the notion of representation dimension, and shows how it relates to differentially private learning in the discrete and finite case, and [3] provides improved convergence bounds with approximate differential privacy that exploit the structure of some specific hypothesis classes.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "The subsequent work of [4] introduces the notion of representation dimension, and shows how it relates to differentially private learning in the discrete and finite case, and [3] provides improved convergence bounds with approximate differential privacy that exploit the structure of some specific hypothesis classes.", "startOffset": 175, "endOffset": 178}, {"referenceID": 9, "context": "For the case of infinite hypothesis classes and continuous data distributions, [10] shows that distribution-free private PAC learning is not generally possible, but distribution-dependent learning can be achieved under certain conditions.", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "Our bound only relies on uniform convergence properties of H, and can be significantly tighter than the bounds from [25] when the number of hypotheses with error close to minh\u2208H err(h) is small.", "startOffset": 116, "endOffset": 120}, {"referenceID": 25, "context": "Indeed, the bounds are a private analogue of the shell bounds of [26], which characterize the structure of the hypothesis class as a function of the properties of a decomposition based on hypotheses\u2019 error rates.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "Following [26], we divide the hypothesis class H into R = O( \u221a n/(d logn)) shells; the t-th shell H(t) is defined by H(t) := { h \u2208 H : err(h) \u2264 min h\u2208H err(h) + C0t \u221a d log(n/\u03b40) n } .", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "The dependence on log |H| from (2) is replaced here by log(|H(t\u2217(n)+1)|/\u03b4), which can be vastly smaller, as discussed in [26].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].", "startOffset": 142, "endOffset": 168}, {"referenceID": 5, "context": "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].", "startOffset": 142, "endOffset": 168}, {"referenceID": 12, "context": "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].", "startOffset": 142, "endOffset": 168}, {"referenceID": 20, "context": "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].", "startOffset": 142, "endOffset": 168}, {"referenceID": 23, "context": "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].", "startOffset": 142, "endOffset": 168}, {"referenceID": 29, "context": "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].", "startOffset": 142, "endOffset": 168}, {"referenceID": 32, "context": "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].", "startOffset": 142, "endOffset": 168}, {"referenceID": 17, "context": "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].", "startOffset": 189, "endOffset": 193}, {"referenceID": 28, "context": "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].", "startOffset": 198, "endOffset": 202}, {"referenceID": 24, "context": "In particular, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypothesis testing [32].", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "In particular, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypothesis testing [32].", "startOffset": 195, "endOffset": 199}, {"referenceID": 11, "context": "In particular, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypothesis testing [32].", "startOffset": 224, "endOffset": 228}, {"referenceID": 31, "context": "In particular, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypothesis testing [32].", "startOffset": 262, "endOffset": 266}, {"referenceID": 2, "context": "A separation between pure and approximate differential privacy has been shown in several previous works [3, 19, 31].", "startOffset": 104, "endOffset": 115}, {"referenceID": 18, "context": "A separation between pure and approximate differential privacy has been shown in several previous works [3, 19, 31].", "startOffset": 104, "endOffset": 115}, {"referenceID": 30, "context": "A separation between pure and approximate differential privacy has been shown in several previous works [3, 19, 31].", "startOffset": 104, "endOffset": 115}, {"referenceID": 18, "context": "The first approximate differentially private algorithm that achieves a separation is the Propose-Test-Release (PTR) framework [19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "In the context of private PAC learning, the work of [3] gives the first separation between pure and approximate differential privacy.", "startOffset": 52, "endOffset": 55}, {"referenceID": 30, "context": "In addition to using the algorithm from [31], they devise two additional algorithmic techniques: a concave maximization procedure for learning intervals, and an algorithm for the private maximization problem under the l-bounded growth condition discussed in Section 2.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].", "startOffset": 69, "endOffset": 83}, {"referenceID": 8, "context": "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].", "startOffset": 69, "endOffset": 83}, {"referenceID": 10, "context": "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].", "startOffset": 69, "endOffset": 83}, {"referenceID": 15, "context": "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].", "startOffset": 69, "endOffset": 83}, {"referenceID": 10, "context": "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "[1] Raef Bassily, Adam Smith, and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Amos Beimel, Kobbi Nissim, and Uri Stemmer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Amos Beimel, Kobbi Nissim, and Uri Stemmer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Raghav Bhaskar, Srivatsan Laxman, Adam Smith, and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Avrim Blum, Katrina Ligett, and Aaron Roth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Luca Bonomi and Li Xiong.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Mark Bun, Jonathan Ullman, and Salil Vadhan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Kamalika Chaudhuri and Daniel Hsu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Kamalika Chaudhuri and Daniel Hsu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Kamalika Chaudhuri and Staal A Vinterbo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Kamalika Chaudhuri, Claire Monteleoni, and Anand D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Kamalika Chaudhuri, Anand D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Rui Chen, Noman Mohammed, Benjamin CM Fung, Bipin C Desai, and Li Xiong.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Anindya De.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Cynthia Dwork.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Cynthia Dwork and Jing Lei.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Moritz Hardt and Guy N Rothblum.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Moritz Hardt and Kunal Talwar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] John Langford and David McAllester.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Ninghui Li, Wahbeh Qardaji, Dong Su, and Jianneng Cao.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Frank McSherry and Kunal Talwar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Adam Smith.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] Adam Smith and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] Caroline Uhler, Aleksandra B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] Larry Wasserman and Shuheng Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Lemma 1 ([17]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "This is an application of the sparse vector technique from [22] that halts as soon as the first \u201cquery\u201d is answered positively.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "By standard composition results for differential privacy [17], Lemma 1, and Lemma 3, the release ofM(D) and S(M(D), D) is (2\u03b1/3)-differentially private.", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "Lemma 6 ([11]).", "startOffset": 9, "endOffset": 13}], "year": 2014, "abstractText": "A basic problem in the design of privacy-preserving algorithms is the private maximization problem: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine-learning. Previous algorithms for this problem are either range-dependent\u2014i.e., their utility diminishes with the size of the universe\u2014or only apply to very restricted function classes. This work provides the first general-purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning.", "creator": "LaTeX with hyperref package"}}}