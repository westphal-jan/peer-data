{"id": "1409.2848", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2014", "title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate", "abstract": "We describe and analyze a simple algorithm for principal component analysis, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to the non-convex PCA problem, using a very different analysis.", "histories": [["v1", "Tue, 9 Sep 2014 19:31:52 GMT  (28kb,D)", "http://arxiv.org/abs/1409.2848v1", null], ["v2", "Sun, 25 Jan 2015 08:28:03 GMT  (32kb,D)", "http://arxiv.org/abs/1409.2848v2", "Some improvements and additions to previous version"], ["v3", "Sun, 19 Apr 2015 14:19:08 GMT  (36kb,D)", "http://arxiv.org/abs/1409.2848v3", "Some improvements and additions to previous version; fixed bug in proof of lemma 1"], ["v4", "Sun, 26 Apr 2015 12:20:12 GMT  (36kb,D)", "http://arxiv.org/abs/1409.2848v4", null], ["v5", "Fri, 31 Jul 2015 04:41:42 GMT  (36kb,D)", "http://arxiv.org/abs/1409.2848v5", "Fixed a minor bug in the proof of lemma 1 (which does not affect the result)"]], "reviews": [], "SUBJECTS": "cs.LG cs.NA math.OC stat.ML", "authors": ["ohad shamir"], "accepted": true, "id": "1409.2848"}, "pdf": {"name": "1409.2848.pdf", "metadata": {"source": "CRF", "title": "A Stochastic PCA Algorithm with an Exponential Convergence Rate", "authors": ["Ohad Shamir"], "emails": ["ohad.shamir@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "This is one of the most common tools for unattended data analysis and processing (1). Given a dataset of n instances x1,.., xn in Rd, we are interested in finding a d \u00b7 k matrix U with orthonormal column that minimizes the particularity of this column, from reduction and data compression to data visualization, and the problem is extremely well researched. In this paper, we will focus on the simplest possible form of this problem, where the projection of the data has the greatest possible variance. Finding this subspace has numerous uses, from dimensionality and data compression to data visualization, and the problem is extremely well researched, where k = 1, and we are interested in finding a single direction along which the variance of the data is maximized (however, how we discuss the algorithm to solve it)."}, {"heading": "2 Algorithm and Analysis", "text": "The Pseudo-codes of our algorithms appear as algorithms 1, 2, 3, 4, 5, 5, 5, 5, 6, 6, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "3 Experiments", "text": "We now turn to some preliminary experimental results that show the performance of the VR-PCA algorithm in the same way. First, we conducted experiments on several synthetic datasets, in which 50,000 examples are extracted from a Gaussian distribution in R1000, with zero mean and covariance matrix I + \u03bbeie > i. The spectrum of this matrix corresponds to another value of 1, 1,..., 1), which corresponds to a eigengap of roughly \u03bb (in practice, due to finite example effects, the eigengap of the data covariance matrix is slightly different).Each dataset corresponds to a different value of \u03bb, 1,. We point out that these datasets do not meet the boundity assumption in our analysis (here the norm of each instance scales like an algorithm), but nevertheless the algorithm seems to work well in practice."}, {"heading": "4 Proof of Thm. 1", "text": "We use c to determine positive numerical constants whose value can vary in different places (even in the same line or expression). < b = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = x = 1 x = 1 x x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 1 x = 2 x = 1 x = 1 x = 0. Part I: Establishing a stochastic recurrence we begin by focusing on a single epoch of algorithms, and a single iteration, and analyze such as 1 \u2212 < v1 > v1 > 2 = 2 x = 1 x = 1 x."}], "references": [{"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "In 2012 50th Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Stochastic optimization of pca with capped msg", "author": ["R. Arora", "A. Cotter", "N. Srebro"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "The fast convergence of incremental pca", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Matrix computations (4", "author": ["G. Golub", "C. van Loan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American statistical association,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1963}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["J. Konecn\u00fd", "P. Richt\u00e1rik"], "venue": "CoRR, abs/1312.1666,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "The method of stochastic approximation for the determination of the least eigenvalue of a symmetrical matrix", "author": ["T.P. Krasulina"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1969}, {"title": "Mixed optimization for smooth functions", "author": ["M. Mahdavi", "L. Zhang", "R. Jin"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1982}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["E. Oja", "J. Karhunen"], "venue": "Journal of mathematical analysis and applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1985}], "referenceMentions": [{"referenceID": 3, "context": "One possible approach is using iterative methods such as power iteration or the Lanczos method [4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 0, "endOffset": 11}, {"referenceID": 10, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 31, "endOffset": 41}, {"referenceID": 1, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 31, "endOffset": 41}, {"referenceID": 5, "context": "VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]).", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]).", "startOffset": 147, "endOffset": 153}, {"referenceID": 6, "context": "VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]).", "startOffset": 147, "endOffset": 153}, {"referenceID": 9, "context": "To understand the structure of the algorithm, it is helpful to consider first the well-known Oja\u2019s algorithm for stochastic PCA optimization [11], on which our algorithm is based.", "startOffset": 141, "endOffset": 145}, {"referenceID": 2, "context": "Recently, [3] gave a rigorous finite-time analysis of this algorithm, showing that if \u03b7t = O(1/t), then under suitable conditions, we get a convergence rate of O(1/T ).", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Inspired by recent variance-reduced stochastic methods for convex optimization [6], we change the algorithm in a way which encourages the variance of the stochastic term to decay over time.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "[3]) to get to this constant accuracy, from which point our algorithm and analysis takes over.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "where \u03bb is the strong convexity parameter of the problem [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "Using the maximal version of the Hoeffding-Azuma inequality [5], it follows that with probability at least 1\u2212 \u03b2, it holds simultaneously for all t = 1, .", "startOffset": 60, "endOffset": 63}], "year": 2017, "abstractText": "We describe and analyze a simple algorithm for principal component analysis, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to the non-convex PCA problem, using a very different analysis.", "creator": "LaTeX with hyperref package"}}}