{"id": "1409.3136", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Sep-2014", "title": "Metric Learning for Temporal Sequence Alignment", "abstract": "In this paper, we propose to learn a Mahalanobis distance to perform alignment of multivariate time series. The learning examples for this task are time series for which the true alignment is known. We cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable. We provide experiments on real data in the audio to audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better performance for the alignment.", "histories": [["v1", "Wed, 10 Sep 2014 16:10:33 GMT  (465kb,D)", "http://arxiv.org/abs/1409.3136v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["r\u00e9mi lajugie", "damien garreau", "francis r bach", "sylvain arlot"], "accepted": true, "id": "1409.3136"}, "pdf": {"name": "1409.3136.pdf", "metadata": {"source": "CRF", "title": "Metric Learning for Temporal Sequence Alignment", "authors": ["Damien Garreau", "R\u00e9mi Lajugie", "Sylvain Arlot"], "emails": ["damien.garreau@ens.fr1,2,", "remi.lajugie@inria.fr1,2,", "sylvain.arlot@ens.fr1,2,", "francis.bach@inria.fr1,2"], "sections": [{"heading": "1 Introduction", "text": "The idea is to establish two similar time systems that share the same structure."}, {"heading": "2 Matricial formulation of alignment problems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notations", "text": "In this thesis, we will consider the alignment problem between two multivariate time series with the same dimension p, but possibly with different lengths TA and TB, namely A-RTA \u00b7 p and B-RTB \u00b7 p. We will refer to the lines of A as a1,.., aTA-Rp and those of B as b1,.., bTB-Rp. From now on, we will refer to the signal pair (A, B) as X as the signal pair (A, B). Let C (X), RTA \u00b7 TB be an arbitrary matrix of affinity pairs associated with the pair X, i.e., C (X) i, j encodes the affinity between A and bj. Our framework can be extended to the case where A and B are multivariate signals of different dimensions, as long as C (X) is well defined. The goal of the alignment task is to create two undiminished sequences of indices and the same length and etc."}, {"heading": "2.2 The Mahalanobis metric", "text": "In many applications, the affinity matrix for a pair of X = (A, B) is calculated by C (A, B) i, j = \u2212 bay, k \u2212 bj, k \u04322. In this paper, we propose to learn the metrix to compare the comparison matrices and bj instead of using the simple Euclidean metrix. That is, C (X) is parameterized by a matrix in which W-Rp \u00b7 p is the set of semi-defined positive matrices, and we use the corresponding Mahalanobis metrix to calculate the paired affinity between ai and bj: C (X; W) i, j = \u2212 (ai \u2212 bj) > W (ai \u2212 bj) > W (ai \u2212 bj) > W (ai \u2212 bj) > W (ai \u2212 bj)."}, {"heading": "3 Learning the metric", "text": "From now on, we assume that we have n pairs of training instances 1 (Xi, Y i) = ((Ai, Bi), Y i) and RT iA \u00b7 p \u00b7 RT iB \u00b7 p \u00b7 RT iA \u00b7 T iB, i = 1,.., n. Our goal is to find a matrix W so that the predicted alignments come close to the basic truth both in these examples and in invisible examples. We first define a loss between alignments in order to quantify this proximity between alignments. 1We will see that it is necessary to have fully labeled instances, which means that for each pair of Xi we need an exact alignment Y i between Ai and Bi."}, {"heading": "3.1 Losses between alignments", "text": "In our framework, the alignments are encrypted by matrices in Y (X) so that we are all interested in functions': Y (X) \u00b7 Y (X) \u00b7 R +. Let us define the Frobenius standard, which proves to be an unnormalized loss [10] for 0 / 1 rated matrices. For two matrices Y1, Y2 and Y (X) it is defined as a simple loss between matrices: \"H (Y1, Y2) = unnormalized loss is the unnormalizable Hamming loss [10] for 0 / 1 rated matrices Y1, Y2 and Y (X), it is defined as:\" H (Y1, Y2)."}, {"heading": "3.2 Empirical loss minimization", "text": "Let us remember that we are given n alignment examples (Xi, Y i) 1 \u2264 i \u2264 n. For a fixed loss, \"our goal now is to solve the following minimization problem in W: min W-W {1 n-i = 1 '(Y i, argmax Y-Y Ti A, Ti B Tr (C (Xi; W) > Y)))) + 1-W (W)}, (7) where 1-W = 2-W-2 F is a convex regulator that prevents overmatching, where 1-W-0 applies."}, {"heading": "4 Large margin approach", "text": "In this section we describe a large marginalization approach to solving a problem in Eq. (7), which leads directly to a loss. (7), which is untractable. (3), the decoding task is the maximum of a linear function in parameter size W and aims at predicting an output over a large and discrete space (the space of potential alignments in relation to the constraints in Eq. (1), so the decoding task is the maximum of a linear prediction [25, 22]. We define the hinge loss, a convex surrogate to, \"byL (X, Y), Y.\" (X), Y. \"(X). \u2212 Tr (W) \u2212.\" We can solve the problem (X, Y). \"(8), the rating of L is usually referred to as\" loss-augmented decoding, \"see [25]."}, {"heading": "5 Experiments", "text": "We applied our method to the task of learning a good similarity measure for the alignment of audio signals. In this area, researchers have put a lot of effort into designing well-suited and meaningful features [13, 5]. However, the problem of combining these features to align temporal sequences remains a challenge."}, {"heading": "5.1 Dataset of Kirchhoff and Lerch [15]", "text": "First, we applied our method to the Kirchhoff and Lerch dataset [15]. In this dataset, pairs of aligned examples (Ai, Bi) are artificially created by stretching an original audio signal. Thus, the basic truth alignment Y i is known, and the data thus falls within our setting. A more precise description of the dataset can be found in [15]. The N = 60 pairs are stretched along two different tempo curves. Each signal consists of 30 pieces of music divided into frames of 46 ms with a hop size of 23 ms, resulting in a typical length of the signals of T \u2248 1300 in our setting. We consider p = 11 characteristics easy to implement and known to be well suited for alignment tasks [15]. These were: five MFCC [9] (labeled as M1,..., M5 in Fig. 3), the spectral flatness (SF), the spectral centrifugal (SC), the individual Sentries (Srid), and the results (Sv)."}, {"heading": "5.2 Chorales dataset", "text": "The dataset. The dataset of Bach 10 consists of ten J. S. Bach's chorals (small four-part pieces). For each chorale, we have converted a MIDI reference file corresponding to the \"score,\" or essentially a representation of the partition. Alignments between the MIDI files and the audio file are given, so we have converted these MIDI files to audio by following what is done classically for orientation (see, for example, [11]). In this way, we fall into the audio audio framework in which our technique is used. Each piece of music is about 25 seconds long, resulting in a similar signal length. We use the same characteristics as in Sec. 5.1. As shown in Fig. 4, optimization with hammering loss works poorly on this dataset. In fact, the best individual attribute performance is much better than the performance of the learned W."}, {"heading": "5.3 Feature selection", "text": "Finally, we conducted experiments on the same data sets. Starting with low characteristics, namely the 13 leading MFCCs coefficients and their first two derivatives, we learn a linear combination of these characteristics, which achieves good alignment performance in terms of surface losses. Note that this incorporates very little musical prior knowledge. In addition, we either improve the best craftsmanship characteristics of the dataset of [15] or perform similarly. In both datasets, the learned combination of craftsmanship characteristics performed similarly to the combination of these 39 MFCCs coefficients."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a structured prediction framework for learning the metric for problems with timing. Technically, we have been able to combine handmade features and automatically build new state-of-the-art features from basic low-level information with very little expert knowledge. Technically, this is possible by taking into account a loss that goes beyond the usual hamming loss, which is typically used because it is \"practical\" within a structured prediction framework (linear in the presentation of results).The present work can be expanded in several ways, the most important being to take into account cases where only incomplete information about alignment is available, which is often the case in music [5] or bioinformatics applications. Note a simple alternating optimization between metric learning and restricted alignment offers an easy first solution that could probably be improved."}, {"heading": "Acknowledgements", "text": "We are grateful for the support of the GARGANTUA project (CNRS Mastodons Programme), the SIERRA-23999 grant from the European Research Council and a doctoral fellowship from the EADS Foundation."}, {"heading": "A Derivation of the BCFW-like algorithm for the quadratic loss.", "text": "We begin with the global structured objective equation of the paper (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\" W \") (\" W \") (\" W \") (\" W \") (W\") (\"W\") \"(W\") (W \") (W\" (W \")\" (W \") (W\") (W \"(W\") (W \") (W\") (W \") (W\") (W \") (W\") (W \"(W\") (W \") (W\") \"(W\") \"(W\") \"(W\") \"(W\") \"(W\") \"(W\") \"(W\" (\")\") \"(W\" (\")\" (\"W\") \"(\") \"(\" W \")\" (\")\" (\")\" (\")\") \"(\" W \"(\") \"(\") \")\" (\")\" (\")\" (\")\" (\")\" (\")\" (\")\" (\")\") \"(\") \"(\" (\")\") \"(\") \"(\") \"(\") \")\" (\"(\") \"(\") \")\" (\")\") \"(\" (\")\" (\")\") \"(\" (\")\") \"(\" (\")\") \"(\") \"(\") \"(\") \"(\" (\")\" (\")\") \"(\") \"(\") \"(\") \"(\") \")\" (\")\" (\")\" (\")\" (\")\" (\")\" (\")\" (\"(\") \"(\") \")\" (\")\") \"(\" ("}, {"heading": "B The dynamic time warping algorithm", "text": "Consider the pseudo-code of dynamic time warping that maximizes the LP (2) of the article. Unlike M\u00fcller [19], we give a version of the algorithm for the affinity matrix C. Intuitively, the cost matrix is the opposite of a cost matrix, so we strive to maximize the cumulative affinity rather than minimize the cumulative cost. This algorithm is O (TATB), which makes the calculation of large time series very expensive."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>In this paper, we propose to learn a Mahalanobis distance to perform alignment<lb>of multivariate time series. The learning examples for this task are time series for<lb>which the true alignment is known. We cast the alignment problem as a struc-<lb>tured prediction task, and propose realistic losses between alignments for which<lb>the optimization is tractable. We provide experiments on real data in the audio to<lb>audio context, where we show that the learning of a similarity measure leads to<lb>improvements in the performance of the alignment task. We also propose to use<lb>this metric learning framework to perform feature selection and, from basic audio<lb>features, build a combination of these with better performance for the alignment.", "creator": "LaTeX with hyperref package"}}}