{"id": "1409.3215", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Sep-2014", "title": "Sequence to Sequence Learning with Neural Networks", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.7 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a strong phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which beats the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "histories": [["v1", "Wed, 10 Sep 2014 19:55:35 GMT  (37kb)", "http://arxiv.org/abs/1409.3215v1", "10 pages"], ["v2", "Wed, 29 Oct 2014 12:13:17 GMT  (66kb)", "http://arxiv.org/abs/1409.3215v2", "9 pages"], ["v3", "Sun, 14 Dec 2014 20:59:51 GMT  (66kb)", "http://arxiv.org/abs/1409.3215v3", "9 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ilya sutskever", "oriol vinyals", "quoc v le"], "accepted": true, "id": "1409.3215"}, "pdf": {"name": "1409.3215.pdf", "metadata": {"source": "CRF", "title": "Sequence to Sequence Learning with Neural Networks", "authors": ["Ilya Sutskever"], "emails": ["ilyasu@google.com", "vinyals@google.com", "qvl@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.32 15v1 [cs.CL] 1.0Se pDeep Neural Networks (DNNs) are powerful models that have performed excellently in difficult learning tasks. Although DNNs always work well when large labeled training sets are available, they cannot be used to map sequences to sequences. In this essay, we present a general end-to-end approach to sequence learning that makes minimal assumptions about the sequence structure. Our method uses a multi-layered long-short-term memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on a translation task in the WMT-14 dataset, the translations produced by the LSTM do not achieve a BLEU score of 4.8 EU over the entire set of BLM, with the SMM not reaching the BLM score of 3."}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 The model", "text": "The recurrent neural network (RNN) [35, 29] is a natural generalization of supplying neural networks to sequences. Faced with a sequence of inputs (x1,., xT), a standard RNN calculates a sequence of outputs (y1,., yT) by iterating the following equation: ht = sigm (W hxxt + W hht \u2212 1) yt = W yhhtThe RNN can easily map sequences to sequences whenever the arrangement between the inputs of the outputs is known. However, it is not clear how to apply an RNN to problems whose input and output sequences have different lengths with complicated and non-monotonic relationships. A simple strategy for general sequence learning is mapping the input sequence to a fixed vector with one RNN and then to the target sequence with another RNN."}, {"heading": "3 Experiments", "text": "We applied our method to the WMT '14 English-French MT task in two ways: We used it to translate the initial sentence directly without using a reference system for SMT, and we used it to revive the n-best lists of an SMT baseline. We report on the accuracy of these translation methods, present sample translations, and visualize the resulting sentence representation."}, {"heading": "3.1 Dataset details", "text": "We used the WMT '14 English-French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, a clean \"selected\" subset [30]. We chose this translation task and this subset of specific training sentences due to the public availability of a symbolized training and test set along with 1000 best lists from the LIUM SMT baseline [30, 31]. Since typical neural language models are based on a vector representation for each word, we used a fixed vocabulary for both languages. We used 160,000 of the most common words for the source language and 80,000 of the most common words for the target language. Each foreign word was replaced by a special \"UNK\" token."}, {"heading": "3.2 Decoding and Rescoring", "text": "The core of our experiments was to train a large deep LSTM on many pairs of sentences. We trained them by maximizing the likelihood of a correct translation T taking into account the source sentence S, so that the training goal is: 1 / | S | \u2211 (T, S), where S is the training sentence. Once the training is complete, we produce translations by finding the most likely translation according to the LSTM: T = argmax T p (T | S) (2) We seek the most likely translation with a simple left-to-right beam search coder that maintains a small number B of partial hypotheses, with one partial hypothesis being a prefix of any translation. At each step of the time, we expand each partial hypothesis in the beam with every possible word in the vocabulary. This considerably increases the number of hypotheses, so that we discard all but the B hypotheses."}, {"heading": "3.3 Reversing the Source Sentences", "text": "Although LSTM is able to solve problems with long-term dependencies, we found that LSTM learns much better when the source sentences are reversed (the target sentences are not reversed), which reduced LSTM's test confusion from 5.8 to 4.7, and the test values of the decrypted translations rose from 25.9 to 30.6. Although we do not have a complete explanation for this phenomenon, we believe it is caused by the introduction of many short-term dependencies with the data set. Normally, when we associate a source sentence with a target sentence, each word in the source sentence is far removed from its corresponding word in the target sentence. As a result, the problem has a large \"minimal time lag.\" [17] By reversing the words in the source sentence, the average distance between the corresponding words in the source and target language is unchanged. However, the first few words in the target language are very close to the first words, so the minimum time in the source sentence is significantly shortened, which significantly improves the communication between the corresponding words and the corresponding words in the source language."}, {"heading": "3.4 Training details", "text": "We have used deep 4-layer LSTMs, with 1000 cells on each layer and 1000 dimensional word embedding, with an input vocabulary of 160,000 and output vocabulary of 80,000. We have found deep LSTMs that significantly exceed flat LSTMs, with each additional layer reducing perplexity by almost 10%, possibly due to their much larger hidden state. We have a na\u00efve softmax of over 80,000 words on each layer. The resulting LSTM has 380M parameters, of which 64M are pure recurring connections (32 M for the \"encoder\" LSTM and 32 M for the \"decoder\" LSTM \"). Full training details are given below: \u2022 We have all the LSTM parameters with uniform distribution between -0.08 and 0.08."}, {"heading": "3.5 Parallelization", "text": "A C + + implementation of Deep LSTM with the configuration from the previous section on a single GPU processed a speed of about 1,700 words per second. This was too slow for our purposes, so we paralleled our model with an 8 GPU calculator. Each layer of the LSTM was run on a different GPU and communicated its activations to the next GPU (or layer) as soon as they were calculated. Our models have 4 layers of LSTMs, each located on a separate GPU. The remaining 4 GPUs were used to parallelise the Softmax, so that each GPU was responsible for multiplying with a 1000 x 20,000 matrix. The resulting implementation reached a speed of 6,300 words per second (both English and French) at a minibatch size of 128. Training took about a week with this implementation."}, {"heading": "3.6 Experimental Results", "text": "We used the aggregated BLEU score [25] to evaluate the quality of our translations. Of particular importance were questions related to tokenization and normalization of test sentences. We made sure that our translations were \"non-normalized\" (for example, to replace \"them\" with \"them\") in order to reproduce the baseline BLEU score using our evaluation tools. Results are given in Tables 1 and 2. Our best results are achieved with an ensemble of LSTMs that differ in their random initializations and in the random sequence of minibatches. While the decrypted translations of the LSTM ensemble do not exceed the state of the art, it is the first time that a pure neural translation system exceeds a strong phrase-based SMT baseline by a considerable distance, although it is able to exceed the vocabulary of the STUM system by 1,000 words."}, {"heading": "3.7 Performance on long sentences", "text": "We were surprised to find that LSTM performed well on long sentences, which is quantitatively shown in Figure 3. Table 3 shows some examples of long sentences and their translations."}, {"heading": "3.8 Model Analysis", "text": "One of the attractive features of our model is its ability to transform a word sequence into a vector of fixed dimensionality. Figure 2 illustrates some of the learned representations. The figure clearly shows that the representations are sensitive to the sequence of words, while they are relatively insensitive to the substitution of an active voice by a passive voice."}, {"heading": "4 Related work", "text": "However, the simplest and most effective way to apply an RNN language model (RNNLM) [24] or a feedback Neural Network Language Model (NNLM) [3] to an MT task is to resorb the best lists of a strong MT baseline that reliably improve translation quality. More recently, researchers have begun looking for ways to include information about the source language in the NNLM. Examples of this work include Auli et al. [1], which combine an NNLM with an input set theme model that improves performance. [8] They followed a similar approach, but integrated their NLM into the decoder of an MT system."}, {"heading": "5 Conclusion", "text": "In this paper, we demonstrated that a large deep LSTM approach with limited vocabulary can outperform a standard SMT-based system whose vocabulary is unlimited in a large-scale MT task. The success of our simple LSTM-based approach to MT suggests that it should do well in many other sequence learning problems, provided they have sufficient training data. We were surprised at the extent of the improvement made by inverting the words in the source sentences. We conclude that it is important to find a problem coding that has the greatest number of short-term dependencies, as they make the learning problem much easier. In particular, we were unable to train a standard RNN on the non-reverse translation problem (shown in Figure 1), but we believe that a standard RNN would be portable if the source sentences are reversible (although we have not experimentally verified it). We were also surprised by the ability of LM to set very long sentences correctly."}, {"heading": "6 Acknowledgments", "text": "We thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolfgang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba and the Google Brain team for their useful comments and discussions."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["M. Auli", "M. Galley", "C. Quirk", "G. Zweig"], "venue": "EMNLP,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research, pages 1137\u20131155,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, 5(2):157\u2013166,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Arxiv preprint arXiv:1406.1078,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing - Special Issue on Deep Learning for Speech and Language Processing,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "ACL,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence transduction with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1211.3711,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "Arxiv preprint arXiv:1308.0850,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Multilingual distributed representations without word alignment", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "ICLR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["S. Hochreiter"], "venue": "Master\u2019s thesis, Institut fur Informatik, Technische Universitat, Munchen,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "long-term dependencies,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "LSTM can solve hard long time lag problems", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M.A. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "ICML,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Statistical Language Models based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, pages 1045\u20131048,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "ACL,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation", "author": ["J. Pouget-Abadie", "D. Bahdanau", "B. van Merrienboer", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1257,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "On small depth threshold circuits", "author": ["A. Razborov"], "venue": "Proc. 3rd Scandinavian Workshop on Algorithm Theory,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning representations by back-propagating errors", "author": ["D. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323(6088):533\u2013536,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1986}, {"title": "University le mans", "author": ["H. Schwenk"], "venue": "http://www-lium.univ-lemans.fr/ \u0303schwenk/cslm_joint_paper/,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "LIUM\u2019s SMT machine translation systems for wmt 2011", "author": ["H. Schwenk", "P. Lambert", "L. Barrault", "C. Servan", "H. Afli", "S. Abdul-Rauf", "K. Shah"], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "LSTM neural networks for language modeling", "author": ["M. Sundermeyer", "R. Schluter", "H. Ney"], "venue": "INTER- SPEECH,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "DeepFace: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "CVPR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P. Werbos"], "venue": "Proceedings of IEEE,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 12, "context": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 34, 21, 20].", "startOffset": 159, "endOffset": 166}, {"referenceID": 6, "context": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 34, 21, 20].", "startOffset": 159, "endOffset": 166}, {"referenceID": 18, "context": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 34, 21, 20].", "startOffset": 197, "endOffset": 216}, {"referenceID": 5, "context": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 34, 21, 20].", "startOffset": 197, "endOffset": 216}, {"referenceID": 32, "context": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 34, 21, 20].", "startOffset": 197, "endOffset": 216}, {"referenceID": 20, "context": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 34, 21, 20].", "startOffset": 197, "endOffset": 216}, {"referenceID": 19, "context": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 34, 21, 20].", "startOffset": 197, "endOffset": 216}, {"referenceID": 27, "context": "A surprising example of the power of DNNs is their ability to sort N N -bit numbers using only 2 hidden layers of quadratic size [28].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "In this paper, we show that a straightforward application of the Long Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.", "startOffset": 108, "endOffset": 112}, {"referenceID": 28, "context": "The second LSTM is essentially a recurrent neural network language model [29, 24, 33] except that it is conditioned on the input sequence.", "startOffset": 73, "endOffset": 85}, {"referenceID": 23, "context": "The second LSTM is essentially a recurrent neural network language model [29, 24, 33] except that it is conditioned on the input sequence.", "startOffset": 73, "endOffset": 85}, {"referenceID": 31, "context": "The second LSTM is essentially a recurrent neural network language model [29, 24, 33] except that it is conditioned on the input sequence.", "startOffset": 73, "endOffset": 85}, {"referenceID": 17, "context": "Our approach is closely related to Kalchbrenner and Blunsom [18] who were the first to map the entire input sentence to vector, and is very similar to Cho et al.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "[5] (although the model in this paper was developed in parallel to Cho et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Graves [10] introduced a novel differentiable attention mechanism that allows neural networks to focus on different parts of their input, and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks, although it assumes a monotonic alignment between the inputs and the outputs [11, 9].", "startOffset": 205, "endOffset": 212}, {"referenceID": 8, "context": "The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks, although it assumes a monotonic alignment between the inputs and the outputs [11, 9].", "startOffset": 205, "endOffset": 212}, {"referenceID": 30, "context": "30 (the LIUM system [31, 30]).", "startOffset": 20, "endOffset": 28}, {"referenceID": 29, "context": "30 (the LIUM system [31, 30]).", "startOffset": 20, "endOffset": 28}, {"referenceID": 29, "context": "Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on the same task [30].", "startOffset": 113, "endOffset": 117}, {"referenceID": 26, "context": "Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other researchers with related architectures [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 33, "context": "The Recurrent Neural Network (RNN) [35, 29] is a natural generalization of feedforward neural networks to sequences.", "startOffset": 35, "endOffset": 43}, {"referenceID": 28, "context": "The Recurrent Neural Network (RNN) [35, 29] is a natural generalization of feedforward neural networks to sequences.", "startOffset": 35, "endOffset": 43}, {"referenceID": 4, "context": "[5]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies [14, 4] (figure 1) [16, 15].", "startOffset": 178, "endOffset": 185}, {"referenceID": 3, "context": "While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies [14, 4] (figure 1) [16, 15].", "startOffset": 178, "endOffset": 185}, {"referenceID": 15, "context": "While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies [14, 4] (figure 1) [16, 15].", "startOffset": 197, "endOffset": 205}, {"referenceID": 14, "context": "While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies [14, 4] (figure 1) [16, 15].", "startOffset": 197, "endOffset": 205}, {"referenceID": 15, "context": "However, the Long Short-Term Memory (LSTM) [16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed in this setting.", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "We use the LSTM formulation from Graves [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "First, we used two different LSTMs: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and because doing so makes it natural to train the LSTM on multiple language pairs simultaneously [18].", "startOffset": 286, "endOffset": 290}, {"referenceID": 29, "context": "We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean \u201cselected\u201d subset from [30].", "startOffset": 147, "endOffset": 151}, {"referenceID": 29, "context": "We chose this translation task and this specific training set subset because of the public availability of a tokenized training and test set together with 1000-best lists from the LIUM SMT baseline [30, 31].", "startOffset": 198, "endOffset": 206}, {"referenceID": 30, "context": "We chose this translation task and this specific training set subset because of the public availability of a tokenized training and test set together with 1000-best lists from the LIUM SMT baseline [30, 31].", "startOffset": 198, "endOffset": 206}, {"referenceID": 29, "context": "We also used the LSTM to rescore the 1000-best lists produced by the LIUM system [30, 31].", "startOffset": 81, "endOffset": 89}, {"referenceID": 30, "context": "We also used the LSTM to rescore the 1000-best lists produced by the LIUM system [30, 31].", "startOffset": 81, "endOffset": 89}, {"referenceID": 16, "context": "As a result, the problem has a large \u201cminimal time lag\u201d [17].", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "Thus we enforced a hard constraint on the norm of the gradient [10, 26, 23] by scaling it when its norm exceeded a threshold.", "startOffset": 63, "endOffset": 75}, {"referenceID": 25, "context": "Thus we enforced a hard constraint on the norm of the gradient [10, 26, 23] by scaling it when its norm exceeded a threshold.", "startOffset": 63, "endOffset": 75}, {"referenceID": 22, "context": "Thus we enforced a hard constraint on the norm of the gradient [10, 26, 23] by scaling it when its norm exceeded a threshold.", "startOffset": 63, "endOffset": 75}, {"referenceID": 24, "context": "We used the cased BLEU score [25] to evaluate the quality of our translations.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "[2] 28.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "45 LIUM System [30] 33.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "Method test BLEU score (ntst14) LIUM System [30] 33.", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "[5] 34.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "So far, the simplest and most effective way of applying an RNN-Language Model (RNNLM) [24] or a Feedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the nbest lists of a strong MT baseline [22], which reliably improves translation quality.", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "So far, the simplest and most effective way of applying an RNN-Language Model (RNNLM) [24] or a Feedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the nbest lists of a strong MT baseline [22], which reliably improves translation quality.", "startOffset": 145, "endOffset": 148}, {"referenceID": 21, "context": "So far, the simplest and most effective way of applying an RNN-Language Model (RNNLM) [24] or a Feedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the nbest lists of a strong MT baseline [22], which reliably improves translation quality.", "startOffset": 219, "endOffset": 223}, {"referenceID": 0, "context": "[1], who combine an NNLM with a topic model of the input sentence, which improves rescoring performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] followed a similar approach, but they incorporated their NNLM into the decoder of an MT system and used the decoder\u2019s alignment information to provide the NNLM with the most useful words in the input sentence.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Our work is closely related to Kalchbrenner and Blunsom [18], who were the first to map the input sentence into a vector and then back to a sentence, although they map sentences to vectors using convolutional neural networks, which lose the ordering of the words.", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "[5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their primary focus was on integrating their neural network into an SMT system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] also attempted direct translations with a neural network that used an attention mechanism to overcome the poor performance on long sentences experienced by Cho et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] and achieved encouraging results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[27] attempted to address the memory problem of Cho et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] by translating pieces of the source sentence in way that produces smooth translations, which is similar to a phrase-based approach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12], whose model represents the inputs and outputs by feedforward networks, and map them to similar points in space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "We were initially convinced that the LSTM would fail on long sentences due to its limited memory, and other researchers reported poor performance on long sentences with a model similar to ours [5, 2, 27].", "startOffset": 193, "endOffset": 203}, {"referenceID": 1, "context": "We were initially convinced that the LSTM would fail on long sentences due to its limited memory, and other researchers reported poor performance on long sentences with a model similar to ours [5, 2, 27].", "startOffset": 193, "endOffset": 203}, {"referenceID": 26, "context": "We were initially convinced that the LSTM would fail on long sentences due to its limited memory, and other researchers reported poor performance on long sentences with a model similar to ours [5, 2, 27].", "startOffset": 193, "endOffset": 203}], "year": 2014, "abstractText": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM\u2019s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a strong phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which beats the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM\u2019s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "creator": "LaTeX with hyperref package"}}}