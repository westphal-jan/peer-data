{"id": "1409.3836", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2014", "title": "Hardness of parameter estimation in graphical models", "abstract": "We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see the monograph by Wainwright and Jordan (2008)) but no proof was known.", "histories": [["v1", "Fri, 12 Sep 2014 19:57:59 GMT  (26kb)", "https://arxiv.org/abs/1409.3836v1", "14 pages. To appear in NIPS 2014"], ["v2", "Wed, 17 Sep 2014 19:57:51 GMT  (26kb)", "http://arxiv.org/abs/1409.3836v2", "15 pages. To appear in NIPS 2014"]], "COMMENTS": "14 pages. To appear in NIPS 2014", "reviews": [], "SUBJECTS": "cs.CC cs.AI cs.IT math.IT stat.CO", "authors": ["guy bresler", "david gamarnik", "devavrat shah"], "accepted": true, "id": "1409.3836"}, "pdf": {"name": "1409.3836.pdf", "metadata": {"source": "CRF", "title": "Hardness of parameter estimation in graphical models", "authors": ["Guy Bresler", "David Gamarnik", "Devavrat Shah"], "emails": ["gbresler@mit.edu", "gamarnik@mit.edu", "devavrat@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.38 36v2 [CC] 1 7"}, {"heading": "1 Introduction", "text": "Graphic models are a powerful framework for the concise representation of complex high-dimensional distributions. As such, they are only the core of machine learning and artificial intelligence, and they are used in a variety of applied fields such as finance, signal processing, communication, biology, as well as the modeling of social and other complex networks. In this paper, we focus on binary pairs of undirected graphic models, which have a rich class of models with broad applicability. This is a parametric family of probability distributions, and for the models we are looking at the canonical parameters uniformly determined by the vector of mean parameters consisting of node-wise and paarticular marginalization."}, {"heading": "2 Main result", "text": "To establish the hardness of learning parameters from marginal for pair-wise binary graphical models, let's focus on a specific instance of this class of graphical models, the Hard Core Model. Given a graph G = (V, E) (where V = {1,.., p}), the collection is independent set vectors I (G) (G), which apparently consist of vectors that generate such a number of vectors, so that each individual set model has such a number of vectors that have no probability of independent set vectors, with the exception of ledge classes (or both). Each vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector class (or both)."}, {"heading": "3 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Exponential families and conjugate duality", "text": "We now provide background information on exponential families (as found in the Wainwright and Jordan monograph [1]) specializing in the hard-core model (2,1) on a fixed diagram G = (V, E). The general theory of conjugate duality that justifies the statements in this subsection can be found in Rockafell's book [13]. The basic relationship between the canonical and the mean parameters is expressed in conjugate (or fennel) duality. The conjugate dual principle of the log partition function \u03a6 (\u03b8) is the following: = sup \u03b8. The inner M \u00b2 of the boundary polytopic, \u2212 zipable is the entropy function. The log partitional function can then be expressed as such."}, {"heading": "3.2 Hardness of inference", "text": "We describe an existing result about the hardness of the conclusion and indicate the conclusion we will use. It is stated that, subject to widespread assumptions about computational complexity, there is no efficient algorithm to approximate the partition function of certain hardcore models. Remember, the hardcore model with volatility \u03bb is given by (2.1) with dependence on (2.1) for each i-V theory 3.1 ([3, 4]). Suppose d \u2265 3 and \u03bb > \u03bbc (d) = (d \u2212 1) d \u2212 1 (d \u2212 2). Assuming NP 6 = RP, there is no FPRAS for calculating the partition function of the hardcore model with volatility. Specifically, there is no FPRAS if between 0 and d \u2265 5.We point out that the source of hardness x is the extensive dependency property of the hardcore model."}, {"heading": "4 Reduction by optimizing over the marginal polytope", "text": "In this section we describe our reduction and prove theorem 2.3. We define the polynomial constants (PRP = 2, q = p5, and s = 2, (4.1), which we will leave next, and s to clarify the calculation. Also, given the asymptotic nature of the results, we assume that p is greater than a universal constant, so that certain inequalities are satisfied."}, {"heading": "5 Proof of Proposition 4.6", "text": "In Section 5.1, we prove estimates of parameters \u03b8 that correspond to \u00b5 near the boundary with M1, and then in Section 5.2, we use these estimates to show that the boundary with M1 has a certain repellent property that keeps the iterate inside."}, {"heading": "5.1 Bounds on gradient", "text": "We start by introducing a helpful notation. For a node i, let's N (i) = (i) [i) [p]: (i) [p]: (i), j) [P]: (i): (i): (i): (i): (i): (i): (i): (i): (i): (i): (i): (i): (i): (i): (i): (i): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S): (S: (S): (S): (S: (S): (S): (S: (S): (S): (S: (S): (S): (S): (S (S): (S): (S (S): (S): (S (S): (S): (S): (S (S): (S): (S (S): (S (S): (S): (S (S): (S (S): (S: (S): (S: (S): (S): (S: (S): (S: (S): (S: (S): (S: (S): (S): (S: (S): (S): (S: (S: (S): (S): (S: (S): (S: (S): (S): (S: (S: (S): (S: (S): (S): (S: (S): (S: (S): (S: (S): (S: (S): (S): (S: (S: (S): (S): (S: (S): (S: (S): (S)"}, {"heading": "5.2 Finishing the proof of Proposition 4.6", "text": "Starting with an arbitrary xt in M1, our goal is to show that xt + 1 = P \u2265 (xt \u2212 s\u00da (xt)) remains in M1. Proof will then follow by induction, because our starting point x1 in M1 is through the hypothesis. The argument considers any hyperplane constraint for M of the form < h, x > \u2264 1. The distance of x from the hyperplane is 1 \u2212 < h, x >. The definition of M1 now implies that if x-M1, then x-egg-M1 for all coordinates i, and thus 1 \u2212 < h, x > h, for all constraints. We call a constraint < h, x > < h, the constraint < h, x > < h, and active < b < h < h < b < h < h, if < h < h, if there are no constraints."}, {"heading": "6 Discussion", "text": "Our main result shows the hardness of the approximation of backward mapping \u00b5 7 \u2192 \u03b8 to a small polynomial factor. This is a fairly strict form of approximation, and it would be interesting to strengthen the result to show hardness even for a weaker form of approximation. A possible goal would be to show that there is a universal constant c > 0, so that the approximation of backward mapping to a factor 1 + c in each coordinate NP-hard."}, {"heading": "Acknowledgments", "text": "GB thanks Sahand Negahban for helpful discussions and Andrea Montanari for sharing his unpublished manuscript [9]. This work was supported in part by NSF grants CMMI-1335155 and CNS-1161964 as well as the MURI Award from the Army Research Office W911NF-11-1-0036."}, {"heading": "Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Miscellaneous proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Corollary 3.2", "text": "The proof is standard and uses the self-reducibility of the hardcore model, which means that conditioning to \u03c3i = 0 amounts to removing the node i from the graph. Repair a graph G and the parameters \u03b8 = 0. We show that with an algorithm to approximate the margins for induced subgraphs H G it is possible to approximate the partition functions labeled Z here (0). First, we claim that Z = p, i = 111 \u2212 \u00b5i (G\\ [i \u2212 1]) is the limit for this graph. (A.1) The graph G\\ [i \u2212 1] is achieved by removing nodes labeled 1, 2,.., i \u2212 1, and \u00b5i (G\\ [i \u2212 1]). We use the induction on the number of nodes. The base case with one node is trivial: Z = 1 + e0 = 2 = ig / 1 (\u2212 Z)."}, {"heading": "A.2 Proof of Lemma 4.2", "text": "We would like to show that \"\u00b5\" (0) - \"M1\" for a diagram G = (V, E) of the maximum degree d and p (2d + 1. Consider a particular node i - V with neighbors N (i), and let us specify di - \u2212 N (i) | its degree. We use the notation Si, S \u2212 i, S i defined in subsection 5.1. A collection of independent set vectors S \u2212 I (G) is assigned to the probability P (S) = | S | / | I (G) | for our choice \u03b8 = 0, so it is sufficient to argue about cardinalities. We first argue that | Si | 2 \u2212 d | S i |. This follows from the observation that each set in S \u2212 i is assigned to a set in Si by removing neighbors Ni (G), and furthermore, the same probability is assigned to most 2d sets."}, {"heading": "B Proofs for projected gradient method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Lemma 4.4", "text": "The proof for this is a slight modification of the proof of theorem 3.1 in [22]. First, note that if P is the projection onto a convex set, then P is a contraction. If using the convexity inequality G (x) - G (x) - G (x) - G (x) - G (x) T (x) - X (x) T (x), then the definition T (xt) - G (x) T (x) T (x). The definition T (xt) - X) - G (x) T (xt)."}, {"heading": "B.2 Proof of Lemma 4.5", "text": "We begin to show that the course of inequality p 32 - > < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &; &; &; &; &; &; &; &; < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &; < < &; < &; &; &; &; &; &; &; &; < &; &; &; &; &;; < < &; &; &; &;; < < < &; &; &;; &;; <; < &;; < <; &; <; <; <; < < <; < < <; < < <; <; < <; < < < <; <;;; <; < < <; < < <; <; <; < < < < <; <;;;; < <;;; < <; <;; <;;; <; <;"}, {"heading": "C Proofs of gradient bounds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Proof of Lemma 5.3", "text": "To deduce an inconsistency, we assume that \u03b8i > p / \u043c. Let's replace the non-negative measurement variable \u03b3 (added to less than one) with a probability measurement that reads: \"Si ashion\" = \"Si.\" (Si), if it comes otherwise: \"Si 0.\" In this way, we define the non-negative measurement variable \u03b3 (added to less than one) with support \"Si ascope.\" (Si), if it comes otherwise: \"Si 0.\" (C.1) and you can check whether the measurement of probability (added to less than one) is supported. (Si), if the calculation does not take place. (Si), if the calculation is different, (C1) and you check that the calculation of the calculation does not take place. (Si) We use the definitions in section 5.1 up to the calculation. (Si), if the calculation does not take place. (Si), (Si), if the calculation does not take place. (Si), the calculation is not performed. (Si)."}, {"heading": "C.2 Proof of Lemma 5.4", "text": "For the sake of contradiction, we assume that \u03b8i < \u2212 p / \u03b4 and show that \u03b8 cannot be the vector of canonical parameters corresponding to \u00b5. Since \u00b5-M there is a non-negative measurement, so that \u00b5 = empirical values corresponding to the I-plane, and beyond that \u03b7 (Si) = \u00b5i. Now there are arguments that resemble the detection of Lemma 5,3 via giveF\u00b5 (\u03b8) = \u00b5 \u00b7 empirical values similar to (S \u2212 i) + f (S-i) + f (S-xi) = empirical values."}, {"heading": "D Proof of Proposition 4.6", "text": "Starting with xt in M1, our goal is to show that xt + 1 = P \u2265 (> SDA) = > remains active in M1. < < < p > p > p > p > p > p > p > p > p > p \"p > p\" p > p > p > p > p > p > p > p > p > p > p > p > p > p + p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "We consider the problem of learning the canonical parameters specifying an undi-<lb>rected graphical model (Markov random field) from the mean parameters. For<lb>graphical models representing a minimal exponential family, the canonical param-<lb>eters are uniquely determined by the mean parameters, so the problem is feasible<lb>in principle. The goal of this paper is to investigate the computational feasibil-<lb>ity of this statistical task. Our main result shows that parameter estimation is in<lb>general intractable: no algorithm can learn the canonical parameters of a generic<lb>pair-wise binary graphical model from the mean parameters in time bounded by a<lb>polynomial in the number of variables (unless RP = NP). Indeed, such a result has<lb>been believed to be true (see [1]) but no proof was known.<lb>Our proof gives a polynomial time reduction from approximating the partition<lb>function of the hard-core model, known to be hard, to learning approximate pa-<lb>rameters. Our reduction entails showing that the marginal polytope boundary has<lb>an inherent repulsive property, which validates an optimization procedure over<lb>the polytope that does not use any knowledge of its structure (as required by the<lb>ellipsoid method and others).", "creator": "LaTeX with hyperref package"}}}