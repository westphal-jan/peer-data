{"id": "1409.4689", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2014", "title": "Compute Less to Get More: Using ORC to Improve Sparse Filtering", "abstract": "Sparse Filtering is a popular feature learning algorithm for image classification pipelines. In this paper, we connect the performance of Sparse Filtering in image classification pipelines to spectral properties of the corresponding feature matrices. This connection provides new insights into Sparse Filtering; in particular, it suggests stopping Sparse Filtering early. We therefore introduce the Optimal Roundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We show that this stopping criterion is related with pre-processing procedures such as Statistical Whitening and that it can make image classification with Sparse Filtering considerably faster and more accurate.", "histories": [["v1", "Tue, 16 Sep 2014 16:31:07 GMT  (172kb,D)", "https://arxiv.org/abs/1409.4689v1", null], ["v2", "Sun, 24 May 2015 09:16:03 GMT  (697kb,D)", "http://arxiv.org/abs/1409.4689v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["johannes lederer", "sergio guadarrama"], "accepted": true, "id": "1409.4689"}, "pdf": {"name": "1409.4689.pdf", "metadata": {"source": "CRF", "title": "Compute Less to Get More: Using ORC to Improve Sparse Filtering", "authors": ["Johannes Lederer", "Sergio Guadarrama"], "emails": ["johanneslederer@cornell.edu", "sergio.guadarrama@berkeley.edu"], "sections": [{"heading": "Introduction", "text": "In practice, the number of samples is typically limited, making the second approach relevant. An important tool for this second approach is feature-learning algorithms, which aim to facilitate the classification task by transforming the data. Recently proposed deep-learning methods aim to jointly learn feature transformation and classification (Krizhevsky, Sutskever and Hinton 2012). However, in this paper we focus on uncontrolled feature learning, especially sparse filtering, due to its simplicity and scalability. Feature-learning algorithms for image-classification pipelines typically consist of three steps: pre-processing, (un) supervised dictionary learning and coding. A plethora number of procedures are available for each of these steps, but for accurate image classification we need procedures that are effective and interact beneficially."}, {"heading": "Feature Learning for Image Classification", "text": "In a first step, a dictionary is learned, and in a second step, the samples are encrypted on the basis of this dictionary."}, {"heading": "The Optimal Roundness Criterion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Roundness of Feature Matrices", "text": "To grasp these effects, we therefore introduce the roundness of a characteristic: F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F = F ="}, {"heading": "Roundness and Randomness", "text": "s theorem for Gaussian random matrices (see Eldar and Kutyniok 2012, Chapter 5) for a current introduction to random matrix theory): Theorem 2 (Gordon). Let F-Rn \u00b7 p be a random matrix with independent standard normal entries. Then, 1 \u2212 n / p \u2264 E [\u221a \u03c3n (F) / p] \u2264 E structures (F) \u2264 1 + n / p.Such exact boundaries are only available for matrices with independent standard normal entries, but sharp boundaries in probability are also available for other random matrices. For our purposes, the common message of all these structures is that random matrices with sufficiently many columns (sample number) have a small spectrum."}, {"heading": "Optimal Roundness Criterion (ORC)", "text": "The discussion above suggests that optimal feature learning is the result of a trade-off between increasing the roundness of the feature matrix and preserving global structures in the data. In this part, we will use this insight to understand and improve iterative feature learning algorithms. Common feature learning algorithms consist of transformations defined as minimizers of a functionality, which are then often iterated via a se-2We set 0k: = 1 for k = 0 and 0k: 0 for gradient-based operations. In this paper, we focus on feature learning algorithms in which transformation F is the sequence of transformations as in (1)."}, {"heading": "Image Classification on CIFAR-10", "text": "For all experiments we use the data set CIFAR-10 (Krizhevsky and Hinton 2009) 3. This data set consists of 60,000 color images, divided into 10 classes of 6,000 images each. Each of the images consists of 32 x 32 pixels. The data set is divided into a training set of 50,000 images and a test set of 10,000 images. From the training set, we randomly select 10,000 patches for the unattended feature learning. These patches are also used to determine the parameters of contrast normalization and statistical whitening (if used).3http: / / www.cs.toronto.edu / \u02dc kriz / cifar. html"}, {"heading": "Random Patches", "text": "For the learning step in the dictionary, it has been shown that simple randomised procedures in combination with statistical whitening work surprisingly well (Coates and Ng 2011; Jarrett et al. 2009; Saxe et al. 2011). A popular example are random patches that create a dictionary matrix by simply stacking randomly selected samples. In Table 1, we report on the influence of contrast normalization and statistical whitening on random patches (see Coates and Ng 2011). We see that static whitening is very beneficial for random patches and increases the roundness of the transformed feature matrix, suggesting that roundness can be used as an indicator of the performance of feature learning. (Note that roundness stands for different number of features on different scales and therefore cannot be compared for different number of features.)"}, {"heading": "Sparse Filtering", "text": "So it is not only the way in which we define the Matrix O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-P-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O-O"}, {"heading": "Conclusions and Outlook", "text": "Spectral analysis of feature matrices is a new and promising approach to feature learning. In particular, our results show that this \"geometric\" approach can offer new interpretations and significant improvements to widely used feature learning tools such as statistical whitening, random patches, and sparse filtering. For example, we have shown that sparse filtering can surprisingly deteriorate with increasing number of iterations, and can be made much faster and more precise by stopping early according to the spectrum of intermediate feature matrices. In theory, it would be of interest to obtain predictions for certain procedures about how roundness changes with iterations and what it converges within the boundaries. In an expanded version of this paper, we plan to include an analysis of roundness in Convolutional Neural Networks (CNNs) (CNNs) (Fukushima 1980). After CNNs have been neglected for many years recently, we have included other possibilities (hence, Krizsky, and Sutskys)."}, {"heading": "Acknowledgments", "text": "We thank the reviewers for their insightful comments."}, {"heading": "Appendix: Proofs", "text": "This means that there is an orthogonal matrix (F) in which there is an orthogonal matrix (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F. (F). (F). (F). (F). (F). (F. (F). (F). (F). (F). (F. (F. (F). (F). (F). (F. (F). (F). (F). (F). (F). (F). (F. (F).). (F.). (F. (F.). (F.).). (F. (F.).)."}, {"heading": "Appendix", "text": "We also present numerical results for a different random division of the CIFAR 10 dataset. In particular, we recalculate Figures 3 and 4 for a different division and return the results in Figures 5 and 6. The conclusions are practically the same as above, which further confirms our results."}], "references": [{"title": "and Triggs", "author": ["A. Agarwal"], "venue": "B.", "citeRegEx": "Agarwal and Triggs 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Ng", "author": ["A. Coates"], "venue": "A.", "citeRegEx": "Coates and Ng 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks", "author": ["Ng Coates", "A. Lee 2011] Coates", "A. Ng", "H. Lee"], "venue": null, "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "and Kutyniok", "author": ["Y. Eldar"], "venue": "G., eds.", "citeRegEx": "Eldar and Kutyniok 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Malik", "author": ["R. Girshick"], "venue": "J.", "citeRegEx": "Girshick and Malik 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation. preprint arxiv:1311.2524", "author": ["Girshick"], "venue": null, "citeRegEx": "Girshick,? \\Q2013\\E", "shortCiteRegEx": "Girshick", "year": 2013}, {"title": "What is the Best Multi-Stage Architecture for Object Recognition", "author": ["Jarrett"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Jarrett,? \\Q2009\\E", "shortCiteRegEx": "Jarrett", "year": 2009}, {"title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "author": ["Huang Jia", "Y. Darrell 2012] Jia", "C. Huang", "T. Darrell"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Jia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2012}, {"title": "and Hinton", "author": ["A. Krizhevsky"], "venue": "G.", "citeRegEx": "Krizhevsky and Hinton 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Huang LeCun", "Y. Bottou 2004] LeCun", "F. Huang", "L. Bottou"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "P", "author": ["Ngiam, J.", "Koh"], "venue": "W.; Chen, Z.; Bhaskar, S.; and Ng, A.", "citeRegEx": "Ngiam et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "On random weights and unsupervised feature learning", "author": ["Saxe"], "venue": "In 28th International Conference on Machine Learning", "citeRegEx": "Saxe,? \\Q2011\\E", "shortCiteRegEx": "Saxe", "year": 2011}], "referenceMentions": [], "year": 2015, "abstractText": "Sparse Filtering is a popular feature learning algorithm for image classification pipelines. In this paper, we connect the performance of Sparse Filtering with spectral properties of the corresponding feature matrices. This connection provides new insights into Sparse Filtering; in particular, it suggests early stopping of Sparse Filtering. We therefore introduce the Optimal Roundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We show that this stopping criterion is related with pre-processing procedures such as Statistical Whitening and demonstrate that it can make image classification with Sparse Filtering considerably faster and more accurate. Introduction Standard ways to improve image classification are to collect more samples or to change the representation and the processing of the data. In practice, the number of samples is typically limited, so that the second approach becomes relevant. An important tool for this second approach are feature learning algorithms, which aim at easing the classification task by transforming the data. Recently proposed deep learning methods intend to jointly learn learn a feature transformation and the classification (Krizhevsky, Sutskever, and Hinton 2012). In this work, however, we focus on unsupervised feature learning, especially on Sparse Filtering, because of their simplicity and scalability. Feature learning algorithms for image classification pipelines typically consists of three steps: pre-processing, (un)supervised dictionary learning, and encoding. An abundance of procedures is available for each of these steps, but for accurate image classification, we need procedures that are effective and interact beneficially with each other (Agarwal and Triggs 2006; Coates and Ng 2011; Coates, Ng, and Lee 2011; Jia, Huang, and Darrell 2012; Le 2013; LeCun, Huang, and Bottou 2004). Therefore, a profound understanding of these procedures is crucial to ensure accurate results and efficient computations. In this paper, we study the performance of Sparse Filtering (Ngiam et al. 2011) for image classification. Our main contributions are: Copyright \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. \u2022 we show that Sparse Filtering can strongly benefit from early stopping; \u2022 we show that the performance of Sparse Filtering is correlated with spectral properties of feature matrices on tests sets; \u2022 we introduce the Optimal Roundness Criterion (ORC), a stopping criterion for Sparse Filtering based on the above correlation, and demonstrate that the ORC can considerably improve image classification. Feature Learning for Image Classification Feature learning algorithms often consist of two steps: In a first step, a dictionary is learned, and in a second step, the samples are encoded based on this dictionary. A typical dictionary learning step for image classification is sketched in Figure 1: First, random patches (samples) are extracted from the training images. These patches are then pre-processed using, for example, Statistical Whitening or Contrast Normalization. Finally, an unsupervised learning algorithm is applied to learn a dictionary from the pre-processed patches. Once a dictionary is learnt, several further steps need to be applied to finally train an image classifier, see, for example, (Coates and Ng 2011; Coates, Ng, and Lee 2011; Jia, Huang, and Darrell 2012; Le 2013). Our pipeline is similar to the one in (Coates and Ng 2011): We extract square patches comprising 9 \u00d7 9 pixels, pre-process them with Contrast Normalization1 and/or Statistical Whitening, and finally pass them to Random Patches or Sparse Filtering. (Note that our outcomes differ slightly from those in (Coates and Ng 2011) because we use square patches comprising 9\u00d79 pixels instead of 6\u00d76 pixels.) Subsequently, we apply soft-thresholding for encoding, 4\u00d74 spatial max pooling for extracting features from the training data images, and finally L2 SVM classification (cf. (Coates and Ng 2011)). Numerous examples show that feature learning can considerably improve classification. Therefore, insight in the underlying principles of feature learning algorithms such as Statistical Whitening and Sparse Filtering is of great interest. Contrast normalization consists of subtracting the mean and dividing by the standard deviation of the pixel values. ar X iv :1 40 9. 46 89 v2 [ cs .C V ] 2 4 M ay 2 01 5 Training images Extraction of random patches Pre-processing Unsupervised learning Dictionary Figure 1: A typical dictionary learning step. Statistical Whitening and Contrast Normalization are examples for preprocessing procedures; Random Patches and Sparse Filtering are examples for unsupervised learning procedures. In mathematical terms, a feature learning algorithm provides a transformation F : Rl\u00d7p \u2192 Rn\u00d7p X 7\u2192 F(X) (1) of an original feature matrix X \u2208 Rl\u00d7p to a new feature matrix F(X) \u2208 Rn\u00d7p. We adopt the convention that the rows of the matrices correspond to the features, the columns to the samples; this convention implies in particular that l \u2208 N is the number of original features, p \u2208 N the number of samples, and n \u2208 N the number of new features. The Optimal Roundness Criterion Roundness of Feature Matrices Feature learning can be seen as trade-off between reducing the correlations of the feature representation and preservation of relevant information. This trade-off can be readily understood looking at Statistical Whitening. For this, recall that pre-processing with Statistical Whitening transforms a set of image patches into a new set of patches by changing the local correlation structure. More precisely, Statistical Whitening transforms patches XPatch \u2208 R \u2032\u00d7p (n\u2032 < n), that is, subsets of the entire feature matrix, into new patches FPatch(XPatch) such that FPatch(XPatch)FPatch(XPatch) = n\u2032 In\u2032 . Statistical Whitening therefore acts locally: while the correlation structures of the single patches are directly and radically changed, the structure of the entire matrix is affected only indirectly. However, these indirect effects on the entire matrix are important for the following. To capture these effects, we therefore introduce the roundness of a feature matrix F := F(X) given an original feature matrix X . On a high level, we say that the new feature matrix F is round if the spectrum of the associated Gram matrixFF \u2208 Rn\u00d7n is narrow. To specify this notion, we denote the ordered eigenvalues of FF by \u03c31(F ) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3n(F ) \u2265 0 and their mean by \u03c3(F ) := 1 n \u2211n i=1 \u03c3i(F ) and define roundness as follows: Definition 1. For any matrix F 6= 0, we define its roundness as", "creator": "LaTeX with hyperref package"}}}