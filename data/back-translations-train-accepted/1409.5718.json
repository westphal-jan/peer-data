{"id": "1409.5718", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2014", "title": "Convolutional Neural Networks over Tree Structures for Programming Language Processing", "abstract": "Deep neural networks have made significant breakthroughs in many fields of artificial intelligence. However, it has not been applied in the field of programming language processing. In this paper, we propose the tree-based convolutional neural network (TBCNN) to model programming languages, which contain rich and explicit tree structural information. In our model, program vector representations are learned by the \"coding\" pretraining criterion based on abstract syntax trees (ASTs); the convolutional layer explicitly captures neighboring features on the tree; with the \"binary continuous tree\" and \"3-way pooling,\" our model can deal with ASTs of different shapes and sizes.We evaluate the program vector representations empirically, showing that the coding criterion successfully captures underlying features of AST nodes, and that program vector representations significantly speed up supervised learning. We also compare TBCNN to baseline methods; our model achieves better accuracy in the task of program classification. To our best knowledge, this paper is the first to analyze programs with deep neural networks; we extend the scope of deep learning to the field of programming language processing. The experimental results validate its feasibility; they also show a promising future of this new research area.", "histories": [["v1", "Thu, 18 Sep 2014 06:50:52 GMT  (220kb,D)", "http://arxiv.org/abs/1409.5718v1", null], ["v2", "Tue, 8 Dec 2015 12:31:51 GMT  (310kb,D)", "http://arxiv.org/abs/1409.5718v2", "Accepted at AAAI-16"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.SE", "authors": ["lili mou", "ge li", "lu zhang 0023", "tao wang", "zhi jin"], "accepted": true, "id": "1409.5718"}, "pdf": {"name": "1409.5718.pdf", "metadata": {"source": "CRF", "title": "TBCNN: A Tree-Based Convolutional Neural Network for Programming Language Processing", "authors": ["Lili Mou", "Ge Li", "Zhi Jin", "Lu Zhang", "Tao Wang"], "emails": ["zhanglu}@sei.pku.edu.cn", "twangcat@stanford.edu"], "sections": [{"heading": "Introduction", "text": "The profound upheavals that we have gone through in recent years have been clearly felt in the USA, Europe and throughout the world, \"he said in an interview with the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times, the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \""}, {"heading": "Tree-based Convolutional Neural Network", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "The Architecture", "text": "Programming languages have the natural tree representation - the abstract syntax tree (AST). Figure 2 shows the AST corresponding to the following C code snippet (analyzed by pycparser1): int a = b + 3; Each node in the AST is an abstract component in the program source code. A node p with children c1, \u00b7 \u00b7 \u00b7, cn represents the construction process of the component p \u2192 c1 \u00b7 \u00b7 \u00b7 c.The overall architecture of TBCNN is shown in Figure 1. In our model, each node in ASTs is initially represented as a distributed, real vector, so that the (anonymous) features of the symbols are captured. Vector representations are learned using the proposed \"coding criterion.\" We then intertwine a series of feature detectors on the AST and apply 3-way pooling, whereupon we add a hidden layer and an output layer."}, {"heading": "Representation Learning for AST Nodes", "text": "In this case, it is as if it were a real deflection, in which both sides use the same deflection mechanisms. (...) It is as if it were a real deflection. (...) It is as if it were a real deflection. (...) It is as if it were a real deflection. (...) It is as if it were a real deflection. (...)"}, {"heading": "Coding Layer", "text": "After pre-training the feature vectors for each symbol, we pass them on to the tree-based folding layer. In the case of leaves, they are only the vector representations learned in the pre-training phase. In the case of a non-leaf node p, there are two representations: the one learned in the pre-training phase (left side of formula 1) and the coded one (right side of formula 1). They are combined linearly before they are fed into the folding layer. Let c1, \u00b7 \u00b7, cn be the children of the node p and we refer to the combined vector as p. We havep = Wcomb1 \u00b7 vec (p) + Wcomb2 \u00b7 tanh (\u2211 i liWcode, i \u00b7 vec (xi) + bcode), with Wcomb1, Wcomb2 and RNf \u00b7 Nf being the parameters for the combination. They are initialized as diagonal matrices and then fine-tuned during the supervised training."}, {"heading": "Tree-based Convolution Layer", "text": "In this case, it is a branch that goes hand in hand with a sentence of finite-supporting cores, which are located in a fixed depth window in which there are n-nodes with vector representations, which are then to be found in a sentence of finite detectors. (n) D \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i. \"\" \"\" i. \"\" i. \"\" i. \"\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i."}, {"heading": "The \u201cContinuous Binary Tree\u201d Model", "text": "One problem is that we cannot determine the number of weight matrices because AST nodes have variable numbers of children. A possible solution is the \"continuous weight matrix\" model (Mikolov et al. 2013) 2, but position information is completely lost. A similar approach is used in (Hermann and Blunsom 2014). In our model, we consider each sub-tree as a \"binary\" tree, regardless of its size and shape. That is, we only have 3 weight matrices for each position. This method fails because there will be a huge number of different positions in our scenario. In our model, we consider each sub-tree as a \"binary\" tree, regardless of its size and shape. That means we only have 3 weight matrices for the weight matrices and 2 for the coding. We call it a \"continuous binary tree.\" Let's take the three parameter matrices as an example: W tconv, W l conv and W r conv."}, {"heading": "Experimental Results", "text": "In this section, we present two experimental results: First, we evaluate program vector representations empirically through hierarchical clustering to show that vector representations successfully capture meaningful features of AST nodes; second, we use TBCNN to classify programs based on functionality to test the feasibility and effectiveness of processing neural programming languages."}, {"heading": "The Dataset", "text": "We use the data set of an Open Judge (OJ) educational programming system 3. There are a large number of programming problems in the OJ system. Students submit their source codes as a solution to specific problems; the OJ system automatically evaluates the validity of the source codes submitted. We download the source codes and the corresponding problem IDs (labels) as our data set. Presentation learning is done via all C codes. When performing the programming classification task, two groups of programming problems are selected for classification. Each group contains 4 problems that are similar in functionality 4. We divide the data set into 3 parts: 60% for training, 20% for cross-validation (CV) and 20% for the test.The results, source codes and data set are available on our project website5."}, {"heading": "Evaluation of Vector Representations", "text": "The result confirms that similar symbols tend to have similar feature vectors. Figure 6 illustrates a subset of AST nodes. (The entire result and original vector representations can be downloaded from our project website.) As shown in Figure 6, the symbols fall mainly into three categories: (1) BinaryOp, ArrayRef, ID, Constant are grouped together because they are related to each other.3The URL of the OJ system is anonymized to be verified by others, and the data set can be verified using our project URL.4This selected data set prevents our classification task from being trivial, and is likely to achieve very high accuracy in randomly selected problems.5https: / sites.google.com / site / treebasedcnn /"}, {"heading": "Training cost (random initialization)", "text": "The result is quite telling because it is consistent with the human understanding of programs.Another evaluation for representative learning is to see if it improves supervised learning of interest. We perform the program classification and in Figure 7 draw the learning curves of the first 40 eras (iterations across all training examples) for randomly initialized and pre-trained weights. For reasons of simplicity and fairness, the hyperparameters are set as Nf = Nc = Nh = 30; '2 Penalty = 0; Dynamics = 0; learning rate is set and fixed to 0.03. These settings are selected manually in advance, unlike the settings for the final classification (they are selected by CV).As we can see from Figure 7, after an extremely slow training, the functions that are less costly are reduced, the setting of 15 weight units."}, {"heading": "Evaluation of the TBCNN model", "text": "To evaluate the feasibility and effectiveness of processing neural programming languages, we use TBCNN for the classification of programs. The results are presented in Table 16. We compare our approach with basic methods, namely logistic regression (LR) and SVM with kernels of linear and radial base function (RBF). These methods adopt the bag model and use counting functions, i.e. the characteristic vector of a program is the number of symbol occurrences. Linear classifiers (logistic regression and linear SVM) achieve 6part of the result is reported first in (Mou et al. 2014).approximately the same accuracy. SVM with RBF kernel is better than linear classifiers. We also apply the existing recursive neural networks (RNN, Socher et al. 2013b) to the program classification task. But the cost function is not effectively trained in our scenario."}, {"heading": "Related Work in Deep Learning", "text": "Deep neural networks have made significant breakthroughs in many areas of artificial intelligence, for example, computer vision (Krizhevsky, Sutskever, and Hinton 2012), speech recognition (Dahl, Mohamed, and Hinton 2010), natural language processing (Collobert et al. 2011), etc. Stacked restricted Boltzmann machines and autoencoders are successful pretraining methods (Hinton, Osindero, and Teh 2006; Bengio et al. 2007). They study the underlying characteristics of data uncontrolled, and give a more meaningful initialization of weights for later supervised learning. These approaches work well with generic data, but they are not suitable for programming speech processing because programs contain rich structural information. AST structures also differ widely between different programs, and therefore they cannot be fed directly into a fixed-size network. To capture explicit structures of data, it may be important and beneficial to integrate human priors into the networks."}, {"heading": "Conclusion", "text": "Based on the rich and explicit tree structures of programs, we propose the novel Tree-Based Convolutional Neural Network (TBCNN). In this model, program vector representations are learned based on the coding criterion; local structural features are recognized by the folding layer; the concepts of continuous binary tree and 3-way pooling are introduced into model trees of different sizes and shapes; empirical experiments show that significant features of AST nodes are successfully captured by the coding criterion; the TBCNN model is evaluated in the task of classifying programs; it achieves greater accuracy than basic methods; the experiments confirm the feasibility of processing neural programs; they also show a bright future for this new field. Based on current knowledge, we believe that deep learning will make great advances in the field of programming language processing."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning 2(1):1\u2013127.", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Mining the execution history of a software system to infer the best time for its adaptation", "author": ["K. Canavera", "N. Esfahani", "S. Malek"], "venue": "Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering.", "citeRegEx": "Canavera et al\\.,? 2012", "shortCiteRegEx": "Canavera et al\\.", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine learning.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Phone recognition with the mean-covariance restricted Boltzmann machine", "author": ["G. Dahl", "A. Mohamed", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Dahl et al\\.,? 2010", "shortCiteRegEx": "Dahl et al\\.", "year": 2010}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K. Hermann", "P. Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Hermann and Blunsom,? 2014", "shortCiteRegEx": "Hermann and Blunsom", "year": 2014}, {"title": "On the naturalness of software", "author": ["A. Hindle", "E. Barr", "Z. Su", "M. Gabel", "P. Devanbu"], "venue": "Proceedings of 34th International Conference on Software Engineering.", "citeRegEx": "Hindle et al\\.,? 2012", "shortCiteRegEx": "Hindle et al\\.", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E. Sackinger"], "venue": "Proceedings of International Conference on Artificial Neural Networks.", "citeRegEx": "LeCun et al\\.,? 1995", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Detecting memory leaks through introspective dynamic behavior modelling using machine learning", "author": ["S. Lee", "C. Jung", "S. Pande"], "venue": "Proceedings of 36th International Conference on Software Engineering.", "citeRegEx": "Lee et al\\.,? 2014", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Software defect prediction using semi-supervised learning with dimension reduction", "author": ["H. Lu", "B. Cukic", "M. Culp"], "venue": "Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering.", "citeRegEx": "Lu et al\\.,? 2012", "shortCiteRegEx": "Lu et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Building program vector representations for deep learning", "author": ["L. Mou", "G. Li", "Y. Liu", "H. Peng", "Z. Jin", "Y. Xu", "L. Zhang"], "venue": "arXiv preprint arXiv:1409.3358.", "citeRegEx": "Mou et al\\.,? 2014", "shortCiteRegEx": "Mou et al\\.", "year": 2014}, {"title": "Studying the language and structure in non-programmers\u2019 solutions to programming problems", "author": ["J. Pane", "C. Ratanamahatana", "B. Myers"], "venue": "International Journal of HumanComputer Studies 54(2):237\u2013264.", "citeRegEx": "Pane et al\\.,? 2001", "shortCiteRegEx": "Pane et al\\.", "year": 2001}, {"title": "The Language Instinct: The New Science of Language and Mind", "author": ["S. Pinker"], "venue": "Pengiun Press.", "citeRegEx": "Pinker,? 1994", "shortCiteRegEx": "Pinker", "year": 1994}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E. Huang", "A. Ng", "C. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "Q. Le", "C. Manning", "A. Ng"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "By exploring multiple layers of non-linear transformation, the deep architecture can extract underlying abstract features of data, which is crucial to artificial intelligence (AI) (Bengio 2009).", "startOffset": 180, "endOffset": 193}, {"referenceID": 4, "context": "Although deep learning has been successful in various fields\u2014like natural language processing (NLP) (Collobert and Weston 2008), computer vision (Krizhevsky, Sutskever, and Hinton 2012), and speech recognition (Dahl, Mohamed, and Hinton 2010)\u2014its advantages are not exploited in the field of programming language processing.", "startOffset": 100, "endOffset": 127}, {"referenceID": 8, "context": "Programs are complex, flexible and powerful; they also contain rich statistical properties (Hindle et al. 2012).", "startOffset": 91, "endOffset": 111}, {"referenceID": 18, "context": "(Pinker 1994) illustrates an interesting example, \u201cThe dog the stick the fire burned beat bit the cat.", "startOffset": 0, "endOffset": 13}, {"referenceID": 5, "context": ", 0\u2019s will give 0 distance but is meaningless), negative sampling is applied like (Collobert et al. 2011).", "startOffset": 82, "endOffset": 105}, {"referenceID": 15, "context": "One possible solution is the \u201ccontinuous bag of words\u201d model (Mikolov et al. 2013)2, but position information will be lost completely.", "startOffset": 61, "endOffset": 82}, {"referenceID": 7, "context": "Similar approach is used in (Hermann and Blunsom 2014).", "startOffset": 28, "endOffset": 54}, {"referenceID": 20, "context": "In (Socher et al. 2013a), a different weight matrix is allocated as parameters for each position.", "startOffset": 3, "endOffset": 24}, {"referenceID": 16, "context": "Part of the result is first reported in (Mou et al. 2014).", "startOffset": 40, "endOffset": 57}, {"referenceID": 5, "context": "Deep neural networks have made significant breakthroughs in many fields of artificial intelligence, for example, computer vision (Krizhevsky, Sutskever, and Hinton 2012), speech recognition (Dahl, Mohamed, and Hinton 2010), natural language processing (Collobert et al. 2011), etc.", "startOffset": 252, "endOffset": 275}, {"referenceID": 0, "context": "Stacked restricted Boltzmann machines and autoencoders are successful pretraining methods (Hinton, Osindero, and Teh 2006; Bengio et al. 2007).", "startOffset": 90, "endOffset": 142}, {"referenceID": 14, "context": "Another example is the recurrent neural network, which can be regarded as a time-decaying network (Mikolov et al. 2010).", "startOffset": 98, "endOffset": 119}, {"referenceID": 19, "context": "A model similar to ours is the recursive neural network (RNN) proposed in (Socher et al. 2011; 2013b) for NLP.", "startOffset": 74, "endOffset": 101}], "year": 2014, "abstractText": "Deep neural networks have made significant breakthroughs in many fields of artificial intelligence. However, it has not been applied in the field of programming language processing. In this paper, we propose the treebased convolutional neural network (TBCNN) to model programming languages, which contain rich and explicit tree structural information. In our model, program vector representations are learned by the \u201ccoding\u201d pretraining criterion based on abstract syntax trees (ASTs); the convolutional layer explicitly captures neighboring features on the tree; with the \u201cbinary continuous tree\u201d and \u201c3-way pooling,\u201d our model can deal with ASTs of different shapes and sizes. We evaluate the program vector representations empirically, showing that the coding criterion successfully captures underlying features of AST nodes, and that program vector representations significantly speed up supervised learning. We also compare TBCNN to baseline methods; our model achieves better accuracy in the task of program classification. To our best knowledge, this paper is the first to analyze programs with deep neural networks; we extend the scope of deep learning to the field of programming language processing. The experimental results validate its feasibility; they also show a promising future of this new research area.", "creator": "TeX"}}}