{"id": "1409.6110", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2014", "title": "Best-Arm Identification in Linear Bandits", "abstract": "We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter $\\theta^*$ and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, we point out the connection to the $G$-optimality criterion used in optimal experimental design.", "histories": [["v1", "Mon, 22 Sep 2014 08:41:02 GMT  (78kb)", "https://arxiv.org/abs/1409.6110v1", null], ["v2", "Tue, 4 Nov 2014 14:21:28 GMT  (79kb)", "http://arxiv.org/abs/1409.6110v2", "In Advances in Neural Information Processing Systems 27 (NIPS), 2014"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marta soare", "alessandro lazaric", "r\u00e9mi munos"], "accepted": true, "id": "1409.6110"}, "pdf": {"name": "1409.6110.pdf", "metadata": {"source": "CRF", "title": "Best-Arm Identification in Linear Bandits", "authors": ["Marta Soare Alessandro Lazaric", "R\u00e9mi Munos"], "emails": ["marta.soare@inria.fr", "alessandro.lazaric@inria.fr", "remi.munos@inria.fr"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.61 10v2 [cs.LG] 4 Nov 201 4"}, {"heading": "1 Introduction", "text": "The Stochastic Multi-Arm Bandit Problem (MAB) [16] provides an easy formalization for studying the sequential design of LB experiments. In the standard model, a learner sequentially selects an arm from K and receives a reward drawn from a fixed, unknown distribution relative to the selected arm. While most of the literature in bandit theory focuses on the problem of maximizing cumulative rewards, where the learner does not need to maximize the sum of rewards - off exploration and exploitation, recently the pure exploration setting [5] has attracted much attention. Here, the learner uses the available budget to identify the best arm as accurately as possible without trying to maximize the sum of rewards. Although many results are now available in a wide range of settings (e.g. best-arm identification with fixed budget [2, 11] and fixed trust [7], partial selection [6, 12], and multi-bandit."}, {"heading": "2 Preliminaries", "text": "We consider the standard model of linear bandits as the gap between the two arms. (...) We consider the standard model of linear bandits as a finite series of arms (...). (...) We consider the standard model of linear bandits as a finite series of arms (...). (...) We consider the standard model of linear bandits (...). (...) We consider the standard model of linear bandits as a finite series of arms (...). (...) We assume that each time an arm x... (...) is generated a random reward r (...). (...) We assume that each time a random reward r (...) is generated according to the linear model r (...)."}, {"heading": "6 Numerical Simulations", "text": "We illustrate the performance of the XY-adaptive and compare it with the XY-Oracle strategy (equation 5), the static mappings XY and G, as well as with the fully adaptive version of XY, where X-Y is updated every turn and the boundary of Prop.2. For a fixed confidence \u03b4 = 0.05, we compare the sampling budget required to identify the best arm with a probability of at least 1 \u2212 \u03b4. We look at a range of weapons X-Rd, with | X-d + 1 including the canonical base (e1,., ed) and an additional arm xd + 1 = [cos (?) sin (?) 0. 0] We opt for the strategies X-Rd = [2 0. 0] and fix the uncertainty Y = 0.01, so that we reduce the probability min = (x1 \u2212 xd + 1)."}, {"heading": "7 Conclusions", "text": "In this paper, we investigated the problem of identifying the best arm with a fixed trust, in the linear setting of bandits. First, we offered a preliminary characterization of the problem-dependent complexity of the identification task of the best arm and showed its connection with the complexity within the framework of the MAB. Then, we designed and analyzed efficient sampling strategies for this problem. The G allocation strategy enabled us to show a close connection with optimal experimental design techniques, in particular with the G optimality criterion. Through the second proposed strategy, the XY algorithms, we introduced a novel optimal design problem where the test arms do not coincide with the weapons chosen in the design. Finally, we pointed out the limits that a fully adaptive allocation strategy could have in linear bandit setting and proposed a step-by-step algorithm, XY adaptive that learns from previous observations without suffering from the dimension."}, {"heading": "B Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Lemmas", "text": "The proof for Lemma 1. The proof stems from the fact that if S * (xn) C (x *) and \u03b8 * (xn) with high probability, then \u03b8 * n * C (x *), which implies that \"(\u03b8 * n) = x * by definition of the cone C (x *). Before proceeding to the proof of Lemma 2, we introduce the following technical tool.Sentence 3 (equivalence theorem in [13]). Let us define f (x *) = x M (*) \u2212 1x, where M (*) is a d \u00b7 d non-singular matrix and x is a column vector in Rd. We consider two extreme problems."}, {"heading": "The first is to choose \u03be so that", "text": "(1) Maximum DetM maximization (D-optimal design)"}, {"heading": "The second one is to choose \u03be so that", "text": "(2) The minimization of max f (x; B) (G-optimal design) We find that the integral in relation to f (x; B) is d; therefore max f (x; B) is \u2265 d, and therefore a sufficient condition for the fulfillment of (2) is (3) max f (x; B) = d."}, {"heading": "Statements (1), (2) and (3) are equivalent.", "text": "The proof for Lemma 2. Upper limit. We have the following sequence of inequalities: max y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y"}, {"heading": "Lower-bound.", "text": "We focus on the numerical value of y x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "B.2 Proofs of Theorem 1 and Theorem 2", "text": "The explanation follows from Prop. 1 and the performance guarantees for the various implementations of the G optimal design. By remembering the empirical stop condition in Eq. 11 and the definition of the G-2 problem, we note that from a simple triangle inequality applied at the level of G-1, a sufficient condition for stopping is that for each x-2 condition the condition for the stop condition is met, which is derived from the continuous relaxation or the greedy progressive algorithm. From Prop. 1 we have that the following inequalities (x-2 condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition) we (x-condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-x-condition, x-condition, x-condition, x-condition, x-x-condition, x-condition, x-condition, x-x-condition, x-condition, x-condition, x-condition, x-x-condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition x-condition, x-condition, x-condition, x-condition x-condition, x-condition, x-condition x-condition, x-condition, x-condition, x-condition x-condition,"}, {"heading": "D Proof of Theorem 3", "text": "Before proceeding to the proof, let us first report on the results of two different approaches which we have reconciled with each other. Lemma 4. The proof of Lemma 4. Let us apply the definition of S (xn) to Eq. 10 and take into account the fact that the condition in Eq. 16 is very likely to apply to us (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (c, c, c, c (c), (c), (c,), (c, (,), (c, (,), (c,), (c, (,), (c,), (c, (,), (,), (, c, (,), (, c), (, c, (,), (, c,), (, (,), (, (,), (, (,), (,), (, (,), (, (, (,), (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (, (,), (, (,), (, (, (), (, (, (, (), (,), (, (,), (, (), (), (, (, (), (, (, (), (, (), (), (, (, (, (, (), (, (, (), (, (, (, (, (,"}, {"heading": "E Additional Empirical Results", "text": "For the setting described in paragraph 6 to show the different redistributions of the sample budget to the weapons, we present in figure 5 the number of samples allocated per arm, in case the input space is X R5. We recall that the samples designated x1,.., x5 form the canonical basis and the arm x6 = [cos (\u03c9) sin (\u03c9) 0 0 0 0]. We note that although the fully adaptive algorithm determines the most informative direction and focuses the sample on arm x2, its sample complexity still exhibits a linear growth in dimension due to the additional d-term in its limit. Consequently, the advantage over the static strategies is eliminated. On the other hand, the XY-adaptive \"learns\" the gaps from the observations and allocates the samples very similar to the XY-oracles without suffering a large loss in terms of sample budget."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Best arm identification in multiarmed bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "R\u00e9mi Munos"], "venue": "In Proceedings of the 23rd Conference on Learning Theory (COLT),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Submodularity and randomized rounding techniques for optimal experimental design", "author": ["Mustapha Bouhtou", "Stephane Gaubert", "Guillaume Sagnol"], "venue": "Electronic Notes in Discrete Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S\u00e9bastien Bubeck", "R\u00e9mi Munos", "Gilles Stoltz"], "venue": "In Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Multiple identifications in multiarmed bandits", "author": ["S\u00e9bastien Bubeck", "Tengyao Wang", "Nitin Viswanathan"], "venue": "In Proceedings of the International Conference in Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems", "author": ["Eyal Even-Dar", "Shie Mannor", "Yishay Mansour"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["Victor Gabillon", "Mohammad Ghavamzadeh", "Alessandro Lazaric"], "venue": "In Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Multi-bandit best arm identification", "author": ["Victor Gabillon", "Mohammad Ghavamzadeh", "Alessandro Lazaric", "S\u00e9bastien Bubeck"], "venue": "In Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning", "author": ["Matthew D. Hoffman", "Bobak Shahriari", "Nando de Freitas"], "venue": "In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "lil\u2019 UCB : An optimal exploration algorithm for multi-armed bandits", "author": ["Kevin G. Jamieson", "Matthew Malloy", "Robert Nowak", "S\u00e9bastien Bubeck"], "venue": "In Proceeding of the 27th Conference on Learning Theory (COLT),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Information complexity in bandit subset selection", "author": ["Emilie Kaufmann", "Shivaram Kalyanakrishnan"], "venue": "In Proceedings of the 26th Conference on Learning Theory (COLT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "The equivalence of two extremum problems", "author": ["Jack Kiefer", "Jacob Wolfowitz"], "venue": "Canadian Journal of Mathematics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1960}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E. Schapire"], "venue": "In Proceedings of the 19th International Conference on World Wide Web (WWW),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Optimal Design of Experiments", "author": ["Friedrich Pukelsheim"], "venue": "Classics in Applied Mathematics. Society for Industrial and Applied Mathematics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1952}, {"title": "Approximation of a maximum-submodular-coverage problem involving spectral functions, with application to experimental designs", "author": ["Guillaume Sagnol"], "venue": "Discrete Appl. Math.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "The stochastic multi-armed bandit problem (MAB) [16] offers a simple formalization for the study of sequential design of experiments.", "startOffset": 48, "endOffset": 52}, {"referenceID": 4, "context": "While most of the literature in bandit theory focused on the problem of maximization of cumulative rewards, where the learner needs to trade-off exploration and exploitation, recently the pure exploration setting [5] has gained a lot of attention.", "startOffset": 213, "endOffset": 216}, {"referenceID": 1, "context": ", best-arm identification with fixed budget [2, 11] and fixed confidence [7], subset selection [6, 12], and multi-bandit [9]), most of the work considered only the multi-armed setting, with K independent arms.", "startOffset": 44, "endOffset": 51}, {"referenceID": 10, "context": ", best-arm identification with fixed budget [2, 11] and fixed confidence [7], subset selection [6, 12], and multi-bandit [9]), most of the work considered only the multi-armed setting, with K independent arms.", "startOffset": 44, "endOffset": 51}, {"referenceID": 6, "context": ", best-arm identification with fixed budget [2, 11] and fixed confidence [7], subset selection [6, 12], and multi-bandit [9]), most of the work considered only the multi-armed setting, with K independent arms.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": ", best-arm identification with fixed budget [2, 11] and fixed confidence [7], subset selection [6, 12], and multi-bandit [9]), most of the work considered only the multi-armed setting, with K independent arms.", "startOffset": 95, "endOffset": 102}, {"referenceID": 11, "context": ", best-arm identification with fixed budget [2, 11] and fixed confidence [7], subset selection [6, 12], and multi-bandit [9]), most of the work considered only the multi-armed setting, with K independent arms.", "startOffset": 95, "endOffset": 102}, {"referenceID": 8, "context": ", best-arm identification with fixed budget [2, 11] and fixed confidence [7], subset selection [6, 12], and multi-bandit [9]), most of the work considered only the multi-armed setting, with K independent arms.", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "An interesting variant of the MAB setup is the stochastic linear bandit problem (LB), introduced in [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 0, "context": ", [1, 14]), in this paper we focus on the pure-exploration scenario.", "startOffset": 2, "endOffset": 9}, {"referenceID": 13, "context": ", [1, 14]), in this paper we focus on the pure-exploration scenario.", "startOffset": 2, "endOffset": 9}, {"referenceID": 9, "context": "Recently, the best-arm identification in linear bandits has been studied in a fixed budget setting [10], in this paper we study the sample complexity required to identify the best-linear arm with a fixed confidence.", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "While different settings can be defined (see [8] for an overview), here we focus on the (\u01eb, \u03b4)-best-arm identification problem (the so-called PAC setting), where given \u01eb and \u03b4 \u2208 (0, 1), the objective is to design an allocation strategy and a stopping criterion so that when the algorithm stops, the returned arm x\u0302(n) is such that P ( Rn \u2265 \u01eb ) \u2264 \u03b4, while minimizing the needed number of steps.", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "In the fixed budget case, HMAB determines the probability of returning the wrong arm [2], while in the fixed confidence case, it characterizes the sample complexity [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "In the fixed budget case, HMAB determines the probability of returning the wrong arm [2], while in the fixed confidence case, it characterizes the sample complexity [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "2 in [1]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "12 (see [4, 17] for some recent results and [18] for a more thorough discussion).", "startOffset": 8, "endOffset": 15}, {"referenceID": 16, "context": "12 (see [4, 17] for some recent results and [18] for a more thorough discussion).", "startOffset": 8, "endOffset": 15}], "year": 2014, "abstractText": "We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter \u03b8 and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the G-optimality criterion used in optimal experimental design.", "creator": "LaTeX with hyperref package"}}}