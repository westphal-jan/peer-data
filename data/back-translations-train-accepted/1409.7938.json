{"id": "1409.7938", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2014", "title": "Lazier Than Lazy Greedy", "abstract": "Is it possible to maximize a monotone submodular function faster than the widely used lazy greedy algorithm (also known as accelerated greedy), both in theory and practice? In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint. We show that our randomized algorithm, Rand-Greedy, can achieve a (1-1/e) approximation guarantee to the optimum solution in time linear in the size of the data and independent of the cardinality constraint. We empirically demonstrate the effectiveness of our algorithm on submodular functions arising in data summarization, including training large-scale kernel methods and exemplar-based clustering. We observe that Rand-Greedy practically achieves the same utility value as lazy greedy but runs much faster. More surprisingly, we observe that in many practical scenarios Rand-Greedy does not evaluate the whole fraction of data points even once and still achieves indistinguishable results compared to lazy greedy.", "histories": [["v1", "Sun, 28 Sep 2014 18:06:23 GMT  (44kb)", "https://arxiv.org/abs/1409.7938v1", "14 pages, 9 figures, sibmitted to AAAI 2014"], ["v2", "Wed, 26 Nov 2014 08:45:32 GMT  (855kb)", "http://arxiv.org/abs/1409.7938v2", "In Proc. Conference on Artificial Intelligence (AAAI), 2015"], ["v3", "Fri, 28 Nov 2014 13:06:54 GMT  (855kb)", "http://arxiv.org/abs/1409.7938v3", "In Proc. Conference on Artificial Intelligence (AAAI), 2015"]], "COMMENTS": "14 pages, 9 figures, sibmitted to AAAI 2014", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IR", "authors": ["baharan mirzasoleiman", "ashwinkumar badanidiyuru", "amin karbasi", "jan vondr\u00e1k", "andreas krause 0001"], "accepted": true, "id": "1409.7938"}, "pdf": {"name": "1409.7938.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Baharan Mirzasoleiman", "Ashwinkumar Badanidiyuru", "Amin Karbasi", "Jan Vondr\u00e1k", "Andreas Krause"], "emails": ["baharanm@inf.ethz.ch", "ashwinkumarbv@google.com", "amin.karbasi@yale.edu", "jvondrak@us.ibm.com", "krausea@ethz.ch"], "sections": [{"heading": null, "text": "ar Xiv: 140 9,79 38v3 [cs.LG] 2 8N ov2 01"}, {"heading": "Introduction", "text": "For the past few years, we have observed the emergence of datasets of an unprecedented magnitude across various scientific disciplines. (The large volume of such datasets presents new computational challenges such as the diverse, feature-rich, unstructured, and usually high-resolution data that do not allow for effective, data-intensive inferences. (In this respect, data aggregation is a compelling (and sometimes the only) approach aimed at both leveraging the wealth of large-format data and at being computationally tractable.) Rather than operating directly on complex and large data, carefully constructed summaries allow not only the execution of different data analysis tasks, but also to improve their efficiency and scalability. To effectively summarize the data, we need to define a measure of representativeness that lies within a selected set. When we think of representative elements that best cover, or are most informative, w.r.the positions in a data set, of course, we add a new element to its representation, which is, say, A."}, {"heading": "Related Work", "text": "In fact, the majority of these applications that have been launched in recent years have to collect ever greater amounts of data in order to process it. (...) Most of these applications have to deal with ever greater amounts of data. (...) Most of these applications are able to identify themselves. (...) Most of these applications have to be able to identify themselves. (...) Most of these applications have to deal with ever greater amounts of data. (...) Most of these applications are able to identify themselves. (...) Most of these applications have to be able to identify themselves with others. (...) Most of these applications have to deal with ever greater amounts of data. (...) Accelerated / lazy variants (...)"}, {"heading": "STOCHASTIC-GREEDY Algorithm", "text": "In this section, we will present our randomized greed algorithm STOCHASTIC-GREEDY and then show how to combine it with rotten evaluations. We will demonstrate that STOCHASTICGREEDY has a proven linear runtime independent of k, with (as expected) the same approximate guarantee. In the following section, we will further demonstrate through experiments that this is also reflected in practice, i.e. STOCHASTIC-GREEDY is significantly faster than LAZY-GREEDY and practically identical to it in terms of application.The basic idea behind STOCHASTIC-GREEDY is to create an element that improves the value of the solution in roughly the same way as greedily, but in a fast manner. This is achieved by a sub-sampling step. At a very high level, this is comparable to how stochastic gradient departure improves the runtime of the gradient for optimization."}, {"heading": "Random Sampling", "text": "The main reason why the classical greedy algorithm works is that with each iteration i an element is identified that reduces the gap to the optimal solution by a significant amount, i.e. by at least (f (A) - f (Ai \u2212 1) / k. This requires n oracle calls per step, the main bottleneck of the classical greedy algorithm. Our main observation here is that we can achieve the same improvement through submodularity by adding a uniform random element from A to our current group A. To achieve this improvement, we will see that it is sufficient to log a series of R of size (n / k) (1 / k), which in turn is associated with the probability 1 \u2212 k. This is the main reason why we are able to achieve an increase in performance. The algorithm is formally represented in algorithm 1."}, {"heading": "Random Sampling with Lazy Evaluation", "text": "While our theoretical results have a demonstrably linear time algorithm, we can combine the random sampling method with a rotten evaluation to increase its performance. There are two main reasons why a rotten evaluation helps. First, the randomly sampled quantities may overlap and we can take advantage of the previously evaluated marginal gains. Second, as in LAZYGREEDY, although the marginal values of the elements may change at each step of the greedy algorithm, their sequence often does not change (Minoux 1978). Therefore, we can apply a directly rotten evaluation in line 4 of algorithm 1 as follows. We retain an upper limit (e) (first) for the marginal increase of all elements sorted in descending order. In each iteration i, STOCHASTIC-GREEDY samples a row R. From this group R, it evaluates the element that is at the top of the list. Let us denounce this element with a row."}, {"heading": "Experimental Results", "text": "In this section we address the following questions: 1) how well does STOCHASTIC-GREEDY work compared to previous methods, and in particular LAZY-GREEDY, and 2) how does STOCHASTIC-GREEDY work in getting near optimal solutions on large data sets by reducing computational complexity? To this end, we compare the performance of our STOCHASTIC-GREEDY method with the following benchmarks: RANDOM-SELECTION, where the output of randomly selected data points from V is; LAZY-GREEDY, where the output of data points produced by the accelerated GREEDY method is."}, {"heading": "Conclusion", "text": "We have developed the first linear time algorithm STOCHASTIC-GREEDY, which does not depend on k for cardinality-related submodular maximization. STOCHASTIC-GREEDY provides a 1 \u2212 1 / e approximation guarantee for the optimal solution with only n log 1\u043e function assessments. We have also demonstrated the effectiveness of our algorithm through an extensive series of experiments. As these demonstrate, STOCHASTIC-GREEDY achieves a large part of the functional benefit at significantly lower computing costs. This improvement is useful even for approaches that calculate submodular function in parallel or break it down into simpler functions to enable faster evaluation. STOCHASTIC-GREEDY's properties make it very attractive and necessary for solving very large problems. Given the importance of submodular optimizations for numerous AI and machine learning applications, we believe that our results represent an important step towards solving such problems on a larger scale."}, {"heading": "Appendix, Analysis", "text": "The following problem gives us the approximation guarantee. Let us estimate the probability that R (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) ()) (A) (A) (A) (A) (A) (A) (A) (A) (A) () (A) (A) (A) () (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) ()) (A) (A) () (A) ()) (A) (A) () (A) () (A) () (A) () (A) () () (A) () (A) () () (A) () (A) () () (A) () () (A) () (A) () (A) () (A) () () (A) () () (A) () (A) () () (A) () (A) () (A) () () ((A) () (A) () (() ((A) () (A) () () ((A) (() ((A) () ((A) () (() (A) () (() (A) () (A) (() (A) () (A) (() (() () (A) () (A) ((A) () () ((A) (A) (A) (("}], "references": [{"title": "and Vondr\u00e1k", "author": ["A. Badanidiyuru"], "venue": "J.", "citeRegEx": "Badanidiyuru and Vondr\u00e1k 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Streaming submodular optimization: Massive data summarization on the fly", "author": ["Badanidiyuru"], "venue": null, "citeRegEx": "Badanidiyuru,? \\Q2014\\E", "shortCiteRegEx": "Badanidiyuru", "year": 2014}, {"title": "G", "author": ["Blelloch"], "venue": "E.; Peng, R.; and Tangwongsan, K.", "citeRegEx": "Blelloch. Peng. and Tangwongsan 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Max-cover in mapreduce", "author": ["Kumar Chierichetti", "F. Tomkins 2010] Chierichetti", "R. Kumar", "A. Tomkins"], "venue": "In Proceedings of the 19th international conference on World wide web", "citeRegEx": "Chierichetti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chierichetti et al\\.", "year": 2010}, {"title": "Summarization through submodularity and dispersion", "author": ["Kumar Dasgupta", "A. Ravi 2013] Dasgupta", "R. Kumar", "S. Ravi"], "venue": null, "citeRegEx": "Dasgupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2013}, {"title": "and Guestrin", "author": ["K. El-Arini"], "venue": "C.", "citeRegEx": "El.Arini and Guestrin 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Turning down the noise in the blogosphere", "author": ["El-Arini"], "venue": null, "citeRegEx": "El.Arini,? \\Q2009\\E", "shortCiteRegEx": "El.Arini", "year": 2009}, {"title": "and Krause", "author": ["D. Golovin"], "venue": "A.", "citeRegEx": "Golovin and Krause 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Krause", "author": ["R. Gomes"], "venue": "A.", "citeRegEx": "Gomes and Krause 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Bilmes", "author": ["A. Guillory"], "venue": "J.", "citeRegEx": "Guillory and Bilmes 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["L. Kaufman", "Rousseeuw"], "venue": "J.", "citeRegEx": "Kaufman and Rousseeuw 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximizing the spread of influence through a social network", "author": ["Kleinberg Kempe", "D. Tardos 2003] Kempe", "J. Kleinberg", "E. Tardos"], "venue": "In Proceedings of the ninth ACM SIGKDD", "citeRegEx": "Kempe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kempe et al\\.", "year": 2003}, {"title": "and Guestrin", "author": ["A. Krause"], "venue": "C.", "citeRegEx": "Krause and Guestrin 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Guestrin", "author": ["A. Krause"], "venue": "C.", "citeRegEx": "Krause and Guestrin 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient sensor placement optimization for securing large water distribution", "author": ["Krause"], "venue": null, "citeRegEx": "Krause,? \\Q2008\\E", "shortCiteRegEx": "Krause", "year": 2008}, {"title": "Fast greedy algorithms in mapreduce and streaming", "author": ["Kumar"], "venue": "In SPAA", "citeRegEx": "Kumar,? \\Q2013\\E", "shortCiteRegEx": "Kumar", "year": 2013}, {"title": "Cost-effective outbreak detection in networks", "author": ["Leskovec"], "venue": null, "citeRegEx": "Leskovec,? \\Q2007\\E", "shortCiteRegEx": "Leskovec", "year": 2007}, {"title": "and Bilmes", "author": ["H. Lin"], "venue": "J.", "citeRegEx": "Lin and Bilmes 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed submodular maximization: Identifying representative elements in massive data", "author": ["Mirzasoleiman"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "Mirzasoleiman,? \\Q2013\\E", "shortCiteRegEx": "Mirzasoleiman", "year": 2013}, {"title": "L", "author": ["G.L. Nemhauser", "Wolsey"], "venue": "A.", "citeRegEx": "Nemhauser and Wolsey 1978", "shortCiteRegEx": null, "year": 1978}, {"title": "M", "author": ["G.L. Nemhauser", "L.A. Wolsey", "Fisher"], "venue": "L.", "citeRegEx": "Nemhauser. Wolsey. and Fisher 1978", "shortCiteRegEx": null, "year": 1978}, {"title": "C", "author": ["A. Ostfeld", "J.G. Uber", "E. Salomons", "J.W. Berry", "W.E. Hart", "Phillips"], "venue": "A.; Watson, J.-P.; Dorini, G.; Jonkergouw, P.; Kapelan, Z.; et al.", "citeRegEx": "Ostfeld et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "M", "author": ["Rodriguez"], "venue": "G.; Leskovec, J.; and Krause, A.", "citeRegEx": "Rodriguez. Leskovec. and Krause 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Temporal corpus summarization using submodular word coverage", "author": ["Sipos"], "venue": "In CIKM", "citeRegEx": "Sipos,? \\Q2012\\E", "shortCiteRegEx": "Sipos", "year": 2012}, {"title": "W", "author": ["A. Torralba", "R. Fergus", "Freeman"], "venue": "T.", "citeRegEx": "Torralba. Fergus. and Freeman 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "L", "author": ["A. Tsanas", "M.A. Little", "P.E. McSharry", "Ramig"], "venue": "O.", "citeRegEx": "Tsanas et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Using document summarization techniques for speech data subset selection", "author": ["Wei"], "venue": null, "citeRegEx": "Wei,? \\Q2013\\E", "shortCiteRegEx": "Wei", "year": 2013}, {"title": "Fast multi-stage submodular maximization", "author": ["Iyer Wei", "K. Bilmes 2014] Wei", "R. Iyer", "J. Bilmes"], "venue": null, "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}], "referenceMentions": [], "year": 2014, "abstractText": "Is it possible to maximize a monotone submodular function faster than the widely used lazy greedy algorithm (also known as accelerated greedy), both in theory and practice? In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can achieve a (1 \u2212 1/e \u2212 \u03b5) approximation guarantee, in expectation, to the optimum solution in time linear in the size of the data and independent of the cardinality constraint. We empirically demonstrate the effectiveness of our algorithm on submodular functions arising in data summarization, including training large-scale kernel methods, exemplar-based clustering, and sensor placement. We observe that STOCHASTIC-GREEDY practically achieves the same utility value as lazy greedy but runs much faster. More surprisingly, we observe that in many practical scenarios STOCHASTIC-GREEDY does not evaluate the whole fraction of data points even once and still achieves indistinguishable results compared to lazy greedy.", "creator": "LaTeX with hyperref package"}}}