{"id": "1409.8185", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2014", "title": "Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models", "abstract": "We develop a sequential low-complexity inference procedure for the Infinite Gaussian Mixture Model (IGMM) for the general case of an unknown mean and covariance. The observations are sequentially allocated to classes based on a sequential maximum a-posterior (MAP) criterion. We present an easily computed, closed form for the conditional likelihood, in which the parameters can be recursively updated as a function of the streaming data. We propose a novel adaptive design for the Dirichlet process concentration parameter at each iteration, and prove, under a simplified model, that the sequence of concentration parameters is asymptotically well-behaved. We sketch an equivalence between the steady-state performance of the algorithm and Gaussian classification. The methodology is applied to the problem of adaptive modulation recognition and obviates the need for storing a large modulation library required for traditional modulation recognition. We also numerically evaluate the bit error rate performance (BER) of the DPMM-trained classifier when used as a demodulator and show that there is critical signal-to-noise ratio (SNR) that characterizes whether successful decoding is possible.", "histories": [["v1", "Mon, 29 Sep 2014 16:47:44 GMT  (472kb,D)", "https://arxiv.org/abs/1409.8185v1", "31 pages, Submitted"], ["v2", "Thu, 5 Feb 2015 15:55:06 GMT  (1062kb,D)", "http://arxiv.org/abs/1409.8185v2", "27 pages, Submitted"], ["v3", "Fri, 11 Sep 2015 20:07:31 GMT  (671kb,D)", "http://arxiv.org/abs/1409.8185v3", "25 pages, To appear in Advances in Neural Information Processing Systems (NIPS) 2015"]], "COMMENTS": "31 pages, Submitted", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["theodoros tsiligkaridis", "keith w forsythe"], "accepted": true, "id": "1409.8185"}, "pdf": {"name": "1409.8185.pdf", "metadata": {"source": "CRF", "title": "Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models", "authors": ["Theodoros Tsiligkaridis", "Keith W. Forsythe"], "emails": ["ttsili@ll.mit.edu", "forsythe@ll.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "This requires significant compilation for high-dimensional datasets or large samples. Bayesian nonparametric modeling are alternative approaches to parametric modeling, which can automatically derive the number of clusters from the data of the Bayesian chain. Bayesian non parametric modeling are alternative approaches to parametric modeling, which can automatically derive the number of clusters from the data. Bayesian non parametric modeling are alternative approaches to parametric modeling, where the number of clusters can be automated."}, {"heading": "2 Sequential Inference Framework for DPMM", "text": "Here we look at the SUGS framework of [15] for online clustering. Here, the non-parametric nature of the Dirichlet process manifests itself as model mixture models with countable infinite components. Let us specify the observations by yi-Rd and \u03b3i to indicate the class name of the ith observation (a latent variable). We define the available information at the moment i as y (i) = {y1,. \u2212 yi} and \u03b3 (i-1) = {\u03b31,. Let us select the best class name for yi-1 (.). The online sequential updating and greedy search (SUGS) algorithm is next summarized for completeness. Let us set g-1 = \u2212 \u2212 \u2212 1 and calculate \u03c0 (1 | y1, \u03b31). For i-Rd 2.1. Let us choose the best class name for yi: The best class name i-arg max."}, {"heading": "2.1 Adaptation of Concentration Parameter \u03b1", "text": "It is well known that the concentration parameter \u03b1 has a strong influence on the growth of the number of classes i. Our experiments show that in this sequential framework, the choice of \u03b1 is even more critical. Selecting a fixed \u03b1 as in the online SVA algorithm of [8] requires cross-validation that is mathematically prohibitive for large datasets. Furthermore, in the streaming data setting, where no estimate of data complexity exists, it may be impossible to perform cross-validation. Although the parameter \u03b1 is handled by a fully Bayesian treatment in [15], a pre-specified grid of possible values \u03b1, say {\u03b1l} Ll = 1, together with the previous distribution over it, can be used. Storage and updating of a matrix of size (ki \u2212 1) \u00b7 L and further marginalization is necessary to calculate P (\u03b3i = h | y)."}, {"heading": "3 Sequential Inference under Unknown Mean &", "text": "Unknown CovarianceWe are considering the general case of an unknown mean and covariance for each class. The probable model for the parameter of each class is rendered as follows: yi | \u00b5 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "4 Growth Rate Analysis of Number of Classes &", "text": "Stability In this section, we derive a model for posterior distribution pn (\u03b1) using large sample approximations = that allows us to derive growth rates by the number of classes and the order of concentration variables \u2212 \u2212 \u2212 \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n. \u2212 n \u2212 n. \u2212 pj (\u03b1) = O (log1 + n) for any small (under certain mild conditions. The probability density of the alpha parameter is updated as follows \u2212 pj + 1 (\u03b1).pj (\u03b1 j + innovation class chosen1 j + \u03b1 \u03b1 \u03b1 \u03b1 \u03b1 \u03b1 \u03b1 \u03b1 \u03b1 \u03b1, showing only the alpha-dependent factors in the update. By standardizing, the initial independent factors are absorbed into a probability density. Choosing the innovation class pushes the mass towards infinity, while selecting another class pushes the mass towards zero. Thus, there is a possibility that the probability of innovation increases in undesirability."}, {"heading": "5 Asymptotic Normality of Conditional Likelihood", "text": "In this section we derive an asymptotic expression for the conditional probability (22) to gain an insight into the equilibrium state of the algorithm. Based on the limits of the gamma function in Theorem 1.6 [2], Lima (a) e \u2212 d / 2 (a \u2212 1 / 2) d / 2 = 1. Under normal convergence conditions of the algorithm (including circumcision and fusing steps) all classes h = 1,.., K is correctly identified and with approximately ni \u2212 1 \u2212 h (i \u2212 1) observations at the time i \u2212 1. Thus, the conditional class converges before each class h to i \u2192 processes resulting from the (14), h (h \u2212 1), h \u2212 d = 1 \u2212 d of the distribution."}, {"heading": "5.1 Prune & Merge", "text": "It is possible that several clusters are similar and classes are formed based on outliers or on the particular order of the streaming data sequence, as noted in [8] These effects can be attenuated by adding a circumcision and merging step in the ASUGS algorithm. The circumcision step can be performed as follows: Define w (i) hdef = \u2211 ij = 1 q (j) h, i.e. the running sum of the rear weights. The relative weight of each component in ith iteration can be expressed as w (i) h = w (i) h (i) h \u2211 k (i) k. Ifw (i) h < r, then the component will be removed. Merging can be performed by merging two clusters k1 and k2, once the \"1 distance between the rear units falls below a threshold over time."}, {"heading": "6 Experiments", "text": "We apply the ASUGS learning algorithm to a synthetic 16-class example and to a real dataset to verify the stability and accuracy of our method.The experiments show the value of adjusting the Dirichlet concentration parameter for online clustering and parameter estimation.Since it is possible that several clusters are similar and classes are formed due to outliers or due to the particular order of the streaming data sequence, we add the truncation and merge step proposed in [8] to the ASUGS algorithm. We compare ASUGS and ASUGS-PM with SUGS, SUGS-PM, SVA and SVA-PM because [8] showed that SVA and SVA-PM exceed the block-based methods that perform iterative updates across the entire dataset, including Collapsed Gibbs Sampling, MCMC with Split-Merge and Truncation-Free Inference."}, {"heading": "6.1 Synthetic Data set", "text": "We consider learning the parameters of a 16-class Gaussian mixture, each with an equal variance of \u03c32 = 0.025. The training set consisted of 500 iid-based samples and the test set consisted of 1000 iid-based samples. The cluster results are shown in Figure 1 (a), which shows that the ASUGS-based approaches are more stable than SVA-based algorithms. ASUGS-PM performs best and identifies the correct number of clusters and their parameters. Figure 1 (b) shows the data log probability on the test set (averaged over 100 Monte Carlo studies), the mean and the variance of the class number on each iteration. The ASUGS-based approaches achieve a higher log probability than SVA-based approaches asymptotically. Figure 6.1 provides some numerical verification for the assumptions of Theorem 2. As expected, the predictable probability of L i converges after the probability of 10 (K) have been worked with the mixed samples."}, {"heading": "6.2 Real Data Set", "text": "We used the non-parametric Bayesian online methods for clustering image data. We used the MNIST dataset, which consists of 60,000 training samples and 10,000 test samples. Each sample is a 28 x 28 image of a handwritten digit (a total of 784 dimensions), and we perform PCA pre-processing to reduce the dimensionality to d = 50 dimensions as in [7]. We use only a random subset of 1.667%, consisting of 1000 random samples for training. This training set contains data from all 10 digits with a roughly uniform proportion. Fig. 3 shows the predictive logic probability of the test set and the averages for clusters obtained using ASUGSPM or SVA-PM. Furthermore, we point out that ASUGS-PM achieves higher log probability values and all digits using only 23 clusters are correct, while SVA-PM finds some clusters using 56 digits."}, {"heading": "6.3 Discussion", "text": "Although both SVA and ASUGS methods have similar computational complexity and use decisions and information from the processing of previous samples to decide class innovations, the mechanics of these methods are quite different. ASUGS uses an adaptive \u03b1 motivated by asymptotic theory, while SVA uses a fixed \u03b1. Additionally, SVA updates the parameters of all components at each iteration (in a weighted manner), while ASUGS only updates the parameters of the most likely cluster, minimizing leakage of unrelated components. ASUGS's \u03bb parameter does not affect performance as much as SVA's threshold parameter, often resulting in instability that requires many truncating and merging steps and increases latency, which is critical for large datasets or streaming applications as cross-validation would be needed to determine accordingly."}, {"heading": "7 Conclusion", "text": "We developed a fast online cluster and parameter estimation algorithm for Gaussian Dirichlet process mixtures that can learn in a single pass. Motivated by large sample asymptotics, we proposed a novel data-driven adaptive design for the concentration parameter and demonstrated that it leads to logarithmic growth rates in the number of classes. By experimenting with synthetic and real data sets, we show that our method performs better and is as fast as other state-of-the-art online learning methods."}, {"heading": "A Appendix A", "text": "Let us consider the general case of an unknown mean and covariance for each class. Let us specify the precision (or inverse covariance) of the matrix (or matrix) (or the inverse covariance) (or the matrix (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) of the matrix) of the matrix (or the matrix) of the matrix (or the matrix) of the matrix (or the matrix) of the matrix) (or the matrix) of the matrix (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) of the matrix) (or (or the matrix) (or the matrix) (or the matrix) (or the matrix) (or the matrix) of the matrix) (or (or the matrix) (or the matrix) (or the matrix) of the matrix) (or the matrix) (or (or the matrix) of the matrix) (or (or the matrix) of the matrix) of the matrix) of the matrix) of the matrix (or (or the matrix) of the matrix) of the number (or the matrix) of the matrix) of the matrix (or the matrix) of the matrix (or the number of the matri"}, {"heading": "B Appendix B", "text": "Now let us proceed to the calculation of (17) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "C Appendix C", "text": "Proof. It is sufficient to set the limit for lemonade \u2192 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "D Appendix D", "text": "Lemma 2. Let's leave rn and r & # 246; n for & # 252; n sequences with the updated legislation P (rn + 1 = rn + 1) = \u03c4nP (rn + 1 = rn) = \u03c4nandP (r & # 246; n + 1 = r & # 246; nP (r & # 246; n + 1 = r & # 246; n) = 1 & # 8222; \u03c3nP (r & # 246; n + 1 = r & # 8220; n & # 8220;) and assume that P (r & # 246; n > t) applies to all n & # 252; n & # 8220;."}, {"heading": "E Appendix E", "text": "We can study the generalized polya model in the slightly modified form: P (r) k (k) k (k) k (n) k (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n n n (n (n) n n (n (n) n n (n (n) n (n (n (n) n n n (n (n) n n n (n (n) n n n n (n n n n (n) n n n n n n (n (n) n n n n n n n (n n (n) n n n n n (n) n n n n n (n) n n n n n (n) n n n n (n) n) n n n n n n n n n (n) n) n n n n n n (n n n n n n) n) n n n n n (n) n n n) n n n) n) n n n n (n) n n n n) n n n n n n) n (n n) n) n n n n) n n (n n) n n n n n n n) n (n n n) n (n) n) n n n n n (n) n (n) n) n n n n (n) n n n n n) n n (n n n n n) n (n) n (n n n) n n (n n n) n (n n n n) n n n n n (n) n n n) n (n) n n (n) n) n n (n) n n n n (n (n n n) n) n n n n) n (n n n n (n n) n) n n n n"}, {"heading": "F Appendix F", "text": "Lemma 3. The following upper limit applies with constant C (\u03c6, N) = e? N logN / log\u03c6N: n \u00b2 k = N (1 + \u03c6k log k) \u2264 C (\u03c6, N) log\u03c6 nProof. On the basis of the elementary inequality protocol (1 + x) \u2264 x for x > \u2212 1 we get: log (n \u00b2 k = N (1 + \u03c6k log k) = n \u00b2 k = N log (1 + \u03c6k log k) \u2264 n \u00b2 k = N\u03c6k log k \u2264 \u03c6 (n \u00b2 x log x + 1N logN) = \u03c6 (n \u00b2 logn dt t + 1N logN) = log (log\u03c6 nlog\u03c6N) + \u03c6N logIf you take the exponential of both sides, you get the desired inequality."}], "references": [{"title": "Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems", "author": ["C.E. Antoniak"], "venue": "The Annals of Statistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1974}, {"title": "Inequalities for the Gamma Function", "author": ["N. Batir"], "venue": "Archiv der Mathematik", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Variational Inference for Dirichlet Process Mixtures", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "Bayesian Analysis", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Fast Search for Dirichlet Process Mixture Models", "author": ["H. Daume"], "venue": "Conference on Artificial Intelligence and Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Bayesian Density Estimation and Inference using Mixtures", "author": ["M.D. Escobar", "M. West"], "venue": "Journal of the American Statistical Association", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Particle Filters for Mixture Models with an Uknown Number of Components, Statistics and Computing", "author": ["P. Fearnhead"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Accelerated Variational Dirichlet Mixture Models, Advances in Neural Information", "author": ["K. Kurihara", "M. Welling", "N. Vlassis"], "venue": "Processing Systems (NIPS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Online learning of nonparametric mixture models via sequential variational approximation, Advances in Neural Information Processing Systems", "author": ["Dahua Lin"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Bayesian Mixture Modeling", "author": ["R.M. Neal"], "venue": "Proceedings of the Workshop on Maximum Entropy and Bayesian Methods of Statistical Analysis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "The infinite gaussian mixture model, Advances in Neural Information", "author": ["C.E. Rasmussen"], "venue": "Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Bayesian Gaussian Process Models: PAC- Bayesian Generalization Error Bounds and Sparse Approximations", "author": ["Matthias W. Seeger"], "venue": "Ph.D. thesis, University of Edinburgh,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "A Sequential Bayesian Inference Framework for Blind Frequency Offset Estimation", "author": ["T. Tsiligkaridis", "K.W. Forsythe"], "venue": "Proceedings of IEEE International Workshop on Machine Learning for Signal Processing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The Variational Approximation for Bayesian Inference", "author": ["D.G. Tzikas", "A.C. Likas", "N.P. Galatsanos"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Fast Bayesian Inference in Dirichlet Process Mixture Models", "author": ["L. Wang", "D.B. Dunson"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "Dirichlet process mixture models (DPMM) have been widely used for clustering data [9, 11].", "startOffset": 82, "endOffset": 89}, {"referenceID": 9, "context": "Dirichlet process mixture models (DPMM) have been widely used for clustering data [9, 11].", "startOffset": 82, "endOffset": 89}, {"referenceID": 2, "context": "Alternatives include variational methods [3], which are deterministic algorithms that convert inference to optimization.", "startOffset": 41, "endOffset": 44}, {"referenceID": 7, "context": "An online algorithm for learning DPMM\u2019s based on a sequential variational approximation (SVA) was proposed in [8], and the authors in [15] recently proposed a sequential maximum a-posterior (MAP) estimator for the class labels given streaming data.", "startOffset": 110, "endOffset": 113}, {"referenceID": 13, "context": "An online algorithm for learning DPMM\u2019s based on a sequential variational approximation (SVA) was proposed in [8], and the authors in [15] recently proposed a sequential maximum a-posterior (MAP) estimator for the class labels given streaming data.", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "The choice of concentration parameter \u03b1 is critical for DPMM\u2019s as it controls the number of clusters [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "While most fast DPMM algorithms use a fixed \u03b1 [6, 4, 7], imposing a prior distribution on \u03b1 and sampling from it provides more flexibility, but this approach still heavily relies on experimentation and prior knowledge.", "startOffset": 46, "endOffset": 55}, {"referenceID": 3, "context": "While most fast DPMM algorithms use a fixed \u03b1 [6, 4, 7], imposing a prior distribution on \u03b1 and sampling from it provides more flexibility, but this approach still heavily relies on experimentation and prior knowledge.", "startOffset": 46, "endOffset": 55}, {"referenceID": 6, "context": "While most fast DPMM algorithms use a fixed \u03b1 [6, 4, 7], imposing a prior distribution on \u03b1 and sampling from it provides more flexibility, but this approach still heavily relies on experimentation and prior knowledge.", "startOffset": 46, "endOffset": 55}, {"referenceID": 4, "context": "Thus, many fast inference methods for Dirichlet process mixture models have been proposed that can adapt \u03b1 to the data, including the works [5] where learning of \u03b1 is incorporated in the Gibbs sampling analysis, [3] where a Gamma prior is used in a conjugate manner directly in the variational inference algorithm.", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "Thus, many fast inference methods for Dirichlet process mixture models have been proposed that can adapt \u03b1 to the data, including the works [5] where learning of \u03b1 is incorporated in the Gibbs sampling analysis, [3] where a Gamma prior is used in a conjugate manner directly in the variational inference algorithm.", "startOffset": 212, "endOffset": 215}, {"referenceID": 13, "context": "[15] also account for model uncertainty on the concentration parameter \u03b1 in a Bayesian manner directly in the sequential inference procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Here, we review the SUGS framework of [15] for online clustering.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "1 Adaptation of Concentration Parameter \u03b1 It is well known that the concentration parameter \u03b1 has a strong influence on the growth of the number of classes [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 7, "context": "Choosing a fixed \u03b1 as in the online SVA algorithm of [8] requires cross-validation, which is computationally prohibitive for large-scale data sets.", "startOffset": 53, "endOffset": 56}, {"referenceID": 13, "context": "Although the parameter \u03b1 is handled from a fully Bayesian treatment in [15], a pre-specified grid of possible values \u03b1 can take, say {\u03b1l}l=1, along with the prior distribution over them, needs to be chosen in advance.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "The model (16) leads to closed-form expressions for Li,h(yi)\u2019s due to conjugacy [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 1, "context": "6 from [2], it follows that lima\u2192\u221e \u03c1d(a) e\u2212d/2(a\u22121/2)d/2 = 1.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "A similar asymptotic normality result was recently obtained in [13] for Gaussian observations with a von Mises prior.", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "1 Prune & Merge It is possible that multiple clusters are similar and classes might be created due to outliers, or due to the particular ordering of the streaming data sequence, as also noted in [8].", "startOffset": 195, "endOffset": 198}, {"referenceID": 7, "context": "Since it is possible that multiple clusters are similar and classes might be created due to outliers, or due to the particular ordering of the streaming data sequence, we add the pruning and merging step in the ASUGS algorithm as done in [8].", "startOffset": 238, "endOffset": 241}, {"referenceID": 7, "context": "We compare ASUGS and ASUGS-PM with SUGS, SUGS-PM, SVA and SVA-PM proposed in [8], since it was shown in [8] that SVA and SVA-PM outperform the block-based methods that perform iterative updates over the entire data set including Collapsed Gibbs Sampling, MCMC with Split-Merge and Truncation-Free Variational Inference.", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "We compare ASUGS and ASUGS-PM with SUGS, SUGS-PM, SVA and SVA-PM proposed in [8], since it was shown in [8] that SVA and SVA-PM outperform the block-based methods that perform iterative updates over the entire data set including Collapsed Gibbs Sampling, MCMC with Split-Merge and Truncation-Free Variational Inference.", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "Each sample is a 28\u00d728 image of a handwritten digit (total of 784 dimensions), and we perform PCA pre-processing to reduce dimensionality to d = 50 dimensions as in [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 12, "context": "This leads to closed-form expressions for Li,h(yi)\u2019s due to conjugacy [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "This quantity can be bounded using convex duality [12]:", "startOffset": 50, "endOffset": 54}], "year": 2015, "abstractText": "We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online stateof-the-art methods.", "creator": "TeX"}}}