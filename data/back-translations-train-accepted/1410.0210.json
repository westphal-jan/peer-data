{"id": "1410.0210", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2014", "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input", "abstract": "We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.", "histories": [["v1", "Wed, 1 Oct 2014 12:59:16 GMT  (927kb,D)", "http://arxiv.org/abs/1410.0210v1", null], ["v2", "Wed, 29 Oct 2014 16:29:44 GMT  (3240kb,D)", "http://arxiv.org/abs/1410.0210v2", null], ["v3", "Tue, 11 Nov 2014 12:13:18 GMT  (3240kb,D)", "http://arxiv.org/abs/1410.0210v3", "Published in NIPS 2014"], ["v4", "Tue, 5 May 2015 17:39:10 GMT  (1493kb,D)", "http://arxiv.org/abs/1410.0210v4", "Published in NIPS 2014"]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CV cs.LG", "authors": ["mateusz malinowski", "mario fritz"], "accepted": true, "id": "1410.0210"}, "pdf": {"name": "1410.0210.pdf", "metadata": {"source": "CRF", "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input", "authors": ["Mateusz Malinowski", "Mario Fritz"], "emails": ["mmalinow@mpi-inf.mpg.de", "mfritz@mpi-inf.mpg.de"], "sections": [{"heading": "1 Introduction", "text": "While most of them strive for the correct labeling of pixels, regions, or boxes in terms of semantic annotations, all predictions made by such methods are inevitably fraught with uncertainties related to the limitations of features or data, or even to the inherent ambiguity of visual input. Equally strong progress has been made on the language side, where methods have been proposed that can answer questions solely from question-answer pairs [1]. These methods operate on a set of facts associated with the system as a world. Based on this knowledge, the answer is derived by marginalizing multiple interpretations of the question."}, {"heading": "2 Related work", "text": "Semantic parsers: Our work is mainly inspired by [1] learning the semantic representation of the question based exclusively on questions and answers in natural language. Although architecture learns to assign weak supervision, it achieves results comparable to semantic parsers based on manual annotations of logical forms ([2], [3]). Unlike our work, [1] the semantic parser has never associated natural language with the perceived world. Language and Perception: Previous work [4] proposed models for the language terminology problem with the aim of linking the meaning of natural linguistic phrases with a perceived world. Both methods use images as representations of the physical world, but focus on a limited range with images consisting of very few objects."}, {"heading": "3 Method", "text": "Our method combines natural language input with output from visual scene analysis in a probable framework, as shown in Figure 1 (Q = Q). In the approach of the single world, we create a single perceived world based on segmentation. Finally, we formulate a multi-world approach that integrates across many latent worlds, so that different hypotheses are marginalized during inferencing. The uppermost part of Figure 1 outlines how we build on [1] by modelling the logical forms associated with a question as a latent variable form. W formally gives the task of predicting an answer. A question Q and a world W is performed by calculating the following posterior logical forms (semantic trees)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 DAtaset for QUestion Answering on Real-world images (DAQUAR)", "text": "Images and semantic segmentation Our new datasets for answering questions are built on top of the NYU-Depth V2 dataset [6]. NYU-Depth V2 contains 1449 RGBD images along with annotated semantic segmentations (Figure 3), in which each pixel is placed in an object class. [6] Originally, 894 classes are considered to investigate the effects of uncertainty in the visual analysis of the scenes, but we also use computer vision techniques for automatic semantic segmentation and use X, Y, Z coordinates from the depth sensor to define the spatial placement of the objects in 3D."}, {"heading": "4.2 Quantitative results", "text": "We are conducting a series of experiments to highlight specific challenges such as uncertain segmentation, unknown true logical forms, some linguistic phenomena, and the benefits of our proposed multi-world approach. In particular, we distinguish between experiments on synthetic question-and-answer pairs (SynthQA) based on templates and those collected by annotators (HumanQA), automatic scene segmentation (AutoSeg) using a computer vision algorithm [14], and human segmentation (HumanSeg) based on the Ground Truth annotations in the NYU dataset, as well as single-world (Single) and multi-world approaches (Multi)."}, {"heading": "4.2.1 Synthetic question-answer pairs (SynthQA)", "text": "Based on human segmentation (HumanSeg, 37 classes) (1st and 2nd row in Table 3), we use automatically generated questions based on the templates in Table 2 and human segmentation. We have generated 20 training pairs and 40 test-question-answer pairs per template category, a total of 140 training sessions and 280 such pairs (as exception negations type 1 and 2 have 10 training and 20 test examples each).This experiment shows how the architecture generalizes across similar question types [19].We use Tukey's Trimean 1 4 (Q1 + 2Q2 + Q3), where Qj refers to the j-th quartile as an interpretation of the average.This measurement combines the advantages of both medians (robustness to the extremes) and the empirical mean (attention to the hinges).Provided we have a human annotation of the image segments. We have also removed denials of Type 3 in the experiment, as they have proven to be particularly sophisticated computational."}, {"heading": "4.3 Human question-answer pairs (HumanQA)", "text": "Based on human segmentation, 894 classes (HumanSeg, 894 classes) (1st row in Table 4) switch to human-generated question-and-answer pairs. Increasing complexity is twofold. First, human annotations show more variation than the synthetic approach based on templates. Second, the questions are typically longer and include more spatially related objects. Figure 4 shows some examples from our data set that highlight challenges, including complex and nested spatial reference frames. In this scenario, we provide an accuracy of 7.86%. As argued above, we also evaluate human data experiments under the softer WUPS results that show different thresholds (Table 4 and Figure 5). To put these numbers into perspective, we also show performance indicators for two simple methods: predicting the most popular answer gives 4.4% accuracy, and our untrained architecture yields 0.18% and 1.3% accuracy and UPS (0.9)."}, {"heading": "4.4 Qualitative results", "text": "As our multi-world approach generates different facts about perceived worlds, we observe a trend towards a better representation of high-level concepts such as \"counting\" (far left of the figure) and linguistic associations. A significant proportion of the wrong answers are attributed to missing segments, e.g. no pillow recognition in the third example in Figure 6."}, {"heading": "5 Summary", "text": "Despite the complexity of uncertain visual perception, language comprehension, and program induction, our results point to promising progress in this direction. We are bringing together ideas from automatic scene analysis, semantic parsing, and logical thinking through a multi-world approach that explicitly takes into account uncertainty in perception. We hope that our sophisticated dataset will inspire further research in this direction. As we have advanced techniques in machine learning, computer vision, natural language processing, and deduction, it seems timely to bring these disciplines together for this open challenge."}], "references": [{"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Computational Linguistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Inducing probabilistic ccg grammars from logical form with higher-order unification", "author": ["T. Kwiatkowski", "L. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Online learning of relaxed ccg grammars for parsing to logical form", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["C. Matuszek", "N. Fitzgerald", "L. Zettlemoyer", "L. Bo", "D. Fox"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["J. Krishnamurthy", "T. Kollar"], "venue": "TACL", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["C. Matuszek", "E. Herbst", "L. Zettlemoyer", "D. Fox"], "venue": "Experimental Robotics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Interpretation of spatial language in a map navigation task. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["M. Levit", "D. Roy"], "venue": "IEEE Transactions on", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Learning to follow navigational directions", "author": ["A. Vogel", "D. Jurafsky"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S.J. Teller", "N. Roy"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Situated dialogue and spatial organization: What, where.", "author": ["G.J.M. Kruijff", "H. Zender", "P. Jensfelt", "H.I. Christensen"], "venue": "IJARS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Scalable probabilistic databases with factor graphs and mcmc. In: VLDB", "author": ["M. Wick", "A. McCallum", "G. Miklau"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Weakly supervised training of semantic parsers", "author": ["J. Krishnamurthy", "T.M. Mitchell"], "venue": "EMNLP-CoNLL", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Perceptual organization and recognition of indoor scenes from rgb-d images", "author": ["S. Gupta", "P. Arbelaez", "J. Malik"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning color names from real-world images", "author": ["J. Van De Weijer", "C. Schmid", "J. Verbeek"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Grounding spatial language in perception: an empirical and computational investigation", "author": ["T. Regier", "L.A. Carlson"], "venue": "Journal of Experimental Psychology: General", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Grounding spatial relations for human-robot interaction", "author": ["S. Guadarrama", "L. Riano", "D. Golland", "D. Gouhring", "Y. Jia", "D. Klein", "P. Abbeel", "T. Darrell"], "venue": "IROS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge university press Cambridge", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Exploratory data analysis", "author": ["J.W. Tukey"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1977}, {"title": "Fuzzy sets. Information and control", "author": ["L.A. Zadeh"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1965}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1994}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "Equally strong progress has been made on the language side, where methods have been proposed that can learn to answer questions solely from question-answer pairs [1].", "startOffset": 162, "endOffset": 165}, {"referenceID": 0, "context": "2 Related work Semantic parsers: Our work is mainly inspired by [1] that learns the semantic representation for the question answering task solely based on questions and answers in natural language.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Although the architecture learns the mapping from weak supervision, it achieves comparable results to the semantic parsers that rely on manual annotations of logical forms ([2], [3]).", "startOffset": 173, "endOffset": 176}, {"referenceID": 2, "context": "Although the architecture learns the mapping from weak supervision, it achieves comparable results to the semantic parsers that rely on manual annotations of logical forms ([2], [3]).", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "In contrast to our work, [1] has never used the semantic parser to connect the natural language to the perceived world.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "Language and perception: Previous work [4, 5] has proposed models for the language grounding problem with the goal of connecting the meaning of the natural language sentences to a perceived world.", "startOffset": 39, "endOffset": 45}, {"referenceID": 4, "context": "Language and perception: Previous work [4, 5] has proposed models for the language grounding problem with the goal of connecting the meaning of the natural language sentences to a perceived world.", "startOffset": 39, "endOffset": 45}, {"referenceID": 4, "context": "For instance [5] considers only two mugs, monitor and table in their dataset, whereas [4] examines objects such as blocks, plastic food, and building bricks.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "For instance [5] considers only two mugs, monitor and table in their dataset, whereas [4] examines objects such as blocks, plastic food, and building bricks.", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "In contrast, our work focuses on a diverse collection of real-world indoor 3d images [6] - with many more objects in the scene and more complex spatial relationship between them.", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "Moreover, our paper considers complex questions - beyond the scope of [4] and [5] - and reason across different images using only textual question-answer pairs for training.", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "Moreover, our paper considers complex questions - beyond the scope of [4] and [5] - and reason across different images using only textual question-answer pairs for training.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Integrated systems that execute commands: Others [7, 8, 9, 10, 11] focus on the task of learning the representation of natural language in the restricted setting of executing commands.", "startOffset": 49, "endOffset": 66}, {"referenceID": 7, "context": "Integrated systems that execute commands: Others [7, 8, 9, 10, 11] focus on the task of learning the representation of natural language in the restricted setting of executing commands.", "startOffset": 49, "endOffset": 66}, {"referenceID": 8, "context": "Integrated systems that execute commands: Others [7, 8, 9, 10, 11] focus on the task of learning the representation of natural language in the restricted setting of executing commands.", "startOffset": 49, "endOffset": 66}, {"referenceID": 9, "context": "Integrated systems that execute commands: Others [7, 8, 9, 10, 11] focus on the task of learning the representation of natural language in the restricted setting of executing commands.", "startOffset": 49, "endOffset": 66}, {"referenceID": 10, "context": "Integrated systems that execute commands: Others [7, 8, 9, 10, 11] focus on the task of learning the representation of natural language in the restricted setting of executing commands.", "startOffset": 49, "endOffset": 66}, {"referenceID": 11, "context": "Probabilistic databases: Similarly to [12] that reduces Named Entity Recognition problem into the inference problem from probabilistic database, we sample multiple-worlds based on the uncertainty introduced by the semantic segmentation algorithm that we apply to the visual input.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "Single-world approach for question answering problem We build on recent progress on endto-end question answering systems that are solely trained on question-answer pairs (Q,A) [1, 13].", "startOffset": 176, "endOffset": 183}, {"referenceID": 12, "context": "Single-world approach for question answering problem We build on recent progress on endto-end question answering systems that are solely trained on question-answer pairs (Q,A) [1, 13].", "startOffset": 176, "endOffset": 183}, {"referenceID": 0, "context": "Top part of Figure 1 outlines how we build on [1] by modeling the logical forms associated with a question as latent variable T given a single world W .", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "More formally the task of predicting an answer A given a question Q and a world W is performed by computing the following posterior which marginalizes over the latent logical forms (semantic trees in [1]) T : P (A|Q,W) := \u2211 T P (A|T ,W)P (T |Q).", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": "Following [1] we use DCS Trees that yield the following recursive evaluation function \u03c3W : \u03c3W(T ) := \u22c2d j {v : v \u2208 \u03c3W(p), t \u2208 \u03c3W(Tj), Rj(v, t)} where T := \u3008p, (T1,R1), (T2,R2), .", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "We use the same feature templates as [1]: string triggers a predicate, string is under a relation, string is under a trace predicate, two predicates are linked via relation and a predicate has a child.", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "For a detailed exposition, we refer the reader to [1].", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "Question answering on real-world images based on a perceived world Similar to [5], we extend the work of [1] to operate now on what we call perceived world W .", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "Question answering on real-world images based on a perceived world Similar to [5], we extend the work of [1] to operate now on what we call perceived world W .", "startOffset": 105, "endOffset": 108}, {"referenceID": 13, "context": "Therefore, we build this world by running a state-of-the-art semantic segmentation algorithm [14] over the images, collecting the recognized information about objects such as object class, 3D position, and color (Figure 1 - middle part).", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "}, instance id is the object\u2019s id, image id is id of the image containing the object, color is estimated color of the object [15], and spatial loc is the object\u2019s position in the image.", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "Note that the X,Y, Z coordinate system is aligned with direction of gravity [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "[16] and [17]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[16] and [17]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "We now draw on ideas from probabilistic databases [12] and propose a multi-world approach as outlined in the lower part of Figure 1 that takes the uncertainty in the segmentation of the visual input into account by marginalizing over multiple possible worlds W derived from the segmentation S.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "IDF to measure similarity [18].", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "1 DAtaset for QUestion Answering on Real-world images (DAQUAR) Images and Semantic Segmentation Our new dataset for question answering is built on top of the NYU-Depth V2 dataset [6].", "startOffset": 179, "endOffset": 182}, {"referenceID": 13, "context": "According to [14], we preprocess the data to obtain canonical views of the scenes and use X , Y , Z coordinates from the depth sensor to define spatial placement of the objects in 3D.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "We use a state-of-the-art scene analysis method [14] which maps every pixel into 40 classes: 37 informative object classes as well as \u2019other structure\u2019, \u2019other furniture\u2019 and \u2019other prop\u2019.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "We use the same data split as [14]: 795 training and 654 test images.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "They were instructed to give valid answers that are either basic colors [15], numbers or objects (894 categories) or sets of those.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "the ground truth (correct/wrong), we propose, an inspired from the work on Fuzzy Sets [20], a soft measure based on the WUP score [21], which we call WUPS (WUP Set) score.", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "the ground truth (correct/wrong), we propose, an inspired from the work on Fuzzy Sets [20], a soft measure based on the WUP score [21], which we call WUPS (WUP Set) score.", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "4, and a set membership measure \u03bc, with properties \u03bc(x \u2208 X) = 1 if x \u2208 X , \u03bc(x \u2208 X) = maxy\u2208X \u03bc(x = y) and \u03bc(x = y) \u2208 [0, 1], in Eq.", "startOffset": 117, "endOffset": 123}, {"referenceID": 20, "context": "For \u03bc we use a variant of Wu-Palmer similarity [21, 22].", "startOffset": 47, "endOffset": 55}, {"referenceID": 21, "context": "For \u03bc we use a variant of Wu-Palmer similarity [21, 22].", "startOffset": 47, "endOffset": 55}, {"referenceID": 22, "context": "WUP(a, b) calculates similarity based on the depth of two words a and b in the taxonomy[23, 24], and define the WUPS score: WUPS(A, T ) = 1 N N \u2211", "startOffset": 87, "endOffset": 95}, {"referenceID": 13, "context": "In particular, we distinguish between experiments on synthetic question-answer pairs (synthQA) based on templates and those collected by annotators (HumanQA), automatic scene segmentation (AutoSeg) with a computer vision algorithm [14] and human segmentations (HumanSeg) based on the ground-truth annotations in the NYU dataset as well as single world (single) and multi-world (multi) approaches.", "startOffset": 231, "endOffset": 235}, {"referenceID": 18, "context": "This experiment shows how the architecture generalizes across similar type of questions We use Tukey\u2019s trimean 1 4 (Q1+2Q2+Q3), whereQj denotes the j-th quartile, as the interpretation of the average [19].", "startOffset": 200, "endOffset": 204}, {"referenceID": 13, "context": "Based on automatic segmentations (AutoSeg, 37 classes, single) (3rd row in Table 3) tests the architecture based on uncertain facts obtained from automatic semantic segmentation [14] where the most likely object labels are used to great a single world.", "startOffset": 178, "endOffset": 182}, {"referenceID": 14, "context": "They are instructed to answer with a number, basic colors [15], or objects (from 37 or 894 categories) or set of those.", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multiworld approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.", "creator": "LaTeX with hyperref package"}}}