{"id": "1410.1141", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2014", "title": "On the Computational Efficiency of Training Neural Networks", "abstract": "It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.", "histories": [["v1", "Sun, 5 Oct 2014 10:54:07 GMT  (45kb,D)", "http://arxiv.org/abs/1410.1141v1", null], ["v2", "Tue, 28 Oct 2014 19:14:37 GMT  (45kb,D)", "http://arxiv.org/abs/1410.1141v2", "Section 2 is revised due to a mistake"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["roi livni", "shai shalev-shwartz", "ohad shamir"], "accepted": true, "id": "1410.1141"}, "pdf": {"name": "1410.1141.pdf", "metadata": {"source": "CRF", "title": "On the Computational Efficiency of Training Neural Networks", "authors": ["Roi Livni", "Shai Shalev-Shwartz"], "emails": ["roi.livni@mail.huji.ac.il", "shais@cs.huji.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "USA, it is not only a question of time, but also of time in which people in the USA, in Europe, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the"}, {"heading": "2 Sufficiently Over-Specified Networks Have No Local Minima", "text": "This is an indication that large neural networks are indeed more susceptible to gradient-based methods used in practice. To present the formal result, it must be noted that the matrix of m-bearing examples is in Rd. We can imagine the network as consisting of two portfolios, the first maps X into a matrix Z-Rn, m, which classifies the number of neurons as insufficient."}, {"heading": "3 The Hardness of Learning Neural Networks", "text": "For simplicity: In most areas of this section, we focus on the PAC model in the case of binary classification, via the Boolean cube, in the feasible case, and with a fixed target accuracy. 2Fix some, from the context, one considers the input space as Xd = {0, 1} d and leaves H a hypothesis class of functions from Xd to {\u00b1 1}. We often ignore the subscript d when it is clear. A learning algorithm A has access to an oracle that samples x according to an unknown distribution D over X and returns (x, f) in which there is an unknown target hypothesis in H. The goal of the algorithm is to return a classification f: X \u2192 {\u00b1 1}, so that it is with a probability of at least 1 \u2212 2."}, {"heading": "4 Polynomial Networks", "text": "In the previous section, we have shown several strong negative results for learning neural networks with threshold, sigmoidal, and ReLU activation functions. One way to bypass these activation functions is to consider another activation function. Perhaps, the simplest nonlinear function is the square function, \u03c32 (x) = x2. We refer to networks that use this activation function because they calculate the polynmic functions of their input. As in the previous section, we refer to Nt, n, \u03c32, L as the class of functions that can be implemented with a neural network of depth t, size n, square activation function, and a bound L on the \"1 standard of input weights of each neuron. If we do not specify L, we refer to polynmic networks with unlimited weights. In the following, we will examine the expressivity and computational complexity of polynomial networks. We point out that algorithms for efficient learning (value), however, we will examine redistributions with unlimited weights."}, {"heading": "4.1 Expressiveness", "text": "Theorem 4 (Polynomial networks can express Turing machines): Let us leave Fd and T as in Thm. 1. Then there are constants b, c, c, R + in such a way that for each d the class Nt, n, \u03c32, L with t = c T (d) log (T (d)) + b, n = t2 and L = b Fd.The proof for the theorem is based on the result of [18] and is given in the appendix. Another relevant expression result, which we will use later, shows that polynomial networks with sigmoidal activation functions can approach: Theorem 5. Fix 0 < 1, L \u2265 3 and t. There is Bt, < 0 < N < N < Bt, L < n, L < n, L < n, which are based on a function based on example."}, {"heading": "4.2 Training Time", "text": "Turning now to the computational complexity of learning polynomial networks, we first show that it is difficult to learn polynomial networks of depth. By combining Thm. 5 and Corollary 2 we obtain the following: Corollars 3. Class Nt, n, \u03c32, where t = 0 (log (d)) and n = 0 (d) are not efficiently learned. On the other hand, polynomial networks with constant depth can be learned in polynomial time, using a simple linearization trick. Specifically, the class of polynomial networks of constant depth t is included in the class of multivariate polynomial polynomials of the total degree s = 2t. This class can be represented as dimensional linear space in which each vector is the coefficient vector of some such polynomial networks."}, {"heading": "4.3 Learning 2-layer and 3-layer Polynomial Networks", "text": "Although such networks can be learned in polynomial time by explicit linearization (as described in Section 4.2), the runtime and resulting network size is square (for Depth-2) or cubic (for Depth-3) with the data dimension. In contrast, our algorithms and guarantees have a much milder dependence on.We will first consider the 2 layers of polynomial networks, the following form: P2, k = {x 7 \u2192 b > 0 x + k = 1 (w > i x)."}, {"heading": "5 Experiments", "text": "To demonstrate the practicality of GECO to train neural networks for real-world problems, we considered a pedestrian recognition problem as follows: We collected 200k training examples for 88x40 pixel image fields containing either pedestrian (positive examples) or hard negative examples (containing images classified as pedestrians by using a simple linear classifier in a sliding window manner). See some examples of images above. We used half of the examples as a breath training set and the other half as a test set. We calculated the HoG properties ([11]) from the images. We then trained using GECO, a polynomial network with depth-2 on the resulting features. We used 40 neurons in the hidden layer. For comparison, we trained the same network architecture (i.e. 40 hidden neurons with a squared activation function). We trained a neural network (also 40 neurons with a hidden activation function)."}, {"heading": "A Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Thm. 2", "text": "Suppose that (W, V) is not a global minimum."}, {"heading": "A.2 Proof of Corollary 2", "text": "A.2.1 Hardness result for class N2, n, \u03c3sig, L: Consider Ha as defined in Thm. 3. Note that for each h-Ha integral w and b are such that h (x) = w > x \u2212 b \u2212 12 and that we have the | h (x) | \u2265 1 / 2. Given k hyperplanes {hi} k = 1 Consider the neuronsgi (x) = 1 / (1 + exp (\u2212 Chi (x))))), where C (1) must be chosen later. Letter (x) = Cd2k + 13 (k) i = 1 gi (x) \u2212 k + 1 3). If you have the g (x) \u2212 b + 12 | \u2264 dwe have the g (x)."}, {"heading": "A.3 Proof of Thm. 4", "text": "We start by showing that we can implement AND, OR, NEG, Id gates by using polynomial networks of fixed depth and size. As a logical consequence, we can implement circuits with fixed number of fan-ins. NEG can be implemented with x 7 \u2192 1 \u2212 x and Id can be implemented with x 7 \u2192 14 (((x + 1) 2 \u2212 (x \u2212 1) 2. Next remark: AND (x1, x2) = x1 \u00b7 x2, and OR (x1, x2) = x1 + x2 \u2212 AND (x1, x2). And that x1 \u00b7 x2 = 14 (((x2 + x1) 2 \u2212 (x2 \u2212 x1) 2). Thus, we can implement with two layers a connection and disjunction of 2 neurons. By adding a fixed number of layers, we can also implement the connection and disjunction of any number of neurons."}, {"heading": "A.4 Proof of Thm. 5", "text": "The idea of the proof for Thm. 5 is as follows: First, we show that we can express any T-degree polynomial with O (log T) layers and O (T) degree neurons (log). The result follows by replacing each sigmoid activation unit with additional layers similar to the sigmoidal function on the output of the previous layer. We will first prove the two lemmas. The proof for Thm. 5 will then be at the end of the section.Lemma 1. The following statements apply: 1. If g-Nt, n, 2, L for some L layers then g-Nt, n + 2 (t layer) that we can prove the two lemmas. The proof for Thm. 5 is then given at the end of the section.Lemma 1."}, {"heading": "A.4.1 Back to proof of Thm. 5", "text": "SetT = log (2L4 + exp (7L \u2212 sig) ln (((4L) t + 3)) + 2 log 8 (4L) t \u2212 1 and haveBt = 1 + log T \u2212 0 (logL log Lt), (5) Bn = 1 + 2T (2 log T + log T). (6) We prove the statement by induction to t, our induction hypothesis will apply to networks with not necessarily a single output neuron. For t = 1, since N1, n, 2 x, \u03c32 = N1, n, p, the statement is trivial. (Next, let us leave F-Nt, n, \u03c3sig, L, suppose F: Rd \u2192 Rs (i.e. the output layer has s nodes). There is a target function F (t \u2212 1)."}, {"heading": "A.5 Proof of Thm. 8", "text": "& f & # 8220; f & # 8222; f & # 8220; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 8222; f & # 8220; f & # 8220; f & # 8220; f & # 8220; f & # 8220; f & # 8220; f & # 8220; f & # 8220; f & # 8220; f; f & # 8220; f & # 8220; f; f & # 8220; f; f & # 8220; f & # 8220; f # 8220; f; f & # 8220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f & # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 2220; f # 220; f & # 220; f # 220; f # 220; f # 220; f # 220; f & # 220; f # 220; f # 220; f & # 220; f # 220; f # 220; f # 220; f # 220; f # 2220; f # 220; f # 220; f # 2220; f # 220; f # 220; f # 220; f # 2220; f # 2220; f # 2220; f # 220; f # 220; f # 220; f # 220; f # 8220; f # 220; f # 8220; f # 220; f # 8220; f # 2220; f # 8220; f # 8220; f # 2220; f # 220; f # 8220; f # 8220; f # 2220; f # 2220; f # 8220; f # 2220; f # 8220; f # 220; f; f # 220; f #"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "It is well-known that neural networks are computationally hard to train. On the<lb>other hand, in practice, modern day neural networks are trained efficiently using SGD<lb>and a variety of tricks that include different activation functions (e.g. ReLU), over-<lb>specification (i.e., train networks which are larger than needed), and regularization. In<lb>this paper we revisit the computational complexity of training neural networks from a<lb>modern perspective. We provide both positive and negative results, some of them yield<lb>new provably efficient and practical algorithms for training certain types of neural net-<lb>works.", "creator": "LaTeX with hyperref package"}}}