{"id": "1410.1165", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2014", "title": "Understanding Locally Competitive Networks", "abstract": "Recently proposed neural network activation functions such as rectified linear, maxout, and local winner-take-all have allowed for faster and more effective training of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a unified explanation for the beneficial properties of such networks. We also show how our insights can be directly useful for efficiently performing retrieval over large datasets using neural networks.", "histories": [["v1", "Sun, 5 Oct 2014 14:46:47 GMT  (5697kb,D)", "http://arxiv.org/abs/1410.1165v1", "11 pages, 8 figures"], ["v2", "Mon, 22 Dec 2014 20:07:17 GMT  (5796kb,D)", "http://arxiv.org/abs/1410.1165v2", "9 pages + 2 supplementary, Under review as a conference paper at ICLR 2015"], ["v3", "Thu, 9 Apr 2015 01:22:49 GMT  (5725kb,D)", "http://arxiv.org/abs/1410.1165v3", "9 pages + 2 supplementary, Accepted to ICLR 2015 Conference track"]], "COMMENTS": "11 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["rupesh kumar srivastava", "jonathan masci", "faustino gomez", "j\\\"urgen schmidhuber"], "accepted": true, "id": "1410.1165"}, "pdf": {"name": "1410.1165.pdf", "metadata": {"source": "CRF", "title": "Understanding Locally Competitive Networks", "authors": ["Rupesh Kumar Srivastava", "Jonathan Masci", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "emails": ["juergen}@idsia.ch"], "sections": [{"heading": null, "text": "A version of this paper was submitted to NIPS 2014 on 06-06-2014."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 Locally Competitive Neural Networks", "text": "This means that the local competition between the units in the network determines which parts of the network are activated or trained for a particular input example. For each unit, the total input or pre-synaptic activation z is first calculated as z = wx + b, where x is the vector of the inputs into the unit, w is a traceable weight vector, and b is a traceable bias. For the rectified linear function, the output or post-synaptic activation of each unit is simply max. (z, 0), which can be interpreted as a competition with a fixed value of 0. Units in a layer are divided into blocks of a fixed size. Then, the output of each unit is used as an indicator, which is 1 if the unit has the maximum in its group and is 0."}, {"heading": "3 Subnetwork Analysis", "text": "This section examines how the model of models implemented in the context of a local competition organizes itself based on the training. To visualize the organization of subnets as a result of the training, they are encoded as bit strings, called submasks. For the input pattern i, the submask si-0, 1} u, where u is the number of units in the entire network, represents the corresponding subnetwork by having a 0 in position j, j = 1.. u if the corresponding unit is not activated, and otherwise 1. The submasks encode each subnetwork uniquely and compactly in a format that can be analyzed by clusters, and, as we show in Section 4.2, enable efficient data recovery. Following, the subnets that arise during the training are first visualized using the t-SNE [13] algorithm. This dimensionality reduction technique allows a good visualization of the relationship between submasks for several examples in a data set, which is most interesting later in this section, visualizing the local structure."}, {"heading": "3.1 Visualization through Dimensionality Reduction", "text": "In this sense, it is important that we see ourselves able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "3.2 Behavior during Training", "text": "In order to measure how the subnets develop during training, the partial masks of each sample were recorded in the training set for each epoch. Figure 4 characterizes the change in the subnets over time by counting the number of input patterns for which one unit changes from one epoch to the other or vice versa. Figure 4 shows the fraction of patterns for which a cross-epoch change has occurred, averaged across all units in the network. Higher values indicate that the assignment of subnets to patterns is not stable. Batch size for this experiment was 100, which means that each transition over the training set consists of 500 weight updates. In the case shown, the average fraction of flips starts at 0.2, but quickly drops below 0.05 and falls further in the training, indicating that the assignment of the subnets to individual examples in this case stabilizes quickly."}, {"heading": "3.3 Classification/Retrieval using Submasks", "text": "Since the visualization of submasks for the test set has task-relevant structures, it is natural to ask: How well can the submask represent the data they have generated? If the submasks are similar for similar examples, perhaps they can be used as data descriptors for tasks such as similarity-based retrieval. Sparse binary codes allow efficient storage and recovery for large and complex data sets, which make learning to generate them an active area of research [15, 16, 17]. This would make representative submasks very attractive, since no explicit training would be required for retrieval to generate them. To assess whether examples that generate similar binary codes are actually similar, we train locally competing networks for classification and use a simple k nearest neighbor (kNN) algorithm to classify data using the generated submasks. This approach is an easy way to examine the information contained in the submasks (without actually activating them)."}, {"heading": "3.4 Effect of Dropout", "text": "The dropout [10] regulation technique has proven to be very useful and efficient in improving the generalization of large models and is often used in combination with locally competing activation functions [2, 4, 5]. We found that networks that were trained with dropout (and thus produced lower errors in the test set) also yielded better submasks in terms of kNN classification performance. To observe the effects of the dropout more closely, we trained a 3-layer network with 800 ReLUs in each hidden layer without dropout at MNIST from 5 different initializations until the validation set did not improve. The networks were then trained again by the same initialization with dropout until the validation error coincided with the non-dropout case or fell below the lowest validation error. In both cases, the minibatch gradient descent with dynamics was used to stop the networks from comparing the dropout organization when the cases of non-dropout classification networks are not classified (when a similar case of non-dropout networks is achieved)."}, {"heading": "4 Experimental Results", "text": "The following experiments apply the methods described in the previous section to more difficult benchmark problems: CIFAR-10, CIFAR-100, and ImageNet. For the CIFAR experiments, we used the models described in [2] because they use locally competitive activations (maxout), are trained with dropouts, and have good hyperparameter settings available to them [18]. We report on the classification error of the test set obtained with the Softmax initial layer, as well as on the kNN classification of the activation and submasks of the penultimate layer unit. The best value of k is achieved with a validation set, although we found that k = 5 generally worked well with distance weighting."}, {"heading": "4.1 CIFAR-10 & CIFAR-100", "text": "CIFAR-10 is a data set of 32 x 32 color images of 10 classes, divided into a training set of 50k and a test set of 10k (6k images per class) [19]. CIFAR-100 is a similar data set of color images, but with 100 classes and 600 images per class, making it more difficult. Results of these data sets are summarized in Table 2. Models from [2] for these data sets use pre-processing using global contrast normalization and ZCA brightening, as well as data augmentation using translational and horizontal reflections. We note that when comparing the closest classification performance with submasks to activation values, we lose an accuracy of 1.4% on the CIFAR-10 data set and 1.75% on the CIFAR-100 data set. This indicates a good level of subnetwork organization by different classes. Figure 5a shows the 2-D test visualization of the CIFAR subset for the subset of the CAR-10 data set are half."}, {"heading": "4.2 ImageNet", "text": "The results of the study show that most of them are not self-portrayal, but self-portrayal."}, {"heading": "5 Discussion", "text": "A system of many networks on a data set specializing in solving simpler tasks can be quite difficult without combining them into a single network with locally competing entities. Without such a local competition, one must have a global gating mechanism, as in [12]. The training algorithm and objective function must also find modifications that promote competition between networks. On the other hand, a locally competing neural network can behave like a model consisting of many subnets, allowing massive transmission of parameters between subnets. Stochastic descent can be used to minimize the desired loss, and the implementation is so simple that one does not even recognize that a model of models is being trained. Figure 4 suggests that the subnets are organized during an early transient phase in such a way that subnets that respond to similar examples."}, {"heading": "Acknowledgments", "text": "This research was funded by the EU projects WAY (FP7-ICT-288551) and NASCENCE (FP7ICT-317662). We thank Jan Koutn\u00edk and Klaus Greff for their helpful comments."}, {"heading": "A Extra visualizations", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Deep sparse rectifier networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Compete to compute", "author": ["Rupesh K. Srivastava", "Jonathan Masci", "Sohrob Kazerounian", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "On rectified linear units for speech processing", "author": ["Matthew D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["Ian J. Goodfellow", "Mehdi Mirza", "Xiao Da", "Aaron Courville", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "On the number of response regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "[cs],", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A local learning algorithm for dynamic feedforward and recurrent networks", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Connection Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Self-delimiting neural networks", "author": ["J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1210.0118,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "venue": "[cs],", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "The dropout learning algorithm", "author": ["Pierre Baldi", "Peter Sadowski"], "venue": "Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Adaptive mixtures of local experts", "author": ["Robert A. Jacobs", "Michael I. Jordan", "Steven J. Nowlan", "Geoffrey E. Hinton"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1991}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Learning binary codes for high-dimensional data using bilinear projections", "author": ["Yunchao Gong", "Sanjiv Kumar", "Henry A. Rowley", "Svetlana Lazebnik"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Sparse similarity-preserving hashing", "author": ["Jonathan Masci", "Alex M. Bronstein", "Michael M. Bronstein", "Pablo Sprechmann", "Guillermo Sapiro"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Learning binary hash codes for large-scale image search. In Machine Learning for Computer Vision, page 49\u201387", "author": ["Kristen Grauman", "Rob Fergus"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Pylearn2: a machine learning research library", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "ImageNet large scale visual recognition competition", "author": ["Jia Deng", "Alex Berg", "Sanjeev Satheesh", "Su Hao", "Aditya Khosla", "Fei-Fei Li"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "DeCAF: a deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "[cs],", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "CloudCV: Large-Scale distributed computer vision as a cloud service", "author": ["D. Batra", "H. Agrawal", "P. Banik", "N. Chavali", "A. Alfadda"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "The manifold tangent classifier", "author": ["Salah Rifai", "Yann Dauphin", "Pascal Vincent", "Yoshua Bengio", "Xavier Muller"], "venue": "Neural Information Processing Systems, page 2294\u20132302,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Recently proposed activation functions for neural networks such as rectified linear (ReL;[1]), maxout [2] and LWTA [3] are quite unlike sigmoidal activation functions.", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Recently proposed activation functions for neural networks such as rectified linear (ReL;[1]), maxout [2] and LWTA [3] are quite unlike sigmoidal activation functions.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "Nevertheless, many researchers have found that such networks can be trained faster and better than sigmoidal networks, and they are increasingly in use for learning from large and complex datasets [4, 5].", "startOffset": 197, "endOffset": 203}, {"referenceID": 3, "context": "Nevertheless, many researchers have found that such networks can be trained faster and better than sigmoidal networks, and they are increasingly in use for learning from large and complex datasets [4, 5].", "startOffset": 197, "endOffset": 203}, {"referenceID": 0, "context": "Past research has shown observational evidence that such networks have beneficial properties such as not requiring unsupervised training for weight initialization [1], better gradient flow [2] and mitigation of catastrophic forgetting [3, 6].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "Past research has shown observational evidence that such networks have beneficial properties such as not requiring unsupervised training for weight initialization [1], better gradient flow [2] and mitigation of catastrophic forgetting [3, 6].", "startOffset": 235, "endOffset": 241}, {"referenceID": 4, "context": "Past research has shown observational evidence that such networks have beneficial properties such as not requiring unsupervised training for weight initialization [1], better gradient flow [2] and mitigation of catastrophic forgetting [3, 6].", "startOffset": 235, "endOffset": 241}, {"referenceID": 5, "context": "Recently, the expressive power of deep networks with such functions has been theoretically analyzed [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "Related activation techniques have been studied in the past decades, including recurrent networks with locally competitive units [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "Selfdelimiting recurrent networks with competitive units [3, 9] can in principle learn to decide their own run time and effective number of parameters, thus learning their own computable regularizers.", "startOffset": 57, "endOffset": 63}, {"referenceID": 7, "context": "Selfdelimiting recurrent networks with competitive units [3, 9] can in principle learn to decide their own run time and effective number of parameters, thus learning their own computable regularizers.", "startOffset": 57, "endOffset": 63}, {"referenceID": 8, "context": "In this paper, we restrict our analysis to networks trained with gradient-based algorithms which are often trained with the dropout regularization technique [10, 11] for improved generalization.", "startOffset": 157, "endOffset": 165}, {"referenceID": 9, "context": "In this paper, we restrict our analysis to networks trained with gradient-based algorithms which are often trained with the dropout regularization technique [10, 11] for improved generalization.", "startOffset": 157, "endOffset": 165}, {"referenceID": 9, "context": "Note that this model of models interpretation is different from the model averaging perspective of dropout [11] which posits that using dropout is equivalent to averaging the predictions of many subnetworks on the same data.", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "Our interpretation is related to mixtures of local experts [12], where a gating network was trained to select one of many subnetworks or modules in a system.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "In what follows, the subnetworks that emerge during training are first visualized using the t-SNE [13] algorithm.", "startOffset": 98, "endOffset": 102}, {"referenceID": 12, "context": "All experiments in this section are performed on the MNIST dataset [14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "Indeed, in a limited qualitative analysis, it has been shown previously [3] that in trained LWTA nets there are more units in common between subnetworks for examples of the same class than those for different class examples.", "startOffset": 72, "endOffset": 75}, {"referenceID": 13, "context": "Sparse binary codes enable efficient storage and retrieval for large and complex datasets due to which learning to produce them is an active research area [15, 16, 17].", "startOffset": 155, "endOffset": 167}, {"referenceID": 14, "context": "Sparse binary codes enable efficient storage and retrieval for large and complex datasets due to which learning to produce them is an active research area [15, 16, 17].", "startOffset": 155, "endOffset": 167}, {"referenceID": 15, "context": "Sparse binary codes enable efficient storage and retrieval for large and complex datasets due to which learning to produce them is an active research area [15, 16, 17].", "startOffset": 155, "endOffset": 167}, {"referenceID": 8, "context": "The dropout [10] regularization technique has proven to be very useful and efficient at improving generalization for large models, and is often used in combination with locally competitive activation functions [2, 4, 5].", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "The dropout [10] regularization technique has proven to be very useful and efficient at improving generalization for large models, and is often used in combination with locally competitive activation functions [2, 4, 5].", "startOffset": 210, "endOffset": 219}, {"referenceID": 3, "context": "The dropout [10] regularization technique has proven to be very useful and efficient at improving generalization for large models, and is often used in combination with locally competitive activation functions [2, 4, 5].", "startOffset": 210, "endOffset": 219}, {"referenceID": 8, "context": "This supports the interpretation of dropout as a regularization technique which prevents \u201cco-adaptation of feature detectors\u201d (units) [10], leading to better representation of data by the subnetworks.", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "For the CIFAR experiments, we used the models described in [2] since they use locally competitive activations (maxout), are trained with dropout, and good hyperparameter settings for them are available [18].", "startOffset": 202, "endOffset": 206}, {"referenceID": 17, "context": "CIFAR-10 is a dataset of 32\u00d732 color images of 10 classes split into a training set of size 50k and testing set of size 10k (6k images per class) [19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "convolutional network trained on the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC-2012) [20] dataset is evaluated.", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "An implementation of the network in [4], with some differences [21], is available publicly.", "startOffset": 36, "endOffset": 39}, {"referenceID": 19, "context": "An implementation of the network in [4], with some differences [21], is available publicly.", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "For the experiments in this section, the penultimate-layer activations obtained using this model were downloaded from CloudCV [22].", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "In [4], it was first shown that the activations from the penultimate layer of the deep network can be used to retrieve similar images.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "This improves the network\u2019s error by about 1-2% [4].", "startOffset": 48, "endOffset": 51}, {"referenceID": 10, "context": "Without such local competition, one needs to have a global gating mechanism as in [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "of the data manifold hypothesis for classification [23].", "startOffset": 51, "endOffset": 55}], "year": 2017, "abstractText": "Recently proposed neural network activation functions such as rectified linear, maxout, and local winner-take-all have allowed for faster and more effective training of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a unified explanation for the beneficial properties of such networks. We also show how our insights can be directly useful for efficiently performing retrieval over large datasets using neural networks. A version of this paper was submitted to NIPS 2014 on 06-06-2014", "creator": "LaTeX with hyperref package"}}}