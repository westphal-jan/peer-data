{"id": "1410.1462", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2014", "title": "Top Rank Optimization in Linear Time", "abstract": "Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the ranking loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.", "histories": [["v1", "Mon, 6 Oct 2014 17:10:23 GMT  (101kb,D)", "http://arxiv.org/abs/1410.1462v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.IR", "authors": ["nan li", "rong jin", "zhi-hua zhou"], "accepted": true, "id": "1410.1462"}, "pdf": {"name": "1410.1462.pdf", "metadata": {"source": "CRF", "title": "Top Rank Optimization in Linear Time", "authors": ["Nan Li", "Rong Jin", "Zhi-Hua Zhou"], "emails": ["zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "Most existing approaches aim to either optimize task-specific indicators or increase ranking loss by placing more emphasis on the error associated with the best placed instances, resulting in a high level of computational effort that is superlinear in the number of training instances. We propose a highly efficient approach called TopPush to optimize accuracy at the top, where computational complexity is linear in the number of training instances. We present a novel analysis that limits the generalization error for the best placed instances of the proposed approach. Empirical studies show that the proposed approach is highly competitive compared to the state of the art and is 10-100 times faster. Keywords: two-sided ranking, accuracy at the top, linear computational complexity, convex conjugation, dual problem, Neterov's method."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Bipartite Ranking: AUC vs Accuracy at the Top", "text": "Let us define X = {x-Rd = 1} as the instance space. Let S = S + C = 1 = 1 be a set of training examples in which S + = x + i-Rank = 1 and S \u2212 i-Rank ni = 1 are a set of positive instances and n negative instances independently of each other from distributions P + and P \u2212, respectively. The goal of the two-tier ranking is to learn a ranking function f: X 7 \u2192 R that probably places a positive instance ahead of most negative instances. In the literature, the two-tier ranking has found applications in many areas, and its theoretical properties have been studied by several studies [for example 2, 7, 28].AUC is a commonly used evaluation metric for the two-tier ranking. [16, 10] By researching its equivalence to Wilcoxon-Mann statistics [16], we have developed many ranking algorithms to optimize AUC by minimizing the loss."}, {"heading": "3. TopPush for Optimizing Top Accuracy", "text": "In this section, we first present a learning algorithm to minimize the loss function in (4), and then the computational complexity and performance guarantee for the proposed algorithm."}, {"heading": "3.1. Dual Formulation", "text": "We view the linear ranking function as f (x) = w > x, where w (\u03b2) = \u03b2 (\u03b2) is the weight vector to be learned. For the non-linear ranking function, we can use kernel methods, and Nystro (m) methods and random Fourier functions can turn the kernel problem into a linear one, see [43] for further discussions on this topic. As a result, the learning problem \"p \u00b2 p \u00b2 p\" p \u00b2 p \"p \u00b2 p\" p \u00b2 p \"p \u00b2 p\" p \u00b2 p \"p \u00b2 p\" p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \u00b2 p \"p \u00b2 p\" p \u00b2 p \u00b2 p \"p \u00b2 p\" p \u00b2 p \u00b2 p \"p \u00b2 p\" p \u00b2 p \"p \u00b2 p\" p \u00b2 p \"p \u00b2 p\" p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \u00b2 p \"p \u00b2 p \u00b2 p \u00b2 p\" p \u00b2 p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \u00b2 p \u00b2 p \"p \u00b2 p\" p \u00b2 p \"p \u00b2 p \u00b2 p\" p \u00b2 p \"p \u00b2 p \u00b2 p\" p \"p \u00b2 p\" p \"p \u00b2 p\" p \u00b2 p \"p\" p \u00b2 p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \""}, {"heading": "3.2. Linear Time Bipartite Ranking Algorithm", "text": "In order to learn a ranking function f (w), it is sufficient according to theorem 1 to learn the dual variables \u03b1 and \u03b2 by solving the problem in (7). To this end, we apply the method of accelerated gradient based on its light calculation per iteration. Since we push positive instances before the best placed negative, we call the obtained algorithm TopPush."}, {"heading": "3.2.1. Efficient Optimization", "text": "We choose the Nesterov method [32, 31], which achieves an optimal convergence rate O (1 / T 2) for a smooth objective function. One of the main features of the Nesterov method is that, in addition to the solution sequence {(\u03b1k, \u03b2k)}, it also maintains a sequence of auxiliary solutions {(s\u03b1k; s \u03b2 k)}, which is introduced to use the smoothness of the objective function to achieve a faster convergence rate. Meanwhile, its step size depends on the smoothness of the objective function, in the current work we adopt the Nemirovski line search scheme [31] to estimate the smoothing parameter. Of course, other schemes, such as the one developed in [25] can also be used. Algorithm 1 summarizes the steps of the TopPush algorithm. At each iteration, the gradients of the objective function g (\u03b1, \u03b2) should be efficiently calculated as a projection."}, {"heading": "3.2.2. Projection Step", "text": "For clear notations we extend the step of projection into the problem min \u03b1 = 0, \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = 2 (11) s.t. 1 > m\u03b1 = 1 > n\u03b2Algorithm 1 The TopPush algorithm 1 = \u03b21 = \u03b20 and \u03b21 = 0n 3: for k = 0, 1, 2,. Let us do 4: Sentence 1 = tk \u2212 2 \u2212 1 and Lk = 1 + n2: let us initialize the solution 1 = \u03b10 = 0m and \u03b21 = \u03b20 = 0n 3: for k = 0, 1, 2,. Let us do 4: Sentence 2 \u2212 \u2212 2 \u2212 tk \u2212 1 and Lk = Lk \u2212 1 5: let us calculate the solution of the help question: sak = \u03b1k + \u00e4k (\u03b1k \u2212 k \u2212) and \u03b21: for the solution of the problem."}, {"heading": "3.3. Convergence and Computational Complexity", "text": "The following theorem describes the convergence of the TopPush algorithm, which results directly from the convergence result for the Nesterov method = \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 = \u03b2 \u03b2 = \u03b2 = \u03b2 = 2 Let us consider the results of the TopPush algorithm according to T iterations as random results. Finally, the computing power of each iteration is dominated by the gradient evaluation and the projection step. Since the complexity of the projection steps O (m + n) and the cost of calculating the gradient O (m + n) d, the time complexity of each iteration is dominated by the gradient evaluation and the projection step. As the complexity of the projection steps O (m + n) is, we have to find a suboptimal solution, the total complexity of the TopPush algorithms is O (m + n) d)."}, {"heading": "3.4. Theoretical Guarantee", "text": "We develop a theoretical guarantee for the ranking performance of TopPush. In [35, 1], the authors have developed a margin-based generalization limit for the loss function L '\u221e. One limitation of the analysis in [35, 1] is that they try to limit the probability of a positive instance being ranked before a negative instance, leading to relatively pessimistic limits. To this end, we first define hb (x, w) the probability of a negative instance as large as 1 if the parameter p is large. Our analysis avoids this trap by considering the probability of a positive instance before most negative instances. To this end, we first define hb (x, w) the probability of a negative instance being ranked above x."}, {"heading": "4. Proofs and Technical Details", "text": "In this section, we list all the detailed evidence that is missing from the main text, along with additional remarks and comments."}, {"heading": "4.1. AUC vs. Accuracy at the Top", "text": "We examine the relationship between AUC and accuracy at the top by means of their corresponding loss functions, i.e. rank loss in (1) and our loss L in (2). Proof: [of sentence 1] It is easy to check whether the loss L in (2) corresponds to the loss L \u221e (f; S) = max. 1 \u2264 j \u2264 n1m m \u00b2 i = 1 I (f (x + i) \u2264 f (x \u2212 j). If we define \u03baj = 1 m \u00b2 m \u00b2 i = 1 I (f (x + i) \u2264 f (x \u2212 j)))), we have [0, 1], andL (f; S) = L \u00b2 (f; S) = max 1 \u2264 j \u2264 n\u0432j, Lrank (f; S) = 1n \u00b2 n \u00b2 n = 1 \u041aj. Based on the relationship between the mean and the maximum of a group of elements, we can draw the conclusion."}, {"heading": "4.2. Proof of Theorem 1", "text": "Since \"(z) is a convex loss function that is not detachable and indistinguishable, it can be rewritten in its convex conjugate form, i.e.\" (z) = max \u03b1 \u2265 0\u03b1z \u2212 \"(\u03b1), where\" \u0445 (\u03b1) is the convex conjugate of \"(z) and therefore the problem is rewritten into (6) asmin w max \u03b1 01m m \u2211 i = 1 \u03b1i (max 1 \u2264 j \u2264 n > x \u2212 j \u2212 w > x + i) \u2212 1 m \u2211 i = 1\" (\u03b1 i = 1 \"), where\" max w \"(15), whose\" x, \". \u2212 m) > are duplicate variables. Leave p\" Rn \"and\" i \"s.\" Then: p \"0 and 1 > np = 1 > np = 1} is the default n-simplex, we havemax 1,\" we havemax, \"j.\""}, {"heading": "4.3. Proof of Theorem 3", "text": "For the convenience of analysis, we consider the limited version of the optimization problem in (6) that ismin w \u00b2 W (w; S) = 1 m \u00b2 i = 1 '(max 1 \u2264 j \u2264 n > x \u2212 j \u2212 w > x + i) (19), where W = {w \u00b2 Rd: 0 x \u00b2 W (w; S) is a domain and 0 indicates the size of the domain that plays a similar role to the regulation parameter \u03bb in (6). First, we refer to G as the Lipschitz constant of disturbed square loss \"(z) on domain [\u2212 2], and define the following two functions based on\" (z), h' (x, w) = E x \u2212 P \u2212 p (w), which we call the Lipschitz constant of disturbed square loss \"(z)."}, {"heading": "5. Experiments", "text": "In order to evaluate the performance of the proposed TopPush algorithm, we conduct a series of experiments on real data sets."}, {"heading": "5.1. Settings", "text": "Table 2 (left column) summarizes the data sets used in our experiments, some of which have been used in previous studies (1, 33, 4], and others are larger data sets from different fields. For example, diabetes is a medical task, News20-forsale is on text classification, Spambase is on email spam filtering, and nslkdd is a network intrusion dataset. It should be noted that news20-forsale is transformed from News20 datasets by treating Forales as a positive class and others as negative. All of these datasets are publicly available.We compare TopPush with state-of-the-art ranking algorithms that focus on accuracy at the top, including SVMMAP [30] with \u03b1 = 0 and \u03b2 = 1 / n, AATP [4] and InfinitePush [1]."}, {"heading": "5.2. Results", "text": "In Table 2 we report on the performance of the algorithms in comparison, with the statistics of the test beds included in the first column of the table. In order to allow a better comparison between the performance of TopPush and the baselines, pairs of t-tests are performed at the significance level of 0.9 and the results are rated in Table 2 as \"\u2022 /\" if they are statistically significantly worse / better than TopPush. If an evaluation task evaluating an algorithm on a data set, including parameter selection, training and testing, cannot be completed in two weeks, it is automatically stopped and no result is reported. Therefore, some algorithms are missing in the table for certain data sets, especially for these large data sets. Table 2 shows that TopPush, LR and cs-SVM can successfully complete the evaluation on all data sets (even the largest data sets url). In contrast, SVMRank, SVMpAUC miss the task in time for several data sets that they consider infinitely difficult to test."}, {"heading": "5.2.1. Ranking Performance", "text": "With regard to the metric evaluation methods Pos @ Top, we note that TopPush performs similar to InfinitePush and AATP and performs significantly better than the other baselines such as LR and cs-SVM, SVMRank, SVMMAP and SVMpAUC. This is in line with the design of TopPush, which aims to maximize accuracy at the top of the ranking. As the loss function optimized by InfinitePush and AATP is similar to that of TopPush, it is not surprising that it performs similarly. The main advantage of using the proposed algorithm over InfinitePush and AATP is that it is more computationally efficient and can be easily scaled to large data sets. With regard to AP and NDCG, we note that TopPush performs similarly, if not better, than the most modern methods, such as SVMMAP and SVMpAUC, which are designed to optimize these metrics."}, {"heading": "5.2.2. Training Efficiency", "text": "To evaluate computational efficiency, we set the parameters of different algorithms so that these values are selected by cross-validation, and execute these algorithms on complete datasets comprising both training and test sets. Table 2 summarizes the training time of various algorithms. Results show that TopPush is faster than state-of-the-art ranking methods for most datasets. In fact, the training time of TopPush is even similar to the training time of LR and cs-SVM implemented by LIBLINEAR. Since the time complexity of learning a binary classification model is usually linear in the number of training instances, this result implicitly suggests a linear time complexity for the proposed algorithm."}, {"heading": "5.2.3. Scalability", "text": "We examine how TopPush scales to different number of training examples using the largest dataset url. Figure 1 shows the log graph for the training time of TopPush vs. the size of the training data, with different lines corresponding to different values of \u03bb. Lines in a log log graph correspond to the polynomic growth \u03b7 (xp), with p corresponding to the slope of the line. For comparison, we also add a black bar-dot line that attempts to adjust the training time by a linear function in the number of training instances (i.e., m + n). The graph shows that the training time of TopPush increases even more slowly than the number of training data with different regulation parameters \u03bb. This is in line with our theoretical analysis in Section 3.3."}, {"heading": "5.2.4. Influence of Parameters", "text": "We examine the influence of precision parameters and regulation parameters on the calculation costs and the predictive performance of TopPush. First, we set \u03bb to 1 and lead TopPushwith........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "6. Conclusion and Future Work", "text": "In this paper, we focus on bilateral ranking algorithms that optimize accuracy at the top of the ranking. To this end, we are looking at maximizing the number of positive instances that rank above all negative instances, and developing an efficient algorithm called TopPush to solve the related optimization problem. Compared to existing work on this topic, the proposed TopPush algorithm scales linearly the number of training instances, which contrasts with most existing algorithms for bilateral rankings, whose time complexity depends on the number of positive-negative instance pairs. Moreover, our theoretical analysis clearly shows that it will lead to a ranking function that puts many positive instances at the top of the ranking. Empirical studies confirm the theoretical assertions: The TopPush algorithm is effective at maximizing accuracy at the top and significantly more efficient than the state of the art."}, {"heading": "Acknowledgments", "text": "This research was supported by the 973 Program (2014CB340501), NSFC (61333014), NSF (IIS1251031) and ONR Award (N000141210431). Z.-H. Zhou is the corresponding author of this paper."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before<lb>negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy<lb>at the top of the ranked list. Most existing approaches are either to optimize task specific metrics<lb>or to extend the ranking loss by emphasizing more on the error associated with the top ranked<lb>instances, leading to a high computational cost that is super-linear in the number of training<lb>instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the<lb>top that has computational complexity linear in the number of training instances. We present a<lb>novel analysis that bounds the generalization error for the top ranked instances for the proposed<lb>approach. Empirical study shows that the proposed approach is highly competitive to the state-<lb>of-the-art approaches and is 10-100 times faster.<lb>", "creator": "LaTeX with hyperref package"}}}