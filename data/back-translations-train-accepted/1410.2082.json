{"id": "1410.2082", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2014", "title": "Contrastive Unsupervised Word Alignment with Non-Local Features", "abstract": "Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top-n alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over state-of-the-art unsupervised word alignment methods.", "histories": [["v1", "Wed, 8 Oct 2014 12:24:38 GMT  (70kb,D)", "https://arxiv.org/abs/1410.2082v1", "15 pages, 4 figures"], ["v2", "Fri, 10 Oct 2014 00:25:46 GMT  (70kb,D)", "http://arxiv.org/abs/1410.2082v2", "Added the missing Table 1; corrected a typo in Related Work"]], "COMMENTS": "15 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yang liu 0005", "maosong sun"], "accepted": true, "id": "1410.2082"}, "pdf": {"name": "1410.2082.pdf", "metadata": {"source": "CRF", "title": "Contrastive Unsupervised Word Alignment with Non-Local Features", "authors": ["Yang Liu"], "emails": ["liuyang2011@tsinghua.edu.cn", "sms@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to surpass themselves, both in terms of their abilities and in terms of their abilities. Most of them are able to unfold their abilities, both in terms of their abilities and in terms of their abilities. Most of them are able to unfold their abilities, both in terms of their abilities and in terms of their abilities. Most of them are able to unfold their abilities and in terms of their abilities. Most of them are able to unfold their abilities, as well as in terms of their abilities, their abilities and abilities. Most of them are able to unfold their abilities and to unfold their abilities."}, {"heading": "2 Latent-Variable Log-Linear Models for Unsupervised Word Alignment", "text": "Figure 1 (a) shows a (Romanized) Chinese sentence, an English sentence and the word alignment between them. The links show the correspondence between Chinese and English words. Word alignment is a challenging task, as both the lexical decisions and the word orders in two languages are significantly different. Thus, for example, the English word \"at\" is usually followed by a prepositional phrase (e.g. \"at the meeting\") in English, but the order is reversed in Chinese. Therefore, it is important to grasp different properties of the word alignment. In order to facilitate a confusing word alignment with arbitrary features, latent linear models have been studied in recent years (Berg-Kirkpatrick et al)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Approximation Evaluation", "text": "In order to measure how well top-n approaches the true expectations at random, we define the approximation error E (D, \u03b8) as1I \u00b7 K I \u2211 i = 1 | | \u03b4Y (x (i), x (i), \u03b8) \u2212 \u03b4N (x (i), x (i), \u03b8) | 1 (19), where \u03b4Y (\u00b7) represents the true difference between the expectations of the observed and the noisy examples: \u03b4Y (x, x) is the L1 standard. In addition, we define the average approximation error on a series of random feature vectors (t) that maintain the average weight vectors (t). Similarly, \u03b4N (\u00b7) returns the approximate difference."}, {"heading": "4.2 Alignment Evaluation", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "5 Related Work", "text": "Our work is inspired by three areas of research: unattended learning of loglinear models, contrastive learning, and samples for structured predictions."}, {"heading": "5.1 Unsupervised Learning of Log-Linear Models", "text": "Unattended learning of loglinear models is widely used in natural language processing, including word segmentation (Berg-Kirkpatrick et al., 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013). Smith and Eisner's (2005) contrasting approach to estimation (CE) comes closest to our work. CE redefines the partition function as the set of each observed example and its noisy \"neighbors,\" but it remains insoluble to calculate the expectations of non-local characteristics. In contrast, our approach cancels out the partition function and introduces top-n-sampling to approximate the expectations of non-local characteristics."}, {"heading": "5.2 Contrastive Learning", "text": "Contrastive learning has gained increasing attention in a variety of areas. Hinton (2002) proposes Contrastive Divergence (CD), which compares data distribution with reconstructions of the data vector generated by a limited number of complete Gibbs sampling steps. It is possible to apply CD to unattended learning of latent-variable loglinear models and to use Top-n sampling to approximate expectations of posterior distributions within each complete Gibbs sampling step. However, one significant limitation of NCE is that it cannot be used for models with latent variables that cannot be integrated analytically. There are also many other efforts in the development of 2005 Sub-Dynamic Development Goals (2008 and 2008 Sub-Dynamic Development Goals for Leckrey and 2008) to observe contrastive learning in models with latent variables that cannot be analytically integrated."}, {"heading": "5.3 Sampling for Structured Prediction", "text": "Widely used in the NLP for inferences (Teh, 2006; Johnson, Griffiths and Goldwater, 2007) and for calculating expectations (DeNero, Bouchard-Cot e and Klein, 2008), Gibbs sampling was not used for the unattended training of loglinear models for word alignment. Tamura, Watanabe and Sumita (2014) propose a similar idea of using beam search to calculate expectations, but they do not offer in-depth analysis, and the accuracy of their unguarded approach to F1 scores is much worse than the monitored counterpart (0.55 vs. 0.89)."}, {"heading": "6 Conclusion", "text": "We have presented a contrasting approach to unattended learning of loglinear word alignment models. By introducing noisy examples, our approach eliminates partition functions that make training computationally expensive. Our most important contribution is the introduction of top-n samples to calculate expectations of non-local characteristics, since the probability mass of loglinear models for word alignment is usually concentrated on top-n axes. Our unattended aligner outperforms state-of-the-art, unattended systems in both closely related (French-English) and distant (Chinese-English) language pairs. As loglinear models are widely used in NLP, we plan to review the effectiveness of our approach to more structured prediction tasks with exponential search spaces such as word segmentation, part-of-the-language marking, dependency sparing, and machine translation. As our contradictory approach still maintains the ability of logging linear development of a large set model, it is one of the most widely observed in 2011."}, {"heading": "Acknowledgements", "text": "This research is supported and managed by the 973 Program (# 2014CB340501), the National Natural Science Foundation of China (# 61331013), the National Key Technology F & D Program (# 2014BAK10B03), the Google Focused Research Award, the Singapore National Research Foundation through its International Research Center @ Singapore Funding Initiative."}], "references": [{"title": "Painless unsupervised learning with features", "author": ["T. Berg-Kirkpatrick", "A. Bouchard-Cot\u0302\u00e9", "J. DeNero", "D. Klein"], "venue": "In Proceedings of NAACL", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Discriminative word alignment with conditional random fields", "author": ["P. Blunsom", "T. Cohn"], "venue": "Proceedings of COLING-ACL 2006.", "citeRegEx": "Blunsom and Cohn,? 2006", "shortCiteRegEx": "Blunsom and Cohn", "year": 2006}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Computational Linguistics.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of ICML 2008.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Sampling alignment structure under a bayesian translation model", "author": ["J. DeNero", "A. Bouchard-Cot\u0302\u00e9", "D. Klein"], "venue": "In Proceedings of EMNLP", "citeRegEx": "DeNero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2008}, {"title": "Unsupervised word alignment with arbitrary features", "author": ["C. Dyer", "J.H. Clark", "A. Lavie", "N.A. Smith"], "venue": "Proceedings of ACL 2011.", "citeRegEx": "Dyer et al\\.,? 2011", "shortCiteRegEx": "Dyer et al\\.", "year": 2011}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "Proceedings of NAACL 2013.", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "Hyv\u00e4rinen."], "venue": "Journal of Machine Learning Research.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2012", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Bayesian inference for pcfgs via markov chain monte carlo", "author": ["M. Johnson", "T. Griffiths", "S. Goldwater"], "venue": "Proceedings of ACL 2007.", "citeRegEx": "Johnson et al\\.,? 2007", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of ACL 2007 (Demo and Poster).", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Loss functions for discriminative training of energybased models", "author": ["Y. LeCun", "F.J. Huang"], "venue": "Proceedings of AISTATS 2005.", "citeRegEx": "LeCun and Huang,? 2005", "shortCiteRegEx": "LeCun and Huang", "year": 2005}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M.I. Jordan"], "venue": "Proceedings of ICML 2008.", "citeRegEx": "Liang and Jordan,? 2008", "shortCiteRegEx": "Liang and Jordan", "year": 2008}, {"title": "Alignment by agreement", "author": ["P. Liang", "B. Taskar", "D. Klein"], "venue": "Proceedings of HLT-NAACL 2006.", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Log-linear models for word alignment", "author": ["Y. Liu", "Q. Liu", "S. Lin"], "venue": "Proceedings of ACL 2005.", "citeRegEx": "Liu et al\\.,? 2005", "shortCiteRegEx": "Liu et al\\.", "year": 2005}, {"title": "Discriminative word alignment by linear modeling", "author": ["Y. Liu", "Q. Liu", "S. Lin"], "venue": "Computational Linguistics.", "citeRegEx": "Liu et al\\.,? 2010", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "An evaluation excercise for word alignment", "author": ["R. Mihalcea", "T. Pedersen"], "venue": "Proceedings of HLT-NAACL 2003 Workshop on Building and Using Parallel Texts.", "citeRegEx": "Mihalcea and Pedersen,? 2003", "shortCiteRegEx": "Mihalcea and Pedersen", "year": 2003}, {"title": "Improved discriminative bilingual word alignment", "author": ["R.C. Moore", "W.-t. Yih", "A. Bode"], "venue": "Proceedings of COLING-ACL 2006.", "citeRegEx": "Moore et al\\.,? 2006", "shortCiteRegEx": "Moore et al\\.", "year": 2006}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F. Och", "H. Ney"], "venue": "Computational Linguistics.", "citeRegEx": "Och and Ney,? 2003", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F. Och"], "venue": "Proceedings of ACL 2003.", "citeRegEx": "Och,? 2003", "shortCiteRegEx": "Och", "year": 2003}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["H. Poon", "C. Cherry", "K. Toutanova"], "venue": "Proceedings of NAACL 2009.", "citeRegEx": "Poon et al\\.,? 2009", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["N. Smith", "J. Eisner"], "venue": "Proceedings of ACL 2005.", "citeRegEx": "Smith and Eisner,? 2005", "shortCiteRegEx": "Smith and Eisner", "year": 2005}, {"title": "Recurrent neural networks for word alignment model", "author": ["A. Tamura", "T. Watanabe", "E. Sumita"], "venue": "Proceedings of EMNLP 2014.", "citeRegEx": "Tamura et al\\.,? 2014", "shortCiteRegEx": "Tamura et al\\.", "year": 2014}, {"title": "A discriminative matching approach to word alignment", "author": ["B. Taskar", "S. Lacoste-Julien", "D. Klein"], "venue": "Proceedings of EMNLP 2005.", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "A hierarchical bayesian language model based on pitman-yor processes", "author": ["Y.W. Teh"], "venue": "Proceedings of COLING/ACL 2006.", "citeRegEx": "Teh,? 2006", "shortCiteRegEx": "Teh", "year": 2006}, {"title": "Non-local contrastive objectives", "author": ["D. Vickrey", "C.C.-Y. Lin", "D. Koller"], "venue": "Proceedings of ICML 2010.", "citeRegEx": "Vickrey et al\\.,? 2010", "shortCiteRegEx": "Vickrey et al\\.", "year": 2010}, {"title": "Hmm-based word alignment in statistical translation", "author": ["S. Vogel", "H. Ney", "C. Tillmann"], "venue": "Proceedings of COLING 1996.", "citeRegEx": "Vogel et al\\.,? 1996", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}], "referenceMentions": [{"referenceID": 2, "context": "Word alignment is a natural language processing (NLP) task that aims to identify the correspondence between words in natural languages (Brown et al., 1993).", "startOffset": 135, "endOffset": 155}, {"referenceID": 2, "context": "Generative approaches are often based on generative models (Brown et al., 1993; Vogel, Ney, and Tillmann, 1996; Liang, Taskar, and Klein, 2006), the parameters of which are learned by maximizing the likelihood of unlabeled", "startOffset": 59, "endOffset": 143}, {"referenceID": 1, "context": "On the other hand, discriminative approaches overcome this problem by leveraging log-linear models (Liu, Liu, and Lin, 2005; Blunsom and Cohn, 2006) and linear models (Taskar, Lacoste-Julien, and Klein, 2005; Moore, Yih, and Bode, 2006; Liu, Liu, and Lin, 2010) to include arbitrary features.", "startOffset": 99, "endOffset": 148}, {"referenceID": 5, "context": "As generative and discriminative approaches seem to be complementary, a number of authors have tried to combine the advantages of both in recent years (BergKirkpatrick et al., 2010; Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013).", "startOffset": 151, "endOffset": 234}, {"referenceID": 0, "context": "As a result, existing approaches have to either restrict log-linear models to be locally normalized (Berg-Kirkpatrick et al., 2010) or only use local features to admit efficient dynamic programming algorithms on compact representations (Dyer et al.", "startOffset": 100, "endOffset": 131}, {"referenceID": 5, "context": ", 2010) or only use local features to admit efficient dynamic programming algorithms on compact representations (Dyer et al., 2011).", "startOffset": 112, "endOffset": 131}, {"referenceID": 21, "context": "Instead of maximizing the likelihood of log-linear models on the observed data, our approach follows contrastive estimation methods (Smith and Eisner, 2005; Gutmann and Hyv\u00e4rinen, 2012) to guide the model to assign higher probabilities to observed data than to noisy data.", "startOffset": 132, "endOffset": 185}, {"referenceID": 7, "context": "Instead of maximizing the likelihood of log-linear models on the observed data, our approach follows contrastive estimation methods (Smith and Eisner, 2005; Gutmann and Hyv\u00e4rinen, 2012) to guide the model to assign higher probabilities to observed data than to noisy data.", "startOffset": 132, "endOffset": 185}, {"referenceID": 0, "context": "To allow for unsupervised word alignment with arbitrary features, latent-variable log-linear models have been studied in recent years (Berg-Kirkpatrick et al., 2010; Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013).", "startOffset": 134, "endOffset": 218}, {"referenceID": 5, "context": "To allow for unsupervised word alignment with arbitrary features, latent-variable log-linear models have been studied in recent years (Berg-Kirkpatrick et al., 2010; Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013).", "startOffset": 134, "endOffset": 218}, {"referenceID": 5, "context": "Consequently, existing approaches have to use only local features to allow dynamic programming algorithms to calculate expectations efficiently on lattices (Dyer et al., 2011).", "startOffset": 156, "endOffset": 175}, {"referenceID": 5, "context": "To calculate the true expectations, we follow Dyer et al. (2011) to use a dynamic programming algorithm on lattices that compactly represent exponentially many asymmetric alignments.", "startOffset": 46, "endOffset": 65}, {"referenceID": 16, "context": "For French-English, we used the dataset from the HLT/NAACL 2003 alignment shared task (Mihalcea and Pedersen, 2003).", "startOffset": 86, "endOffset": 115}, {"referenceID": 16, "context": "For French-English, we used the dataset from the HLT/NAACL 2003 alignment shared task (Mihalcea and Pedersen, 2003). The training set consists of 1.1M sentence pairs with 23.61M French words and 20.01M English words, the validation set consists of 37 sentence pairs, and the test set consists of 447 sentence pairs. Both the validation and test sets are annotated with gold-standard alignments. For Chinese-English, we used the dataset from Liu, Liu, and Lin (2005). The training set consists of 1.", "startOffset": 87, "endOffset": 466}, {"referenceID": 18, "context": "metric is alignment error rate (AER) (Och and Ney, 2003).", "startOffset": 37, "endOffset": 56}, {"referenceID": 18, "context": "GIZA++ (Och and Ney, 2003): unsupervised training of IBM models 1-5 (Brown et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": "GIZA++ (Och and Ney, 2003): unsupervised training of IBM models 1-5 (Brown et al., 1993) and HMM (Vogel, Ney, and Tillmann, 1996) using EM, 2.", "startOffset": 68, "endOffset": 88}, {"referenceID": 19, "context": "Vigne (Liu, Liu, and Lin, 2010): supervised training of log-linear models using minimum error rate training (Och, 2003).", "startOffset": 108, "endOffset": 119}, {"referenceID": 10, "context": "As both GIZA++ and fast align produce asymmetric alignments, we use the growdiag-final-and heuristic (Koehn et al., 2007) to generate symmetric alignments for evaluation.", "startOffset": 101, "endOffset": 121}, {"referenceID": 5, "context": "However, if we model translation equivalence using millions of sparse features (Dyer et al., 2011), the unsupervised learning algorithm must make full use of all parallel corpora available like GIZA++.", "startOffset": 79, "endOffset": 98}, {"referenceID": 10, "context": ", the grow, diag, final operators in symmetrizing alignments) seems to be more important for improving translation translation quality than developing unsupervised training algorithms (Koehn et al., 2007).", "startOffset": 184, "endOffset": 204}, {"referenceID": 0, "context": "1 Unsupervised Learning of Log-Linear Models Unsupervised learning of log-linear models has been widely used in natural language processing, including word segmentation (Berg-Kirkpatrick et al., 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al.", "startOffset": 169, "endOffset": 200}, {"referenceID": 21, "context": ", 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al.", "startOffset": 85, "endOffset": 109}, {"referenceID": 21, "context": ", 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al.", "startOffset": 129, "endOffset": 153}, {"referenceID": 5, "context": ", 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013).", "startOffset": 174, "endOffset": 227}, {"referenceID": 0, "context": "1 Unsupervised Learning of Log-Linear Models Unsupervised learning of log-linear models has been widely used in natural language processing, including word segmentation (Berg-Kirkpatrick et al., 2010), morphological segmentation (Poon, Cherry, and Toutanova, 2009), POS tagging (Smith and Eisner, 2005), grammar induction (Smith and Eisner, 2005), and word alignment (Dyer et al., 2011; Dyer, Chahuneau, and Smith, 2013). The contrastive estimation (CE) approach proposed by Smith and Eisner (2005) is in spirit most close to our work.", "startOffset": 170, "endOffset": 499}, {"referenceID": 7, "context": "The noise-contrastive estimation (NCE) method (Gutmann and Hyv\u00e4rinen, 2012) casts density estimation, which is a typical unsupervised learning problem, as supervised classification by introducing noisy data.", "startOffset": 46, "endOffset": 75}, {"referenceID": 11, "context": "There are also many other efforts in developing contrastive objectives to avoid computing partition functions (LeCun and Huang, 2005; Liang and Jordan, 2008; Vickrey, Lin, and Koller, 2010).", "startOffset": 110, "endOffset": 189}, {"referenceID": 12, "context": "There are also many other efforts in developing contrastive objectives to avoid computing partition functions (LeCun and Huang, 2005; Liang and Jordan, 2008; Vickrey, Lin, and Koller, 2010).", "startOffset": 110, "endOffset": 189}, {"referenceID": 3, "context": "Using noisy examples to guide unsupervised learning has also been pursued in deep learning (Collobert and Weston, 2008; Tamura, Watanabe, and Sumita, 2014).", "startOffset": 91, "endOffset": 155}, {"referenceID": 6, "context": "Hinton (2002) proposes contrastive divergence (CD) that compares the data distribution with reconstructions of the data vector generated by a limited number of full Gibbs sampling steps.", "startOffset": 0, "endOffset": 14}, {"referenceID": 24, "context": "3 Sampling for Structured Prediction Widely used in NLP for inference (Teh, 2006; Johnson, Griffiths, and Goldwater, 2007) and calculating expectations (DeNero, Bouchard-Cot\u0302\u00e9, and Klein, 2008), Gibbs sampling has not been used for unsupervised training of log-linear models for word alignment.", "startOffset": 70, "endOffset": 122}, {"referenceID": 24, "context": "3 Sampling for Structured Prediction Widely used in NLP for inference (Teh, 2006; Johnson, Griffiths, and Goldwater, 2007) and calculating expectations (DeNero, Bouchard-Cot\u0302\u00e9, and Klein, 2008), Gibbs sampling has not been used for unsupervised training of log-linear models for word alignment. Tamura, Watanabe, and Sumita (2014) propose a similar idea to use beam search to calculate expectations.", "startOffset": 71, "endOffset": 331}, {"referenceID": 5, "context": "Finally, it is interesting to include millions of sparse features (Dyer et al., 2011) to directly model the translation equivalence between words rather than relying on GIZA++.", "startOffset": 66, "endOffset": 85}], "year": 2014, "abstractText": "Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top-n alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over stateof-the-art unsupervised word alignment methods.", "creator": "LaTeX with hyperref package"}}}