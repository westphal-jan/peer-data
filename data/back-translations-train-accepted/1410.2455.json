{"id": "1410.2455", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2014", "title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "abstract": "We introduce BilBOWA (\"Bilingual Bag-of-Words without Alignments\"), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large datasets and does not require word-aligned training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. We introduce a novel sampled bag-of-words cross-lingual objective and use it to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We evaluate our model on cross-lingual document classification and lexical translation on the WMT11 data. Our code will be made available as part of the open-source word2vec toolkit.", "histories": [["v1", "Thu, 9 Oct 2014 13:41:18 GMT  (307kb,D)", "https://arxiv.org/abs/1410.2455v1", null], ["v2", "Thu, 4 Dec 2014 20:52:32 GMT  (242kb,D)", "http://arxiv.org/abs/1410.2455v2", null], ["v3", "Thu, 4 Feb 2016 05:51:59 GMT  (627kb,D)", "http://arxiv.org/abs/1410.2455v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["stephan gouws", "yoshua bengio", "greg corrado"], "accepted": true, "id": "1410.2455"}, "pdf": {"name": "1410.2455.pdf", "metadata": {"source": "META", "title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "authors": ["Stephan Gouws", "Yoshua Bengio"], "emails": ["SGOUWS@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead, in the same way as it has done in the past."}, {"heading": "2. Learning Cross-lingual Word Embeddings", "text": "These algorithms are trained using large datasets to predict words from the contexts in which they appear. Their work can be understood intuitively, as if each word can be mapped to a learned vector in an embedded space, and updating these vectors in an attempt to simultaneously minimize the distance from a word vector to the vectors of the words with which it frequently occurs. The result of this optimization process is a rich geometric encoding of natural language distribution properties, where words with similar distribution properties are grouped together. By their general nature, these functions work well for multiple NLP prediction tasks (Collobert et al.), 2011; Turian et al."}, {"heading": "3. The BilBOWA Model", "text": "As discussed in \u00a7 2, the primary challenges with existing bilingual embedding models are their computational complexity (due to an expensive Softmax or an expensive regularization term, or both), but above all the strong domain bias introduced by models based only on parallel data such as Europarliament.The BilBOWA model is designed to overcome these problems in order to enable computationally efficient, distributed feature learning across large volumes of monolingual text.A schematic overview of the model is presented in Figure 2. The two main aspects (discussed in the following sections) are 1. Similar to (Zou et al., 2013), we are using advances in monolingual feature learning algorithms by replacing the Softmax target with a more efficient noise-contrasting target (\u00a7 3.1), whereby monolingual training updates can be scaled regardless of vocabulary size. Secondly, we are introducing a new word computational loss, which is considered only for 3."}, {"heading": "3.1. Learning Monolingual Features: The L term", "text": "Since we are not interested in language modeling, but rather in feature learning, an alternative to Softmax is to evaluate valid, observed word combinations using randomly sampled, unlikely word combinations using a noise-contrasting approach, an idea introduced by Collobert and Weston (Collobert et al., 2011), where they use opti-2If we limit each word to aligning with k-words, it is still O (V k).mized a difference between the observed score and the noise score. In their formulation, the values were calculated using word sequences, but in (Mikolov et al., 2013a) this idea was taken a step further and successfully applied to slack word representations of contexts in their continuous dictionaries (CBOW) and skipgram models trained on the negative sampling training target (a simplified version of the screw-contrastive estimate (Mnih & Teh)."}, {"heading": "3.2. Learning Cross-lingual Features: The BilBOWA-loss (\u2126 term)", "text": "It is a question of what the future of the world is like, and it is a question of what the future of the world is like. (...) It is a question of what the future of the world is like. (...) It is a question of what the future of the world is like. (...) It is a question of what the future of the world is like. (...) It is a question of what the future of the world is like. (...) It is a question of what the future is like. (...) It is a question of what the future of the world is like. (...) It is a question of what the future is like. (...) It is a question of what the future is like. (...) It is a question of what the future is like, what the future is like, what the future is like. (...) It is a question of what the future is like, what the future is like, what the future is like, what the future is like, what is about the future, what is about the future, what is about the future, what is about the future, what is about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, about the future, the future, about the future, about the future, about the future, the future, about the future, the future, about the future, the future, the future, about the future, the future, the future, the future, the future"}, {"heading": "3.3. Parallel subsampling for better results", "text": "Equation 7 is an approximation of Equation 3. As illustrated in Figure 3, we are really interested in estimating the global word alignment statistics for a word pair, i.e. aij. Therefore, since language has a very strong ciphers distribution, we find in practice that Equation 7 over-regulates the common words. A simple solution to this is to stamp (discard) words from the parallel sentences that are proportional to their unigram probabilities of occurrence. In other words, we discard each word from the parallel sentences with a probability depending on its unigram frequency of occurrence, this strongly discarded frequency of common words and the effective flattening of the unigram distribution to an even distribution."}, {"heading": "4. Implementation and Training Details", "text": "We implemented our model in C by using the popular open source program word2vec toolkit3, which results in a monolingual skipgram model as a separate thread for each language, as well as a multilingual thread. All threads access the common embedding parameters asynchronously. To train the model, we use online3https: / / code.google.com / p / word2vec / asynchron stochastic gradient Descent Descent Descent (ASGD), where the parameters are updated at the time t. (t \u2212 1) \u2212 Our initial implementation synchronizes updates between threads, but we found that individual updates to [\u2212 0.1] per thread are sufficient to ensure training stability and significantly improved training speed."}, {"heading": "5. Experiments", "text": "In this section, we present experiments that evaluate the usefulness of induced representations. We evaluate embedding in a linguistic task for classifying documents that verifies the semantic transfer of information between languages, as well as a word-level translation task that verifies fine-grained lexical translations."}, {"heading": "5.1. Cross-lingual Document Classification", "text": "In fact, most of them will be able to play by the rules that they have established in recent years, and they will be able to play by the rules."}, {"heading": "5.2. WMT11 Word Translation", "text": "We evaluated the induced translingual embeddings on the word translation task performed by Mikolov et al. (Mikolov et al., 2013a) using the publicly available WMT11 Data4. In this task, the authors extracted the 6K most common words from the WMT11 English-Spanish data, and then used the online service Google Translate to derive dictionaries by translating those source words into the target language (individually for English and Spanish). Since their method requires translation pairs for training, they used the first 5K most common words to learn the \"translation matrix,\" and then rated their method on the remaining 1K words used as a test sentence. To translate a source word, one finds one's closest neighbors in the target language that embeds the space, and then rated the translation accuracy of the P @ k translations, as the fraction of the target translations."}, {"heading": "6. Discussion", "text": "The BilBOWA model, as presented in this paper, uses a sampled L2 bag of words that exhibits cross-language losses, which is the main source of significant acceleration compared to the Klementiev model, allowing the training to be scaled to much larger sets of data, which in turn delivers more accurate characteristics. We found that asynchronous implementation significantly speeds up the training without having any noticeable impact on the quality of the embeddings learned, and the parallel subsampling method improves the accuracy of the traits learned. To make asynchronous training work, the updates had to be truncated, especially as the dimensionality of the embeddings increases. The parallel subsampling method makes training more precise, especially with the frequent words, and thus proves important for both monolingual and cross-linguistic alignment."}, {"heading": "7. Conclusion", "text": "We introduce BilBOWA, a computationally efficient model for generating bilingual, distributed word representations directly from monolingual raw text and a limited number of parallel data, with no text alignments or dictionaries. BilBOWA combines advances in the development of monolingual word embeddings with a particularly efficient novel, sampled cross-lingual target. The result is that the required calculations per training step are scaled only with the number of words in the sentences, enabling efficient, large-language training. We achieve state-of-the-art results for the linguistic classification of documents in English and German, while achieving up to three orders of acceleration and improving the previous state of the art in an English-Spanish word translation task."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Al-Rfou", "Rami", "Perozzi", "Bryan", "Skiena", "Steven"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,", "citeRegEx": "Al.Rfou. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou. et al\\.", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Y Bengio", "R Ducharme", "P. Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Bengio", "Yoshua", "Senecal", "J-S"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2008}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Conference on Empirical Methods in Natural Language Processing, Sydney, Australia,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Chandar", "Sarath", "Lauly", "Stanislas", "Larochelle", "Hugo", "Khapra", "Mitesh M", "Ravidran", "Balaraman", "Raykar", "Vikas", "Saha", "Amrita"], "venue": "Proceedings of NIPS 2014,", "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Frustratingly easy domain adaptation", "author": ["Daum\u00e9 III", "Hal"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "III and Hal.,? \\Q2009\\E", "shortCiteRegEx": "III and Hal.", "year": 2009}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer", "Chris", "Chahuneau", "Victor", "Smith", "Noah A"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Manaal", "Dyer", "Chris"], "venue": "In Proceedings of EACL 2014,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Deriving mikolov et al.s negative-sampling word-embedding method", "author": ["Goldberg", "Yoav", "Levy", "Omer"], "venue": "arXiv preprint arXiv:1402.3722,", "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Multilingual distributed representations without word alignment", "author": ["Hermann", "Karl Moritz", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1312.6173,", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Klementiev", "Alexandre", "Titov", "Ivan", "Bhattarai", "Binod"], "venue": "In Proceedings of the International Conference on Computational Linguistics (COLING),", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Koehn", "Philipp"], "venue": "In MT summit,", "citeRegEx": "Koehn and Philipp.,? \\Q2005\\E", "shortCiteRegEx": "Koehn and Philipp.", "year": 2005}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis", "David D", "Yang", "Yiming", "Rose", "Tony G", "Li", "Fan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Franz Josef", "Ney", "Hermann"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "A survey on transfer learning", "author": ["Pan", "Sinno Jialin", "Yang", "Qiang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Pan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "An efficient method for generating discrete random variables with general distributions", "author": ["Walker", "Alastair J"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "Walker and J.,? \\Q1977\\E", "shortCiteRegEx": "Walker and J.", "year": 1977}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou", "Will Y", "Socher", "Richard", "Cer", "Daniel", "Manning", "Christopher D"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Although several techniques exist that can learn to map hand-crafted features from one domain to another (Blitzer et al., 2006; Daum\u00e9 III, 2009; Pan & Yang, 2010), it is in general non-trivial to come up with good features which generalize well across tasks, and even harder across different languages.", "startOffset": 105, "endOffset": 162}, {"referenceID": 5, "context": "Unsupervised distributed representations of words capture important syntactic and semantic information about languages and these techniques have been succesfully applied to a wide range of tasks (Collobert et al., 2011; Turian et al., 2010), across many different languages (Al-Rfou\u2019 et al.", "startOffset": 195, "endOffset": 240}, {"referenceID": 20, "context": "Unsupervised distributed representations of words capture important syntactic and semantic information about languages and these techniques have been succesfully applied to a wide range of tasks (Collobert et al., 2011; Turian et al., 2010), across many different languages (Al-Rfou\u2019 et al.", "startOffset": 195, "endOffset": 240}, {"referenceID": 0, "context": ", 2010), across many different languages (Al-Rfou\u2019 et al., 2013).", "startOffset": 41, "endOffset": 64}, {"referenceID": 1, "context": "Traditionally, inducing these representations involved training a neural network language model (Bengio et al., 2003) which was slow to train.", "startOffset": 96, "endOffset": 117}, {"referenceID": 19, "context": "However, contemporary word embedding models are much faster in comparison, and can scale to train on billions of words per day on a single desktop machine (Mnih & Teh, 2012; Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 155, "endOffset": 221}, {"referenceID": 11, "context": "This is especially useful for transferring limited label information from high-resource to low-resource languages, and has been demonstrated to be effective for document classification (Klementiev et al., 2012), outperforming a strong machine-translation baseline; as well as namedentity recognition and machine translation (Zou et al.", "startOffset": 185, "endOffset": 210}, {"referenceID": 22, "context": ", 2012), outperforming a strong machine-translation baseline; as well as namedentity recognition and machine translation (Zou et al., 2013; Mikolov et al., 2013a).", "startOffset": 121, "endOffset": 162}, {"referenceID": 19, "context": "Since these techniques are fundamentally data-driven techniques, the quality of the learned representations improves as the size of the training data improves (Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 159, "endOffset": 207}, {"referenceID": 19, "context": "Monolingual word embedding algorithms (Mikolov et al., 2013b; Pennington et al., 2014) learn useful features about words from raw text (e.", "startOffset": 38, "endOffset": 86}, {"referenceID": 5, "context": "Due to their general nature, these features work well for several NLP prediction tasks (Collobert et al., 2011; Turian et al., 2010).", "startOffset": 87, "endOffset": 132}, {"referenceID": 20, "context": "Due to their general nature, these features work well for several NLP prediction tasks (Collobert et al., 2011; Turian et al., 2010).", "startOffset": 87, "endOffset": 132}, {"referenceID": 4, "context": "This is the approach followed by the BiCVM (Hermann & Blunsom, 2013) and the bilingual auto-encoder (BAE, (Chandar et al., 2014)).", "startOffset": 106, "endOffset": 128}, {"referenceID": 11, "context": "This approach was shown to be useful by (Klementiev et al., 2012).", "startOffset": 40, "endOffset": 65}, {"referenceID": 22, "context": "First, similar to (Zou et al., 2013), we leverage advances in monolingual feature learning algorithms by replacing the softmax objective with a more efficient noise-contrastive objective (\u00a73.", "startOffset": 18, "endOffset": 36}, {"referenceID": 5, "context": "This idea was introduced by Collobert and Weston (Collobert et al., 2011) where they opti-", "startOffset": 49, "endOffset": 73}, {"referenceID": 7, "context": "However, performing word alignment requires running Giza++ (Och & Ney, 2003) or FastAlign (Dyer et al., 2013) software and training HMM word-alignment models.", "startOffset": 90, "endOffset": 109}, {"referenceID": 0, "context": "For monolingual training data, we use the freely available, pretokenized Wikipedia datasets (Al-Rfou\u2019 et al., 2013).", "startOffset": 92, "endOffset": 115}, {"referenceID": 11, "context": "Unlike the approach of (Klementiev et al., 2012) however, we do not need to perform a word-alignment step first.", "startOffset": 23, "endOffset": 48}, {"referenceID": 11, "context": "(Klementiev et al., 2012) to evaluate their cross-lingual embeddings.", "startOffset": 0, "endOffset": 25}, {"referenceID": 13, "context": "Similar to Klementiev, we induce cross-lingual embeddings for the English-German language pair, and use the induced representations to classify a subset of the English and German sections of the Reuters RCV1/RCV2 multilingual corpora (Lewis et al., 2004) as pertaining to one of four categories: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets).", "startOffset": 234, "endOffset": 254}, {"referenceID": 4, "context": "8 (de2en) reported using the Bilingual Auto-encoder (BAE) model by (Chandar et al., 2014).", "startOffset": 67, "endOffset": 89}, {"referenceID": 4, "context": "2012), Bilingual Auto-encoders (Chandar et al., 2014), and the BiCVM model (Hermann & Blunsom, 2013), on an exact replica of the", "startOffset": 31, "endOffset": 53}], "year": 2016, "abstractText": "We introduce BilBOWA (Bilingual Bag-ofWords without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.", "creator": "LaTeX with hyperref package"}}}