{"id": "1410.4210", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2014", "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets", "abstract": "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the $\\ell_1$ and $\\ell_2$ norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel Two-Layer Feature REduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method---called DPC (DecomPosition of Convex set)---for the nonnegative Lasso problem. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude.", "histories": [["v1", "Wed, 15 Oct 2014 20:08:21 GMT  (391kb,D)", "http://arxiv.org/abs/1410.4210v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jie wang", "jieping ye"], "accepted": true, "id": "1410.4210"}, "pdf": {"name": "1410.4210.pdf", "metadata": {"source": "CRF", "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets", "authors": ["Jie Wang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Basics and Motivation", "text": "In this section we briefly review some basics of the SGL problem. Let us y-RN be the response vector and X-RN-p the matrix of features. With the available group information, the SGL problem [7] ismin \u03b2-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Gg = 1 Xg\u03b2g-Rp12-Rp11-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp12-Rp11-Rp12-Rp11-Rp11-Rp11 Rp11-11 Rp11-Rp11 Rp11-11 Rp11-Rp11-Rp11-Rp12-Rp11 Rp11-Rp12-Rp12-Rp11 Rp12-Rp12-Rp12-12-Rp11 Rp11 Rp12-12-Rp11-12-Rp11 Rp11-11 Rp11-11 Rp11-11-11 Rp11-Rp11-11-11 Rp11-Rp11-Rp11-11-Rp11-Rp11-Rp11-11-Rp11-11-Rp12-Rp12-Rp11-Rp11-11-Rp12-Rp11-Rp11-11-Rp11-11-Rp11-Rp12-Rp11"}, {"heading": "3 The Fenchel\u2019s Dual Problem of SGL", "text": "In Section 3.1, we derive the fennel dual of SGL via Fennel's duality theorem. We then motivate TLFre and outline our approach in Section 3.2. In Section 3.3, we discuss the geometric properties of the fennel dual of SGL and derive the theorem of (\u03bb, \u03b1) that leads to zeros."}, {"heading": "3.1 The Fenchel\u2019s Dual of SGL via Fenchel\u2019s Duality Theorem", "text": "s double problem from SGL = SGL = SGL = SGL = SGL). (5) Theorem 1 (SGL = SGL) is the function f + SGL (SGL = SGL), SGL = SGL (SGL), and T (S) = SGL < w, z > f (W). (5) Theorem 1 (SGL = SGL). (SGL) Let us use SGL (SGL), d) SGL (SGL = SGL), SGL (SGL), SGL (SGL), SGL (SGL), SGL (SGL), SGL (SGL), SGL (SGL), SGL (SGL), SGL (SGL), SGL (SGL), SGL ()"}, {"heading": "3.2 Motivation of the Two-Layer Screening Rules", "text": "The first layer aims to identify the inactive groups, and the second layer is designed to capture the inactive features for the remaining groups. As implied by the name, there are two layers in our method by identifying the following cases by pointing out the following layers: The first layer aims to identify the inactive groups, and the second layer aims to identify the inactive features for the remaining groups, if w 6 = 0, the second layer aims to identify the inactive features for the remaining groups, if w 6 = 0, the second layer aims to identify the following: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o o: o o: o: o o: o o o: o o: o o: o o: o o: o o: o: o o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o: o o: o: o: o: o: o: o o: o o o: o: o: o o: o: o o: o: o: o o o o: o: o o o o: o: o o o o: o: o o o: o: o o o: o: o: o o: o: o o o o: o: o o o o: o: o o: o: o: o o o o: o o: o o o o: o: o o o o: o o o o o: o: o: o o o o o o o: o o o o o: o: o o o o o o: o o o o: o: o o: o o o: o: o o o o o: o: o: o"}, {"heading": "3.3 The Set of Parameter Values Leading to Zero Solutions", "text": "In this section, the geometric properties of the fennel dual of SGL are examined in detail - on the basis of which we can derive the set of parameter values so that the optimum initial solutions are zero. We look at the SGL problem in (3) and (2), respectively, in Section 3.3.1 and 3.3.2."}, {"heading": "3.3.1 The Set of Parameter Values Leading to Zero Solutions of Problem (3)", "text": "Let's look at the SGL problem in (3). For notational convenience, letF\u03b1g = \u03b2 = \u03b2 = \u03b2 \u00b2. Let's assume that S1 (..., GF\u03b1g). Considering the problem (13) [or (20)], we can see that it is the possible amount of the fennel dual of SGL, i.e., we can see that SGL components (n). (4). Considering the problem (13) [or (20)]. We can see that SGL components (n). (4). (4). We can see that SGL components (n.)."}, {"heading": "3.3.2 The Set of Parameter Values Leading to Zero Solutions of Problem (2)", "text": "Theorem 8 implies that the optimal solution \u03b2 1, 2) can be such that the corresponding solution of the problem (2) is actually 0. (Simion) We designate the optimal solution of the problem (2). (Simion 10) For the SGL problem in (2), we leave the solution max1 (2) = maxg 1). (Simion 2 (XTg y). (Then, (i) \u03b2 2). (Ciao 2). (Ciao 1). (Ciao 1). (Ciao 2). (Ciao 2). (Ciao 2). (Ciao 2). (Ciao 2). (Ciao 2). (Ciao 2). (Ciao 2). (Ciao 2). (Ciao 2). (Ciao 2). (Ciao 2)."}, {"heading": "4 The Two-Layer Screening Rules for SGL", "text": "We follow the three steps in Section 3.2 to develop TLFre. In Section 4.1 we give a precise estimate of \u03b8 * (\u03bb, \u03b1) over normal cones [20]. Then we calculate the highest values in (R1 \u0445) and (R2 \u0445) by solving non-convex problems in Section 4.2. We present the TLFre rules in Section 4.3."}, {"heading": "4.1 Estimation of the Dual Optimal Solution", "text": "Due to the geometric property of the dual problem in (13), i.e. we (both) have a very useful characterization of the dual optimal solution via the so-called normal cones [20]. Suggestion 11. [20, 2] For a closed convex set C + Rn and a point w + C, the normal cone to C is defined an w (w). (v) PC (w + v) = w, w. \"\u2212 w.\" (32) Then applies: (i) NC (w) NC (w) = {v: PC (w + v). (w). (w). (w). (w). (w). (w). (iiii) Leave w / C. Then, w = PC (w)."}, {"heading": "4.2 Solving for the Supreme Values via Nonconvex Optimization", "text": "To simplify the notation, we must solve the following optimization problems in (R1) and (R2): (R1), (R2), (1), (1), (1), (1), (1), (1), (1), (1), (2), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1, (1), (1), (1), (1, (1), (1), (1), (1), (1), (1, (1), (1), (1), (1), (1), (1, (1), (1), (1), (1), (1, (1), (1), (1), (1), (, (1), (1), (1), (1), (, (1), (1), (1), (, (1), (1), (1), (, (1), (1), (, (1), (1), (, (1), (1), (1), (1), (, (, (, (1), (1), (), (1), (, (, (), (, (, (, (, (), (, (, (, (,), (1), (, (, (), (), (, (, (), (, (1), (, (, (,), (, (, (,), (1), (, (, (,), (1), (, (, (), (,),"}, {"heading": "4.2.1 The Solution of Problem (54)", "text": "We consider the following equivalent problem of (54): 1 2 (s) g (44) g (44) g (64) g (63) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g (64) g)). We can first derive the necessary optimum conditions in Lemma 13 and then derive the closed form solutions of problems (54) and (56) in Theorem 15.Lemma 13 (56)."}, {"heading": "4.2.2 The Solution of Problem (55)", "text": "Problem (55) can be solved directly via the Cauchy-Schwarz-Inequality. Theorem 16. For problem (55) we have t-gi (44) = | x T-gio\u03b1 (44) | + 1 2-v-g (44). Proof. To simplify the notation, we leave o = o\u03b1 (52), r = 1 2-v-g (44) and t-g = t-g (44). Therefore, the amount can be written in equation (52) so: o = o\u03b1 (52), r = 1-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-"}, {"heading": "4.3 The Proposed Two-Layer Screening Rules", "text": "In order to develop the two-layer screening rules for SGL, we only need to insert the uppermost values s * g (2, 2, 2, 1) and t * gi (2, 2, 2, 1) in (R1) and (R2). We present the TLFre rule as follows. Theorem 17. For the SGL problem in (3), we assume that we are given an integer 0 \u2264 j < J. Let us leave it like this (1), v \u00b2 (1) > \u03bb (1) and s \u00b2 g (1) >. Furthermore, we assume that \u03b2 \u00b2 (j), \u03b1 (1) < J."}, {"heading": "5 Extension to Nonnegative Lasso", "text": "The framework of TLFre is applicable to a large class of sparse models with multiple regularizers. As an example, we extend TLFre to non-negative lasso: min \u03b2-X\u03b2-X\u03b2-X\u03b2-Rp + 1: \u03b2-Rp +, (80) where \u03bb > 0 is the regularization parameter and Rp + is the non-negative orthant of Rp. In Section 5.1 we transform the limitation \u03b2-Rp + into a regularizer and derive the fennel dual of the non-negative lasso problem. Then we motivate the screening method - called DPC, since the key step is to dissect a convex quantity via Fennel's duality theorem - via the CCT conditions in Section 5.2. In Section 5.3 we analyze the geometric properties of the dual problem and derive the set of parameter values leading to zero solutions."}, {"heading": "5.1 The Fenchel\u2019s Dual of Nonnegative Lasso", "text": "Let us see IRp + as an indicator function of R p +. By noting that IRp + = \u03bbIR p + (for each \u03bb > 0). (81) In other words, we are integrating the constraint \u03b2-Rp + into the objective function as an additional regulator. Consequently, the non-negative lasso problem in (81) has two regulators. Thus, much like SGL, we can derive the fennel dual from non-negative lasso via theorem 1. We proceed by following a procedure similar to that in Section 3.1. We find that the non-negative lasso problem in (81) can also be formulated as the dual of non-negative lasso via theorem 1 mentioned in (6)."}, {"heading": "On the other hand, Lemma 7 implies that", "text": "Therefore we have B + Rp \u2212 = C1. From theorem 1 and Lemma 18 we can derive the fennel dual of nonnegative lasso in the following theorem (which is actually the counterpart of theorem 5). Theorem 19. The nonnegative lasso problem is: (i) The fennel dual of nonnegative lasso is represented by: inf.: inf.: inf.: inf.: inf.: inf.: inf: inf: inf: inf: inf: inv: 1: 2: 2: 2: 2: 2: 2: theorem; theorem: < xi: 1: 3: 1, i: 1,.., p: 3: 2: 2: 2: 2: 4: 4: 5: 5; theorem: (82) (ii: 2: 4: 5: 5)."}, {"heading": "5.2 Motivation of the Screening Method via KKT Conditions", "text": "The key to the development of the DPC rule for nonnegative lasso is the KKT condition in (84). We can see that the KKT condition in (84) implies that < xi, \u2082 (\u03bb) > 1, if [\u03b2] i > 0,%,% \u2264 0, if [w] i = 0,}. Therefore, the KKT condition in (84) implies that we can apply the following rule: < xi, \u2082 (\u03bb) > < 1, if [\u03b2] i > 0,%,% \u2264 1, if [\u03b2] i = 0. (85) According to Equation (85) we have the following rule: < xi, \u0432 (\u03bb) > < 1 \u21d2 [\u03b2) i = 0. (R3) Because the method is not known, we can apply the inactive features (R3) - which have 0 coefficients in \u03b2."}, {"heading": "5.3 Geometric Properties of the Fenchel\u2019s Dual of Nonnegative Lasso", "text": "Considering the fennel dual of the nonnegative lasso in (82), we can see that the optimal solution is in fact the projection of y / \u03bb onto the realizable quantity F = {\u03b8: < xi, \u03b8 > \u2264 1, i = 1,..., p}, i.e., if y / \u03bb = PF (y \u03bb). (86) Therefore, if we follow y / \u03bb F, Equation (86) implies that the further y / \u03bb is an inner point of F, then R3 \u0445 implies that \u03b2 (\u03bb) = 0. The next theorem indicates the set of parameter values leading to 0 solutions of the nonnegative lasso. Theorem 20. For the nonnegative lasso problem (81), let us leave circle max = maxi < xi, y >. Then the following statements are equivalent: (i) y-ig-ig \u0445 F, (ii)."}, {"heading": "5.4 The Proposed Screening Rule for Nonnegative Lasso", "text": "We will follow the three steps in Section 5.2 to develop the non-negative lasso screening rule, starting from a region that contains non-negative lasso problems, and since we accept a closed-loop solution that does not have negative lasso problems, we assume that all cases that do not have negative lasso problems will be assumed to have x = argmaxxi problems. < xi, y >, v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception), v (Exception)."}, {"heading": "6 Experiments", "text": "We evaluate TLFre for SGL and DPC for nonnegative lasso in Section 6.1 and 6.2, respectively, on both synthetic and real data sets. To the best of our knowledge, TLFre and DPC are the first screening methods for SGL and nonnegative lasso, respectively."}, {"heading": "6.1 TLFre for SGL", "text": "In order to measure the performance of TLFre, we calculate the repulsion ratios of (L1) and (L2). Specifically, m should be the number of features that have 0 coefficients in the solution. Repulsion ratios of (L1) and (L2) are defined by the index of the groups that are discarded by (L1) and p by the number of inactive characteristics detected by (L2). The repulsion ratios of (L1) and (L2) are defined by r1 = \u2211 g-G ng m and r2 = | p | m, respectively. Furthermore, we specify the acceleration obtained by TLFre, i.e. the ratio of the runtime of the solver without shielding to the runtime of the solver with TLFre. The solver used in this essay comes from SLEP [12]. In order to determine the corresponding values of TLFre and stability by cross-validation, we can choose from the values of Fre (without shielding) with a value of L001 of each solder (without shielding time)."}, {"heading": "6.1.1 Simulation Studies", "text": "We conduct experiments with two synthetic datasets commonly used in literature [26, 36]. The true model is y = X\u03b2 * + 0.01, \u0445 N (0, 1). We create two datasets with 250 x 10000 entries: Synthetic 1 and Synthetic 2. We randomly break down the 10,000 characteristics into 1000 groups. For synthetic 1, the entries of data matrix X are drawn from i.i.d. standard gaussian with paired correlation 0, i.e., corr (xi, xi) = 0. For synthetic 2, the entries of data matrix X are drawn from i.i.i.d. standard gaussian with paired correlation 0.5 | j |, i.e., corr (xi, xi) = 0.5 | i \u2212 j |. To construct \u03b2 values, we first randomly select 1 percent of the groups. Then, for each selected group, we randomly select 2 percent of the characteristics."}, {"heading": "6.1.2 Experiments on Real Data Set", "text": "s Disease Neuroimaging Initiative (ADNI) dataset (http: / / adni.loni.usc.edu /). The data matrix consists of 747 samples with 426040 single nucleotide polymorphisms (SNPs), divided into 94765 groups, the response vectors being the volume of grey matter (GMV) and the volume of white matter (WMV), respectively, the images in the upper left corner of Fig. 3 and Fig. 4 show the diagrams of \u03bbmax1 (\u03bb2) (see episode 10) and the sampled parameter values of \u03b1 and \u03bb. The other images show the repulsion ratios of (L1) and (L2) in blue and red regions, respectively. We can see that almost all inactive groups / features of TLFre are discarded. The repulsion ratios of r1 + r2 are very close to 1st in all cases."}, {"heading": "6.2 DPC for Nonnegative Lasso", "text": "This year, more than ever before in the history of the city, which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country,"}, {"heading": "7 Conclusion", "text": "To our knowledge, TLFre is the first method applicable to sparse models with multiparity-inducing regulators. More importantly, the proposed approach provides a new framework for developing screening methods for complex sparse models with multiplesity-inducing regulators, such as' 1 SVM, which selects both samples and features, fused lasso and tree lasso with more than two regulators. To demonstrate the flexibility of the proposed framework, we are developing the DPC screening rule for the non-negative lasso problem. Experiments on synthetic and real data sets demonstrate the effectiveness and efficiency of TLFre and DPC. We plan to generalize the idea of TLFre to '1 SVM, fuse lasso and tree lasso, which are likely to consist of multiple layers of screening."}, {"heading": "A Sparse-Group Lasso", "text": "The Lagrangian double problem of SGLWe derive the double problem of SGL = > q in (4) using the Lagrangian multiplier method. By introducing an auxiliary variable of SGLZ = y \u2212 G \u00b2 g = 1 Xg\u00dfg, (90) the SGL problem becomes in (3): min \u03b2 12% z \u00b2 2 + 3% G \u00b2 2 + 4% G \u00b2 1: z = y \u2212 G \u00b2 g = 1 Xg\u00dfg. Let the Lagrangian multiplier, the Lagrangian function isL (\u03b2, z \u00b2 G) = 12% z \u00b2 2 + 4% G \u00b2 G \u00b2 1 + 4% G \u00b2 1 + 4% G \u00b2, y \u2212 G \u00b2, y \u2212 G \u00b2 G \u00b2 s = 1 Xg\u00dfg > (91) = 5% G \u00b2 s \u00b2 that we have."}], "references": [{"title": "MLL translocations specify a distinct gene expression profile that distinguishes a unique leukemia", "author": ["S. Armstrong", "J. Staunton", "L. Silverman", "R. Pieters", "M. den Boer", "M. Minden", "S. Sallan", "E. Lander", "T. Golub", "S. Korsmeyer"], "venue": "Nature Genetics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": "Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex Analysis and Nonlinear Optimization, Second Edition", "author": ["J. Borwein", "A. Lewis"], "venue": "Canadian Mathematical Society,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient kernel discriminant analysis via spectral regression", "author": ["D. Cai", "X. He", "J. Han"], "venue": "ICDM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Safe feature elimination in sparse supervised learning", "author": ["L. El Ghaoui", "V. Viallon", "T. Rabbani"], "venue": "Pacific Journal of Optimization, 8:667\u2013698,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J. Tropp"], "venue": "SIAM Review, 53:217\u2013288,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "From convex optimization to nonconvex optimization", "author": ["J.-B. Hiriart-Urruty"], "venue": "necessary and sufficient conditions for global optimality. In Nonsmooth optimization and related topics. Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1988}, {"title": "A note on the Legendre-Fenchel transform of convex composite functions", "author": ["J.-B. Hiriart-Urruty"], "venue": "Nonsmooth Mechanics and Analysis. Springer,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Moreau-Yosida regularization for grouped tree structure learning", "author": ["J. Liu", "J. Ye"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Safe screening with variational inequalities and its application to lasso", "author": ["J. Liu", "Z. Zhao", "J. Wang", "J. Ye"], "venue": "International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Reading digits in nature images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A. Ng"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Safe sample screening for Support Vector Machine", "author": ["K. Ogawa", "Y. Suzuki", "S. Suzumura", "I. Takeuchi"], "venue": "arXiv:1401.6740,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Safe screening of non-support vectors in pathwise SVM computation", "author": ["K. Ogawa", "Y. Suzuki", "I. Takeuchi"], "venue": "ICML,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularized multivariate regression for indentifying master predictors with application to integrative genomics study of breast cancer", "author": ["J. Peng", "J. Zhu", "A. Bergamaschi", "W. Han", "D. Noh", "J. Pollack", "P. Wang"], "venue": "The Annals of Appliced Statistics, 4:53\u201377,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Serum proteomic patterns for detection of prostate cancer", "author": ["E. Petricoin", "D. Ornstein", "C. Paweletz", "A. Ardekani", "P. Hackett", "B. Hitt", "A. Velassco", "C. Trucco", "L. Wiegand", "K. Wood", "C. Simone", "P. Levine", "W. Linehan", "M. Emmert-Buck", "S. Steinberg", "E. Kohn", "L. Liotta"], "venue": "Journal of National Cancer Institute, 94:1576\u20131578,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonlinear Optimization", "author": ["A. Ruszczy\u0144ski"], "venue": "Princeton University Press,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "A simple and efficient algorithm for gene selection using sparse logistic regression", "author": ["S. Shevade", "S. Keerthi"], "venue": "Bioinformatics, 19:2246\u20132253,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "The CMU pose, illumination, and expression database", "author": ["T. Sim", "B. Baker", "M. Bsat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 25:1615\u20131618,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "A Sparse-Group Lasso", "author": ["N. Simon", "J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "C-HiLasso: a collaborative hierarchical sparse modeling framework", "author": ["P. Sprechmann", "I. Ram\u0131\u0301rez", "G. Sapiro", "Y. Eldar"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Regression shringkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B, 58:267\u2013288,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B, 74:245\u2013266,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Machine learning methods in the cocomputation biology of cancer", "author": ["M. Vidyasagar"], "venue": "Proceedings of the Royal Society A,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse group lasso and high dimensional multinomial classification", "author": ["M. Vincent", "N. Hansen"], "venue": "Computational Statistics and Data Analysis, 71:771\u2013786,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Scaling svm and least absolute deviations via exact data reduction", "author": ["J. Wang", "P. Wonka", "J. Ye"], "venue": "International Conference on Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"], "venue": "Advances in neural information processing systems,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting the clinical status of human breast cancer by using gene expression profiles", "author": ["M. West", "C. Blanchette", "H. Dressman", "E. Huang", "S. Ishida", "R. Spang", "H. Zuzan", "J. Olson", "J. Marks", "J. Nevins"], "venue": "Proceedings of the National Academy of Sciences, 98:11462\u201311467,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast lasso screening tests based on correlations", "author": ["Z.J. Xiang", "P.J. Ramadge"], "venue": "IEEE ICASSP,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Linguistic structured sparsity in text categorization", "author": ["D. Yogatama", "N. Smith"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society Series B, 68:49\u201367,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society Series B, 67:301\u2013320,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "Sparse-Group Lasso (SGL) [7, 23] is a powerful regression technique in identifying important groups and features simultaneously.", "startOffset": 25, "endOffset": 32}, {"referenceID": 23, "context": "To yield sparsity at both group and individual feature levels, SGL combines the Lasso [25] and group Lasso [35] penalties.", "startOffset": 86, "endOffset": 90}, {"referenceID": 32, "context": "To yield sparsity at both group and individual feature levels, SGL combines the Lasso [25] and group Lasso [35] penalties.", "startOffset": 107, "endOffset": 111}, {"referenceID": 25, "context": "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.", "startOffset": 124, "endOffset": 132}, {"referenceID": 31, "context": "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.", "startOffset": 124, "endOffset": 132}, {"referenceID": 22, "context": "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.", "startOffset": 173, "endOffset": 177}, {"referenceID": 21, "context": "Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28].", "startOffset": 80, "endOffset": 95}, {"referenceID": 11, "context": "Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28].", "startOffset": 80, "endOffset": 95}, {"referenceID": 26, "context": "Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28].", "startOffset": 80, "endOffset": 95}, {"referenceID": 5, "context": "[6] proposed a promising feature reduction method, called SAFE screening, to screen out the so-called inactive features, which have zero coefficients in the solution, from the optimization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 128, "endOffset": 144}, {"referenceID": 12, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 128, "endOffset": 144}, {"referenceID": 24, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 128, "endOffset": 144}, {"referenceID": 30, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 128, "endOffset": 144}, {"referenceID": 28, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 158, "endOffset": 170}, {"referenceID": 24, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 158, "endOffset": 170}, {"referenceID": 5, "context": "SAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution.", "startOffset": 5, "endOffset": 8}, {"referenceID": 30, "context": "SAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution.", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "SAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "However, heuristic feature screening methods like Strong Rule [26] may mistakenly discard features which have nonzero coefficients in the solution.", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30].", "startOffset": 144, "endOffset": 152}, {"referenceID": 27, "context": "More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30].", "startOffset": 144, "endOffset": 152}, {"referenceID": 27, "context": "More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30].", "startOffset": 161, "endOffset": 165}, {"referenceID": 14, "context": "As a promising data reduction tool, exact feature/sample screening would be of great practical importance because they can effectively reduce the data size without sacrificing the optimality [16].", "startOffset": 191, "endOffset": 195}, {"referenceID": 3, "context": "By the Lagrangian multipliers method [4] (see the supplement), the dual problem of SGL is", "startOffset": 37, "endOffset": 40}, {"referenceID": 27, "context": "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 12, "context": "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 28, "context": "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 5, "context": "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 2, "context": "5 in [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "[2] Let h, g \u2208 \u03930(R).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Let f1, \u00b7 \u00b7 \u00b7 , fk \u2208 \u03930(R).", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[Fenchel-Young inequality] [3] Any point z \u2208 Rn and w in the domain of a function h : Rn \u2192 (\u2212\u221e,\u221e] satisfy the inequality h(w) + h\u2217(z) \u2265 \u3008w, z\u3009.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "(15) are the so-called KKT conditions [4] and can also be obtained by the Lagrangian multiplier method (see A.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "[2] Let C1 and C2 be nonempty subsets of Rn.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Inspired by the SAFE rules [6], we can first estimate a region \u0398 containing \u03b8\u2217(\u03bb, \u03b1).", "startOffset": 27, "endOffset": 30}, {"referenceID": 18, "context": "1, we give an accurate estimation of \u03b8\u2217(\u03bb, \u03b1) via normal cones [20].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": ", \u03b8\u2217(\u03bb, \u03b1) = PF\u03b1(y/\u03bb), we have a very useful characterization of the dual optimal solution via the so-called normal cones [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "[20, 2] For a closed convex set C \u2208 Rn and a point w \u2208 C, the normal cone to C at w is defined by NC(w) = {v : \u3008v,w\u2032 \u2212w\u3009 \u2264 0, \u2200w\u2032 \u2208 C}.", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "[20, 2] For a closed convex set C \u2208 Rn and a point w \u2208 C, the normal cone to C at w is defined by NC(w) = {v : \u3008v,w\u2032 \u2212w\u3009 \u2264 0, \u2200w\u2032 \u2208 C}.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[9] Suppose that h \u2208 \u03930 and C is a nonempty closed convex set.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The solver used in this paper is from SLEP [12].", "startOffset": 43, "endOffset": 47}, {"referenceID": 24, "context": "1 Simulation Studies We perform experiments on two synthetic data sets that are commonly used in the literature [26, 36].", "startOffset": 112, "endOffset": 120}, {"referenceID": 33, "context": "1 Simulation Studies We perform experiments on two synthetic data sets that are commonly used in the literature [26, 36].", "startOffset": 112, "endOffset": 120}, {"referenceID": 10, "context": "01 by (a): the solver [12] without screening; (b): the solver combined with TLFre.", "startOffset": 22, "endOffset": 26}, {"referenceID": 6, "context": ", G, which can be efficiently computed by the power method [8].", "startOffset": 59, "endOffset": 62}, {"referenceID": 10, "context": "We integrate DPC with the solver [12] to solve the nonnegative Lasso problem along a sequence of 100 parameter values of \u03bb equally spaced on the logarithmic scale of \u03bb/\u03bbmax from 1.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "01 by (a): the solver [12] without screening; (b): the solver combined with TLFre.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "a) Breast Cancer data set [32, 21]: this data set contains 7129 gene expression values of 44 tumor samples (thus the data matrix X is of 44 \u00d7 7129).", "startOffset": 26, "endOffset": 34}, {"referenceID": 19, "context": "a) Breast Cancer data set [32, 21]: this data set contains 7129 gene expression values of 44 tumor samples (thus the data matrix X is of 44 \u00d7 7129).", "startOffset": 26, "endOffset": 34}, {"referenceID": 0, "context": "b) Leukemia data set [1]: this data set contains 11225 gene expression values of 52 samples (X \u2208 R52\u00d711225).", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "c) Prostate Cancer data set [19]: this data set contains 15154 measurements of 132 patients (X \u2208 R132\u00d715154).", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "d) PIE face image data set [22, 5]: this data set contains 11554 gray face images (each has 32\u00d7 32 pixels) of 68 people, taken under different poses, illumination conditions and expressions.", "startOffset": 27, "endOffset": 34}, {"referenceID": 4, "context": "d) PIE face image data set [22, 5]: this data set contains 11554 gray face images (each has 32\u00d7 32 pixels) of 68 people, taken under different poses, illumination conditions and expressions.", "startOffset": 27, "endOffset": 34}, {"referenceID": 9, "context": "e) MNIST handwritten digit data set [11]: this data set contains grey images of scanned handwritten digits (each has 28 \u00d7 28 pixels).", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "f) Street View House Number (SVHN) data set [15]: this data set contains color images of street view house numbers (each has 32 \u00d7 32 pixels), including 73257 images for training and 26032 for testing.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "01 by (a): the solver [12] without screening; (b): the solver combined with DPC.", "startOffset": 22, "endOffset": 26}], "year": 2014, "abstractText": "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the `1 and `2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method\u2014called DPC (decomposition of convex set)\u2014for the nonnegative Lasso problem. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude.", "creator": "LaTeX with hyperref package"}}}