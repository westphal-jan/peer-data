{"id": "1410.4510", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2014", "title": "Graph-Sparse LDA: A Topic Model with Structured Sparsity", "abstract": "Originally designed to model text, topic modeling has become a powerful tool for uncovering latent structure in domains including medicine, finance, and vision. The goals for the model vary depending on the application: in some cases, the discovered topics may be used for prediction or some other downstream task. In other cases, the content of the topic itself may be of intrinsic scientific interest.", "histories": [["v1", "Thu, 16 Oct 2014 17:35:31 GMT  (62kb)", "https://arxiv.org/abs/1410.4510v1", null], ["v2", "Fri, 21 Nov 2014 16:38:59 GMT  (62kb)", "http://arxiv.org/abs/1410.4510v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["finale doshi-velez", "byron c wallace", "ryan adams"], "accepted": true, "id": "1410.4510"}, "pdf": {"name": "1410.4510.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ryan P. Adams"], "emails": ["finale@seas.harvard.edu", "byron.wallace@utexas.edu", "rpa@seas.harvard.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 0.45 10v2 [st at.M L] 21 Unfortunately, due to the high dimensionality of the underlying space, the discovered topics are often difficult to interpret even using modern sparse techniques. To improve the interpretability of the topics, we introduce Graph-Sparse LDA, a hierarchical theme model that harnesses knowledge of the relationships between words (e.g. in the form of an ontology). In our model, the topics are summarized by a few latent conceptual words from the underlying diagram that explain the observed words. Graph-Sparse LDA provides sparse, interpretable summaries of two real biomedical datasets, while complying with state-of-the-art prediction power."}, {"heading": "1 Introduction", "text": "This year, it has come to the point where there is only one occasion when there is a scandal, and that is when there is a scandal."}, {"heading": "2 Graph-Sparse LDA", "text": "In this paper, our data are documents modeled with the \"bag of words,\" that is, we have a common explanation for the topic of HIV. Let's derive the data X from the number of vocabulary words of each vocabulary in the vocabulary of each vocabulary. (1) The standard model LDA [1] represents the following generative process for the words that comprise each vocabulary (1). (2) The number of vocabulary words in the vocabulary of the vocabulary B is the number of vocabulary words in the vocabulary. (2) The number of vocabulary words in the vocabulary B is the number of vocabulary vocabularies in the vocabulary. (2) The lines of vocabulary in the vocabulary B are the documentary-specific distributions over the topics, and the K-V matrix A represents the distribution of each topic over the words. The notation refers to the line B of Bth and Bn"}, {"heading": "3 Inference", "text": "We describe a blocked Gibbs procedure for sample B, B, A, A, A, and P, as well as an additional Metropolis-Hastings (MH) procedure that helps the sampler move toward more economical topic-concept word matrices A. Specifically, our MH application distribution is designed to favor proposals of new A \"and P\" so that overall expectation does not change significantly. To our knowledge, MCMC, which uses traits that lead to near-constant probability to promote large changes in the previous one, is a new approach. We first describe how to take up instantial parameters of the graph-sparse LDA model, and then describe how we work on new topics."}, {"heading": "3.1 Blocked Gibbs Sampling", "text": "The first, CNKV counts how often the word \"v\" is associated with the second, CKV V counts how often the word \"v\" is associated with the subject \"k\" is associated with the subject \"k\" is associated with the subject \"k.\" The second, CKV V counts how often the word \"k\" is associated with the subject \"k.\" The second, CKV V, is how often the word \"k\" is associated with the subject \"k\" is associated with the subject \"k.\""}, {"heading": "3.2 MH Moves for Improved Sparsity", "text": "Remember that one of our modeling goals is to identify a small, interpretable group of conceptual words in each topic (A). To this end, we have put a sparsity-inducing approach ahead of onA. While the Gibbs approach is mathematically simple upwards, it is often not possible to give us the desired thrift in A (A). The mixture is slow because the only time we put is when no number of W \"s is assigned to the topic K. If there are many documents, it is unlikely that we will achieve the desired thrift in A (A), and therefore the sampler is slow to address the topic of the word matrix A.1We present an MH approach to facilitate the movement of the theme-word matrix A toward greater thrift through joint movements on A and P.\" Given a proposed distribution Q (A), P \"| A\" | A, P), the acceptance rate for MH is given."}, {"heading": "3.3 Adding and Deleting Topics", "text": "To propose new topics, we first select a random document n. We propose a new A \"k\" from the previous one and suggest that B \"nk\" = 1. Finally, we propose a new B \"n\" dirichlet (\u03b1B (B \"n\" \u2211 w [CNKV] n: w). The probability of acceptance for adding the new topic Ak \"is given by \u03b1add = 1\" p (Xn | B \"n, A,\" P) p (B \"n) \u03b3A Np (Xn | Bn, A, P) p (Bn), whereby the probability of adding the new topic Ak\" is determined by the addition of exactly one new topic in the IBP."}, {"heading": "4 Results", "text": "In fact, it is such that the majority of them are able to survive themselves without a process occurring in which a process occurs in which a process occurs in which a process occurs in which a process occurs in which a process occurs in which a process occurs in which a process occurs in which a process occurs in which a process occurs in which a process occurs in which a process occurs in which a process occurs in which a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process,"}, {"heading": "5 Discussion and Related Work", "text": "This year is the highest in the history of the country."}, {"heading": "6 Conclusions", "text": "Topic models have revolutionized prediction and classification models in many domains, and many scientists are now trying to use them to discover structure from their data. However, prediction is not enough for these applications: Scientists want to be able to understand the structure to formulate new theories. At the same time, structured knowledge bases often exist for scientific areas; these are information resources that capture a wealth of expertise. In this paper, we have proposed a model that uses such resources to achieve the stated goal of identifying interpretable topics. More specifically, we have described a novel Bayesian non-parametric model, Graph-Sparse LDA, which uses existing controlled vocabulary structures to induce interpretable topics. The Bayesian non-parametric aspect of the model allows us to discover the number of topics in our dataset. The use of ontological knowledge allows us to uncover sparse groups of terms that provide preametric topics that can be summarized hierarchically."}, {"heading": "Acknowledgments", "text": "We are grateful to Isaac Kohane and the i2b2 team at Boston Children's Hospital for providing us with the autism data and feedback on the GS-LDA model as a data mining tool."}], "references": [{"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Probabilistic topic models", "author": ["M. Steyvers", "T. Griffiths"], "venue": "Handbook of latent semantic analysis, vol. 427, no. 7, pp. 424\u2013440, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM, vol. 55, no. 4, pp. 77\u201384, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 101, pp. 5228\u20135235, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR, vol. 2, pp. 524\u2013531, 2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Human action recognition by semilatent topic models", "author": ["Y. Wang", "G. Mori"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 10, pp. 1762\u20131774, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Topic models for mortality modeling in intensive care units", "author": ["M. Ghassemi", "T. Naumann", "R. Joshi", "A. Rumshisky"], "venue": "ICML 2012 Machine Learning for Clinical Data Analysis Workshop, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Latent IBP compound Dirichlet allocation", "author": ["C. Archambeau", "B. Lakshminarayanan", "G. Bouchard"], "venue": "NIPS Bayesian Nonparametrics Workshop, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "The IBP compound Dirichlet process and its application to focused topic modeling", "author": ["S. Williamson", "C. Wang", "K.A. Heller", "D.M. Blei"], "venue": "ICML, pp. 1151\u20131158, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse additive generative models of text", "author": ["J. Eisenstein", "A. Ahmed", "E.P. Xing"], "venue": "ICML, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "The unified medical language system (UMLS): integrating biomedical terminology", "author": ["O. Bodenreider"], "venue": "Nucleic acids research, vol. 32, pp. D267\u2013D270, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Medical subject headings (MeSH)", "author": ["C.E. Lipscomb"], "venue": "Bull Med Libr Assoc., 2000. 88(3): 265266.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "The Indian buffet process: An introduction and review", "author": ["T. Griffiths", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 1185\u20131224, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process", "author": ["C. Wang", "D. Blei"], "venue": "Advances in Neural Information Processing Systems 22 (Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, eds.), pp. 1982\u20131989, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "Comorbidity clusters in autism spectrum disorders: An electronic health record time-series analysis", "author": ["F. Doshi-Velez", "Y. Ge", "I. Kohane"], "venue": "Pediatrics, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Effect of clinical guidelines on medical practice: a systematic review of rigorous evaluations", "author": ["J.M. Grimshaw", "I.T. Russell"], "venue": "The Lancet, vol. 342, no. 8883, pp. 1317\u20131322, 1993. 11", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "Reducing workload in systematic review preparation using automated citation classification", "author": ["A.M. Cohen", "W.R. Hersh", "K. Peterson", "P.-Y. Yen"], "venue": "Journal of the American Medical Informatics Association, vol. 13, no. 2, pp. 206\u2013219, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Active learning for biomedical citation screening", "author": ["B.C. Wallace", "K. Small", "C.E. Brodley", "T.A. Trikalinos"], "venue": "KDD, pp. 173\u2013182, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J. Chang", "J.L. Boyd-Graber", "S. Gerrish", "C. Wang", "D.M. Blei"], "venue": "NIPS, pp. 288\u2013296, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality", "author": ["J.H. Lau", "D. Newman", "T. Baldwin"], "venue": "14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic evaluation of topic coherence", "author": ["D. Newman", "J.H. Lau", "K. Grieser", "T. Baldwin"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT \u201910, pp. 100\u2013108, Association for Computational Linguistics, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing semantic coherence in topic models", "author": ["D. Mimno", "H. Wallach", "E. Talley", "M. Leenders", "A. McCallum"], "venue": "EMNLP, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan"], "venue": "Journal of the ACM, vol. 57, no. 2, pp. 7:1\u20137:30, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Topic modeling with nonparametric Markov tree", "author": ["H. Chen", "D.B. Dunson", "L. Carin"], "venue": "ICML, pp. 377\u2013384, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient tree-based topic modeling", "author": ["Y. Hu", "J. Boyd-Graber"], "venue": "ACL, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Pachinko allocation: DAG-structured mixture models of topic correlations", "author": ["W. Li", "A. McCallum"], "venue": "ICML, pp. 577\u2013584, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Concept modeling with superwords.", "author": ["K. El-Arini", "E.B. Fox", "C. Guestrin"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Hiding a semantic hierarchy in a Markov model", "author": ["S. Abney", "M. Light"], "venue": "Workshop on Unsupervised Learning in Natural Language Processing, pp. 1\u20138, 1999.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "A topic model for word sense disambiguation", "author": ["J.L. Boyd-Graber", "D.M. Blei", "X. Zhu"], "venue": "EMNLP-CoNLL, pp. 1024\u20131033, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Tree labeled LDA: A hierarchical model for web summaries", "author": ["A. Slutsky", "X. Hu", "Y. An"], "venue": "IEEE International Conference on Big Data, pp. 134\u2013140, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Incorporating domain knowledge into topic modeling via Dirichlet forest priors", "author": ["D. Andrzejewski", "X. Zhu", "M. Craven"], "venue": "ICML, pp. 25\u201332, 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchically supervised latent Dirichlet allocation", "author": ["A.J. Perotte", "F. Wood", "N. Elhadad", "N. Bartlett"], "venue": "NIPS, pp. 2609\u20132617, 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Wordnet: A lexical database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM, vol. 38, pp. 39\u201341, 1995. 12", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "Probabilistic topic models [1, 2, 3] were originally developed to discover latent structure in unorganized text corpora, but these models have been generalized to provide a powerful and flexible framework for uncovering structure in a variety of domains including medicine, finance, and vision.", "startOffset": 27, "endOffset": 36}, {"referenceID": 1, "context": "Probabilistic topic models [1, 2, 3] were originally developed to discover latent structure in unorganized text corpora, but these models have been generalized to provide a powerful and flexible framework for uncovering structure in a variety of domains including medicine, finance, and vision.", "startOffset": 27, "endOffset": 36}, {"referenceID": 2, "context": "Probabilistic topic models [1, 2, 3] were originally developed to discover latent structure in unorganized text corpora, but these models have been generalized to provide a powerful and flexible framework for uncovering structure in a variety of domains including medicine, finance, and vision.", "startOffset": 27, "endOffset": 36}, {"referenceID": 0, "context": "In the popular Latent Dirichlet Allocation (LDA) [1] model, topics are distributions over the words in the vocabulary, and documents can then be summarized by the mixture of topics they contain.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "LDA has been applied to diverse applications such as finding scientific topics in articles [4], classifying images [5], and recognizing human actions [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "LDA has been applied to diverse applications such as finding scientific topics in articles [4], classifying images [5], and recognizing human actions [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "LDA has been applied to diverse applications such as finding scientific topics in articles [4], classifying images [5], and recognizing human actions [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": ", [7]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "Sparse topic models [8, 9, 10] offer a partial solution to this problem by enforcing the constraint that many of the word probabilities for a given topic should be zero.", "startOffset": 20, "endOffset": 30}, {"referenceID": 8, "context": "Sparse topic models [8, 9, 10] offer a partial solution to this problem by enforcing the constraint that many of the word probabilities for a given topic should be zero.", "startOffset": 20, "endOffset": 30}, {"referenceID": 9, "context": "Sparse topic models [8, 9, 10] offer a partial solution to this problem by enforcing the constraint that many of the word probabilities for a given topic should be zero.", "startOffset": 20, "endOffset": 30}, {"referenceID": 10, "context": "For example, diseases are organized into billing hierarchies, and clinical concepts are related by directed acyclic graphs (DAGs) [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "publications are organized in a hierarchy known as MeSH [12]; searching with MeSH terms is standard practice for biomedical literature retrieval tasks.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "For this we use a diagnosis hierarchy [11] to recover clinically relevant subtypes described by a small set of concepts.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "The second is a corpus of biomedical abstracts annotated with hierarchicallystructured Medical Subject Headings (MeSH) [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "In both cases, the topic models found by Graph-Sparse LDA have the same or better predictive performance as a state-of-the-art sparse topic model (Latent IBP compound Dirichlet Allocation [8]) while providing much sparser topic descriptions.", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "The standard LDA model [1] posits the following generative process for the words win comprising each document (data instance) in X :", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Our Bayesian nonparametric model, Graph-Sparse LDA, builds upon a recent nonparametric extension of LDA, Latent IBP compound Dirichlet Allocation (LIDA) [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 12, "context": "where \u2299 is the element-wise Hadamard product and IBP is the Indian Buffet Process [13].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "The priors over the document-topic and topic-concept matrices B and A (and their respective masks B\u0304 and \u0100) follow those in LIDA [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Document-Topic Assignments B and B\u0304: Given the count tensor CNKV , we can sample the sparsity mask B\u0304 by marginalizing out B and the \u03c0k using the formula derived in [8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "In each case we compare our model with the state-of-theart Bayesian nonparametric topic modeling approach LIDA [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "We focus on LIDA because it subsumes two other popular sparse topic models, the focused topic model [9] and sparse topic model [14], and because the proposed model is a generalization of LIDA.", "startOffset": 100, "endOffset": 103}, {"referenceID": 13, "context": "We focus on LIDA because it subsumes two other popular sparse topic models, the focused topic model [9] and sparse topic model [14], and because the proposed model is a generalization of LIDA.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "2 Diagnoses are organized in a tree-structured hierarchy known as ICD-9CM [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "This topic\u2014which shows a connection between the more severe form of ASD, intellectual disability, and epilepsy\u2014as well as the other topics, matched recently published clinical results on ASD subtypes [16].", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "Medical Subject Headings for Biomedical Literature The National Library of Medicine maintains a controlled structured vocabulary of Medical Subject Headings (MeSH) [12].", "startOffset": 164, "endOffset": 168}, {"referenceID": 15, "context": "For example, when conducting a systematic review (SR) [17], one looks to summarize the totality of the published evidence pertaining to a precise clinical question.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "for reducing the labor involved in this process have therefore been investigated [18, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 17, "context": "for reducing the labor involved in this process have therefore been investigated [18, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 16, "context": "We consider a dataset of 1218 documents annotated with 5347 unique MeSH terms (23 average terms per document) that were screened for a systematic review of the effects of calcium-channel blocker (CCB) drugs [18].", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": "Topic models [1, 2] have gained wide popularity as a flexible framework for uncovering latent structure in corpora.", "startOffset": 13, "endOffset": 19}, {"referenceID": 1, "context": "Topic models [1, 2] have gained wide popularity as a flexible framework for uncovering latent structure in corpora.", "startOffset": 13, "endOffset": 19}, {"referenceID": 18, "context": "[20] introduced the idea of \u201cintrusion detection\u201d where they hypothesized that a more coherent, or interpretable, topic would be one where a human annotator would be able to identify an inserted \u201cintruder\u201d word among the top 5 words in a topic; [21] automated this process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] introduced the idea of \u201cintrusion detection\u201d where they hypothesized that a more coherent, or interpretable, topic would be one where a human annotator would be able to identify an inserted \u201cintruder\u201d word among the top 5 words in a topic; [21] automated this process.", "startOffset": 245, "endOffset": 249}, {"referenceID": 20, "context": "[22] and [23] developed measures of topic coherence that strongly correlated with human annotations of topic quality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] and [23] developed measures of topic coherence that strongly correlated with human annotations of topic quality.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "For example, [24] use a nested Chinese Restaurant Process to learn hierarchies of topics where subtopics are more specific than their parents.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "[25] expand on this idea with a nonparametric Markov model that allows a subtopic to have multiple parents.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] develop inference techniques for sparse versions of these tree-structured topic models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Learned hierarchies have also been used to capture correlations between topics, such as [27].", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "Among the fully unsupervised approaches, the closest to our work is the super-word concept modeling of [28], which uses a nested Beta process to describe a document with a sparse set of super-words, or concepts, each of which are associated with a sparse set of words.", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "Early work by [29] used hierarchies for word-sense disambiguation in n-gram tuples.", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "This idea was later incorporated into a topic modeling context by [30].", "startOffset": 66, "endOffset": 70}, {"referenceID": 29, "context": "[31] consider representing the content of website summaries via a hierarchical model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] use Dirichlet forest priors to enforce expert-provided \u201cmust be in same topic\u201d and \u201ccannot be in same topic\u201d constraints between words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Finally, [33] propose a hierarchically supervised LDA model where there is a hierarchy on the document labels (rather than on the vocabulary).", "startOffset": 9, "endOffset": 13}, {"referenceID": 32, "context": "While we have focused on controlled vocabularies in the biomedical domain, this approach could be more generally applied to text corpora using standard hierarchies such as WordNet [34].", "startOffset": 180, "endOffset": 184}], "year": 2014, "abstractText": "Originally designed to model text, topic modeling has become a powerful tool for uncovering latent structure in domains including medicine, finance, and vision. The goals for the model vary depending on the application: in some cases, the discovered topics may be used for prediction or some other downstream task. In other cases, the content of the topic itself may be of intrinsic scientific interest. Unfortunately, even using modern sparse techniques, the discovered topics are often difficult to interpret due to the high dimensionality of the underlying space. To improve topic interpretability, we introduce Graph-Sparse LDA, a hierarchical topic model that leverages knowledge of relationships between words (e.g., as encoded by an ontology). In our model, topics are summarized by a few latent concept-words from the underlying graph that explain the observed words. Graph-Sparse LDA recovers sparse, interpretable summaries on two real-world biomedical datasets while matching state-of-the-art prediction performance.", "creator": "LaTeX with hyperref package"}}}