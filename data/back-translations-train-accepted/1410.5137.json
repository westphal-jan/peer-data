{"id": "1410.5137", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2014", "title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation", "abstract": "The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard $L_0$ constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. We also extend our analysis to large family of \"fully corrective methods\" that includes two-stage and partial hard-thresholding algorithms. We show that our results hold for the problem of sparse regression, as well as low-rank matrix recovery.", "histories": [["v1", "Mon, 20 Oct 2014 02:29:27 GMT  (158kb,D)", "https://arxiv.org/abs/1410.5137v1", "20 pages, 3 figures, To appear in the proceedings of the 28th Annual Conference on Neural Information Processing Systems, NIPS 2014"], ["v2", "Tue, 21 Oct 2014 08:45:56 GMT  (79kb,D)", "http://arxiv.org/abs/1410.5137v2", "20 pages, 3 figures, To appear in the proceedings of the 28th Annual Conference on Neural Information Processing Systems, NIPS 2014"]], "COMMENTS": "20 pages, 3 figures, To appear in the proceedings of the 28th Annual Conference on Neural Information Processing Systems, NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["prateek jain 0002", "ambuj tewari", "purushottam kar"], "accepted": true, "id": "1410.5137"}, "pdf": {"name": "1410.5137.pdf", "metadata": {"source": "CRF", "title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation", "authors": ["Prateek Jain", "Ambuj Tewari", "Purushottam Kar"], "emails": ["prajain@microsoft.com,", "t-purkar@microsoft.com,", "tewaria@umich.edu"], "sections": [{"heading": "1 Introduction", "text": "As a rule, it is not possible to consistently estimate the parameters in such a situation. Consequently, a rich working group has focused on models that meet specific structural assumptions such as economy or low structure. However, the question of efficient estimation is confronted with feasibility issues, since consistent estimation routines are often the solution to NP problems."}, {"heading": "1.1 Insufficiency of RIP based Guarantees for M-estimation", "text": "As mentioned above, PGD / IHT-style methods are very popular in the literature for sparse recovery attempts and multiple algorithms, including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR (\") [17]. However, analysis of these algorithms has traditionally been limited to settings that meet the Restricted Isometry Property (RIP) or Incoherence Property. As the discussion below shows, these analyses are inaccessible to high-dimensional statistical estimation problems.Any existing results analyzing these methods require the conditional number of the loss function, limited to sparse vectors, to being smaller than a universal constant."}, {"heading": "1.2 Overview of Results", "text": "Our main contribution in this paper is an analysis of the PGD / IHT-like methods in statistical settings. Our limits are narrow, reaching known minmax lower limits [20] and considering them arbitrarily differentiable, possibly even non-convex functions. Our results apply even if the underlying number of conditions is arbitrarily large and requires only the function to fulfill RSC / RSS conditions. In particular, this shows that these iterative methods are actually applicable to statistical settings, a result that has escaped all previous work. Our first result shows that the PGD / IHT methods achieve global convergence when applied with a relaxed projection step. Formally, if the optimal parameter s-sparse and the problem RSC and RSS constraints \u03b1 and L meet each other (see section 2), then PGD methods show global convergence as long as this convergence occurs, as they use a projection 2T (where an IHT)."}, {"heading": "1.3 Organization of the Paper", "text": "Section 2 specifies the notation and problem definition. Section 3 introduces the PGD / IHT algorithm we are investigating and proves that the method, assuming the RSC / RSS property, guarantees recovery. We also generalize our guarantees to the low-level matrix regression problem. Section 4 then provides clear sample complexity limits and statistical guarantees for the PGD / IHT estimators. Section 5 broadens our analysis to include a broad family of \"fully corrective\" compressive hard threshold methods, which include compressive sensor algorithms that include the so-called two-step hard threshold algorithms and partial hard threshold algorithms, which yield similar results."}, {"heading": "2 Problem Setup and Notations", "text": "Considering the data points X = [X1,. >, Xn] T, where Xi-Rp and the target Y = [Y1,. \u00b7, Yn] T, where Yi-Rp, the goal is to calculate an s-Rank-Rp-Rp, and the target Y = [Y1,. \u00b7, Yn] T, where Yi-Rp, the goal is to calculate an s-Rank-Rp-Rp-Rp-Rp. (1) Typically, f-Rank can be considered an empirical risk function, i.e. f (\u03b8) = 1n-Rank i '(< Xi, \u03b8 >, Yi) for a loss function \"(see examples in Section 4). However, for our analysis of the PGD and other algorithms, we need assume no property other than differentiability and the following two RSC and RSS properties. Definition 1 (RSS property) is low (S-RSS property)."}, {"heading": "3 Iterative Hard-thresholding Method", "text": "In this section, we examine the popular projected gradients descending (a.k.a iterative hard threshold) procedures in the case of feasibility, with the number of sparse vectors (see algorithm 1 for pseudocode).In this case, the projection operator Ps (z) can be efficiently implemented by projecting z onto the group of s-sparse vectors by selecting the largest elements (in the order of magnitude) of z. Standard projection property implies that Ps (z) \u2212 z \u00b2 22 is selected for all types of s-sparse vectors. However, it turns out that we can demonstrate a significantly stronger property of hard thresholds in the case when the standard projection properties of St (z) \u2212 z \u00b2 s property is crucial to the analysis of IHT and is formalized."}, {"heading": "3.1 Low-rank Matrix Regression", "text": "We generalize our previous analysis to a projected gradient descend method (PGD) for low-grade matrix regression. Formally speaking, we can solve the following problem by using SVD i.e.PMs (W) = U.S. \u2212 VI, rank (W) \u2264 VI s. \u2212 VI, rank (W) \u2264 V T is the singular value of the decomposition of W. Us, Vs are the uppermost s singular vectors (left and right) of W and II is the diagonal matrix of the uppermost s singular values of W. To continue, let us first note a property of the above projection similar to Lemma 1. Lemma 2. Let us leave W, Rp1 \u00d7 p2, a ranged matrix of W and let us leave p1."}, {"heading": "4 High Dimensional Statistical Estimation", "text": "This section describes how the results of the previous section can be used to provide guarantees for IHT-like techniques in a variety of statistical estimation problems. We will first present a generic convergence result and then specialize it in various settings. (Suppose we have a sample of data points Z1: n and a loss function L (\u03b8; Z1: n), which depends on a parameter and the sample. (Appendix B for a proof) Theorem 3. Let us replace any s-sparse vector. (Souose L (\u03b8; Z1: n) is differentiable and meets the RSC and RSS at the thriftiness level s s s s s s s s with parameters. (Appendix B for a proof.) Theorem 3. Let us be prepared that all s-sparse vector. (Suppose L; Z1: n) is differentiable and meets the RSC and RSS at the thriftiness level s s s s s s s s s s s s s s with parameters."}, {"heading": "5 Fully-corrective Methods", "text": "In this section, we will examine a variety of \"full correction methods,\" which fully minimize the optimization goal by supporting the current iteration. To this end, we will first prove a basic theorem for full correction methods that formalizes the intuition that in such methods, a large functional value should imply a large gradient even with a sparse iteration. This result is similar to Lemma 1 of [17], but applies under RSC / RSS conditions (and not under the RIP state as in [17]), as well as to the general loss functions. Lemma 3. Consider a function f with RSC parameters given by L2s + s parameters (f) = L and RSS parameters given by \u03b12s + s conditions (f)."}, {"heading": "5.1 Two-stage Hard Thresholding Methods", "text": "Here we focus on a family of two-step full correction methods that include popular compression scanning algorithms such as CoSaMP and Subspace Pursuit (see algorithm 2 for pseudocode). \u2212 \u03b2These algorithms have so far only been objectively analyzed under RIP conditions for the smallest squares. Using our analytical framework developed in the previous sections, we present a generic RSC / RSS-based analysis for general two-step methods for arbitrary loss functions. Our analysis will use the following key observation: The hard threshold step in two-step methods does not increase the objective function by a lot.Lemma 4. Leave Zt [n] and | Zt | \u2264 q. Leave \u03b2 t = arg min\u03b2, supp (\u03b2) t = Pq (\u03b2t) and V = property (\u03b2t)."}, {"heading": "5.2 Partial Hard Thresholding Methods", "text": "It is well known that this family provides the best known RIP guarantees in compressing sensors, but the proof is limited to the RIP threshold."}, {"heading": "6 Experiments", "text": "We performed simulations on high-dimensional sparse linear vector regression problems to verify our predictions = 350 parent values. Our experiments show that hard thresholds and projected gradients not only provide recovery in stochastic environments, but also much more scalable routines for the same techniques as those described in the previous section. We fixed a parameter-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "7 Discussion and Conclusions", "text": "In our work, we examined iterative hard threshold algorithms and demonstrated that these techniques can provide global convergence guarantees for arbitrary, possibly non-convex, differentiable objective functions that still meet Restricted Strong Convexity / Smoothness (RSC / RSM) conditions. Our results apply to a large family of algorithms that includes existing algorithms such as IHT, GraDeS, CoSaMP, SP and OMPR. Previously, the analysis of these algorithms required strict RIP conditions that did not allow the (limited) number of conditional values to be greater than universal constants specific to these algorithms. Our basic finding was to loosen this strict requirement by executing these iterative algorithms with an increased support size. We demonstrated that guarantees for high-dimensional M estimates follow our results seamlessly by drawing on results from RSC / RSM conditions already established in the multitude of literature."}, {"heading": "A Proofs for Section 3", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "B Proofs for Section 4", "text": "The proof for theorem 1 with f = L (\u00b7; Z1: n) is the empirical loss minimizer over the set of s-sparse vectors. Then we call theorem 1 with f = L (\u00b7; Z1: n), and we get L (\u03b8\u0432, Z1: n) \u2212 \u2264 L (\u03b8, Z1: n) \u2264 L (\u0432; Z1: n) + < L (\u03b8; Z1: n), (\u03b8; Z1: n). Duality gives us the upper limit < Z1: n), where the second inequality is by definition of \u043d and the third inequality is by definition of \u03b8 and the third of RSC (since we do not know it; Z1: n). Duality gives us the upper limit < Z1: n), where the second inequality is by definition of \u043d and the third of RSC (since we do not know it; Z1: n). The duality gives us the upper limit < Z< second < Z< second ineth; < second < second ineth; < < second ineth \u2212."}, {"heading": "C Proofs for Section 5", "text": "5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 -"}, {"heading": "D Supplementary Experimental Results", "text": "In the following we present diagrams that were not included in the main text. 0 0.1 0.2 0.3 0.4 020406080Noise Level (sigma) S uppo rtR ecov ery Err orOMPR CoSaMP L1 SP (a) 0.5 1 1.5 2 2.5x 104050100150200Dimensionality (p) R untim e (s ec) OMPR CoSaMP L1 SP (b) 0 100 200 300 400 500 10 \u2212 3 10 \u2212 2100102104Sparsity (s *) R untim e (s ec) OMPR CoSaMP L1 SP (c)"}], "references": [{"title": "Statistics for high-dimensional data: methods, theory and applications", "author": ["Peter B\u00fchlmann", "Sara Van De Geer"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Minimax rates of estimation for high-dimensional linear regression over `q-balls", "author": ["Garvesh Raskutti", "Martin J Wainwright", "Bin Yu"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Estimation of high-dimensional low-rank matrices", "author": ["Angelika Rohde", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Sparse approximate solutions to linear systems", "author": ["Balas Kausik Natarajan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Forward-backward greedy algorithms for general convex smooth functions over a cardinality constraint", "author": ["Ji Liu", "Ryohei Fujimaki", "Jieping Ye"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "On learning discrete graphical models using greedy methods", "author": ["Ali Jalali", "Christopher C Johnson", "Pradeep D Ravikumar"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1935}, {"title": "Trading accuracy for sparsity in optimization problems with sparsity constraints", "author": ["Shai Shalev-Shwartz", "Nathan Srebro", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87 of Applied Optimization", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima", "author": ["P. Loh", "M.J. Wainwright"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["Thomas Blumensath", "Mike E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property", "author": ["Rahul Garg", "Rohit Khandekar"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Hard thresholding pursuit: an algorithm for compressive sensing", "author": ["Simon Foucart"], "venue": "SIAM J. on Num. Anal.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "CoSaMP: Iterative Signal Recovery from Incomplete and Inaccurate Samples", "author": ["Deanna Needell", "Joel A. Tropp"], "venue": "Appl. Comput. Harmon. Anal.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["Wei Dai", "Olgica Milenkovic"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Orthogonal matching pursuit with replacement", "author": ["Prateek Jain", "Ambuj Tewari", "Inderjit S. Dhillon"], "venue": "In Annual Conference on Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Greedy sparsity-constrained optimization", "author": ["Sohail Bahmani", "Bhiksha Raj", "Petros T Boufounos"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Gradient hard thresholding pursuit for sparsityconstrained optimization", "author": ["Xiaotong Yuan", "Ping Li", "Tong Zhang"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Lower bounds on the performance of polynomial-time algorithms for sparse linear regression", "author": ["Yuchen Zhang", "Martin J. Wainwright", "Michael I. Jordan"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1918}, {"title": "High-dimension regression with noisy and missing data: Provable guarantees with non-convexity", "author": ["P. Loh", "M.J. Wainwright"], "venue": "Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Fast global convergence of gradient methods for high-dimensional statistical recovery", "author": ["Alekh Agarwal", "Sahand N. Negahban", "Martin J. Wainwright"], "venue": "Annals of Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Graphical Model Structure Learning with L1-Regularization", "author": ["Mark Schmidt"], "venue": "PhD thesis, University of British Columbia,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Adaptive Forward-Backward Greedy Algorithm for Learning Sparse Representations", "author": ["Tong Zhang"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 1, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 2, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 3, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 4, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 5, "context": "Examples include sparse regression which requires loss minimization with sparsity constraints and low-rank regression which requires dealing with rank constraints which are not efficiently solvable in general [6].", "startOffset": 209, "endOffset": 212}, {"referenceID": 4, "context": "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.", "startOffset": 111, "endOffset": 120}, {"referenceID": 7, "context": "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.", "startOffset": 111, "endOffset": 120}, {"referenceID": 8, "context": "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.", "startOffset": 111, "endOffset": 120}, {"referenceID": 9, "context": "[10] do not apply to these techniques due to the non-convex structure of the problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "An exception to this is the recent work [11] that demonstrates that PGD with non-convex regularization can offer consistent estimates for certain high-dimensional problems.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "However, the work in [11] is only able to analyze penalties such as SCAD, MCP and capped L1.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 162, "endOffset": 166}, {"referenceID": 12, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 177, "endOffset": 181}, {"referenceID": 13, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 215, "endOffset": 219}, {"referenceID": 14, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 228, "endOffset": 232}, {"referenceID": 15, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 256, "endOffset": 260}, {"referenceID": 16, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 274, "endOffset": 278}, {"referenceID": 16, "context": "The best known such constant is due to the work of [17] that requires a bound on the RIP constant \u03b42k \u2264 0.", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "Although recent attempts have been made to extend this to general differentiable objectives [18, 19], the results continue to require that the restricted condition number be less than a universal constant and remain unsatisfactory in a statistical setting.", "startOffset": 92, "endOffset": 100}, {"referenceID": 18, "context": "Although recent attempts have been made to extend this to general differentiable objectives [18, 19], the results continue to require that the restricted condition number be less than a universal constant and remain unsatisfactory in a statistical setting.", "startOffset": 92, "endOffset": 100}, {"referenceID": 19, "context": "Our bounds are tight, achieve known minmax lower bounds [20], and hold for arbitrary differentiable, possibly even non-convex functions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "Our results effortlessly extend to the noisy setting as a corollary and give bounds similar to those of [21] that relies on solving an L1 regularized problem.", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "Two models of noise are popular in literature [21]: a) (additive noise) X\u0303i = Xi+Wi where Wi \u223c N (0,\u03a3W ), and b) (missing data) X\u0303 is an R\u222a{?}-valued matrix obtained by independently, with probability \u03bd \u2208 [0, 1), replacing each entry in X with ?.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "This result is similar to Lemma 1 of [17] but holds under RSC/RSS conditions (rather than the RIP condition as in [17]), as well as for the general loss functions.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "This result is similar to Lemma 1 of [17] but holds under RSC/RSS conditions (rather than the RIP condition as in [17]), as well as for the general loss functions.", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": "We now study Partial Hard Thresholding methods (PHT), a family of fully-corrective iterative methods introduced by [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 148, "endOffset": 152}, {"referenceID": 22, "context": "We compared them with a standard implementation of the L1 projected scaled sub-gradient technique [23] for the lasso problem and a greedy method FoBa [24] for the same.", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "We compared them with a standard implementation of the L1 projected scaled sub-gradient technique [23] for the lasso problem and a greedy method FoBa [24] for the same.", "startOffset": 150, "endOffset": 154}, {"referenceID": 4, "context": "A unified analysis for general structures will probably create interesting connections with existing unified frameworks such as those based on decomposability [5] and atomic norms [25].", "startOffset": 159, "endOffset": 162}], "year": 2014, "abstractText": "The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard L0 constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. We also extend our analysis to a large family of \u201cfully corrective methods\u201d that includes two-stage and partial hard-thresholding algorithms. We show that our results hold for the problem of sparse regression, as well as low-rank matrix recovery.", "creator": "LaTeX with hyperref package"}}}