{"id": "1410.5877", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2014", "title": "Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation", "abstract": "We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.", "histories": [["v1", "Tue, 21 Oct 2014 22:55:48 GMT  (391kb,D)", "http://arxiv.org/abs/1410.5877v1", "11 pages, 14 figures; appeared in Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 2010"]], "COMMENTS": "11 pages, 14 figures; appeared in Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 2010", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["michael bloodgood", "chris callison-burch"], "accepted": true, "id": "1410.5877"}, "pdf": {"name": "1410.5877.pdf", "metadata": {"source": "CRF", "title": "Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation", "authors": ["Michael Bloodgood", "Chris Callison-Burch"], "emails": ["bloodgood@jhu.edu", "ccb@cs.jhu.edu"], "sections": [{"heading": "1 Introduction", "text": "Figure 1 shows the learning curves for two state-of-the-art statistical machine translation (SMT) systems for Urdu-English translation. Watch how the learning curves first rise rapidly, but then a trend of declining yields occurs: Simply put, the curves flatten out. This paper examines whether we can buck the trend of declining yields, and if so, how we can effectively do it. Active learning (AL) was recently applied to SMT (Haffari et al., 2009; Haffari and Sarkar, 2009), but they were interested in starting with a tiny data set, and stopped their research after adding only a relatively small amount of data, as shown in Figure 1. In contrast, we are interested in applying AL when a large amount of data already exists, as is the case for many important language pairs. We are developing an AL algorithm that focuses on keeping annotation costs (measured in seconds) low."}, {"heading": "2 Related Work", "text": "Active learning has been shown to be effective in improving NLP systems and reducing the annotation burden for a number of NLP tasks, the costs of which are focused on a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010). The current paper is most closely related to previous work, which falls into three main areas: using AL when large companies already exist; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to us. Although their focus is mainly on studying the performance of learning methods orders of magnitude larger than before, they outline how AL could be useful in acquiring data because they recognize the problem of diminimizing returns."}, {"heading": "3 Simulation Experiments", "text": "At this point, we report on the results of simulation experiments that help to illustrate and motivate the design decisions of the algorithm presented in Section 4. We use the Linguistic Data Consortium (LDC) Urdu-English Language Pack 1, which contains \u2248 88000 Urdu-English sentence translation pairs, equivalent to an English translation of \u2248 1.7 million Urdu dictionaries. All experiments in this essay evaluate a genre-balanced breakdown of the NIST2008 Urdu-English test sentence. In addition, the language pack contains an Urdu-English dictionary consisting of \u2248 114000 entries. In all experiments, we use the dictionary for each iteration of the training. This will make it difficult for us to demonstrate our methods, which offer considerable advantages, as the dictionary initially provides a higher baseline performance. However, it would be artificial to ignore dictionary resources when they exist."}, {"heading": "3.1 Annotation Costs", "text": "In fact, most of them are able to play by the rules that they have adopted in recent years."}, {"heading": "3.2 Managing Uncertainty", "text": "One of the most successful of all AL methods developed to date is scanning uncertainty, which has been successfully applied many times (e.g. Lewis and Gale, 1994; Tong and Koller, 2002). Intuition is clear: when there is great uncertainty, much can (potentially) be learned. However, since MT is a relatively complicated task (compared to, say, binary classification), it may be the case that the uncertainty approach needs to be rethought. If words have never occurred in the training data, it can be assumed that the uncertainty is high. However, we are concerned that when a sentence is translated for which (almost) no words have been seen in training, although the uncertainty will be high (which is usually good for AL), the alignments may be wrong and subsequent learning from this translation pair will be severely impeded. We tested this hypothesis and Figure 7 shows empirical evidence that it is true."}, {"heading": "3.3 Automatic Stopping", "text": "In our simulation, we stop VG as soon as all n-grams (n in {1,2,3,4}) are covered. Although simple, this stop criterion seems to work well, as can be seen from where the curve for VG is truncated in Figures 3 and 4. It ends after 1,293,093 translated words, with jHere having BLEU = 21.92 and jSyntax BLEU = 26.10 at the stop point. Finally, BLEU values (with the full commented corpus) are 21.87 and 26.01 for jHere and jSyntax, respectively. Thus, our stop criterion saves 22.3% of the comment (in words) and actually achieves slightly higher BLEU values than if all data were used."}, {"heading": "4 Highlighted N-Gram Method", "text": "In this section, we describe a method of requesting human translations that we have successfully applied to improve translation quality in real (not simulated) conditions. We call this method the Highlighted N-Gram Method, or HNG for short. HNG requires translations only for trigger n-grams and not for whole sentences. We provide a sentence context, mark the trigger n-gram we want to translate, and ask for a translation only of the highlighted trigger n-gram. HNG requests translations for triggers in the same order in which the triggers are encountered by the algorithm in Figure 2. A screenshot of our interface is shown in Figure 8. The same stop criterion is used as it was used in the last section. If the stop criterion becomes true, it is time to tap a new unlabeled pool of foreign words, if available. Our motivations for requesting translations are for cases of only two sentences."}, {"heading": "5 Experiments and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 General Setup", "text": "We set out to see if we could use the HNG method to improve translation quality by collecting additional translations to supplement the training data of the entire LDC language package, including its dictionary. Specifically, we wanted to see if we could make translation improvements in addition to the state-of-the-art systems already in place across the LDC corpus. Note that this is an ambitious undertaking at the outset (remember the flattening of the curves in Figure 1 from Section 1). Snow et al. (2008) explored the use of the Amazon Mechanical Turk (MTurk) web service to collect notes for a variety of natural language processing tasks, and recently, MTurk has proven to be a fast, cost-effective way to collect translations in Urdu English (Bloodgood and Callison-Burch, 2010). We used the Web service MTurk to gather our big anecdotes on the Internet first."}, {"heading": "5.2 Accounting for Translation Time", "text": "MTurk delivers the \"WorkTimeInSeconds\" with each order. This is the time span between a worker accepting an order and submitting the completed order. Using this value, we estimate the annotation times as shown in Figure 4Figure 9 shows HNG capture versus random capture by MTurk. The x-axis measures the number of seconds of annotation time. Note that HNG is more effective. Perhaps of particular interest is that HNG results in a time acceleration that is more than just the reduction in translated words. The average time for translating a word from Urdu with the sentence postings to MTurk was 32.92 seconds. The average time for translating a word with the HNG postings to MTurk was 11.98 seconds. This is almost three times faster. Figure 10 shows the distribution of the speed (in seconds per word) for HNG postings versus full sentence postings."}, {"heading": "5.3 Bucking the Trend", "text": "This is a relatively small amount, \u2248 3% of the LDC corpus. Figure 11 shows the performance when we add this training data to the LDC corpus. The average speed for the HNG postings seems to be slower than the histogram suggests. This is because there were some extremely slow outliers for a handful of HNG postings. These are almost certainly not cases where the Turk is continuously working on the task and thus the average speed we have calculated for the HNG postings is slower than the histogram."}, {"heading": "5.4 Beyond BLEU Scores", "text": "BLEU is an imperfect metric (Callison-Burch et al., 2006). One reason for this is that it evaluates all ngram histograms that show the distribution of translation speeds (in seconds per foreign word) when translations are collected over n \u2212 grams versus over complete sentence mismatches, although some are much more important than others. Another reason is that it is not intuitive what gaining x BLEU points means in practice. Figure 13 shows an example where the strategy partially works, but not as well as it could. The Urdu phrase was translated by Turks as \"veiled\" because the word \"veiled\" is that we only align the word \"veiled\" to \"gowned,\" we only see \"gowned.\""}, {"heading": "6 Conclusions and Future Work", "text": "We have managed to buck the trend of declining returns and improve translation quality while keeping annotation costs low. In future work, we would like to apply these ideas to domain customization (for example, a universal MT system to work in scientific fields such as chemistry), test with more languages, increase the amount of data we can collect, and further investigate stop criteria, and explore the efficiency of the selection algorithm by addressing issues raised by the May 12 example presented above."}, {"heading": "Acknowledgements", "text": "This work has been supported by the Human Language Technology Center of Excellence at Johns Hopkins University. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsor."}], "references": [{"title": "Scaling to very very large corpora for natural language disambiguation", "author": ["Michele Banko", "Eric Brill."], "venue": "Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 26\u201333, Toulouse, France, July. Association for Com-", "citeRegEx": "Banko and Brill.,? 2001", "shortCiteRegEx": "Banko and Brill.", "year": 2001}, {"title": "Using mechanical turk to build machine translation evaluation sets", "author": ["Michael Bloodgood", "Chris Callison-Burch."], "venue": "Proceedings of the Workshop on Creating Speech and Language Data With Amazon\u2019s Mechanical Turk, Los Angeles, California, June.", "citeRegEx": "Bloodgood and Callison.Burch.,? 2010", "shortCiteRegEx": "Bloodgood and Callison.Burch.", "year": 2010}, {"title": "An approach to reducing annotation costs for bionlp", "author": ["Michael Bloodgood", "K Vijay-Shanker."], "venue": "Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 104\u2013105, Columbus, Ohio, June. Association for", "citeRegEx": "Bloodgood and Vijay.Shanker.,? 2008", "shortCiteRegEx": "Bloodgood and Vijay.Shanker.", "year": 2008}, {"title": "A method for stopping active learning based on stabilizing predictions and the need for user-adjustable stopping", "author": ["Michael Bloodgood", "K Vijay-Shanker."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning", "citeRegEx": "Bloodgood and Vijay.Shanker.,? 2009a", "shortCiteRegEx": "Bloodgood and Vijay.Shanker.", "year": 2009}, {"title": "Taking into account the differences between actively and passively acquired data: The case of active learning with support vector machines for imbalanced datasets", "author": ["Michael Bloodgood", "K Vijay-Shanker."], "venue": "Proceedings of Human Lan-", "citeRegEx": "Bloodgood and Vijay.Shanker.,? 2009b", "shortCiteRegEx": "Bloodgood and Vijay.Shanker.", "year": 2009}, {"title": "Re-evaluating the role of Bleu in machine translation research", "author": ["Chris Callison-Burch", "Miles Osborne", "Philipp Koehn."], "venue": "11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), Trento, Italy.", "citeRegEx": "Callison.Burch et al\\.,? 2006", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2006}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics, 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Investigating the effects of selective sampling on the annotation task", "author": ["Ben Hachey", "Beatrice Alex", "Markus Becker."], "venue": "Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 144\u2013151, Ann Arbor, Michi-", "citeRegEx": "Hachey et al\\.,? 2005", "shortCiteRegEx": "Hachey et al\\.", "year": 2005}, {"title": "Active learning for multilingual statistical machine translation", "author": ["Gholamreza Haffari", "Anoop Sarkar."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language", "citeRegEx": "Haffari and Sarkar.,? 2009", "shortCiteRegEx": "Haffari and Sarkar.", "year": 2009}, {"title": "Active learning for statistical phrase-based machine translation", "author": ["Gholamreza Haffari", "Maxim Roy", "Anoop Sarkar."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Associa-", "citeRegEx": "Haffari et al\\.,? 2009", "shortCiteRegEx": "Haffari et al\\.", "year": 2009}, {"title": "Sample selection for statistical grammar induction", "author": ["Rebecca Hwa."], "venue": "Hinrich Sch\u00fctze and KehYih Su, editors, Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing, pages 45\u201353. Association for", "citeRegEx": "Hwa.,? 2000", "shortCiteRegEx": "Hwa.", "year": 2000}, {"title": "Selective supervision: Guiding supervised learning with decision-theoretic active learning", "author": ["Ashish Kapoor", "Eric Horvitz", "Sumit Basu."], "venue": "Manuela M. Veloso, editor, IJCAI 2007, Proceedings of the 20th International Joint Conference on", "citeRegEx": "Kapoor et al\\.,? 2007", "shortCiteRegEx": "Kapoor et al\\.", "year": 2007}, {"title": "Functional genomic hypothesis generation and experimentation by a robot", "author": ["Ross D. King", "Kenneth E. Whelan", "Ffion M. Jones", "Philip G.K. Reiser", "Christopher H. Bryant", "Stephen H. Muggleton", "Douglas B. Kell", "Stephen G. Oliver"], "venue": null, "citeRegEx": "King et al\\.,? \\Q2004\\E", "shortCiteRegEx": "King et al\\.", "year": 2004}, {"title": "A sequential algorithm for training text classifiers", "author": ["David D. Lewis", "William A. Gale."], "venue": "SIGIR \u201994: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 3\u201312, New", "citeRegEx": "Lewis and Gale.,? 1994", "shortCiteRegEx": "Lewis and Gale.", "year": 1994}, {"title": "Joshua: An open source toolkit for parsing-based machine translation", "author": ["Zhifei Li", "Chris Callison-Burch", "Chris Dyer", "Juri Ganitkevitch", "Sanjeev Khudanpur", "Lane Schwartz", "Wren Thornton", "Jonathan Weese", "Omar Zaidan."], "venue": "Proceedings of the Fourth", "citeRegEx": "Li et al\\.,? 2009", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Phrase-based statistical language generation using graphical models and active learning", "author": ["Francois Mairesse", "Milica Gasic", "Filip Jurcicek", "Simon Keizer", "Jorge Prombonas", "Blaise Thomson", "Kai Yu", "Steve Young."], "venue": "Proceedings of the 48th Annual", "citeRegEx": "Mairesse et al\\.,? 2010", "shortCiteRegEx": "Mairesse et al\\.", "year": 2010}, {"title": "Bleu: a method for automatic", "author": ["Jing Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2002\\E", "shortCiteRegEx": "Zhu.", "year": 2002}, {"title": "Cheap and fast \u2013 but is it", "author": ["Andrew Ng"], "venue": null, "citeRegEx": "Ng.,? \\Q2008\\E", "shortCiteRegEx": "Ng.", "year": 2008}, {"title": "A stopping criterion for active learning", "author": ["Andreas Vlachos."], "venue": "Computer Speech and Language, 22(3):295\u2013312.", "citeRegEx": "Vlachos.,? 2008", "shortCiteRegEx": "Vlachos.", "year": 2008}, {"title": "Syntax augmented machine translation via chart parsing", "author": ["Andreas Zollmann", "Ashish Venugopal."], "venue": "Proceedings of the NAACL-2006 Workshop on Statistical Machine Translation (WMT06), New York, New York.", "citeRegEx": "Zollmann and Venugopal.,? 2006", "shortCiteRegEx": "Zollmann and Venugopal.", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": "Active learning (AL) has been applied to SMT recently (Haffari et al., 2009; Haffari and Sarkar, 2009) but they were interested in starting with a tiny seed set of data, and they stopped their investigations after only adding a relatively tiny amount of data as depicted in Figure 1.", "startOffset": 54, "endOffset": 102}, {"referenceID": 8, "context": "Active learning (AL) has been applied to SMT recently (Haffari et al., 2009; Haffari and Sarkar, 2009) but they were interested in starting with a tiny seed set of data, and they stopped their investigations after only adding a relatively tiny amount of data as depicted in Figure 1.", "startOffset": 54, "endOffset": 102}, {"referenceID": 10, "context": ", (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)).", "startOffset": 2, "endOffset": 143}, {"referenceID": 2, "context": ", (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)).", "startOffset": 2, "endOffset": 143}, {"referenceID": 15, "context": ", (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)).", "startOffset": 2, "endOffset": 143}, {"referenceID": 0, "context": "In a sense, the work of Banko and Brill (2001) is closely related to ours.", "startOffset": 24, "endOffset": 47}, {"referenceID": 9, "context": "An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences.", "startOffset": 59, "endOffset": 70}, {"referenceID": 9, "context": "An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking.", "startOffset": 59, "endOffset": 295}, {"referenceID": 9, "context": "An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. Osborne and Baldridge (2004) argued for the use of discriminant cost over unit cost for the task of Head Phrase Structure Grammar parse selection.", "startOffset": 59, "endOffset": 466}, {"referenceID": 9, "context": "An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. Osborne and Baldridge (2004) argued for the use of discriminant cost over unit cost for the task of Head Phrase Structure Grammar parse selection. King et al. (2004) design a robot that tests gene functions.", "startOffset": 59, "endOffset": 603}, {"referenceID": 7, "context": "Hachey et al. (2005) showed that selectively sampled examples for an NER task took longer to annotate and had lower inter-annotator agreement.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "Hachey et al. (2005) showed that selectively sampled examples for an NER task took longer to annotate and had lower inter-annotator agreement. This work is related to ours because it shows that how examples are selected can impact the cost of annotation, an idea we turn around to use for our advantage when developing our data selection algorithm. Haertel et al. (2008) emphasize measuring costs carefully for AL for POS tagging.", "startOffset": 0, "endOffset": 371}, {"referenceID": 7, "context": "Hachey et al. (2005) showed that selectively sampled examples for an NER task took longer to annotate and had lower inter-annotator agreement. This work is related to ours because it shows that how examples are selected can impact the cost of annotation, an idea we turn around to use for our advantage when developing our data selection algorithm. Haertel et al. (2008) emphasize measuring costs carefully for AL for POS tagging. They develop a model based on a user study that can estimate the time required for POS annotating. Kapoor et al. (2007) assign costs for AL based on message length for a voicemail classification task.", "startOffset": 0, "endOffset": 551}, {"referenceID": 7, "context": "Hachey et al. (2005) showed that selectively sampled examples for an NER task took longer to annotate and had lower inter-annotator agreement. This work is related to ours because it shows that how examples are selected can impact the cost of annotation, an idea we turn around to use for our advantage when developing our data selection algorithm. Haertel et al. (2008) emphasize measuring costs carefully for AL for POS tagging. They develop a model based on a user study that can estimate the time required for POS annotating. Kapoor et al. (2007) assign costs for AL based on message length for a voicemail classification task. In contrast, we show for SMT that annotation times do not scale according to length in words and we show our method can achieve a speedup in annotation time above and beyond what the reduction in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task.", "startOffset": 0, "endOffset": 874}, {"referenceID": 8, "context": "(2009), Haffari and Sarkar (2009), and Ambati et al.", "startOffset": 8, "endOffset": 34}, {"referenceID": 8, "context": "(2009), Haffari and Sarkar (2009), and Ambati et al. (2010) investigate AL for SMT.", "startOffset": 8, "endOffset": 60}, {"referenceID": 9, "context": "See Figure 1 from Section 1, which contrasts where (Haffari et al., 2009; Haffari and Sarkar, 2009) stop their investigations with where we begin our studies.", "startOffset": 51, "endOffset": 99}, {"referenceID": 8, "context": "See Figure 1 from Section 1, which contrasts where (Haffari et al., 2009; Haffari and Sarkar, 2009) stop their investigations with where we begin our studies.", "startOffset": 51, "endOffset": 99}, {"referenceID": 9, "context": "The other major difference is that (Haffari et al., 2009; Haffari and Sarkar, 2009) measure annotation cost by # of sentences.", "startOffset": 35, "endOffset": 83}, {"referenceID": 8, "context": "The other major difference is that (Haffari et al., 2009; Haffari and Sarkar, 2009) measure annotation cost by # of sentences.", "startOffset": 35, "endOffset": 83}, {"referenceID": 6, "context": "We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al.", "startOffset": 81, "endOffset": 95}, {"referenceID": 19, "context": "We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al.", "startOffset": 129, "endOffset": 159}, {"referenceID": 14, "context": "We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al., 2009).", "startOffset": 213, "endOffset": 230}, {"referenceID": 9, "context": "VocabGrowth (hereafter VG) selection is modeled after the best methods from previous work (Haffari et al., 2009; Haffari and Sarkar, 2009), which are based on preferring sentences that contain phrases that occur frequently in unlabeled data and infrequently in the so-far labeled data.", "startOffset": 90, "endOffset": 138}, {"referenceID": 8, "context": "VocabGrowth (hereafter VG) selection is modeled after the best methods from previous work (Haffari et al., 2009; Haffari and Sarkar, 2009), which are based on preferring sentences that contain phrases that occur frequently in unlabeled data and infrequently in the so-far labeled data.", "startOffset": 90, "endOffset": 138}, {"referenceID": 13, "context": ",(Lewis and Gale, 1994; Tong and Koller, 2002)).", "startOffset": 1, "endOffset": 46}, {"referenceID": 18, "context": ", (Bloodgood and VijayShanker, 2009a; Schohn and Cohn, 2000; Vlachos, 2008)).", "startOffset": 2, "endOffset": 75}, {"referenceID": 3, "context": ", (Bloodgood and Vijay-Shanker, 2009a; Schohn and Cohn, 2000)).", "startOffset": 2, "endOffset": 61}, {"referenceID": 1, "context": "(2008) explored the use of the Amazon Mechanical Turk (MTurk) web service for gathering annotations for a variety of natural language processing tasks and recently MTurk has been shown to be a quick, cost-effective way to gather Urdu-English translations (Bloodgood and Callison-Burch, 2010).", "startOffset": 255, "endOffset": 291}, {"referenceID": 5, "context": "BLEU is an imperfect metric (Callison-Burch et al., 2006).", "startOffset": 28, "endOffset": 57}], "year": 2014, "abstractText": "We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.", "creator": "LaTeX with hyperref package"}}}