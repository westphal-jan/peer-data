{"id": "1410.6776", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2014", "title": "Online and Stochastic Gradient Methods for Non-decomposable Loss Functions", "abstract": "Modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, prec@k and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes prec@k, pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method.", "histories": [["v1", "Fri, 24 Oct 2014 18:45:23 GMT  (50kb,D)", "http://arxiv.org/abs/1410.6776v1", "25 pages, 3 figures, To appear in the proceedings of the 28th Annual Conference on Neural Information Processing Systems, NIPS 2014"]], "COMMENTS": "25 pages, 3 figures, To appear in the proceedings of the 28th Annual Conference on Neural Information Processing Systems, NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["purushottam kar", "harikrishna narasimhan", "prateek jain 0002"], "accepted": true, "id": "1410.6776"}, "pdf": {"name": "1410.6776.pdf", "metadata": {"source": "CRF", "title": "Online and Stochastic Gradient Methods for Non-decomposable Loss Functions", "authors": ["Purushottam Kar", "Harikrishna Narasimhan", "Prateek Jain"], "emails": ["t-purkar@microsoft.com,", "prajain@microsoft.com,", "harikrishna@csa.iisc.ernet.in", "precision@k,", "Prec@k", "Prec@k,", "Precision@k,"], "sections": [{"heading": "1 Introduction", "text": "Modern learning applications often require a level of fine-grained control over predictive performance that is not offered by traditional \"pro-point\" performance measures such as hinge loss. Examples include data sets with slight to severe label imbalances such as spam classification, where positive instances (spam emails) make up only a tiny fraction of the available data, and learning tasks such as those in medical diagnostics, which make it imperative for learning algorithms to be sensitive to class imbalances. Other popular examples are tasks where the precision of the best-placed results is valued higher than general precision / retrieval activities. The performance benchmarks of choice in these situations are those that evaluate algorithms across the entire dataset in a holistic way. Consequently, these benchmarks are often not separable by data points."}, {"heading": "1.1 Our Contributions", "text": "Our first contribution is a framework for online learning with non-degradable loss functions. The main hurdle in this task is a proper definition of the immediate penalties for non-degradable losses. Instead of relying on canonical definitions, we set up our framework in a principled manner that meets the objectives of an online model. Our framework has a very desirable feature that allows existing online learning models to be restored when instantiated with point-loss functions. Our framework also allows online conversion limits to be met."}, {"heading": "1.2 Related Work", "text": "Non-detachable loss functions such as Prec @ k, (partial) AUC, F measurements, etc., have generated considerable interest within the learning community due to their proven ability to perform better in situations with label imbalances, etc. From their role in early work as performance indicators for unbalanced data sets [8], their importance has grown to the point where they themselves have become learning objectives. Due to their complexity, methods that attempt to indirectly optimize these measurements are very common, e.g. [9], [10] and [11] that study F measurements. However, such methods often attempt to learn a complex probability model, a task that is arguably more difficult than the present one. On the other hand, they are algorithms that perform optimization directly through structured losses. Based on the underlying work of [3], this method has much interest in measurement variables such as F-7, [4] and [4] losses."}, {"heading": "2 Problem Formulation", "text": "Allow x1: t: x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 Online Learning with Non-decomposable Loss Functions", "text": "We provide our online learning system for the non-decompatible loss functions."}, {"heading": "3.1 Low Regret Online Learning", "text": "We propose an efficient impact assessment. (...) We propose an efficient impact assessment. (...) We propose an efficient impact assessment. (...) We propose an efficient impact assessment. (...) We would like to stress that despite the non-existence of Lt, the FTRL target is strongly convex if \"P\" can be convex and thus the update can be implemented efficiently by solving a regulated batch problem to x1. (...) We provide our regret for the FTRL update. (...) Let \"P (...) be a convex loss function and W.\" Assuw.o.g."}, {"heading": "3.2 Online-to-batch Conversion", "text": "To present our limitations, we generalize our framework slightly: we now consider the stream of T-points as composed of T / s stacks Z1,.., ZT / s stacks of size s each. Thus, the current penalty is now called Lt (w) = \"P (Z1,.., Zt, w) \u2212\" P (Z1,. \u2212 \"P) for t = 1. T / s and the regret becomes R (T, s) = 1T T T T / s t = 1 Lt (wt) \u2212 arg minw \u00b2 W 1T\" P (x1: T, y1: T, w). Let RP determine the population risk for the (normalized) measure of performance P. Then we have: Theorem 4. Let's set the order of the points (xt, yt) will i.d. be generated and let w1, wT / s be a total ensemble of models generated by learning these algorithms."}, {"heading": "4 Stochastic Gradient Methods for Non-decomposable Losses", "text": "The online learning algorithms discussed in the previous section provide attractive guarantees in the sequential prediction model, but must solve batch problems at each stage, which quickly becomes unfeasible for large amounts of data. To address this, we now present memory-efficient stochastic genealogy methods for batch learning with non-separable loss functions. The motivation for our approach comes from mini-batch methods that harness learning methods for point-loss functions in distributed computer environments [15, 16]. We use these techniques to provide scalable algorithms for non-separable loss functions."}, {"heading": "4.1 Single-pass Method with Mini-batches", "text": "The method requires access to a limited memory buffer and passes through the data stream. The stream is divided into epochs. In each epoch, the method collects points in the stream, uses them to create gradient estimates and performs downward steps. The buffer is flushed after each epoch. Algorithm 1 describes the 1PMB method. Gradient calculations can be performed with Danskin's theorem (see Appendix H)."}, {"heading": "4.2 Two-pass Method with Mini-batches", "text": "The previous algorithm is unable to exploit relationships between data points across epochs that could help improve the performance of loss functions such as pAUC. To address this, we observe that several real-world learning scenarios exhibit slight to severe imbalances in labeling (see Table 1 in Appendix H) that allow all or a large portion of the rare labeling points to be stored. Our two labeling methods take advantage of this by using two transitions over the data: the first pass captures all (or a random subset) of the rare labeling points using a stream sampling technique [13]; the second pass then passes over the stream limited to the non-rare labeling points and performs gradient updates. See Algorithm 2 for details of the 2PMB method."}, {"heading": "4.3 Error Bounds", "text": "Given a set of n marked data points (xi, yi), i = 1, and a performance metric P, our goal is to approximate the empirical risks we have used for empirical risk mitigation. (However, we assume that our methods will show whether we can demonstrably converge for empirical risk mitigation with respect to a number of W predictors (s, log 1) when we introduce the notion of uniform convergence for a performance measurement. (We say that a loss function \"demonstrates uniform convergence with respect to a number of W predictors, if for some (s, log 1) when we introduce the notion of uniform convergence for a performance measurement. (x, x) s selected random values from an arbitrary set of n points {x1, y1),. (xn, yn)."}, {"heading": "5 Experimental Results", "text": "This year it is more than ever before in the history of the city."}, {"heading": "Acknowledgements", "text": "The authors thank Shivani Agarwal for helpful comments and the anonymous reviewer for her suggestions. HN thanks the support of a Google India PhD Fellowship."}, {"heading": "A Proof of Theorem 1", "text": "Broadly speaking, we follow the evidence structure of the FTFL given in [1, 23]. First, we note that the \"forward repentance\" analysis, despite the non-convexity, easily follows the FTT (1), i.e., T + 1 Lt = 1 Lt (wt + 1) \u2264 x1: T, y1: T, w \u00b2, (7), where w \u00b2, w \u00b2, w \u00b2, w \u00b2, w \u00b2, w \u00b2, w \u00b2, w \u00b2. The proof for this statement can be found in [23, Theorem 7] and is given below as Lemma 9 for completeness. Furthermore, using the strong convexity of the regulator T \u00b2 w \u00b2 22 and the optimality of the WT \u2212 wt \u2212 wt and wt \u2212 wt of the respective update steps, we obtain: \"P (x1: t, y1: t, wt: t, wt: t, wt: wt, wt: wt, wt: wt, wt: wt, wt, wt: wt, wt."}, {"heading": "B Proof of Lemma 2", "text": "We are looking at the following four exhaustive cases: Case 1. This leads to a specific case. (...) We are looking at the following four exhaustive cases. (...) We are looking at the following four cases. (...) We are looking at the following three cases. (...) We are looking at the following four cases. (...) We are looking at the following four cases. (...) We are looking at the following four cases. (...) We are looking at the following three cases. (...) We are looking at the following three cases. (...) We are looking at the following three cases. (...) We are looking at the following three cases. (...) We are looking at the same. (...) We are looking at the same situation. (...) We are looking at the same situation. (...) We are looking at the same situation. (...). (..................................................... we. (...)."}, {"heading": "D Extension to Precision-Recall Break Even Point (PRBEP)", "text": "We note that the above discussion can be slightly extended to prove stability results for the structural surrogate loss for the PRBEP performance metric [3]. Since we have Prec = TPTP + FP and Rec = TPTP + FN, the break-even point is reached at a threshold at which TP + FP = TP + FN is measured. Note that the left side corresponds to the number of points predicted as positive, while the right side corresponds to the number of points that are actually positive. Thus, the PRBEP is reached at a threshold at which as many points are predicted as there are actually positive points, which gives us the formal definition of this performance measurement PRBEP (w): = = j j (t + t, w) (xj, w) = I & \u2212 jj = 1]."}, {"heading": "E Online-to-batch Conversion", "text": "This section is proof of regret that will help us answer our online questions. (...) In this section, we will consider the pAUC measurement in the 2PMB environment, where positives are assumed to remain in the buffer and negatives. (...) The case of the Prec @ k measurement in the usual 1PMB measurement can be handled similarly. (...) In addition, we will show in Appendix G that the contributions from a sufficiently large buffer of randomly selected positive points mixes the contributions of the entire population of the positive points. (...) Thus, it is sufficient to show the pAUC conversion only in relation to the negatives. (...) We will clarify this further in the Discussion.E.1 Regret Bounds in the Modified FrameworkWe demonstrate the following problem that will help us in answering our online measurements."}, {"heading": "F Proof of Theorem 6", "text": "The proof runs in two parts: the first part uses the fact that the 1PMB method essentially simulates the GIGA method of [24] with the non-resolvable loss function, and the second part uses the uniform convergence properties of the loss function to determine the error limit. However, to continue, let us list some notations. Let's consider the next epoch of the 1PMB algorithm to prove the performance of the online gradient descent in relation to the instantaneous loss functions Le (w) = L (Xe, w): \"P (xe1: s, ye1: s).Since the loss function Le (w) convex, the standard analysis would be applied to online convex optimization."}, {"heading": "G Uniform Convergence Bounds for Partial Area under the ROC", "text": "In this section, we present a proof sketch of theorem 7, which we reproduce below for the smallest elements of the smallest group. (Theorem 13) Consider any convergence at the rate \u03b1 (s): \"P (x1: n, y1: n, w) + n (0, \u03b2) -partial AUC power measure defined as follows: (yi > 0), n (p), p (x1: n, y1: n, w) = 1\u03b2n \u2212 n: i = 1 I [yi > 0] n: 1 I [yi > 0] n: 1 I [yj < 0] T \u2212 \u03b2, n (xj, w) s (w). (w > (xi \u2212 xj)), where n + = yi > 0: 1 I [yi >) n: 1 I [yi >."}, {"heading": "H Methodology for implementing 1PMB and 2PMB for pAUC", "text": "tasksIn this section, we clarify the mechanisms used to implement the 1PMB and 2PMB routines = > \u03b2 routines. If we start from the data set statistics (see Table 1), we consider the variant of the 2PMB routine with the positive class as a rare class. Remember the definition of the replacement loss function for pAUC (5) \"pAUC (w)\" (see Table 1) \"pAUC (w).\" We now rewrite this in slightly different ways. Define, for each i: yi > 0 \"+ S \u2212 (xi, w) = jp < 0 T \u2212 \u03b2, t (xj, w) \u00b7 h (x > j)."}], "references": [{"title": "A Structural SVM Based Approach for Optimizing Partial AUC", "author": ["Harikrishna Narasimhan", "Shivani Agarwal"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "A Support Vector Method for Multivariate Performance Measures", "author": ["Thorsten Joachims"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "A Support Vector Method for Optimizing Average Precision", "author": ["Yisong Yue", "Thomas Finley", "Filip Radlinski", "Thorsten Joachims"], "venue": "In SIGIR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Structured Learning for Non-Smooth Ranking Losses", "author": ["Soumen Chakrabarti", "Rajiv Khanna", "Uma Sawant", "Chiru Bhattacharyya"], "venue": "In KDD,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Metric Learning to Rank", "author": ["Brian McFee", "Gert Lanckriet"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "SVM  pAUC: A New Support Vector Method for Optimizing Partial AUC Based on a Tight Convex Upper Bound", "author": ["Harikrishna Narasimhan", "Shivani Agarwal"], "venue": "In KDD,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Addressing the Curse of Imbalanced. Training Sets: One- Sided Selection", "author": ["Miroslav Kubat", "Stan Matwin"], "venue": "In 24th International Conference on Machine Learning (ICML),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "An Exact Algorithm for F-Measure Maximization", "author": ["Krzysztof Dembczy\u0144ski", "Willem Waegeman", "Weiwei Cheng", "Eyke H\u00fcllermeier"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Optimizing F-Measures: A Tale of Two Approaches", "author": ["Nan Ye", "Kian Ming A. Chai", "Wee Sun Lee", "Hai Leong Chieu"], "venue": "In 29th International Conference on Machine Learning (ICML),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization", "author": ["Krzysztof Dembczy\u0144ski", "Arkadiusz Jachnik", "Wojciech Kotlowski", "Willem Waegeman", "Eyke H\u00fcllermeier"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Online Learning: Beyond Regret", "author": ["Alexander Rakhlin", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In 24th Annual Conference on Learning Theory (COLT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions", "author": ["Purushottam Kar", "Bharath K Sriperumbudur", "Prateek Jain", "Harish Karnick"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Online AUC Maximization", "author": ["Peilin Zhao", "Steven C.H. Hoi", "Rong Jin", "Tianbao Yang"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Optimal Distributed Online Prediction Using Mini-Batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Communication-Efficient Algorithms for Statistical Optimization", "author": ["Yuchen Zhang", "John C. Duchi", "Martin J. Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Ranking and empirical minimization of U-statistics", "author": ["St\u00e9phan Cl\u00e9men\u00e7on", "G\u00e1bor Lugosi", "Nicolas Vayatis"], "venue": "Annals of Statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Logarithmic Regret Algorithms for Online Convex Optimization", "author": ["Elad Hazan", "Adam Kalai", "Satyen Kale", "Amit Agarwal"], "venue": "In COLT,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Evaluation of Classifiers for an Uneven Class Distribution Problem", "author": ["Sophia Daskalaki", "Ioannis Kopanas", "Nikolaos Avouris"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Oksana Yakhnenko", "author": ["R. Bharath Rao"], "venue": "and Balaji Krishnapuram. KDD Cup 2008 and the Workshop on Mining Medical Data. SIGKDD Explorations Newsletter, 10(2):34\u201338", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Evaluation of Different Biological Data and Computational Classification Methods for Use in Protein Interaction", "author": ["Yanjun Qi", "Ziv Bar-Joseph", "Judith Klein-Seetharaman"], "venue": "Prediction. Proteins,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "The UCI Machine Learning Repository", "author": ["A. Frank", "Arthur Asuncion"], "venue": "http://archive. ics.uci.edu/ml,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "The interplay between stability and regret in online learning", "author": ["Ankan Saha", "Prateek Jain", "Ambuj Tewari"], "venue": "CoRR, abs/1211.6158,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent", "author": ["Martin Zinkevich"], "venue": "In ICML, pages 928\u2013936,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Probability Inequalities for the Sum in Sampling without Replacement", "author": ["Robert J. Serfling"], "venue": "Annals of Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1974}], "referenceMentions": [{"referenceID": 0, "context": "Next, we instantiate within our framework, convex surrogates for two popular performances measures namely, Precision at k (Prec@k) and partial area under the ROC curve (pAUC) [2] and show, via a stability analysis, that we do indeed achieve sublinear regret bounds for these loss functions.", "startOffset": 175, "endOffset": 178}, {"referenceID": 1, "context": "Our methods apply to a wide family of loss functions (including Prec@k, pAUC and F-measure) that were introduced in [3] and have been widely adopted [4, 5, 6] in the literature.", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "Our methods apply to a wide family of loss functions (including Prec@k, pAUC and F-measure) that were introduced in [3] and have been widely adopted [4, 5, 6] in the literature.", "startOffset": 149, "endOffset": 158}, {"referenceID": 3, "context": "Our methods apply to a wide family of loss functions (including Prec@k, pAUC and F-measure) that were introduced in [3] and have been widely adopted [4, 5, 6] in the literature.", "startOffset": 149, "endOffset": 158}, {"referenceID": 4, "context": "Our methods apply to a wide family of loss functions (including Prec@k, pAUC and F-measure) that were introduced in [3] and have been widely adopted [4, 5, 6] in the literature.", "startOffset": 149, "endOffset": 158}, {"referenceID": 5, "context": "We compare our methods to state-of-the-art methods that are based on cutting plane techniques [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "of performance on imbalanced datasets [8], their importance has risen to a point where they have become the learning objectives themselves.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "[9], [10] and [11] who study the F-measure.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9], [10] and [11] who study the F-measure.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "[9], [10] and [11] who study the F-measure.", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": "Starting from the seminal work of [3], this method has received a lot of interest for measures such as the F-measure [3], average precision [4], pAUC [7] and various ranking losses [5, 6].", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Starting from the seminal work of [3], this method has received a lot of interest for measures such as the F-measure [3], average precision [4], pAUC [7] and various ranking losses [5, 6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "Starting from the seminal work of [3], this method has received a lot of interest for measures such as the F-measure [3], average precision [4], pAUC [7] and various ranking losses [5, 6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "Starting from the seminal work of [3], this method has received a lot of interest for measures such as the F-measure [3], average precision [4], pAUC [7] and various ranking losses [5, 6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 3, "context": "Starting from the seminal work of [3], this method has received a lot of interest for measures such as the F-measure [3], average precision [4], pAUC [7] and various ranking losses [5, 6].", "startOffset": 181, "endOffset": 187}, {"referenceID": 4, "context": "Starting from the seminal work of [3], this method has received a lot of interest for measures such as the F-measure [3], average precision [4], pAUC [7] and various ranking losses [5, 6].", "startOffset": 181, "endOffset": 187}, {"referenceID": 10, "context": "In particular [12] provides a generic framework for online learning with non-additive notions of regret with a focus on showing regret bounds for mixed strategies in a variety of problems.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "Recently, online learning for AUC maximization has received some attention [13, 14].", "startOffset": 75, "endOffset": 83}, {"referenceID": 12, "context": "Recently, online learning for AUC maximization has received some attention [13, 14].", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "Although AUC is not a point loss function, it still decomposes over pairs of points in a dataset, a fact that [13] and [14] crucially use.", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "Although AUC is not a point loss function, it still decomposes over pairs of points in a dataset, a fact that [13] and [14] crucially use.", "startOffset": 119, "endOffset": 123}, {"referenceID": 1, "context": "A popular technique for constructing such loss functions is the structural SVM formulation [3] given below.", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "The structural surrogate for this performance measure can be equivalently expressed in a simpler form by replacing the indicator functions I [\u00b7] with hinge loss as follows (see [7], Theorem 4) `pAUC(w) = \u2211", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "[3] uses a slightly modified, but equivalent, definition that considers labels to be Boolean.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "We note that our framework also recovers the model for online AUC maximization used in [13] and [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "We note that our framework also recovers the model for online AUC maximization used in [13] and [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "In Appendix D, we show that the same technique can be used to prove a stability result for the structural SVM surrogate of the Precision-Recall Break Even Point (PRBEP) performance measure [3] as well.", "startOffset": 189, "endOffset": 192}, {"referenceID": 13, "context": "The motivation for our approach comes from mini-batch methods used to make learning methods for point loss functions amenable to distributed computing environments [15, 16], we exploit these techniques to offer scalable algorithms for non-decomposable loss functions.", "startOffset": 164, "endOffset": 172}, {"referenceID": 14, "context": "The motivation for our approach comes from mini-batch methods used to make learning methods for point loss functions amenable to distributed computing environments [15, 16], we exploit these techniques to offer scalable algorithms for non-decomposable loss functions.", "startOffset": 164, "endOffset": 172}, {"referenceID": 11, "context": "method exploits this by utilizing two passes over the data: the first pass collects all (or a random subset of) points of the rare label using some stream sampling technique [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 15, "context": "However, the same is not true for non-decomposable loss functions barring a few exceptions [17, 10].", "startOffset": 91, "endOffset": 99}, {"referenceID": 8, "context": "However, the same is not true for non-decomposable loss functions barring a few exceptions [17, 10].", "startOffset": 91, "endOffset": 99}, {"referenceID": 16, "context": "Using regularized formulations, we can also exploit logarithmic regret guarantees [18], offered by online gradient descent, to improve this result - however we do not explore those considerations here.", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "A similar result for the special case \u03b2 = 1 is due to [17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "The above result can be extended to a large family of performances measures introduced in [3] that have been widely adopted [10, 19, 8] such as F-measure, G-mean, and PRBEP.", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "The above result can be extended to a large family of performances measures introduced in [3] that have been widely adopted [10, 19, 8] such as F-measure, G-mean, and PRBEP.", "startOffset": 124, "endOffset": 135}, {"referenceID": 17, "context": "The above result can be extended to a large family of performances measures introduced in [3] that have been widely adopted [10, 19, 8] such as F-measure, G-mean, and PRBEP.", "startOffset": 124, "endOffset": 135}, {"referenceID": 6, "context": "The above result can be extended to a large family of performances measures introduced in [3] that have been widely adopted [10, 19, 8] such as F-measure, G-mean, and PRBEP.", "startOffset": 124, "endOffset": 135}, {"referenceID": 5, "context": "Algorithms: For partial AUC, we compare against the state-of-the-art cutting plane (CP) and projected subgradient methods (PSG) proposed in [7]; unlike the (online) stochastic methods", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "For Prec@k and F-measure, we compare our methods against cutting plane methods from [3].", "startOffset": 84, "endOffset": 87}, {"referenceID": 18, "context": "Datasets: We used several data sets for our experiments (see Table 1); of these, KDDCup08 is from the KDD Cup 2008 challenge and involves a breast cancer detection task [20], PPI contains data for a protein-protein interaction prediction task [21], and the remaining datasets are taken from the UCI repository [22].", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "Datasets: We used several data sets for our experiments (see Table 1); of these, KDDCup08 is from the KDD Cup 2008 challenge and involves a breast cancer detection task [20], PPI contains data for a protein-protein interaction prediction task [21], and the remaining datasets are taken from the UCI repository [22].", "startOffset": 243, "endOffset": 247}, {"referenceID": 20, "context": "Datasets: We used several data sets for our experiments (see Table 1); of these, KDDCup08 is from the KDD Cup 2008 challenge and involves a breast cancer detection task [20], PPI contains data for a protein-protein interaction prediction task [21], and the remaining datasets are taken from the UCI repository [22].", "startOffset": 310, "endOffset": 314}], "year": 2014, "abstractText": "Modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, Prec@k and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes Prec@k, pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method.", "creator": "LaTeX with hyperref package"}}}