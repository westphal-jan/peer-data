{"id": "1410.6991", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2014", "title": "A provable SVD-based algorithm for learning topics in dominant admixture corpus", "abstract": "Making a strong assumption called \\emph{separability},", "histories": [["v1", "Sun, 26 Oct 2014 06:00:36 GMT  (48kb,D)", "https://arxiv.org/abs/1410.6991v1", null], ["v2", "Mon, 3 Nov 2014 17:27:07 GMT  (48kb,D)", "http://arxiv.org/abs/1410.6991v2", null], ["v3", "Tue, 4 Nov 2014 05:14:25 GMT  (58kb,D)", "http://arxiv.org/abs/1410.6991v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["trapit bansal", "chiranjib bhattacharyya", "ravindran kannan"], "accepted": true, "id": "1410.6991"}, "pdf": {"name": "1410.6991.pdf", "metadata": {"source": "CRF", "title": "A provable SVD-based algorithm for learning topics in dominant admixture corpus", "authors": ["Trapit Bansal"], "emails": ["trapitbansal@gmail.com", "chiru@csa.iisc.ernet.in", "kannan@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of us are able to go in search of a solution that is capable, that they are able to find a solution, that they are able to find a solution, that they are able, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution for themselves."}, {"heading": "2 Previous Results", "text": "In this section, we review the literature related to the development of detectable algorithms for topic models. For an overview of topic models, we refer the reader to the excellent survey [1]. Demonstrable algorithms for restoring topic models were started by [7]. Latent semantic indexing (LSI) [8] remains a successful method for retrieving similar documents using SVD. [7] showed that M can be restored from a collection of documents with pure topics by applying some additional assumptions based on SVD topics with the addition of primary words. [6] showed that in the admixture case, assuming the dirichlet distribution for the topic, one can actually learn M to l2 errors using tensor methods, provided some additional assumptions about numerical parameters such as the conditional number are met. [6] The first demonstrably polynomial time algorithm for the admiration corpus in 4, [5] is given."}, {"heading": "3 Learning Topics from Dominant Admixtures", "text": "In addition to its simplicity, we show empirical evidence from real corpora to show that topic domination is a reasonable assumption; the assumption of the dominant topic is weaker than the assumption of the pure topic. More importantly, the SVD-based procedures proposed by [7] are not applicable. Inspired by the assumption of the primary words, we introduce the assumption that each topic has a set of keywords that occur more frequently in this topic than others. This, in turn, is a much weaker assumption than the assumptions of the primary words and anchors, and can be verified experimentally. In this section, we note that applying SVD to a matrix that is achieved by the threshold of the word-document matrix, and subsequent k means that clusters can learn topics with keywords from a dominant admin body."}, {"heading": "3.1 Assumptions: Catchwords and Dominant admixtures", "text": "(1)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3.2 The TSVD Algorithm", "text": "Existing SVD-based procedures for clustering on raw word document matrices fail because the prevalence of the frequency of a word within a topic is often more (at least not significantly less) than the gap between the frequencies of the word in two different topics. Hypothetically, the frequency of the word run, in sports terms, can range from say 0.01 to 0.005. But in other topics, it can range from 0 to 0.005, say. The success of the algorithm will lie in the correct identification of dominant topics such as sports by identifying that the word run has occurred with high frequency. In this example, the gap (0.01-0.005) between sports and other topics is lower than the prevalence within sports (1.0-0.01), so a 2-cluster approach (on SVD basis) will split the topic of sports into two parts. While this is a game example, we note that if we say the frequencies at 0.01, ideally sports will all be below the threshold."}, {"heading": "3.3 Topic recovery using Thresholded SVD", "text": "(1)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "4 Experimental Results", "text": "We compare the threshold SVD-based k-means (TSVD2) algorithm 3.2 with the algorithms of [5], Recover-KL and Recover-L2, using the code provided by the authors. 3 We first provide empirical support for the assumptions of the algorithm in Section 3.1, namely removing the dominant theme and keyword assumption.Then we show on 4 different semi-synthetic data that TSVD provides a good or better recovery of topics than the recovery algorithms. Finally, we show on real data sets that the algorithm works as well as [5] in terms of perplexity and topic coherence.Implementation details: TSVD parameters (w0, \u03b50, \u03b50, \u03b3) are not known in advance for real data sets. We empirically tested the multiple settings and the following values gave the best performance."}, {"heading": "4.1 Algorithm Assumptions", "text": "To verify the dominant theme and keyword assumptions, we first perform 1,000 iterations of Gibbs samples on the real corpus and learn the posterior distribution of the document on the theme ({W., j}) for each document in the corpus (by separating over 10 stored states, separated by 50 iterations after the 500 burn-in iterations).We will use this posterior distribution of the document as the document that generates the distribution of the two assumptions.The fraction of documents that have almost pure themes with the highest topic weight of at least 0.95 (previous year = 0.05) shows the proportion of documents in each corpus that this assumption is well justified (on average 64% of documents satisfy the assumption) and that there are also substantial themes with the highest topic weight of at least 0.95 (previous year = 0.05).The results indicate that the prevailing assumption is well justified (on average 64% of documents satisfy the assumption) and that there are also substantial numbers of documents that contain virtually unsatisfactory documents."}, {"heading": "4.2 Empirical Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Topic Recovery on Semi-Synthetic Data", "text": "Semi-synthetic data: In the following [5] we create semi-synthetic corpora from the MCMC-trained LDA model to ensure that the synthetic corpora retains the properties of real data. Gibbs sampling5http: / / qwone.com / \u02dc jason / 20Newsgroups 6 (1 k \u2211 k = 1 1 1 | Tl | \u2211 i-Sl-j-Tl Aij)"}, {"heading": "Corpus Documents Recover-L2 Recover-KL TSVD % Improvement", "text": "For NIPS, NYT, and Pubmed, we use k = 50 topics, for 20NewsGroup k = 20, and mean document lengths of 1000, 300, 100, and 200. Note that synthetic data is not guaranteed to meet the prevailing theme assumption for each document (on average, about 80% of the documents meet the assumption for the value of (\u03b1, \u03b2) tested in Section 4.1, Topic Recovery: We learn the word-theme distributions (M) for the semi-synthetic corpora with TSVD, and the recovery algorithms of [5]. Given these learned topic distributions and the original data generation of M (M), we align the themes of M and M by bipartite, and rearrange the columns of the recovery construction from M = 1."}, {"heading": "4.2.2 Topic Recovery on Real Data", "text": "The perplexity of a set of D test documents, where each document consists of mj words denoted by wj, is also defined as follows: perp (Dtest) = exp {\u2212 \u2211 D = 1 log p (wj) \u2211 Dj = 1mj}. To evaluate the perplexity on real data, the subject sets held consist of 350 documents for NIPS, 10,000 documents for NYT and Pubmed, and 6780 documents for 20NewsGroup. Table 3 shows the results of the perplexity on the 4 data sets. TSVD gives comparable perplexity on Recover-KL, results that are slightly better on NYT and 20NewsGroup, which are larger records with a moderately high medium data column. (Topic Coherence: [11] proposed Topic Coherence as a measure of the setic quality of the topics learned by approximating the quality of a TC topic."}, {"heading": "Conclusion", "text": "A standard SVD-based approach will not be able to detect these topics, but TSVD, a threshold-based SVD-based approach proposed in this paper, discovers these topics. Although SVD is time consuming, there have been a variety of random sample-based approaches recently that make SVD more easily applicable to massive corporations that can be distributed to many servers. We believe that the threshold-based SVD can be applied more broadly to problems similar to matrix factorization, in addition to topic restoration, and will form the basis for future research."}, {"heading": "TSVD Recover-KL Gibbs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "TSVD Recover-KL Gibbs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Line of Proof", "text": "We describe the lemmas that we prove in order to determine the result. Detailed evidence can be found in section B."}, {"heading": "A.1 General Facts", "text": "We start with a sequence of non-local minimum assumptions. We use this assumption exclusively through this lemm.Lemma A.1 Let pi (, l) be as in (6). If we have for some time 0, 0, 1,.., m) and 1, 0, 1, 0 pi (, l) as well as 2, 0 pi (, l) and also 3, 0 pi (, l), then, pi (, l). Next, we cite a technical lemma that is used repeatedly. It states that for each i, q, l, the empirical probability that Aij = 0 pi (, l) is close to the true probability. It is not surprising that we will specify it with H-C. But we will specify a consequence in the form we need in the sequel.Lemma A.2 Let us be pi (, l) and qi (, l) as in (6) and (7)."}, {"heading": "A.1.1 Properties of Thresholding", "text": "Say that a threshold (A) ij (2) l (A) l (A) l (A) l (A) l (A) l (A) l (A) l (A) l (A) l (A) l (B) l (B) l (B) l (B) l (B) l (B) l (B) l (B) l (B) l (B) l) l (B) l (B) l (B) l (B) l (B) l (B) l) l (B) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l (B) l) l (B) l (B) l) l (B) l (B) l) l (B) l (B) l) l (B) l) l (B) l) l) l (B) l) l (B) l) l) l (B) l) l (B) l) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l) l) l (B) l) l (B) l) l) l (B) l) l (B) l) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l (B) l) l (B) l) l (B) l) l (B) l) l (B) l (B) l (B) l) l (B) l (B) l) l (B) l (B) l) l (B) l (B) l (B) l (B) l (B) l) l (B) l (B) l) l (B) l (B) l) l (B) l) l (B) l (B) l (B) l) l) l (B) l (B) l) l) l (l (B) l) l"}, {"heading": "A.1.2 Proximity", "text": "Next, we would like to show that clusters like in TSVD correctly identify the dominant themes for most documents, i.e. that Rl \u2248 Tl for all l. For this purpose, we will use a theorem from [9] [see also [10], which in this context states: Theorem A.7 If all but a fraction of B \u00b7, j meet the \"proximity condition,\" then TSVD correctly identifies the dominant theme in all but c1f fractions of documents according to the polynomial number of iterations. To describe the proximity condition, first of all is the maximum over all directions v of the square root of the mean square distance from B., j to \u00b5., j, i.e., \u03c32 = Max."}, {"heading": "B Proofs of Correctness", "text": "We start by remembering the Ho \ufffd ffding-Chernoff (H-C) inequality in the form in which we use it (< Ho \ufffd ffding-Chernoff IfX is the average of r & # 246; public random variables with values in [0, 1] and E (X) = f & # 246; ll (f & # 246; ell)."}, {"heading": "B.1 Bounding the Spectral norm", "text": "Theorem B.4.Theorem B.5 [15, Theorem 5.44] Suppose R is a d \u00b7 r matrix with columns R \u00b7 j that are independently identical vector-weighted variables. We will apply the random matrix theory, in particular the following theorem to theorem B.4.Theorem B.5 [15, Theorem 5.44] Suppose R is a d \u00b7 r matrix with columns R \u00b7 j that are independently identical vector-weighted variables."}, {"heading": "B.2 Proving Proximity", "text": "Theorem (B.4) shows that the probability of failure is higher in the definition A.1.2 than in the definition A.1.2. Thus, it is sufficient to prove that the probability of failure is higher than the probability of failure. The probability of failure is higher than the probability of failure. The probability of failure is higher than the probability of failure. The probability of failure is higher than the probability of failure. The probability of failure is higher than the probability of failure. The probability of failure is higher than the probability of failure. The probability of failure is higher than the probability of failure. The probability of failure of bankruptcy is higher than the probability of failure."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents<lb>are drawn from admixtures of distributions over words, known as topics. The<lb>inference problem of recovering topics from such a collection of documents drawn<lb>from admixtures, is NP-hard. Making a strong assumption called separability, [4]<lb>gave the first provable algorithm for inference. For the widely used LDA model,<lb>[6] gave a provable algorithm using clever tensor-methods. But [4, 6] do not learn<lb>topic vectors with bounded l1 error (a natural measure for probability vectors).<lb>Our aim is to develop a model which makes intuitive and empirically supported<lb>assumptions and to design an algorithm with natural, simple components such as<lb>SVD, which provably solves the inference problem for the model with bounded l1<lb>error. A topic in LDA and other models is essentially characterized by a group of<lb>co-occurring words. Motivated by this, we introduce topic specific Catchwords,<lb>a group of words which occur with strictly greater frequency in a topic than any<lb>other topic individually and are required to have high frequency together rather<lb>than individually. A major contribution of the paper is to show that under this<lb>more realistic assumption, which is empirically verified on real corpora, a singu-<lb>lar value decomposition (SVD) based algorithm with a crucial pre-processing step<lb>of thresholding, can provably recover the topics from a collection of documents<lb>drawn from Dominant admixtures. Dominant admixtures are convex combination<lb>of distributions in which one distribution has a significantly higher contribution<lb>than the others. Apart from the simplicity of the algorithm, the sample complexity<lb>has near optimal dependence on w0, the lowest probability that a topic is domi-<lb>nant, and is better than [4]. Empirical evidence shows that on several real world<lb>corpora, both Catchwords and Dominant admixture assumptions hold and the pro-<lb>posed algorithm substantially outperforms the state of the art [5].", "creator": "LaTeX with hyperref package"}}}