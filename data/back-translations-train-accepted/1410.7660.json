{"id": "1410.7660", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2014", "title": "Non-convex Robust PCA", "abstract": "We propose a new method for robust PCA -- the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support. Our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices, and the set of sparse matrices; each projection is {\\em non-convex} but easy to compute. In spite of this non-convexity, we establish exact recovery of the low-rank matrix, under the same conditions that are required by existing methods (which are based on convex optimization). For an $m \\times n$ input matrix ($m \\leq n)$, our method has a running time of $O(r^2mn)$ per iteration, and needs $O(\\log(1/\\epsilon))$ iterations to reach an accuracy of $\\epsilon$. This is close to the running time of simple PCA via the power method, which requires $O(rmn)$ per iteration, and $O(\\log(1/\\epsilon))$ iterations. In contrast, existing methods for robust PCA, which are based on convex optimization, have $O(m^2n)$ complexity per iteration, and take $O(1/\\epsilon)$ iterations, i.e., exponentially more iterations for the same accuracy.", "histories": [["v1", "Tue, 28 Oct 2014 15:33:13 GMT  (553kb,D)", "http://arxiv.org/abs/1410.7660v1", "Extended abstract to appear in NIPS 2014"]], "COMMENTS": "Extended abstract to appear in NIPS 2014", "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT stat.ML", "authors": ["praneeth netrapalli", "u n niranjan", "sujay sanghavi", "animashree anandkumar", "prateek jain 0002"], "accepted": true, "id": "1410.7660"}, "pdf": {"name": "1410.7660.pdf", "metadata": {"source": "CRF", "title": "Non-convex Robust PCA", "authors": ["Praneeth Netrapalli", "U N Niranjan", "Sujay Sanghavi", "Animashree Anandkumar", "Prateek Jain"], "emails": [], "sections": [{"heading": null, "text": "(r2mn) per iteration and requires O (log (1 /)) iterations to achieve an accuracy of. This is close to the runtimes of simple PCA through the power method, which requires O (rmn) iterations per iteration and O (log (1 /) iterations. In contrast, the existing robust PCA methods based on convex optimization have O (m2n) complexity per iteration and take O (1 /) iterations, i.e. exponentially more iterations for the same accuracy. Experiments with synthetic and real data demonstrate the improved speed and accuracy of our method over existing convex implementations. Keywords: Robust PCA, matrix decomposition, non-convex methods, alternating projections."}, {"heading": "1 Introduction", "text": "Principal Component Analysis (PCA) is a common method of pre-processing and denoization, where a low approximation to the input matrix (such as the covariance matrix) is performed. Although PCA is easy to implement via self-decomposition, it is sensitive to the presence of outlier techniques as it attempts to fit the outliers into the low order. To overcome this, the term rugged PCA is used, where the goal is to remove sparse corruption from an input matrix and maintain a low marginal number of approximations. Robust PCA has been used in a wide range of applications, including background modeling of PCA, with the goal of removing sparse corruption from an input matrix and maintaining a low marginal number of approximations."}, {"heading": "1.1 Summary of Contributions", "text": "We propose a simple, intuitive algorithm to achieve an error of low iteration costs and a fast convergence rate. (We demonstrate tight guarantees for the recovery of low and low ranking components that correspond to those for the convex thresholds.) In the process, we derive a new matrix of simple exchange rates when subject to them. Our experiments show significant gains in terms of speed-ups over the convex relaxation techniques, especially since we describe the size of the input matrices (non-convex) projections on low rankings and sparse matrices. For a m \u00d7 n matrix, our method has a runtime of O (r2mn), where r is the rank of the low ranking component. Thus, our method has a linear convergence, i.e. it requires O (log) iterations to achieve an error where r is the low ranking component."}, {"heading": "1.2 Related Work", "text": "In recent years we have received a lot of attention, starting from the basic work of [CSPW11, CLMW11], in which they have considered the restoration of an incoherent, low ranking to be insufficient. [CSPW11, CLMW11], where they have an insufficient, low ranking quota. [CSPW11, CLMW11], where they have an insufficient ranking quota. [CSPW11], [CSPW11], [CSPW11], [CSPW11], [CSPW11], [CSPW11], [CSPWSPW11], [CSPWSPW11], [CSPSPW11], and CSPS, [CSPW11, and CSPS], [CSPW11, CSPS, CSP11, CSP11, CSPCSP11, CSPCSPCSP11, and WSP11], and WSP11, CSPWSP11, CSPWSP11, and WSP11, CSPWSP11, and WSP11."}, {"heading": "2 Algorithm", "text": "In this section we present our algorithms for the robust PCA problematic: \"The problem of the robust PCA can be formulated as the following optimization problem:\" There is only a limited number of. \"(\" There is only a limited number of. \")\" There is only a limited number of. \"(\" There is only a limited number of. \")\" (\"There is only a limited number of.\") \"(\" There is only a limited number of. \")\" (\"There is only a limited number of.\") \"(\" There is only a limited number of. \")\" (\"There is only a limited number of.\" (\"There is only a limited number of.\") \"(\" There is only a limited number of. \"(\" There is only a limited number of. \")\" (\"There is only a limited number of.\" (\"There is only a limited number of.\") \"(\" There is only a limited number of. \"(\" There is only a limited number of. \")\" (\"There is only a limited number of.\" (\")\" (\"There is only a limited number of.\"). \"(\" (\"There is only a limited number of.\"). \"(\"). \"(\" (\"There.\"). \"(\" (\"There.\"). \"(\" There. \"(\"). \"(\" There. \").\" (\"There.\" (\"There.\". \".\". \".\". \".\". \".\"... \".\". \"..\". \".\". \".\"..... \"....\". \"...\". \".....\". \"....\".. \"...\". \".\".. \".\". \".\".. \"..\". \".\". \".\" (\".....\". \".\". \".\". \"..\". \".\".. \"..\". \".\".. \".\". \".\"... \".\". \"..\". \"....\". \".\".. \".\". \"....\". \".."}, {"heading": "3 Analysis", "text": "In this section we present our main result about the correctness of AltProj. We assume the following conditions: (L1) The rank of L * is at most r. (L2) L * is \u00b5-incoherent, that is, if L * = U * p * (V *) > is the SVD of L *, then (U *) i * p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p p p \u00b2 p p p p p p p p p \u00b2 p p p p p p p p p p p \u00b2 p p p p p p p = p p p p p p p \u00b2 p p p p p p) p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p p \u00b2 p p p \u00b2 p p p p \u00b2 p p p p p \u00b2 p p p p p = p p p p p = p p p p p p p p p p p p = p p p p p p = p p p p p p p p p p p p p p p p p p) p = p) p \u00b2 p p p \u00b2 p p p p p p \u00b2 p p p p p \u00b2 p p p p p p p p \u00b2 p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p"}, {"heading": "L\u0302 and S\u0302 of Algorithm 1 satisfy:\u2225\u2225\u2225L\u0302\u2212 L\u2217\u2225\u2225\u2225", "text": "4. Our conditions L1, L2 and S1 also coincide with the conditions required by the convex method of recovery as defined in [HKZ11]. Note (Convergence Rate): Our method has a linear convergence rate, i.e. O (Protocol (1 /))), to achieve an error, and therefore we offer a strongly polynomic method for robust PCA. In contrast, the best known limit for convex methods for robust PCA is O (1 /), iterations to convergence (Log (1 /), and therefore we offer a strongly polynomial method for robust PCA."}, {"heading": "3.1 Proof Sketch", "text": "We now present the most important steps in the proof of our analysis: When an incoherence and a low level is reached, a detailed proof is provided. (Step I: Reduction to the symmetric case, with all the matrices involved being symmetrical.) See Appendix II: Show the decay in the individual steps. (Step II: Show the decay in the individual steps.) After projecting onto the series of rank-k matrices, we can reduce the problem to the symmetrical case, in which all the matrices involved are symmetrical. (Step II: Show the decay in the individual steps.) Hence, L (t + 1) is achieved by using the uppermost major components of a perturbation of L (S)."}, {"heading": "4 Experiments", "text": "The aim of this study is twofold: a) We find that our method actually restores the low rate and the sparse part of the IALM method without any significant parameters. b) We have implemented our method in Matlab and applied a Matlab implementation of the IALM method by looking at both synthetic experiments and experiments on real data that include the problem of superficial separation in a video. Each of our results for synthetic datasets is set over 5 runs.Parameter Setting: Our pseudo-code (algorithm 1) writes the thresholds in Step 4, which depend on the singular values of the low ranks."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a non-convex method for robust PCA, which consists of alternating projections on low and sparse matrices. We established a global convergence of our method under conditions consistent with those of convex methods. At the same time, our method has much faster runtimes and superior experimental performance. This work raises a number of interesting questions for future studies. While we are comparing the convex methods under the deterministic thrift model, the study of the random thrift model is of interest. Our noise recovery results are based on deterministic noise; improving the results under random noise needs to be investigated. There are many decomposition problems outside the robust PCA environment, such as structured thrift models, robust tensor PCA problem, etc. It is interesting to see if we can establish a global convergence for non-convex methods in this environment."}, {"heading": "Acknowledgements", "text": "AA and UN would like to acknowledge NSF funding CCF-1219234, ONR N00014-14-1-0665 and the Microsoft Faculty Scholarship. SS would like to acknowledge NSF funding 1302435, 0954059, 1017525 and DTRA funding HDTRA113-1-0024. PJ would like to thank Nikhil Srivastava and Deeparnab Chakrabarty for some insightful discussions during the project."}, {"heading": "A Proof of Theorem 1", "text": "The first problem is the well-known Weyl inequality in the matrix setting (Bha97). (Bha97) Let us assume that B = A + E is a n \u00b7 n matrix. (Bha97) Let us assume that B + E is a n \u00b7 n matrix. (Bha97) Let us assume that B + E is a n \u00b7 n matrix. (Bha97) Let us assume. (Bha97) Let us assume. (Bha97) Let us assume. (Bha97) Let us assume. (Bha97) Let us assume. (Bha97) Let us assume. (Bha97) Let us accept. (Bha97) Let us accept. (Bha97) Let us accept. (Bha97) Let us accept. (Bha97) Let us accept. (Bha97) Let us accept. (Bha97) Let us accept. (Bha97) Let us accept. (Bha97) Let us accept. (Bha97) Let us accept. (Bha97). (Bha97) Let us accept."}, {"heading": "B Proof of Theorem 2", "text": "In this section, we demonstrate that theorem 2 is essentially the same in relation to the structure of L (t) and E (t) as in Appendix A. The first problem is a generalization of Lemma 6 and shows that the threshold in (2) is close to the threshold at which M (t) is replaced by L (t). Let L (t), S (n), N) be symmetrical and fulfill the assumptions of Theorem 2, and let S (t) be the lowest level of the lowest level of Algorithm 1. Let us leave the eigenvalues of L (t) so that we have the eigenvalues of Lemm."}, {"heading": "C Additional experimental results", "text": "This year, the time has come for us to find a solution that enables us to find a solution, that enables us to find a solution, that enables us to find a solution, that enables us to find a solution, that enables us to find a solution, that enables us to find a solution, that enables us to find a solution, that enables us to find a solution, that enables us to find a solution, and that enables us to find a solution that we are able to find a solution."}], "references": [{"title": "Learning Sparsely Used Overcomplete Dictionaries via Alternating Minimization", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"], "venue": "Available on arXiv:1310.7991,", "citeRegEx": "Agarwal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2013}, {"title": "Tensor Methods for Learning Latent Variable Models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Available at arXiv:1210.7559,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "author": ["A. Agarwal", "S. Negahban", "M. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Incoherence-Optimal Matrix Completion", "author": ["Y. Chen"], "venue": "ArXiv e-prints,", "citeRegEx": "Chen.,? \\Q2013\\E", "shortCiteRegEx": "Chen.", "year": 2013}, {"title": "Robust principal component analysis", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "J. ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["Venkat Chandrasekaran", "Sujay Sanghavi", "Pablo A. Parrilo", "Alan S. Willsky"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2011}, {"title": "Clustering sparse graphs", "author": ["Yudong Chen", "Sujay Sanghavi", "Huan Xu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Spectral statistics of Erd\u0151s\u2013R\u00e9nyi graphs I: Local semicircle law", "author": ["L\u00e1szl\u00f3 Erd\u0151s", "Antti Knowles", "Horng-Tzer Yau", "Jun Yin"], "venue": "The Annals of Probability,", "citeRegEx": "Erd\u0151s et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Erd\u0151s et al\\.", "year": 2013}, {"title": "On the provable convergence of alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": null, "citeRegEx": "Hardt.,? \\Q2013\\E", "shortCiteRegEx": "Hardt.", "year": 2013}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "ITIT,", "citeRegEx": "Hsu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2011}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In STOC,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Matrix alps: Accelerated low rank and sparse matrix reconstruction", "author": ["Anastasios Kyrillidis", "Volkan Cevher"], "venue": "In SSP Workshop,", "citeRegEx": "Kyrillidis and Cevher.,? \\Q2012\\E", "shortCiteRegEx": "Kyrillidis and Cevher.", "year": 2012}, {"title": "Efficient algorithms for collaborative filtering", "author": ["Raghunandan H. Keshavan"], "venue": "Phd Thesis, Stanford University,", "citeRegEx": "Keshavan.,? \\Q2012\\E", "shortCiteRegEx": "Keshavan.", "year": 2012}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Zhouchen Lin", "Minming Chen", "Yi Ma"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Statistical modeling of complex backgrounds for foreground object detection", "author": ["Liyuan Li", "Weimin Huang", "IY-H Gu", "Qi Tian"], "venue": "ITIP,", "citeRegEx": "Li et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Li et al\\.", "year": 2004}, {"title": "Holistic 3d reconstruction of urban structures from low-rank textures", "author": ["Hossein Mobahi", "Zihan Zhou", "Allen Y. Yang", "Yi Ma"], "venue": "In ICCV Workshops,", "citeRegEx": "Mobahi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mobahi et al\\.", "year": 2011}, {"title": "Phase retrieval using alternating minimization", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In NIPS,", "citeRegEx": "Netrapalli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Netrapalli et al\\.", "year": 2013}, {"title": "Guarantees for Stochastic ADMM in High Dimensions", "author": ["H. Sedghi", "A. Anandkumar", "E. Jonckheere"], "venue": null, "citeRegEx": "Sedghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sedghi et al\\.", "year": 2014}, {"title": "Sparse additive text models with low rank background", "author": ["Lei Shi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shi.,? \\Q2013\\E", "shortCiteRegEx": "Shi.", "year": 2013}, {"title": "Solving multiple-block separable convex minimization problems using two-block alternating direction method of multipliers", "author": ["X. Wang", "M. Hong", "S. Ma", "Z. Luo"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Robust pca via outlier pursuit", "author": ["Huan Xu", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}], "referenceMentions": [], "year": 2014, "abstractText": "We propose a new method for robust PCA \u2013 the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support. Our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices, and the set of sparse matrices; each projection is non-convex but easy to compute. In spite of this non-convexity, we establish exact recovery of the lowrank matrix, under the same conditions that are required by existing methods (which are based on convex optimization). For anm\u00d7n input matrix (m \u2264 n), our method has a running time of O ( rmn ) per iteration, and needs O (log(1/ )) iterations to reach an accuracy of . This is close to the running times of simple PCA via the power method, which requires O (rmn) per iteration, and O (log(1/ )) iterations. In contrast, the existing methods for robust PCA, which are based on convex optimization, have O ( mn ) complexity per iteration, and take O (1/ ) iterations, i.e., exponentially more iterations for the same accuracy. Experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations.", "creator": "LaTeX with hyperref package"}}}