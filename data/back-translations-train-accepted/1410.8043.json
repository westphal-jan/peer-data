{"id": "1410.8043", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2014", "title": "High-Performance Distributed ML at Scale through Parameter Server Consistency Models", "abstract": "As Machine Learning (ML) applications increase in data size and model complexity, practitioners turn to distributed clusters to satisfy the increased computational and memory demands. Unfortunately, effective use of clusters for ML requires considerable expertise in writing distributed code, while highly-abstracted frameworks like Hadoop have not, in practice, approached the performance seen in specialized ML implementations. The recent Parameter Server (PS) paradigm is a middle ground between these extremes, allowing easy conversion of single-machine parallel ML applications into distributed ones, while maintaining high throughput through relaxed \"consistency models\" that allow inconsistent parameter reads. However, due to insufficient theoretical study, it is not clear which of these consistency models can really ensure correct ML algorithm output; at the same time, there remain many theoretically-motivated but undiscovered opportunities to maximize computational throughput. Motivated by this challenge, we study both the theoretical guarantees and empirical behavior of iterative-convergent ML algorithms in existing PS consistency models. We then use the gleaned insights to improve a consistency model using an \"eager\" PS communication mechanism, and implement it as a new PS system that enables ML algorithms to reach their solution more quickly.", "histories": [["v1", "Wed, 29 Oct 2014 16:19:21 GMT  (2446kb,D)", "http://arxiv.org/abs/1410.8043v1", "19 pages, 2 figures"]], "COMMENTS": "19 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["wei dai", "abhimanu kumar", "jinliang wei", "qirong ho", "garth a gibson", "eric p xing"], "accepted": true, "id": "1410.8043"}, "pdf": {"name": "1410.8043.pdf", "metadata": {"source": "CRF", "title": "High-Performance Distributed ML at Scale through Parameter Server Consistency Models", "authors": ["Wei Dai", "Abhimanu Kumar", "Jinliang Wei", "Qirong Ho", "Garth Gibson", "Eric P. Xing"], "emails": ["wdai@cs.cmu.edu,", "abhimank@cs.cmu.edu,", "jinlianw@cs.cmu.edu,", "garth@cs.cmu.edu,", "epxing@cs.cmu.edu,", "hoqirong@gmail.com"], "sections": [{"heading": "Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "Consistency Models for Parameter Servers", "text": "This is possible because ML algorithms have an iterative convergence and error tolerance: ML algorithms will adapt to a local optimum, even if there are errors in the algorithmic approach itself (such as stochasticity in randomized methods).In a distributed environment, multiple workers must simultaneously generate updates to shared global parameters. Hence, the assertion of strong consistency is directly reflected) quickly leads to frequent, time-consuming synchronizations, and thus to very limited speed from parallelization. Therefore, a loose model needs to be defined that allows for loose consistency, which is closely related to the strong synchronization of strong execution of strong insight."}, {"heading": "Eager Stale Synchronous Parallel (ESSP)", "text": "In fact, most people are able to move to a different world, in which they are able to live than in a world in which they are able to live, in which they are able to live."}, {"heading": "Theoretical Analysis", "text": "In this section, we will theoretically analyze VAP and ESSP and show how they influence the convergence of ML algorithms. For reasons of space, all the evidence is in the appendix. As explained above, we base our analysis on ML algorithms of stochastic gradient descent (SGD) (due to their high popularity for big data) and prove the convergence of SGD under VAP and ESSP. We will now explain SGD in the context of a matrix completion problem."}, {"heading": "SGD for Low Rank Matrix Factorization", "text": "& & & the problem is that we have these missing entries based on the known entries Dobs (# 2F # 2F # 2F # 2F # 2F & # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F # 2F & # 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F & 2F"}, {"heading": "Comparison of VAP and ESSP", "text": "From theorems 2 and 6, we see that both VAP and (E) SSP achieve a decreasing variance. However, the VAP convergence is much more sensitive to its tuning parameters (the VAP threshold) than (E) SSP, whose tuning parameter is durability, as shown by the O (B) terms in shelf life. This implies that the shelf life variance quickly disappears in the (E) SSP deviation containing only the shelf life threshold."}, {"heading": "ESSPTable: An efficient ESSP System", "text": "In fact, it is so that it will be able to fix the mentioned bugs in order to fix them."}, {"heading": "Experiments", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Related Work and Discussion", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "Appendix", "text": "Theorem 1 (SGD under VAP, convergence in expectation) Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex function f (x) = Convex (x) Size t (x) T x (x) Size T x (x) T x (x) Size T x (x) Convex (x) Size T (x) + Convex (x) Size + Convex (x) Convex (x) Convex (x) Convex (x) Convex (x) Size f (x) Convex (x) Convex (x) Convex (x) Convex (x) Convex (x) Convex (x) Size f (x) Convex (x) Convex (x) Convex (x) Convex (x) Convex (x) Convex (x) Convex (x) Convex (x) Convex (x) Convex (x) Convex (convex (x) Convex (x) Convex (x) Convex (x) x) x (x) x (x) x (x) x (x (x) Convex) Convex (x) Convex (x) Convex (x) x (x) x (x) x (x) x) x (x (x) x (x (x) x) x (x) x) x (x"}, {"heading": "Let at := \u03b7\u221atL", "text": "It should be noted that the following terms are: b), b), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c)), c))), c), c)))), c))), \"c)),\" c))))), \"c)),\"))), \"))))),\" c), \")),\")), \")),\")), \")),\")), \"),\")), \"),\"), \"),\"), \"),\"), \"),\"), \"),\"), \"),\")), \"),\"), \"),\"), \"),\"), \")))),\"), \")),\"), \"),\")), \")))),\"), \"))))),\"), \")))),\")))))))), \")))),\")))))))))))))"}], "references": [{"title": "Scalable inference in latent variable models", "author": ["Amr Ahmed", "Moahmed Aly", "Joseph Gonzalez", "Shravan Narayanamurthy", "Alexander J. Smola"], "venue": "In WSDM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The hadoop distributed file system: Architecture and design", "author": ["Dhruba Borthakur"], "venue": "Hadoop Project Website,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Parallel coordinate descent for l1regularized loss minimization", "author": ["Joseph K. Bradley", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin"], "venue": "In International Conference on Machine Learning (ICML", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Solving the straggler problem with bounded staleness", "author": ["James Cipar", "Qirong Ho", "Jin Kyu Kim", "Seunghak Lee", "Gregory R. Ganger", "Garth Gibson", "Kimberly Keeton", "Eric Xing"], "venue": "In HotOS \u201913. Usenix,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Exploiting bounded staleness to speed up big data analytics", "author": ["Henggang Cui", "James Cipar", "Qirong Ho", "Jin Kyu Kim", "Seunghak Lee", "Abhimanu Kumar", "Jinliang Wei", "Wei Dai", "Gregory R. Ganger", "Phillip B. Gibbons", "Garth A. Gibson", "Eric P. Xing"], "venue": "In 2014 USENIX Annual Technical Conference (USENIX ATC", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["J Dean", "G Corrado", "R Monga", "K Chen", "M Devin", "Q Le", "M Mao", "M Ranzato", "A Senior", "P Tucker", "K Yang", "A Ng"], "venue": "NIPS 2012", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "J.-K. Kim", "S. Lee", "P.B. Gibbons", "G. Gibson", "G.R. Ganger", "E.P. Xing"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "and A Strehl", "author": ["J Langford", "L Li"], "venue": "Vowpal wabbit online learning project", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Primitives for dynamic big model parallelism", "author": ["Seunghak Lee", "Jin Kyu Kim", "Xun Zheng", "Qirong Ho", "Garth A Gibson", "Eric P Xing"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["Mu Li", "David G. Andersen", "Jun Woo Park", "Alexander J. Smola", "Amr Ahmed", "Vanja Josifovski", "James Long", "Eugene J. Shekita", "Bor-Yiing Su"], "venue": "In Operating Systems Design and Implementation (OSDI),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Parameter server for distributed machine learning", "author": ["Mu Li", "Li Zhou Zichao Yang", "Aaron Li Fei Xia", "David G. Andersen", "Alexander Smola"], "venue": "NIPS workshop,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Parameter server for distributed machine learning, big learning workshop", "author": ["Mu Li", "Li Zhou", "Zichao Yang", "Aaron Li", "Fei Xia", "Dave Andersen", "Alex Smola"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Graphlab: A new parallel framework for machine learning", "author": ["Yucheng Low", "Joseph Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin", "Joseph M. Hellerstein"], "venue": "In Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Feng Niu", "Benjamin Recht", "Christopher R\u00e9", "Stephen J Wright"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Plda: Parallel latent dirichlet allocation for large-scale applications", "author": ["Yi Wang", "Hongjie Bai", "Matt Stanton", "Wen-Yen Chen", "Edward Y. Chang"], "venue": "In Proceedings of the 5th International Conference on Algorithmic Aspects in Information and Management,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Si Si", "Inderjit S Dhillon"], "venue": "In ICDM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Spark: Cluster computing with working sets", "author": ["Matei Zaharia", "N.M. Mosharaf Chowdhury", "Michael Franklin", "Scott Shenker", "Ion Stoica"], "venue": "Technical Report UCB/EECS-2010-53, EECS Department,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "The surging data volumes generated by internet activity and scientific research [7] put tremendous pressure on Machine Learning (ML) methods to scale beyond the computation and memory of a single machine.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 7, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 4, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 5, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 10, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 0, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 7, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 4, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 5, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 10, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 7, "context": "Many general-purpose Parameter Server (PS) systems [8, 5, 6] of ML computation provide a Distributed Shared Memory (DSM) solution to the Big Data and Big Model issues.", "startOffset": 51, "endOffset": 60}, {"referenceID": 4, "context": "Many general-purpose Parameter Server (PS) systems [8, 5, 6] of ML computation provide a Distributed Shared Memory (DSM) solution to the Big Data and Big Model issues.", "startOffset": 51, "endOffset": 60}, {"referenceID": 5, "context": "Many general-purpose Parameter Server (PS) systems [8, 5, 6] of ML computation provide a Distributed Shared Memory (DSM) solution to the Big Data and Big Model issues.", "startOffset": 51, "endOffset": 60}, {"referenceID": 10, "context": "It should be noted that not all PS systems provide a DSM interface; some espouse an arguably less-convenient push/pull interface that requires users to explicitly decide which parts of the ML model need to be communicated [12].", "startOffset": 222, "endOffset": 226}, {"referenceID": 2, "context": "At the same time, the iterative-convergent nature of ML programs presents unique opportunities and challenges that do not manifest in traditional database applications: for example, ML programs lend themselves well to stochastic subsampling or randomized algorithms, but at the same time exhibit complex dependencies or correlations between parameters that can make parallelization difficult [3, 11].", "startOffset": 392, "endOffset": 399}, {"referenceID": 9, "context": "At the same time, the iterative-convergent nature of ML programs presents unique opportunities and challenges that do not manifest in traditional database applications: for example, ML programs lend themselves well to stochastic subsampling or randomized algorithms, but at the same time exhibit complex dependencies or correlations between parameters that can make parallelization difficult [3, 11].", "startOffset": 392, "endOffset": 399}, {"referenceID": 14, "context": "Recent works [16, 8, 9] have introduced relaxed consistency models to trade off between parameter read accuracy and read throughput, and show promising speedups over fully-consistent models; their success is underpinned by the error-tolerant nature of ML, that \u201cplays nicely\u201d with relaxed synchronization guarantees \u2014 and in turn, relaxed synchronization allows system designers to achieve higher read throughput, compared to fully-consistent models.", "startOffset": 13, "endOffset": 23}, {"referenceID": 7, "context": "Recent works [16, 8, 9] have introduced relaxed consistency models to trade off between parameter read accuracy and read throughput, and show promising speedups over fully-consistent models; their success is underpinned by the error-tolerant nature of ML, that \u201cplays nicely\u201d with relaxed synchronization guarantees \u2014 and in turn, relaxed synchronization allows system designers to achieve higher read throughput, compared to fully-consistent models.", "startOffset": 13, "endOffset": 23}, {"referenceID": 3, "context": "Recent works on PS have only focused on system optimizations in PS using various heuristics like async relaxation [4] and uneven updates propagation based on parameter values [12].", "startOffset": 114, "endOffset": 117}, {"referenceID": 10, "context": "Recent works on PS have only focused on system optimizations in PS using various heuristics like async relaxation [4] and uneven updates propagation based on parameter values [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 12, "context": "However, VAP, of which the basic idea or principle is attempted in [14], can be problematic because bounding the value of in-transit updates amounts to tight synchronization.", "startOffset": 67, "endOffset": 71}, {"referenceID": 7, "context": "We propose Eager Stale Synchronous Parallel (ESSP), a variant of Stale Synchronous Parallel (SSP, a bounded-iteration model that is fundamentally different from VAP) introduced in [8], and formally and empirically show that ESSP is a practical and easily realizable scheme for parallelization.", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "These variance bounds provide a deeper characterization of convergence (particularly solution stability) under SSP and VAP, unlike existing PS theory that is focused only on expectation bounds [8].", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "Eager Stale Synchronous Parallel (ESSP) In order to design a consistency model that is practically efficient while providing proper correctness guarantees, we consider an iteration-based consistency model called Stale Synchronous Parallel (SSP) [8], that lends itself to an efficient PS implementation.", "startOffset": 245, "endOffset": 248}, {"referenceID": 7, "context": "In the sequel, we will show that better staleness profiles lead to faster ML algorithm convergence, by proving new, tighter convergence bounds based on average staleness and the staleness distributions (unlike the simpler worst-case bounds in [8]).", "startOffset": 243, "endOffset": 246}, {"referenceID": 7, "context": "The analysis is similar to [8], but we use the real-time sequence x\u0302t as our reference sequence and VAP condition instead of SSP.", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "(SGD under SSP, convergence in expectation [8], Theorem 1) Given convex function f(x) = \u2211T t=1 ft(x) with suitable conditions as in Theorem 1, we use gradient descent with updates ut = \u2212\u03b7t\u2207ft(x\u0303t) generated from noisy view x\u0303t and \u03b7t = \u03b7 t .", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "Then Var t = \u2211d i=1 E[x\u0306 2 ti] \u2212 E[x\u0306ti] In contrast to [8], we do not assume read-my-write.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "In contrast to VAP, ESSP does not suffer as much from these drawbacks, because: (1) SSP has a weaker theoretical dependency on its staleness threshold (than VAP does on its value-bound threshold), thus it is usually unnecessary to decrease the staleness as the ML algorithm approaches convergence; this is evidenced by the SSP paper [8], which achieved stable convergence even though they did not decrease staleness gradually during ML algorithm execution.", "startOffset": 333, "endOffset": 336}, {"referenceID": 7, "context": "This differs from the SSPTable in [8] where the server passively sends out updates upon client\u2019s read request (which happens each time a client\u2019s local cache becomes too stale).", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Our server-push model causes more eager communication and thus lower empirical staleness than SSPTable in [8] as shown in Fig.", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "This is consistent with the fact that in SSP, computation making use of fresh data makes more progress [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 228, "endOffset": 232}, {"referenceID": 7, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 262, "endOffset": 269}, {"referenceID": 12, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 262, "endOffset": 269}, {"referenceID": 16, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 356, "endOffset": 360}, {"referenceID": 8, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 498, "endOffset": 502}, {"referenceID": 15, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 560, "endOffset": 564}, {"referenceID": 7, "context": "As a paper about general-purpose distributed ML, we focus on consistency models and systems code, and we deliberately use (relatively) simple algorithms for our benchmark applications, for two reasons: (1) to provide a fair comparison, we must match the code/algorithmic complexity of the benchmarks for other frameworks like GraphLab and SSP PS [8] (practically, this means our applications should use the same update equations); (2) a general-purpose ML framework should not depend on highly-specialized algorithmic techniques tailored only to specific ML categories.", "startOffset": 346, "endOffset": 349}, {"referenceID": 11, "context": "In [13], the authors propose and implement a PS consistency model that has similar theoretical guarantees to the ideal VAP model presented herein.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "On the wider subject of Big Data, Hadoop [2] and Spark [19] are popular programming frameworks, which ML applications are sometimes developed on top of.", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "On the wider subject of Big Data, Hadoop [2] and Spark [19] are popular programming frameworks, which ML applications are sometimes developed on top of.", "startOffset": 55, "endOffset": 59}], "year": 2014, "abstractText": "As Machine Learning (ML) applications increase in data size and model complexity, practitioners turn to distributed clusters to satisfy the increased computational and memory demands. Unfortunately, effective use of clusters for ML requires considerable expertise in writing distributed code, while highly-abstracted frameworks like Hadoop have not, in practice, approached the performance seen in specialized ML implementations. The recent Parameter Server (PS) paradigm is a middle ground between these extremes, allowing easy conversion of single-machine parallel ML applications into distributed ones, while maintaining high throughput through relaxed \u201cconsistency models\u201d that allow inconsistent parameter reads. However, due to insufficient theoretical study, it is not clear which of these consistency models can really ensure correct ML algorithm output; at the same time, there remain many theoretically-motivated but undiscovered opportunities to maximize computational throughput. Motivated by this challenge, we study both the theoretical guarantees and empirical behavior of iterative-convergent ML algorithms in existing PS consistency models. We then use the gleaned insights to improve a consistency model using an \u201ceager\u201d PS communication mechanism, and implement it as a new PS system that enables ML algorithms to reach their solution more quickly.", "creator": "LaTeX with hyperref package"}}}