{"id": "1410.8206", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2014", "title": "Addressing the Rare Word Problem in Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) has recently attracted a lot of attention due to the very high performance achieved by deep neural networks in other domains. An inherent weakness in existing NMT systems is their inability to correctly translate rare words: end-to-end NMTs tend to have relatively small vocabularies with a single \"unknown-word\" symbol representing every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement a simple technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to output, for each OOV word in the target sentence, its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT'14 English to French translation task show that this simple method provides a substantial improvement over an equivalent NMT system that does not use this technique. The performance of our system achieves a BLEU score of 36.9, which improves the previous best end-to-end NMT by 2.1 points. Our model matches the performance of the state-of-the-art system while using three times less data.", "histories": [["v1", "Thu, 30 Oct 2014 00:20:31 GMT  (26kb)", "http://arxiv.org/abs/1410.8206v1", null], ["v2", "Fri, 31 Oct 2014 19:44:50 GMT  (26kb)", "http://arxiv.org/abs/1410.8206v2", null], ["v3", "Tue, 9 Dec 2014 23:11:46 GMT  (27kb)", "http://arxiv.org/abs/1410.8206v3", "Results updated. The best system now achieves 37.5 BLEU"], ["v4", "Sat, 30 May 2015 19:57:28 GMT  (31kb)", "http://arxiv.org/abs/1410.8206v4", "ACL 2015 camera-ready version"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["thang luong", "ilya sutskever", "quoc v le", "oriol vinyals", "wojciech zaremba"], "accepted": true, "id": "1410.8206"}, "pdf": {"name": "1410.8206.pdf", "metadata": {"source": "CRF", "title": "Addressing the Rare Word Problem in Neural Machine Translation", "authors": ["Thang Luong"], "emails": ["lmthang@stanford.edu", "ilyasu@google.com", "qvl@google.com", "vinyals@google.com", "woj.zaremba@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 141 0.82 06v1 [cs.CNeural Machine Translation (NMT)] has lately attracted a lot of attention due to the very high performance achieved by deep neural networks in other areas. An inherent weakness of existing NMT systems is their inability to translate rare words correctly: end-to-end NMTs tend to have relatively small vocabulary with a single \"Unknown Word\" symbol representing any possible out-of-of-the-box vocabulary (OOV) word. In this essay, we suggest and implement a simple technique to solve this problem. We train an NMT system on data that is supplemented by the output of a word algorithm, which allows the NMT system to output the corresponding word in the source sentence for each OOV word in the target sentence. This information is later used in a post-processing step that translates each OOV word with the help of a dictionary."}, {"heading": "1 Introduction", "text": "In recent years, MT researchers have attempted to integrate new network models into the standard pipeline as an additional component [20, 21], 6 primary, 6 systems. Primarily, these systems have been fixed. [11], visual object recognition [15], and other challenging tasks, so there has been much interest in applying them to natural language processing (NLP), since the space of possible translations for a particular set of sources is vast. Among the important NLP tasks, the standard MT approach [14] has been the subject of intense work, while the author in Google is likely to achieve strong results. \u00b2) points to equivalent approaches and led to better systems over time [13, 3, 8]. However, this comes at the expense of a complex pipeline with many subcomponents that need to be coordinated."}, {"heading": "2 Neural Machine Translation", "text": "A neural machine translation system is any neural network that maps a source sentence, s1,.., sn, to a target sentence, t1,.., tm, assuming that all sentences end with a special end-of-sentence token < eos >. Specifically, an NMT system uses a neural network to parameterise the probability of the target sentence, with the source sentence p (tj | t < j, s \u2264 n) (1) for 1 \u2264 j \u2264 m. This makes it possible to calculate and maximise the protocol probability of the target sentence taking into account the source sentence p (t | s) = m \u2211 j = 1log (tj | t < j, s \u2264 n) (2)."}, {"heading": "3 Rare Word Models", "text": "To address the rare word problem discussed in Section 1, we train our neural machine translation system to track the source of the unknown words in the target sentences. If we knew the source word responsible for each unknown target word, we could introduce a post-processing step that would replace every single word in the output of the system with a translation of its source word, using either a dictionary or identity translation. For example, if the model knows that the second unknown symbol in the NMT (line nn) comes from the source word ecotax, it can perform a dictionary to replace this unknown symbol with e-cotaxe. Similarly, an identity translation of the source word Pont-de-Buis can be applied to the third unknown symbol. We present three annotation strategies that are easy to apply to any NMT system. We treat the NMT system [12, 22, 5] as a black box and train it on an annotated part."}, {"heading": "3.1 Copyable Model", "text": "In this approach, we introduce multiple tokens to represent the unknown words in the source and in the target language, rather than just a token token. We comment on the OOV words in the source sentence with unk1, unk2, unk3,..., in the order in which the repetition of unknown words gets identical tokens. Annotating the unknown words in the target language is a bit more elaborate: (a) any unknown target word that is aligned with an unknown source word is assigned the same unknown token (hence the \"copy model\"), and (b) an unknown target word that has no alignment or is aligned with a known word uses the special null token unkn. See Figure 2 for an example. This comment allows us to translate any non-null token."}, {"heading": "3.2 Positional All Model (PosAll)", "text": "The copyable model is limited by its inability to translate unknown target words aligned with known words in the source sentence, such as the pair of words portique - portico in our current example. This is because the source vocabulary tends to be much larger than target vocabulary due to the cost of Softmax (although there are much faster alternatives to Softmax that could potentially alleviate this problem).This limitation has motivated us to develop a model that predicts the complete matches between source and target sentence, which is easy since the complete matches are available during training. Specifically, we return to using only one universal token. On the target page, however, we insert a position symbol after each word. Here, d indicates a relative position (d = \u2212 7,... \u2212 1, 0,...,... 7) to indicate that an target word at position j is aligned with a source word."}, {"heading": "3.3 Positional Unknown Model (PosUnk)", "text": "A major weakness of the PosAll model is that it doubles the length of the target sentence, making it difficult to learn and almost 2 times slower per parameter update. However, our post-processing step only affects the alignment of the unknown words, so it makes more sense to comment only on the alignment of the unknown words. This motivates our positional unknown model, which uses the unkposd characters (for d in \u2212 7,.., 7 or n) to simultaneously mark (a) the fact that a word is unknown, and (b) its relative position d in relation to its aligned source word, similar to the positional all-model (where d is set to the null symbol n if the word has no alignment). We use the universal for all other unknown characters in the source language. It is possible that the PosAll model, despite its slower speed, learns better alignments as it is trained on many other words and their alignment. We answer this question in the experimental section."}, {"heading": "4 Experiments", "text": "We evaluate the effectiveness of our OOV models using the WMT '14 translation task from English to French. [1] The quality of the translation is measured using the BLEU metric [17] on the 2014 Newstestest (which includes 3003 sentences)."}, {"heading": "4.1 Training Data", "text": "To be comparable with the results of previous work on neural machine translation systems [22, 5, 2], we train our models using the same training data of 12M parallel sentences (348M French words and 304M English words).2Due to the computer-intensive nature of naive Softmax in the target language, we limit French vocabulary to the 40K most common French words (note: [22] we used a vocabulary of 80k French words).On the source side, however, we can afford a much larger vocabulary, so that we use the 200K most common English words. The model treats all other words as unknowns. If the French (target) vocabulary has 40K words, there are an average of 1.33 unknown words per sentence on the target page of the test sentence. We base our training data on the three schemes described in the previous section."}, {"heading": "4.2 Training Details", "text": "Our training sequence and hyperparameter selection are similar to those of Sutskever et al. [22]. Specifically, we train multi-layered, deep LSTMs, each of which has 1000 cells, with 1000 dimensional embeddings. Like Sutskever et al. [22], we reverse the words in the source sentences, which have been shown to improve the usage of the LSTM memory and lead to better translations of long sentences. Our hyperparameters can be summarized as follows: (a) The parameters are uniformly initialized in [-0.08, 0.08], (b) SGD has a fixed learning rate of 0.7, (c) we train for 8 epochs (after 5 epochs, we begin to halve the learning rate every 0.5 epochs), (d) the size of the mini-stack is 128, and (e) we scale the normalized gradient to ensure that its norm does not exceed 5 [18]. We also follow the Gisschema of 22, which allows us to achieve a higher speed of K with a higher speed."}, {"heading": "4.3 A note on BLEU scores", "text": "The http: / / matrix.statmt.org / matrix website states that the state-of-the-art (SOTA) system [7] has a BLEU score of 35.8 on the English-French language pair on WMT '14. This numerical score is based on detokenised translations, but all the other systems we have compared with have been rated on the basis of tokenised translations using the multi-bleu.pl script, which is consistent with previous work [5, 2, 19, 22] so to enable a comparison of our system with Durrani et al. [7] system, we have evaluated its tokenised predictions (which can be downloaded from statmt.org [7]) on the test kit (newstest2014) and arrived at the BLEU score of 37.0 points [22].1http: / / www.statmt.org / wmt14 / translation-task.html 2: http: / wwliv / slum.slum.slum.m / ic.m."}, {"heading": "4.4 Main Results", "text": "We compare our system with other systems trained on the basis of the same training data from 12M set pairs, including several current end-to-end neural systems, as well as phrase-based baselines with neural components. We also compare the performance of the state-of-the-art MT system [7] of the WMT '14 competition, which is trained on 36M set pairs. Results shown in Table 1 show that our unknown word translation technique (in particular the PosUnk model) significantly improves translation quality for both the individual (non-ensemble) LSTM models (+ 2.3 BLEU) and the ensemble model (+ 2.8 BLEU). For the non-ensemble models, we report the performance of models with 4 and 6 layers (see Section 5.2 for further analysis). For the ensemble translation, we use a combination of 5 depth-4 models and 3 depth-6 models. Our best result (EU 36.9 MNT) exceeds all other multiple translations."}, {"heading": "5 Analysis", "text": "We analyze and quantify the improvements made by our approach to rare word translation and offer a detailed comparison of the various in Section 3. We examine the impact of depth on LSTM architectures and show a strong correlation between helplessness and BLEU values. We also highlight some examples of translation where our models successfully translate OOV words and multiple errors."}, {"heading": "5.1 Rare Word Analysis", "text": "To analyze the impact of rare words on translation quality, we follow Sutskever et al. [22] and sort sentences in the latest 2014 by the average frequency of their words. We divide the test sentences into groups in which the sentences within each group have a comparable number of rare words and rate each group independently. We evaluate our systems before and after the translation of the OOOV words and compare them with the standard MT systems - we use the state-of-the-art (SOTA) system of WMT '14 [7] and neural MT systems - we use the ensemble system described in [22] (see section 4). Rare word translations a challenge for neural machine translation systems, as shown in Figure 5. The translation quality of our model before the application of unknown word translations is shown by the green star line, and the currently best NMT system [22] is the purple diamond line. While [22] excellent translations of sentences with frequent SOTA phrases on the left-hand part of the diagram are significant (SOTA) words."}, {"heading": "5.2 Other Effects", "text": "In this area, all models are designed for the unchanged sentences, and we use the following hyperparameters: we initiate the parameters evenly in [-0.1, 0.1], the learning rate is 1, the maximum graduation rate is 1, and we study the effect of the various rare words, namely: (a) the ability to recognize the invariant words is still useful for analyzing them. (b)"}, {"heading": "5.3 Sample Translations", "text": "We present three sample translations of our best system (with 36.9 BLEU) in Table 2. In our first example, the model correctly translates all unknown words: 2600, orthopaedics and cataract. It is interesting to note that the model can accurately predict the spacing of 5 and 6 words. The second example highlights the fact that our model can translate long sentences reasonably well and that it was able to translate the unknown word correctly for JPMorgan at the extreme end of the initial sentence. Finally, our examples also show several penalties arising from our model: (a) erroneous dictionary entries, as in the case of a Gociateur vs. trader in the second example, and (b) incorrect alignment predictions, such as that the unkpos3 word does not match the initial word correctly and does not match the task, which led to an incorrect translation in the third sentence."}, {"heading": "6 Conclusion", "text": "We have shown that a simple alignment-based technique can mitigate and even overcome one of the main weaknesses of current NMT systems, namely their inability to translate words that are not in their vocabulary. A key advantage of our technique is the fact that it is applicable to any NMT system and not only to the deep LSTM model of Sutskever et al. [22]. A technique such as ours is probably necessary if an NMT system is to be state-of-the-art in machine translation. We have empirically demonstrated that our technique in the WMT '14 Anglo-French translation task brings a consistent and substantial improvement of 2-3 BLEU points over different NTM systems of different architectures. Our system outperforms the current best end-to-end-to-end system by a large margin of 2.1 BLEU points."}], "references": [{"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Amittai Axelrod", "Xiaodong He", "Jianfeng Gao"], "venue": "In EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Phrasal: A statistical machine translation toolkit for exploring new model features", "author": ["D. Cer", "M. Galley", "D. Jurafsky", "C.D. Manning"], "venue": "In ACL, Demonstration Session,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "In ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Edinburgh\u2019s phrase-based machine translation systems for WMT-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield"], "venue": "In WMT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Jonathan Weese", "Hendra Setiawan", "Adam Lopez", "Ferhan Ture", "Vladimir Eidelman", "Juri Ganitkevitch", "Phil Blunsom", "Philip Resnik"], "venue": "In ACL, Demonstration Session,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "In Arxiv preprint arXiv:1308.0850,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In EMNLP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In ACL, Demonstration Session,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In NAACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Alignment by agreement", "author": ["P. Liang", "B. Taskar", "D. Klein"], "venue": "In NAACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei jing Zhu"], "venue": "In ACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "University le mans", "author": ["H. Schwenk"], "venue": "http://www-lium.univ-lemans.fr/ \u0303schwenk/cslm_joint_paper/,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Holger Schwenk"], "venue": "In COLING,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Continuous space translation models with neural networks", "author": ["Le Hai Son", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "venue": "In NAACL-HLT,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "In EMNLP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Deep Neural Networks (DNNs) have achieved excellent results on speech recognition [11], visual object recognition [15], and other challenging tasks, so there has been much interest in applying them to natural language processing (NLP) problems as well.", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "Deep Neural Networks (DNNs) have achieved excellent results on speech recognition [11], visual object recognition [15], and other challenging tasks, so there has been much interest in applying them to natural language processing (NLP) problems as well.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "For more than a decade, the standard MT approach [14] has been subject to intensive \u2217Work done while the author was in Google.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "research and resulted in better systems over time [13, 4, 3, 8].", "startOffset": 50, "endOffset": 63}, {"referenceID": 3, "context": "research and resulted in better systems over time [13, 4, 3, 8].", "startOffset": 50, "endOffset": 63}, {"referenceID": 2, "context": "research and resulted in better systems over time [13, 4, 3, 8].", "startOffset": 50, "endOffset": 63}, {"referenceID": 7, "context": "research and resulted in better systems over time [13, 4, 3, 8].", "startOffset": 50, "endOffset": 63}, {"referenceID": 19, "context": "In recent years, MT researchers have been seeking to incorporate neural network models into the standard pipeline as an additional subcomponent [20, 21, 23, 6].", "startOffset": 144, "endOffset": 159}, {"referenceID": 20, "context": "In recent years, MT researchers have been seeking to incorporate neural network models into the standard pipeline as an additional subcomponent [20, 21, 23, 6].", "startOffset": 144, "endOffset": 159}, {"referenceID": 22, "context": "In recent years, MT researchers have been seeking to incorporate neural network models into the standard pipeline as an additional subcomponent [20, 21, 23, 6].", "startOffset": 144, "endOffset": 159}, {"referenceID": 5, "context": "In recent years, MT researchers have been seeking to incorporate neural network models into the standard pipeline as an additional subcomponent [20, 21, 23, 6].", "startOffset": 144, "endOffset": 159}, {"referenceID": 11, "context": "Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22].", "startOffset": 105, "endOffset": 119}, {"referenceID": 4, "context": "Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22].", "startOffset": 105, "endOffset": 119}, {"referenceID": 1, "context": "Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22].", "startOffset": 105, "endOffset": 119}, {"referenceID": 21, "context": "Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22].", "startOffset": 105, "endOffset": 119}, {"referenceID": 12, "context": "In addition, NMT systems are easy to train with backpropagation and their decoder is easy to implement, unlike the highly intricate decoders used by the phrase-based systems [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 21, "context": "NMT systems also use minimal domain knowledge, which makes them applicable to any other problem that can be formulated as mapping a sequence to another sequence [22].", "startOffset": 161, "endOffset": 165}, {"referenceID": 21, "context": "[22] and Bahdanau et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] have observed that sentences with many rare words tend to be translated much more poorly than sentences containing mainly frequent words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "9 BLUE points, matching the performance of the state-of-the-art system [7] while using three times less data.", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "[12] used a combination of a convolutional neural network and a recurrent neural network, Sutskever et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] used a large and deep Long Short-Term Memory (LSTM) model, Cho et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] used an architecture similar to the LSTM, and Bahdanau et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] used a more elaborate neural network architecture that uses an attentional mechanism over the input sequence, similarly to Graves [9] and Graves et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[2] used a more elaborate neural network architecture that uses an attentional mechanism over the input sequence, similarly to Graves [9] and Graves et al.", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22], which has a large deep LSTM to encodue the input sequence and a separate deep LSTM to produce a translation from the input sequence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "We treat the NMT system [12, 22, 5] as a black box and train it on a dataset annotated with alignment information specified by one of the models below.", "startOffset": 24, "endOffset": 35}, {"referenceID": 21, "context": "We treat the NMT system [12, 22, 5] as a black box and train it on a dataset annotated with alignment information specified by one of the models below.", "startOffset": 24, "endOffset": 35}, {"referenceID": 4, "context": "We treat the NMT system [12, 22, 5] as a black box and train it on a dataset annotated with alignment information specified by one of the models below.", "startOffset": 24, "endOffset": 35}, {"referenceID": 16, "context": "1 Translation quality is measured with the BLEU metric [17] on the newstest2014 (which has 3003 sentences).", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "To be comparable with the results reported by previous work on neural machine translation systems [22, 5, 2], we train our models on the same training data of 12M parallel sentences (348M French and 304M English words).", "startOffset": 98, "endOffset": 108}, {"referenceID": 4, "context": "To be comparable with the results reported by previous work on neural machine translation systems [22, 5, 2], we train our models on the same training data of 12M parallel sentences (348M French and 304M English words).", "startOffset": 98, "endOffset": 108}, {"referenceID": 1, "context": "To be comparable with the results reported by previous work on neural machine translation systems [22, 5, 2], we train our models on the same training data of 12M parallel sentences (348M French and 304M English words).", "startOffset": 98, "endOffset": 108}, {"referenceID": 0, "context": "The 12M subset was selected from the full WMT\u201914 parallel corpora using the method proposed in [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 21, "context": "2 Due to the computationally intensive nature of the naive softmax in the target language, we limit the French vocabulary to the 40K most frequent French words (note that [22] used a vocabulary of 80k French words).", "startOffset": 171, "endOffset": 175}, {"referenceID": 15, "context": "The alignment is computed with the Berkeley aligner [16] using its default settings.", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22], we reverse the words in the source sentences which has been shown to improve LSTM memory utilization and results in better translations of long sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "5 epoch), (d) the size of the mini-batch is 128, and (e) we rescale the normalized gradient to ensure that its norm does not exceed 5 [18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 21, "context": "We also follow the GPU parallelization scheme proposed in [22], allowing us to reach a training speed of 9.", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "0K words per second ([22] achieved 6.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "org/matrix states that the state-of-the-art (SOTA) system [7] achieves a BLEU score of 35.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "pl script, which is consistent with previous work [5, 2, 19, 22].", "startOffset": 50, "endOffset": 64}, {"referenceID": 1, "context": "pl script, which is consistent with previous work [5, 2, 19, 22].", "startOffset": 50, "endOffset": 64}, {"referenceID": 18, "context": "pl script, which is consistent with previous work [5, 2, 19, 22].", "startOffset": 50, "endOffset": 64}, {"referenceID": 21, "context": "pl script, which is consistent with previous work [5, 2, 19, 22].", "startOffset": 50, "endOffset": 64}, {"referenceID": 6, "context": "[7], we evaluated its tokenized predictions (which can be downloaded from statmt.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "org [7]) on the test set (newstest2014) and arrived at the BLEU score of 37.", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "0 points [22].", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "We also compare to the performance of the state-of-the-art MT system [7] from the WMT\u201914 competition, which is trained on 36M sentence pairs.", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "9 BLEU) outperforms all other NMT systems by a large margin, and in particular, it outperforms the current best NMT system [22] by 2.", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "[22] when they rerank the n-best list of a phrase-based baseline [22]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] when they rerank the n-best list of a phrase-based baseline [22]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "System BLEU State of the art [7] 37.", "startOffset": 29, "endOffset": 32}, {"referenceID": 18, "context": "0 Standard MT + neural components LIUM [19] \u2013 neural language model 33.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "[5] \u2013 phrase table neural features 34.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] \u2013 ensemble 5 LSTMs, reranking 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] \u2013 bi-directional gated single RNN 28.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] \u2013 single LSTM 30.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] \u2013 ensemble of 5 LSTMs 34.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] and sort the sentences in newstest2014 by the average frequency rank of their words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "We evaluate our systems before and after translating the OOV words and compare with the standard MT systems \u2013 we use the state-of-the-art (SOTA) system from WMT\u201914 [7], and neural MT systems \u2013 we use the ensemble system described in [22] (See Section 4).", "startOffset": 164, "endOffset": 167}, {"referenceID": 21, "context": "We evaluate our systems before and after translating the OOV words and compare with the standard MT systems \u2013 we use the state-of-the-art (SOTA) system from WMT\u201914 [7], and neural MT systems \u2013 we use the ensemble system described in [22] (See Section 4).", "startOffset": 233, "endOffset": 237}, {"referenceID": 21, "context": "The translation quality of our model before applying the unknown word translations is shown by the green star line, and the current best NMT system [22] is the purple diamond line.", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "While [22] produces excellent translations of sentences with frequent words (the left part of the graph), they are worse than SOTA system (red triangle line) on sentences with many rare words (the right side of the graph).", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "[22], which allows us to outperform SOTA on sentences that consist predominantly of frequent words and approach its performance on sentences with many OOV words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Neural Machine Translation (NMT) has recently attracted a lot of attention due to the very high performance achieved by deep neural networks in other domains. An inherent weakness in existing NMT systems is their inability to correctly translate rare words: end-to-end NMTs tend to have relatively small vocabularies with a single \u201cunknown-word\u201d symbol representing every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement a simple technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to output, for each OOV word in the target sentence, its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT\u201914 English to French translation task show that this simple method provides a substantial improvement over an equivalent NMT system that does not use this technique. The performance of our system achieves a BLEU score of 36.9, which improves the previous best end-to-end NMT by 2.1 points. Our model matches the performance of the state-of-the-art system while using three times less data.", "creator": "LaTeX with hyperref package"}}}