{"id": "1410.8516", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2014", "title": "NICE: Non-linear Independent Components Estimation", "abstract": "We propose a deep learning framework for modeling complex high-dimensional densities via Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable, and unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.", "histories": [["v1", "Thu, 30 Oct 2014 19:44:20 GMT  (2325kb,D)", "http://arxiv.org/abs/1410.8516v1", null], ["v2", "Fri, 19 Dec 2014 22:40:18 GMT  (1454kb,D)", "http://arxiv.org/abs/1410.8516v2", null], ["v3", "Tue, 6 Jan 2015 18:10:44 GMT  (1454kb,D)", "http://arxiv.org/abs/1410.8516v3", "11 pages, under review as a conference paper at ICLR 2015"], ["v4", "Mon, 9 Mar 2015 18:06:58 GMT  (1455kb,D)", "http://arxiv.org/abs/1410.8516v4", "11 pages, under review as a conference paper at ICLR 2015"], ["v5", "Thu, 12 Mar 2015 06:25:20 GMT  (1456kb,D)", "http://arxiv.org/abs/1410.8516v5", "10 pages and 2 pages Appendix, under review as a conference paper at ICLR 2015"], ["v6", "Fri, 10 Apr 2015 12:27:56 GMT  (1457kb,D)", "http://arxiv.org/abs/1410.8516v6", "11 pages and 2 pages Appendix, workshop paper at ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["laurent dinh", "david krueger", "yoshua bengio"], "accepted": true, "id": "1410.8516"}, "pdf": {"name": "1410.8516.pdf", "metadata": {"source": "CRF", "title": "NICE: Non-linear Independent Components Estimation", "authors": ["Laurent Dinh", "David Krueger", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "One of the central questions in the field of unsupervised learning is how to capture complex data distributions that have an unknown structure. Deep learning approaches (Bengio, 2009) are based on learning a representation of the data that would capture its most important variation factors. This raises the question: What is a good representation? As in the recent paper (Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014), we assume that a good representation is one in which the distribution of the data is easy to model. In this paper, we consider the specific case in which we ask the student to find a transformation h = f (x) of the data in a new space in which the resulting distribution is factored, i.e. the components hd are independent: pH (h) = d pHd (hd).We can imagine transformation f as acting on the distribution by rotating and folding the space to achieve this goal."}, {"heading": "2 Issues of training with continuous data", "text": "We are looking at the problem of learning a density from a parametric family of densities, which is usually used less than a computer, but rather for processing data via finite data sets. In this environment, there is no natural upper limit on the log probability that can be achieved (as opposed to discrete data), and as a result, several problems may arise from naive attempts to maximize probability. For example, a fully parameterized mix of Gaussians can use one of its compounds to model a model of datapoints with arbitrary precision to increase training protocols. Since the test log probability of such a model is often correspondingly low, this can be considered an overfilling problem. However, we will show two cases where similar singularity problems occur and evolve into test data.Continuous data are recorded with a degree of precision that is generally much lower than is generally available."}, {"heading": "3 Learning bijective transformations", "text": "Instead of directly modeling complex data by learning about a complex parametric family of distributions, we will learn a nonlinear transformation of the data distribution into a simpler distribution with maximum probability using the following formula: log (pX (x)) = log (pH (f (x))) + log (| det (\u2202 f (x) \u2202 x) |), where pH (h) is a predefined density function for simplicity, the previous distribution2, which will often be a factorized distribution, i.e., with independent dimensions, e.g. a standard isotropic Gaussian. Let h = f (x) be the code or latent variable. If the previous distribution is factorial, we will get the following criterion of nonlinear independent components (NICE): log (pX (x)) = D-d = 1 log (fd (x))))) + log (f (x)."}, {"heading": "4 Triangular structure", "text": "The architecture of the model becomes crucial at this point, because we want a family of bijection elements whose determinants are easily comprehensible in their quadratic elements and whose calculation is simple, both forward and backwards. If we use a layered or composed transformation, we will first try to define these more elementary components. First, we will look at linear transformations. (Rezende et al., 2014) and their Jacobian determinant is the product of their determinants. Therefore, we will first look at these more elementary components. (Rezende et al.) And then we will use the formal and determinant when we use diagonal matrices with rank-1 correction. Another family of matrices with tractable determinants are trianular matrices whose determinants are simply the product of their quadrative elements."}, {"heading": "5 Related methods", "text": "In fact, most of them are able to surpass themselves by going in search of their own identity. (...) Most of them are able to surpass themselves. (...) Most of them are not able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. \"(...) Most of them are able to surpass themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them have surpassed themselves. (...) Most of them have surpassed themselves. (...) Most of them have surpassed themselves. (...)"}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Log-likelihood and generation", "text": "We train NICE on MNIST (LeCun and Cortes, 1998), the Toronto Face Dataset 3 (TFD) (Susskind et al., 2010), the Street View House Numbers dataset (SVHN) (Netzer et al., 2011) and CIFAR-10 (Krizhevsky, 2010). As previously mentioned, we use a dequantized version of the data, and the data is rescaled to be in [0, 1] D. These two steps correspond to the following pre-processing: x = 255256 x + u (5), where we use a dequantized version of the data (0, 256 \u2212 1]). CIFAR-10 is standardized to be in [\u2212 1, 1] D.3We train on unlabeled data for this dataset.We train on unlabeled data for MNIST and SVHN used is a stack of four coupling layers with the last stage, parametrized space."}, {"heading": "6.2 Inpainting", "text": "Here we consider a naive iterative approach to implementing inpainting with the trained generative models. In inpainting, we attach the observed dimensions to their values and maximize the log probability with respect to the hidden dimensions by projected gradient ascent with step \u03b1 = 11 + i, where i is the iteration. The result is shown in test examples of MNIST or TFD Figure 6 or 7, respectively. Although the model is not trained for this task, this inpainting method seems to provide an adequate qualitative performance, although we have the presence of false modes.4We specify the dimensions of inputs, hidden and output levels."}, {"heading": "7 Conclusion", "text": "In this paper, we presented a new flexible architecture for learning a highly nonlinear transformation that maps training data into a space where its distribution is approximately factorized, and a framework to achieve this by directly maximizing log probability. Our model has efficient, unbiased ancestor capture and achieves competitive results in terms of log probability and painting. Note that the architecture of our model could be trained using other inductive principles that are able to take advantage of its advantages, such as toroidal sub-space analysis (TSA) (Cohen and Welling, 2014). We also briefly establish a link to variational auto-encoders. Additional work can be done in this direction to enable more powerful approximate conclusions with a more complex family of safe posterior a richer priest family."}, {"heading": "Acknowledgements", "text": "We would like to thank Yann Dauphin, Vincent Dumoulin, Aaron Courville, Kyle Kastner, Dustin Webb and Li Yao for their discussions and feedback. Vincent Dumoulin provided the code for visualization. They are grateful to the developers of Theano (Bergstra et al., 2011) (Bastien et al., 2012) and Pylearn2 (Goodfellow et al., 2013) as well as the computer resources of Comute Canada and Calcul Que \u0301 bec and the research funds of NSERC, CIFAR and Canada Research Chairs."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Artificial Neural Networks and their Application to Sequence Recognition", "author": ["Y. Bengio"], "venue": "PhD thesis, McGill University, (Computer Science), Montreal, Canada.", "citeRegEx": "Bengio,? 1991", "shortCiteRegEx": "Bengio", "year": 1991}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Now Publishers.", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "How auto-encoders could provide credit assignment in deep networks via target propagation", "author": ["Y. Bengio"], "venue": "Technical report, arXiv preprint arXiv:1407.7906.", "citeRegEx": "Bengio,? 2014", "shortCiteRegEx": "Bengio", "year": 2014}, {"title": "Modeling high-dimensional discrete data with multi-layer neural networks", "author": ["Y. Bengio", "S. Bengio"], "venue": "Solla, S., Leen, T., and M\u00fcller, K.-R., editors, Advances in Neural Information Processing Systems 12 (NIPS\u201999), pages 400\u2013406. MIT Press.", "citeRegEx": "Bengio and Bengio,? 2000", "shortCiteRegEx": "Bengio and Bengio", "year": 2000}, {"title": "Better mixing via deep representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML\u201913). ACM.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: Deep learning on gpus with python", "author": ["J. Bergstra", "F. Bastien", "O. Breuleux", "P. Lamblin", "R. Pascanu", "O. Delalleau", "G. Desjardins", "D. Warde-Farley", "I.J. Goodfellow", "A. Bergeron", "Y. Bengio"], "venue": "Big Learn workshop, NIPS\u201911.", "citeRegEx": "Bergstra et al\\.,? 2011", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "Springer.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Learning the irreducible representations of commutative lie groups", "author": ["T. Cohen", "M. Welling"], "venue": "arXiv preprint arXiv:1402.4437.", "citeRegEx": "Cohen and Welling,? 2014", "shortCiteRegEx": "Cohen and Welling", "year": 2014}, {"title": "Generative adversarial networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Technical Report arXiv:1406.2661, arxiv.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Pylearn2: a machine learning research library", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "arXiv preprint arXiv:1308.4214.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "I. Danihelka", "A. Mnih", "C. Blundell", "D. Wierstra"], "venue": "ICML\u20192014.", "citeRegEx": "Gregor et al\\.,? 2014", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Natural Image Statistics: A probabilistic approach to early computational vision", "author": ["A. Hyv\u00e4rinen", "J. Hurri", "P.O. Hoyer"], "venue": "Springer-Verlag.", "citeRegEx": "Hyv\u00e4rinen et al\\.,? 2009", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2009}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural networks, 13(4):411\u2013430.", "citeRegEx": "Hyv\u00e4rinen and Oja,? 2000", "shortCiteRegEx": "Hyv\u00e4rinen and Oja", "year": 2000}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Welling,? 2014", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Convolutional deep belief networks on CIFAR-10", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto. Unpublished Manuscript: http://www.cs.utoronto.ca/ kriz/conv-cifar10-aug2010.pdf.", "citeRegEx": "Krizhevsky,? 2010", "shortCiteRegEx": "Krizhevsky", "year": 2010}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u20192011), volume 15 of JMLR: W&CP.", "citeRegEx": "Larochelle and Murray,? 2011", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes", "year": 1998}, {"title": "A wavelet tour of signal processing", "author": ["S. Mallat"], "venue": "Academic press.", "citeRegEx": "Mallat,? 1999", "shortCiteRegEx": "Mallat", "year": 1999}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "ICML\u20192014.", "citeRegEx": "Mnih and Gregor,? 2014", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS.", "citeRegEx": "Netzer et al\\.,? 2011", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Deep directed generative autoencoders", "author": ["S. Ozair", "Y. Bengio"], "venue": "Technical report, U. Montreal, arXiv:1410.0630.", "citeRegEx": "Ozair and Bengio,? 2014", "shortCiteRegEx": "Ozair and Bengio", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "Technical report, arXiv:1401.4082.", "citeRegEx": "Rezende et al\\.,? 2014", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "High-dimensional probability estimation with deep density models", "author": ["O. Rippel", "R.P. Adams"], "venue": "arXiv preprint arXiv:1302.5125.", "citeRegEx": "Rippel and Adams,? 2013", "shortCiteRegEx": "Rippel and Adams", "year": 2013}, {"title": "Annealing between distributions by averaging moments", "author": ["C.M. Roger Grosse", "R. Salakhutdinov"], "venue": "ICML\u20192013.", "citeRegEx": "Grosse and Salakhutdinov,? 2013", "shortCiteRegEx": "Grosse and Salakhutdinov", "year": 2013}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 5, pages 448\u2013455.", "citeRegEx": "Salakhutdinov and Hinton,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "Cohen, W. W., McCallum, A., and Roweis, S. T., editors, Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML\u201908), volume 25, pages 872\u2013879. ACM.", "citeRegEx": "Salakhutdinov and Murray,? 2008", "shortCiteRegEx": "Salakhutdinov and Murray", "year": 2008}, {"title": "The Toronto face dataset", "author": ["J. Susskind", "A. Anderson", "G.E. Hinton"], "venue": "Technical Report UTML TR 2010001, U. Toronto.", "citeRegEx": "Susskind et al\\.,? 2010", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Deep learning approaches (Bengio, 2009) rely on the learning of a representation of the data that would capture its most important factors of variation.", "startOffset": 25, "endOffset": 39}, {"referenceID": 14, "context": "This raises the question: what is a good representation? Like in recent work (Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014), we take the view that a good representation is one in which the distribution of the data is easy to model.", "startOffset": 77, "endOffset": 149}, {"referenceID": 22, "context": "This raises the question: what is a good representation? Like in recent work (Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014), we take the view that a good representation is one in which the distribution of the data is easy to model.", "startOffset": 77, "endOffset": 149}, {"referenceID": 21, "context": "This raises the question: what is a good representation? Like in recent work (Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014), we take the view that a good representation is one in which the distribution of the data is easy to model.", "startOffset": 77, "endOffset": 149}, {"referenceID": 5, "context": "In this paper, we can observe such flattening as found in (Bengio et al., 2013) with stacked autoencoders and RBMs.", "startOffset": 58, "endOffset": 79}, {"referenceID": 7, "context": "For instance, as highlighted in (Bishop, 2006), a fully parametrized a mixture of gaussians can use one of its mixture component to model one of the datapoints with arbitrary precision, arbitrarily raising the training log-likelihood.", "startOffset": 32, "endOffset": 46}, {"referenceID": 18, "context": "For example wavelets (Mallat, 1999) may provide a sparser signal to model, and normalization and whitening remove trivial scaling and correlation allowing the model to focus on more interesting structure of the data and ease the optimization process.", "startOffset": 21, "endOffset": 35}, {"referenceID": 22, "context": "(Rezende et al., 2014) and (Kingma and Welling, 2014) provide formulas for the inverse and determinant when using diagonal matrices, or diagonal matrices with rank-1 correction, as transformation matrices.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": ", 2014) and (Kingma and Welling, 2014) provide formulas for the inverse and determinant when using diagonal matrices, or diagonal matrices with rank-1 correction, as transformation matrices.", "startOffset": 12, "endOffset": 38}, {"referenceID": 25, "context": "Undirected graphical models like deep Boltzmann machines (DBM) (Salakhutdinov and Hinton, 2009) were for a while the most successful due to efficient approximate inference and learning techniques that these models allowed.", "startOffset": 63, "endOffset": 95}, {"referenceID": 26, "context": "In addition, the log-likelihood is intractable, and the best known estimation procedure, annealed importance sampling (AIS) (Salakhutdinov and Murray, 2008), might yield an overly optimistic evaluation (Roger Grosse and Salakhutdinov, 2013).", "startOffset": 124, "endOffset": 156}, {"referenceID": 14, "context": "However, recent advances in the framework of variational auto-encoders (VAE) - which come under a variety of names and variants (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effective approximate inference for training.", "startOffset": 128, "endOffset": 220}, {"referenceID": 22, "context": "However, recent advances in the framework of variational auto-encoders (VAE) - which come under a variety of names and variants (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effective approximate inference for training.", "startOffset": 128, "endOffset": 220}, {"referenceID": 19, "context": "However, recent advances in the framework of variational auto-encoders (VAE) - which come under a variety of names and variants (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effective approximate inference for training.", "startOffset": 128, "endOffset": 220}, {"referenceID": 11, "context": "However, recent advances in the framework of variational auto-encoders (VAE) - which come under a variety of names and variants (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effective approximate inference for training.", "startOffset": 128, "endOffset": 220}, {"referenceID": 3, "context": "More specifically, as the transformation and its inverse can be seen as a perfect autoencoder pair (Bengio, 2014), the reconstruction term is a constant that can be ignored.", "startOffset": 99, "endOffset": 113}, {"referenceID": 14, "context": "We also observe that by combining the variational criterion with the reparametrization trick, (Kingma and Welling, 2014) is effectively maximizing the joint log-likelihood of the pair (x, ) in a NICE model with two affine coupling layers (where is the auxiliary noise variable).", "startOffset": 94, "endOffset": 120}, {"referenceID": 21, "context": "Another interesting comparison is with the work of (Ozair and Bengio, 2014), which also considers a deterministic encoder, but with discrete input and latent variables, which is not always invertible.", "startOffset": 51, "endOffset": 75}, {"referenceID": 13, "context": "Independent component analysis (ICA) (Hyv\u00e4rinen and Oja, 2000), and more specifically its maximum likelihood formulation, learn an orthogonal transformation of the data, necessitating a costly orthogonalization proceedure between parameter updates.", "startOffset": 37, "endOffset": 62}, {"referenceID": 1, "context": "Learning a richer family of transformations was proposed in (Bengio, 1991), but the proposed class of transformations, neural networks, lacks the structure to make the inference and optimization practical.", "startOffset": 60, "endOffset": 74}, {"referenceID": 23, "context": "(Rippel and Adams, 2013) reintroduces this idea but drops the bijectivity constraint but has to rely on a composite proxy to optimize the log-likelihood.", "startOffset": 0, "endOffset": 24}, {"referenceID": 14, "context": "A more principled proxy of log-likelihood, the variational lower bound, is used more successfully in (Kingma and Welling, 2014) and (Rezende et al.", "startOffset": 101, "endOffset": 127}, {"referenceID": 22, "context": "A more principled proxy of log-likelihood, the variational lower bound, is used more successfully in (Kingma and Welling, 2014) and (Rezende et al., 2014).", "startOffset": 132, "endOffset": 154}, {"referenceID": 9, "context": "Generative adversarial networks (GAN) (Goodfellow et al., 2014) also train a generative model to transform a simple (e.", "startOffset": 38, "endOffset": 63}, {"referenceID": 16, "context": "The triangular structure used in NICE to obtain tractability is also present in another tractable density model, the neural autoregressive density estimator (NADE) (Larochelle and Murray, 2011), inspired by (Bengio and Bengio, 2000).", "startOffset": 164, "endOffset": 193}, {"referenceID": 4, "context": "The triangular structure used in NICE to obtain tractability is also present in another tractable density model, the neural autoregressive density estimator (NADE) (Larochelle and Murray, 2011), inspired by (Bengio and Bengio, 2000).", "startOffset": 207, "endOffset": 232}, {"referenceID": 17, "context": "1 Log-likelihood and generation We train NICE on MNIST (LeCun and Cortes, 1998), the Toronto Face Dataset 3 (TFD) (Susskind et al.", "startOffset": 55, "endOffset": 79}, {"referenceID": 27, "context": "1 Log-likelihood and generation We train NICE on MNIST (LeCun and Cortes, 1998), the Toronto Face Dataset 3 (TFD) (Susskind et al., 2010), the Street View House Numbers dataset (SVHN) (Netzer et al.", "startOffset": 114, "endOffset": 137}, {"referenceID": 20, "context": ", 2010), the Street View House Numbers dataset (SVHN) (Netzer et al., 2011) and CIFAR-10 (Krizhevsky, 2010).", "startOffset": 54, "endOffset": 75}, {"referenceID": 15, "context": ", 2011) and CIFAR-10 (Krizhevsky, 2010).", "startOffset": 21, "endOffset": 39}, {"referenceID": 12, "context": "A standard logistic distribution is used as prior for MNIST and TFD, as its negative log-likelihood function correspond to the smooth L1 penalty (Hyv\u00e4rinen et al., 2009) x 7\u2192 log cosh(x).", "startOffset": 145, "endOffset": 169}, {"referenceID": 28, "context": "The models are training with RMSProp (Tieleman and Hinton, 2012) with learning rate 10\u22123 exponentially decreasing to 10\u22124 with exponential decay of 1.", "startOffset": 37, "endOffset": 64}, {"referenceID": 8, "context": "Note that the architecture of our model could be trained using other inductive principles capable of exploiting its advantages, like toroidal subspace analysis (TSA) (Cohen and Welling, 2014).", "startOffset": 166, "endOffset": 191}], "year": 2017, "abstractText": "We propose a deep learning framework for modeling complex high-dimensional densities via Nonlinear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable, and unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.", "creator": "LaTeX with hyperref package"}}}