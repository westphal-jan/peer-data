{"id": "1410.8675", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2014", "title": "Partition-wise Linear Models", "abstract": "Region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations. One of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models. This paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models. Our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partition-specific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations. In addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations. Experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models.", "histories": [["v1", "Fri, 31 Oct 2014 09:01:27 GMT  (477kb,D)", "http://arxiv.org/abs/1410.8675v1", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["hidekazu oiwa", "ryohei fujimaki"], "accepted": true, "id": "1410.8675"}, "pdf": {"name": "1410.8675.pdf", "metadata": {"source": "CRF", "title": "Partition-wise Linear Models", "authors": ["Hidekazu Oiwa", "Ryohei Fujimaki"], "emails": ["hidekazu.oiwa@gmail.com", "rfujimaki@nec-labs.com"], "sections": [{"heading": null, "text": "Region-specific linear models are widely used in practice due to their non-linear but highly interpretable model representations, and one of the most important challenges in their application is non-convexity in the simultaneous optimization of regions and region-specific models. This paper proposes novel convex, region-specific linear models, which we call partition-specific linear models. Our key ideas are 1) the allocation of linear models not to regions, but to partitions (region specifiers), and the representation of region-specific linear models through linear combinations of partition-specific models, and 2) the optimization of regions by partition selection from a large number of predetermined partition candidates using convex structured regulations. In addition to providing initialization-free globally optimal solutions, our convex formulation allows the derivation of a generalization limit and the use of advanced optimization techniques such as proximal methods and the decomposition of the linear models to at least show that our inductive models are better than the inductive models."}, {"heading": "1 Introduction", "text": "Among the methods of pre-processing, the partitioning of data is one of the most basic, in which an entrance space is divided into several sub-spaces (regions) and assigned a simple model for each region. In addition to better predictive performance resulting from the non-linear nature, the regional structure also provides a better understanding of data (i.e., better ways of interpreting it). Region-specific linear models learn both partitioning structures and predictors in each region. Such models vary - from traditional decision / regression problems [1] to advanced models [2, 3, 4] - depending on how they characterize regions (regions), region-specific prediction models and the objective functions that need to be optimized. An important challenge that remains in learning these models is the non-conventional exposure resulting from the interdependence of the optimization of regions and predictive models in individual regions."}, {"heading": "1.1 Related Work", "text": "The first category, to which our models belong, assumes a predictor in a particular region and has an advantage in the clarity of interpretation possibilities, while the latter assign a predictor to each individual date and have an advantage in greater model flexibility. Interpretation models are able to clearly show where and how the relationships between the inputs and outputs are changed. Another traditional framework is a hierarchical mix of experts [8], which is a probable, region-specific model system. Recently, Local Supervised Learning through Space Partitioning (LSL-SP) has been proposed. LSL-SP uses a linear chain of linear predictors."}, {"heading": "1.2 Notations", "text": "Scalars and vectors are indicated by lowercase letters x, matrices by uppercase letters X. Training samples and names are indicated by xn, RD, and yn. The basic notations used in this paper are summarized in Table 2."}, {"heading": "2 Partition-wise Linear Models", "text": "This section explains partition-by-partition linear models, assuming that effective partitioning is already known and fixed. Section 3 explains how to optimize partitions and region-specific linear models."}, {"heading": "2.1 Framework", "text": "Suppose we have P partitions (red dashed lines) that essentially specify 2P regions. Partition-wise linear models are defined as follows: First, we assign a linear weight vector ap to the p-th partition. This partition has an activity function, fp, which indicates whether or not the associated weight vector p is applied to individual data points. Second, we specify the weight vector a1 to be applied to the right side of the partition p1. In this case, the corresponding activity function is defined as f1 (x) = 1 if x is on the right side of p1. Second, region-specific predictors (square regions surrounded by partitions in Figure 1) are defined by a linear combination of active partition modes f-vectors."}, {"heading": "2.2 Partition Activeness Functions", "text": "A partition activity function fp divides the input space into two regions, and a series of activity functions defines the entire regional structure. Although each function is principally applicable to use as a partition activity function, due to our practical motivation to use region-specific linear models (i.e. interpretability is a priority), we prefer the simplest possible regional representation. Although this \"rule representation\" is simpler than others [2, 3] that use dense linear hyperplanes as region-specific, our empirical evaluation (section 5) shows that our partition-based linear models are competitive or even better than the others by optimizing the simple regional specifics (partition activity functions) accordingly."}, {"heading": "2.3 Global and Local Residual Model", "text": "As a special instance of partition-wise linear models, we propose here a model that we call the global and local residual model. It uses a global linear weight vector a0 in addition to the partition-wise linear weights. The global weight vector is active for all data. The predictor model (1) can be rewritten as follows: g (x) = aT0 x + \u2211 p fp (x) \u2211 d adpxd. (3) The integration of the global weight vector allows the model to determine how characteristics affect results not only locally, but also globally. Consider a new partition activity function f0 (x), which returns to 1 independently of x. Then, by setting f (\u00b7) = (f0 (\u00b7), f1 (\u00b7),... fp (\u00b7),... fP (\u00b7),... fP (\u00b7),... fP (\u00b7), T = (a0, a1),..., we can represent the same section of the global model as the residual model in local Saduct 1."}, {"heading": "3 Convex Optimization of Regions and Predictors", "text": "In Section 2, we presented a convex formulation of partition-wise linear models in (2), assuming that a number of partition activity functions were given. This section loosens this assumption and suggests a convex partition optimization algorithm."}, {"heading": "3.1 Region Optimization as Sparse Partition Selection", "text": "Let's say we got P + 1 partition activity functions, f0, f1,..., fP, and their associated linear weight vectors, a0, a1,..., aP, where f0 and a0, respectively, are the global activity function and the global weight vector. We formulate the regional optimization problem here as partition selection by setting most of the aps to zero, since ap = 0 corresponds to the situation in which the p-th partition does not exist. Formally, we formulate our optimization program in terms of regions and weight vectors by introducing two types of parity-inducing constraints on (2) as follows: min A-n '(yn, g (xn) s.t. p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-x-x-p-p-x-p-p-x-p-p-p-p-p-p-p-x-p-p-p-p-p-x-p-p-p-p-x-p-p-p-p-p-x-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-x-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-x-p-p-p-p-p-p-x-p-p-p-p-p-p-p-p-x-p-p-p-p-p-p-x-p-p-p-p-p-p-p-x-p-p-p-p-p-x-p-p-p-p-p-p-x-p-p-p-p-p-p-x-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "3.2 Convex Optimization via Decomposition of Proximal Maps", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 The Tightest Convex Envelope", "text": "The constraints in (5) are non-convex, and it is very difficult to find the global optimum due to the indicator functions and L0 penalties. This makes optimizing a non-convex region a very complicated task, and we therefore apply a convex relaxation. A standard approach to convex relaxation would be a combination of group L1 (the first constraint) and L1 (the second constraint) penalties. However, here we consider the closest convex relaxation of (4) as follows: min A \u2211 n '(yn, g (xn)) s.t. P \u2211 p = 1-convex-convexe-convexe-convexe-convexe-convexe-convexe-convexe-convexe-convexe-convexe-convexe-convexe-convexix-convexix-convexp-5-convexp-convexp-5-convexix-exp-convexp-5-convexp-convexp-ip-5-convexp-convex-exp-exp-ip-5-convex-exp-convex-exp-exp-exp-exp-5-convex-exp-exp-convexp-exp-5-convex-exp-convexp-5-convex-exp-exp-convex-exp-exp-convexp-ip-5-convexp-exp-5-convexp-convexp-convex-exp-exp-exp-exp-5-exp-convexp-convex-convex-convex-exp-convex-exp-exp-exp-exp-exp-exp-5-convex-convex-exp-convex-convex-exp-exp-convex-exp-convex-exp-ip-convexp-5-"}, {"heading": "3.2.2 Proximal Method and FISTA", "text": "The proximal method is an efficient standard tool for solving convex optimization problems with non-differential regulation mechanisms. It uses iterative gradient steps and proximal steps to update parameters, thus achieving O (1 / t) convergence [5] below the Lipschitz continuity of the loss gradient or even O (1 / t2) convergence when an acceleration technology such as a fast iterative shrinkage threshold (FISTA) [6, 15] is installed. Let us define A (t) as a weight matrix in the T iteration. In the gradient step, the weight vectors are updated to reduce the empirical loss by approximating first order (gradients) of the loss functions as follows: A (t + 1) = A (t) = 2) = Mind (t) \u2212 t ()."}, {"heading": "3.2.3 Decomposition of Proximal Maps", "text": "The cost of the proximal method strongly depends on the efficiency of solving the proximal problem (9).A number of approaches have been developed to improve efficiency, including the minimum standard point approach [14] and the network flow approach [16, 17].The key idea is to break down the proximal step into a sequence of sub-problems that are easy to solve. We first present two easily solvable proximal maps, as follows: M1 = argmin A1 2. \u2212 B \u2212 B \u2212 B \u2212 2F + p = 1 (t).ap p = 1 ap."}, {"heading": "3.2.4 Time Complexity and Convergence Analysis", "text": "Although the temporal complexity of a single gradient step (8) is O (NPD) with naive full-gradient computation, we can reduce the sequence by using the partition spareness. In the tenth gradient step, we only need to calculate the gradients of active partitions, which are much lower in practice than in P. A two-step gradient computation, i.e. first looking for active partitions and then calculating their gradients, yields the time complexity O (NP + P + D), where P \u00b2 is the number of active partitions. Time complexity in the proximal steps is dominated by the cost of calculating (11) and (12); the first is a simple soft-threshold operation, and the computational complexity is O (PD); the second is an L-shaped regulation operation, and the dominant factor is the sorting of the features in each group. The computational complexity of features O (D), the logical complexity of PD (O) and PD (D) results in complexity (D)."}, {"heading": "3.2.5 Warm Start of Global Model", "text": "Although partition-wise linear models using the proximal method can derive global optimum values from any starting point, the choice of starting point significantly influences the practical convergence velocity. We initialize the global weight vector using the L1 regularization solution as follows: a (1) 0 = argmina0 \u2211 n '(yn, a T 0 xn) + 0 \u00b2 a0 \u00b2 1. (17) The local weight vectors are uniformly initialized. Empirical comparisons between so few initialization methods such as random initialization, zero initialization, etc., indicate that this initialization reliably achieves a better empirical convergence. The result is given in Section 5.2.3. The iterative update procedure for global and local residuality is then expressed in algorithm 1."}, {"heading": "4 Generalization Bound Analysis", "text": "This section represents the derivation of a generalization error linked to the upper limit of expected satisfaction if we can increase the number of partition candidates P by the number of samples N. (1) Our bound analysis refers to the [19], which indicates limits to the general overlap of lasso cases, while our definition is specifically designed for partition-based linear models. (1) First, let us derive an empirical analysis of complexity [20], in which a practicable weight space is conditioned. (7) The definition of wheel-maker complexity is as follows: < X (A) = 2n E [sup A] n = 1 ig (6), where X = 1 ig (x2), xN). Expectation is for all i, i.e. (1) -weighted random variables. We can derive the complexity of our model using the lemma."}, {"heading": "5 Experiments", "text": "We conducted two types of experiments: 1) Assessment of the performance of partition-based linear models based on a simple synthetic dataset and 2) comparisons with state-of-the-art region-specific and locally linear models based on standard classification and regression benchmark datasets."}, {"heading": "5.1 Evaluation Using Synthetic Dataset", "text": "We created a synthetic binary classification dataset as follows: xns were uniformly sampled from a 20-dimensional input space in which each dimension had values between [\u2212 1, 1]; the target variables were determined using the XOR rule over the first and second characteristics (the other 18 characteristics were added as noise for predictive purposes), i.e., if the signs of the first characteristic value and the second characteristic value are equal, y = 1, otherwise y = \u2212 1. This is commonly known as a case in which linear models do not work. For example, the L1-regulated logistic regression produced almost random results with an error rate of 0.421. We created a partition for each characteristic except the first characteristic, and each partition became active when the corresponding characteristic value was greater than 0.0. Thus, the number of candidate partitions was red."}, {"heading": "5.2 Comparisons Using Benchmark Datasets", "text": "Next, we used benchmark data sets to compare our models with other state-of-the-art region-specific models. In these experiments, we simply created partition candidates (activity functions) as follows: For continuous value properties, we calculated all 5 quantiles for each attribute and created partitions at each quantile point. Partitions became active when a attribute value was larger than the corresponding quantity. For binary categorical characteristics, we created two partitions, conducting several experiments with other hyperparameter settings and confirming that variations in hyperparameter settings did not significantly affect the results. 4For example, a decision tree cannot be used to find a \"true\" XOR structure because boundary distributions for the first and second attribute cannot distinguish between positive and negative classes. One became active when attribute 1 (yes) was active and the other only when attribute 0 was active."}, {"heading": "5.2.1 Classification", "text": "For the classification, we compared the global and local residual model (global / local) with L1 logistic regression (linear), LSL-SP with linear discrimination analyses 5, LDKL with support for L2-5The source code is provided by the author of [3].regularized hinged losses 6, FaLK-SVM with linear kernel 7, and C-SVM with RBF kernel 8. Note that C-SVM is neither a regional nor a local linear classification model; it is rather a nonlinear model. We compared it with ours as a reference to a common nonlinear classification model. For our models, we used logistic functions for loss functions. The maximum iteration number was specified as 1000, and the algorithm stopped early when the gap in the empirical analysis of LSL became smaller than 10 \u2212 9 in 10 consecutive iterations."}, {"heading": "5.2.2 Regression", "text": "For regression, we compared Global / Local with Linear, Regression Tree10 from CART (RegTree) [1] and epsilon-SVR with RBF kernel 11. Target variables were standardized so that their mean was 0 and their variance was 1. Performance was evaluated based on the average square loss in the test data. RegTree tree depth and in RBF-SVR were determined by 10-fold cross validation. Other experimental settings were the same as in the classification tasks. Table 5 summarizes the RMSE values. In classification tasks, Global / Local consistently performed well. In kinematics, RBF-SVR performed significantly better than Global / Local, but Global / Local was better than Linear and RegTree in many other datasets."}, {"heading": "5.2.3 Warm Start Effect", "text": "We also performed time comparisons between our partition-wise linear models with and without warm start. We used the a1a dataset for the comparisons and took into account the relationship between 6https: / / research.microsoft.com / en-us / um / people / manik / code / LDKL / download.html7http: / / disi.unitn.it / \u02dc segata / FaLKM-lib / 8We used a libsvm packet for the experiment. http: / / www.csie.ntu.edu.tw / assess / cjlin / libsvm / 9 \u03bb1, \u03bb2p in Global / Local, \u03bb1 in Linear, \u03bbW, ziped in LDKL, ziped in C in FaLK-SVM, and C in RBF-SVM.10We used a scikit-learn.org / 11We used a libsvm training package that shows lower warm-time results than our models."}, {"heading": "6 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Scaling Up Optimization", "text": "When dealing with large-scale data where the number of samples is very large, the calculation of the full gradient (8) becomes a computational bottleneck. To expand our algorithm, stochastic and parallel optimization techniques seem promising. In terms of stochastic optimization, Mairal recently proposed Minimization by Incremental Surrogate Optimization (MISO) [21] as an incremental optimization method for a framework for majorization and minimization that includes proximal methods. Although MISO approaches stochastically gradients (samples), it has a linear convergence characteristic for the global optimum if the data size is known in advance; this convergence rate is the same as in the case of full gradient. Another promising direction is parallelization. Parallelization of complete gradient calculations is a simple process and may be of insignificant importance in practice."}, {"heading": "6.2 Advanced Partition Generation", "text": "In Section 5, we created partition candidates (activity functions) based on simple rules. Although this worked to some extent, more advanced methods of partition generation could improve predictivity accuracy. Taking locally linear SVMs [10] as an analogy, it is known that local coordinate (anchor) selection significantly influences prediction performance, and advanced methods of coordinate generation have been proposed [11, 12]. The algorithm proposed by Dekel and Shamir [23] gradually adds piecemeal regions based on detection of \"poorly predictable\" regions and trains piecemeal predictors. Although this approach is not directly applicable to partition-based linear models, such a \"boosting type\" approach to partition candidate generation is an interesting concept."}, {"heading": "6.3 Hierarchical Structured Sparseness", "text": "In this study, we treated partition candidates equally and enforced group-sparing punishment. In many real-world applications, data have structures, and it would be interesting to include such structures in partitional linear models in the learning process. In this respect, the \"tree-structured\" sparsity-inducing regularization proposed by Huang et al. [24] is particularly noteworthy. Defining hierarchical partition structures and automatically learning \"hierarchical\" regional structures could give us a better understanding of data structures. Note that such tree-structured regularization is also convex, and we could apply it directly to our optimization techniques."}, {"heading": "7 Summary", "text": "We have proposed a novel convex formulation of region-specific linear models, which we call partitionwise linear models. At the same time, our approach optimizes regions and predictors using sparsity-inducing structured penalties. In order to solve the optimization problem efficiently, we have derived an efficient algorithm based on the decomposition of proximal maps. Thanks to its convexity, our method is free from initialization dependence, and a generalization-related margin of error can be derived. Empirical results show the superiority of partitionwise linear models over other region-specific and locally linear models."}, {"heading": "Acknowledgments", "text": "Most of the work was done during the internship of the first author in the central research laboratories of the NEC."}], "references": [{"title": "Classification and Regression Trees", "author": ["Leo Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1984}, {"title": "Local deep kernel learning for efficient non-linear svm prediction", "author": ["Cijo Jose", "Prasoon Goyal", "Parv Aggrwal", "Manik Varma"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Local supervised learning through space partitioning", "author": ["Joseph Wang", "Venkatesh Saligrama"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Cost-Sensitive Tree of Classifiers", "author": ["Zhixiang Xu", "Matt Kusner", "Minmin Chen", "Kilian Q. Weinberger"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Approximation accuracy, gradient methods, and error bound for structured convex optimization", "author": ["Paul Tseng"], "venue": "Mathematical Programming,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "On decomposing the proximal map", "author": ["Yaoliang Yu"], "venue": "In NIPS, pages 91\u201399,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["Michael I. Jordan", "Robert A. Jacobs"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Fast and scalable local kernel machines", "author": ["Nicola Segata", "Enrico Blanzieri"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Locally Linear Support Vector Machines", "author": ["Lubor Ladicky", "Philip H.S. Torr"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Nonlinear learning using local coordinate coding", "author": ["Kai Yu", "Tong Zhang", "Yihong Gong"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Learning anchor planes for classification", "author": ["Ziming Zhang", "Lubor Ladicky", "Philip H.S. Torr", "Amir Saffari"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Clustered support vector machines", "author": ["Quanquan Gu", "Jiawei Han"], "venue": "In AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Structured sparsity-inducing norms through submodular functions", "author": ["Francis R. Bach"], "venue": "In NIPS, pages 118\u2013126,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Yurii Nesterov"], "venue": "Core discussion papers,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "A fast parametric maximum flow algorithm and applications", "author": ["Giorgio Gallo", "Michael D. Grigoriadis", "Robert E. Tarjan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1989}, {"title": "Structured convex optimization under submodular constraints", "author": ["Kiyohito Nagano", "Yoshinobu Kawahara"], "venue": "In UAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John Duchi", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Structured sparsity and generalization", "author": ["Andreas Maurer", "Massimiliano Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Rademacher and gaussian complexities: risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Optimization with first-order surrogate functions", "author": ["Julien Mairal"], "venue": "In ICML, pages 783\u2013791,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"], "venue": "In ASILOMAR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1993}, {"title": "There\u2019s a hole in my data space: Piecewise predictors for heterogeneous learning problems", "author": ["Ofer Dekel", "Ohad Shamir"], "venue": "In AISTATS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Learning with structured sparsity", "author": ["Junzhou Huang", "Tong Zhang", "Dimitris N. Metaxas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Such models vary\u2014from traditional decision/regression trees [1] to more advanced models [2, 3, 4]\u2014depending on their region-specifiers (how they characterize regions), region-specific prediction models, and the objective functions to be optimized.", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "Such models vary\u2014from traditional decision/regression trees [1] to more advanced models [2, 3, 4]\u2014depending on their region-specifiers (how they characterize regions), region-specific prediction models, and the objective functions to be optimized.", "startOffset": 88, "endOffset": 97}, {"referenceID": 2, "context": "Such models vary\u2014from traditional decision/regression trees [1] to more advanced models [2, 3, 4]\u2014depending on their region-specifiers (how they characterize regions), region-specific prediction models, and the objective functions to be optimized.", "startOffset": 88, "endOffset": 97}, {"referenceID": 3, "context": "Such models vary\u2014from traditional decision/regression trees [1] to more advanced models [2, 3, 4]\u2014depending on their region-specifiers (how they characterize regions), region-specific prediction models, and the objective functions to be optimized.", "startOffset": 88, "endOffset": 97}, {"referenceID": 4, "context": "We have developed an efficient algorithm to solve structured-sparse optimization problems, and in it we utilize both a proximal method [5, 6] and the decomposition of proximal maps [7].", "startOffset": 135, "endOffset": 141}, {"referenceID": 5, "context": "We have developed an efficient algorithm to solve structured-sparse optimization problems, and in it we utilize both a proximal method [5, 6] and the decomposition of proximal maps [7].", "startOffset": 135, "endOffset": 141}, {"referenceID": 6, "context": "We have developed an efficient algorithm to solve structured-sparse optimization problems, and in it we utilize both a proximal method [5, 6] and the decomposition of proximal maps [7].", "startOffset": 181, "endOffset": 184}, {"referenceID": 0, "context": "Well-known precursors to region-specific linear models are decision/regression trees [1], which use rule-based region-specifiers and constant-valued predictors.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "Another traditional framework is a hierarchical mixture of experts [8], which is a probabilistic tree-based region-specific model framework.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Recently, Local Supervised Learning through Space Partitioning (LSL-SP) has been proposed [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "Additionally, a Cost-Sensitive Tree of Classifiers (CSTC) algorithm has also been developed [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "FaLK-SVMs produce test-point-specific weight vectors by learning local predictive models from the neighborhoods of individual test points [9].", "startOffset": 138, "endOffset": 141}, {"referenceID": 9, "context": "Another advanced locally linear model is that of Locally Linear Support Vector Machines (LLSVMs) [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "LLSVMs assign linear SVMs to multiple anchor points produced by manifold learning [11, 12] and construct test-point-specific linear predictors according to the weights of anchor points with respect to individual test points.", "startOffset": 82, "endOffset": 90}, {"referenceID": 11, "context": "LLSVMs assign linear SVMs to multiple anchor points produced by manifold learning [11, 12] and construct test-point-specific linear predictors according to the weights of anchor points with respect to individual test points.", "startOffset": 82, "endOffset": 90}, {"referenceID": 12, "context": "Similarly, clustered SVMs (CSVMs) [13] assume given data clusters and learn multiple SVMs for individual clusters simultaneously.", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "have proposed Local Deep Kernel Learning (LDKL) [2], which adopts an intermediate approach with respect to region-specific and locally linear models.", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "Although this \u201crule-representation\u201d is simpler than others [2, 3] which use dense linear hyperplanes as region-specifiers, our empirical evaluation (Section 5) indicates that our partition-wise linear models perform competitively with or even better than those others by appropriately optimizing the simple region-specifiers (partition activeness functions).", "startOffset": 59, "endOffset": 65}, {"referenceID": 2, "context": "Although this \u201crule-representation\u201d is simpler than others [2, 3] which use dense linear hyperplanes as region-specifiers, our empirical evaluation (Section 5) indicates that our partition-wise linear models perform competitively with or even better than those others by appropriately optimizing the simple region-specifiers (partition activeness functions).", "startOffset": 59, "endOffset": 65}, {"referenceID": 13, "context": "It is easy to confirm that the constraints in (5) represent the Lov\u00e1sz extension of the constraints in (4), and this extension gives the tightest convex envelope of non-decreasing sub-modular functions [14].", "startOffset": 202, "endOffset": 206}, {"referenceID": 4, "context": "This achieves O(1/t) convergence [5] under Lipschitz-continuity of the loss gradient, or even O(1/t) convergence if an acceleration technique, such as a fast iterative shrinkage thresholding algorithm (FISTA) [6, 15], is incorporated.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "This achieves O(1/t) convergence [5] under Lipschitz-continuity of the loss gradient, or even O(1/t) convergence if an acceleration technique, such as a fast iterative shrinkage thresholding algorithm (FISTA) [6, 15], is incorporated.", "startOffset": 209, "endOffset": 216}, {"referenceID": 14, "context": "This achieves O(1/t) convergence [5] under Lipschitz-continuity of the loss gradient, or even O(1/t) convergence if an acceleration technique, such as a fast iterative shrinkage thresholding algorithm (FISTA) [6, 15], is incorporated.", "startOffset": 209, "endOffset": 216}, {"referenceID": 5, "context": "Furthermore, we have adopted a backtracking rule [6] to avoid the difficulty of calculating appropriate step widths beforehand.", "startOffset": 49, "endOffset": 52}, {"referenceID": 5, "context": "We also employ FISTA [6] to achieve a faster convergence rate, O(1/t), for weakly convex problems.", "startOffset": 21, "endOffset": 24}, {"referenceID": 13, "context": "A number of approaches have been developed for improving efficiency, including the minimum-norm-point approach [14] and the networkflow approach [16, 17].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "A number of approaches have been developed for improving efficiency, including the minimum-norm-point approach [14] and the networkflow approach [16, 17].", "startOffset": 145, "endOffset": 153}, {"referenceID": 16, "context": "A number of approaches have been developed for improving efficiency, including the minimum-norm-point approach [14] and the networkflow approach [16, 17].", "startOffset": 145, "endOffset": 153}, {"referenceID": 6, "context": "Alternatively, this paper employs the decomposition of proximal maps [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "Proof: From the discussion of Theorem 1 in [7], we can determine the sufficient condition for satisfying this decomposition of proximal maps.", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "Each proximal operator with respect For example, the fastest algorithm for the networkflow approach hasO(M(B+1) log(M/(B+1))) time complexity, where B is the number of breakpoints determined by the structure of the graph (B \u2264 D(P +1) = O(DP )) and M is the number of nodes, that is P + D(P + 1) = O(DP ) [16].", "startOffset": 304, "endOffset": 308}, {"referenceID": 13, "context": "to each group can be computed through a projection on an L1-norm ball (derived from the Moreau decomposition [14]), that is,", "startOffset": 109, "endOffset": 113}, {"referenceID": 17, "context": "This projection problem can be efficiently solved [18].", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "Our bound analysis is related to that of [19], which gives bounds for general overlapping group Lasso cases, while ours is specifically designed for partition-wise linear models.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "Let us first derive an empirical Rademacher complexity [20] for a feasible weight space conditioned on (7).", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "This is a special case of Theorem 2 of [19] in which the number of groups is P + D(P + 1).", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "Applying Theorem 2 of [19] to (20) gives us (19).", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "This bound is straightforwardly derived from Lemma 1 and the discussion of [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "In [20], it has been shown that the uniform bound on the estimation error can be obtained through the upper bound of Rademacher complexity derived in Lemma 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "5 in [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "sampled from a specific data distribution D and let us assume loss functions `(\u00b7, \u00b7) to be L-Lipschitz functions with respect to a norm \u2016 \u00b7 \u2016 and its range to be within [0, 1].", "startOffset": 169, "endOffset": 175}, {"referenceID": 2, "context": "The source code is provided by the author of [3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "2 Regression For regression, we compared Global/Local with Linear, regression tree10 by CART (RegTree) [1], and epsilon-SVR with RBF kernel11.", "startOffset": 103, "endOffset": 106}, {"referenceID": 20, "context": "With respect to stochastic optimization, Mairal has recently proposed Minimization by Incremental Surrogate Optimization (MISO) [21] as an incremental optimization method for a majorization-minimization framework that includes proximal methods.", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "Another possible future direction is a greedy solver for original non-convex problems (4), such as orthogonal matching pursuits [22].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "Taking locally linear SVMs [10] as an analogy, it is known that local coordinate (anchor) selection considerably affects predictive performance, and advanced coordinate generation", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "methods have been proposed [11, 12].", "startOffset": 27, "endOffset": 35}, {"referenceID": 11, "context": "methods have been proposed [11, 12].", "startOffset": 27, "endOffset": 35}, {"referenceID": 22, "context": "The algorithm proposed by Dekel and Shamir [23] gradually adds piece-wise regions on the basis of the detection of \u201cpoorly predictable\u201d regions, and it trains piece-wise predictors.", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "[24] is particularly notable.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations. One of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models. This paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models. Our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partitionspecific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations. In addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations. Experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models.", "creator": "LaTeX with hyperref package"}}}