{"id": "1410.8750", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2014", "title": "Learning Mixtures of Ranking Models", "abstract": "This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a Mallows Mixture Model. Despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-k prefix in both the rankings. Before this work, even the question of identifiability in the case of a mixture of two Mallows models was unresolved.", "histories": [["v1", "Fri, 31 Oct 2014 14:31:54 GMT  (50kb)", "http://arxiv.org/abs/1410.8750v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pranjal awasthi", "avrim blum", "or sheffet", "aravindan vijayaraghavan"], "accepted": true, "id": "1410.8750"}, "pdf": {"name": "1410.8750.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Aravindan Vijayaraghavan"], "emails": ["pawashti@cs.princeton.edu", "avrim@cs.cmu.edu", "osheffet@seas.harvard.edu", "vijayara@cims.nyu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 0.87 50v1 [cs.LG] 3 1"}, {"heading": "1 Introduction", "text": "Probabilistic modeling of ranking data is a comprehensively studied problem with a rich body of past work [1, 2, 3, 4, 5, 6, 7, 8, 9]. Ranking with such models has applications in a variety of areas, ranging from understanding user preferences in voting systems and the theory of social choice to more modern learning in online search, crowd sourcing and referral systems. (For example, the population might correspond to a \"ground truth ranking\" across a number of films.) Each individual user generates his own ranking as a noisy version of this central ranking and independent of other users. The most popular ranking model of choice is the allows model [1], where scaling takes place in addition to the individual parameters (0, 1)."}, {"heading": "2 Notations and Properties of the Mallows Model", "text": "We represent permutations about the elements in Un by their indices [n]. (E.g., \u03c0 = (n, n-1,..) We allow the permutation (en, en-1,., e1). (E.g., n-2,.) We allow the permutation (en, en-1,., e1). (Z.2) Let us allow the permutation (en-1,.). (E.2) Let us allow the permutation (en-1,.) Let us allow the permutation (en-1,.) Let us allow the permutation (en-1,.,.) Let us allow the two permutations (n) as the Kendall-tau distance [24] between them (number of pairs of reversations between the two models). (0, 1) Let us name the Zi (.) = 1 \u2212 Z.2 We name the two permutations."}, {"heading": "3 Algorithm Overview", "text": "(c) If we have one (a), (a), (b), (b), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c)."}, {"heading": "4 Experiments", "text": "The most important contribution of our work is the development of an algorithm that has been shown to learn a mixture of two Mallows models, but could it be that the previously existing heuristics, even if not proven, are still performing well in practice? We compare our algorithm with existing techniques to see if and under what settings our algorithm outperforms them. We compare our algorithm with the popular EM-based algorithm of [5], as it is the most popular method of learning a mixture of Mallows models as EM-based heuristics. The EM algorithm starts with a random guess for the two central permutations. In iteration, EM maintains a guess about the two Mallows models that generated the algorithms. First, the algorithm assigns weight (expectation step) to each ranking in our sample, with the weight of a ranking reflecting the probability that it was generated from the first or second of the current Mallows models."}, {"heading": "5 Acknowledgements", "text": "We would like to thank Ariel Procaccia for drawing our attention to various references to the mallow model in the theory of social decision-making."}, {"heading": "6 Properties of the Mallows Model", "text": "In this section we will sketch some of the properties of the mallows model. Some of these properties have been shown before (see [27]), but we will add them to this appendix for completion. Our algorithm and its analysis rely heavily on these properties. Note: In the face of a mallows model Mn (\u03c6, \u03c00), we call Zn = 1 \u2212 \u03c6, and we call Z [n] the trick we get by omitting the element e (by projecting it onto all elements except e). Notation \u03c0 = (e, \u03c3) refers to a permutation whose first element is e and the elements 2 to n, we abuse the notation and designate the permutation we obtain by omitting the element e (by projecting it onto all elements except e)."}, {"heading": "6.1 Proofs of Lemmas 6.1, 6.2, 6.3", "text": "Observation. All the properties we specify in this appendix and prove are based on the following important observation. Given are two permutations \u03c0 and \u03c0 \u2032, whereby the first element in \u03c0 is referred to as e1. Then we have that # pairs (e1, ei) i6 = 1, that \u03c0, \u03c0 \u2032 divergence to = (position of e1 in \u03c0 \u2032) \u2212 1, the same applies to the last element designated as \u03c0p \u2212 en, only using the distance between pos\u03c0 \u2032 (en) and the nth position (i.e., \u03c0 \u2032 -posp \u2032 (en)). We begin with the characterization of Z [n]. Property 6,4. For each n and each other \u03c00 \u04320 \u04320 \u04320 \u04320 \u04320 V0 V0 V0 V0 V0 V0 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V8 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 IF 0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0 V0"}, {"heading": "6.2 Total Variation Distance", "text": "This subsection aims to prove that the total variation distance between the two models M (1 / 0) and M (1 / 0) is at most as large as the total variation distance between the two models M (1 / 0). \u2212 ln (1 / 0). \u2212 ln (1 / 0). \u2212 ln (1 / 2). \u2212 ln (1 / 2). \u2212 ln (1 / 2)."}, {"heading": "7 Algorithm and Subroutines", "text": "(1), (2), (2), (2), (3), (3), (3), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5), ("}, {"heading": "8 Retrieving the Top elements", "text": "Here we show how the first stage of the algorithm i.e. steps (a) - (e.i) manages to restore the top few elements of both rankings (1) and (2) and also estimate the parameters (1) and (2), then the decomposition is unique, and therefore we can recover the top few elements and parameters from INFER TOP-K. Otherwise we show that this procedure did not work for all O (log n) iterations, we are in the degenerated case (Lemma 9.1 and Lemma 9.6), and we can handle this separation.For the sake of analysis, we denounce the smallest length of the vectors in the partition i.e."}, {"heading": "8.1 Proof of Lemma 3.2: the Full Rank Case", "text": "If such a partition is S (a), S (a), S (b), S (b), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c (c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c) c), c) c) c), c) c) c) c), c) c) c) c) c), c) c) c) c) c) c), c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c), c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c"}, {"heading": "9 Degenerate Case", "text": "While we know that we succeed when Ma, Mb, Mc are not negligible, we show that we apply the same yardsticks. < Ma, Mc have not negligible minimum single values for one of the O (log n) random partitions, we will now understand if this does not happen. < Ma, Mc have not negligible single values for the elements in L. We call a blending model w1M (1, 2), which describes W2M (2, 2) as degenerated when the parameters of the two Mallows models are the same, and except for most of the 2 elements, all elements in L fall into the majority bucket. In other words, we first show that we have equality when the tensor method fails, then the parameters of the two models are the same."}, {"heading": "9.2 Equal Scaling Parameters", "text": "The following simple properties of our random division will be decisive for our algorithms. Lemma 9,4. Random division of [m] into A, B, C and M (both) is highly probable (at least 1 \u2212 exp (\u2212 1C9,4 \u00b7 m): 1. The claimed boundaries are followed by a simple application of Chernoff Bounds, since each element in A is likely to be chosen 1 / 3 independently of each other. The second part follows by looking at the m / 2 consecutive pairs of elements and observing that each pair falls completely into A with probability 1 / 9.Lemma 9,5."}, {"heading": "9.3 Establishing Degeneracy", "text": "Next, we note that none of the O (log n) rounds was successful, then the two central permutations (limited to the uppermost O (log1 / \u03c6min n) positions would be essentially the same of at most one pair of elements.Lemma 9,6. We have two cases, depending on whether the large elements L (2) / log (2) -2 (2) -2 (2) -2) -2 (2) / log (1) -2) -2 (2) -2 (2) -2 (2) -2 (2) -2 (2) -2 (2) -2 (2) -2) -2) -2 (2) -3 (2) -3) -3 (2) -3 (2) -4 (2) -4) -4 (2) -4 (2) -4 (2) -4). Let i, j, k indicate the indices of elements in L that are not in Lemma. With constant probability, the random partition Sp, Sp, Sc, Sp, Sc, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp, Sp,"}, {"heading": "9.4 Auxiliary Lemmas for Degenerate Case", "text": "Lemma 9,10. For each mallows model with the parameters \u03c61, \u03c62, the probability that neither of the two elements belongs to A has increased to a maximum of 1 / nC. This easily results in the necessary conclusions. Lemma 9,11. Let us also consider a partition A and the uppermost m \u00b2 logn elements according to the probability that two large elements belong to egg, ej \u00b2 L belonging to different buckets. The probability that none of them belongs to A is at most 1 / nC. This results in the necessary conclusions. Lemma 9,11. Let us also note that these elements are in partition Sa if two large elements are egg, ej \u00b2 L \u00b2 belonging to different buckets."}, {"heading": "10 Recovering the complete rankings", "text": "Let f (1) (i) j (i) be the probability that the element ei (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (1) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i) i (1) i) i (1) i (1) i) i (1) i) i (1) i) i (1) i) i (1) i) i i (1) i) i i i (1) i i (1) i) i (1) i (1) i) i i (1) i i (1) i (1) i (1) i (1) i (1) i (1) i (1) i) i (1) i (1) i) i (1) i (1) i (1) i) i (1) i (1) i (1) i) i (1) i (1) i (1) i) i (1) i (1) i) i (1) i (1) i) i (1) i (1) i) i (1) i) i (1) i (1) i) i (1) i (1) i) i (1) i (1) i) i (1) i (1) i) i (1) i (1) i) i (1) i) i (1) i) i (1) i (1) i) i (1) i) i (1) i) i (1) i) i (1 (1) i) i (1) i) i (1) i (1) i (1) i (1 (1) i) i) i (1) i (1 (1) i) i (1) i (1) i) i (1) i (1) i (1) i (1) i (1) i) i (1) i (1) i (1) i (1) i) i (1) i (1) i (1) i) i (1) i (1) i (1) i (1) i (1) i"}, {"heading": "11 Wrapping up the Proof", "text": "The Evidence of Theorem 3.1. Let us estimate the initial error in P on the basis of the estimates. Let us also, for convenience, determine the sample 2 = 9.1 (n, 0). Let us also restore the sample 2 = 9.1 (n, \u03c6min, wmin, imm). Let us select the sample 2 as a parameter large enough that the sample 2 x 2 x 2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.1 x 2.1 x 2.1 x 2.1 x 2.1 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.1 x 2.2 x 2.2 x 2.2 x 2.2 x 2.1 x 2.2 x 2.1 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.1 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2 x 2.2"}, {"heading": "12 Conclusions and Future Directions", "text": "In this paper, we have outlined the first polynomic time algorithm for learning the parameters of a mixture of two mallows models. Our algorithm works for an arbitrary mixture and does not require separation between the underlying base rankings. We would like to point out that in the first stage (tensor decompositions) of our algorithm, we can achieve significant acceleration by reducing to an instance with only k \u0445 log1 / \u03c6 n elements. Several interesting directions emerge from this work. A natural next step is to generalize our results to a mixture of k-mallows models for k > 2. We believe that most of these techniques can be extended to design algorithms that require poly (n, 1 / year) k-time.It would also be more interesting to obtain algorithms to learn a mixture of k-mallows models running at the time poly (k, n), perhaps not in a suitable generative analysis position [23] or under other conditions."}, {"heading": "13 Some Useful Lemmas for Error Analysis", "text": "u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u,"}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "This work concerns learning probabilistic models for ranking data in a heteroge-<lb>neous population. The specific problem we study is learning the parameters of a<lb>Mallows Mixture Model. Despite being widely studied, current heuristics for this<lb>problem do not have theoretical guarantees and can get stuck in bad local optima.<lb>We present the first polynomial time algorithm which provably learns the param-<lb>eters of a mixture of two Mallows models. A key component of our algorithm is<lb>a novel use of tensor decomposition techniques to learn the top-k prefix in both<lb>the rankings. Before this work, even the question of identifiability in the case of a<lb>mixture of two Mallows models was unresolved.", "creator": "LaTeX with hyperref package"}}}