{"id": "1410.8864", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2014", "title": "Greedy Subspace Clustering", "abstract": "We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity between subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.", "histories": [["v1", "Fri, 31 Oct 2014 19:50:42 GMT  (380kb,D)", "http://arxiv.org/abs/1410.8864v1", "To appear in NIPS 2014"]], "COMMENTS": "To appear in NIPS 2014", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["dohyung park", "constantine caramanis", "sujay sanghavi"], "accepted": true, "id": "1410.8864"}, "pdf": {"name": "1410.8864.pdf", "metadata": {"source": "CRF", "title": "Greedy Subspace Clustering", "authors": ["Dohyung Park", "Constantine Caramanis", "Sujay Sanghavi"], "emails": ["dhpark@utexas.edu", "constantine@utexas.edu", "sanghavi@mail.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "Subspace clustering is a classical problem in which a point is given in a high-dimensional ambient space and one wants to approach it by uniting low-dimensional linear subspaces. In particular, each subspace contains a subset of points. This problem is difficult because one has to find the subspaces together and the associated points; the data given to us are not labeled. Of course, the connections of subspace models arise in constellations in which data from several latent phenomena are mixed and separated. Applications of subspace clustering include motion segmentation [23], face clustering [8], gene expression analysis [10] and system identification [22]. In these applications, data points are labeled with the same label (e.g. facial images of a person under different lighting conditions, feature points of a moving rigid object in a video sequence)."}, {"heading": "1.1 Related work", "text": "Most of the related work in this area assumes that an underlying subspace is parallel to some canonical axes. Subspace clustering for connections of arbitrary subspaces is mainly considered in the fields of machine learning and computer vision. [21] Most of the results from these communities are based on empirical reasoning, providing algorithms derived from theoretical intuition and showing that they work empirically well with practical datasets. To name just a few, GPCA [21], spectral curvature clustering (SCC), and many iterative methods showing their good empirical performance for subspace clustering."}, {"heading": "1.2 Notation", "text": "In Rp there are a number of N data points, which are denoted by Y = {y1,..., yN}. The data points are located on or near a union of L sub-spaces D = \u0445Li = 1Di. Each sub-space Di is of the dimension di, which is smaller than p. For each point yj, wj denotes the index of the next sub-space. Let Ni denote the number of points whose closest sub-space Di is, i.e. Ni = \u0445 Nj = 1 Iwj = i. In this paper, quantities and sub-spaces are denoted by calligraphic letters. Matrices and key parameters are denoted by letters in uppercase letters, vectors and scalars by letters in lowercase letters. We often denote the set of n-indices by [n] = {1, 2,.., n}. As usual, span {\u00b7} denotes a subspace spanned by a series of vectors."}, {"heading": "2 Algorithms", "text": "We propose two algorithms for clustering in subspace: \u2022 NSN + GSR: Run Nearest Subspace Neighbor (NSN) to construct a neighborhood matrix, and then Greedy Subspace Recovery (GSR) to construct a neighborhood matrix. \u2022 NSN + Spectral: Run Nearest Subspace Neighbor (NSN) to construct a neighborhood matrix, and then perform spectral clustering for Z = W + W >."}, {"heading": "2.1 Nearest Subspace Neighbor (NSN)", "text": "In the next steps, if we have found a few correct neighborhood points that are on the same true subspace, and have no other knowledge of the actual subspace and the rest of the points, then the potentially correct point that comes closest to the subspace that is closest to the correct neighbors we have is the one that motivates us to describe NSN on the same true subspace.Algorithm 1 Nearest Subspace ghbor (NSN) Input: A set of N samples Y = ymal,. yN}, The number of neighbors required K, Maximum subspace kmax."}, {"heading": "2.2 Greedy Subspace Recovery (GSR)", "text": "Suppose NSN has found the right neighbors for a data point. How can we verify that they are actually correct, that is, that they are on the same true subspace? A natural way is to count the number of points near the subspace that is spanned by the neighbors. If they include one of the true subspaces, then many other points will be on the span. If they do not include real subspaces, few points will be near the subspace. This fact motivates us to use a greedy algorithm to restore the subspaces. With the neighborhood constructed by NSN (or any other algorithm), we recover the L-subspaces. If there is a neighborhood that contains only the points on the same subspace, the algorithm will successfully restore the connections of the true subspaces."}, {"heading": "3 Theoretical results", "text": "We analyze our algorithms in two standard noiseless models. The main laws represent sufficient conditions under which the algorithms will most likely precisely group the points. To simplify the analysis, we assume that each subspace has the same dimension and the number of data points on each subspace is equal, i.e., d, d1 = \u00b7 \u00b7 = dL, n, N1 = \u00b7 \u00b7 \u00b7 = NL. We assume that the algorithm knows d. Nevertheless, our analysis can extend to the general case."}, {"heading": "3.1 Statistical models", "text": "We consider two models used in the literature for clustering in sub-spaces: \u2022 Completely random model: The sub-spaces are drawn iid uniformly randomly, and the points are also generated iid randomly. \u2022 Semi-random model: The sub-spaces are determined arbitrarily, but the points are generated iid randomly. Let Di-Rp-d, i-d [L] be a matrix whose columns form an orthonormal basis for Di. An important measure we use in the analysis is the affinity between two sub-spaces, defined as aff (i, j), and aff (i, j), and only if it is randomly implied (i, j) = 1. If aff (i \u2212 xi) = 0, each vector on Di [0, 1] is the kest main angle between Di and Dj."}, {"heading": "3.2 Main theorems", "text": "Suppose L d-dimensional subspaces and n points on each subspace are generated in a completely random model with n polynomial in d. There are constants C1, C2 > 0, which are guaranteed to be simpler, however, that ifn d > C1 (log ned\u03b4) 2, dp < C2 log nlog nlog (ndL3 \u2212 1), (1) then with probability at least 1 \u2212 3L2 \u2212 \u03b4, NSN + GSR 3 cluster identify the points accurately, and there are other constants C \u2032 1, C \u2032 2 > 0 such that (1) with C1 and C2 replaced by C \u2032 1 and C \u2032 2 spaces the points exactly with probability 1 \u2212 3L2."}, {"heading": "4 Experimental results", "text": "In this section we compare our algorithms empirically with the existing algorithms in terms of cluster performance and computing time (on a single desktop). For NSN we used the fast5NSN with K = d \u2212 1 and kmax = d2 log de followed by GSR with arbitrarily smaller. Implementation described in Section A.1. The compared algorithms are K-mean, K-level 6, SSC, LRR, SCC, TSC7 and SSC-OMP8. The number of replications in K-mean, K-level and the means used in the spectral cluster are all fixed at 10. The algorithms are compared in terms of cluster error (CE) and neighborhood selection error (NSE), defined as (CE) = min."}, {"heading": "4.1 Synthetic data", "text": "We compare the performance on synthetic data generated from the completely random model. In Rp, five d-dimensional sub-spaces are generated uniformly according to the random principle. Then, for each sub-space n unitnorm points are generated iid uniformly according to the random principle on the sub-space. To see the agreement with the theoretical result, we introduced the algorithms under fixed d / p and varied n and d. We set d / p = 3 / 5 so that each pair of sub-spaces has intersections. Figures 1 and 2 show CE and NSE, respectively. Each error value is averaged over 100 attempts. Figure 1 shows that our algorithm clusters the data points better than the other algorithms. As predicted in the theorems, the cluster performance improves as the number of points increases. However, it also improves because the dimension of sub-spaces 6K apartments is similar to the K-mean. Figure 1 shows that our algorithm bundles the data points better than the other algorithms."}, {"heading": "4.2 Real-world data : motion segmentation and face clustering", "text": "For motion segmentation, we used the data set Hopkins 155 [17], which contains 155 video sequences with 2 or 3 movements; for facial segmentation, we used the data set Extended Yale B with cropped images from [5, 13]. The data set contains 64 images for each of 38 people in frontal view and different lighting conditions. To compare the existing algorithms, we used the set of 48 x 42 enlarged raw images provided by the authors of [4]. The parameters of the existing algorithms were determined as provided in their source codes.10 Tables 2 and 3 show CE and average computing time.11 We can see that NSN + Spectral works competitively with the methods with the lowest errors, but much faster. Compared to the other greedy neighborhood construction algorithms, SSC-OMP and TSC, our algorithm shapes clearly show that we do not represent the proposed number of SC numbers correctly."}, {"heading": "A Discussion on implementation issues", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 A faster implementation for NSN", "text": "At each step of NSN, the algorithm calculates the projections of all points onto a subspace and finds one with the largest norm. Instead of finding the maximum norm of the algorithm, we can find the maximum squared norm of the projections. Let Uk find the subspace U at step k. that is, for each data point y we have the maximum norm for ProjUky 2 = maximum norm for ProjUk 2 + | u > k y | 2wo uk is the new orthogonal axis added by Uk \u2212 1 NSuk and Uk = Uk \u2212 1 yuk. As a result of ProjUk \u2212 1y \u00b2 2 + | u > k | 2wo uk is the new orthogonal axis added by Uk \u2212 1 NSuk to make Uk = maximum norm."}, {"heading": "A.2 Estimation of the number of clusters", "text": "If L is unknown, it can be estimated in the cluster step. In spectral clustering, a well-known approach to estimating L is to find a knee point in the singular values of the neighborhood matrix. It is the point at which the difference between two consecutive singular values is greatest. In GSR, we do not need to estimate the number of clusters a priori. Once the algorithms are complete, the number of resulting groups will be our estimate of L."}, {"heading": "A.3 Parameter setting", "text": "The selection of K and kmax depends on the dimension of the sub-spaces. If the data points are exactly on the sub-spaces of the model, K = kmax = d is sufficient for GSR to restore a sub-space. In practical situations where the points are close to the sub-spaces, it is better to set K to be larger than d. kmax can also be larger than d, because the kmax \u2212 d additional dimensions that can be caused by noise do not in practice overlap with the other sub-spaces. For the extended Yale B dataset and the Hopkins155 dataset, we found that NSN + Spectral performs well when K is set to about 2d."}, {"heading": "B Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof outline", "text": "Exact clustering of our algorithms depends on NSN being able to find all the correct neighbors for the data points, so that the following algorithm (GSR or spectral clustering) can precisely group the points. NSN + GSR guarantees exact clustering if there is a point on each subspace that has all the correct neighbors that are at least d \u2212 1. For NSN + Spectral, exact clustering is guaranteed if each data point has only n \u2212 1 other points on the same subspace than neighbors. In the following, we explain why these are exactly right. Step 1-1: Exact clustering Condition for GSRThe two statistical models have a property that is different from the true subspaces for each d-dimensional subspace in Rp."}, {"heading": "B.2 Preliminary lemmas", "text": "Before entering into the technical parts of the proof, we will introduce the main components to be used. The following problem concerns upper and lower limits of the order statistics for the projections of uniformly random unit vectors. Lemma B.3. Let x1,.., xn draw uniformly randomly from the d-dimensional unit sphere Sd \u2212 1. Let z (n \u2212 m + 1) draw the m'th largest value of {zi, 0, 2, 1 \u2264 i \u2264 n}, where A-Rk \u00b7 i.e. assume that the rows of A are orthonormally related to each other. For each x-dimensional unit there is a constant C > 0, so that for n, m, d, k-N wheren \u2212 m + 1 \u2264 i \u2264 n}, where A-Rk qualities 2 (4) we have the vez2 (n \u2212 m + 1) > kd-dimensional capability."}, {"heading": "B.3 Proof of Theorem 3.2", "text": "The reason for this is that NSN selects the first wrong point in level k for y1. (...) The first wrong point in level k is that we select the next points in level k. (...) The second point is that we select the next points in level K. (...) The third point is that we select the next points in level K. (...) The third point is that we select the next points in level K. (...) The third point is that we select the next points in level K. (...) The third point is that we select the first points in level K. (...) The third point is that we select the next points in level K. (...) The third point is that we select the next points in level K. (...) The third point is that we select the next points in level K. (...) The third point is that we select the next points in level K. (...) The third point is that we select the next points in level K."}, {"heading": "B.4 Proof of Theorem 3.1", "text": "As we have done in section B.3, we prove in this section that if (1) all correct neighbors for y1 with a probability of at least 1 \u2212 3\u03b41 \u2212 \u03b4. \u2212 The only difference between the semi-random model and the completely random model is the statistical dependence between subranges. We can follow step 3 in section B.3 because they do not consider statistical dependence between subranges. We claim that (7) the success condition is also for the completely random model. Since K = kmax = d, there is no case in which k > kmax in this projection. Now, we offer a new proof for the last three steps for the completely random model. Step 4: Lower limits on the projection of the correct points (the LHS of (7)))) We use again Lemma B.5. For k > d / 2, we use the fact that the \"V > k xj.\""}, {"heading": "B.5 Proof of Lemma B.5", "text": "We construct a generative model for two random variables that are equal in the distribution: K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K"}, {"heading": "B.6 Proof of Lemma B.3a", "text": "Let be x one unit -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2 -k \u00b2"}, {"heading": "B.7 Proof of Lemma B.3b", "text": "For x-Unif (Sd-1), Pr-Ax-2 > m-Ax-2 + t), Pr-Ax-2 < m-Ax-2 < m-Ax-2 > m-Ax-2 + t, Pr-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < m-Ax-2 < Ax-2 < A< Ax-22 < Ax < Ax < Ax-22 < Ax < Ax-22 < Ax < Ax-22 < Ax < Ax-22 < Ax < Ax-22 < Ax-Ax < Ax-22 < Ax-Ax < Ax-22 < Ax-Ax-22 < Ax-Ax-22 < Ax-22 < Ax-Ax-Ax-22 < Ax-22 < Ax-Ax-22 < Ax-22 < Ax-Ax-Ax-Ax-22 < Ax-22 < Ax-Ax-22 < Ax-22 < Ax-Ax-22 < Ax-Ax-22 < Ax-Ax-Ax-22 < Ax-22 < Ax-22 < Ax-Ax-Ax-Ax-Ax-22 < Ax-22 < Ax-Ax-Ax-Ax-22 < Ax-Ax-22 < Ax-Ax-22 < A"}, {"heading": "B.8 Proof of Lemma B.4a", "text": "Let A = URV > the singular value of decomposition of A. Then we have E [EAX-2F] = E [X-2F] = E [EAX-2F] = E [EAX-2F] EAX-2F] EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F EAX-2F"}, {"heading": "B.9 Proof of Lemma B.4b", "text": "Consider the boot distributor Vk (Rd), which is equipped with the Euclidean metric. We see that X of Vk (Rd) is drawn using the normalized Harr probability measure. We have for any X, Y, Rd, K, A, F, A, A, A, A, A, A, A, A, A, A, F, F for any X, Y, Rd, K. Da \"A, 2, 1, AX, F is a 1-Lipschitz function of X. Then it follows [15, 12] that F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, T, 8, where m, AX, F, F, F, F is the mean of\" AX, F, F, F, F, \". In addition, we have Pr, AX, AX, AF, B, F, B, F, F, F, > t, \u2264 2e, (d \u2212 1) t2, and then it follows that\" A, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, A, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, A, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F,"}, {"heading": "B.10 Proof of Lemma B.6", "text": "For k \u2265 2 it follows [9, sentence 3.1] that Pr {\u03c72k \u2265 k (1 +)} \u2265 1 \u2212 e \u2212 22k (1 +) k + 2 \u221a k exp (\u2212 12 (k \u2212 (k \u2212 2) log (1 +) + log k)) \u2265 13 \u221a k + 6 exp (\u2212 k2 (\u2212 log (1 +))) \u2265 13 \u221a k + 6 exp (\u2212 k2). For k = 1 we can see numerically that the inequality holds."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>We consider the problem of subspace clustering: given points that lie on or near the union of<lb>many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies<lb>sets of points close to the same subspace and uses the sets to estimate the subspaces. As the<lb>geometric structure of the clusters (linear subspaces) forbids proper performance of general<lb>distance based approaches such as K-means, many model-specific methods have been proposed.<lb>In this paper, we provide new simple and efficient algorithms for this problem. Our statistical<lb>analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under<lb>certain conditions on the number of points and the affinity between subspaces. These conditions<lb>are weaker than those considered in the standard statistical literature. Experimental results on<lb>synthetic data generated from the standard unions of subspaces model demonstrate our theory.<lb>We also show that our algorithm performs competitively against state-of-the-art algorithms on<lb>real-world applications such as motion segmentation and face clustering, with much simpler<lb>implementation and lower computational cost.", "creator": "LaTeX with hyperref package"}}}