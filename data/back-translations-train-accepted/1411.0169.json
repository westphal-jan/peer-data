{"id": "1411.0169", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2014", "title": "Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms", "abstract": "Let $p$ be an unknown and arbitrary probability distribution over $[0,1)$. We consider the problem of {\\em density estimation}, in which a learning algorithm is given i.i.d. draws from $p$ and must (with high probability) output a hypothesis distribution that is close to $p$. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function.", "histories": [["v1", "Sat, 1 Nov 2014 21:03:59 GMT  (25kb)", "http://arxiv.org/abs/1411.0169v1", "conference version appears in NIPS 2014"]], "COMMENTS": "conference version appears in NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG cs.DS math.ST stat.TH", "authors": ["siu-on chan", "ilias diakonikolas", "rocco a servedio", "xiaorui sun"], "accepted": true, "id": "1411.0169"}, "pdf": {"name": "1411.0169.pdf", "metadata": {"source": "CRF", "title": "Near\u2013Optimal Density Estimation in Near\u2013Linear Time Using Variable\u2013Width Histograms", "authors": ["Siu-On Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "emails": ["sochan@gmail.com.", "ilias.d@ed.ac.uk.", "rocco@cs.columbia.edu.", "xiaoruisun@cs.columbia.edu."], "sections": [{"heading": null, "text": "ar Xiv: 141 1.01 69v1 [cs.LSpecifically, for each k and \u03b5 we output an algorithm that draws O (k / \u03b52) from p, expires in O (k / \u03b52) time, and produces a hypotheses distribution h, which is constant piece by piece with O (k log2 (1 / \u03b5) pieces. It is highly probable that the hypothesis h fulfills dTV (p, h) \u2264 C \u00b7 optk (p) + \u03b5, where dTV denotes the total variation distance (statistical distance), C a universal constant, and optk (p) the smallest total variation distance between p and any k-piecewise constant distribution. Sample size and runtime of our algorithm are optimal up to logarithmic factors. The \"approximation factor\" C in our result is inherent to the problem, since we prove that no algorithm with sample size limited in k can achieve C < whichever hypothesis it uses."}, {"heading": "1 Introduction", "text": "This is the question which arises primarily from the fact that it is a purely theoretical view which is based on a purely theoretical view, a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based on a purely theoretical view which is based"}, {"heading": "2 Preliminaries", "text": "Throughout the work, we assume that the underlying distributions have a non-necessarily negative function, which does not necessarily lead to a negative distribution.For a pdf p: [0, 1) and a measurable subset A (0, 1), i.e., A, L (0, 1), we use p (A) to name a certain number of p (A).The statistical distance or total difference between two densities p: [0, 1) \u2192 R + is dTV (p, q): = supA, L ([0,1) | p (A) \u2212 q (A).The statistical distance or total variation between two densities p: [0, q) = 2 [p \u2212 q], where a distribution of p \u2212 q [1], which is L1 distance between p and q, is a distance between p and q: [0,1)."}, {"heading": "3 The algorithm and its analysis", "text": "In this section, we will prove our most important algorithmic result, theorem 1. Our approach has the following high-level structure: In Section 3.1, we will specify an algorithm for agnostic learning of a target distribution p that is \"beautiful\" in two respects: (i) p is well behaved (i.e., it does not contain heavy atomic elements), and (ii) optk (p) is limited from above by the error parameter \u03b5. In Section 3.2, we will give a general efficient reduction that shows how to remove the second assumption, and in Section 3.3, we will briefly explain how to remove the first assumption, thus giving theorem 1."}, {"heading": "3.1 The main algorithm", "text": "In this section we specify our main algorithmic result, which deals with well-preserved distributions p, for which optk (p) is not too large: Theorem 4. There is an algorithm Learn-WB-small-opt-k-histogram, which results as input O (k / \u03b52) i.i.d. from a target distribution p and a parameter \u03b5 > 0, then, with a probability of at least 19 / 20, outputs an O (k \u00b7 log2 (1 / \u03b5) flat distribution h in such a way that dTV (p, h) \u2264 2 \u00b7 optk (p) + 3\u03b5.We need a notation and terminology. Let us leave r a distribution over [0, 1) and P a flat distribution h so that dTV (p, h) \u2264 2 \u00b7 optk (p) \u2264 2 \u00b7 optk (p) + 3\u03b5.I is a distribution between I and I."}, {"heading": "3.1.1 Intuition for the algorithm", "text": "It begins in step 1 by building a partition of [0, 1) in step 2, in which the algorithm draws a sample of O (k / \u03b52) points from p and uses it to define an empirical distribution p. This is the only step in which points are drawn from p. For the rest of this intuitive explanation, we pretend that the empirical distribution p. \"m is actually the same as the actual weight distribution p.\" (Lemma 3.1 below shows that this is not too far from the truth.) Before proceeding with our explanation of the algorithms, let us briefly consider the actual distribution p. \"m"}, {"heading": "3.1.2 The algorithm", "text": "Algorithm Learn-WB-small-opt-k-histogram: Input: Parameter k \u2265 1, \u03b5 > 0; Access to i.i.d. pulls from the target distribution p over [0, 1) Output: If (i) p is \u03b5 / log (1 / \u03b5) 384k -well-behaved and (ii) optk (p) \u2264 \u2212 \u2212 \u2212 p the output is a distribution q such that dTV (p, q) \u2264 2optk (p) + 3\u03b5.1. Let us leave \u03b5 = \u03b5 / log (1 / \u03b5).It runs algorithm approximate-equal partition on input parameters \u03b5 6k to partition [0, 1) \u2264 2optk (p). Intervals I1 = [i0, i1),., Iz = [iz-iz), wherei0 and iz = 1, so."}, {"heading": "3.1.3 Analysis of the algorithm and proof of Theorem 4", "text": "It is easy to verify the claimed term, which has given Lemma 2.1, which limits the running time of J-J. (J-J) It is indeed so that we find with high probability that the empirical distribution p-2 is defined and the resulting empirical distribution of all consecutive intervals of I1,.., Iz. The following problem from [CDSS14] follows with high probability from the multiplicative standard distribution p-1 (Lemma 12, [CDSS14]). with high probability 99 / 100 about the sample drawn in step 2, for every 0-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1"}, {"heading": "3.3 Dealing with distributions that are not well behaved", "text": "The assumption that the target distribution p is based on linear time samples and using O samples (k / \u03b5) can be easily eliminated by following the approach in Section 3.6 of [CDSS14]. This paper is a simple procedure based on a linear time sample, which is highly likely to identify all \"heavy\" elements (atoms that cause p if there are such points).Our overall algorithm first performs this procedure to find the set S of \"heavy\" elements, and then executes the above algorithm (which is successful for well-educated distributions, i.e. distributions that do not have \"heavy\" elements), performing the conditional distribution p over [0, 1)\\ S as the target distribution (let us denote this conditional distribution by p \u2032).An uncomplicated analysis given in [CDSS14] shows that (i) optk (p), (p) and beyond (ik), (K) distributions by T (K) and T (K) scales (T)."}, {"heading": "4 Lower bounds on agnostic learning", "text": "In this section we note that alpha-agnostic learning with Alpha < 2 is theoretically impossible, so Theorem 2.Fix any 0 < t < 1 / 2. We define a probability distribution that Dt via a finite series of discrete distributions over the domain [2N] = less than 1,., 2N} as follows: (We assume that without loss of generality the distribution is not rational and that tN is an integral distribution.) A tie of the pS1, S2, t of Dt is obtained as a result. 1. A sentence S1 [N] is randomly selected from all subsets of [N] that contain exactly tN elements, because i \u2264 [N], the distribution pS1, S2, t assigns probability weights as follows: pS1, S2, t (i) = 14N, pS1, S2, S2."}], "references": [{"title": "Near-optimal-sample estimators for spherical gaussian mixtures", "author": ["J. Acharya", "A. Jafarpour", "A. Orlitsky", "A.T. Suresh"], "venue": "Technical Report http://arxiv.org/abs/1402.4746,", "citeRegEx": "Acharya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2014}, {"title": "Statistical Inference under Order Restrictions", "author": ["R.E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H.D. Brunk"], "venue": null, "citeRegEx": "Barlow et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Barlow et al\\.", "year": 1972}, {"title": "Estimating a density under order restrictions: Nonasymptotic minimax risk", "author": ["L. Birg\u00e9"], "venue": "Annals of Statistics,", "citeRegEx": "Birg\u00e9.,? \\Q1987\\E", "shortCiteRegEx": "Birg\u00e9.", "year": 1987}, {"title": "Estimation of unimodal densities without smoothness assumptions", "author": ["L. Birg\u00e9"], "venue": "Annals of Statistics,", "citeRegEx": "Birg\u00e9.,? \\Q1997\\E", "shortCiteRegEx": "Birg\u00e9.", "year": 1997}, {"title": "On the estimation of parameters restricted by inequalities", "author": ["H.D. Brunk"], "venue": "Ann. Math. Statist.,", "citeRegEx": "Brunk.,? \\Q1958\\E", "shortCiteRegEx": "Brunk.", "year": 1958}, {"title": "Learning mixtures of structured distributions over discrete domains", "author": ["S. Chan", "I. Diakonikolas", "R. Servedio", "X. Sun"], "venue": "In SODA,", "citeRegEx": "Chan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2013}, {"title": "Efficient density estimation via piecewise polynomial approximation", "author": ["S. Chan", "I. Diakonikolas", "R. Servedio", "X. Sun"], "venue": "Technical Report http://arxiv.org/abs/1305.3207, conference version in STOC,", "citeRegEx": "Chan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2014}, {"title": "Random sampling for histogram construction: How much is enough", "author": ["S. Chaudhuri", "R. Motwani", "V. Narasayya"], "venue": "In SIGMOD Conference,", "citeRegEx": "Chaudhuri et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 1998}, {"title": "Inverse problems in approximate uniform generation", "author": ["A. De", "I. Diakonikolas", "R. Servedio"], "venue": "Available at http://arxiv.org/pdf/1211.1722v1.pdf,", "citeRegEx": "De et al\\.,? \\Q2012\\E", "shortCiteRegEx": "De et al\\.", "year": 2012}, {"title": "Nonparametric Density Estimation: The L1 View", "author": ["L. Devroye", "L. Gy\u00f6rfi"], "venue": null, "citeRegEx": "Devroye and Gy\u00f6rfi.,? \\Q1985\\E", "shortCiteRegEx": "Devroye and Gy\u00f6rfi.", "year": 1985}, {"title": "Faster and sample near-optimal algorithms for proper learning mixtures of gaussians", "author": ["C. Daskalakis", "G. Kamath"], "venue": "In COLT,", "citeRegEx": "Daskalakis and Kamath.,? \\Q2014\\E", "shortCiteRegEx": "Daskalakis and Kamath.", "year": 2014}, {"title": "Combinatorial methods in density estimation", "author": ["L. Devroye", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye and Lugosi.,? \\Q2001\\E", "shortCiteRegEx": "Devroye and Lugosi.", "year": 2001}, {"title": "Fast, small-space algorithms for approximate histogram maintenance", "author": ["A. Gilbert", "S. Guha", "P. Indyk", "Y. Kotidis", "S. Muthukrishnan", "M. Strauss"], "venue": "In STOC,", "citeRegEx": "Gilbert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gilbert et al\\.", "year": 2002}, {"title": "Approximation and streaming algorithms for histogram construction problems", "author": ["S. Guha", "N. Koudas", "K. Shim"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "Guha et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2006}, {"title": "On the theory of mortality", "author": ["U. Grenander"], "venue": "measurement. Skand. Aktuarietidskr.,", "citeRegEx": "Grenander.,? \\Q1956\\E", "shortCiteRegEx": "Grenander.", "year": 1956}, {"title": "Estimating a monotone density", "author": ["P. Groeneboom"], "venue": "In Proc. of the Berkeley Conference in Honor of Jerzy Neyman and Jack Kiefer,", "citeRegEx": "Groeneboom.,? \\Q1985\\E", "shortCiteRegEx": "Groeneboom.", "year": 1985}, {"title": "Consistency in concave regression", "author": ["D.L. Hanson", "G. Pledger"], "venue": "The Annals of Statistics,", "citeRegEx": "Hanson and Pledger.,? \\Q1976\\E", "shortCiteRegEx": "Hanson and Pledger.", "year": 1976}, {"title": "Approximating and Testing k-Histogram Distributions in Sub-linear Time", "author": ["P. Indyk", "R. Levi", "R. Rubinfeld"], "venue": "In PODS,", "citeRegEx": "Indyk et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 2012}, {"title": "Optimal histograms with quality guarantees", "author": ["H.V. Jagadish", "N. Koudas", "S. Muthukrishnan", "V. Poosala", "K. Sevcik", "T. Suel"], "venue": "In VLDB,", "citeRegEx": "Jagadish et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jagadish et al\\.", "year": 1998}, {"title": "On the learnability of discrete distributions", "author": ["M. Kearns", "Y. Mansour", "D. Ron", "R. Rubinfeld", "R. Schapire", "L. Sellie"], "venue": "In Proc. 26th STOC,", "citeRegEx": "Kearns et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1994}, {"title": "Estimation of a unimodal density", "author": ["B.L.S. Prakasa Rao"], "venue": "Sankhya Ser. A,", "citeRegEx": "Rao.,? \\Q1969\\E", "shortCiteRegEx": "Rao.", "year": 1969}, {"title": "Estimation of a function under shape restrictions", "author": ["L. Reboul"], "venue": "Applications to reliability. Ann. Statist.,", "citeRegEx": "Reboul.,? \\Q2005\\E", "shortCiteRegEx": "Reboul.", "year": 2005}, {"title": "Multivariate Density Estimation: Theory, Practice and Visualization", "author": ["D.W. Scott"], "venue": null, "citeRegEx": "Scott.,? \\Q1992\\E", "shortCiteRegEx": "Scott.", "year": 1992}, {"title": "Density Estimation", "author": ["B.W. Silverman"], "venue": null, "citeRegEx": "Silverman.,? \\Q1986\\E", "shortCiteRegEx": "Silverman.", "year": 1986}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "In Proc. 16th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "Maximum likelihood estimation of a unimodal density", "author": ["E.J. Wegman"], "venue": "I. and II. Ann. Math. Statist.,", "citeRegEx": "Wegman.,? \\Q1970\\E", "shortCiteRegEx": "Wegman.", "year": 1970}], "referenceMentions": [], "year": 2014, "abstractText": "Let p be an unknown and arbitrary probability distribution over [0, 1). We consider the problem of density estimation, in which a learning algorithm is given i.i.d. draws from p and must (with high probability) output a hypothesis distribution that is close to p. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function. In more detail, for any k and \u03b5, we give an algorithm that makes \u00d5(k/\u03b5) draws from p, runs in \u00d5(k/\u03b5) time, and outputs a hypothesis distribution h that is piecewise constant with O(k log(1/\u03b5)) pieces. With high probability the hypothesis h satisfies dTV(p, h) \u2264 C \u00b7optk(p)+\u03b5, where dTV denotes the total variation distance (statistical distance), C is a universal constant, and opt k (p) is the smallest total variation distance between p and any k-piecewise constant distribution. The sample size and running time of our algorithm are optimal up to logarithmic factors. The \u201capproximation factor\u201d C in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of k and \u03b5 can achieve C < 2 regardless of what kind of hypothesis distribution it uses.", "creator": "LaTeX with hyperref package"}}}