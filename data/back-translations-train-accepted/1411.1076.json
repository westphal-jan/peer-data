{"id": "1411.1076", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2014", "title": "A statistical model for tensor PCA", "abstract": "We consider the Principal Component Analysis problem for large tensors of arbitrary order $k$ under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio $\\beta$ becomes larger than $C\\sqrt{k\\log k}$ (and in particular $\\beta$ can remain bounded as the problem dimensions increase).", "histories": [["v1", "Tue, 4 Nov 2014 21:01:56 GMT  (102kb,D)", "http://arxiv.org/abs/1411.1076v1", "Neural Information Processing Systems (NIPS) 2014 (slightly expanded: 30 pages, 6 figures)"]], "COMMENTS": "Neural Information Processing Systems (NIPS) 2014 (slightly expanded: 30 pages, 6 figures)", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["emile richard", "andrea montanari"], "accepted": true, "id": "1411.1076"}, "pdf": {"name": "1411.1076.pdf", "metadata": {"source": "CRF", "title": "A statistical model for tensor PCA", "authors": ["Andrea Montanari"], "emails": [], "sections": [{"heading": null, "text": "We show that unless the signal-to-noise ratio deviates in the system dimensions, none of these approaches is successful, possibly due to a fundamental limitation of the mathematically tractable estimators for this problem. We discuss various initializations for tensor power iteration and show that tractable initialization based on the spectrum of the matrified tensor is statistically and computationally significantly better than basic methods. Finally, we consider the case where additional page information about the unknown signal is available. We characterize the amount of page information that allows the iterative algorithms to converge to a good estimate."}, {"heading": "1 Introduction", "text": "Given a data matrix X that is not necessarily suitable for scientific models, it can be considered a denoising process that replaces X by its narrowest ratio. (These optimization problems can be efficiently solved, and their statistical properties are well understood.) The generalization of PCA to tensors is motivated by problems where it is important to use higher alignments, or data elements are naturally given more than two indices. (Examples include topic modeling [AGH + 12], collaborative filtering in the presence of temporal / context-related information, community recognition [AGHK13], spectral hypergraphic theory, and hypergraphic matching [DBKP09]. Furthermore, looking for a ranking order to a tensor is a bottleneck for tensor-rated optimization algorithms that use conditional gradients of schemes. (While sor-factorization is not necessarily NP)."}, {"heading": "1.1 Notations", "text": "We are going to deal with the vectors (e.g. u, v, and so on). < v > < v > pr > pr > pr > pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr, pr"}, {"heading": "2 Ideal estimation", "text": "In this section we will consider the problem of estimation v0 from the point of view of randomness or optimization in this case. (...) Let us consider the problem of estimation v0 from the point of view of optimization, if no limitation is imposed on the complexity of the estimator. (...) Let us consider the problem of estimation v0 from the point of view of the point of view (...). (...) Let us (...) Allow us (...) that we consider the maximum estimate v2 from the point of view of the point of view v0 from the point of view of the point of view v0 from the point of view of the point of view v2 from the point of view of the point of view v2 from the point of view v2 from the point of view of the point of view v2 (...) Let us consider the maximum estimate v2 from the point of view v2 from the point of view v2 from the point of the point of view (...)."}, {"heading": "2.1 Historical Background", "text": "The random cost function v 7 \u2192 HZ (v) \u2261 < Z, v k > (defined on the unit sphere v Sn \u2212 1) has been studied in the context of statistical physics under the name \"spherical p-spin model,\" and its results have been rigorously confirmed by Talagrand [Tal06].The most noteworthy prediction from statistical physics is that the HZ (v) function has an exponential number of local maximums on the unit sphere [CS95].In addition, there is an approach of k < \u00b5k which states that the number of local maxims with the value HZ (v) has an exponential number of local maximums on the unit sphere [CS95]."}, {"heading": "3 Tensor Unfolding", "text": "A simple and popular heuristic to obtain tractable estimators of v0 is to construct a suitable matrix with the inputs of \u03b2 \u00b2 and perform a major component analysis on this matrix. Since the number of unique entries of X is of the order nk, the resulting matrix Matq (X) has the dimension \u044b (nq) \u00d7 \u044b (nk \u2212 q). This operation is variously referred to as matricization, unfolding, flattening. While the details of this construction may vary, we do not expect it to qualitatively affect our results, which we will summarize for convenience: 1. The best way to unfold X is to form a matrix as square as possible. 2. b = (dk / 2e \u2212 1) / 2 (especially b = 1 / 2 for k = 3, 4}), the unfolding approach is successful if size is greater than nb."}, {"heading": "3.1 Symmetric noise", "text": "It is natural that this approach will only be successful if the signal-noise ratio exceeds the value of the noise that the operator indicates. (It is). (It is). (It is). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It). (It. (It). (It). (It). (It). (It. (It). (It). (It). (It). (It. (It). (It). (It. (It). (It). (It. (It). (It). (It). (It. (It). (It. (It). (It). (It). (It. (It). (It). (It). (It. (It). (It. (It). (It. (It). (It.). (It. (It). (It). (It. (It. (It.). (It.). (It.). (It. (It. (It.). (It.). (It.). (It. (It.). (It.). (It. (It.). (It.). (It. (It.). (It. (.). (It. (.). (It.). (. (It.).).). (It."}, {"heading": "3.2 Asymmetric noise and recursive unfolding", "text": "A technical complication in the analysis of the random matrix Matq (X) lies in the fact that its inputs are not independent, since the noise tensor Z is assumed to be symmetrical. (i) We consider the case of a non-symmetrical noise ratio to be non-symmetrical. (ii) This allows us to refer to the known results in the random matrix theory (Pau07, FP09, BGN12). (ii) A lower limit to the loss below the critical signal-to-noise ratio. (ii) We consider the observationX to be uncertain. (i) We consider observationX to be unpredictable. (31) Where G-kRn is a standard Gaussian tensor (i.e.) A tensor with i.d. standard entries.).Let w = w (X-Rnk / 2 denote the upper right vector of Mat (X)."}, {"heading": "4 Power Iteration", "text": "Iteration of (multi-) linear maps induced by a (tensor) matrix is a standard method for finding leading eigenpairs, see [KM11] and references to tensor-related results. In this section, we will consider a simple potentiality iteration and then its possible applications in connection with the deployment of the tensor. Finally, we will compare our analysis with results available in the literature. Another iterative strategy is the Approximate Message Passing (AMP), which is discussed in Section 5. While the qualitative behavior is the same as with naive potentiality iteration, a sharper asymptotic analysis is possible for AMP."}, {"heading": "4.1 Naive power iteration", "text": "The simplest iterative approach is defined by the following recursive v0 = y (1), (1), (2), (2), (2), (3), (3), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5), (5, (5), (5,"}, {"heading": "4.2 Comparison with Tensor Unfolding", "text": "It is instructive to compare the results of the previous section with those for the development of the tensor, cf. section 3. In summary, the development of the tensor at standard Gaussian noise is guaranteed to be successful, provided \u03b2 & nb is b = (dk / 2e \u2212 1) / 2. We assume that a necessary and sufficient condition is actually \u03b2 & n (k \u2212 2) / 4 (e.g. \u03b2 & n1 / 4 for order 3 tensors). \u2022 The development of the tensor at random initialization requires \u03b2 & n (k \u2212 1) / 2. Our heuristic calculation suggests that a necessary and sufficient condition is actually \u03b2 & n (k \u2212 2) / 2 (e.g. \u03b2 & n1 / 2 for order 3 tensors). In other words, the development of the tensor is successful under a signal-to-noise ratio that is in the order of magnitude smaller than the iteration of the current."}, {"heading": "4.3 Related work", "text": "As mentioned above, power siteration is a natural approach to tensor factorization and has been studied in several previous papers. Recently, the interest in machine learning was piqued by [AGH + 12]. Our theorem 6 corresponds to the main result of [AGH + 12], although it is incomparable: \u2022 In [AGH + 12] it is assumed that the \"signal part\" of tensor X exhibits orthogonal decomposition. \u2022 In [AGH + 12] only the case of third-order tensors (k = 3) is considered. We characterize power siteration for the general k. \u2022 We determine convergence in a number of iterations independent of dimensions. In [AGH + 12] the number of iterations is limited by a polynomial."}, {"heading": "5 Asymptotics via Approximate Message Passing", "text": "This is not the first time that we have embarked on a \"state evolution\" in a country where we can identify as \"state evolution.\" Here, we are developing an AMP algorithm for tensor data, and its state evolution analysis focuses on the fixed \u03b2, n. \"Proofs follows the approach of [BM11] and is published in a journal where our AMP PCA can be considered a sophisticated version of the power iteration method of the last section. Using the notation f (x) = x / x 2, we define the AMP iteration over vectors v0."}, {"heading": "6 Numerical experiments", "text": "Let us highlight two practical suggestions arising from our work: \u2022 The deployment of the tensor is superior to the iteration of the tensor power within our sting model. \u2022 For smaller values of \u03b2, we expect that iteration of the tensor power requires \u03b2 & n1 / 4 and the unfolding \u03b2 & n1 / 2. \u2022 For smaller values of \u03b2, iterative methods (the iteration of the tensor power or approximate message delivery) can only provide a good estimate if the initialization has a scalar product with the basic truth v0, which is limited from zero. \u2022 As a consequence of the above, page information about the unknown vector v0 can significantly improve performance. In this particular case, we will investigate the behavior of warm-start algorithms that first perform a singular value replacement of Mat (X) and then apply an iterative method (the iteration of the tensor approximate message delivery)."}, {"heading": "6.1 PSD-constrained principal component", "text": "Note that the outer product v'v (considered as n \u00b7 n matrix) is positively semi-defined (PSD) for v \u00b2 Rn. Considering the case k = 3, we have Mat (X) = \u03b2vec (v0 v0) v0T + Mat (Z). (55) This remark proposes to perform a main cone component analysis of Mat (X), in which the left singular vector (viewed as a matrix) belongs to the PDS cone. To formalize this, it is convenient to redesign the operator \u00b7 n: Rn2 \u2192 Rn \u00b7 n, which matrices vectors as newly shaped \u00d7 n (w) i = wn (i \u2212 1) + j. The PSD-cone-confined main component of Mat (X) is defined by (w \u00b2, v \u00b2) rigorously max {< Tw.X (this problem is emptively designed > MiD)."}, {"heading": "6.2 Comparison of different algorithms", "text": "Figure 1 compares different algorithms on data generated using the spiked tensor model with k = 3 and n \u00b2 {25, 50, 100, 200, 400, 800} and for a value range of \u03b2 \u00b2 [2, 10]. The diagrams represent absolute correlation measurements | < v \u00b2, v0 > | versus \u03b2, averaged over 50 samples (with the exception of n = 800, in which we used 8 samples). The main results agree with the theory described above: \u2022 Tensor potentialization (with random initialization) performs poorly compared to other approaches that use any form of tensor deployment. The gap widens when the dimension n increases. \u2022 The PSD-limited main component analysis (described in the last section) is slightly superior to simple deployment. \u2022 All algorithms based on initial deployment have essentially the same threshold."}, {"heading": "6.3 The value of side information", "text": "Our next experiment concerns a simultaneous matrix and tensor-PCA task: We obtain a tensor X-3Rn of the spiked tensor model with k = 3 and the signal-to-noise ratio \u03b2 = 3. In addition, we observe M = \u03bbv0v0T + N, where N-Rn \u00b7 n varies a symmetrical noise matrix with the upper diagonal elements i < j iid Ni, j-N (0, 1 / n) and the value of \u03bb [0, 2]. This experiment mimics a Ranking 1 version of the theme modeling method presented in [AGH + 12], where M is a matrix representing paired co-events and X triplexes. Analysis in earlier sections suggests using the leading eigenvector of M as the starting point for AMP algorithm for PCA on X. We performed the experiments on 100 randomly generated instances with 200, 50 AMP algorithm, and 50 AMP algorithm, and 3 MP algorithm."}, {"heading": "Acknowledgements", "text": "& # 8222; & # 8220; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10;"}, {"heading": "B Maximum likelihood: Proof Theorem 2", "text": "It is not that we have the number of local maximums of HZ (\u00b7) over Sn \u2212 1, which value is greater or equal to x. The next lemma of the growth rate of local minima.Theorem 8 (ABAC13). The next lemma of the number of local maximums of HZ (\u00b7) over Sn \u2212 1, which value is greater or equal to x. The next lemma of the growth rate of local minima.Theorem 8 (ABAC13) over Sn \u2212 1, which value is greater or equal to x. The next lemma of the growth rate of local minima.Theorem 8 (ABAC13) over Sn \u2212 1, which we have value."}, {"heading": "C Power Iteration: Proof of Theorem 6", "text": "It is true for t = 0 on assumption. Then it is true for t + 1 using Eq. (101) We will prove the first inequality by induction. (102) We will prove the first inequality by induction. (99) We assume that the first inequality is caused by induction. (99) We will prove the first inequality by induction. (99) We assume that the first inequality is caused by induction. (99) We assume that the first inequality is caused by induction. (99) We assume that the second inequality is caused by induction. (99) We assume that the first inequality is caused by induction. (99) We assume that the second inequality is caused by an inequality. (99) We assume that the first inequality is caused by an inequality. (99) We assume that the second inequality is caused by an inequality. (99) We assume that the first inequality is caused by an inequality."}, {"heading": "D Approximate Message Passing: Proof of Theorem 7", "text": "Let us recall the recursion of state development (53) \u03c42t + 1 = f (\u03c4 2 t; \u03b2), (109) f (z; \u03b2) \u0445 \u03b22 (z1 + z) k \u2212 1. (110) Note that f (\u00b7; \u03b2) is strictly positive and monotonous increasing to R > 0. The theorem follows by proving that the following claims apply to \u03b2 > \u03c9k1. The fixed point equation \u03c42 = f (\u03c42; \u03b2) has two strictly positive solutions \u03c421 (\u03b2) < \u03c4 2 (\u03b2). 2. The smallest fixed point is by \u03c41 (\u03b2) = 1 / k (\u03b2) \u2212 1 as in statement.3. The largest fixed point satisfies \u03c42 (\u03b2) > 1 \u2212 (2 / \u03b22).The behavior of the function f (\u03c42; \u03b2) is in Figure.5.To prove the above statements, it is convenient to use monotonametrization."}], "references": [{"title": "Random matrices and complexity of spin glasses", "author": ["A. Auffinger", "G. Ben Arous", "J. Cerny"], "venue": "Communications on Pure and Applied Mathematics", "citeRegEx": "Auffinger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auffinger et al\\.", "year": 2013}, {"title": "A tensor spectral approach to learning mixed membership community", "author": ["Anima Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade"], "venue": null, "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "The singular values and vectors of low rank perturbations of large rectangular random matrices", "author": ["Florent Benaych-Georges", "Raj Rao Nadakuditi"], "venue": "Journal of Multivariate Analysis", "citeRegEx": "Benaych.Georges and Nadakuditi,? \\Q2012\\E", "shortCiteRegEx": "Benaych.Georges and Nadakuditi", "year": 2012}, {"title": "The dynamics of message passing on dense graphs, with applications to compressed sensing", "author": ["M. Bayati", "A. Montanari"], "venue": "IEEE Trans. on Inform. Theory", "citeRegEx": "Bayati and Montanari,? \\Q2011\\E", "shortCiteRegEx": "Bayati and Montanari", "year": 2011}, {"title": "Spectral Analysis of Large Dimensional Random Matrices (2nd edition)", "author": ["Z. Bai", "J. Silverstein"], "venue": null, "citeRegEx": "Bai and Silverstein,? \\Q2010\\E", "shortCiteRegEx": "Bai and Silverstein", "year": 2010}, {"title": "Exact matrix completion via convex optimization, Foundations of Computational mathematics", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": null, "citeRegEx": "Cand\u00e8s and Recht,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht", "year": 2009}, {"title": "The spherical p-spin interaction spin glass model: the statics", "author": ["Andrea Crisanti", "H-J Sommers"], "venue": "Zeitschrift fu\u0308r Physik B Condensed Matter", "citeRegEx": "Crisanti and Sommers,? \\Q1992\\E", "shortCiteRegEx": "Crisanti and Sommers", "year": 1992}, {"title": "Thouless-Anderson-Palmer approach to the spherical p-spin spin glass model", "author": ["A Crisanti", "H-J Sommers"], "venue": "Journal de Physique I", "citeRegEx": "Crisanti and Sommers,? \\Q1995\\E", "shortCiteRegEx": "Crisanti and Sommers", "year": 1995}, {"title": "The Dantzig selector: Statistical estimation when p is much larger than n, The Annals of Statistics", "author": ["E. Candes", "T. Tao"], "venue": null, "citeRegEx": "Candes and Tao,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao", "year": 2007}, {"title": "Elements of information", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas", "year": 2012}, {"title": "A tensor-based algorithm for high-order graph matching", "author": ["O. Duchenne", "F. Bach", "I. Kweon", "J. Ponce"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Duchenne et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchenne et al\\.", "year": 2009}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via `1 minimization", "author": ["D.L. Donoho", "M. Elad"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Donoho and Elad,? \\Q2003\\E", "shortCiteRegEx": "Donoho and Elad", "year": 2003}, {"title": "Message Passing Algorithms for Compressed Sensing", "author": ["D.L. Donoho", "A. Maleki", "A. Montanari"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Donoho et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Donoho et al\\.", "year": 2009}, {"title": "Cone-constrained principal component analysis", "author": ["Y. Deshpande", "A. Montanari", "E. Richard"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Deshpande et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2014}, {"title": "Local operator theory, random matrices and Banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "Handbook on the Geometry of Banach spaces,", "citeRegEx": "Davidson and Szarek,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek", "year": 2001}, {"title": "Local operator theory, random matrices and banach spaces, Handbook of the geometry of Banach spaces", "author": ["Kenneth R Davidson", "Stanislaw J Szarek"], "venue": null, "citeRegEx": "Davidson and Szarek,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek", "year": 2001}, {"title": "P\u00e9ch\u00e9, The largest eigenvalues of sample covariance matrices for a spiked population: diagonal case", "author": ["S.D. F\u00e9ral"], "venue": "Journal of Mathematical Physics", "citeRegEx": "F\u00e9ral,? \\Q2009\\E", "shortCiteRegEx": "F\u00e9ral", "year": 2009}, {"title": "Neural reconstruction with approximate message passing (neuramp)", "author": ["A.K. Fletcher", "S. Rangan", "L.R. Varshney", "A. Bhargava"], "venue": null, "citeRegEx": "Fletcher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fletcher et al\\.", "year": 2011}, {"title": "A limit theorem for the norm of random matrices", "author": ["S. Geman"], "venue": "Annals of Probability", "citeRegEx": "Geman,? \\Q1980\\E", "shortCiteRegEx": "Geman", "year": 1980}, {"title": "Most tensor problems are np-hard", "author": ["Christopher J Hillar", "Lek-Heng Lim"], "venue": "Journal of the ACM (JACM)", "citeRegEx": "Hillar and Lim,? \\Q2013\\E", "shortCiteRegEx": "Hillar and Lim", "year": 2013}, {"title": "Generalizing the fano inequality", "author": ["Te Han", "Sergio Verdu"], "venue": "Information Theory, IEEE Transactions on", "citeRegEx": "Han and Verdu,? \\Q1994\\E", "shortCiteRegEx": "Han and Verdu", "year": 1994}, {"title": "On consistency and sparsity for principal components analysis in high dimensions", "author": ["I. M Johnstone", "A.Y. Lu"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Johnstone and Lu,? \\Q2009\\E", "shortCiteRegEx": "Johnstone and Lu", "year": 2009}, {"title": "Shifted power method for computing tensor eigenpairs", "author": ["Tamara G Kolda", "Jackson R Mayo"], "venue": "SIAM Journal on Matrix Analysis and Applications", "citeRegEx": "Kolda and Mayo,? \\Q2011\\E", "shortCiteRegEx": "Kolda and Mayo", "year": 2011}, {"title": "Approximate message passing with consistent parameter estimation and applications to sparse learning", "author": ["U. Kamilov", "S. Rangan", "A.K. Fletcher", "M. Unser"], "venue": null, "citeRegEx": "Kamilov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kamilov et al\\.", "year": 2012}, {"title": "The concentration of measure phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs,", "citeRegEx": "Ledoux,? \\Q2001\\E", "shortCiteRegEx": "Ledoux", "year": 2001}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Square deal: Lower bounds and improved relaxations for tensor recovery", "author": ["C. Mu", "J. Huang", "B. Wright", "D. Goldfarb"], "venue": "International Conference in Machine Learning (ICML),", "citeRegEx": "Mu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mu et al\\.", "year": 2013}, {"title": "Non-negative principal component analysis: Message passing algorithms and sharp asymptotics", "author": ["Andrea Montanari", "Emile Richard"], "venue": null, "citeRegEx": "Montanari and Richard,? \\Q2014\\E", "shortCiteRegEx": "Montanari and Richard", "year": 2014}, {"title": "Asymptotics of sample eigenstructure for a large dimensional spiked covariance model", "author": ["Debashis Paul"], "venue": "Statistica Sinica", "citeRegEx": "Paul,? \\Q2007\\E", "shortCiteRegEx": "Paul", "year": 2007}, {"title": "A new convex relaxation for tensor completion", "author": ["B. Romera-Paredes", "M. Pontil"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Romera.Paredes and Pontil,? \\Q2013\\E", "shortCiteRegEx": "Romera.Paredes and Pontil", "year": 2013}, {"title": "Approximate message passing for bilinear models", "author": ["P. Schniter", "V. Cevher"], "venue": "Proc. Workshop Signal Process. Adaptive Sparse Struct. Repr.(SPARS),", "citeRegEx": "Schniter and Cevher,? \\Q2011\\E", "shortCiteRegEx": "Schniter and Cevher", "year": 2011}, {"title": "Compressive phase retrieval via generalized approximate message passing, Communication, Control, and Computing (Allerton), 2012", "author": ["P. Schniter", "S. Rangan"], "venue": "50th Annual Allerton Conference on,", "citeRegEx": "Schniter and Rangan,? \\Q2012\\E", "shortCiteRegEx": "Schniter and Rangan", "year": 2012}, {"title": "Free energy of the spherical mean field model, Probability theory and related fields", "author": ["Michel Talagrand"], "venue": null, "citeRegEx": "Talagrand,? \\Q2006\\E", "shortCiteRegEx": "Talagrand", "year": 2006}, {"title": "Greed is good: Algorithmic results for sparse approximation, Information Theory", "author": ["J. A Tropp"], "venue": "IEEE Transactions on", "citeRegEx": "Tropp,? \\Q2004\\E", "shortCiteRegEx": "Tropp", "year": 2004}, {"title": "Statistical performance of convex tensor decomposition", "author": ["R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Tomioka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tomioka et al\\.", "year": 2011}, {"title": "Some comparisons for gaussian processes", "author": ["R.A. Vitale"], "venue": "Proceedings of the American Mathematical Society", "citeRegEx": "Vitale,? \\Q2000\\E", "shortCiteRegEx": "Vitale", "year": 2000}, {"title": "The absolute-value estimate for symmetric multilinear forms, Linear Algebra and its Applications", "author": ["W.C. Waterhouse"], "venue": null, "citeRegEx": "Waterhouse,? \\Q1990\\E", "shortCiteRegEx": "Waterhouse", "year": 1990}, {"title": "Perturbation bounds in connection with singular value decomposition, BIT Numerical Mathematics", "author": ["P.A. Wedin"], "venue": null, "citeRegEx": "Wedin,? \\Q1972\\E", "shortCiteRegEx": "Wedin", "year": 1972}], "referenceMentions": [], "year": 2014, "abstractText": "We consider the Principal Component Analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio \u03b2 becomes larger than C \u221a k log k (and in particular \u03b2 can remain bounded as the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate.", "creator": "LaTeX with hyperref package"}}}