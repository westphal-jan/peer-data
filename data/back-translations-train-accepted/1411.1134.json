{"id": "1411.1134", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems", "abstract": "The Burer-Monteiro decomposition ($X = Y Y^T$) with stochastic gradient descent is commonly employed to speed up and scale up matrix problems including matrix completion, subspace tracking, and SDP relaxation. Although it is widely used in practice, there exist no known global convergence results for this method. In this paper, we prove that, under broad sampling conditions, a first-order rank-1 stochastic gradient descent (SGD) matrix recovery scheme converges globally from a random starting point at a $O(\\epsilon^{-1} n \\log n)$ rate with constant probability. We demonstrate our method experimentally.", "histories": [["v1", "Wed, 5 Nov 2014 03:05:43 GMT  (48kb)", "https://arxiv.org/abs/1411.1134v1", null], ["v2", "Thu, 6 Nov 2014 03:10:29 GMT  (48kb)", "http://arxiv.org/abs/1411.1134v2", null], ["v3", "Tue, 10 Feb 2015 20:19:28 GMT  (67kb)", "http://arxiv.org/abs/1411.1134v3", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["christopher de sa", "christopher r\u00e9", "kunle olukotun"], "accepted": true, "id": "1411.1134"}, "pdf": {"name": "1411.1134.pdf", "metadata": {"source": "CRF", "title": "Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems", "authors": ["Christopher De Sa", "Kunle Olukotun"], "emails": ["cdesa@stanford.edu,", "kunle@stanford.edu,", "chrismre@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.11 34v3 [cs.LG] 1 0"}, {"heading": "1 Introduction", "text": "Y, Y, Y, Y, Y, Y, Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y,\" Y, \"Y, Y,\" Y, Y, Y, \"Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y,\" Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y,"}, {"heading": "1.1 Related Work", "text": "In fact, most of them are able to determine for themselves what they want and what they want to do."}, {"heading": "2 Algorithmic Derivation", "text": "We focus on the lower ranks in which we use example. (2) We can use objective asE (Y) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) -1 (D) (D) -1 (D) -2 (D) -1 (D) -1 (D) -1)."}, {"heading": "3 Convergence Analysis", "text": "D (D). D (D). D (D). D (D). D (D). D (D). D (D). D (D). D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). ("}, {"heading": "3.1 Martingale Technique", "text": "A proof of Theorem 1 and complete formal definitions will appear in Appendix C of this document (R + constant), but since the method is not by default for non-convex optimization (although it was used in Shamir [34] to show convergence for convex problems, we will outline it here. First, we define an error event fk in any timeframe that occurs when the iteration gets \"too close\" to the unstable fixed points. Next, we define a sequence \u03c4k = failure Y Tk in any timeframe. Y Tk (n \u2212 1p \u2212 2qI + (1 \u2212 2 \u2212 p \u2212 2q) U) Yk in any timeframe in which we define the determinant of X; the intuition here is that it is close to 1 when and only when success occurs, and close to 0 when we fail. We show that neither success nor failure occurs."}, {"heading": "4 Application Examples", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Entrywise Sampling", "text": "A sample distribution that occurs in many applications (most important is matrix completion [12]) is the initial sample. This happens when the samples are selected independently of the entries of A. Specifically, it is standard for these types of problems to introduce a matrix coherence that is bound [25]. Definition 4. A matrix A-Rn-n is incoherent with the parameter \u00b5 if and only if for each unit eigenvector ui of the matrix and for all standard base vectors ej, as well as eTj-n-12. Under an incoherence assumption, we can provide a limit for the second moment of A-Rn-n, which is all we need to apply theorem 1 to this problem."}, {"heading": "4.2 Rectangular Entrywise Sampling", "text": "To solve this problem with Alecton, we first turn it into a symmetrical matrix problem by constructing the block matrixA = [0 M MT 0]; it is known that restoring the dominant eigenvectors of A equals restoring the dominant singular vectors of M. Helios, scanning M corresponds to selecting a random i-1,..., m and j-1,..., n, and then restoring the dominant eigenvectors of A = mnMij (eie T m + j + em + each T i). In the case where we can bind the inputs of M (this is, of course, for recommended systems), we can prove the following parameters. Lemma 3. If M-Rm \u00b7 n does not meet the problems above the entry time, we meet the entry values of M \u2212 2M \u2212 2M."}, {"heading": "4.3 Trace Sampling", "text": "Another frequent sample distribution results from the matrix sensitivity problem [25]. In this problem, we obtain the value of vTAw for the underlying vectors v and w, which were randomly selected. (This problem was solved for the more general complex case in [11] using Wirtinger flow.) With the help of a trace sample, we can construct an unbiased sample in Rn, then for each positive semidefinitive sample A, if we allow A = n2vTAwwT, then the distribution of A meets the alectonic variance condition with the parameters \u03c32a = 16, and v and w are uniformly sampled from the unity sphere in Rn, then for each positive semidefinitive sample A, if we allow A = n2vTAwwT, then we will select the parameters \u03c32a = 16, and v and w equally from the unity class in Rn, then for each positive sample we will allow our semidefinite sample we will continue to have TWWW = 2T problems, then we will allow TWW = 2T problems, then we will allow the parameters \u03c32a = 16, and v and v and w uniformly from the unity class in Rn, then for each positive sample we will allow TWW = TWW = 2T problems, then we will allow TWW = 2T problems."}, {"heading": "4.4 Subspace Sampling", "text": "Our analysis can handle more complicated sampling schemes. Consider the following distribution, which results in subspace tracking [6]. Our matrix A is a series-r projection matrix, and each sample consists of randomly selected entries from a randomly selected vector in its column space. Specifically, we get Qv and Rv, where v is a randomly selected vector of C (A), and Q and R are independent random diagonal projection matrices with an expected value mn \u2212 1I. With this, we can create the distributionA = rn2m \u2212 2QvTR. This distribution is unbiased because E [qvT] = A. If we limit their second moment, we encounter the same coherence problem as in the central case, which motivates us to introduce a coherence constraint for subspaces."}, {"heading": "4.5 Noisy Sampling", "text": "Using the additive property of variance for independent random variables, we can show that additive noise only increases the variance of the sample distribution by a constant amount proportional to the variance of noise. Similarly, multiplicative noise multiplies the variance of the sample distribution by only a constant factor proportional to the variance of noise. In both cases, we can show that the noise sample distribution meets AVC."}, {"heading": "4.6 Extension to Higher Ranks", "text": "It is possible to use multiple iterations of the rank-1 version of Alecton to restore additional eigenvalue / eigenvector pairs of data matrix A one at a time. This is a standard technique for using power-iteration algorithms to restore multiple eigenvalues. Sometimes, this is preferable to using a single higher-ranking call from Alecton (for example, we may not know from the outset how many eigenvectors we want). We outline this technique as an algorithm 2. This strategy allows us to produce the largest p-eigenvectors of A using p-executations. yi \u2192 Alecton one-at-a-time request: A-sampling distribution AA1 \u2192 A for i = 1 to p do run rank-1 Alecton to J-J J-J J J-J-J J-J-J J-J-J-J J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-"}, {"heading": "5 Experiments", "text": "All experiments were performed on a machine with a single twelve-core socket (Intel Xeon E5-2697, 2.70GHz), and 256 GB of shared memory. All experiments were written in C + +, except for the Netflix Prize problem experiment written in Julia. No data was collected for the radial phase of Alecton, since the performance of averaging is already significantly undervalued. The first experiments were performed with randomly generated rank-10 data matrices A-Rn-n. Each experiment was generated by selecting a random orthogonal matrix U-Rn-n, since the performance of averaging is already significantly undervalued. The first experiments were generated with randomly generated rank-10 data matrices A-Rn-n-n-n-data matrices, which illustrate the maximum data distribution of Alecton-n-n-n-n, then independently calculate a diagonal matrix time with 10 positive non-eigenvalues n n, and cn-n construct n-n-n with n-light n-UVP-n = n n (we expect n to calculate n-value n from the UVP = 2)."}, {"heading": "5.1 Discussion", "text": "The Hogwild! algorithm [30] is a parallel, latch-free version of the stochastic gradient descend that has been shown to work similarly to sequential SGD for convex problems, while providing good parallel acceleration. It is unclear whether a Hogwild! version of Alecton converges at a good rate for non-convex problems, but we are optimistic that it will."}, {"heading": "6 Conclusion", "text": "This paper showed Alecton, a stochastic gradient descent algorithm applied to a low-level non-convex factorized problem; it is similar to the algorithms used in practice to solve a variety of problems. We demonstrate that Alecton converges globally and provides a convergence rate. We do not require a specific initialization step, but rather initialize randomly. Moreover, our results depend only on the variance of the samples and are therefore subject to broad sample conditions that include both matrix completion and matrix sensors, and are also capable of considering noisy samples. We show these results with a martyrdom-based technique that is new in the field of nonconvex optimization, and we are optimistic that this technique can be applied to other problems in the future."}, {"heading": "A Negative Results", "text": "We will not do any of this if we opt for an initial condition that this iteration becomes infinity.Proposition 1. Assuming that we apply the above rule for which we do not decrease overexponentially; that is, for some C > 1 and some C > 2k for all. Then, if x20 (C + 1), for all kx2k (C + 1), for all kx2k (C + 1), for all kx2k (C + 1).Proof.We will then make these assumptions if x20 (C + 1), for all kx2k (C + 1), for all kx2k (C + 1).Proof.We will make these assumptions if x20 (C + 1), for all kk + 2k (C + 1), for all kx1 (C + 1)."}, {"heading": "B Comparison with Other Methods", "text": "There are several other algorithms that solve similar matrix recovery problems in literature. In Table B, we list some other algorithms and their convergence rates, both in terms of the number of samples required (sample complexity) and the number of iterations performed (computational complexity); for this table, it is assumed that the data has the dimension n, and the rank (where applicable) is assumed to be p. (To save space, some formulas omit log recording factors (Log-1).)"}, {"heading": "C Proofs of Main Results", "text": "In this appendix we will look at the filtration we do for each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat, each fat we put in Fs, and each fat, each fat we put in Ft, and each fat we put in Ft, and each fat we put in Ft, and each fat we put in Ft, and each fat we put in Ft, and each fat we put in Ft, and each fat we put in Ft, and each fat we put in Ft, and each fat we put in Ft, we put in Ft, we put in Ft, we put in Ft, we put in Ft, we put in Ft, we put in Ft, we put in Ft, we put in Ft, a filtration."}, {"heading": "D Proofs of Lemmas", "text": "First, we are the only ones who are able to show the general results. (1 + 2 \u2212 B) + (2a \u2212 B) + (2a \u2212 B) + (2ab \u2212 B) + (2ab \u2212 B) + (2ab \u2212 B) + (2ab \u2212 B) + (2ab \u2212 B) + (a \u2212 B) x + (c \u2212 C + (2a \u2212 B) b) x2 + (a2 \u2212 B) x3 \u2212 c2x4 = (2ab \u2212 B) x2 \u2212 C2ax + (a \u2212 b) cx3 \u2212 c2x4 = (a \u2212 b) 2 \u2212 mmx2 \u2212 (a2 \u2212 b2 \u2212 b2) x2 \u2212 (a \u2212 b) cx3 \u2212 c2x4 = 1 + 2ax + 2ax (a \u2212 b) c2x2 \u2212 (a \u2212 b) c2x2 \u2212 (a)."}, {"heading": "As a corollary, for any symmetric matrix W 0,", "text": "vTQWQv \u2264 (\u00b5mr + m2) n \u2212 2tr (W) n = 2tr (W) n = 2tr (V) vWA = 1.Proof. Let us be 1 in the case that egg is in the slot space of Q, and 0 otherwise. Then a self-decomposition of Q isQ = n \u00b2 i = 1\u03bbieie T i. Therefore (xTQv) 2 = (n \u00b2 T eie T i v) 2 = n \u00b2 i = 1n \u00b2 j = 1s \u00b2 i.jxivivj.Take the expected value and determine that they are independent, and the expected value E [2 \u00b2 T] = mn \u2212 1, E \u2212 (xTQv) 2 \u2212 2 m \u00b2 i.Da v \u2212 y.- y.n \u2212 y.n = 1n \u00b2 i.TRvj = 1 (1 \u2212 mn \u00b2) i.TQvj + mn \u2212 1 (1 \u2212 mn Qv2) (1 \u2212 m \u00b2 \u2212 v \u2212 iv \u00b2"}, {"heading": "E Lower Bound on Alecton Rate", "text": "In this section, we show a rough lower limit of the convergence rate of an Alecton-like algorithm for limited sampling distributions. Specifically, we analyze the case where, instead of choosing a constant sampling distribution rate, we let the step size vary at each sampling distribution step. Our result shows that we cannot hope for a better sampling size rule that improves the convergence rate of Alecton to, for example, a linear rate. To show this lower limit, we assume that we run Alecton with p = 1 for any sampling distribution such that for all y, for any constant C, for any constant C, and a constant C, for a constant C, and a constant C, for a constant C, and a constant C, for a constant C, and another C, for a constant C, for a constant C."}, {"heading": "F Handling Constraints", "text": "Alecton can be easily adapted to solve the problem of finding a slight approximation to a matrix under a spectacular limitation. That is, we want to minimize the problem by minimizing the number of resolved problems by working them under X-RN-N-RN-RN-RN-RN-RN-RN-RN-RN-RN-RN-2F-RN-RN-RN-2F-RN-2F-RN-RN-2F-RN-RN-2F-RN-2F-RN-2F-RN-RN-RN-2F-RN-RN-2F-RN-RN-1 RN-RN-1 RN-2F-RN-RN-1 RN-1 RN-1 RN-2F-RN-2N-RN-1 RN-1 RN-1 RN-1 RN-2F RN-2F RN-1 RN-1 RN-1 RN-1 RN-2F-2F RN-1 RN-1 RN-1"}, {"heading": "G Towards a Linear Rate", "text": "In this section, we will consider a specific case of the matrix recovery problem: one in which the samples given to us would allow us to accurately regenerate. That is, for some linear operators (Rn \u00b7 n \u2192 Rs, we will get the value of \"A\" as input, and we know that the unique solution to optimization problems (X \u2212 A) is the results of \"A\" (X \u2212 A) and \"T\" (X \u2212 R) the results of \"X \u2212 P\" and \"X \u2212 P\" as input-p square substitution of this problem in: minimize \"Y\" (Y \u2212 A) and \"V\" (V \u2212 A) the results of \"A.\" The specific case we are looking for is where the operator uses the p-RIP-RIP-Constraint.Definition 8 (Restricted isometry property). A linear operator that satisfies Rn \u00b7 n \u2192 Rs with constant."}], "references": [{"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "A reliable effective terascale linear learning system", "author": ["Alekh Agarwal", "Olivier Chapelle", "Miroslav Dud\u0131\u0301k", "John Langford"], "venue": "CoRR, abs/1110.4198,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Stochastic optimization for pca and pls", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Stochastic optimization of pca with capped msg", "author": ["Raman Arora", "Andy Cotter", "Nati Srebro"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "The fast convergence of incremental pca", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Online identification and tracking of subspaces from highly incomplete information", "author": ["Laura Balzano", "Robert Nowak", "Benjamin Recht"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["Lon Bottou"], "venue": "In Proceedings of COMP- STAT\u20192010,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "The tradeoffs of large scale learning", "author": ["Lon Bottou", "Olivier Bousquet"], "venue": "In IN: ADVANCES IN NEU- RAL INFORMATION PROCESSING SYSTEMS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "author": ["Samuel Burer", "Renato DC Monteiro"], "venue": "Mathematical Programming,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Local minima and convergence in low-rank semidefinite programming", "author": ["Samuel Burer", "Renato DC Monteiro"], "venue": "Mathematical Programming,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel Cand\u00e8s", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "arXiv preprint arXiv:1407.1065,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J. Cand\u00e8s", "Benjamin Recht"], "venue": "FoCM, 9(6):717\u2013772,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Solving quadratic equations via phaselift when there are about as many equations as unknowns. FoCM", "author": ["EmmanuelJ. Cand\u00e8s", "Xiaodong Li"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Matrix completion via an alternating direction method", "author": ["Caihua Chen", "Bingsheng He", "Xiaoming Yuan"], "venue": "IMAJNA,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Counting processes and survival analysis", "author": ["Thomas R Fleming", "David P Harrington"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Michel X. Goemans", "David P. Williamson"], "venue": "J. ACM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Wtf: The who to follow service at twitter", "author": ["Pankaj Gupta", "Ashish Goel", "Jimmy Lin", "Aneesh Sharma", "Dong Wang", "Reza Zadeh"], "venue": "WWW \u201913,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "The noisy power method: A meta algorithm with applications", "author": ["Moritz Hardt", "Eric Price"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Design and performance of parallel and distributed approximation algorithms for maxcut", "author": ["Steven Homer", "Marcus Peinado"], "venue": "J. Parallel Distrib. Comput.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Solving ptychography with a convex relaxation", "author": ["R. Horstmeyer", "R.Y. Chen", "X. Ou", "B. Ames", "J.A. Tropp", "C. Yang"], "venue": "ArXiv e-prints,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Accelerated gradient methods for stochastic optimization and online learning", "author": ["Chonghai Hu", "James T. Kwok", "Weike Pan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the Forty-fifth Annual ACM STOC,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Robust stochastic principal component analysis", "author": ["Raman Arora John Goes", "Teng Zhang", "Gilad Lerman"], "venue": "In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Low-rank optimization on the cone of positive semidefinite matrices", "author": ["M. Journ\u00e9e", "F. Bach", "P.-A. Absil", "R. Sepulchre"], "venue": "SIAM J. on Optimization,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "Sewoong Oh"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Low-rank optimization with trace norm penalty", "author": ["Bamdev Mishra", "Gilles Meyer", "Francis Bach", "Rodolphe Sepulchre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Feng Niu", "Benjamin Recht", "Christopher R\u00e9", "Stephen J. Wright"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["Erkki Oja"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1985}, {"title": "Summingbird: A framework for integrating batch and online mapreduce computations", "author": ["Sam Ritchie"], "venue": "In Proceedings of the VLDB Endowment,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Parallel stochastic gradient algorithms for large-scale matrix completion", "author": ["Benjamin Recht", "Christopher R\u00e9"], "venue": "Mathematical Programming Computation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Ohad Shamir"], "venue": "CoRR, abs/1109.5647,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "A stochastic PCA algorithm with an exponential convergence rate", "author": ["Ohad Shamir"], "venue": "CoRR, abs/1409.2848,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Distributed matrix completion", "author": ["Christina Teflioudi", "Faraz Makari", "Rainer Gemulla"], "venue": "IEEE 13th ICDM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "edu Departments of Electrical Engineering and Computer Science Stanford University, Stanford, CA 94309 February 11, 2015 Abstract Stochastic gradient descent (SGD) on a low-rank factorization [9] is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation.", "startOffset": 192, "endOffset": 195}, {"referenceID": 13, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 149, "endOffset": 161}, {"referenceID": 22, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 149, "endOffset": 161}, {"referenceID": 33, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 149, "endOffset": 161}, {"referenceID": 5, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 209, "endOffset": 212}, {"referenceID": 2, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 243, "endOffset": 246}, {"referenceID": 9, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 261, "endOffset": 277}, {"referenceID": 20, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 261, "endOffset": 277}, {"referenceID": 24, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 261, "endOffset": 277}, {"referenceID": 26, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 261, "endOffset": 277}, {"referenceID": 17, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 306, "endOffset": 314}, {"referenceID": 29, "context": "This problem, or problems that can be transformed to this problem, appears in a variety of machine learning applications including matrix completion [14, 25, 36], general data analysis [37], subspace tracking [6], principle component analysis [3], optimization [10, 23, 27, 29], and recommendation systems [20, 32].", "startOffset": 306, "endOffset": 314}, {"referenceID": 8, "context": "Sometimes, (1) arises under conditions in which the samples \u00c3 are sparse, but the matrix X would be too large to store and operate on efficiently; a standard heuristic to use in this case is a low-rank factorization [9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 1, "context": "Efficient SGD implementations can scale to very large datasets [2, 7, 8, 16, 24, 30, 33, 36].", "startOffset": 63, "endOffset": 92}, {"referenceID": 6, "context": "Efficient SGD implementations can scale to very large datasets [2, 7, 8, 16, 24, 30, 33, 36].", "startOffset": 63, "endOffset": 92}, {"referenceID": 7, "context": "Efficient SGD implementations can scale to very large datasets [2, 7, 8, 16, 24, 30, 33, 36].", "startOffset": 63, "endOffset": 92}, {"referenceID": 14, "context": "Efficient SGD implementations can scale to very large datasets [2, 7, 8, 16, 24, 30, 33, 36].", "startOffset": 63, "endOffset": 92}, {"referenceID": 21, "context": "Efficient SGD implementations can scale to very large datasets [2, 7, 8, 16, 24, 30, 33, 36].", "startOffset": 63, "endOffset": 92}, {"referenceID": 27, "context": "Efficient SGD implementations can scale to very large datasets [2, 7, 8, 16, 24, 30, 33, 36].", "startOffset": 63, "endOffset": 92}, {"referenceID": 30, "context": "Efficient SGD implementations can scale to very large datasets [2, 7, 8, 16, 24, 30, 33, 36].", "startOffset": 63, "endOffset": 92}, {"referenceID": 33, "context": "Efficient SGD implementations can scale to very large datasets [2, 7, 8, 16, 24, 30, 33, 36].", "startOffset": 63, "endOffset": 92}, {"referenceID": 24, "context": "People have attempted to compensate for this with sophisticated methods like geodesic step rules [27] and manifold projections [1]; however, even these methods cannot guarantee global convergence.", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "People have attempted to compensate for this with sophisticated methods like geodesic step rules [27] and manifold projections [1]; however, even these methods cannot guarantee global convergence.", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "We make the following contributions: \u2022 We establish the convergence rate to a global optimum of Alecton using a random initialization; in contrast, prior analyses [11, 25] have required more expensive initialization methods, such as the singular value decomposition of an empirical average of the data.", "startOffset": 163, "endOffset": 171}, {"referenceID": 22, "context": "We make the following contributions: \u2022 We establish the convergence rate to a global optimum of Alecton using a random initialization; in contrast, prior analyses [11, 25] have required more expensive initialization methods, such as the singular value decomposition of an empirical average of the data.", "startOffset": 163, "endOffset": 171}, {"referenceID": 18, "context": "\u2022 In contrast to previous work that uses bounds on the magnitude of the noise [21], our analysis depends only on the variance of the samples.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "As a result, we are able to be robust to different noise models, and we apply our technique to these problems, which did not previously have global convergence rates: \u2013 matrix completion, in which we observe entries of A one at a time [25, 28] (Section 4.", "startOffset": 235, "endOffset": 243}, {"referenceID": 25, "context": "As a result, we are able to be robust to different noise models, and we apply our technique to these problems, which did not previously have global convergence rates: \u2013 matrix completion, in which we observe entries of A one at a time [25, 28] (Section 4.", "startOffset": 235, "endOffset": 243}, {"referenceID": 10, "context": "1), \u2013 phase retrieval, in which we observe tr(uAv) for randomly selected u, v [11, 13] (Section 4.", "startOffset": 78, "endOffset": 86}, {"referenceID": 12, "context": "1), \u2013 phase retrieval, in which we observe tr(uAv) for randomly selected u, v [11, 13] (Section 4.", "startOffset": 78, "endOffset": 86}, {"referenceID": 5, "context": "3), and \u2013 subspace tracking, in which A is a projection matrix and we observe random entries of a random vector in its column space [6] (Section 4.", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "Foundational work in this space was done by Burer and Monteiro [9, 10], who analyzed the low-rank factorization of general semidefinite programs.", "startOffset": 63, "endOffset": 70}, {"referenceID": 9, "context": "Foundational work in this space was done by Burer and Monteiro [9, 10], who analyzed the low-rank factorization of general semidefinite programs.", "startOffset": 63, "endOffset": 70}, {"referenceID": 24, "context": "[27] exhibits a second-order algorithm that converges to a local solution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "These approaches have attempted to correct for falling off the manifold using Riemannian retractions [27], geodesic steps [6], or projections back onto the manifold.", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "These approaches have attempted to correct for falling off the manifold using Riemannian retractions [27], geodesic steps [6], or projections back onto the manifold.", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "General non-convex manifold optimization techniques [1] tell us that first-order methods, such as SGD, will converge to a fixed point, but they provide no convergence rate to the global optimum.", "startOffset": 52, "endOffset": 55}, {"referenceID": 22, "context": "[25] study matrix completion and provides a convergence rate for an exact recovery algorithm, alternating minimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] provide a similar result for phase retrieval.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "A related class of algorithms that are similar to Alecton is stochastic power iteration [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "Stochastic power iteration has been applied to a wide variety of problems [3, 26].", "startOffset": 74, "endOffset": 81}, {"referenceID": 23, "context": "Stochastic power iteration has been applied to a wide variety of problems [3, 26].", "startOffset": 74, "endOffset": 81}, {"referenceID": 28, "context": "Oja [31] show convergence of this algorithm, but provides no rate.", "startOffset": 4, "endOffset": 8}, {"referenceID": 3, "context": "[4] analyze this problem, and state that \u201cobtaining a theoretical understanding of the stochastic power method, or of how the step size should be set, has proved elusive.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "Shamir [35] provide exponential-rate local convergence results for a stochastic power iteration algorithm for PCA.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "[5] and Hardt and Price [21] provide a global convergence rate for the stochastic power iteration algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[5] and Hardt and Price [21] provide a global convergence rate for the stochastic power iteration algorithm.", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "Previous work has used manifold optimization techniques to solve such symmetric problems [27].", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "[1] state that stochastic gradient descent on a manifold has the general form xk+1 = xk \u2212 \u03b1kG xk \u2207f\u0303k(xk), where Gx is the matrix such that for all u and v, uGxv = \u3008u, v\u3009x, 3", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], this manifold has induced Riemannian metric \u3008U, V \u3009Y = tr ( UY Y V T ) .", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "We then use the optional stopping theorem [17] to bound both the probability and rate of convergence of xk, from which we derive convergence of the original algorithm.", "startOffset": 42, "endOffset": 46}, {"referenceID": 31, "context": "1 Martingale Technique A proof for Theorem 1 and full formal definitions will appear in Appendix C of this document, but since the method is nonstandard for non-convex optimization (although it has been used in Shamir [34] to show convergence for convex problems), we will outline it here.", "startOffset": 218, "endOffset": 222}, {"referenceID": 15, "context": "We show that, if neither success nor failure occurs at time k, E [\u03c4k+1|Fk] \u2265 \u03c4k (1 +R (1\u2212 \u03c4k)) (6) for some constant R; here, Fk denotes the filtration at time k, which contains all the events that have occurred up to time k [17].", "startOffset": 225, "endOffset": 229}, {"referenceID": 15, "context": "We use the optional stopping Theorem [17] (here we state a discrete-time version).", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "1 Entrywise Sampling One sampling distribution that arises in many applications (most importantly, matrix completion [12]) is entrywise sampling.", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "It is standard for these types of problems to introduce a matrix coherence bound [25].", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "3 Trace Sampling Another common sampling distribution arises from the matrix sensing problem [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "(This problem has been handled for the more general complex case in [11] using Wirtinger flow.", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "Consider the following distribution, which arises in subspace tracking [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 27, "context": "1 Discussion The Hogwild! algorithm [30] is a parallel, lock-free version of stochastic gradient descent that has been shown to perform similarly to sequential SGD on convex problems, while allowing for a good parallel speedup.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "References [1] P.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Alekh Agarwal, Olivier Chapelle, Miroslav Dud\u0131\u0301k, and John Langford.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Raman Arora, Andy Cotter, and Nati Srebro.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Laura Balzano, Robert Nowak, and Benjamin Recht.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Lon Bottou.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Lon Bottou and Olivier Bousquet.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Samuel Burer and Renato DC Monteiro.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Samuel Burer and Renato DC Monteiro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Emmanuel Cand\u00e8s, Xiaodong Li, and Mahdi Soltanolkotabi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Emmanuel J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] EmmanuelJ.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Caihua Chen, Bingsheng He, and Xiaoming Yuan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] John Duchi, Elad Hazan, and Yoram Singer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Thomas R Fleming and David P Harrington.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Michel X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Zadeh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Moritz Hardt and Eric Price.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] Steven Homer and Marcus Peinado.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] Chonghai Hu, James T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] Raman Arora John Goes, Teng Zhang and Gilad Lerman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] Bamdev Mishra, Gilles Meyer, Francis Bach, and Rodolphe Sepulchre.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] Feng Niu, Benjamin Recht, Christopher R\u00e9, and Stephen J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] Erkki Oja.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[32] Ian O\u2019Connell Jimmy Lin Oscar Boykin, Sam Ritchie.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[33] Benjamin Recht and Christopher R\u00e9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[34] Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[36] Christina Teflioudi, Faraz Makari, and Rainer Gemulla.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Algorithm Sampling Scheme Complexity Sampling Computational Alecton Any O(\u01eb\u22121p3n log n) SVD Various o(pn) O(n3) Spectral Matrix Completion [28] Elementwise o(pn) O(p2n log n) PhaseLift [13] Phase Retrieval o(n) O(\u01eb\u22121n3) Alternating Minimization [41] Phase Retrieval o(n log(\u01eb\u22121)) O(n2 log2(\u01eb\u22121)) Wirtinger Flow [11] Phase Retrieval o(n log n) O(pn log(\u01eb\u22121)) Equivalently, if we let A denote the edge-matrix of the graph, we can represent this as a matrix problem [19, 22] minimize yAy subject to yi \u2208 {\u22121, 1}.", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "Algorithm Sampling Scheme Complexity Sampling Computational Alecton Any O(\u01eb\u22121p3n log n) SVD Various o(pn) O(n3) Spectral Matrix Completion [28] Elementwise o(pn) O(p2n log n) PhaseLift [13] Phase Retrieval o(n) O(\u01eb\u22121n3) Alternating Minimization [41] Phase Retrieval o(n log(\u01eb\u22121)) O(n2 log2(\u01eb\u22121)) Wirtinger Flow [11] Phase Retrieval o(n log n) O(pn log(\u01eb\u22121)) Equivalently, if we let A denote the edge-matrix of the graph, we can represent this as a matrix problem [19, 22] minimize yAy subject to yi \u2208 {\u22121, 1}.", "startOffset": 185, "endOffset": 189}, {"referenceID": 10, "context": "Algorithm Sampling Scheme Complexity Sampling Computational Alecton Any O(\u01eb\u22121p3n log n) SVD Various o(pn) O(n3) Spectral Matrix Completion [28] Elementwise o(pn) O(p2n log n) PhaseLift [13] Phase Retrieval o(n) O(\u01eb\u22121n3) Alternating Minimization [41] Phase Retrieval o(n log(\u01eb\u22121)) O(n2 log2(\u01eb\u22121)) Wirtinger Flow [11] Phase Retrieval o(n log n) O(pn log(\u01eb\u22121)) Equivalently, if we let A denote the edge-matrix of the graph, we can represent this as a matrix problem [19, 22] minimize yAy subject to yi \u2208 {\u22121, 1}.", "startOffset": 311, "endOffset": 315}, {"referenceID": 16, "context": "Algorithm Sampling Scheme Complexity Sampling Computational Alecton Any O(\u01eb\u22121p3n log n) SVD Various o(pn) O(n3) Spectral Matrix Completion [28] Elementwise o(pn) O(p2n log n) PhaseLift [13] Phase Retrieval o(n) O(\u01eb\u22121n3) Alternating Minimization [41] Phase Retrieval o(n log(\u01eb\u22121)) O(n2 log2(\u01eb\u22121)) Wirtinger Flow [11] Phase Retrieval o(n log n) O(pn log(\u01eb\u22121)) Equivalently, if we let A denote the edge-matrix of the graph, we can represent this as a matrix problem [19, 22] minimize yAy subject to yi \u2208 {\u22121, 1}.", "startOffset": 463, "endOffset": 471}, {"referenceID": 19, "context": "Algorithm Sampling Scheme Complexity Sampling Computational Alecton Any O(\u01eb\u22121p3n log n) SVD Various o(pn) O(n3) Spectral Matrix Completion [28] Elementwise o(pn) O(p2n log n) PhaseLift [13] Phase Retrieval o(n) O(\u01eb\u22121n3) Alternating Minimization [41] Phase Retrieval o(n log(\u01eb\u22121)) O(n2 log2(\u01eb\u22121)) Wirtinger Flow [11] Phase Retrieval o(n log n) O(pn log(\u01eb\u22121)) Equivalently, if we let A denote the edge-matrix of the graph, we can represent this as a matrix problem [19, 22] minimize yAy subject to yi \u2208 {\u22121, 1}.", "startOffset": 463, "endOffset": 471}, {"referenceID": 15, "context": "1 Definitions Fleming and Harrington [17] provide the following definitions of filtration and martingale.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "\u2223 M +W [ 0 1 1 1 ]", "startOffset": 7, "endOffset": 18}, {"referenceID": 0, "context": "\u2223 M +W [ 0 1 1 1 ]", "startOffset": 7, "endOffset": 18}, {"referenceID": 0, "context": "\u2223 M +W [ 0 1 1 1 ]", "startOffset": 7, "endOffset": 18}, {"referenceID": 0, "context": "[ 0 1 1 1 ]\u22121 = [ \u22121 1 1 0 ]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[ 0 1 1 1 ]\u22121 = [ \u22121 1 1 0 ]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[ 0 1 1 1 ]\u22121 = [ \u22121 1 1 0 ]", "startOffset": 0, "endOffset": 11}, {"referenceID": 10, "context": "Secondary Literature [11] Emmanuel Cand\u00e8s, Xiaodong Li, and Mahdi Soltanolkotabi.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "[13] EmmanuelJ.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] R.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Abstract Stochastic gradient descent (SGD) on a low-rank factorization [9] is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within O(\u01ebn logn) steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show experiments to illustrate the runtime and convergence of the algorithm.", "creator": "gnuplot 4.6 patchlevel 4"}}}