{"id": "1411.1147", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction", "abstract": "We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observable data using a feature-rich conditional random field. Then a reconstruction of the input is (re)generated, conditional on the latent structure, using models for which maximum likelihood estimation has a closed-form. Our autoencoder formulation enables efficient learning without making unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. We show competitive results with instantiations of the model for two canonical NLP tasks: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.", "histories": [["v1", "Wed, 5 Nov 2014 04:49:38 GMT  (557kb,D)", "http://arxiv.org/abs/1411.1147v1", null], ["v2", "Mon, 10 Nov 2014 05:58:04 GMT  (556kb,D)", "http://arxiv.org/abs/1411.1147v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["waleed ammar", "chris dyer", "noah a smith"], "accepted": true, "id": "1411.1147"}, "pdf": {"name": "1411.1147.pdf", "metadata": {"source": "CRF", "title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction", "authors": ["Waleed Ammar", "Chris Dyer", "Noah A. Smith"], "emails": ["wammar@cs.cmu.edu", "cdyer@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it will be able to bring to life the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated braintecsrteeeFnllrrrrrrrrrtee\u00fce."}, {"heading": "2 Conditional Random Field Autoencoder", "text": "Considering a number of observations (e.g. sentences or pairs of sentences that are translatively equivalent), we consider the problem of creating a hidden structure. Examples of hidden structures include shallow syntactic properties (part of speech or POS tags), correspondences between words in translation (word alignment), syntactic parsing and morphological analysis. Let us leave each observation x = < x1,. Read each observation as a tuple whose length is also determined by discrete values. Let us assume that the hidden variables y y = < y1,. The hidden variables y y y = < y1,."}, {"heading": "2.1 Learning & Inference", "text": "Model parameters are selected to maximize the regularized probability of reconstructed observations. (\"(\u03bb, \u03b8) = R1 (\u03bb) + R2 (\u03b8) + \u2211 (x, x) = T log\" y \"(y | x) \u00b7 p\u03b8 (x, x) \u00b7 Origin of block coordinates, alternately in relation to the CRF parameters (\u03bb step) and the reconstruction parameters (\u03b8 step). Each \u03bb step applies one or two iterations of a gradient-based convex optimization. (5) The \u03b8 step applies one or two iterations of the EM [10], with a closed solution in the M step in each EM iteration. The assumptions of independence under y make the marginal date indication in both steps straightforward."}, {"heading": "3 Connections To Previous Work", "text": "This work relates to several strands of unsupervised learning. Unsupervised learning with flexible attribute representations has been studied for a long time, and there are by and large two types of models that we have experimented with AdaGrad [12] and L-BFGS. When using AdaGrad, we accumulate the gradient vectors via block coordinate ascension siterations.6In POS induction, | Y | is a constant, the number of syntactic classes that we configure to be 12 in our experiments.In word alignment, | Y | is the size of the source set plus one, so | Y | max is the maximum length of a source set in the text corpus.Support this. Both are fully generative models that define common distributions via x and y. We will refer to them as \"undirected\" and \"directed alternatives, which we will discuss next and then turn to less closely related methods."}, {"heading": "3.1 Existing Alternatives to Learning with Features", "text": "The directed alternative uses an undirected model to encode distribution by local potential functions parameterized using attributes. Such models \"normalize globally\" and require, during the training, the calculation of a partition function that summarizes all the values of both (in our notation). The key difficulty lies in summarizing all possible observations. Approximations have been proposed, including contrasting estimates, sums of subsets of X [38, 43] (applied to POS learning by Haghighi and Klein [18] and word matching by Dyer et al. [14] and contradictory estimation models [30]. The directed alternative avoids the global partition function by factoring the localized probabilities of repair."}, {"heading": "3.2 Other Related Work", "text": "The aforementioned suggestions have not been followed by the aforementioned institutions, so they will be able to play by the rules."}, {"heading": "4 Evaluation", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "5 Conclusion", "text": "We have presented a general and scalable framework for unattended learning of latent structures, enabling features with global scope in observation variables with advantageous asymptotic inference runtime. We achieve this by embedding a CRF as a coding model in the input layer of an autoencoder and by using categorical distributions to reconstruct the observations in the output layer. A key advantage of the proposed model is scalability, since inference is not costly.9We only compare runtime rather than alignment quality, because retraining the MRF model with exact inference was too costly.10When comparing the runtimes of the CRF autoencoder and the HMM function of Berg-Kirkpatrick et al. [3], which informed us in personal communication about a right trick, we did not observe a similar pattern when we avoided the runtimes of the CRF autoencoder and the cost MM trainings of Berg-Kirkpatrick et algorithm backwards. [3]"}, {"heading": "Acknowledgments", "text": "We thank Brendan O'Connor, Jeffrey Flanigan, Dani Yogatama, Nathan Schneider, Manaal Faruqui and the anonymous reviewers for their helpful suggestions. We thank Taylor Berg-Kirkpatrick for providing his implementation of POS induction and Phil Blunsom for providing POS evaluation scripts. This work was sponsored by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract / funding number W911NF-10-1-0533."}, {"heading": "A Experimental Details", "text": "In this section, we describe our experimental model for part of speech induction and bitext verbatim alignment in some areas. \u2022 Whether there is a classic NLP problem that aims to detect syntactic classes of tokens in a monolingual corpus, with a predefined number of classes. \u2212 An example of a POS-tagged English sentence is in Figure 1.Data. We use the plaintext of CoNLL-X [7] and CoNLL 2007 [31] training data in seven languages to train the models: Arabic, Basque, Greek, Italian and Turkey.11 For evaluation, we obtain gold standard POS tags by mapping the language-specific POS tags from the common task training data to the corresponding universal POS sets."}], "references": [{"title": "Unsupervised learning of multiple motifs in biopolymers using expectation maximization", "author": ["T.L. Bailey", "C. Elkan"], "venue": "Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Alternating projections for learning with expectation constraints", "author": ["K. Bellare", "G. Druck", "A. McCallum"], "venue": "In Proc. of UAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Painless unsupervised learning with features", "author": ["T. Berg-Kirkpatrick", "A. Bouchard-C\u00f4t\u00e9", "J. DeNero", "D. Klein"], "venue": "In Proc. of NAACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Discriminative word alignment with conditional random fields", "author": ["P. Blunsom", "T. Cohn"], "venue": "In Proc. of Proceedings of ACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "In Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["S. Buchholz", "E. Marsi"], "venue": "In CoNLL-X,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proc. of ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Unsupervised search-based structured prediction", "author": ["H. Daum\u00e9 III"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal statistical Society,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1977}, {"title": "High-performance semi-supervised learning using discriminatively constrained generative models", "author": ["G. Druck", "A. McCallum"], "venue": "In Proc. of ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and contextfree translation models", "author": ["C. Dyer", "A. Lopez", "J. Ganitkevitch", "J. Weese", "F. Ture", "P. Blunsom", "H. Setiawan", "V. Eidelman", "P. Resnik"], "venue": "In Proc. of ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Unsupervised word alignment with arbitrary features", "author": ["C. Dyer", "J. Clark", "A. Lavie", "N.A. Smith"], "venue": "In Proc. of ACL-HLT,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A simple, fast, and effective reparameterization of IBM Model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "In Proc. of NAACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Gra\u00e7a", "J. Gillenwater", "B. Taskar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Parallel implementations of word alignment tool", "author": ["Q. Gao", "S. Vogel"], "venue": "In In Proc. of the ACL workshop,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Prototype-driven learning for sequence models", "author": ["A. Haghighi", "D. Klein"], "venue": "In Proc. of NAACL- HLT,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Statistical Methods for Speech Recognition", "author": ["F. Jelinek"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["M. Johnson"], "venue": "In Proc. of EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Corpus-based induction of syntactic structure: Models of dependency and constituency", "author": ["D. Klein", "C.D. Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Statistical Machine Translation", "author": ["P. Koehn"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proc. of NAACL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proc. of ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "In Thesis,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "The cmu submission for the shared task on language identification in code-switched data", "author": ["C.-C. Lin", "W. Ammar", "C. Dyer", "L. Levin"], "venue": "In First Workshop on Computational Approaches to Code Switching at EMNLP,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Genemark. hmm: new solutions for gene finding", "author": ["A.V. Lukashin", "M. Borodovsky"], "venue": "Nucleic acids research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Tagging english text with a probabilistic model", "author": ["B. Merialdo"], "venue": "In Comp. Ling.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1994}, {"title": "Discriminative models, not discriminative training", "author": ["T. Minka"], "venue": "Technical report, Technical Report MSR-TR-2005-144, Microsoft Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "In Proc. of ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "The CoNLL 2007 shared task on dependency parsing", "author": ["J. Nivre", "J. Hall", "S. Kubler", "R. McDonald", "J. Nilsson", "S. Riedel", "D. Yuret"], "venue": "In Proc. of CoNLL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In Proc. of ACL,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2002}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "In Proc. of LREC,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Joint unsupervised coreference resolution with Markov logic", "author": ["H. Poon", "P. Domingos"], "venue": "In Proc. of EMNLP,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Substring-based transliteration with conditional random fields", "author": ["S. Reddy", "S. Waxmonsky"], "venue": "In Proc. of the Named Entities Workshop,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["N.A. Smith", "J. Eisner"], "venue": "In Proc. of ACL,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In NIPS workshop,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure", "author": ["V. Stoyanov", "A. Ropson", "J. Eisner"], "venue": "In Proc. of AISTATS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data", "author": ["J. Suzuki", "H. Isozaki"], "venue": "In Proc. of ACL,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Unsupervised semantic role labelling", "author": ["R. Swier", "S. Stevenson"], "venue": "In Proc. of EMNLP,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2004}, {"title": "Non-local contrastive objectives", "author": ["D. Vickrey", "C.C. Lin", "D. Koller"], "venue": "In Proc. of ICML,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In Proc. of ICML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Hmm-based word alignment in statistical translation", "author": ["S. Vogel", "H. Ney", "C. Tillmann"], "venue": "In Proc. of COLING,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1996}, {"title": "Unsupervised learning of models for recognition", "author": ["M. Weber", "M. Welling", "P. Perona"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2000}], "referenceMentions": [{"referenceID": 23, "context": "Conditional random fields [24] are used to model structure in numerous problem domains, including natural language processing (NLP), computational biology, and computer vision.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "Third, it is easy to simultaneously learn from labeled and unlabeled examples in this architecture, as we did in [26].", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 148, "endOffset": 158}, {"referenceID": 44, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 148, "endOffset": 158}, {"referenceID": 3, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 148, "endOffset": 158}, {"referenceID": 27, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 172, "endOffset": 180}, {"referenceID": 19, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 172, "endOffset": 180}, {"referenceID": 18, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 200, "endOffset": 204}, {"referenceID": 26, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 219, "endOffset": 223}, {"referenceID": 35, "context": "This class of latent structures is a popular choice for modeling a variety of problems such as human action recognition [47], bitext word alignment [6, 45, 4], POS tagging [28, 20], acoustic modeling [19], gene finding [27], and transliteration [36], among others.", "startOffset": 245, "endOffset": 249}, {"referenceID": 37, "context": "For example, morphology, word spelling information, and other linguistic knowledge encoded as features were shown to improve POS induction [38], word alignment [14], and other unsupervised learning problems.", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "For example, morphology, word spelling information, and other linguistic knowledge encoded as features were shown to improve POS induction [38], word alignment [14], and other unsupervised learning problems.", "startOffset": 160, "endOffset": 164}, {"referenceID": 4, "context": "For POS tagging, we use Brown clusters [5]; other alternatives might introduce other helpful forms of bias (e.", "startOffset": 39, "endOffset": 42}, {"referenceID": 20, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 95, "endOffset": 99}, {"referenceID": 41, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 122, "endOffset": 126}, {"referenceID": 34, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 187, "endOffset": 190}, {"referenceID": 45, "context": "For example, instantiations of this model can be used for unsupervised learning of parse trees [21], semantic role labels [42], and coreference resolution [35] (in NLP), motif structures [1] in computational biology, and objects [46] in computer vision.", "startOffset": 229, "endOffset": 233}, {"referenceID": 9, "context": "5 The \u03b8-step applies one or two iterations of EM [10], with a closed-form solution in the M-step in each EM iteration.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "We experimented with AdaGrad [12] and L-BFGS.", "startOffset": 29, "endOffset": 33}, {"referenceID": 37, "context": "Approximations have been proposed, including contrastive estimation, which sums over subsets of X \u2217 [38, 43] (applied variously to POS learning by Haghighi and Klein [18] and word alignment by Dyer et al.", "startOffset": 100, "endOffset": 108}, {"referenceID": 42, "context": "Approximations have been proposed, including contrastive estimation, which sums over subsets of X \u2217 [38, 43] (applied variously to POS learning by Haghighi and Klein [18] and word alignment by Dyer et al.", "startOffset": 100, "endOffset": 108}, {"referenceID": 17, "context": "Approximations have been proposed, including contrastive estimation, which sums over subsets of X \u2217 [38, 43] (applied variously to POS learning by Haghighi and Klein [18] and word alignment by Dyer et al.", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "[14]) and noise contrastive estimation [30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[14]) and noise contrastive estimation [30].", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In directed models [3], each iteration requires time", "startOffset": 19, "endOffset": 22}, {"referenceID": 36, "context": "6 Standard HMM Featurized HMM CRF autoencoder Figure 3: V-Measure [37] of induced parts of speech in seven languages.", "startOffset": 66, "endOffset": 70}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "The goal of neural autoencoders is to learn feature representations that improve generalization in otherwise supervised learning problems [44, 8, 39].", "startOffset": 138, "endOffset": 149}, {"referenceID": 7, "context": "The goal of neural autoencoders is to learn feature representations that improve generalization in otherwise supervised learning problems [44, 8, 39].", "startOffset": 138, "endOffset": 149}, {"referenceID": 38, "context": "The goal of neural autoencoders is to learn feature representations that improve generalization in otherwise supervised learning problems [44, 8, 39].", "startOffset": 138, "endOffset": 149}, {"referenceID": 39, "context": "[40] presented a related approach for discriminative graphical model learning, including features and latent variables, based on backpropagation, which could be used to instantiate the CRF autoencoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Daum\u00e9 III [9] introduced a reduction of an unsupervised problem instance to a series of singlevariable supervised classification; the first series of these construct a latent structure y given the entire x, then the second series reconstruct the input again using only y.", "startOffset": 10, "endOffset": 13}, {"referenceID": 37, "context": "On unsupervised POS induction, this approach performed on par with the undirected model of [38].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "Minka [29] proposed cascading a generative model and a discriminative model, where class labels (to be predicted at test time) are marginalized out in the generative part first, and then (re)generated in the discriminative part.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "[16], posterior regularization imposes constraints on the learned model\u2019s posterior; a similar idea was proposed independently [2].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[16], posterior regularization imposes constraints on the learned model\u2019s posterior; a similar idea was proposed independently [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "In a semi-supervised setting, when some labeled examples of the hidden structure are available, Druck and McCallum [11] used labeled examples to estimate desirable expected values.", "startOffset": 115, "endOffset": 119}, {"referenceID": 40, "context": "We leave semi-supervised applications of CRF autoencoders to future work; see also Suzuki and Isozaki [41].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "[3] and a standard (feature-less) first-order HMM, using the V-Measure evaluation metric [37] (higher is better).", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "[3] and a standard (feature-less) first-order HMM, using the V-Measure evaluation metric [37] (higher is better).", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "1) which uses the manyto-one evaluation metric [20], albeit the difference in performance between CRF autoencoders and featurized HMMs, on average, is much smaller.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "First, we consider an intrinsic evaluation on a Czech-English dataset of manual alignments, measuring the alignment error rate (AER; [32]).", "startOffset": 133, "endOffset": 137}, {"referenceID": 32, "context": "We also perform an extrinsic evaluation of translation quality for all data sets, using case-insensitive BLEU [33] of a machine translation system (cdec [13]) built using the word alignment predictions of each model.", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "We also perform an extrinsic evaluation of translation quality for all data sets, using case-insensitive BLEU [33] of a machine translation system (cdec [13]) built using the word alignment predictions of each model.", "startOffset": 153, "endOffset": 157}, {"referenceID": 13, "context": "[14] with a similar feature set, as a function of the number of sentences in the corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] it grows substantially with the vocabulary size.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] who informed us in personal communication of a computational trick to avoid expensive log operations in the forward-backward algorithm which sped up their training by an order of magnitude.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input\u2019s latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. Finally, we show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.", "creator": "LaTeX with hyperref package"}}}