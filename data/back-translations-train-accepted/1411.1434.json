{"id": "1411.1434", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "On the Information Theoretic Limits of Learning Ising Models", "abstract": "We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erd\\H{o}s-R\\'{e}nyi graphs in a certain dense setting.", "histories": [["v1", "Wed, 5 Nov 2014 22:28:00 GMT  (39kb)", "https://arxiv.org/abs/1411.1434v1", "21 pages; to appear in NIPS 2014"], ["v2", "Fri, 5 Dec 2014 22:02:55 GMT  (39kb)", "http://arxiv.org/abs/1411.1434v2", "21 pages; to appear in NIPS 2014"]], "COMMENTS": "21 pages; to appear in NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rashish tandon", "karthikeyan shanmugam", "pradeep ravikumar", "alexandros g dimakis"], "accepted": true, "id": "1411.1434"}, "pdf": {"name": "1411.1434.pdf", "metadata": {"source": "CRF", "title": "On the Information Theoretic Limits of Learning Ising Models", "authors": ["Karthikeyan Shanmugam", "Rashish Tandon", "Alexandros G. Dimakis", "Pradeep Ravikumar"], "emails": ["karthiksh@utexas.edu,", "rashish@cs.utexas.edu", "dimakis@austin.utexas.edu,", "pradeepr@cs.utexas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.14 34v2 [cs.LG] 5 DWe provide a general framework for calculating lower boundaries of sample complexity when restoring the underlying graphs of Ising models taking into account i.i.d. samples. Although there are recent results for certain graph classes, these include relatively extensive technical arguments specializing in each particular graph class. In contrast, we isolate two important graph structural components that can then be used to determine sample complexity for lower boundaries. The presence of these structural properties makes learning the graph class more difficult. We not only deduce from our main result the restoration of existing current results, but also provide lower boundaries for novel graph classes that were not previously considered. We also expand our framework to include random graph setting and derive conclusions for erdo-s-r\u00e9nyi graphs in a specific environment."}, {"heading": "1 Introduction", "text": "Graphic models provide compact representations of multivariate distributions using graphs that represent Markov conditional dependencies in the distribution. Therefore, they are often used in a number of machine learning areas where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15]. In many of these areas, a key problem of interest is the restoration of the underlying dependencies represented by the graph, i.e. the estimation of the graph of dependencies based on the instances drawn from the distribution. A common regime in which this graph selection problem is of interest is the high-dimensional setting in which the number of samples n is potentially smaller than the number of variables p. Given the importance of this problem, it is instructive to have lower boundaries on the sample dependencies to be drawn from the distribution."}, {"heading": "2 Preliminaries and Definitions", "text": "Notation: R represents the real line. [p] denotes the set of integers from 1 to 1. Let 1S denote the vector of ones and zeros, where S is the set of coordinates that contains 1. Let A \u2212 B denotes A Bc and A B denotes the symmetrical difference for two sets A and B. In this thesis we consider the problem of learning the graph structure of an Ising model. Ising models are a class of graphical model distributions over binary vectors, characterized by the pair (G, E), where G (V, E) is an undirected graph over the graph structure of an Ising model. (p2): The models are a class of graphical model distributions over binary vectors, characterized by the pair (G, E), listed by the pair (V, E), where G (V) is an undirected graph."}, {"heading": "3 Fano\u2019s Lemma and Variants", "text": "It provides a lower limit on the error probability of any estimator. The case of pmax is interesting only if we have a deterministic graph class G. Let's consider a graph class G with measurement \u00b5. Let's leave the G class with measurement \u00b5, and let's leave Xn = xx1,. xn} become independent samples like this xi-fG, i [n]. Then, for pmax and pavg as defined in (1) and (2) respectively pmax."}, {"heading": "4 Structural conditions governing Correlation", "text": "As discussed in the previous section, we want to find subsets T that are large but have a small KL diameter. In this section, we will summarize certain structural properties that lead to a small KL diameter. Thereafter, finding a large set T would amount to finding a large number of graphs in graph class G that meet these structural properties. In a first step, we need to get a sense of when two graphs would have corresponding distributions with a small KL divergence. To do this, we need a general upper limit for the KL divergence between the corresponding distributions. A simple strategy is to bind them simply by their symmetric divergence [16]. In this case, a small calculation shows: D (fG-fG) \u2264 D (fG-fG) + D (fG-fG) + D (fG-fG) = two (s-fG), t-E-divergence [xxt]."}, {"heading": "4.1 Structural Characterization with Large Correlation", "text": "One scenario in which there might be a small difference in correlations is when one of the correlations is very large, especially arbitrarily close to 1 (say EC) [xsxt] [[xsxt] \u2265 1 \u2212 \u0432, for some cases] > 0. Then EG [xsxt] \u2212 EG [xsxt] \u2264 1, since EG [xsxt] \u2264 1. In fact, this is achieved when s, t are part of a clique [16], since the large number of connections between them forces a higher probability of concordance, i.e. PG (xsxt = + 1) is large. In this paper, we provide a more general characterization of when this could happen by relying on the following key dilemma, which links the presence of \"many\" node separations \"of short distances between a pair of nodes in the diagram to a high correlation between them. We formally define the property below. Definition 1. Two nodes a and b in an unguighted diagram are connected if they are nodes G."}, {"heading": "4.2 Structural Characterization with Low Correlation", "text": "Another scenario in which there could be a small difference in the correlations between a pair of edges over two graphs is if the graphs themselves are close to each other in the hamming distance, i.e. they differ only by a few edges. This is formalized below for the situation if they differ by only one edge. Definition 2 (Hamming distance): Let us consider two graphs G (V, E) and G (V, E). The hamming distance between the graphs, which is denoted by H (G, G), is the number of edges at which the two graphs differ, i.e. H (G, G) = (s, t) | (s, t); E (5) Lemma 4. Let us consider two graphs G (V, E) and G (V, E) so that H (G) = 1, and (a, b).E is the only edge in E \u00b2."}, {"heading": "4.3 Influence of Structure on Sample Complexity", "text": "Suppose \u03bb > 0 is a positive real constant. In a diagram, solving the question of the presence / absence of the edge (n, t) would be difficult even if it were connected - which would require many samples. In principle, this is analogous to the argument in [16], which is used to determine the learning hardness of a series of graphs, each achieved by removing a single edge from a clique, while still guaranteeing many short distances between two arbitrary vertices. Similarly, if the graphs G and G are close to the hamming distance, then their respective distributions, fG and fG ', become difficult even if they are separated from the samples."}, {"heading": "5 Application to Deterministic Graph Classes", "text": "In this section, we offer estimates with lower limits for a number of deterministic graph families. This is done by explicitly finding a subset T of graph class G, based on the structural properties of the previous section. See the supplementary material for details of these constructions. A common theme is this: We try to find a graph in G that contains many edge pairs (u, v) so that its endpoints u and v have many paths between them (possibly a node disjunction). Once we have such a graph, we construct a subset T by removing one of the edges for these well-connected edge pairs, ensuring that the new graphs differ from the original only in the well-connected pairs. Alternatively, we can obtain another larger family T by removing any edge (not just well-connected pairs) that moves away from the original graph once."}, {"heading": "5.1 Path Restricted Graphs", "text": "Let Gp, \u03b7 be the class of all graphs on p vertices with at most \u03b7 paths (\u03b7 = o (p)) between any two vertices. We have the following theorem: Theorem 1. For the class Gp, \u03b7, if n \u2264 (1 \u2212 \u03b4) max {log (p / 2) \u03bb tanh\u03bb, 1 + cosh (2\u03bb) \u03b7 \u2212 1 2\u03bb log (p 2 (p + 1)), then pmax. To understand the scaling, it is useful to think of cosh (2\u03bb) to be roughly exponential (i.e. cosh (2\u03bb).p In this case, from the second term on, we need n."}, {"heading": "5.2 Girth Bounded Graphs", "text": "The circumference of a diagram is defined as the length of its shortest cycle. Leave Gp, g, d the set of all diagrams with a circumference of at least g and a maximum degree d. Note that as the circumference increases, the learning problem becomes easier, with the extreme case of g = \u221e (i.e. trees) being solved by the well-known ChowLiu algorithm [3] in O (log p) samples. We have the following sentence: Theorem 3. Consider the graph class Gp, g, d. For each sample (0, 1) we leave d\u03bd = min (d, p 1 \u2212 \u03bdg). Suppose that (1 \u2212) maxlog (p / 2) \u03bb tanh\u03bb, 1 + (1 + tanh (\u03bb) g \u2212 1 \u2212 tanh (1) g \u2212 1) djp = 2) djp (p), then pmax."}, {"heading": "5.4 Approximate Edge Bounded Graphs", "text": "Leave Gapproxp, k the set of all diagrams with the number of edges (k 2, k). This class is a subset of the class of diagrams with the edges at most k. Here we have: Theorem 5. Consider the class Gapproxp, k, and leave k \u2265 9. If we have the number of samples n \u2264 (1 \u2212 \u03b4) max {log (k2) \u03bb tanh\u03bb, e\u03bb (270 \u2212 1) 2\u03bbe\u03bb (2k + 1) log (k 2)}, then pmax \u2265 9. Note that the second term in the above limit comes from [16]. If we restrict the growth of the number of samples, we get a sample request of n = (k log k) k). Again, we adjust the lower limit for the limit class in [16], but by a smaller class c. 6 Erdo, s-R\u00e9nyi graphs G (p, c / p).In this section we move the number of required samples."}, {"heading": "6.1 Proof Outline", "text": "The proof skeleton is based on Lemma 2. The core of the proof consists in covering a series of large-scale graphs T by an exponentially small amount, with the KL divergence between any coverage and the coverage graph also very low. To this end, we use Korollar3. The most important steps in the proof are outlined below: 1. We identify a subclass of graphs T, as in Lemma 2, whose measure is close to 1, i.e. \u00b5 (T) = 1 \u2212 o (1). A natural candidate is the \"typical\" theorem T, which is defined as a series of graphs each defined with (cp2 \u2212 cp\u0430 2, cp 2 + cp\u0430 2) edges in the diagram. 2. (Path property) We show that most graphs in T have the property R: There are O (p2) pairs of nodes defined so that each pair is well covered by O (2p) node."}, {"heading": "7 Summary", "text": "By explicitly highlighting the dependence on the weights of the model, we have shown that the model can be difficult to learn without a limitation of the weights. Thus, for example, it is difficult to learn a graph that has many paths between many vertices unless it is controlled. We have shown lower limits for c > p0.75 for the random graph setting Gp, c / p, while accessibility is possible in the case of c = poly log p [1]. Closing this gap remains a problem for future consideration. Also, the application of our approaches to other deterministic / random graph classes such as the ChungLu model [4] (a generalization of Erdo-s-R\u00e9nyi graphs) or petty-worldly graphs [18] would be interesting."}, {"heading": "Acknowledgments", "text": "R.T. and P.R. confirm support for ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574 and DMS-1264033. K.S. and A.D. confirm support for NSF via CCF 1422549, 1344364, 1344179 and DARPA STTR as well as an ARO YIP Award."}, {"heading": "8 Appendix A - Proofs for Section 3 and Section 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Proof of Lemma 1", "text": "Proof: Starting with the original statement of Fano's Lemma (see [5, Theorem 2.10.1]) we get: pavg \u2265 H (G) \u2212 I (G; \u03c6 (Xn)) \u2212 log 2log | G | a \u2265 H (G) \u2212 I (G; Xn) \u2212 log 2 log | G | (7) Here we have: (a) due to the inequality in data processing (see [5, Theorem 2.8.1])) Note now that: pavg = tically G-G Pr\u00b5 (G).Pr (G-6 = G) \u2264 max G-GPr (G-6 = G) = pmax (8)"}, {"heading": "8.2 Proof of Corollary 1", "text": "Proof. We get the specified limit by selecting \u00b5 as a uniform measure for G in Lemma 1 and then using H (G) = log | G | and I (G; Xn) \u2264 H (Xn) \u2264 np."}, {"heading": "8.3 Proof of Lemma 2", "text": "Proof: The conditional version of the Fano-Lemma-Problem (see [1, Lemma 9]) results in: E\u00b5 [Pr [Pr (G-6 = G)] \u2265 H (G-G) \u2212 I (G; X n | G-T) \u2212 log 2log | T | (9) Now, pavg = E\u00b5 [Pr (G-6 = G)] = Pr\u00b5 (G-T) E\u00b5 [Pr (G-6 = G))."}, {"heading": "8.4 Proof of Corollary 2", "text": "The proof: We select \u00b5 to be a uniform measure, and use H (G) = log | G |. Furthermore, we limit the mutual information by an approach in [20], which relates it to covers relating to the KL divergence as follows: I (G; Xn; G; G; T) a = 4 G; T (G; G; T) D (fG; T) D (fG; T) D (fG (xn)) D (fG (Q (xn)))))) c = 4 G (G; T) (G; T) D (G; T)) D (x; T) DfG (x; T) T)."}, {"heading": "8.5 Proof of Lemma 3", "text": "Proof. Let's look at a diagram G (V, E) with two nodes a and b so that there are at least d node disjunction paths of length between a and b. Let's look at another diagram G (V, E) with an edge that E'E has set so that E'contains only edges leading to the d-node disjunction paths of length between a and b. All other edges are absent in P \u2212 p. Suppose P \u2212 1 (13) discrepancies. According to Griffith's inequality (see [7, Theorem 3.1]), EfG [xaxb] \u2265 EfG \u2032 [xaxb] [xaxb] = 2PG \u00b2 and xaxx \u00b2 d \u00b2), we are the probability of an event under the distribution fG \u2032. We calculate the ratio PG \u00b2 (xaxb = 1) / PG \u00b2 \u00b2 (xaxaxaxaxb)."}, {"heading": "8.6 Proof of Corollary 3", "text": "Evidence: Equation (4) gives: D (fG), (s, t), (s, t), (EG), (xsxt), (s, t), (EG), (s, t), (EG), (xsxt), (EG), (xsxt), (s, t), (E), (s, t), (s, t), (s, t), (E), (E), (E), (1), (E), (1), (E), (1), (tanh), (18), (a), (a), (xsxt), (b)."}, {"heading": "8.7 Proof of Lemma 4", "text": "Since the graphs G (V, E) and G (V, E) (V, E) differ only in the edge (a, b) (E), we have here: PG (xaxb = + 1) PG (xaxb = \u2212 1) = e2\u03bbPG (xaxb = + 1) PG (xaxb = \u2212 1) (19) Here, PG (\u00b7) corresponds to the probability of an event under fG. Leave q = PG (xaxb = + 1)) a = 2 (e2\u03bbq1 \u2212 q + e2\u03bbq) = 2 (e2\u03bb \u2212 1) (EG [xaxb] \u2212 EG [1) (q \u2212 q2) \u2212 q1) (qxaxb \u2212 1) (20)."}, {"heading": "9 Appendix B - Proofs for Section 5", "text": "For the proofs in this section, we will use the estimate of the number of samples represented in note 2. To summarize, we had the following general statement: For each graph of class G and its subgroup T-G, we can cover T with a single point (denoted by G0) with the KL radius \u03c1, i.e. for each other G-T, D (fG-fG0) \u2264 \u03c1. Well, ifn \u2264 log | T | \u03c1 (1 \u2212 \u03b4) (24) then pmax \u2265 \u043c. Note that, assuming T grows with p, we have ignored the lower order. So, for each of the graph classes considered, we will show how to construct G0, T and vice versa."}, {"heading": "9.1 Proof of Theorem 1", "text": "Proof. The graph class is Gp, \u03b7, the set of all graphs on p vertices with at most \u03b7 (\u03b7 = o (p)) paths between any two verticals. Let's construct G0: We look at the following basic building block. Let's take two vertices (s, t) and connect them to each other. Let's take \u03b1 \u2212 1 more vertices and connect them to both verticals. We find that we must have exactly \u03b7 paths between (s, t). Suffice it (\u03b7 + 1) total nodes and (2\u03b7 \u2212 1) total edges. Now let's look at the family of graphs T, which is obtained by removing the main (s, t) edge from one of the blocks. Let's choose \u03b1 = p-point + 1 (\u03b7 + 1) total edges. Let's construct T - Ensemble 1: Beginning with G0, let's look at the family of graphs T, which is obtained by removing the main (s, t) edge from one of the blocks."}, {"heading": "9.2 Proof of Theorem 2", "text": "Proof: The graph class is Gp, \u03b7, \u03b3, the set of all graphs on p-vertices with at most p-vertices (removal of the length lines). Construction G0: Let us consider the following basic building block. Let us take two vertices (s, t) and connect them to each other. Let us now take exactly p-1 more vertices and connect them to both s and t. Let us also take another k-vertice-vertice-vertice-vertices, each with a length \u03b3 + 1, between (s, t). Now, there are exactly p-vertices between (s, t), but at most p-vertices. There are (k + 1) total nodes and (k + 1) total nodes and (g + 1) -1) total edges. Let us now take square copies of these blocks. Note that we must select square and k vertices that \u03b1 and k vertices of length are T-vertices."}, {"heading": "9.3 Proof of Theorem 3", "text": "Proof. The graph class is Gp, g, d, the set of all graphs on p vertices with a circumference of at least g and a degree of at most d.Construct G0: Let us consider the following basic building block. Let us take two vertices (s, t) and connect them together. Let us also take k vertices and (k (g \u2212 1) total edges. Let us now take the disjointed copies of these blocks. Let us note that we must select \u03b1 and k vertices between (s, t), so that \u03b1 (g \u2212 2) + 2) \u2264 p vertices and k vertices and (g \u2212 1) total edges. Let us take the disjointed copies of these blocks. Let us note that we must select \u03b1 and k vertices, so that \u03b1 (g \u2212 2) + 2) \u2264 p vertices. For some of these graphs (0, 1) we choose \u03b1 = vertices. In this case k = dp vertices = dp vertices (d, d-2) the curve is \u2212 p."}, {"heading": "9.4 Proof of Theorem 4", "text": "Proof. The graph class is gapproxp, d, the set of all graphs on p-corners with degree d or d \u2212 1 (we assume that p is a multiple of d + 1 - if not, we can look at a smaller class instead, ignoring most d-corners). The construction here is the same as in [16]. Construction of G0: We divide the corners into p / (d + 1) groups, each of size d + 1, and then form cliques in each group. Construction of T: Starting from G0, we look at the family of graphs T, which is obtained by removing any edge. Thus, we get pd + 1 (d + 1 2) \u2265 pd4 such graphs. Also, such a graph, Gi, differs from G0 by a single edge, and also only by a pair, which is part of a clique minus an edge."}, {"heading": "9.5 Proof of Theorem 5", "text": "Proof. The graph class is Gapproxp, k, the set of all graphs on p-corners with at most k-edges. The construction here is the same as in [16] construction of G0: We select the greatest possible number of vertices m, so that we can have a clique on them, i.e. (m 2) \u2264 k. Then we get (m 2) \u2265 k2 of such graphs. Moreover, such a graph, Gi, differs from G0 only by a single edge and differs only in a pair that is part of a clique minus one edge. Thus, if we combine the estimates of [16] and Lemma 4, we have D (fG0, fGi) \u2264 min (2.2k + 1) e\u043c (2.0 \u2212 k, p \u00b2) and the plugging value of 24."}, {"heading": "10 Appendix C: Proof of Theorem 6", "text": "In this section, we outline the congruence arguments in detail along with a variant of the Fano lemmas to prove theorem 6. We remember some definitions and results from [1].Definition 3. Let T nB = {G: | d \u00b2 (G) \u2212 c | c \u00b2 -typical graphs where d \u00b2 (G) is the ratio of the sum of the degree of the nodes to the total number of nodes.A graph G to p = X (1),.. X (n) is drawn according to the distribution that the Erdo \u00b2 s-R\u00e9nyi ensemble G (p, c / p) (also referred to as CEFR without the parameter c).Then n i.d samples Xn = X (1),.. X (n) according to fG (x) with the scalar weight (p) > 0."}, {"heading": "10.1 Covering Argument through Fano\u2019s Inequality", "text": "Now we look at the random graph class G (p, c / p). If we look at a learning algorithm that draws the results of the learning algorithm (p, c / p) and the n samples Xn according to the distribution fG (x) described above (weighted \u03bb > 0), let G = \u03c6 (Xn) be the results of the learning algorithm. (Let fX (.) be the boundary distribution of Xn as described above. (Then the following applies to pavg = EG (p, c / p) [EXn, fG [1G] [1G] [6 = G]. (1G [6 = G]."}, {"heading": "10.2 The covering set S: dense case", "text": "First, we discuss certain properties that most graphs in T p possess when built on term 3. (With these properties we describe the congruence set S. Consider a graph G on p-nodes. (2) Divide the node into three equal parts A, B, and C of equal size (p / 3). Let D (G) A and c C be (2) connected by B if there are at least p-nodes in B connected to a and c (with parameters defined in section 4.3). Let D (G) A and c C be the set of all pairs (a, c): a A, c C so that the nodes a and c C are connected to each other. Let E (G) have the edge in graphs G.10.2.1 Technical results on D (G) nodes a A and c C are clear (2, d) connected if there are d-nodes in B."}, {"heading": "10.2.2 Covering set S and its properties", "text": "For each graph G & # 252 & # 8222; for each graph G & # 252; for each graph G & # 252; for each graph G & # 252; for each graph G & # 8222;, for each graph G & # 252; for each graph G & # 8220;, for each graph G & # 252; for each graph G & # 8220;, for each graph G & # 252; for each graph G & # 252;, for each graph G & # 252;, for each graph G & # 252; for each graph G & # 252; for each graph G & # 252; for each graph G & # 252;, for each graph G & # 252;, for each graph G & # 252;, for each graph G & # 252;, for each graph G & # 252;, for each graph G & # 252; for each graph G & # 252;, for each graph G & # 252; for each graph G & # 252; for each graph G & # 252; for each graph G & # 252; for each graph G & # 252;, for each graph G & # 252; for each graph G & # 252;, for each graph G & # 252; for each graph G & # 252;, for each graph G & # 252;, for each graph G & # 252;, for each graph G & # 252; for each graph G & # 252;, for each graph G & # 252;, for each graph G & # 252;, for each graph G & # 252; for the graph G & # 252;, for the graph G & # 252;, for the graph G & # 252;, for the graph G & # 252;, & 252; for the graph G & 252;, for the graph G & 252; for each graph G & 252;"}, {"heading": "10.3 Completing the covering argument:dense case", "text": "We will resume the cover argument from section 10.1. \u2212 \u2212 pnp (p) \u2212 pp (p, c / p) \u2212 pnp (p, c / p) \u2212 pnp (p, c / p) \u2212 pnp (p, c / p) \u2212 pnp (G) D (x, n) D (n) D (x, p) Q (xn) Q (xn) Q (xn) Q (xn) Q (xn) Q (xn) Q (xn) Q (xn) Q (xn) P (xn) P (p, c / p) P (p) P (p) P (p) P) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P) P (p) P (p) P) P n (n) P (n) P (p) P (n) P (n) P (p) P (n) P (p) P (n) P (p) P (n) P (p) P (p) P (p) P (p) P G (p) P) P (p) P (p) P (p) P) P (p) P (p) P) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p (p) P (p) P (p) P (p) P (p) P (p) P (p (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p) P (p (p) P (p) P (p) P (p) P (p) P (p (p) P (p) P (p) P n P (p) P n P (n P (n P (n P (p) P (p) P (p) P"}], "references": [{"title": "Highdimensional structure estimation in ising models: Local separation criterion", "author": ["Animashree Anandkumar", "Vincent YF Tan", "Furong Huang", "Alan S Willsky"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Complex Graphs and Networks", "author": ["Fan Chung", "Linyuan Lu"], "venue": "American Mathematical Society,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)", "author": ["Thomas M. Cover", "Joy A. Thomas"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Markov random field texture models", "author": ["G. Cross", "A. Jain"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1983}, {"title": "Ising models on locally tree-like graphs", "author": ["Amir Dembo", "Andrea Montanari"], "venue": "The Annals of Applied Probability, 20(2):565\u2013592,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Network information theory", "author": ["Abbas El Gamal", "Young-Han Kim"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Perfect matchings in o(n\\logn) time in regular bipartite graphs", "author": ["Ashish Goel", "Michael Kapralov", "Sanjeev Khanna"], "venue": "SIAM Journal on Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Markov random field models of digitized image texture", "author": ["M. Hassner", "J. Sklansky"], "venue": "In ICPR78,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1978}, {"title": "Beitrag zur theorie der ferromagnetismus", "author": ["E. Ising"], "venue": "Zeitschrift fu\u0308r Physik,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1925}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Schutze"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Minimax rates of estimation for highdimensional linear regression over lq-balls", "author": ["Garvesh Raskutti", "Martin J. Wainwright", "Bin Yu"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Spatial statistics", "author": ["B.D. Ripley"], "venue": "Wiley, New York,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1981}, {"title": "Information-theoretic limits of selecting binary graphical models in high dimensions", "author": ["Narayana P Santhanam", "Martin J Wainwright"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "On the difficulty of learning power law graphical models", "author": ["R. Tandon", "P. Ravikumar"], "venue": "IEEE International Symposium on Information Theory (ISIT),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Collective dynamics of \u2019small-world", "author": ["Duncan J. Watts", "Steven H. Strogatz"], "venue": "networks. Nature,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Markov image modeling", "author": ["J.W. Woods"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1978}, {"title": "Information-theoretic determination of minimax rates of convergence", "author": ["Yuhong Yang", "Andrew Barron"], "venue": "Annals of Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}], "referenceMentions": [{"referenceID": 10, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 156, "endOffset": 160}, {"referenceID": 4, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 179, "endOffset": 190}, {"referenceID": 8, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 179, "endOffset": 190}, {"referenceID": 16, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 179, "endOffset": 190}, {"referenceID": 9, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 212, "endOffset": 216}, {"referenceID": 12, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 241, "endOffset": 245}, {"referenceID": 14, "context": "[2, 17] for instance derive such bounds for the case of degree-bounded and power-law graph classes respectively.", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "This approach however is purely graph-theoretic, and thus fails to capture the interaction of the graphical model parameters with the graph structural constraints, and thus typically provides suboptimal lower bounds (as also observed in [16]).", "startOffset": 237, "endOffset": 241}, {"referenceID": 13, "context": "classes of bounded degree and bounded edge graphs for Ising models, [16] required fairly extensive arguments in using the above approach to provide lower bounds.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "As corollaries of this framework, we not only recover the results in [16] for the simple cases of degree and edge bounded graphs, but to several more classes of graphs, for which achievability results have already been proposed[1].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "As corollaries of this framework, we not only recover the results in [16] for the simple cases of degree and edge bounded graphs, but to several more classes of graphs, for which achievability results have already been proposed[1].", "startOffset": 227, "endOffset": 230}, {"referenceID": 0, "context": "Here, we show that under a certain scaling of the edge-weights \u03bb, Gp,c/p requires exponentially many samples, as opposed to a polynomial requirement suggested from earlier bounds[1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "3 Fano\u2019s Lemma and Variants Fano\u2019s lemma [5] is a primary tool for obtaining bounds on the average probability of error, pavg .", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "In contrast to the trivial upper bound used in the corollary above, we next use a tighter bound from [20], which relates the mutual information to coverings in terms of the KL-divergence, applied to Lemma 2.", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "We note that Fano\u2019s lemma and variants described in this section are standard, and have been applied to a number of problems in statistical estimation [1, 14, 16, 20, 21].", "startOffset": 151, "endOffset": 170}, {"referenceID": 11, "context": "We note that Fano\u2019s lemma and variants described in this section are standard, and have been applied to a number of problems in statistical estimation [1, 14, 16, 20, 21].", "startOffset": 151, "endOffset": 170}, {"referenceID": 13, "context": "We note that Fano\u2019s lemma and variants described in this section are standard, and have been applied to a number of problems in statistical estimation [1, 14, 16, 20, 21].", "startOffset": 151, "endOffset": 170}, {"referenceID": 17, "context": "We note that Fano\u2019s lemma and variants described in this section are standard, and have been applied to a number of problems in statistical estimation [1, 14, 16, 20, 21].", "startOffset": 151, "endOffset": 170}, {"referenceID": 13, "context": "A simple strategy is to simply bound it by its symmetric divergence[16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "Indeed, when s, t are part of a clique[16], this is achieved since the large number of connections between them force a higher probability of agreement i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": ", albeit with node overlaps) which allows for a stronger bound of \u223c 1\u2212 \u03bbke e\u03bbk (see [16])2 Now, as discussed earlier, a high correlation between a pair of nodes contributes a small term to the KL-divergence.", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "Both the bound from [16] and the bound from Lemma 3 have exponential asymptotic behaviour (i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "For smaller \u03bb, the bound from [16] is strictly better.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "This is analogous in principle to the argument in [16] used for establishing hardness of learning of a set of graphs each of which is obtained by removing a single edge from a clique, still ensuring many short paths between any two vertices.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "We note that the family Gp,\u03b7,\u03b3 is also studied in [1], and for which, an algorithm is proposed.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "Under certain assumptions in [1], and the restrictions: \u03b7 = O(1), and \u03b3 is large enough, the algorithm in [1] requires log p \u03bb2 samples, which is matched by the first term in our lower bound.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "Under certain assumptions in [1], and the restrictions: \u03b7 = O(1), and \u03b3 is large enough, the algorithm in [1] requires log p \u03bb2 samples, which is matched by the first term in our lower bound.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "Therefore, the algorithm in [1] is optimal, for the setting considered.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "trees) being solved by the well known ChowLiu algorithm[3] in O(log p) samples.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "Note that the second term in the bound above is from [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "This matches the lower bound for degree d bounded graphs in [16].", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "Note that the second term in the bound above is from [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "Again, we match the lower bound for the edge bounded class in [16], but through a smaller class.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "(This bound is from [1] ) Remark 4.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "For the random graph setting, Gp,c/p, while achievability is possible in the c = poly log p case[1], we have shown lower bounds for c > p.", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "The application of our approaches to other deterministic/random graph classes such as the ChungLu model[4] (a generalization of Erd\u0151s-R\u00e9nyi graphs), or small-world graphs[18] would also be interesting.", "startOffset": 103, "endOffset": 106}, {"referenceID": 15, "context": "The application of our approaches to other deterministic/random graph classes such as the ChungLu model[4] (a generalization of Erd\u0151s-R\u00e9nyi graphs), or small-world graphs[18] would also be interesting.", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "References [1] Animashree Anandkumar, Vincent YF Tan, Furong Huang, Alan S Willsky, et al.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[3] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Fan Chung and Linyuan Lu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Thomas M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Amir Dembo and Andrea Montanari.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Abbas El Gamal and Young-Han Kim.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Ashish Goel, Michael Kapralov, and Sanjeev Khanna.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Garvesh Raskutti, Martin J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Narayana P Santhanam and Martin J Wainwright.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Duncan J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Yuhong Yang and Andrew Barron.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In addition, we upper bound the mutual information through an approach in [20] which relates it to coverings in terms of the 10", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "ferromagnetic ising model, we know that q \u2208 [ 1 2 , 1].", "startOffset": 44, "endOffset": 54}, {"referenceID": 0, "context": "ferromagnetic ising model, we know that q \u2208 [ 1 2 , 1].", "startOffset": 44, "endOffset": 54}, {"referenceID": 0, "context": "Also, differentiating h(q), we get: h\u2032(q) = 1\u2212 2q \u2212 ( e \u2212 1 ) q (1 \u2212 q + e2\u03bbq)2 (21) It is easy to check that h\u2032(q) \u2264 0 for q \u2208 [ 1 2 , 1].", "startOffset": 128, "endOffset": 138}, {"referenceID": 0, "context": "Also, differentiating h(q), we get: h\u2032(q) = 1\u2212 2q \u2212 ( e \u2212 1 ) q (1 \u2212 q + e2\u03bbq)2 (21) It is easy to check that h\u2032(q) \u2264 0 for q \u2208 [ 1 2 , 1].", "startOffset": 128, "endOffset": 138}, {"referenceID": 13, "context": "The construction here is the same as in [16].", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "So, combining the estimates from [16] and Lemma 4, we have, D (fG0\u2016fGi) \u2264 min ( 2\u03bbde e\u03bbd , \u03bb tanh(\u03bb) )", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "The construction here is the same as in [16] Constructing G0: We choose a largest possible number of vertices m such that we can have a clique on them i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "So, combining the estimates from [16] and Lemma 4, we have, D (fG0\u2016fGi) \u2264 min ( 2\u03bbe( \u221a 2k+1) e\u03bb( \u221a 2k\u22121) , \u03bb tanh(\u03bb) )", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "We recall some definitions and results from [1].", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "(Lemma 8, 9 and Proof of Theorem 4 in [1] ) The \u01eb- typical set satisfies: 1.", "startOffset": 38, "endOffset": 41}, {"referenceID": 17, "context": "Now, use a result by Yang and Barron [20] to bound this term.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "See [9] ).", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "[8](Size of a Typical set) For any 0 \u2264 p \u2264 1, m \u2208 Z and a small \u01eb > 0, let Nm,p \u01eb = {x \u2208 {0, 1}m : \u2223", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "From counting arguments in [1], we have the following lower bound for G(p, c/p).", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "[1] Let G \u223c G(p, c/p).", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d. samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erd\u0151s-R\u00e9nyi graphs in a certain dense setting.", "creator": "LaTeX with hyperref package"}}}