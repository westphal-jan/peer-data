{"id": "1411.1752", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2014", "title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets", "abstract": "To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.", "histories": [["v1", "Thu, 6 Nov 2014 20:07:37 GMT  (9246kb,D)", "http://arxiv.org/abs/1411.1752v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.IR stat.ML", "authors": ["adarsh prasad", "stefanie jegelka", "dhruv batra"], "accepted": true, "id": "1411.1752"}, "pdf": {"name": "1411.1752.pdf", "metadata": {"source": "CRF", "title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets", "authors": ["Adarsh Prasad", "Stefanie Jegelka"], "emails": ["adarsh@cs.utexas.edu", "stefje@eecs.berkeley.edu", "dbatra@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "1.1 Preliminaries and Notation", "text": "We choose from a principle V of the N points. Each element is a label y = {y1, y2,.., yn} of the n point variables. To clarify, we use non-bold letters a-V for the points and bold letters y for the principle configurations. Capital letters clearly refer to functions via the principles F (a-A), R (a-A), D (a-A), and lowercase letters to the functions via the base variables f (y), r (y).1The accuracy of the most accurate segmentation in the Set.1 Formally speaking, there is a bijection list: V 7 \u2192 [L] m, which displays the positions a-V on their representation as base variable labelings y (a), d (a).1The accuracy of the most accurate segmentation in the Set.1 level does not exist."}, {"heading": "2 Marginal Gains in Configuration Space", "text": "To solve the greedy expansion step via optimization via y, we have to transfer the marginal gain from the world of items to the world of base variables and transfer functions to y from F: F (\u03c6 \u2212 1 (y) | S) f (y \u2212 1 (y)) r (y) (a) (a) (a) (a) (a) (a). \""}, {"heading": "3 Structured Diversity Functions", "text": "It is a general recipe for the construction of monotonous submodular diversities D (S), and for reducing their marginal gains to structured representations. (S) The segmentation of the individual segments is less likely than the segments that comprise the label. (S) The segmentations of the individual segments may be overlapping. (S) The segmentations of the individual segments of the individual segments may be overlapping. (S) The segmentations of the individual segments are characterized by the individual segments. (S) The segmentations of the individual segments of the individual segments of the individual segments may be overlapping. (S) The segmentations of the individual segments of the individual segments of the individual segments Segtio-Segmenti-Segmenti-Segmenti-Segmenti-Segmenti-Segmenttio-Segmenti-Segmento-Segmenti-Segmenti-Segmenti-Segmenti-o-Segmenti-Segmenti-o-o-o-Segi-Segmenti-o-o-o Segi Segi-o-o-Segi-o-o-Segi Segi-o-o-o Segi Segi-o-o Segi-o-o-o-i Segi Segi Segi-o-o-o-o-o Segi Segi-o-o-o Segi Segi-o-o-o-i Segi Segi Segi-o-o-o-o-o-o-i Segi Segi Segmenti-o-o-o-o-i Segi Segmenti-o-o-o-Segi Segmenti-o-o-o-o-o-o-o-Segi Segi Segi-o-i Segmenti-Segi-i-o-o-o-o-o-i Segi Segi Segi-o-o-i Segi-i-o-i"}, {"heading": "3.1 Diversity of Labels", "text": "For the first example, letG is \"the set of all labels y that contain the label,\" i.e. y-G, \"if and only if yj =\" for some j-n. \"Such a diversity function results in multi-class image segmentation - if the highest score contains diversity\" heaven \"and\" grass, \"then we would like to add complementary segmentations that contain an unused class designation, say\" sheep \"or\" cow. \"The marginal gain for this diversity function turns out to be a HOP called Label Cost [9]. It penalizes any label that occurs in an earlier segment. Let lcountS (\") be the number of segments in S that contain labels. \"In the simplest case of coverage diversity (4), the marginal gain provides a constant reward for each hitherto invisible label.\""}, {"heading": "3.2 Diversity via Hamming Balls", "text": "Ask for the \"Why,\" the \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \"Why,\" \",\", \",\", \",\", \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\", \"\", \",\" \",\" \",\", \",\" \",\", \"\", \"\", \"\", \"\" \",\" \",\" \",\" \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \",\" \",\", \",\" \",\", \",\", \"\", \"\", \",\", \"\" \"\", \"\", \"\", \"\" \"\", \",\" \"\", \"\", \"\" \",\" \"\" \",\" \"\", \"\" \",\" \"\" \",\", \"\" \"\", \"\" \"\" \",\", \"\" \"\" \",\" \",\" \",\" \"\" \",\", \"\" \"\", \"\" \",\" \"\" \"\" \",\" \"\" \",\" \",\" \"\" \"\" \",\" \"\", \"\", \"\" \"\" \",\" \"\" \"\", \"\" \",\" \"\" \"\" \",\" \",\" \"\", \"\" \"\" \"\", \",\" \"\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \",\" \"\" \",\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \",\" \"\" \"\" \",\" \",\" \"\", \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\""}, {"heading": "4 Experiments", "text": "We apply our greedy maximization algorithms to two image segmentation problems: (1) interactive binary segmentation (section 4.1); (2) category-level object segmentation on the PASCAL VOC 2012 dataset [11] (section 4.2). We compare all methods based on their respective oracle accuracy, i.e. the accuracy of the most accurate segmentation in the set of different segmentations returned by this method. For a small value of M \u2248 5 to 10, a high oracle accuracy indicates that the algorithm has achieved a high recall factor accuracy and has identified a good pool of candidate solutions for further processing in a cascaded pipeline. In both experiments, the \"background\" label is expected to appear somewhere in the image, and therefore does not play a role in the labeling costs / transition diversities of MYY diversity."}, {"heading": "4.1 Interactive segmentation", "text": "In interactive foreground-background segmentation, the user delivers partial scribbled labels. One way to minimize interactions is for the system to provide the user with a set of candidate segmentations to choose from. We replicate the experimental setup of [2], which curates 100 images from the PASCAL VOC 2012 dataset and manually delivers scribbles to each adjacent superpixel pair. For each image, we extract the relevance model r (y) is a pair of CRF with two labels, with a node term for each superpixel in the image and an edge term for each adjacent superpixel pair. At each superpixel, we extract color and texture features. We train a transductive SVM from the partial monitoring provided by the user scribbling. The node potentials are derived from the scores of these superpixels in the image and a pair of adjacent edges for each superpixel."}, {"heading": "4.2 Category level Segmentation", "text": "In the category level object segmentation, we label each pixel with one of 20 object categories or background information. We construct a multi-label CRF in pairs on superpixels. Our node potentials are outputs of category-specific regressors trained by [6], and our edge potentials are multi-label potts. Conclusions about the presence of diversity terms are made with the implementations of Delong et al. [9] for label costs, Tarlow et al. [32] for Hamming Ball diversity and Boykov et al. [3] for label transitions. Results. We evaluate all methods using the PASCAL VOC 2012 data [11], consisting of traction, valve and test partitions, each with approximately 1450 images. We train the regressors of [6] in train and report on diversity of different methods on val (we cannot report oracle results on these attributes, as these are not available to the public)."}, {"heading": "5 Discussion and Conclusion", "text": "This problem, of course, arises in areas such as Computer Vision, Natural Language Processing or Computational Biology, where we want to look for a range of different, high-quality solutions in a structured output space. The diversity functions we propose are easy to learn to be positive. Greedy algorithms for maximizing monotonous, submodular functions have proven useful in unstructured spaces. To achieve the best of our knowledge, this is the first generalization to exponentially large structured output spaces. In particular, our contribution lies in reducing greedy augmentation with structured, efficiently solvable HOPs."}, {"heading": "A Structured SVMs with nonnegativity constraint", "text": "In this section we will show that SSVMs have no natural origin and that parameters learned with non-negativity constraints achieve exactly the same hinged loss as without the non-negativity constraint. For a series of \"training instances\" (xn, yn), \"X-Y\" (n = 1,.), \"from an example space X and the label space Y, the structured SVM minimizes the following regulated risk functionality. (min w), which has a distance in the label space and represents an arbitrary function (y, y) + w\" (xn, y). (xn, y), \"from an example space X\" (xn, yn, yn)."}, {"heading": "B Label Transitions", "text": "In this section, we generalize the label cost diversification function to reward not only the presence of certain labels, but also the presence of certain label transitions. For example, if the highest segmentation contains a \"cow\" on \"grass,\" this diversification function will reward other segmentations for containing new label transitions (such as \"sheep grass\" or \"sheep sky\"). Formally, we define a group G, \"\" per label pair, \"and an element belongs to G,\" \"if y = (a) contains two adjacent variables,\" yi, yj with labels yi =, \"yj =.\" Structured Representation of Marginal Gains, \"and an element a belongs to G.\" For the diversity of label transitions, the marginal gain D (a | S) becomes a HOP designated as cooperative cuts. \""}, {"heading": "C Experiments", "text": "For the sake of completeness and to show the differences in the solution approaches of different diversity functions, we show sample sets of solutions generated for a particular image (Figs. 5, 6 and 7). These results help to understand the behavior of different diversity functions."}, {"heading": "D Proof of Lemma 1", "text": "There are monotonous submodular functions where E [F (S)] \u2264 (M / N + / M) max | S | \u2264 M F (S) for all \u2265 0.The limit contained in the main paper follows with = 0.Evidence. To prove this statement, let us consider a specific worst-case function. Let R'V be a fixed group of quantities M and deny that the cardinality-related optimum ismax | S | \u2264 M F (S) = F (R) = M (18) The expected value of an M-sized sample is the expectation of a hypergeometric distribution plus a correction taking into account that each group except R will have a value of at least 1 (using the second part of F): ES [F] (S) = Notone (S) (M) (M) (M) (M) + M (narrowly limited)."}, {"heading": "E Proof of Lemma 2", "text": "If each step of the greedy algorithm provides an approximate profit maximizer bi + 1 with F (bi + 1 | Si). \u2212 \u2212 \u2212 \u2212 To Lemma 2 \u2212 \u2212 \u2212 i (a | Si) \u2212 i (a | Si) \u2212 i + 1, thenF (SM) \u2265 (1 \u2212 1 e\u03b1 (S) max. \u2212 M (S) \u2212 i \u2212 1 i. To prove that Lemma 2 \u2212 \u2212 i + 1, thenF (bi + 1 | Si) the optimal solution based on S \u0432 arg max (S) arg max (S). \u2212 M (bi + 1 | Si) sp. \u2212 V F (a \u2212 Si) \u2212 i + 1, thenF (bi + 1 | Si)."}, {"heading": "F Relative Error", "text": "If we have a monotonous but not nonnegative function, we can shift the function and get a relative limit: Lemma 6. Let F be any monotonous submodular function, and let Fmin = minS V F (S). We can define a new, shifted monotonous non-negative version of F as F + (S), F (S) \u2212 Fmin. If we apply the greedy algorithm to F +, we get a solution S that fulfills F + (S) \u2265 F + (S) \u0445, then the solution S \u0445 has a limited relative approximation error: F (S) \u2212 Fmin F (S) \u2212 Fmin. Proof. By a proof analogous to Lemma 2 we get for \u03b1 = (1 \u2212 1e), F + (S \u0445) \u2265 F + (S \u0445) \u2212 F (38) \u2212 Fmin."}, {"heading": "G Generalization: Upper Envelope Potentials", "text": "In addition to the three specific examples mentioned in the main document (Section 4.1, 4.1, 4.2), we can generalize these constructions to a broad class of HOPs called upper envelope potentials [19]. However, if we leave G1, G2,.., Gb split into groups in which b is polynomic in the size of the number of base variables (s), we look at the group number of diversity. If we iterate t in the greedy algorithm, we assume that we are G1, G2,.., Gk. Then, for the (t + 1) th iteration, the limit increase of y, i.e. St) = 0, if y value {G1,.., Gk} 1 otherwise, we can now express d (y | St) as upper envelope potential, i.e. d (y | St)."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "To cope with the high level of ambiguity faced in domains such as Computer<lb>Vision or Natural Language processing, robust prediction methods often search<lb>for a diverse set of high-quality candidate solutions or proposals. In structured<lb>prediction problems, this becomes a daunting task, as the solution space (image<lb>labelings, sentence parses, etc.) is exponentially large. We study greedy algo-<lb>rithms for finding a diverse subset of solutions in structured-output spaces by<lb>drawing new connections between submodular functions over combinatorial item<lb>sets and High-Order Potentials (HOPs) studied for graphical models. Specifically,<lb>we show via examples that when marginal gains of submodular diversity functions<lb>allow structured representations, this enables efficient (sub-linear time) approxi-<lb>mate maximization by reducing the greedy augmentation step to inference in a<lb>factor graph with appropriately constructed HOPs. We discuss benefits, trade-<lb>offs, and show that our constructions lead to significantly better proposals.", "creator": "LaTeX with hyperref package"}}}