{"id": "1411.1792", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2014", "title": "How transferable are features in deep neural networks?", "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.", "histories": [["v1", "Thu, 6 Nov 2014 23:09:37 GMT  (339kb,D)", "http://arxiv.org/abs/1411.1792v1", "To appear in Advances in Neural Information Processing Systems 27 (NIPS 2014)"]], "COMMENTS": "To appear in Advances in Neural Information Processing Systems 27 (NIPS 2014)", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jason yosinski", "jeff clune", "yoshua bengio", "hod lipson"], "accepted": true, "id": "1411.1792"}, "pdf": {"name": "1411.1792.pdf", "metadata": {"source": "CRF", "title": "How transferable are features in deep neural networks?", "authors": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "emails": [], "sections": [{"heading": null, "text": "Many deep neural networks trained on natural images exhibit a curious phenomenon: On the first layer, they learn properties that resemble gabor filters and color blotches. Such properties of the first layer do not appear to be specific to a particular dataset or task, but rather general, since they are applicable to many datasets and tasks. Properties eventually have to pass from general to specific through the last layer of the network, but this transition has not been comprehensively investigated. In this paper, we experimentally quantify generality versus specificity of neurons in each layer of a deep revolutionary neural network and report some surprising results. Transferability is negatively affected by two different problems: (1) the specialization of higher layers of neurons to their original task at the expense of expected performance in the target task, and (2) optimization difficulties related to the division of networks between co-adjusted neurons, which was not expected. In an image-traced network, depending on whether or not one of these two problems can increase the lower layer, we show that, depending on the image-tracked networking ability."}, {"heading": "1 Introduction", "text": "This year it is so far that it only takes one year to get there, to get there."}, {"heading": "2 Generality vs. Specificity Measured as Transfer Performance", "text": "This year it is more than ever before."}, {"heading": "3 Experimental Setup", "text": "Since Krizhevsky et al. (2012) won the ImageNet 2012 competition, there has been a great deal of interest and work on optimizing the hyperparameters of large Convolutionary Models. In this study, however, we are not aiming to maximize absolute performance, but to study transfer results on a known architecture. We use the reference implementation of Caffe (Jia et al., 2014) to make our results comparable, extensible and useful for a large number of researchers. Further details on training setup (learning rates, etc.) are included in the supplementary material, and code and parameter files for reproducing these experiments are available at http: / / yosinski.com / transfer."}, {"heading": "4 Results and Discussion", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "1-7 1.6% 1.4%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3-7 1.8% 1.4%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5-7 2.1% 1.7%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2 Dissimilar Datasets: Splitting Man-made and Natural Classes Into Separate Datasets", "text": "As mentioned above, the effectiveness of feature transfer is expected to decrease as the basic and target tasks become less similar. We test this hypothesis by comparing the transfer performance of similar datasets (the random A / B splits discussed above) with that of man-made object classes A and natural object classes B. This artificial / natural division produces datasets that are as unequal as possible within the ImageNet datasets. Figure 3 above shows the accuracy of a BaseA and BaseB network (white circles), as well as BnA and AnB networks (orange hexagons).Lines connect common objectives. The top of the two rows contains those networks that are trained to target task with natural categories (BaseB and AnB).These networks work better than those that are aligned with man-made categories, which may be due to the fact that they only have 449 classes instead of 551, or are a simple task or both."}, {"heading": "4.3 Random Weights", "text": "We are comparing random, untrained weights because Jarrett et al. (2009) showed - rather strikingly - that the combination of random convolutionary filters, rectification, pooling and local normalization can work almost as well as learned features. They reported a deeper network of two or three learned layers and the smaller Caltech dataset (Fei-Fei et al., 2004). It is natural to ask whether the almost optimal performance of random filters that transfer them to a larger dataset is achieved. The upper level of Figure 3 shows the accuracy achieved by using random filters for the first n layers."}, {"heading": "5 Conclusions", "text": "We have demonstrated a method for quantifying the transferability of traits from each level of a neural network that reveals their universality or specificity. We have shown how transferability is negatively impacted by two distinct problems: optimization difficulties associated with splitting networks in the middle of fragile coadapted layers, and specializing higher layer characteristics on the original task at the expense of the performance of the target task. We have observed that these two problems can prevail depending on whether traits are transmitted from the bottom, the center, or the top of the network. We have also quantified how the transferability gap grows with increasing distance between tasks, especially when transferring higher layers, but found that even traits transmitted from remote tasks are better than random weights. Finally, we have found that initializing with transmitted characteristics can improve generalization performance even after considerable fine-tuning on a new task, which could be a generally useful technique for improving neural network performance."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Kyunghyun Cho and Thomas Fuchs for their helpful discussions, Joost Huizinga, Anh Nguyen and Roby Velez for their editing and the grants of the NASA Space Technology Research Fellowship (JY), the DARPA project W911NF-12-1-0449, NSERC, Ubisoft and CIFAR (YB is a CIFAR Fellow)."}, {"heading": "A Training Details", "text": "Since Krizhevsky et al. (2012) won the ImageNet 2012 competition, there was of course a lot of interest and work toward tweaking hyperparameters of large Convolutionary models. For example, Zeiler and Fergus (2013) found that it is better to reduce the first layer filter sizes from 11 \u00d7 11 to 7 \u00d7 7 and use a smaller step of 2 instead of 4. However, since this study is not aimed at maximum absolute performance but at using a generally studied architecture, we used the reference implementation of Caffe (Jia et al., 2014) We followed Donahue et al. (2013) in producing a few minor deviations from Krizhevsky et al. (2012) when the formation of the Convnets in this study skipped, we skipped the data augmentation trick by adding random multiplications of the main components of the pixel RGB values, which only led to an improvement of the original papers, and then to the scaling ratio of 256 and 256."}, {"heading": "B How Much Does an AlexNet Architecture Overfit?", "text": "In fact, most of them will be able to play by the rules they have established in the past, and they will be able to play by the rules they have established in the past."}, {"heading": "C Man-made vs. Natural Split", "text": "In order to compare the transfer performance between tasks A and B in such a way that A and B are semantically as unequal as possible, we tried to find two separate subsets of the 1000 classes in the ImageNet that had as little to do with each other as possible. To this end, we designated each node xi in the WordNet graph as ni, so that ni is the number of different ImageNet classes that can be achieved by starting at xi and traversing the graph only in the parent \u2192 child direction. The 20 nodes with the largest ni are the following: n _ i x _ i"}, {"heading": "1000 n00001740: entity", "text": "797 n00001930: physical entity 958 n00002684: object, physical objects 073609: 700003553: whole, unit 522 n00021939: artefact 410 n00004475: organism, being 410 n00004258: living thing, animate thing 398 n00015388: chordate being, beast n01861778: mammal, mammalian 212 n018867240: instrumentality, instrumentation 337 n01471682: vertebrate, craniate 337 n01466257: chordate 218 n01861778: mammalian, mammalian 212 ngno.1886756: placental, placental mammal, eutherian, eutherian mammal 158 n02075296: carnivore 130: canine, canine, canid 123 013013013013013013013013013013030:"}], "references": [{"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "In Proc. International Conference on Computer Vision", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Technical Report Arxiv", "citeRegEx": "Zeiler and Fergus,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and Fergus", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "This phenomenon occurs not only for different datasets, but even with very different training objectives, including supervised image classification (Krizhevsky et al., 2012), unsupervised density learning (Lee et al.", "startOffset": 148, "endOffset": 173}, {"referenceID": 6, "context": "When the target dataset is significantly smaller than the base dataset, transfer learning can be a powerful tool to enable training a large target network without overfitting; Recent studies have taken advantage of this fact to obtain state-of-the-art results when transferring from higher layers (Donahue et al., 2013a; Zeiler and Fergus, 2013; Sermanet et al., 2014), collectively suggesting that these layers of neural networks do indeed compute features that are fairly general.", "startOffset": 297, "endOffset": 368}, {"referenceID": 3, "context": "On the relatively large ImageNet dataset, we find lower performance than has been previously reported for smaller datasets (Jarrett et al., 2009) when using features computed from random lower-layer weights vs.", "startOffset": 123, "endOffset": 145}, {"referenceID": 0, "context": "The ImageNet dataset, as released in the Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) (Deng et al., 2009) contains 1,281,167 labeled training images and 50,000 test images, with each image labeled with one of 1000 classes.", "startOffset": 100, "endOffset": 119}, {"referenceID": 4, "context": "We use the reference implementation provided by Caffe (Jia et al., 2014) so that our results will be comparable, extensible, and useful to a large number of researchers.", "startOffset": 54, "endOffset": 72}, {"referenceID": 4, "context": "Since Krizhevsky et al. (2012) won the ImageNet 2012 competition, there has been much interest and work toward tweaking hyperparameters of large convolutional models.", "startOffset": 6, "endOffset": 31}, {"referenceID": 3, "context": "We also compare to random, untrained weights because Jarrett et al. (2009) showed \u2014 quite strikingly \u2014 that the combination of random convolutional filters, rectification, pooling, and local normalization can work almost as well as learned features.", "startOffset": 53, "endOffset": 75}, {"referenceID": 3, "context": "Performance falls off quickly in layers 1 and 2, and then drops to near-chance levels for layers 3+, which suggests that getting random weights to work in convolutional neural networks may not be as straightforward as it was for the smaller network size and smaller dataset used by Jarrett et al. (2009). However, the comparison is not straightforward.", "startOffset": 282, "endOffset": 304}, {"referenceID": 3, "context": "Performance falls off quickly in layers 1 and 2, and then drops to near-chance levels for layers 3+, which suggests that getting random weights to work in convolutional neural networks may not be as straightforward as it was for the smaller network size and smaller dataset used by Jarrett et al. (2009). However, the comparison is not straightforward. Whereas our networks have max pooling and local normalization on layers 1 and 2, just as Jarrett et al. (2009) did, we use a different nonlinearity (relu(x) instead of abs(tanh(x))), different layer sizes and number of layers, as well as other differences.", "startOffset": 282, "endOffset": 464}, {"referenceID": 3, "context": "One possible reason this latter result may differ from Jarrett et al. (2009) is because their fully-trained (non-random) networks were overfitting more on the smaller Caltech-101 dataset than ours on the larger ImageNet", "startOffset": 55, "endOffset": 77}], "year": 2014, "abstractText": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.", "creator": "LaTeX with hyperref package"}}}