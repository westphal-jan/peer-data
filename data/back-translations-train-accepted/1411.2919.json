{"id": "1411.2919", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2014", "title": "Bounded Regret for Finite-Armed Structured Bandits", "abstract": "We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problem-dependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.", "histories": [["v1", "Tue, 11 Nov 2014 18:55:35 GMT  (27kb)", "http://arxiv.org/abs/1411.2919v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore", "r\u00e9mi munos"], "accepted": true, "id": "1411.2919"}, "pdf": {"name": "1411.2919.pdf", "metadata": {"source": "CRF", "title": "Bounded Regret for Finite-Armed Structured Bandits", "authors": ["Tor Lattimore"], "emails": ["tlattimo@ualberta.ca", "remi.munos@inria.fr"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.29 19v1 [cs.LG] 1 1N ov"}, {"heading": "1 Introduction", "text": "The goal is to maximize the cumulative reward that currently appears to be optimal. (a) This is perhaps the simplest setting in which the known regret of regret becomes apparent, with a learner forced to choose between researching weapons about which it has little information and exploiting it by choosing the arm that currently appears optimal. (a) \u2212 1 \u2212 101 (b) \u2212 1 (c) We are looking at a general class of Karmed bandit problems where the expected return of each arm may depend on other weapons. This model has already been considered when the dependencies are linear and also in the general attitude we have studied here [12, 1]."}, {"heading": "2 Notation", "text": "Generally. Most of our notation is common with [8]. The indicator function is denoted with 1 {expr} = highest possible reward and is 1 if expr is true and 0 otherwise. We use log for the natural logarithm. Logically and / or are denoted with \"and\" respectively. \"Define the function \u03c9 (x) = min {y\" N: z, \"\" z \"and\" y \"that fulfills the logarithm (x)\" O \"(log x). Actually, limx\" \u00b7 \u00b7 log (x) / log (x) = 1.bandits. Let us specify a sentence. A K \"arm\" structured bandit is characterized by a series of functions: \"R,\" in which the expected return of arm k \"A: = {1, \u00b7 \u00b7, K\" the gap of arm \") / log (x) are defined. We define the mean of the optimal arm of arm.\""}, {"heading": "3 Structured UCB", "text": "We propose a new algorithm called UCB-S, which is a simple modification of UCB [6], but in which the known structure of the problem is exploited. In each time step, it constructs a confidence interval over the mean of each arm. From this, a subspace is constructed that contains the true parameter \u03b8 with a high probability. The algorithm takes the optimistic action over all other arms. Algorithm 1 UCB-S1: Input: functions \u00b51, \u00b7 \u00b7, \u00b5k: functions < [0, 1] 2: for t-1,...,.. do3: Defined trust overrides all arms."}, {"heading": "4 Theorems", "text": "The first is for the two main theorems that limit the regret of the UCB-S algorithms; the second is for the two main theorems that limit the regret of the UCB-S algorithms; the first is for the two main theorems that lead to a logarithmic regret that leads to a logarithmic regret comparable to the regret experienced for UCB by [6]. The analysis is slightly different, since UCB-S maintains upper and lower confidence and optimistically selects its actions from the model class, rather than by maximizing the uppermost trust as UCB does.Theorem 2. If the regret of the UCB-S algorithm exhibits an expected regret of the mostERn-2, then the upper lamentation of the maxK-1).The second and lower lamentation of the upper trust limit as UCB does.Theorem 2. If the samples from the optimal arm are sufficient, we will be able to learn the optimum regret of the action 6 by acting in the upper trust limit."}, {"heading": "5 Proof of Theorems 2 and 3", "text": "We begin by limiting the probability that any mean is not within the trust chain. Lemma 5. P {Ft = 1} \u2264 2Kt exp (\u2212 \u03b1 log (t)), whereFt = 1 \u00b2 \u00b2 (a), Ti (t) \u2212 1), (n), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t)."}, {"heading": "Ft is false. Then It = i\u2217.", "text": "The proof that Ft is wrong, in case it is wrong, cannot be provided later. It is wrong, in case it is wrong, in case it is wrong. It is wrong, in case it is wrong, in case it is wrong, in case it is wrong. It is wrong, in case it is wrong. It is wrong, in case it is wrong. It is wrong, in case it is wrong. It is wrong, in case it is wrong. It is wrong, in case it is wrong. It is wrong, in case it is wrong."}, {"heading": "6 Lower Bounds and Ambiguous Examples", "text": "The famous work of Lai and Robbins [17] shows that the boundary of Theorem 2 in general cannot be greatly improved. Many of the techniques here are by Bubeck et. al. [11]. Faced with a fixed algorithm and different dependencies, we denote the regret and expectation of Rn (a) or Figure 2. (c), then for all orders of magnitude > 0 and all algorithms that satisfy regret, so \u03c32 = 1.Theorem 8. Faced with the structured bandit represented in Figure 3. (a) or Figure 2. (c), then for all orders of magnitude > 0 and all algorithms that satisfy regret (\u2212), the evolution Rn (\u2212), the evolution Rn (2), which we consider sufficiently large n.Proof. The proof uses the same technique as proof of Theorem 5. in the work of [11]. Fix an algorithm and leave the probability that evolution is measured."}, {"heading": "7 Experiments", "text": "We tested algorithm 1 on a selection of structured bandits shown in Figure 2 and compared them with UCB [6, 8]. Rewards were sampled from normal distributions with deviations from units of measurement. For UCB, we chose \u03b1 = 2, while for algorithm 1 we used the theoretically justified \u03b1 = 4. Each data point is the average of 500 independent samples with blue crosses and red squares indicating the regret of UCB-S and UCB respectively. \u2212 0.2 \u2212 0.1 0 0.1 0.1 0.2 0100200nE available in the supplement material."}, {"heading": "8 Conclusion", "text": "The limitation of the new approach is that the evidence techniques and the algorithm are best suited for cases where the number of measures is relatively small, so generalizing the techniques to large spaces of action is an important open problem. There is still a small gap between the upper and lower limits, and the lower limits have only been proven for specific examples. Another question is how to define Thompson Sampling for structured bandits. Thompson Sampling has attracted a lot of attention lately [13, 2, 14, 3, 9], but so far we have not even been able to define an algorithm that resembles Thompson Sampling for the general structured bandit problem, not only because we have not provided Thompson Sampling with a topology, but also because the choice of a reasonable predecessor seems quite problem-dependent."}, {"heading": "A Ambiguous Case", "text": "The second assumption is not restrictive, since an algorithm cannot perform poorly for which it behaves according to Theorem 3, or impossible according to Theorem 9, so we can simply remove these points from the parameter space."}, {"heading": "B Technical Lemmas", "text": "Define the functions of (x) and (x): = min {z > 1: y \u2265 x log y, http: / / www.xxlog.com /, http: / / www.xlog.com, http: / / www.xlog.com, http: / / www.xlog.com, http: / / www.xlog.com, http: / / www.xlog.com, http: / / www.xlog.com, http: / / www.xlog.com, http: / / www.xlog.com, http: / / www.xlog.com, http: / / www.xlog.com, http: / www.xlog.com, http: / / www.xloglog.com, http: / www.xlogloglog.com, http: / / www.xloglogloglog.com, http: / www.xlogloglogloglog.com, http: / www.logloglogloglog.com, http: / www.logloglogloglogloglog.com, http: / www.logloglogloglogloglogloglog.com, http, http: / www.logloglogloglogloglogloglogloglogloglogloglog.com, http, http, http: / www.logloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglog.com, http, http, http, http: / / / / / www.logloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglogloglo"}, {"heading": "C Table of Notation", "text": "K-count of arms, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-parameter, p-p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p parameter, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p parameter, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}], "references": [{"title": "Asymptotically efficient adaptive allocation schemes for controlled markov chains: Finite parameter space", "author": ["Rajeev Agrawal", "Demosthenis Teneketzis", "Venkatachalam Anantharam"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "Proceedings of the 25th Annual Conference on Learning Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Further optimal regret bounds for thompson sampling", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "Proceedings of the 16th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Bandits, query learning, and the haystack dimension", "author": ["Kareem Amin", "Michael Kearns", "Umar Syed"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Variance estimates and exploration function in multi-armed bandit", "author": ["Jean-Yves Audibert", "R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "venue": "Technical report, research report 07-31, Certis-Ecole des Ponts,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem", "author": ["Peter Auer", "Ronald Ortner"], "venue": "Periodica Mathematica Hungarica,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Now Publishers Incorporated,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Prior-free and prior-dependent regret bounds for thompson sampling", "author": ["S\u00e9bastien Bubeck", "Che-Yu Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Online optimization in X-armed bandits", "author": ["S\u00e9bastien Bubeck", "R\u00e9mi Munos", "Gilles Stoltz", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Bounded regret in stochastic multiarmed bandits", "author": ["S\u00e9bastien Bubeck", "Vianney Perchet", "Philippe Rigollet"], "venue": "Proceedings of the 26th Annual Conference on Learning Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled Markov chains", "author": ["Todd L Graves", "Tze Leung Lai"], "venue": "SIAM journal on control and optimization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "R\u00e9mi Munos"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Thompson sampling for 1-dimensional exponential family bandits", "author": ["Nathaniel Korda", "Emilie Kaufmann", "R\u00e9mi Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Asymptotically optimal allocation of treatments in sequential experiments", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Design of Experiments: Ranking and Selection,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1984}, {"title": "Optimal sequential sampling from two populations", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1984}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1985}, {"title": "A structured multiarmed bandit problem and the greedy policy", "author": ["Adam J Mersereau", "Paat Rusmevichientong", "John N Tsitsiklis"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}], "referenceMentions": [{"referenceID": 17, "context": "This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1].", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1].", "startOffset": 122, "endOffset": 129}, {"referenceID": 0, "context": "This model has already been considered when the dependencies are linear [18] and also in the general setting studied here [12, 1].", "startOffset": 122, "endOffset": 129}, {"referenceID": 5, "context": "Our main contribution is a new algorithm based on UCB [6] for the structured bandit problem with strong problem-dependent guarantees on the regret.", "startOffset": 54, "endOffset": 57}, {"referenceID": 16, "context": "The improved algorithm exploits the known structure and so avoids the famous negative results by Lai and Robbins [17].", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].", "startOffset": 175, "endOffset": 182}, {"referenceID": 8, "context": "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].", "startOffset": 175, "endOffset": 182}, {"referenceID": 15, "context": "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].", "startOffset": 250, "endOffset": 254}, {"referenceID": 14, "context": "The most comparable previous work studies the case where both the return of the best arm and a bound on the minimum gap between the best arm and some sub-optimal arm is known [11, 9], which extended the permutation bandits studied by Lai and Robbins [16] and more general results by the same authors [15].", "startOffset": 300, "endOffset": 304}, {"referenceID": 0, "context": "[1], which studied a similar setting, but where \u0398 was finite.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Graves and Lai [12] extended the aforementioned contribution to continuous parameter spaces (and also to MDPs).", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "Most of our notation is common with [8].", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "3 Structured UCB We propose a new algorithm called UCB-S that is a straight-forward modification of UCB [6], but where the known structure of the problem is exploited.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "Algorithm 1 UCB-S 1: Input: functions \u03bc1, \u00b7 \u00b7 \u00b7 , \u03bck : \u0398 \u2192 [0, 1] 2: for t \u2208 1, .", "startOffset": 59, "endOffset": 65}, {"referenceID": 5, "context": "The first is for arbitrary \u03b8, which leads to a logarithmic bound on the regret comparable to that obtained for UCB by [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "[1] had essentially the same condition to achieve finite regret as (1), but specified to the case where \u0398 is finite.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] where if the expected return of the best arm is known and \u03b5 is a known bound on the minimum gap, then a regret bound of O (", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The improved UCB algorithm [7] enjoys a bound on the expected regret of O( \u2211", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "Before the proof of Theorem 3 we need a high-probability bound on the number of times arm i is pulled, which is proven along the lines of similar results by [5].", "startOffset": 157, "endOffset": 160}, {"referenceID": 16, "context": "The famous paper by Lai and Robbins [17] shows that the bound of Theorem 2 cannot in general be greatly improved.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The proof uses the same technique as the proof of Theorem 5 in the paper by [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "(c) follows from Lemma 4 by [11] where KL(P\u2212\u03b8,t,P\u03b8,t) is the relative entropy between measures P\u2212\u03b8,t and P\u03b8,t.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "Again, we make use of the techniques of [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "(c) by Lemma 4 of [11].", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "7 Experiments We tested Algorithm 1 on a selection of structured bandits depicted in Figure 2 and compared to UCB [6, 8].", "startOffset": 114, "endOffset": 120}, {"referenceID": 7, "context": "7 Experiments We tested Algorithm 1 on a selection of structured bandits depicted in Figure 2 and compared to UCB [6, 8].", "startOffset": 114, "endOffset": 120}], "year": 2014, "abstractText": "We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problemdependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.", "creator": "Creator"}}}