{"id": "1411.3224", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2014", "title": "On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence", "abstract": "We provide non-asymptotic bounds for the well-known temporal difference learning algorithm TD(0) with linear function approximators. These include high-probability bounds as well as bounds in expectation. Our analysis suggests that a step-size inversely proportional to the number of iterations cannot guarantee optimal rate of convergence unless we assume knowledge of the mixing rate for the Markov chain underlying the policy considered. This problem is alleviated by employing the well-known Polyak-Ruppert averaging scheme, leading to optimal rate of convergence without any knowledge of the mixing rate. Furthermore, we propose a variant of TD(0) with linear approximators that incorporates a centering sequence, and we establish that it exhibits an exponential rate of convergence in expectation.", "histories": [["v1", "Wed, 12 Nov 2014 16:22:28 GMT  (24kb)", "https://arxiv.org/abs/1411.3224v1", null], ["v2", "Tue, 1 Sep 2015 18:20:52 GMT  (571kb)", "http://arxiv.org/abs/1411.3224v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["nathaniel korda", "prashanth l a"], "accepted": true, "id": "1411.3224"}, "pdf": {"name": "1411.3224.pdf", "metadata": {"source": "CRF", "title": "On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence", "authors": ["Nathaniel Korda", "Prashanth L.A"], "emails": ["nathaniel.korda@eng.ox.ac.uk", "prashla@isr.umd.edu"], "sections": [{"heading": null, "text": "An important problem in RL is the estimation of the value function V \u03c0 for a given stationary policy. We focus on discounted reward MDPs with a high-dimensional state space S, in which one can only hope to approximate the value function, and this is the political evaluation that occurs in several approximate political methods, e.g. in action-critical algorithms [Konda and Tsitsiklis, 2003]. Temporal differential learning is a well-known political evaluation method that works both online and with a single example path achieved through the simulation of the underlying MDP."}], "references": [{"title": "Approximate dynamic programming", "author": ["Dimitri P Bertsekas"], "venue": null, "citeRegEx": "Bertsekas.,? \\Q2011\\E", "shortCiteRegEx": "Bertsekas.", "year": 2011}, {"title": "Transport-entropy inequalities and deviation estimates for stochastic approximation schemes", "author": ["Max Fathi", "Noufel Frikha"], "venue": "arXiv preprint arXiv:1301.7740,", "citeRegEx": "Fathi and Frikha.,? \\Q2013\\E", "shortCiteRegEx": "Fathi and Frikha.", "year": 2013}, {"title": "Concentration Bounds for Stochastic Approximations", "author": ["Noufel Frikha", "St\u00e9phane Menozzi"], "venue": "Electron. Commun. Probab.,", "citeRegEx": "Frikha and Menozzi.,? \\Q2012\\E", "shortCiteRegEx": "Frikha and Menozzi.", "year": 2012}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Actor-Critic Algorithms", "author": ["Vijay R Konda"], "venue": "PhD thesis, Department of Electrical Engineering and Computer Science,", "citeRegEx": "Konda.,? \\Q2002\\E", "shortCiteRegEx": "Konda.", "year": 2002}, {"title": "On Actor-Critic Algorithms", "author": ["Vijay R Konda", "John N Tsitsiklis"], "venue": "SIAM journal on Control and Optimization,", "citeRegEx": "Konda and Tsitsiklis.,? \\Q2003\\E", "shortCiteRegEx": "Konda and Tsitsiklis.", "year": 2003}, {"title": "Finite-sample analysis of lstd", "author": ["Alessandro Lazaric", "Mohammad Ghavamzadeh", "R\u00e9mi Munos"], "venue": "In ICML,", "citeRegEx": "Lazaric et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lazaric et al\\.", "year": 2010}, {"title": "Markov chains and stochastic stability", "author": ["Sean P Meyn", "Richard L Tweedie"], "venue": "Cambridge university press,", "citeRegEx": "Meyn and Tweedie.,? \\Q2009\\E", "shortCiteRegEx": "Meyn and Tweedie.", "year": 2009}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Boris T Polyak", "Anatoli B Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Polyak and Juditsky.,? \\Q1992\\E", "shortCiteRegEx": "Polyak and Juditsky.", "year": 1992}, {"title": "Stochastic approximation. Handbook of Sequential Analysis, pages 503\u2013529", "author": ["David Ruppert"], "venue": null, "citeRegEx": "Ruppert.,? \\Q1991\\E", "shortCiteRegEx": "Ruppert.", "year": 1991}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["John N Tsitsiklis", "Benjamin Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}, {"title": "Convergence results for some temporal difference methods based on least squares", "author": ["Huizhen Yu", "Dimitri P Bertsekas"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Yu and Bertsekas.,? \\Q2009\\E", "shortCiteRegEx": "Yu and Bertsekas.", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "actor-critic algorithms [Konda and Tsitsiklis, 2003], [Bhatnagar et al.", "startOffset": 24, "endOffset": 52}, {"referenceID": 0, "context": "3 of [Bertsekas, 2011]) A\u03b8 = b, where A = \u03a6T\u03a8(I \u2212 \u03b2P )\u03a6 and b = \u03a6T\u03a8r.", "startOffset": 5, "endOffset": 22}, {"referenceID": 3, "context": "A more direct approach is to center the updates, and this was pioneered recently for solving batch problems via stochastic gradient descent in convex optimization [Johnson and Zhang, 2013].", "startOffset": 163, "endOffset": 188}, {"referenceID": 2, "context": "Concentration bounds for general stochastic approximation schemes have been derived in [Frikha and Menozzi, 2012] and later expanded to include iterate averaging in [Fathi and Frikha, 2013].", "startOffset": 87, "endOffset": 113}, {"referenceID": 1, "context": "Concentration bounds for general stochastic approximation schemes have been derived in [Frikha and Menozzi, 2012] and later expanded to include iterate averaging in [Fathi and Frikha, 2013].", "startOffset": 165, "endOffset": 189}, {"referenceID": 4, "context": "An asymptotic normality result for TD(\u03bb) is available in [Konda, 2002].", "startOffset": 57, "endOffset": 70}, {"referenceID": 4, "context": "Asymptotic convergence rate results for LSTD(\u03bb) and LSPE(\u03bb), two popular least squares methods, are available in [Konda, 2002] and [Yu and Bertsekas, 2009], respectively.", "startOffset": 113, "endOffset": 126}, {"referenceID": 12, "context": "Asymptotic convergence rate results for LSTD(\u03bb) and LSPE(\u03bb), two popular least squares methods, are available in [Konda, 2002] and [Yu and Bertsekas, 2009], respectively.", "startOffset": 131, "endOffset": 155}, {"referenceID": 6, "context": "A related work in this direction is the finite time bounds for LSTD in [Lazaric et al., 2010].", "startOffset": 71, "endOffset": 93}, {"referenceID": 10, "context": "s\u2032 p(s, \u03c0(s), s)V (s), (5) TD(0) [Sutton and Barto, 1998] performs a fixed point-iteration using stochastic approximation: Starting with an arbitrary V0, update Vn(sn) := Vn\u22121(sn) + \u03b3n ( r(sn, \u03c0(sn)) + \u03b2Vn\u22121(sn+1)\u2212 Vn\u22121(sn) ) , (6) where \u03b3n are step-sizes that satisfy standard stochastic approximation conditions.", "startOffset": 33, "endOffset": 57}, {"referenceID": 7, "context": "See Chapters 15 and 16 of [Meyn and Tweedie, 2009] for a detailed treatment of the subject matter.", "startOffset": 26, "endOffset": 50}, {"referenceID": 9, "context": "This principle was introduced independently by Ruppert [Ruppert, 1991] and Polyak [Polyak and Juditsky, 1992], for accelerating stochastic approximation schemes.", "startOffset": 55, "endOffset": 70}, {"referenceID": 8, "context": "This principle was introduced independently by Ruppert [Ruppert, 1991] and Polyak [Polyak and Juditsky, 1992], for accelerating stochastic approximation schemes.", "startOffset": 82, "endOffset": 109}, {"referenceID": 3, "context": "The approach is inspired by the SVRG algorithm, proposed in [Johnson and Zhang, 2013], for a optimising a strongly-convex function.", "startOffset": 60, "endOffset": 85}, {"referenceID": 3, "context": "However, the setting for TD(0) with function approximation that we have is considerably more complicated owing to the following reasons: (i) Unlike [Johnson and Zhang, 2013], we are not optimising a function that is a finite-sum of smooth functions in a batch setting.", "startOffset": 148, "endOffset": 173}, {"referenceID": 3, "context": "(iv) Finally, there are extra difficulties owing to the fact that we have a fixed point iteration, while the corresponding algorithm in [Johnson and Zhang, 2013] is stochastic gradient descent (SGD).", "startOffset": 136, "endOffset": 161}, {"referenceID": 1, "context": "For the second inequality we have used discrete integration by parts (see page 15 in Fathi and Frikha [2013], display (2.", "startOffset": 85, "endOffset": 109}, {"referenceID": 1, "context": "For the second inequality we have used discrete integration by parts (see page 15 in Fathi and Frikha [2013], display (2.2), for details). For the last inequality we have noted, as in page 15 in Fathi and Frikha [2013], that n\u22121 \u2211", "startOffset": 85, "endOffset": 219}, {"referenceID": 12, "context": "This is a two-state toy example, which is borrowed from [Yu and Bertsekas, 2009].", "startOffset": 56, "endOffset": 80}], "year": 2015, "abstractText": "We provide non-asymptotic bounds for the well-known temporal difference learning algorithm TD(0) with linear function approximators. These include high-probability bounds as well as bounds in expectation. Our analysis suggests that a step-size inversely proportional to the number of iterations cannot guarantee optimal rate of convergence unless we assume (partial) knowledge of the stationary distribution for the Markov chain underlying the policy considered. We also provide bounds for the iterate averaged TD(0) variant, which gets rid of the step-size dependency while exhibiting the optimal rate of convergence. Furthermore, we propose a variant of TD(0) with linear approximators that incorporates a centering sequence, and establish that it exhibits an exponential rate of convergence in expectation. We demonstrate the usefulness of our bounds on two synthetic experimental settings.", "creator": "LaTeX with hyperref package"}}}