{"id": "1411.3880", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2014", "title": "Optimal Cost Almost-Sure Reachability in POMDPs", "abstract": "We consider partially observable Markov decision processes (POMDPs) with a set of target states and every transition is associated with an integer cost. The optimization objective we study asks to minimize the expected total cost till the target set is reached, while ensuring that the target set is reached almost-surely (with probability 1). We show that for integer costs approximating the optimal cost is undecidable. For positive costs, our results are as follows: (i) we establish matching lower and upper bounds for the optimal cost and the bound is double exponential; (ii) we show that the problem of approximating the optimal cost is decidable and present approximation algorithms developing on the existing algorithms for POMDPs with finite-horizon objectives. While the worst-case running time of our algorithm is double exponential, we also present efficient stopping criteria for the algorithm and show experimentally that it performs well in many examples of interest.", "histories": [["v1", "Fri, 14 Nov 2014 12:13:45 GMT  (3791kb,D)", "http://arxiv.org/abs/1411.3880v1", "Full Version of Optimal Cost Almost-sure Reachability in POMDPs, AAAI 2015. arXiv admin note: text overlap witharXiv:1207.4166by other authors"]], "COMMENTS": "Full Version of Optimal Cost Almost-sure Reachability in POMDPs, AAAI 2015. arXiv admin note: text overlap witharXiv:1207.4166by other authors", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["krishnendu chatterjee", "martin chmelik", "raghav gupta", "ayush kanodia"], "accepted": true, "id": "1411.3880"}, "pdf": {"name": "1411.3880.pdf", "metadata": {"source": "CRF", "title": "Optimal Cost Almost-sure Reachability in POMDPs (Full Version)", "authors": ["Krishnendu Chatterjee", "Martin Chme\u013a\u0131k", "Raghav Gupta", "Ayush Kanodia"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2 Definitions", "text": "We present the definitions of POMDPs, strategies, goals and other basic concepts required for our results. (D) During this work, we follow the standard notations of [28,19].Notation. Faced with a finite set of X, we are given by P (X) the set of subsets of X, i.e., P (X) is the power set of X. A probability distribution f on X is a function f: X (x) > 1 such that x (x) = 1, and we from D (X) the set of all probability distributions on X. For f: D (X), we denote from Supp (f) = {x) > 0} the support of f: POMDPs. A partially observable Markov decision process (POMDP) is a tuple G = (S, A, L, Z, Z) where: (i) S is a finite set of states."}, {"heading": "4.3 Optimal finite-horizon strategies", "text": "Our algorithms for the approximation of optCost will use algorithms to optimize the finite-light horizon costs. We will first remember the well-known construction of the optimal finite-light horizon strategies, which calculate the expected total costs in POMDPs for the length k. For the minimization of the expected total costs, strategies based on information states are sufficient [32]. An information state b is defined as a probability distribution over the total height of the states, where for s \u00b2 S the value b (s) denotes the probability of being in state s. We will use H to denote the amount of all information states. In view of an information state b, an action a, and an observation z, which calculates the resulting information state b \u00b2 in a straight way."}, {"heading": "4.4 Approximation algorithm", "text": "In this section, we will show that for all > 0 there is a limit that is designed in such a way that the strategies are inconsistent in relation to the OptCost within most countries. First, we will consider a cap on the OptCost. We will consider a cap of UAllow on the expected total cost of the strategy, which starts in any state, and consider the estimated total cost of the strategy, which starts in the states with the initial support of faith. Then, the upper limit is defined as UAllow = maxU. (G, T), s, s, TAllow (s, U) refers to the expected total cost of the strategy, which starts in the states with the initial support of faith. Then, the upper limit is defined as UAllow = maxU."}, {"heading": "5 Experimental Results", "text": "The fact is that we are able to put ourselves in a situation in which we are able to plunge ourselves into a crisis, in which we are able, in which we are able to solve it, in which we are able, and in which we are able to plunge ourselves into a crisis, in which we are able, in which we are able to solve the crisis, in which we are able, in which we are able to solve this crisis."}], "references": [{"title": "Solving POMDPs: RTDP-Bel vs. point-based algorithms", "author": ["B. Bonet", "H. Geffner"], "venue": "In IJCAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Properly Acting under Partial Observability with Action Feasibility Constraints. volume 8188 of Lecture Notes in Computer Science, pages 145\u2013161", "author": ["C.C.P Carvalho", "F. Teichteil-K\u00f6nigsbuch"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Pomdp-solve [software, version 5.3", "author": ["A. Cassandra"], "venue": "http://www.pomdp.org/,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Exact and approximate algorithms for partially observable Markov decision processes", "author": ["A.R. Cassandra"], "venue": "Brown University,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Faster and dynamic algorithms for maximal end-component decomposition and related graph problems in probabilistic verification", "author": ["K. Chatterjee", "M. Henzinger"], "venue": "In SODA. ACM-SIAM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "The complexity of probabilistic verification", "author": ["C. Courcoubetis", "M. Yannakakis"], "venue": "Journal of the ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Digital images and formal languages", "author": ["K. Culik", "J. Kari"], "venue": "Handbook of formal languages,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Biological sequence analysis: probabilistic models of proteins and nucleic acids", "author": ["R. Durbin", "S. Eddy", "A. Krogh", "G. Mitchison"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Solving POMDPs using selected past events", "author": ["A. Dutech"], "venue": "In ECAI, pages 281\u2013285,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Dynamic Programming and Markov Processes", "author": ["H. Howard"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1960}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "J. of Artif. Intell. Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Heuristic search for generalized stochastic shortest path MDPs", "author": ["A. Kolobov", "Mausam", "D.S. Weld", "H. Geffner"], "venue": "In ICAPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Temporal-logic-based reactive mission and motion planning", "author": ["H. Kress-Gazit", "G.E. Fainekos", "G.J. Pappas"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W.S. Lee"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Algorithms for Sequential Decision Making", "author": ["M.L. Littman"], "venue": "PhD thesis, Brown University,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "Learning policies for partially observable environments: Scaling up", "author": ["M.L. Littman", "A.R. Cassandra", "L. P Kaelbling"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "First results with utile distinction memory for reinforcement learning", "author": ["R.A. McCallum"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "Online discovery and learning of predictive state representations", "author": ["P. McCracken", "M.H. Bowling"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Finite-state transducers in language and speech processing", "author": ["M. Mohri"], "venue": "Computational Linguistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "The complexity of Markov decision processes", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1987}, {"title": "Approximating optimal policies for partially observable stochastic domains", "author": ["R. Parr", "S.J. Russell"], "venue": "In IJCAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Introduction to probabilistic automata (Computer science and applied mathematics)", "author": ["A. Paz"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1971}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In IJCAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Markov Decision Processes", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1994}, {"title": "Probabilistic automata", "author": ["M.O. Rabin"], "venue": "Information and Control,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1963}, {"title": "Artificial intelligence: a modern approach, volume 74", "author": ["S.J. Russell", "P. Norvig", "J.F. Canny", "J.M. Malik", "D.D. Edwards"], "venue": "Prentice hall Englewood Cliffs,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1995}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "The Optimal Control of Partially Observable Markov Processes", "author": ["E.J. Sondik"], "venue": "Stanford University,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1971}, {"title": "A point-based POMDP algorithm for robot planning", "author": ["M.T.J. Spaan"], "venue": "In Robotics and Automation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}], "referenceMentions": [{"referenceID": 9, "context": "Markov decision processes (MDPs) are standard models for probabilistic systems that exhibit both probabilistic as well as nondeterministic behavior [12].", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "MDPs are widely used to model and solve control problems for stochastic systems [11,28]: nondeterminism represents the freedom of the controller to choose a control action, while the probabilistic component of the behavior describes the system response to control actions.", "startOffset": 80, "endOffset": 87}, {"referenceID": 20, "context": ", given the current state, the controller can only view the observation of the state (the partition the state belongs to), but not the precise state [24].", "startOffset": 149, "endOffset": 153}, {"referenceID": 7, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 110, "endOffset": 113}, {"referenceID": 19, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 156, "endOffset": 159}, {"referenceID": 13, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 176, "endOffset": 183}, {"referenceID": 10, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 176, "endOffset": 183}, {"referenceID": 11, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 208, "endOffset": 212}, {"referenceID": 25, "context": "POMDPs also subsume many other powerful computational models such as probabilistic finite automata (PFA) [29,26] (since probabilistic finite automata (aka blind POMDPs) are a special case of POMDPs with a single observation).", "startOffset": 105, "endOffset": 112}, {"referenceID": 22, "context": "POMDPs also subsume many other powerful computational models such as probabilistic finite automata (PFA) [29,26] (since probabilistic finite automata (aka blind POMDPs) are a special case of POMDPs with a single observation).", "startOffset": 105, "endOffset": 112}, {"referenceID": 24, "context": "In stochastic optimization problems related to POMDPs, the transitions in the POMDPs are associated with integer costs, and the two classical objectives that have been widely studied are finite-horizon and discounted-sum objectives [11,28,24].", "startOffset": 232, "endOffset": 242}, {"referenceID": 20, "context": "In stochastic optimization problems related to POMDPs, the transitions in the POMDPs are associated with integer costs, and the two classical objectives that have been widely studied are finite-horizon and discounted-sum objectives [11,28,24].", "startOffset": 232, "endOffset": 242}, {"referenceID": 14, "context": "While there are several works for discounted POMDPs [18,31,27], as mentioned above the problem we consider is different from discounted POMDPs.", "startOffset": 52, "endOffset": 62}, {"referenceID": 27, "context": "While there are several works for discounted POMDPs [18,31,27], as mentioned above the problem we consider is different from discounted POMDPs.", "startOffset": 52, "endOffset": 62}, {"referenceID": 23, "context": "While there are several works for discounted POMDPs [18,31,27], as mentioned above the problem we consider is different from discounted POMDPs.", "startOffset": 52, "endOffset": 62}, {"referenceID": 0, "context": "The most closely related works are Goal-MDPs and POMDPs [2,16].", "startOffset": 56, "endOffset": 62}, {"referenceID": 12, "context": "The most closely related works are Goal-MDPs and POMDPs [2,16].", "startOffset": 56, "endOffset": 62}, {"referenceID": 24, "context": "Throughout this work, we follow standard notations from [28,19].", "startOffset": 56, "endOffset": 63}, {"referenceID": 15, "context": "Throughout this work, we follow standard notations from [28,19].", "startOffset": 56, "endOffset": 63}, {"referenceID": 22, "context": "The strict emptiness problem asks for the existence of a strategy w (a finite word over the alphabet A) such that the measure of the runs ending in the desired final states F is strictly greater than 1 2 ; and the strict emptiness problem for PFA is undecidable [26].", "startOffset": 262, "endOffset": 266}, {"referenceID": 0, "context": "Consider a strategy in the POMDP u = ($ w[1] $ w[2] .", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "as a word u = ($ w[1] $ w[2] .", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "The sequence of costs can be partitioned into blocks of length 2 \u00b7 n+ 1, intuitively corresponding to the transitions of a single run on the word ($w[1] $w[2] .", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "The framework that restricts playable actions was also considered in [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "The strategy \u03c3Allow can be obtained by computing the set of almost-sure winning states in the belief-support MDP, and for discrete graph-based algorithms to compute almost-sure winning states in perfect-observation MDPs see [7,6].", "startOffset": 224, "endOffset": 229}, {"referenceID": 4, "context": "The strategy \u03c3Allow can be obtained by computing the set of almost-sure winning states in the belief-support MDP, and for discrete graph-based algorithms to compute almost-sure winning states in perfect-observation MDPs see [7,6].", "startOffset": 224, "endOffset": 229}, {"referenceID": 28, "context": "For minimizing the expected total cost, strategies based on information states are sufficient [32].", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "Given an information state b, an action a, and an observation z, computing the resulting information state b\u2032 can be done in a straight forward way, see [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "The first approach is the exact finite-horizon value iteration using a modified version of POMDPSolve [4]; and the second approach is an approximate finite-horizon value iteration using a modified version of RTDP-Bel [2]; and in both the cases our straightforward modification is that the computation of the finite-horizon value iteration is restricted to allowed actions and almost-sure winning belief-supports.", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "The first approach is the exact finite-horizon value iteration using a modified version of POMDPSolve [4]; and the second approach is an approximate finite-horizon value iteration using a modified version of RTDP-Bel [2]; and in both the cases our straightforward modification is that the computation of the finite-horizon value iteration is restricted to allowed actions and almost-sure winning belief-supports.", "startOffset": 217, "endOffset": 220}, {"referenceID": 17, "context": "The POMDP examples we considered are as follows: (A) We experimented with the Cheese maze POMDP example which was introduced in [21] and also studied in [10,20,22].", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "The POMDP examples we considered are as follows: (A) We experimented with the Cheese maze POMDP example which was introduced in [21] and also studied in [10,20,22].", "startOffset": 153, "endOffset": 163}, {"referenceID": 16, "context": "The POMDP examples we considered are as follows: (A) We experimented with the Cheese maze POMDP example which was introduced in [21] and also studied in [10,20,22].", "startOffset": 153, "endOffset": 163}, {"referenceID": 18, "context": "The POMDP examples we considered are as follows: (A) We experimented with the Cheese maze POMDP example which was introduced in [21] and also studied in [10,20,22].", "startOffset": 153, "endOffset": 163}, {"referenceID": 26, "context": "(B) We considered the Grid POMDP introduced in [30] and also studied in [20,25,22].", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "(B) We considered the Grid POMDP introduced in [30] and also studied in [20,25,22].", "startOffset": 72, "endOffset": 82}, {"referenceID": 21, "context": "(B) We considered the Grid POMDP introduced in [30] and also studied in [20,25,22].", "startOffset": 72, "endOffset": 82}, {"referenceID": 18, "context": "(B) We considered the Grid POMDP introduced in [30] and also studied in [20,25,22].", "startOffset": 72, "endOffset": 82}, {"referenceID": 16, "context": "(C) We experimented with the robot navigation problem POMDP introduced in [20], where we considered both deterministic transition and a randomized version.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "(D) We consider the Hallway example from [20,33,31,2].", "startOffset": 41, "endOffset": 53}, {"referenceID": 29, "context": "(D) We consider the Hallway example from [20,33,31,2].", "startOffset": 41, "endOffset": 53}, {"referenceID": 27, "context": "(D) We consider the Hallway example from [20,33,31,2].", "startOffset": 41, "endOffset": 53}, {"referenceID": 0, "context": "(D) We consider the Hallway example from [20,33,31,2].", "startOffset": 41, "endOffset": 53}, {"referenceID": 0, "context": "(E) We consider the RockSample example from [2,31].", "startOffset": 44, "endOffset": 50}, {"referenceID": 27, "context": "(E) We consider the RockSample example from [2,31].", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "Our experimental results are shown in Table 1, where we compare our approach to RTDP-Bel [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 14, "context": "Other approaches such as SARSOP [18], anytime POMDP [27], ZMDP [31] are for discounted setting, and hence are different from our approach.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "Other approaches such as SARSOP [18], anytime POMDP [27], ZMDP [31] are for discounted setting, and hence are different from our approach.", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "Other approaches such as SARSOP [18], anytime POMDP [27], ZMDP [31] are for discounted setting, and hence are different from our approach.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "RockSample[4,4] {1, 50, 100} 257, 9, 2 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 2, "context": "RockSample[4,4] {1, 50, 100} 257, 9, 2 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 3, "context": "RockSample[5,5] {1, 50, 100} 801, 10, 2 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 3, "context": "RockSample[5,5] {1, 50, 100} 801, 10, 2 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 3, "context": "RockSample[5,7] {1, 50, 100} 3201, 12, 2 4.", "startOffset": 10, "endOffset": 15}, {"referenceID": 5, "context": "RockSample[5,7] {1, 50, 100} 3201, 12, 2 4.", "startOffset": 10, "endOffset": 15}, {"referenceID": 5, "context": "RockSample[7,8] {1, 50, 100} 12545, 13, 2 78.", "startOffset": 10, "endOffset": 15}, {"referenceID": 6, "context": "RockSample[7,8] {1, 50, 100} 12545, 13, 2 78.", "startOffset": 10, "endOffset": 15}, {"referenceID": 16, "context": "We consider two versions of the Hallway example introduced in in [20] and used later in [33,31,2].", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "We consider two versions of the Hallway example introduced in in [20] and used later in [33,31,2].", "startOffset": 88, "endOffset": 97}, {"referenceID": 27, "context": "We consider two versions of the Hallway example introduced in in [20] and used later in [33,31,2].", "startOffset": 88, "endOffset": 97}, {"referenceID": 0, "context": "We consider two versions of the Hallway example introduced in in [20] and used later in [33,31,2].", "startOffset": 88, "endOffset": 97}, {"referenceID": 5, "context": "RockSample[7,8]", "startOffset": 10, "endOffset": 15}, {"referenceID": 6, "context": "RockSample[7,8]", "startOffset": 10, "endOffset": 15}, {"referenceID": 27, "context": "The RockSample problem introduced in [31] and used later in [2] is a scalable problem that models rover science exploration (Figure 10).", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "The RockSample problem introduced in [31] and used later in [2] is a scalable problem that models rover science exploration (Figure 10).", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 52, "endOffset": 57}, {"referenceID": 2, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 52, "endOffset": 57}, {"referenceID": 3, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 84, "endOffset": 89}, {"referenceID": 3, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 84, "endOffset": 89}, {"referenceID": 3, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 116, "endOffset": 121}, {"referenceID": 5, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 116, "endOffset": 121}, {"referenceID": 5, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 153, "endOffset": 158}, {"referenceID": 6, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 153, "endOffset": 158}], "year": 2014, "abstractText": "We consider partially observable Markov decision processes (POMDPs) with a set of target states and every transition is associated with an integer cost. The optimization objective we study asks to minimize the expected total cost till the target set is reached, while ensuring that the target set is reached almost-surely (with probability 1). We show that for integer costs approximating the optimal cost is undecidable. For positive costs, our results are as follows: (i) we establish matching lower and upper bounds for the optimal cost and the bound is double exponential; (ii) we show that the problem of approximating the optimal cost is decidable and present approximation algorithms developing on the existing algorithms for POMDPs with finite-horizon objectives. While the worst-case running time of our algorithm is double exponential, we also present efficient stopping criteria for the algorithm and show experimentally that it performs well in many examples of interest.", "creator": "LaTeX with hyperref package"}}}