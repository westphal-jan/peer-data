{"id": "1411.4166", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2014", "title": "Retrofitting Word Vectors to Semantic Lexicons", "abstract": "We have proposed a simple, effective and fast method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons constructed either automatically or by humans. Retrofitting is used as a post-processing step to improve vector quality and is simpler to use than other approaches that use semantic information while training. It can be used for improving vectors obtained from any word vector training model and performs better than current state-of-the-art approaches to semantic enrichment of word vectors. We validated the applicability of our method across tasks, semantic lexicons, and languages.", "histories": [["v1", "Sat, 15 Nov 2014 17:34:20 GMT  (93kb,D)", "http://arxiv.org/abs/1411.4166v1", "Appeared in NIPS Deep learning and representation learning workshop 2014"], ["v2", "Fri, 19 Dec 2014 04:16:50 GMT  (191kb,D)", "http://arxiv.org/abs/1411.4166v2", "Appeared in NIPS Deep learning and Representation learning workshop 2014"], ["v3", "Sun, 1 Feb 2015 03:13:17 GMT  (199kb,D)", "http://arxiv.org/abs/1411.4166v3", null], ["v4", "Sun, 22 Mar 2015 17:55:20 GMT  (200kb,D)", "http://arxiv.org/abs/1411.4166v4", "Proceedings of NAACL 2015"]], "COMMENTS": "Appeared in NIPS Deep learning and representation learning workshop 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["manaal faruqui", "jesse dodge", "sujay kumar jauhar", "chris dyer", "eduard h hovy", "noah a smith"], "accepted": true, "id": "1411.4166"}, "pdf": {"name": "1411.4166.pdf", "metadata": {"source": "CRF", "title": "Retrofitting Word Vectors to Semantic Lexicons", "authors": ["Manaal Faruqui", "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "emails": ["mfaruqui@cs.cmu.edu", "jessed@cs.cmu.edu", "sjauhar@cs.cmu.edu", "cdyer@cs.cmu.edu", "ehovy@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "These word vectors can in turn be used to identify semantically related word pairs [1, 2] or as features in downstream word processing models [5]. Due to their value as lexical semantic representations, much research is underway to improve the quality of vectors. For example, coexistence statistics have been expanded to take advantage of the internal representations of word sequences. [5] Due to their value as lexical semantic representations based on improving the quality of vectors."}, {"heading": "2 Retrofitting with Semantic Lexicons", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3 Semantic Lexicons during Learning", "text": "Many vector learning methods are presented as maximum probability estimation problems, which are based on gradient-based algorithms to update word representations to improve the log probability iteratively [18, 19, 20, among others]. In these cases, instead of \"retrofitting\" vectors, we can change the learning target with a default that encourages semantically related vectors to be close to each other. In this default, semantic encyclopedias can play the role of a previous on-Q, which we define as follows: p (Q) - \"Exp-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p\" -p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "4 Word Vector Representations", "text": "In some cases, we use pre-formed vectors; in others, we train models on our own data.Latent Semantic Analysis (LSA) We perform latent semantic analysis [4] on a word-word occurrence matrix. In our case, each column / context is a word that could occur in a window of 5 positions around the target word; for scalability, we include words with a frequency of 10 and exclude the 100 most common words. We then replace each entry in the sparse frequency matrix with its mutual information (PMI)."}, {"heading": "5 Semantic Lexicons", "text": "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}, {"heading": "6 Evaluation Benchmarks", "text": "We evaluate the quality of our word vector representations on tasks that test how well they both have semantic and syntactic aspects of representations along with an extrinsic sentiment analysis task.5http: / / www.cis.upenn.edu / / ppdbWord Similarity We evaluate our word representations on a variety of benchmarks that are widely used to measure the word similarity. The first is the WS-353 dataset, which includes 353 pairs of English words assigned by humans. The second benchmark is the RG-65 [29] dataset, which contains 65 pairs of notes."}, {"heading": "7 Experiments & Results", "text": "Using a corpus of English news from WMT-2011,6, we trained word vectors of length 80 using the methods LSA, LBL and SG. After normalization, the corpus comprised 360 million word marks and 180,000 word types. As mentioned in \u00a7 4, we also looked at pre-trained, publicly published vectors trained with GC and Multi. GC vectors are trained on 990 million Wikipedia tokens and are 50-dimensional. Multilingual vectors are also trained on the WMT-2011 data and are 512-dimensional."}, {"heading": "7.1 Retrofitting", "text": "We use Equation 1 to retrofit each of the five sets of word vectors (\u00a7 4), using each of the five graphs derived from semantic lexicon (\u00a7 5). Figure 2 shows the absolute performance changes (Spearman's correlation ratio or accuracy, depending on what is appropriate) for different tasks (as columns) with different semantic lexicon (as rows). Colored bars correspond to the five different groups of input vectors. All lexicon provide improvements in word similarity tasks (the first three columns). FrameNet's performance is weaker, in some cases leading to worse performance (e.g. with SG and multi-vectors). TOEFL task 6http: / / www.statmt.org / wmt11 gives us huge improvements in the sequence of 10 absolute points in the accuracy of pltologies, with the exception of FrameNetzen."}, {"heading": "7.2 Lexicon Information during Training", "text": "Next, we look at the approach to the formation of word vectors with the lexical knowledge included as a precursor (\u00a7 3). Here, we look at length 80 LBL vectors - for which we have our own training implementation, and which performed reasonably well in the retrofitting experiments (\u00a7 7.1) - and the PPDB graphs. We look at the lazy and periodic algorithms described in \u00a7 3. For the lazy method, we update the previous ones of each k = 100,000 words7 and test for different values of priorities7 experiments with k [10000, 50000] yielded almost similar results. The results see Table 1. For laziness, we get the best results, but the method is in most cases not highly sensitive to the value of Ges. For periodic methods, we update the word vectors with equation. 1 of each k [25, 50, 100} million words. Results see Table 1. For laziness, we get the best performance 01 = the value, but the human method is not highly sensitive in most cases."}, {"heading": "7.3 Comparison to Prior Work", "text": "The only publicly available system that incorporates semantic information while training word vectors was approved by Yu & Dredze [35] 8. Their system incorporates semantic relationships between two words, while we train as an additional punitive term similar to our semantic lexicon during the learning model (\u00a7 3).Their model works only with the word2vec tool, so we train embedding using their common model and compare it with the retrofitting model, while using exactly the same training settings specified in [35].9 Table 2 shows the results, although Yu & Dredze sometimes improve beyond the baseline, but we consistently outperform them in all tasks."}, {"heading": "7.4 Multilingual Evaluation", "text": "We tested our method on three other languages: German, French, and Spanish. We used Universal WordNet [36], an automatically constructed multilingual lexical knowledge base based on 8https: / / github.com / Gorov / JointRCM 9These training settings differ from our default setting, and the details are given here on WordNet.10 It contains words that are connected via different lexical relationships with other words both in and between languages. We construct separate diagrams for different languages (i.e., only linking words with other words in the same language) and apply them retroactively. For German, French, and Spanish, we constructed diagrams that each contain about 87K, 49K, and 31K word vertebrae points (with 200K, 100K, and 52K edges, respectively). As there are not many word similarity benchmarks available for other languages, we tested our base values and improved vectors on 38 of each benchmarks for each language [We used 339 for one RMC]."}, {"heading": "8 Conclusion", "text": "We have proposed a simple, effective and quick method called retrofitting to improve word vectors by using the knowledge of word relationships found in semantic lexicon constructed either automatically or by humans. Retrofitting is used as a post-processing step to improve vector quality and is easier to use than other approaches that use semantic information during training. It can be used to improve vectors obtained from any word vector training model and is better than current approaches to semantic enrichment of word vectors. We have confirmed the applicability of our method across tasks, semantic lexicographs and languages."}], "references": [{"title": "Similarity of semantic relations", "author": ["Peter D. Turney"], "venue": "Comput. Linguist.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proceedings of NAACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc. of ACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.4641,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of EACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Dependency-based construction of semantic space models", "author": ["Sebastian Pad\u00f3", "Mirella Lapata"], "venue": "Comput. Linguist.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "The berkeley framenet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe"], "venue": "ACL \u201998,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of NAACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Label propagation and quadratic criterion", "author": ["Yoshua Bengio", "Olivier Delalleau", "Nicolas Le Roux"], "venue": "In Semi-Supervised Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Efficient graph-based semi-supervised learning of structured tagging models", "author": ["Amarnag Subramanya", "Slav Petrov", "Fernando Pereira"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Dipanjan Das", "Slav Petrov"], "venue": "In Proc. of ACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Semi-supervised frame-semantic parsing for unknown predicates", "author": ["Dipanjan Das", "Noah A. Smith"], "venue": "In Proc. of ACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh"], "venue": "In Proceedings of ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS-2010-24,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Lazy sparse stochastic gradient descent for regularized multinomial logistic regression", "author": ["Bob Carpenter"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth Ward Church", "Patrick Hanks"], "venue": "Comput. Linguist.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "Mining the web for synonyms: Pmi-ir versus lsa on toefl", "author": ["Peter D. Turney"], "venue": "In ECML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Matrix computations (3rd ed.)", "author": ["Gene H. Golub", "Charles F. Van Loan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of ACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Placing search in context: the concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1965}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "In Proceedings of ACL,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T. Dumais"], "venue": "Psychological review,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze"], "venue": "In ACL,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Towards a universal wordnet by learning from combined evidence", "author": ["Gerard de Melo", "Gerhard Weikum"], "venue": "In Proceedings of CIKM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Using the structure of a conceptual network in computing semantic relatedness", "author": ["Iryna Gurevych"], "venue": "In Proceedings of IJCNLP,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Comparison of semantic similarity for different languages using the google n-gram corpus and second- order co-occurrence measures", "author": ["Colette Joubarne", "Diana Inkpen"], "venue": "In Proceedings of CAAI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Cross-lingual semantic relatedness using encyclopedic knowledge", "author": ["Samer Hassan", "Rada Mihalcea"], "venue": "In Proc. of EMNLP,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "These word vectors can in turn be used for identifying semantically related word pairs [1, 2] or as features in downstream text processing applications [3].", "startOffset": 87, "endOffset": 93}, {"referenceID": 1, "context": "These word vectors can in turn be used for identifying semantically related word pairs [1, 2] or as features in downstream text processing applications [3].", "startOffset": 87, "endOffset": 93}, {"referenceID": 2, "context": "These word vectors can in turn be used for identifying semantically related word pairs [1, 2] or as features in downstream text processing applications [3].", "startOffset": 152, "endOffset": 155}, {"referenceID": 3, "context": "A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics [4] and using internal representations from neural network models of word sequences [5].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics [4] and using internal representations from neural network models of word sequences [5].", "startOffset": 253, "endOffset": 256}, {"referenceID": 5, "context": "For example, cooccurrence statistics have been expanded to incorporate multilingual context [6, 7, 8] or define context in terms of dependency links [9].", "startOffset": 92, "endOffset": 101}, {"referenceID": 6, "context": "For example, cooccurrence statistics have been expanded to incorporate multilingual context [6, 7, 8] or define context in terms of dependency links [9].", "startOffset": 92, "endOffset": 101}, {"referenceID": 7, "context": "For example, cooccurrence statistics have been expanded to incorporate multilingual context [6, 7, 8] or define context in terms of dependency links [9].", "startOffset": 92, "endOffset": 101}, {"referenceID": 8, "context": "For example, cooccurrence statistics have been expanded to incorporate multilingual context [6, 7, 8] or define context in terms of dependency links [9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "Examples of such resources include WordNet [10], FrameNet [11] and the Paraphrase database [12].", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "Examples of such resources include WordNet [10], FrameNet [11] and the Paraphrase database [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "Examples of such resources include WordNet [10], FrameNet [11] and the Paraphrase database [12].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "To do so, we use an efficient iterative updating method [14, 15, 16, 17].", "startOffset": 56, "endOffset": 72}, {"referenceID": 13, "context": "To do so, we use an efficient iterative updating method [14, 15, 16, 17].", "startOffset": 56, "endOffset": 72}, {"referenceID": 14, "context": "To do so, we use an efficient iterative updating method [14, 15, 16, 17].", "startOffset": 56, "endOffset": 72}, {"referenceID": 15, "context": "To do so, we use an efficient iterative updating method [14, 15, 16, 17].", "startOffset": 56, "endOffset": 72}, {"referenceID": 19, "context": "In the first, we apply adaptive gradient descent (AdaGrad [21]), using the sum of gradients of the log-likelihood (given by the extant vector learning model) and the logprior (from Eq.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "2 is linear in the vocabulary size n, we use lazy updates [22] every k words during training.", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "Latent Semantic Analysis (LSA) We perform latent semantic analysis [4] on a word-word cooccurrence matrix.", "startOffset": 67, "endOffset": 70}, {"referenceID": 21, "context": "We then replace every entry in the sparse frequency matrix by its pointwise mutual information (PMI) [23, 24] resulting in X.", "startOffset": 101, "endOffset": 109}, {"referenceID": 22, "context": "We then replace every entry in the sparse frequency matrix by its pointwise mutual information (PMI) [23, 24] resulting in X.", "startOffset": 101, "endOffset": 109}, {"referenceID": 23, "context": "We factorize the matrix X = U\u03a3V> using singular value decomposition (SVD) [25].", "startOffset": 74, "endOffset": 78}, {"referenceID": 18, "context": "Skip-Gram Vectors (SG) The word2vec tool [20] is fast and currently in wide use.", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "Global Context Vectors (GC) These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features [26].", "startOffset": 161, "endOffset": 165}, {"referenceID": 7, "context": "Multilingual Vectors (Multi) Faruqui and Dyer [8] learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora.", "startOffset": 46, "endOffset": 49}, {"referenceID": 17, "context": "Log-bilinear Vectors (LBL) The log bilinear language model [19] predicts a word token w\u2019s vector given the set of words in its context (h), also represented as vectors:", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "Since it is costly to renormalize over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model [19] using AdaGrad [21] with a learning rate of 0.", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": "Since it is costly to renormalize over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model [19] using AdaGrad [21] with a learning rate of 0.", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "WordNet WordNet [10] is a large human-constructed semantic lexicon of English words.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "FrameNet FrameNet [11, 27] is a rich linguistic resource containing information about lexical and predicate-argument semantics in English.", "startOffset": 18, "endOffset": 26}, {"referenceID": 25, "context": "The first one is the WS-353 dataset [28] containing 353 pairs of English words that have been assigned similarity ratings by humans.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "The second benchmark is the RG-65 [29] dataset that contain 65 pairs of nouns.", "startOffset": 34, "endOffset": 38}, {"referenceID": 27, "context": "Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset [30] that contains 3,000 word pairs which have been sampled from words that occur at least 700 times in a large web corpus.", "startOffset": 114, "endOffset": 118}, {"referenceID": 28, "context": "al [32] present a syntactic relation dataset composed of analogous word pairs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "We use the vector offset method [20], computing q = qa\u2212 qb + qc and returning the vector from Q which has the highest cosine similarity to q.", "startOffset": 32, "endOffset": 36}, {"referenceID": 29, "context": "The dataset we use on this task is the TOEFL dataset [33] which consists of a list of target words that appear with 4 candidate lexical substitutes each.", "startOffset": 53, "endOffset": 57}, {"referenceID": 30, "context": "al, [34] created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.", "startOffset": 4, "endOffset": 8}, {"referenceID": 31, "context": "Table 2: Semantic enrichment of word2vec CBOW vectors using Yu & Dredze (2014) [35] and retrofitting model using PPDB.", "startOffset": 79, "endOffset": 83}, {"referenceID": 31, "context": "The only publicly available system that incorporates semantic information while training word vectors has been released by Yu & Dredze [35]8.", "startOffset": 135, "endOffset": 139}, {"referenceID": 31, "context": "Their model works only with the word2vec tool, thus we train embeddings using their joint model and compare against the retrofitting model while using exactly the same training settings specified in [35].", "startOffset": 199, "endOffset": 203}, {"referenceID": 32, "context": "We used the Universal WordNet [36], an automatically constructed multilingual lexical knowledge base based", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "We used RG-65 [37], WS-353 [38] and MC-30 [39] for German, French and Spanish respectively.", "startOffset": 14, "endOffset": 18}, {"referenceID": 34, "context": "We used RG-65 [37], WS-353 [38] and MC-30 [39] for German, French and Spanish respectively.", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "We used RG-65 [37], WS-353 [38] and MC-30 [39] for German, French and Spanish respectively.", "startOffset": 42, "endOffset": 46}], "year": 2017, "abstractText": "Vector space word representations are typically learned using only co-occurrence statistics from text corpora. Although such statistics are informative, they disregard easily accessible (and often carefully curated) information archived in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a technique to leverage both distributional and lexicon-derived evidence to obtain better representations. We run belief propagation on a word type graph constructed from word similarity information from lexicons to encourage connected (related) words to have similar representations, and also to be close to the unsupervised vectors. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, using several different underlying word vector models, we obtain substantially improved vectors and consistently outperform existing approaches of incorporating semantic knowledge in word vectors.", "creator": "LaTeX with hyperref package"}}}