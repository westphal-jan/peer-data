{"id": "1411.4503", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2014", "title": "Outlier-Robust Convex Segmentation", "abstract": "We derive a convex optimization problem for the task of segmenting sequential data, which explicitly treats presence of outliers. We describe two algorithms for solving this problem, one exact and one a top-down novel approach, and we derive a consistency results for the case of two segments and no outliers. Robustness to outliers is evaluated on two real-world tasks related to speech segmentation. Our algorithms outperform baseline segmentation algorithms.", "histories": [["v1", "Mon, 17 Nov 2014 14:59:25 GMT  (127kb,D)", "https://arxiv.org/abs/1411.4503v1", "Accepted to AAAI-15, this version includes the appendix/supplementary material referenced in the AAAI-15 submission"], ["v2", "Tue, 18 Nov 2014 07:59:33 GMT  (127kb,D)", "http://arxiv.org/abs/1411.4503v2", "* Accepted to AAAI-15, this version includes the appendix/supplementary material referenced in the AAAI-15 submission, as well as color figures * This version include some minor typos correction"]], "COMMENTS": "Accepted to AAAI-15, this version includes the appendix/supplementary material referenced in the AAAI-15 submission", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["itamar katz", "koby crammer"], "accepted": true, "id": "1411.4503"}, "pdf": {"name": "1411.4503.pdf", "metadata": {"source": "CRF", "title": "Outlier-Robust Convex Segmentation", "authors": ["Itamar Katz", "Koby Crammer"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Segmentation of sequential data, also known as changepoint detection, is a fundamental problem in the field of unattended learning and has applications in various fields such as language processing (Brent 1999; Qiao, Shimomura, and Minematsu 2008; Shriberg et al. 2000), word processing (Beeferman, Berger, and Lafferty 1999), bioinformatics (Olshen et al. 2004), and network anomaly detection (Le \u0301 vyLeduc and Roueff 2009), to name but a few. We are interested in formulating the segmentation task as a convective optimization problem that avoids problems such as local minima or sensitivity to initializations, and explicitly integrating robustness into outliers."}, {"heading": "2 Outlier-Robust Convex Segmentation", "text": "Segmentation is the task of subdividing a sequence of n data samples = zi = zi = zi = q = q = 1 into K-groups of consecutive samples or segments, so that each group is homogeneous in terms of some criteria. A common selection of such a criterion often involves minimizing the squared euclidean distance of a sample to some representative samples. This criterion is highly sensitive to outliers and in fact, as we show empirically below, the performance of segmentation algorithms is drastically degraded when the data is contaminated with outliers. It is therefore desirable to integrate the robustness of outliers into the model by identifying some of the input samples xi as outliers where we do not need proximity to these samples. Toar Xiv: 141 1.45 03v2 [cs.LG] 1 8N ov2 014this we suggest to minimize them:"}, {"heading": "2.1 Algorithms", "text": "The decoupling between \u00b5 and z allows us to optimize the problem alternately. (1) We call these algorithms Outlier-Robust Convex Sequential (ORCS) segmentation. (1) We call these algorithms Outlier-Robust Convex Sequential (ORCS) segmentation. (1) The definition of the proximal operator is evaluated at each coordinate for which a closed solution exists. (1) The target as a function of z is divisible across both coordinates and data samples, and the proximal operator is the shrink and threshold operator evaluated at each coordinate. (2) In this case, the target is divisible through data samples, and the proximal operator is calculated. (2) However, we are interested in the erosion of some of the zi as a whole, so we set it as a whole. (2) The target is divisible via data samples, and the proximal operator is calculated."}, {"heading": "2.2 Analysis of Lemma 1 for K=2", "text": "We now assume that the probability that the solution given by Lemma 1 does not recognize the correct limit is higher than the actual one."}, {"heading": "3 Empirical Study", "text": "We compared the unweighted (TD-ORCS) and weighted (WTD-ORCS) versions of our top-down algorithm with LASSO and the group merged LARS 2 of (Bleakley and Vert 2011), which are based on the reformulation of Equation (3) as a group LASSO regression and solve either accurately or roughly the optimization problem. Both the TD-ORCS and LARS algorithms have a complexity of O (nK). We also report on the results of a Bayesian change point recognition algorithm (BCP) as formulated by (Erdman and Emerson 2008)."}, {"heading": "3.1 Biphones subsequences segmentation", "text": "In this experiment, we used the TIMIT corpus (Garofolo and others 1988), which comprises 4,620 comments with commented phoneme boundaries amounting to more than 170,000 boundaries. Audio was divided into frames of 16ms duration and 1ms hop length, each represented by 13 MFCC coefficients. The task is to find the boundary between two consecutive phonemes (biphones), and the performance is evaluated as the mean absolute distance between the detected and the ground-truth boundaries. Since the number of segments is isK = 2, the ORCS and the TD-ORCS algorithms are essentially identical, and the same is true for LASSO and LARS. Outliers were incorporated by adding short (0.25 frame lengths) synthetic transients to the audio source. The percentage of outliers reflects the percentage of contaminated frames."}, {"heading": "3.2 Radio show segmentation", "text": "In this experiment, we used a 35-minute, hand-annotated audio recording of a radio talk show, consisting of various sections such as the opening title, the monologues, dialogues, and songs. A determined segment boundary is considered a2http: / / cbio.ensmp.fr / jvert / svn / GFLseg / html / true positive when it falls around a ground-truth boundary within a tolerance window of two frames. Segmentation quality is usually measured with the F measurement defined as 2pr / (p + r), where p is the precision and r the callback. Instead, we used the R measurement introduced by (Ra, Laine, and Altosaar 2009), which is more robust than over-segmentation. It is defined as R, 1 \u2212 0.5 (s1 \u2212 s2), where s1, 2 (s1 \u2212 r) 2 + (2), 2 \u2212 r, 2 \u2212 r, 2 \u2212 \u2212 p \u2212 p, 2 \u2212 p \u2212 p \u2212 p."}, {"heading": "4 Related Work and Conclusion", "text": "There is a large amount of literature on the detection of change points, see for example (Basseville and Nikiforov 1993; Brodsky and Darkhovsky 1993). Optimal segmentation can be found using dynamic programming (Lavielle and Teyssie, both 2006); however, the complexity of this approach is square in the number of samples n, and may therefore be unfeasible for large datasets. Some approaches that achieve complexity linearly in n (Levy-leduc and others 2007; Killick, Fearnhead, and Eckley 2012) deal only with one-dimensional data. Some related work deals with objective equilibrium. (3) We presented Binocular Dimensions in Sec. 2.1. It has been proposed to reformulate Eq. (3) for the one-dimensional case as a LASSO regression problem."}, {"heading": "A Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Lemma 1", "text": "The proof: Our starting point is the following problem, which makes further analysis easier for us. Lemma 4 assumes that there is an optimal solution to the equation (3), and therefore we also know to which segment each sample belongs. If we replace all samples in a segment with the mean of these samples, the optimal solution will not change. The proof appears in Sec. A.5. We now analyze the transition of the solution to Equation. (3) from K = 1 to K = 2 segments. During the analysis, we use the fact that the solution path is continuous, as previously shown in another context by (Chi and Lange 2013). We denounce the value of the equation at the split point, and we assume that for the two segments we have n1 samples in the first segment and n2 = n \u2212 n1 samples in the second segment."}, {"heading": "A.2 Proof of Lemma 2", "text": "11: For K = 2, the unforced optimization problem Eq. (5) is equivalent to the minimization of 1, 2, 2, 3, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11"}, {"heading": "A.3 Proof of Eq. (7)", "text": "Evidence: The random variable Ym is the difference between the empirical means of two subsequences generated by splitting by n1 + m samples. < n < n < n < n < p (4) as optimization over m: g (m), (n1 + m) (n2 \u2212 m) n | Ym |, m \u00b2 = argmax m {g (m)}, where m \u00b2 [\u2212 n1, n2 \u2212 1] is used. Without loss of generality, we deal with the case in which m + 0. Note that m 6 = 0 if g (0) < g (m) for some m > 0. The probability of this event is: P (g (0) < g (m) < g (m), g (m) = (12) P (n1n1n2 n2 n2 n."}, {"heading": "A.4 Proof of Theorem 3", "text": "Proof: Using Equation (7) and the compound limit, we obtain P (m \u0445 \u2265 m0) \u2264 P (m \u2265 m0: g (0) < g (m)) \u2264 n2 \u2211 m = m0P (g (0) < g (m)) \u2264 n2P (g (0) < g (m0)) \u2264 2n2 exp (\u2212 Cm0) (?) \u2264 \u03b4, where (?) results from the definition of m0."}, {"heading": "A.5 Proof of Lemma 4", "text": "Proof: Consider a segment of n0 data samples and call it \u00b50 as the center of that segment. The mean value of the segment is given by x-0 = 1n0 \u2211 xi-0 xi. The contribution of this segment to the first term in the target equation. (3) is derived from the 12-fold contribution to the target xi-0-0 = 1 2-2xTi-0 + 0-2). (15) Now, if we replace x-0 for each sample xi in this segment, the contribution to the target xi-0 2-2x0-0-2 = n0-2 (2-2xTi-2-2xT0-0 + 0-2). (16) It is straightforward to show that equation. (15) and equation. (16) differ by a constant that depends only on the data samples xi-2xT0 + 1-2xT0-0-2, not on \u00b50-2. Since this argument applies to all individual results, we do not replace the results with a constant (15)."}], "references": [{"title": "Convex optimization with sparsityinducing norms. Optimization for Machine Learning 19\u201353", "author": ["Bach"], "venue": null, "citeRegEx": "Bach,? \\Q2011\\E", "shortCiteRegEx": "Bach", "year": 2011}, {"title": "I", "author": ["M. Basseville", "Nikiforov"], "venue": "V.", "citeRegEx": "Basseville and Nikiforov 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems", "author": ["Beck", "A. Teboulle 2009a] Beck", "M. Teboulle"], "venue": "Image Processing, IEEE Transactions on", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Beck", "A. Teboulle 2009b] Beck", "M. Teboulle"], "venue": "SIAM J. Img. Sci", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "Statistical models for text segmentation. Machine learning 34(1-3):177\u2013210", "author": ["Berger Beeferman", "D. Lafferty 1999] Beeferman", "A. Berger", "J. Lafferty"], "venue": null, "citeRegEx": "Beeferman et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Beeferman et al\\.", "year": 1999}, {"title": "and Vert", "author": ["K. Bleakley"], "venue": "J.-P.", "citeRegEx": "Bleakley and Vert 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "M", "author": ["Brent"], "venue": "R.", "citeRegEx": "Brent 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "B", "author": ["B.E. Brodsky", "Darkhovsky"], "venue": "S.", "citeRegEx": "Brodsky and Darkhovsky 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "and Lange", "author": ["E.C. Chi"], "venue": "K.", "citeRegEx": "Chi and Lange 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "J", "author": ["C. Erdman", "Emerson"], "venue": "W.", "citeRegEx": "Erdman and Emerson 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "G", "author": ["P.A. Forero", "V. Kekatos", "Giannakis"], "venue": "B.", "citeRegEx": "Forero. Kekatos. and Giannakis 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "J", "author": ["Garofolo"], "venue": "S., et al.", "citeRegEx": "Garofolo and others 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "and Binefa", "author": ["C. Gracia"], "venue": "X.", "citeRegEx": "Gracia and Binefa 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimal detection of changepoints with a linear computational cost", "author": ["Fearnhead Killick", "R. Eckley 2012] Killick", "P. Fearnhead", "I. Eckley"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Killick et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Killick et al\\.", "year": 2012}, {"title": "and Teyssi\u00e8re", "author": ["M. Lavielle"], "venue": "G.", "citeRegEx": "Lavielle and Teyssi\u00e8re 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Catching change-points with lasso", "author": ["Levy-leduc", "C others 2007] Levy-leduc"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy.leduc and Levy.leduc,? \\Q2007\\E", "shortCiteRegEx": "Levy.leduc and Levy.leduc", "year": 2007}, {"title": "and Roueff", "author": ["C. L\u00e9vy-Leduc"], "venue": "F.", "citeRegEx": "L\u00e9vy.Leduc and Roueff 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "G", "author": ["G. Mateos", "Giannakis"], "venue": "B.", "citeRegEx": "Mateos and Giannakis 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["Olshen"], "venue": "B.; Venkatraman, E.; Lucito, R.; and Wigler, M.", "citeRegEx": "Olshen et al. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A study on unsupervised phoneme segmentation and its application to automatic evaluation of shadowed utterances", "author": ["Luo Qiao", "Y. Minematsu 2012] Qiao", "D. Luo", "N. Minematsu"], "venue": "Technical report", "citeRegEx": "Qiao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Qiao et al\\.", "year": 2012}, {"title": "Unsupervised optimal phoneme segmentation: Objectives, algorithm and comparisons", "author": ["Shimomura Qiao", "Y. Minematsu 2008] Qiao", "N. Shimomura", "N. Minematsu"], "venue": "In ICASSP", "citeRegEx": "Qiao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Qiao et al\\.", "year": 2008}, {"title": "U", "author": ["R\u00e4s\u00e4nen, O.J.", "Laine"], "venue": "K.; and Altosaar, T.", "citeRegEx": "R\u00e4s\u00e4nen. Laine. and Altosaar 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "L", "author": ["Rudin"], "venue": "I.; Osher, S.; and Fatemi, E.", "citeRegEx": "Rudin. Osher. and Fatemi 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication 32(1\u20132)", "author": ["Shriberg"], "venue": null, "citeRegEx": "Shriberg,? \\Q2000\\E", "shortCiteRegEx": "Shriberg", "year": 2000}, {"title": "and Lin", "author": ["M. Yuan"], "venue": "Y.", "citeRegEx": "Yuan and Lin 2006", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [], "year": 2014, "abstractText": "We derive a convex optimization problem for the task of segmenting sequential data, which explicitly treats presence of outliers. We describe two algorithms for solving this problem, one exact and one a top-down novel approach, and we derive a consistency results for the case of two segments and no outliers. Robustness to outliers is evaluated on two real-world tasks related to speech segmentation. Our algorithms outperform baseline segmentation algorithms.", "creator": "LaTeX with hyperref package"}}}