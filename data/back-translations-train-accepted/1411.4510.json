{"id": "1411.4510", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2014", "title": "Parallel Gaussian Process Regression for Big Data: Low-Rank Representation Meets Markov Approximation", "abstract": "The expressive power of a Gaussian process (GP) model comes at a cost of poor scalability in the data size. To improve its scalability, this paper presents a low-rank-cum-Markov approximation (LMA) of the GP model that is novel in leveraging the dual computational advantages stemming from complementing a low-rank approximate representation of the full-rank GP based on a support set of inputs with a Markov approximation of the resulting residual process; the latter approximation is guaranteed to be closest in the Kullback-Leibler distance criterion subject to some constraint and is considerably more refined than that of existing sparse GP models utilizing low-rank representations due to its more relaxed conditional independence assumption (especially with larger data). As a result, our LMA method can trade off between the size of the support set and the order of the Markov property to (a) incur lower computational cost than such sparse GP models while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale. Interestingly, varying the Markov order produces a spectrum of LMAs with PIC approximation and full-rank GP at the two extremes. An advantage of our LMA method is that it is amenable to parallelization on multiple machines/cores, thereby gaining greater scalability. Empirical evaluation on three real-world datasets in clusters of up to 32 computing nodes shows that our centralized and parallel LMA methods are significantly more time-efficient and scalable than state-of-the-art sparse and full-rank GP regression methods while achieving comparable predictive performances.", "histories": [["v1", "Mon, 17 Nov 2014 15:31:04 GMT  (777kb,D)", "http://arxiv.org/abs/1411.4510v1", "29th AAAI Conference on Artificial Intelligence (AAAI 2015), Extended version with proofs, 10 pages"]], "COMMENTS": "29th AAAI Conference on Artificial Intelligence (AAAI 2015), Extended version with proofs, 10 pages", "reviews": [], "SUBJECTS": "stat.ML cs.DC cs.LG", "authors": ["kian hsiang low", "jiangbo yu", "jie chen", "patrick jaillet"], "accepted": true, "id": "1411.4510"}, "pdf": {"name": "1411.4510.pdf", "metadata": {"source": "META", "title": "Parallel Gaussian Process Regression for Big Data: Low-Rank Representation Meets Markov Approximation", "authors": ["Kian Hsiang Low", "Jiangbo Yu", "Jie Chen", "Patrick Jaillet"], "emails": ["yujiang}@comp.nus.edu.sg,", "chenjie@smart.mit.edu,", "jaillet@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Gaussian process (GP) of the fullrank GP (FGP) model are well-suited for slowing functions with large data. (1) There is limited scalability in the size of the data, which can limit their practical application to small data. (1) To improve their scalability, two families of sparse GP regression methods have been proposed: (a) Low-Rank approach to representations (Hensman, Fusi and Lawrence 2013). (2) Kian Hsiang Low and Jiangbo Yu are co-first authors. (2) Copyright c \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). (2) All rights reserved.La-zaro-Gredilla et al. 2010; Quin the onero-Candela and Ghahramani 2005; Snelson and Ghahramani 2005) of the fullrank GP (FGP) model are well suited for the large functions they are complemented by the size of the reference data."}, {"heading": "2 Full-Rank Gaussian Process Regression", "text": "Let X be a quantity representing the input domain in such a way that each input x-X denotes a d-dimensional characteristic vector and is associated with a realised output value yx (random output variable Yx) when observed (unnoticed). Let {Yx} x-X denote a GP, that is, any finite subset of {Yx} x-X follows a multivariate Gaussian distribution. Then the GP is fully determined by its previous averages \u00b5x, E [Yx] and covariance \u0441xx, \"cov [Yx, Yx\"] for all x, x \"X.\" The assumption of a column vector yD of realised outputs is observed for some set D-X inputs, a full GP (FGP) model can perform probable regression by determining a Gaussian posterior / predictive distribution method UKDUK- / UK- / UK- / UK- / UK- / UK- DUK- / UK- / UK- / UK- / UK- / UK- DUK- / UK- / UK- DUK- / UK- DUK- / UK- DUK- / UK- DUK- / UK- DUK-, whereby the DUK- / FGP (FGP) model can perform a probable regression by providing a Gaussian posterior / predictive distribution method UK- DUK- / UK- / UK- DUK- / UK- DUK- / UK- DUK- / UK- / UK- DUK- / UK- DUK- DUK- / UK- / UK- / UK- DUK- / UK- DUK- / UK- DUK- / UK- DUK- / UK- / UK- DUK- / UK- DUK- DUK-, whereby all components are used for the"}, {"heading": "3 Low-Rank-cum-Markov Approximation", "text": "\"It is not as if it is a pure..,\" he says. \"It is as if it were a pure.\" \"It is as if\". \"\" It is as if \".\" \"It is as if\". \"\" It is as if \".\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"\" It is as if. \"\" \"It is as if.\" \"\" It is as if. \"\" \"It is as if.\" \"\" It is as if. \"\" \"\". \"\" \"\" \"It is as if.\" \"\" \"It is as if.\" \"\" It is. \"\" \"It is.\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \".\" \"\" \"\" \"..\" \"\" \".\" \"\" \"\". \"\" \"\" \"\" \"..\" \"\" \"\" \".\" \"\" \".\" \"\" \"\""}, {"heading": "4 Experiments and Discussion", "text": "This section evaluates empirically the predictive performance and scalability of our proposed centralized and parallel LMA methods against those of the state-of-the-art centralized PIC (Snelson and Ghahramani 2007), parallel PIC (Chen et al. 2013), sparse spectrum GP (SSGP) (La \u0301 zaroGredilla et al. 2010), and FGP on two real world datasets: (a) The SARCOS datasets (Vijayakumar, D'Souza, and Schaal 2005) of size 48933 are determined using an inverse dynamic problem for a 7 degree freedom SARCOS robotic arm. Each input is specified by a vector of common positions, speeds, and accelerations.The output corresponds to one of the 7 common torques. (b) The AIMPEAK datasets (Chen et al. 2013) of size 41850 include traffic flows (7 km / 7 km) along urban networks."}, {"heading": "5 Conclusion", "text": "This paper describes an LMA method that utilizes the dual computational advantages derived from supplementing the low-level covariance matrix approximation, based on the dense residual covariance matrix approximation based on the Markov assumption. As a result, the LMA can perform a more relaxed conditional approach to independence (especially with larger data) than many existing sparse GP regression methods, which use low-level representations, the latter having a sparse residual covariance matrix approximation. Empirical results have shown that our centralized (parallel) LMA method is much more scalable and time-efficient than the centralized PIC (parallel PIC) and SSGP, while achieving a comparable predictive approach. In our future work, we plan a technique for automatically determining the \"optimal\" support set size and Markov sequence, as well as the Lawrence variant \"at any time."}, {"heading": "A Proof of Theorem 1", "text": "DKL (RDD, RDD) + DKL (RDD, R) = 12 (tr (RDDR \u2212 1 DD) \u2212 log | RDDR \u2212 1 DD | \u2212 | D |) + 12 (tr (RDDR \u2212 1) \u2212 log | RDDR \u2212 1 | \u2212 | D |) = 12 (tr (RDDR \u2212 1) = DKL (RDD \u2212 1)) The second equality is based on tr (RDDR \u2212 1 DD) = tr (RDDR \u2212 1) = | D |, which follows from the observations that the blocks within the B block bands of RDD and RDD are equal and R \u2212 1 DD (RDDR \u2212 1 DD) = tr (I | D |) = D |."}, {"heading": "B Proof of Theorem 2", "text": "The following problem is necessary to derive our main result here. It shows that the rarity of B block-banded R = > D = > DD (Proposition 1) extends to the cholesky factor (Figure 3): Lemma 1 Let R \u2212 1DD, U > U where the cholesky factor U = [Umn] m, n = 1,..., M is an upper triangular matrix (Figure 3). Then, Umn = 0 if m \u2212 n > 0 or n \u2212 m > B. In addition, for m = 1,., M, Umm = cholesky (R) and UBm, [Umn] n = m + 1,..., min (m) = UmmRDDmDmDBmR \u2212 1."}, {"heading": "C Parallel Computation of \u03a3DmU and \u03a3DBmU", "text": "DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD"}, {"heading": "D Toy Example", "text": "In our LMA method, the settings M = 4, Markov order B = 1, support set (black) size | S | = 16 and training data (red) size | D | = 400, so that x < \u2212 2.5, if x-D1, \u2212 2.5 \u2264 x < 0, if x-D2, 0 \u2264 x < 2.5, if x-D3, x-D3, x-D5, and | D1 | = | D2 | = | D3 | = 100. The hyperparameters estimated with maximum probability are length scale '= 1.2270, \u03c3n = 0.0939, \u03c3s = 0.6836 and \u00b5x = 1.1072. Fig. 6 shows that the rear / predictive center curve (blue curve) of our LMA method has no discontinuity / jump. The area surrounding the green curves is the 95% confidence region. In contrast, the posterior / predictive center curve (no) has no disturbances."}], "references": [{"title": "Daily mean sea level pressure reconstructions for the European-North Atlantic region for the period 1850-2003", "author": ["T.J. Ansell et al."], "venue": "J. Climate 19(12):2717\u20132742.", "citeRegEx": "al.,? 2006", "shortCiteRegEx": "al.", "year": 2006}, {"title": "Block matrices with L-block-banded inverse: Inversion algorithms", "author": ["A. Asif", "J.M.F. Moura"], "venue": "IEEE Trans. Signal Processing 53(2):630\u2013642.", "citeRegEx": "Asif and Moura,? 2005", "shortCiteRegEx": "Asif and Moura", "year": 2005}, {"title": "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms", "author": ["N. Cao", "K.H. Low", "J.M. Dolan"], "venue": "Proc. AAMAS.", "citeRegEx": "Cao et al\\.,? 2013", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan", "A. Oran", "P. Jaillet", "J.M. Dolan", "G.S. Sukhatme"], "venue": "Proc. UAI, 163\u2013173.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Parallel Gaussian process regression with low-rank covariance matrix approximations", "author": ["J. Chen", "N. Cao", "K.H. Low", "R. Ouyang", "C.K.-Y. Tan", "P. Jaillet"], "venue": "Proc. UAI, 152\u2013161.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan"], "venue": "Proc. RSS.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Cooperative aquatic sensing using the telesupervised adaptive ocean sensor fleet", "author": ["J.M. Dolan", "G. Podnar", "S. Stancliff", "K.H. Low", "A. Elfes", "J. Higinbotham", "J.C. Hosler", "T.A. Moisan", "J. Moisan"], "venue": "Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water", "citeRegEx": "Dolan et al\\.,? 2009", "shortCiteRegEx": "Dolan et al\\.", "year": 2009}, {"title": "Covariance tapering for interpolation of large spatial datasets", "author": ["R. Furrer", "M.G. Genton", "D. Nychka"], "venue": "JCGS 15(3):502\u2013523.", "citeRegEx": "Furrer et al\\.,? 2006", "shortCiteRegEx": "Furrer et al\\.", "year": 2006}, {"title": "Gaussian processes for big data", "author": ["J. Hensman", "N. Fusi", "N. Lawrence"], "venue": "Proc. UAI, 282\u2013290.", "citeRegEx": "Hensman et al\\.,? 2013", "shortCiteRegEx": "Hensman et al\\.", "year": 2013}, {"title": "Active learning is planning: Nonmyopic -Bayesoptimal active learning of Gaussian processes", "author": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "Proc. ECML/PKDD Nectar Track, 494\u2013498.", "citeRegEx": "Hoang et al\\.,? 2014a", "shortCiteRegEx": "Hoang et al\\.", "year": 2014}, {"title": "Nonmyopic -Bayes-optimal active learning of Gaussian processes", "author": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "Proc. ICML, 739\u2013747.", "citeRegEx": "Hoang et al\\.,? 2014b", "shortCiteRegEx": "Hoang et al\\.", "year": 2014}, {"title": "Sparse spectrum Gaussian process regression", "author": ["M. L\u00e1zaro-Gredilla", "J. Qui\u00f1onero-Candela", "C.E. Rasmussen", "A.R. Figueiras-Vidal"], "venue": "JMLR 11:1865\u20131881.", "citeRegEx": "L\u00e1zaro.Gredilla et al\\.,? 2010", "shortCiteRegEx": "L\u00e1zaro.Gredilla et al\\.", "year": 2010}, {"title": "Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing", "author": ["K.H. Low", "J. Chen", "J.M. Dolan", "S. Chien", "D.R. Thompson"], "venue": "Proc. AAMAS, 105\u2013112.", "citeRegEx": "Low et al\\.,? 2012", "shortCiteRegEx": "Low et al\\.", "year": 2012}, {"title": "Recent advances in scaling up Gaussian process predictive models for large spatiotemporal data", "author": ["K.H. Low", "J. Chen", "T.N. Hoang", "N. Xu", "P. Jaillet"], "venue": "Proc. DyDESS.", "citeRegEx": "Low et al\\.,? 2014a", "shortCiteRegEx": "Low et al\\.", "year": 2014}, {"title": "Generalized online sparse Gaussian processes with application to persistent mobile robot localization", "author": ["K.H. Low", "N. Xu", "J. Chen", "K.K. Lim", "E.B. \u00d6zg\u00fcl"], "venue": "Proc. ECML/PKDD Nectar Track, 499\u2013503.", "citeRegEx": "Low et al\\.,? 2014b", "shortCiteRegEx": "Low et al\\.", "year": 2014}, {"title": "Adaptive multi-robot wide-area exploration and mapping", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 23\u201330.", "citeRegEx": "Low et al\\.,? 2008", "shortCiteRegEx": "Low et al\\.", "year": 2008}, {"title": "Informationtheoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. ICAPS.", "citeRegEx": "Low et al\\.,? 2009", "shortCiteRegEx": "Low et al\\.", "year": 2009}, {"title": "Active Markov information-theoretic path planning for robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 753\u2013760.", "citeRegEx": "Low et al\\.,? 2011", "shortCiteRegEx": "Low et al\\.", "year": 2011}, {"title": "Multirobot active sensing of non-stationary Gaussian processbased environmental phenomena", "author": ["R. Ouyang", "K.H. Low", "J. Chen", "P. Jaillet"], "venue": "Proc. AAMAS.", "citeRegEx": "Ouyang et al\\.,? 2014", "shortCiteRegEx": "Ouyang et al\\.", "year": 2014}, {"title": "Domain decomposition approach for fast Gaussian process regression of large spatial data sets", "author": ["C. Park", "J.Z. Huang", "Y. Ding"], "venue": "JMLR 12:1697\u20131728.", "citeRegEx": "Park et al\\.,? 2011", "shortCiteRegEx": "Park et al\\.", "year": 2011}, {"title": "Telesupervised remote surface water quality sensing", "author": ["G. Podnar", "J.M. Dolan", "K.H. Low", "A. Elfes"], "venue": "Proc. IEEE Aerospace Conference.", "citeRegEx": "Podnar et al\\.,? 2010", "shortCiteRegEx": "Podnar et al\\.", "year": 2010}, {"title": "A unifying view of sparse approximate Gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": "JMLR 6:1939\u20131959.", "citeRegEx": "Qui\u00f1onero.Candela and Rasmussen,? 2005", "shortCiteRegEx": "Qui\u00f1onero.Candela and Rasmussen", "year": 2005}, {"title": "Sparse Gaussian processes using pseudo-inputs", "author": ["E. Snelson", "Z. Ghahramani"], "venue": "Proc. NIPS.", "citeRegEx": "Snelson and Ghahramani,? 2005", "shortCiteRegEx": "Snelson and Ghahramani", "year": 2005}, {"title": "Local and global sparse Gaussian process approximations", "author": ["E. Snelson", "Z. Ghahramani"], "venue": "Proc. AISTATS.", "citeRegEx": "Snelson and Ghahramani,? 2007", "shortCiteRegEx": "Snelson and Ghahramani", "year": 2007}, {"title": "Incremental online learning in high dimensions", "author": ["S. Vijayakumar", "A. D\u2019Souza", "S. Schaal"], "venue": "Neural Comput", "citeRegEx": "Vijayakumar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vijayakumar et al\\.", "year": 2005}, {"title": "GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model", "author": ["N. Xu", "K.H. Low", "J. Chen", "K.K. Lim", "E.B. \u00d6zg\u00fcl"], "venue": "Proc. AAAI, 2585\u20132592.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Hierarchical Bayesian nonparametric approach to modeling and learning the wisdom of crowds of urban traffic route planning agents", "author": ["J. Yu", "K.H. Low", "A. Oran", "P. Jaillet"], "venue": "Proc. IAT, 478\u2013485.", "citeRegEx": "Yu et al\\.,? 2012", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Recent sparse GP regression methods (Chen et al. 2013; Snelson and Ghahramani 2007) have unified approaches from the two families described above to harness their complementary modeling and predictive capabilities (hence, eliminating their deficiencies) while retaining their computational advantages.", "startOffset": 36, "endOffset": 83}, {"referenceID": 23, "context": "Recent sparse GP regression methods (Chen et al. 2013; Snelson and Ghahramani 2007) have unified approaches from the two families described above to harness their complementary modeling and predictive capabilities (hence, eliminating their deficiencies) while retaining their computational advantages.", "startOffset": 36, "endOffset": 83}, {"referenceID": 4, "context": ", (Chen et al. 2013; Snelson and Ghahramani 2007)) or number of spectral points (L\u00e1zaro-Gredilla et al.", "startOffset": 2, "endOffset": 49}, {"referenceID": 23, "context": ", (Chen et al. 2013; Snelson and Ghahramani 2007)) or number of spectral points (L\u00e1zaro-Gredilla et al.", "startOffset": 2, "endOffset": 49}, {"referenceID": 11, "context": "2013; Snelson and Ghahramani 2007)) or number of spectral points (L\u00e1zaro-Gredilla et al. 2010) as the varying parameter while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale.", "startOffset": 65, "endOffset": 94}, {"referenceID": 4, "context": "Interestingly, varying the Markov order produces a spectrum of LMAs with the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007) and FGP at the two extremes.", "startOffset": 131, "endOffset": 178}, {"referenceID": 23, "context": "Interestingly, varying the Markov order produces a spectrum of LMAs with the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007) and FGP at the two extremes.", "startOffset": 131, "endOffset": 178}, {"referenceID": 6, "context": ", ocean sensing (Cao, Low, and Dolan 2013; Dolan et al. 2009; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2012; Podnar et al. 2010), traffic monitoring (Chen et al.", "startOffset": 16, "endOffset": 139}, {"referenceID": 12, "context": ", ocean sensing (Cao, Low, and Dolan 2013; Dolan et al. 2009; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2012; Podnar et al. 2010), traffic monitoring (Chen et al.", "startOffset": 16, "endOffset": 139}, {"referenceID": 20, "context": ", ocean sensing (Cao, Low, and Dolan 2013; Dolan et al. 2009; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2012; Podnar et al. 2010), traffic monitoring (Chen et al.", "startOffset": 16, "endOffset": 139}, {"referenceID": 3, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 9, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 13, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 18, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 25, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 26, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 0, "context": "D and U are partitioned according to a simple parallelized clustering scheme employed in the work of Chen et al. (2013).", "startOffset": 49, "endOffset": 120}, {"referenceID": 4, "context": "Note that when B = 0, \u03a3VmVn = QVmVn for |m \u2212 n| > B, thus yielding the prior covariance matrix \u03a3VV of the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007).", "startOffset": 160, "endOffset": 207}, {"referenceID": 23, "context": "Note that when B = 0, \u03a3VmVn = QVmVn for |m \u2212 n| > B, thus yielding the prior covariance matrix \u03a3VV of the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007).", "startOffset": 160, "endOffset": 207}, {"referenceID": 0, "context": "Its proof follows directly from a block-banded matrix result of Asif and Moura (2005) (specifically, Theorem 3).", "startOffset": 64, "endOffset": 86}, {"referenceID": 4, "context": "For example, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) assumes YDm and YDn to be conditionally independent given only YS if |m\u2212 n| > 0.", "startOffset": 17, "endOffset": 64}, {"referenceID": 23, "context": "For example, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) assumes YDm and YDn to be conditionally independent given only YS if |m\u2212 n| > 0.", "startOffset": 17, "endOffset": 64}, {"referenceID": 4, "context": "In contrast, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) (sparse spectrum GP (L\u00e1zaro-Gredilla et al.", "startOffset": 17, "endOffset": 64}, {"referenceID": 23, "context": "In contrast, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) (sparse spectrum GP (L\u00e1zaro-Gredilla et al.", "startOffset": 17, "endOffset": 64}, {"referenceID": 11, "context": "2013; Snelson and Ghahramani 2007) (sparse spectrum GP (L\u00e1zaro-Gredilla et al. 2010)) can only vary support set size (number of spectral points) to obtain the desired predictive performance.", "startOffset": 55, "endOffset": 84}, {"referenceID": 0, "context": "A notable exception is the work of Chen et al. (2013) that parallelizes PIC.", "startOffset": 43, "endOffset": 54}, {"referenceID": 23, "context": "This section first empirically evaluates the predictive performance and scalability of our proposed centralized and parallel LMA methods against that of the state-of-the-art centralized PIC (Snelson and Ghahramani 2007), parallel PIC (Chen et al.", "startOffset": 190, "endOffset": 219}, {"referenceID": 4, "context": "This section first empirically evaluates the predictive performance and scalability of our proposed centralized and parallel LMA methods against that of the state-of-the-art centralized PIC (Snelson and Ghahramani 2007), parallel PIC (Chen et al. 2013), sparse spectrum GP (SSGP) (L\u00e1zaroGredilla et al.", "startOffset": 234, "endOffset": 252}, {"referenceID": 4, "context": "(b) The AIMPEAK dataset (Chen et al. 2013) of size 41850 comprises traffic speeds (km/h) along 775 road segments of an urban road network during morning peak hours on April 20, 2011.", "startOffset": 24, "endOffset": 42}, {"referenceID": 3, "context": "This traffic dataset is modeled using a relational GP (Chen et al. 2012) whose correlation structure can exploit the road segment features and road network topology information.", "startOffset": 54, "endOffset": 72}, {"referenceID": 3, "context": ", of road segments) onto the Euclidean space (Chen et al. 2012) before applying the covariance function.", "startOffset": 45, "endOffset": 63}, {"referenceID": 4, "context": "From Table 1b, when training data is small (|D| = 8000) for AIMPEAK dataset, parallel PIC incurs more time than FGP due to its huge |S| = 5120, which causes communication latency to dominate the incurred time (Chen et al. 2013).", "startOffset": 209, "endOffset": 227}], "year": 2014, "abstractText": "The expressive power of a Gaussian process (GP) model comes at a cost of poor scalability in the data size. To improve its scalability, this paper presents a low-rank-cum-Markov approximation (LMA) of the GP model that is novel in leveraging the dual computational advantages stemming from complementing a low-rank approximate representation of the full-rank GP based on a support set of inputs with a Markov approximation of the resulting residual process; the latter approximation is guaranteed to be closest in the Kullback-Leibler distance criterion subject to some constraint and is considerably more refined than that of existing sparse GP models utilizing low-rank representations due to its more relaxed conditional independence assumption (especially with larger data). As a result, our LMA method can trade off between the size of the support set and the order of the Markov property to (a) incur lower computational cost than such sparse GP models while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale. Interestingly, varying the Markov order produces a spectrum of LMAs with PIC approximation and full-rank GP at the two extremes. An advantage of our LMA method is that it is amenable to parallelization on multiple machines/cores, thereby gaining greater scalability. Empirical evaluation on three real-world datasets in clusters of up to 32 computing nodes shows that our centralized and parallel LMA methods are significantly more time-efficient and scalable than state-of-the-art sparse and full-rank GP regression methods while achieving comparable predictive performances.", "creator": "TeX"}}}