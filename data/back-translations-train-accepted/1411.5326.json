{"id": "1411.5326", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2014", "title": "Compress and Control", "abstract": "This paper describes a new information-theoretic policy evaluation technique for reinforcement learning. This technique converts any compression or density model into a corresponding estimate of value. Under appropriate stationarity and ergodicity conditions, we show that the use of a sufficiently powerful model gives rise to a consistent value function estimator. We also study the behavior of this technique when applied to various Atari 2600 video games, where the use of suboptimal modeling techniques is unavoidable. We consider three fundamentally different models, all too limited to perfectly model the dynamics of the system. Remarkably, we find that our technique provides sufficiently accurate value estimates for effective on-policy control. We conclude with a suggestive study highlighting the potential of our technique to scale to large problems.", "histories": [["v1", "Wed, 19 Nov 2014 19:32:45 GMT  (140kb,D)", "http://arxiv.org/abs/1411.5326v1", "8 pages, 5 figures"]], "COMMENTS": "8 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI cs.IT math.IT", "authors": ["joel veness", "marc g bellemare", "marcus hutter", "alvin chua", "guillaume desjardins"], "accepted": true, "id": "1411.5326"}, "pdf": {"name": "1411.5326.pdf", "metadata": {"source": "CRF", "title": "Compress and Control", "authors": ["Joel Veness", "Marc G. Bellemare", "Marcus Hutter", "Alvin Chua", "Guillaume Desjardins"], "emails": ["veness@google.com", "bellemare@google.com", "alschua@google.com", "gdesjardins@google.com", "marcus.hutter@anu.edu.au"], "sections": [{"heading": "1 Introduction", "text": "In recent years, a number of information theory approaches have established themselves as practical alternatives to traditional methods of machine learning. Notable examples include the compression-based approaches of Frank, Chui, and Witten (2000) and Bratko et al. (2006) to classification, and Cilibrasi \"nyi\" (2005) to clustering. What distinguishes these techniques from more traditional machine learning methods is that they rely on the ability to compress the raw inputs rather than combine or learn what is relevant to the task. Thus, this family of techniques has proven most successful in situations where the nature of the data makes them somewhat unmanageable to specify or learn appropriate characteristics. This class of methods can be formally justified by addressing various terms within algorithmic information theory, such as Kolmogorov Complexity (Li and Vita) nyi, 2008. In this paper, we show how a similarly inspired approach to learning can be applied."}, {"heading": "2 Background", "text": "We begin with a brief overview of the parts of reinforcement learning and information theory required to describe our work, before reviewing the compression-based classification."}, {"heading": "2.1 Markov Decision Processes", "text": "In this work, we limit our attention to finite horizons, temporally homogeneous MDPs whose spaces of action and state are finite. Formally, an MDP is a triplet (S, A, \u00b5), where S is a finite, not empty group of states, A is a finite, not empty group of actions, and \u00b5 is the transition forecaster core, which assigns a probability value to each state-action pair (s, a), where S is a finite, not empty group of states. A is a finite, not empty group of actions, and \u00b5 is the transition forecaster core, which assigns a probability value to each state-action pair (s, a)."}, {"heading": "2.2 Compression and Sequential Prediction", "text": "We now check the sequential probability prediction in the context of statistical data compression. An alphabet X is a set of symbols. A data string x1x2.. xn. Xn. Length n is x1: n. The prefix x1: j of x1: n, j \u2264 n, is given by x \u2264 j or x < j + 1. The empty string is given by. The concatenation of two strings s and r is denoted by sr.A encoding distribution \u03c1 is a sequence of probability mass functions \u03c1n: Xn \u2192 [0, 1] that for all n. N meet the condition that n. & x1: n) = essentially y. Code X: n + 1 (x1: ny) for all x1: n. Xn: Clear, with the base case \u03c10 (): = ltxltx."}, {"heading": "2.3 Compression-based classification", "text": "Compression-based classification was introduced by Frank, Chui and Witten (2000). Given a sequence of n with the designation i.i.d. training examples D: = (y1, c1),..., (yn, cn), where yi and ci are the input and class names respectively, one can apply Baye's rule to express the likelihood that a new example Y will be classified in the light of the training examples D byP [C | Y, D] = P [Y | C, D] P [C | D] \u2211 C [Y | c, D] P [c | D] P [c | D] P [c | D]. (1) The basic idea behind the compression-based classification is the model P [Y | C, D] using a coding distribution for the inputs trained on the subset of examples from D that coincide with class C. Well-known improbable compression methods such as LEMPEL-Zempel (1977) can be used in the ixempel (codification)."}, {"heading": "3 Compression and Control", "text": "We are now introducing Compress and Control (CNC), our new policy assessment method."}, {"heading": "3.1 Overview", "text": "Policy evaluation is based on the estimation of the state impact function Q\u03c0 (s, a). Here, we assume that the environment is a finite, time-homogeneous case of MDP M: = (S, A, \u00b5) and that the policy to be evaluated is a stationary Markov policy \u03c0. To simplify the representation, we will consider the finite m-horizon case and assume that all rewards are drawn from a finite amount of R-R; later, we will discuss how this time-independent conditional probability can be well defined. Our technique involves the construction of a certain type of augmented Markov chain, whose stationary distribution enables the recovery of P (Z | S, A)."}, {"heading": "3.2 Transformation", "text": "Our goal is to define a transformed chain whose stationary distribution can be marginalized to obtain a distribution across states, actions, and the m-horizon. We need two terms for this purpose. To make these statements precise, we will use some standard terms from the Markov chain literature; for further details, we recommend the textbook of Bre \"maud\" (1999). Definition 1. A homogeneous Markov chain (HMC) is said by {Xt \"t\" about the state space X: (AP) aperiodic iff gcd \"n\" n \": P [Xn = x], [X0 = x] > 0\" (HMC] t. \""}, {"heading": "3.3 Online Policy Evaluation", "text": "We now provide an online algorithm for the compression-based policy valuation + 2 + 2 + 2 + 2 + 2 + 2 + 2 + 2 + 2 + 2. This will be for all times t-Q = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 + 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 + 1 + + 1 = 1 = 1 = 1 = 1 + 1 + 1 = 1 + 1 = 1 + 1 = 1 = 1 + 1 + 1 = 1 = 1 + 1 + 1 = 1 = 1 + 1 = 1 = 1 + 1 + 1 = 1 = 1 + 1 + 1 + 1 + 1 = 1 + 1 + 1 = 1 + 1 = 1 + 1 = 1 + 1 = 1 = 1 = 1 + 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 + 1 + 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1"}, {"heading": "3.4 Analysis", "text": "We now show that the state estimates defined by Eq.4 are consistent, provided that consistent density estimates are used for both types. Furthermore, we will say fn converges stochastically to 0 at the rate n \u2212 1 / 2 if and only if c > 0, 0, 1]: P (| fn (0), finite state space, finite action space, time homogeneous MDPM: = (S, A, \u00b5) and a stationary political text leading to an (IR + EA + PR) HMC, for all > 0, we have this for each state s, S and action a, alpit state space, finite action space, timomogeneous MDPM: = (S, A, \u00b5)."}, {"heading": "4 Experimental Results", "text": "In this section we describe two types of experiments: The first sentence is an experimental validation of our theoretical results using a standard policy assessment yardstick; the second sentence combines CNC with a variety of density estimators and examines the resulting behavior in a major policy control task."}, {"heading": "4.1 Policy Evaluation", "text": "Our first experiment includes a simplified version of the game of Blackjack (Sutton and Barto, 1998, Section 5.1). In blackjack, the broker requests cards from the dealer. A game is won if the dealer's total card amount exceeds the dealer's total. We used CNC to estimate the value of the policy that remains if the player's total is 20 or 21, and hits in all other cases. A state is represented by the single card the dealer has held so far, the player's total card amount, and whether the player has a usable ace. In total, there are 200 states, two possible actions (hit or stay), and three possible returns (-1, 0, and 1). A richlet multinomic model with hyperparameters \u03b1i = 1 2 was used for both area S and area ZZ. Figure 2 shows the estimated MSE and the average maximum square error of Q-2 over 100,000 episodes, and the maximum of 10,000 studies across all states and averages."}, {"heading": "4.2 On-policy Control", "text": "The purpose of these experiments is to demonstrate the potential of CNC to scale large control tasks when combined with a variety of different density estimators. Note that Theorem 1 is not nearly applicable here: the use of CNC in this way violates the assumption that \u03c0 is stationary. Observations in ALE consist of frames of 160 x 210 bit color pixels generated by the Stella Atari 2600 emulator. Although the emulator generates frames at 60Hz, in our experiments we look at the last 4 consecutive frames, following the existing literature (Bellemare, Veness, and Talvitie, 2014)."}, {"heading": "5 Discussion and Limitations", "text": "The main strength and main limitation of the CNC approach seems to be its dependence on an appropriate choice of the Den-Av erag eSc ore (Sca led) Freeway Pong Q * bert20.016.43190-19.0 497.213.0DQN BASS CNCFigure 4: Average score over the last 500 episodes for three Atari 2600 games. Error bars suggest that there is a standard error estimate between the trials. One could only expect the method to perform well if the learned models can capture the observation structure specifically for high and low yield states. Defining a model can therefore be seen as an obligation to a certain type of compression-based similarity metrically across the state. The attractive part of this approach is that density modeling is a well-studied area that opens the possibility of incorporating many ideas from machine learning, statistics and information theory into the basic questions of learning itself, which is of course a problem of modeling."}, {"heading": "6 Closing Remarks", "text": "The most interesting aspect of this approach is the way it uses a learned probabilistic model that sets the conditions for future returns; remarkably, this counterintuitive idea can be justified both in theory and in practice. Although our initial results are promising, a number of open questions remain, for example, the CNC estimates have so far only been constructed by using the Monte Carlo return as a learning signal. However, one of the central themes in Reinforcement Learning is boot strapping, the idea of constructing valuations based on other estimates (Sutton and Barto, 1998), a natural question that needs to be explored if boot strapping can be integrated into the learning signal used by CNC. In the case of on-policy control, it would also be interesting to examine the use of compression techniques or density estimators that can automatically adapt to non-stationary data."}], "references": [{"title": "Learning is planning: near Bayes-optimal reinforcement learning via MonteCarlo tree search", "author": ["J. Asmuth", "M.L. Littman"], "venue": "Uncertainty in Artificial Intelligence (UAI), 19\u201326.", "citeRegEx": "Asmuth and Littman,? 2011", "shortCiteRegEx": "Asmuth and Littman", "year": 2011}, {"title": "Planning by Probabilistic Inference", "author": ["H. Attias"], "venue": "Proceedings of the 9th International Workshop on Artificial Intelligence and Statistics.", "citeRegEx": "Attias,? 2003", "shortCiteRegEx": "Attias", "year": 2003}, {"title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research (JAIR) 47:253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Skip Context Tree Switching", "author": ["M.G. Bellemare", "J. Veness", "E. Talvitie"], "venue": "Proceedings of the Thirty-First International Conference on Machine Learning (ICML).", "citeRegEx": "Bellemare et al\\.,? 2014", "shortCiteRegEx": "Bellemare et al\\.", "year": 2014}, {"title": "Random search for hyperparameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research (JMLR) 13:281\u2013305.", "citeRegEx": "Bergstra and Bengio,? 2012", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific, 1st edition.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": "Secaucus, NJ, USA: Springer-Verlag New York, Inc.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Planning as inference", "author": ["M. Botvinick", "M. Toussaint"], "venue": "Trends in Cognitive Sciences 10, 485\u2013588.", "citeRegEx": "Botvinick and Toussaint,? 2012", "shortCiteRegEx": "Botvinick and Toussaint", "year": 2012}, {"title": "Spam filtering using statistical data compression models", "author": ["A. Bratko", "G.V. Cormack", "D. R", "B. Filipi", "P. Chan", "T.R. Lynam"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Bratko et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bratko et al\\.", "year": 2006}, {"title": "Markov chains : Gibbs fields, Monte Carlo simulation and queues", "author": ["P. Br\u00e9maud"], "venue": "Texts in applied mathematics. New York, Berlin, Heidelberg: Springer.", "citeRegEx": "Br\u00e9maud,? 1999", "shortCiteRegEx": "Br\u00e9maud", "year": 1999}, {"title": "Clustering by compression", "author": ["R. Cilibrasi", "P.M.B. Vit\u00e1nyi"], "venue": "IEEE Transactions on Information Theory 51:1523\u20131545.", "citeRegEx": "Cilibrasi and Vit\u00e1nyi,? 2005", "shortCiteRegEx": "Cilibrasi and Vit\u00e1nyi", "year": 2005}, {"title": "Continuous upper confidence trees", "author": ["A. Cou\u00ebtoux", "J.-B. Hoock", "N. Sokolovska", "O. Teytaud", "N. Bonnard"], "venue": "Proceedings of the 5th International Conference on Learning and Intelligent Optimization, LION\u201905, 433\u2013 445. Springer-Verlag.", "citeRegEx": "Cou\u00ebtoux et al\\.,? 2011", "shortCiteRegEx": "Cou\u00ebtoux et al\\.", "year": 2011}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "New York, NY, USA: Wiley-Interscience.", "citeRegEx": "Cover and Thomas,? 1991", "shortCiteRegEx": "Cover and Thomas", "year": 1991}, {"title": "The Infinite Partially Observable Markov Decision Process", "author": ["F. Doshi-Velez"], "venue": "Advances in Neural Information Processing Systems (NIPS) 22.", "citeRegEx": "Doshi.Velez,? 2009", "shortCiteRegEx": "Doshi.Velez", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR) 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Text categorization using compression models", "author": ["E. Frank", "C. Chui", "I.H. Witten"], "venue": "Proceedings of Data Compression Conference (DCC), 200\u2013209. IEEE Computer Society Press.", "citeRegEx": "Frank et al\\.,? 2000", "shortCiteRegEx": "Frank et al\\.", "year": 2000}, {"title": "Efficient BayesAdaptive Reinforcement Learning using Sample-based Search", "author": ["A. Guez", "D. Silver", "P. Dayan"], "venue": "Advances in Neural Information Processing Systems (NIPS) 25.", "citeRegEx": "Guez et al\\.,? 2012", "shortCiteRegEx": "Guez et al\\.", "year": 2012}, {"title": "Efficient Tracking of Large Classes of Experts", "author": ["A. Gy\u00f6rgy", "T. Linder", "G. Lugosi"], "venue": "IEEE Transactions on Information Theory 58(11):6709\u20136725.", "citeRegEx": "Gy\u00f6rgy et al\\.,? 2012", "shortCiteRegEx": "Gy\u00f6rgy et al\\.", "year": 2012}, {"title": "Modelling Sparse Dynamical Systems with Compressed Predictive State Representations", "author": ["W.L. Hamilton", "M.M. Fard", "J. Pineau"], "venue": "ICML, volume 28 of JMLR Proceedings, 178\u2013186.", "citeRegEx": "Hamilton et al\\.,? 2013", "shortCiteRegEx": "Hamilton et al\\.", "year": 2013}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": "MIT Press.", "citeRegEx": "Howard,? 1960", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "Fast non-parametric Bayesian inference on infinite trees", "author": ["M. Hutter"], "venue": "Proceedings of 10th International", "citeRegEx": "Hutter,? 2005a", "shortCiteRegEx": "Hutter", "year": 2005}, {"title": "Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability", "author": ["M. Hutter"], "venue": "Springer.", "citeRegEx": "Hutter,? 2005b", "shortCiteRegEx": "Hutter", "year": 2005}, {"title": "Sparse adaptive dirichlet-multinomiallike processes", "author": ["M. Hutter"], "venue": "Conference on Computational Learning Theory (COLT), 432\u2013459.", "citeRegEx": "Hutter,? 2013", "shortCiteRegEx": "Hutter", "year": 2013}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": "Springer, third edition.", "citeRegEx": "Li and Vit\u00e1nyi,? 2008", "shortCiteRegEx": "Li and Vit\u00e1nyi", "year": 2008}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602.", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Escaping Local Optima in POMDP Planning as Inference", "author": ["P. Poupart", "T. Lang", "M. Toussaint"], "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 3, AAMAS \u201911, 1263\u2013 1264.", "citeRegEx": "Poupart et al\\.,? 2011", "shortCiteRegEx": "Poupart et al\\.", "year": 2011}, {"title": "Approximate Dynamic Programming: Solving the Curses of Dimensionality", "author": ["W.B. Powell"], "venue": "Wiley-Interscience, 2nd edition.", "citeRegEx": "Powell,? 2011", "shortCiteRegEx": "Powell", "year": 2011}, {"title": "Goal-directed decision making as probabilistic inference: A computational framework and potential neural correlates", "author": ["A. Solway", "M. Botvinick"], "venue": "Psycholological Review 119:120\u2013154.", "citeRegEx": "Solway and Botvinick,? 2012", "shortCiteRegEx": "Solway and Botvinick", "year": 2012}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Cambridge, MA: MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers.", "citeRegEx": "Szepesv\u00e1ri,? 2010", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2010}, {"title": "Model Regularization for Stable Sample Rollouts", "author": ["E. Talvitie"], "venue": "Uncertainty in Artificial Intelligence (UAI).", "citeRegEx": "Talvitie,? 2014", "shortCiteRegEx": "Talvitie", "year": 2014}, {"title": "Context tree weighting: Multi-alphabet sources", "author": ["T.J. Tjalkens", "Y.M. Shtarkov", "F.M.J. Willems"], "venue": "Proceedings of the 14th Symposium on Information Theory Benelux.", "citeRegEx": "Tjalkens et al\\.,? 1993", "shortCiteRegEx": "Tjalkens et al\\.", "year": 1993}, {"title": "Cover Tree Bayesian Reinforcement Learning", "author": ["N. Tziortziotis", "C. Dimitrakakis", "K. Blekas"], "venue": "Journal of Machine Learning Research (JMLR) 15:2313\u20132335.", "citeRegEx": "Tziortziotis et al\\.,? 2014", "shortCiteRegEx": "Tziortziotis et al\\.", "year": 2014}, {"title": "Reinforcement Learning via AIXI Approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "D. Silver"], "venue": "Proceedings of the Conference for the Association for the Advancement of Artificial Intelligence (AAAI).", "citeRegEx": "Veness et al\\.,? 2010", "shortCiteRegEx": "Veness et al\\.", "year": 2010}, {"title": "A Monte Carlo AIXI approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "W. Uther", "D. Silver"], "venue": "Journal of Artificial Intelligence Research (JAIR) 40:95\u2013142.", "citeRegEx": "Veness et al\\.,? 2011", "shortCiteRegEx": "Veness et al\\.", "year": 2011}, {"title": "Partition Tree Weighting", "author": ["J. Veness", "M. White", "M. Bowling", "A. Gyorgy"], "venue": "Proceedings of Data Compression Conference (DCC), 321\u2013330.", "citeRegEx": "Veness et al\\.,? 2013", "shortCiteRegEx": "Veness et al\\.", "year": 2013}, {"title": "Integrating Sample-Based Planning and Model-Based Reinforcement Learning", "author": ["T.J. Walsh", "S. Goschin", "M.L. Littman"], "venue": "Proceedings of the Conference for the", "citeRegEx": "Walsh et al\\.,? 2010", "shortCiteRegEx": "Walsh et al\\.", "year": 2010}, {"title": "Stable dual dynamic programming", "author": ["T. Wang", "M. Bowling", "D. Schuurmans", "D.J. Lizotte"], "venue": "Advances in Neural Information Processing Systems (NIPS) 20, 1569\u2013 1576.", "citeRegEx": "Wang et al\\.,? 2008", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Dual representations for dynamic programming and reinforcement learning", "author": ["T. Wang", "M. Bowling", "D. Schuurmans"], "venue": "IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, 44\u201351.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "The Context Tree Weighting Method: Basic Properties", "author": ["F.M. Willems", "Y.M. Shtarkov", "T.J. Tjalkens"], "venue": "IEEE Transactions on Information Theory 41:653\u2013664.", "citeRegEx": "Willems et al\\.,? 1995", "shortCiteRegEx": "Willems et al\\.", "year": 1995}, {"title": "Managing gigabytes: compressing and indexing documents and images", "author": ["I.H. Witten", "A. Moffat", "T.C. Bell"], "venue": "Morgan Kaufmann.", "citeRegEx": "Witten et al\\.,? 1999", "shortCiteRegEx": "Witten et al\\.", "year": 1999}, {"title": "Arithmetic coding for data compression", "author": ["I.H. Witten", "R.M. Neal", "J.G. Cleary"], "venue": "Communications of the ACM. 30:520\u2013540.", "citeRegEx": "Witten et al\\.,? 1987", "shortCiteRegEx": "Witten et al\\.", "year": 1987}, {"title": "A universal algorithm for sequential data compression", "author": ["J. Ziv", "A. Lempel"], "venue": "Information Theory, IEEE Transactions on 23(3):337\u2013343.", "citeRegEx": "Ziv and Lempel,? 1977", "shortCiteRegEx": "Ziv and Lempel", "year": 1977}], "referenceMentions": [{"referenceID": 23, "context": "This class of methods can be formally justified by appealing to various notions within algorithmic information theory, such as Kolmogorov complexity (Li and Vit\u00e1nyi, 2008).", "startOffset": 149, "endOffset": 171}, {"referenceID": 8, "context": "Noteworthy examples include the compression-based approaches of Frank, Chui, and Witten (2000) and Bratko et al. (2006) to classification, and Cilibrasi and Vit\u00e1nyi (2005) to clustering.", "startOffset": 99, "endOffset": 120}, {"referenceID": 8, "context": "Noteworthy examples include the compression-based approaches of Frank, Chui, and Witten (2000) and Bratko et al. (2006) to classification, and Cilibrasi and Vit\u00e1nyi (2005) to clustering.", "startOffset": 99, "endOffset": 172}, {"referenceID": 19, "context": "(Howard, 1960), approximate dynamic programming (Bertsekas and Tsitsiklis, 1996; Powell, 2011) and actor-critic methods (Sutton and Barto, 1998), for example, all crucially depend on how well policy evaluation can be performed.", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "(Howard, 1960), approximate dynamic programming (Bertsekas and Tsitsiklis, 1996; Powell, 2011) and actor-critic methods (Sutton and Barto, 1998), for example, all crucially depend on how well policy evaluation can be performed.", "startOffset": 48, "endOffset": 94}, {"referenceID": 26, "context": "(Howard, 1960), approximate dynamic programming (Bertsekas and Tsitsiklis, 1996; Powell, 2011) and actor-critic methods (Sutton and Barto, 1998), for example, all crucially depend on how well policy evaluation can be performed.", "startOffset": 48, "endOffset": 94}, {"referenceID": 28, "context": "(Howard, 1960), approximate dynamic programming (Bertsekas and Tsitsiklis, 1996; Powell, 2011) and actor-critic methods (Sutton and Barto, 1998), for example, all crucially depend on how well policy evaluation can be performed.", "startOffset": 120, "endOffset": 144}, {"referenceID": 30, "context": "The main difficulty that arises when using learnt forward models is that the modeling errors tend to compound when reasoning over long time horizons (Talvitie, 2014).", "startOffset": 149, "endOffset": 165}, {"referenceID": 12, "context": "Noteworthy recent examples include the work of Doshi-Velez (2009), Walsh, Goschin, and Littman (2010), Veness et al.", "startOffset": 47, "endOffset": 66}, {"referenceID": 12, "context": "Noteworthy recent examples include the work of Doshi-Velez (2009), Walsh, Goschin, and Littman (2010), Veness et al.", "startOffset": 47, "endOffset": 102}, {"referenceID": 12, "context": "Noteworthy recent examples include the work of Doshi-Velez (2009), Walsh, Goschin, and Littman (2010), Veness et al. (2010), Veness et al.", "startOffset": 47, "endOffset": 124}, {"referenceID": 12, "context": "Noteworthy recent examples include the work of Doshi-Velez (2009), Walsh, Goschin, and Littman (2010), Veness et al. (2010), Veness et al. (2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014).", "startOffset": 47, "endOffset": 146}, {"referenceID": 0, "context": "(2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014).", "startOffset": 8, "endOffset": 34}, {"referenceID": 0, "context": "(2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014).", "startOffset": 8, "endOffset": 66}, {"referenceID": 0, "context": "(2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014).", "startOffset": 8, "endOffset": 101}, {"referenceID": 0, "context": "(2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014). Although the aforementioned works demonstrate quite impressive performance on small domains possessing complicated dynamics, scaling these methods to large state or observation spaces has proven challenging.", "startOffset": 8, "endOffset": 151}, {"referenceID": 7, "context": "These ideas have been recently explored in both the neuroscience (Botvinick and Toussaint, 2012; Solway and Botvinick, 2012) and machine learning (Attias, 2003; Poupart, Lang, and Toussaint, 2011) literature.", "startOffset": 65, "endOffset": 124}, {"referenceID": 27, "context": "These ideas have been recently explored in both the neuroscience (Botvinick and Toussaint, 2012; Solway and Botvinick, 2012) and machine learning (Attias, 2003; Poupart, Lang, and Toussaint, 2011) literature.", "startOffset": 65, "endOffset": 124}, {"referenceID": 1, "context": "These ideas have been recently explored in both the neuroscience (Botvinick and Toussaint, 2012; Solway and Botvinick, 2012) and machine learning (Attias, 2003; Poupart, Lang, and Toussaint, 2011) literature.", "startOffset": 146, "endOffset": 196}, {"referenceID": 37, "context": "can approximate a type of dual representation (Wang, Bowling, and Schuurmans, 2007; Wang et al., 2008) of the value function.", "startOffset": 46, "endOffset": 102}, {"referenceID": 28, "context": "A Markov Decision Process (MDP) is a type of probabilistic model widely used within reinforcement learning (Sutton and Barto, 1998; Szepesv\u00e1ri, 2010) and control (Bertsekas and Tsitsiklis, 1996).", "startOffset": 107, "endOffset": 149}, {"referenceID": 29, "context": "A Markov Decision Process (MDP) is a type of probabilistic model widely used within reinforcement learning (Sutton and Barto, 1998; Szepesv\u00e1ri, 2010) and control (Bertsekas and Tsitsiklis, 1996).", "startOffset": 107, "endOffset": 149}, {"referenceID": 5, "context": "A Markov Decision Process (MDP) is a type of probabilistic model widely used within reinforcement learning (Sutton and Barto, 1998; Szepesv\u00e1ri, 2010) and control (Bertsekas and Tsitsiklis, 1996).", "startOffset": 162, "endOffset": 194}, {"referenceID": 12, "context": "We refer the reader to the standard text of Cover and Thomas (1991) for further information.", "startOffset": 44, "endOffset": 68}, {"referenceID": 42, "context": "Well known non-probabilistic compression methods such as LEMPEL-ZIV (Ziv and Lempel, 1977) can be used by forming their associated coding distribution 2\u2212`z(x1:n), where `z(x1:n) is the length of the compressed data x1:n in bits under compression method z.", "startOffset": 68, "endOffset": 90}, {"referenceID": 8, "context": "On one hand, it is straightforward to apply generic compression techniques (including those operating at the bit or character level) to complicated input types such as richly formatted text or DNA strings (Frank, Chui, and Witten, 2000; Bratko et al., 2006).", "startOffset": 205, "endOffset": 257}, {"referenceID": 9, "context": "To make these statements precise, we will use some standard terminology from the Markov chain literature; for more detail, we recommend the textbook of Br\u00e9maud (1999).", "startOffset": 152, "endOffset": 167}, {"referenceID": 9, "context": "The second result allows the HMC {Yt}t\u22651 to be further augmented to give the snake HMC {Yt:t+m}t\u22651 (Br\u00e9maud, 1999).", "startOffset": 99, "endOffset": 114}, {"referenceID": 1, "context": "\u03bd(s|z, a) conditions on the return, similar in spirit to prior work on planning as inference (Attias, 2003; Botvinick and Toussaint, 2012; Solway and Botvinick, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 7, "context": "\u03bd(s|z, a) conditions on the return, similar in spirit to prior work on planning as inference (Attias, 2003; Botvinick and Toussaint, 2012; Solway and Botvinick, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 27, "context": "\u03bd(s|z, a) conditions on the return, similar in spirit to prior work on planning as inference (Attias, 2003; Botvinick and Toussaint, 2012; Solway and Botvinick, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 34, "context": "The next result applies to a factored application of multi-alphabet Context Tree Weighting (CTW) (Tjalkens, Shtarkov, and Willems, 1993; Willems, Shtarkov, and Tjalkens, 1995; Veness et al., 2011), which can handle considerably larger state spaces in practice.", "startOffset": 97, "endOffset": 196}, {"referenceID": 29, "context": "We also compared CNC to a first-visit Monte Carlo value estimate (Szepesv\u00e1ri, 2010).", "startOffset": 65, "endOffset": 83}, {"referenceID": 2, "context": "We evaluated CNC using ALE, the Arcade Learning Environment (Bellemare et al., 2013), a reinforcement learning interface to the Atari 2600 video game platform.", "startOffset": 60, "endOffset": 84}, {"referenceID": 24, "context": "Although the emulator generates frames at 60Hz, in our experiments we consider time steps that last 4 consecutive frames, following the existing literature (Bellemare, Veness, and Talvitie, 2014; Mnih et al., 2013).", "startOffset": 156, "endOffset": 214}, {"referenceID": 22, "context": "We studied four different CNC agents, with each agent corresponding to a different choice of model for \u03c1S; the Sparse Adapative Dirichlet (SAD) estimator (Hutter, 2013) was used for \u03c1Z for all agents.", "startOffset": 154, "endOffset": 168}, {"referenceID": 28, "context": "Each agent used an -greedy policy (Sutton and Barto, 1998) with respect to its current value function estimates.", "startOffset": 34, "endOffset": 58}, {"referenceID": 6, "context": "The second model is an auto-regressive application of logistic regression (Bishop, 2006), that assigns a probability to each pixel using a shared set of parameters.", "startOffset": 74, "endOffset": 88}, {"referenceID": 4, "context": ") were optimized via the random sampling technique of Bergstra and Bengio (2012).", "startOffset": 54, "endOffset": 81}, {"referenceID": 42, "context": "The third model uses the LEMPEL-ZIV algorithm (Ziv and Lempel, 1977), a dictionary-based compression technique.", "startOffset": 46, "endOffset": 68}, {"referenceID": 2, "context": "We also compared our method to existing results from the literature (Bellemare et al., 2013; Mnih et al., 2013), although note that the DQN scores, which correspond to a different training regime and do not include Freeway, are included only for illustrative purposes.", "startOffset": 68, "endOffset": 111}, {"referenceID": 24, "context": "We also compared our method to existing results from the literature (Bellemare et al., 2013; Mnih et al., 2013), although note that the DQN scores, which correspond to a different training regime and do not include Freeway, are included only for illustrative purposes.", "startOffset": 68, "endOffset": 111}, {"referenceID": 11, "context": "Interestingly, we found SKIPCTS to be insufficiently accurate for effective MCTS planning when used as a forward model, even with enhancements such as double progressive widening (Cou\u00ebtoux et al., 2011).", "startOffset": 179, "endOffset": 202}, {"referenceID": 20, "context": "Furthermore, one could attempt to adaptively learn the best discretization (Hutter, 2005a) or approximate Equation 4 using Monte Carlo sampling.", "startOffset": 75, "endOffset": 90}, {"referenceID": 28, "context": "However, one of the central themes in Reinforcement Learning is bootstrapping, the idea of constructing value estimates on the basis of other value estimates (Sutton and Barto, 1998).", "startOffset": 158, "endOffset": 182}, {"referenceID": 35, "context": "A promising line of investigation might be to consider the class of meta-algorithms given by Gy\u00f6rgy, Linder, and Lugosi (2012), that can convert any stationary coding distribution into its piece-wise stationary extension; efficient algorithms from this class have shown promise for data compression applications, and come with strong theoretical guarantees (Veness et al., 2013).", "startOffset": 357, "endOffset": 378}, {"referenceID": 19, "context": "4 to cover the case of onpolicy control or policy iteration (Howard, 1960) would be highly desirable.", "startOffset": 60, "endOffset": 74}, {"referenceID": 24, "context": "However, one of the central themes in Reinforcement Learning is bootstrapping, the idea of constructing value estimates on the basis of other value estimates (Sutton and Barto, 1998). A natural question to explore is whether bootstrapping can be incorporated into the learning signal used by CNC. For the case of on-policy control, it would be also interesting to investigate the use of compression techniques or density estimators that can automatically adapt to nonstationary data. A promising line of investigation might be to consider the class of meta-algorithms given by Gy\u00f6rgy, Linder, and Lugosi (2012), that can convert any stationary coding distribution into its piece-wise stationary extension; efficient algorithms from this class have shown promise for data compression applications, and come with strong theoretical guarantees (Veness et al.", "startOffset": 159, "endOffset": 611}, {"referenceID": 19, "context": "4 to cover the case of onpolicy control or policy iteration (Howard, 1960) would be highly desirable. Finally, we remark that information-theoretic perspectives on reinforcement learning have existed for some time; in particular, Hutter (2005b) described a unification of algorithmic information theory and reinforcement learning, leading to the AIXI optimality notion for reinforcement learning agents.", "startOffset": 61, "endOffset": 245}], "year": 2014, "abstractText": "This paper describes a new information-theoretic policy evaluation technique for reinforcement learning. This technique converts any compression or density model into a corresponding estimate of value. Under appropriate stationarity and ergodicity conditions, we show that the use of a sufficiently powerful model gives rise to a consistent value function estimator. We also study the behavior of this technique when applied to various Atari 2600 video games, where the use of suboptimal modeling techniques is unavoidable. We consider three fundamentally different models, all too limited to perfectly model the dynamics of the system. Remarkably, we find that our technique provides sufficiently accurate value estimates for effective on-policy control. We conclude with a suggestive study highlighting the potential of our technique to scale to large problems.", "creator": "LaTeX with hyperref package"}}}