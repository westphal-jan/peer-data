{"id": "1411.5379", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2014", "title": "Type-Driven Incremental Semantic Parsing with Polymorphism", "abstract": "Semantic parsing is a burgeoning field, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation (simply typed lambda calculus). We introduce two new techniques to tackle these problems. First, we design a linear-time, type-driven, incremental parsing algorithm that use type checking to reduce the search space, which is orders of magnitude faster than conventional cubic-time bottom-up semantic parsers, and also eliminates the need for a formal grammar such as CCG. Second, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we introduce a sophisticated subtype hierarchy and parametric polymorphism to the system, so that the type system is powerful enough to better guide the parsing. Together with max-violation perceptron training, our system learns very accurate parses in GeoQuery, Jobs and Atis domains.", "histories": [["v1", "Wed, 19 Nov 2014 21:06:15 GMT  (182kb,D)", "https://arxiv.org/abs/1411.5379v1", null], ["v2", "Sat, 13 Dec 2014 21:35:32 GMT  (40kb,D)", "http://arxiv.org/abs/1411.5379v2", null], ["v3", "Tue, 16 Dec 2014 16:41:48 GMT  (57kb,D)", "http://arxiv.org/abs/1411.5379v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kai zhao", "liang huang 0001"], "accepted": true, "id": "1411.5379"}, "pdf": {"name": "1411.5379.pdf", "metadata": {"source": "CRF", "title": "Type-Driven Incremental Semantic Parsing with Polymorphism", "authors": ["Kai Zhao", "Liang Huang"], "emails": ["kzhao.hf@gmail.com", "liang.huang.sh@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Most existing parsing strategies are based on the fact that these strategies are a purely theoretical strategy, which is limited to a purely theoretical view of things and not a purely theoretical view of things. Most of these strategies are based on a purely theoretical view of things. Most of these strategies are based on a purely theoretical view of things, which refers to a purely theoretical view of things. Most of these strategies are based on a purely theoretical view of things, which refers to a purely theoretical view of things. Most of these strategies are based on a purely theoretical view of things, which refers to a purely theoretical view of things, which refers to a purely theoretical view of things, which refers to a purely theoretical view of things, which refers to a purely theoretical view of things."}, {"heading": "2 Type-Driven Incremental Parsing", "text": "We start with the simplest meaning representation (MR), untyped lambda calculation, and then introduce the typing and incremental parsing algorithm for it. Later in Section 3 we add subtyping and type polymorphism to enrich the system."}, {"heading": "2.1 Meaning Representation with Types", "text": "The untyped MR for the running example is: Q: \u2192 \u2192 \u2192 \u2192 \u2192 \u2192 Function of the largest state by area? < MR: (max. state size) max. (max. state size) Note that the binary function argmax (\u00b7) is a function of higher order that takes two additional functions as input: the first argument is a \"domain\" function that defines the set, and the second argument is a \"evaluation\" function that returns an integer number for an element in that domain. In other words, argmax (f, g) = argmax x: f (x) g (x).1There are three types of polymorphisms in programming languages: parametric (e.g. C + + templates), subtyping, and ad-hoc (e.g. operator overload). See (Pierce, chapter 15) for details.Step Action Stack 0 - what happens after the action queue... 1-3: Shapapapapapapi \u2192 \u2192 \u2192 \u2192"}, {"heading": "2.2 Incremental Semantic Parsing: An Example", "text": "Similar to a standard shift reduction parser, we run a stack and a queue. The queue contains words that need to be analyzed, while the stack contains subexpressions of the last MR where each subexpression is a valid typed lambda expression. At each step, the parser chooses to shift or reduce, but unlike the standard shift reduction parser, there is also a third possible action, namely skip, where a semantically insignificant word is skipped (e.g., \"of,\" \"is,\" etc.) The first three words of the sample question \"What is this...\" are all skipped (steps 1-3 in Figure 1 (a). The parser then moves the next word, \"capital\" not to the stack, \"etc."}, {"heading": "2.3 Type-Driven Reduce", "text": "At this step we can finally perform a reduction of the action, because the top two expressions on the stack go through the type check for right-hand function application (a partial application): argmax expects a (e \u2192 t) argument that is exactly the type of state. So we perform a right-hand reduction by applying Argmax to the state, and the resulting expression is: (argmax state): (e \u2192 i) \u2192 ewhile the stack becomes (step 9) capital: (e \u2192 i) \u2192 eNow, if we want to continue the reduction, there will be no type check for a left or right-hand reduction, so we will have to move again. So we can move on to move the last word \"area\" with the grounded predicate in the GeoQuery database: Size: e \u2192 iand the stack will (step 11): Capital: e (argmax state): e (argmax state): e (size): e (i) \u2192 e: c: c: c: c: c size: c: c size: \u00b2 size: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e: e:: e: e: e: e: e: e: e: e: e:: e: e: e: e: e: e:"}, {"heading": "3 Subtype and Type Polymorphisms", "text": "As mentioned in Section 1, the single-typed lambda spreadsheet cannot distinguish between Mississippi the river and Mississippi the state, as both have the same type e. Furthermore, functional capital can currently be applied to any entity type, for example, capital (Boston), which should have been banned by the type check. Therefore, we need a more sophisticated type system to help real-world entities ground terms, and this sophisticated type system will in turn support type-controlled parsing."}, {"heading": "3.1 Augmenting MR with Subtyping", "text": "For example, Figure 2 shows a (slightly simplified) version of the type hierarchy for GEOQUERY domains. < < < < p > Functions consisting of two different types of places, administrative units (au) including states (st) and cities (ct) and natural units (nu) including rivers (rv < lk). We use <: to denote the (transitive, reflexive and anti-symmetrical) differentiation between the types; for example, in GEOQUERY we have st <: lo, rv <: top for each type of mission T.In addition, we have an integer type i derived from root type Top. The Boolean type t does not belong to the type hierarchy because it does not represent the secs from the task domain."}, {"heading": "3.2 Parsing with Subtype Polymorphism and Parametric Polymorphism", "text": "We modify the previous incremental parsing algorithm with simple types (section 2) to accommodate subtypes and polymorphic types. Figure 1 (b) shows the derivation of the running example using the new parsing algorithm. Below, we will focus on the differences that the new algorithm entails. In contrast to the uppercase letter: e \u2192 e, in step 4, we move the forecast letter: st \u2192 ctand in step 7, we move the polymorphic expression for \"largest\" argmax: (\"a \u2192 t) \u2192 (\" a \u2192 i) \u2192 \"aAnd after the shift in step 8, the stack becomes uppercase: st \u2192 ct argmax: (\" a \u2192 t) \u2192 (\"a \u2192 i) \u2192\" a state: st \u2192 tAt step 9 to apply Argmax to the state: st \u2192 t, \u2192 \"aAnd after the shift in step 8, the stack becomes \u2192 \u2192\" max \"(a) \u2192 gst \u2192 9 (a) to apply the state: max (a)."}, {"heading": "4 Training: Latent Variable Perceptron", "text": "We follow the framework for latent variable violation-fixing Perceptron Huang et al. (2012); Yu et al. (2013) for training."}, {"heading": "4.1 Framework", "text": "The central challenge in training is that for each question there could be many different unknown derivatives that lead to their commented MR (known as the false ambiguity). In our type-related semantic derivative task, the false ambiguity is caused by how the stamps are selected and justified during the shift step, and the various reduced jobs that lead to the same result. We treat this unknown information as latently variable. Formally, we refer to D (x) as the set of all partial and complete derivatives for an input set x and mr (d) as the MR resulting from a complete derivative d. Then, we define the sets of (partial and complete) reference derivatives as: good i (x, y) = {d \u00b2 derivatives for D (x) | | d | x, complete derivative d \u2212 d = y."}, {"heading": "4.2 Forced Decoding", "text": "Unlike syntactic incremental parsing, where forced decoding can be performed in polynomial time Goldberg et al. (2014), we do not have an algorithm designed for efficient forced decoding. In practice, we use an exponential brute force search to calculate well (x, y), during which we puff up based on predicate application orders. However, this requires a heavy calculation that we cannot afford. In practice, we choose multi-pass forced decoding. First, we use brute force search to decode, but with a time limit. Then, we train perception using successfully decoded reference derivatives and use the trained perceptron to decipher the unfinished questions with a large bar. Then, we add the newly discovered reference derivatives to the next step."}, {"heading": "5 Experiments", "text": "We implement our type-controlled incremental semantic parser (TISP) using Python and evaluate its performance in terms of both speed and accuracy on GEOQUERY and JOBS datasets. Our feature design is inspired by the very effective word edge features in the syntactic parsing of Charniak and Johnson (2005) and MT He et al. (2008). From each parsing state, we collect atomic characteristics, including the types and the most left and right words of the span of the top 3 MR expressions in the stack, the top 3 words in the queue, the generated predicate names, and the ID of the expression template used in the shift action. To ease the overfill problem caused by the lack of functionality, we assign different types of features to different budgets and generate only feature combinations within a budget limit. In 2005, we receive 84 combined feature calls, where we correctly follow the prefix and Collins."}, {"heading": "5.1 Evaluation on GEOQUERY Dataset", "text": "Following the scheme of Zettlemoyer and Collins (2007), we use the first 600 sets of Geo880 as a training set and the remaining 280 sets as a test set. Note that due to the relatively small size of Geo880, we do not have a separate development set. So in order to find the best number of iterations to complete the training, we perform a 10x cross-validation training on the training set and select 20 iterations to train and then evaluate. We use two forced decodes. In the initial brachial run, we set the time limit to 1200 seconds and find the reference derivatives for 530 of the total 600 training sets, a coverage of 88%. In the second run, we set the beam size to 16,384 and get 581 sets covered (approximately 97%).In the first training and in the evaluation of time, we use the reference derivatives for 530 of the total 600 training sets, a coverage of 88%."}, {"heading": "5.2 Evaluations on JOBS and ATIS Datasets", "text": "The JOBS domain contains descriptions of the required and desired qualifications of a job. Qualifications include programming language (la), years of experience (ye), diplomacy degree (de), area of fields (ar), platform (pa), job title (ti), etc. In Figure 3. According to the splitting scheme of Zettlemoyer and Collins (2005), we use 500 sentences as a training set and 140 sentences as a test set. Table 1 shows that our algorithm achieves a significantly higher recall rate than the existing method of Zettlemoyer and Collins (2005), although our precision is not as high as theirs. In fact, this is because our method analyzes many more questions in the dataset as the column of the percentage of successfully written sentences suggests. We also evaluate the performance of TISP on ATIS datasets as in Table 1. ATIS datasets contains and is much larger than GEOQERY and JBS datasets that we cannot compare with this method."}, {"heading": "6 Related Work", "text": "Zettlemoyer and Collins (2005) introduce a type hierarchy for semantic parsing and parsing with typed lambda calculation combined with CCG. However, the simple introduction of subtyped predicates without polymorphism leads to errors in type checking when dealing with high-order functions, as in Section 3. Furthermore, our system, which is type-driven, relies almost entirely on the types of MR expressions to guide parsing (with the exception of some simple POS tag triggers), while their system is strongly CCG-based and syntax-driven. Kwiatkowski et al. (2013) use on-the-fly matching to get the best predicate in the dataset for some MR subexpression. Matching occurs at the end of parsing and is limited by the type of subexpression."}, {"heading": "7 Conclusions and Future Work", "text": "We have introduced an incremental semantic parser guided by a powerful type system of subtyping and parametric polymorphism, which has greatly reduced the number of templates and effective search space during parsing. Our parser is competitive with state-of-the-art accuracy, but because it is linear, it is orders of magnitude faster in theory and practice than CKY-based parsers. For future work, we would like to work on weakly monitored learning that learns from question pairs rather than question-MR pairs, where the data sets are larger, and TISP should benefit more from such problems."}], "references": [{"title": "Coarse-to-fine n-best parsing and maxent discriminative reranking", "author": ["E. Charniak", "M. Johnson", "June"], "venue": "In Proceedings of ACL, Ann Arbor, Michigan,", "citeRegEx": "Charniak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2005}, {"title": "A tabular method for dynamic oracles in transition-based parsing", "author": ["Y. Goldberg", "F. Sartorio", "G. Satta"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Semantics in Generative Grammar", "author": ["I. Heim", "A. Kratzer"], "venue": null, "citeRegEx": "Heim and Kratzer,? \\Q1998\\E", "shortCiteRegEx": "Heim and Kratzer", "year": 1998}, {"title": "Structured perceptron with inexact search", "author": ["L. Huang", "S. Fayong", "Y. Guo"], "venue": "In Proceedings of NAACL", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Joint syntactic and semantic parsing with combinatory categorial grammar", "author": ["J. Krishnamurthy", "T.M. Mitchell"], "venue": null, "citeRegEx": "Krishnamurthy and Mitchell,? \\Q2014\\E", "shortCiteRegEx": "Krishnamurthy and Mitchell", "year": 2014}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["T. Kwiatkowski", "E. Choi", "Y. Artzi", "L. Zettlemoyer"], "venue": null, "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Lexical generalization in ccg grammar induction for semantic parsing", "author": ["T. Kwiatkowski", "L. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "A probabilistic forest-to-string model for language generation from typed lambda calculus expressions", "author": ["W. Lu", "H.T. Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Lu and Ng,? \\Q2011\\E", "shortCiteRegEx": "Lu and Ng", "year": 2011}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["J. Nivre"], "venue": "Computational Linguistics", "citeRegEx": "Nivre,? \\Q2008\\E", "shortCiteRegEx": "Nivre", "year": 2008}, {"title": "Advanced Topics in Types and Programming Languages", "author": ["B. Pierce"], "venue": null, "citeRegEx": "Pierce,? \\Q2005\\E", "shortCiteRegEx": "Pierce", "year": 2005}, {"title": "Types and Programming Languages", "author": ["B.C. Pierce"], "venue": null, "citeRegEx": "Pierce,? \\Q2002\\E", "shortCiteRegEx": "Pierce", "year": 2002}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Y.W. Wong", "R.J. Mooney"], "venue": "In Annual Meeting-Association for computational Linguistics,", "citeRegEx": "Wong and Mooney,? \\Q2007\\E", "shortCiteRegEx": "Wong and Mooney", "year": 2007}, {"title": "Max-violation perceptron and forced decoding for scalable mt training", "author": ["H. Yu", "L. Huang", "H. Mi", "K. Zhao"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Yu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2013}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L. Zettlemoyer", "M. Collins"], "venue": "In Proceedings of UAI", "citeRegEx": "Zettlemoyer and Collins,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins", "year": 2005}, {"title": "Online learning of relaxed ccg grammars for parsing to logical form", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "Proceedings of EMNLP-CoNLL-2007. Citeseer", "citeRegEx": "Zettlemoyer and Collins,? \\Q2007\\E", "shortCiteRegEx": "Zettlemoyer and Collins", "year": 2007}, {"title": "Shift-reduce ccg parsing", "author": ["Y. Zhang", "S. Clark"], "venue": "In Proceedings of ACL", "citeRegEx": "Zhang and Clark,? \\Q2011\\E", "shortCiteRegEx": "Zhang and Clark", "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "Most existing semantic parsing efforts employ a CKY-style bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus Zettlemoyer and Collins (2005); Lu and Ng (2011) or its variants Wong and Mooney (2007); Liang et al.", "startOffset": 154, "endOffset": 185}, {"referenceID": 7, "context": "Most existing semantic parsing efforts employ a CKY-style bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus Zettlemoyer and Collins (2005); Lu and Ng (2011) or its variants Wong and Mooney (2007); Liang et al.", "startOffset": 186, "endOffset": 203}, {"referenceID": 7, "context": "Most existing semantic parsing efforts employ a CKY-style bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus Zettlemoyer and Collins (2005); Lu and Ng (2011) or its variants Wong and Mooney (2007); Liang et al.", "startOffset": 186, "endOffset": 242}, {"referenceID": 7, "context": "Most existing semantic parsing efforts employ a CKY-style bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus Zettlemoyer and Collins (2005); Lu and Ng (2011) or its variants Wong and Mooney (2007); Liang et al. (2011). Although these works led to fairly accurate semantic parsers, there are two major drawbacks: efficiency and expressiveness.", "startOffset": 243, "endOffset": 263}, {"referenceID": 7, "context": "Most existing semantic parsing efforts employ a CKY-style bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus Zettlemoyer and Collins (2005); Lu and Ng (2011) or its variants Wong and Mooney (2007); Liang et al. (2011). Although these works led to fairly accurate semantic parsers, there are two major drawbacks: efficiency and expressiveness. First, as many researches in syntactic parsing Nivre (2008); Zhang and Clark (2011) have shown, compared to cubic-time CKY-style parsing, incremental parsing can achieve comparable accuracies while being linear-time, which means orders of magnitude faster in practice.", "startOffset": 243, "endOffset": 448}, {"referenceID": 7, "context": "Most existing semantic parsing efforts employ a CKY-style bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus Zettlemoyer and Collins (2005); Lu and Ng (2011) or its variants Wong and Mooney (2007); Liang et al. (2011). Although these works led to fairly accurate semantic parsers, there are two major drawbacks: efficiency and expressiveness. First, as many researches in syntactic parsing Nivre (2008); Zhang and Clark (2011) have shown, compared to cubic-time CKY-style parsing, incremental parsing can achieve comparable accuracies while being linear-time, which means orders of magnitude faster in practice.", "startOffset": 243, "endOffset": 472}, {"referenceID": 2, "context": "\u2022 In line with classical Montague theory Heim and Kratzer (1998), our parser is type-driven parsing instead of syntax-driven as in CCG-based efforts Zettlemoyer and Collins (2005); Kwiatkowski et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 2, "context": "\u2022 In line with classical Montague theory Heim and Kratzer (1998), our parser is type-driven parsing instead of syntax-driven as in CCG-based efforts Zettlemoyer and Collins (2005); Kwiatkowski et al.", "startOffset": 41, "endOffset": 180}, {"referenceID": 2, "context": "\u2022 In line with classical Montague theory Heim and Kratzer (1998), our parser is type-driven parsing instead of syntax-driven as in CCG-based efforts Zettlemoyer and Collins (2005); Kwiatkowski et al. (2011); Krishnamurthy and Mitchell (2014) (Section 2.", "startOffset": 41, "endOffset": 207}, {"referenceID": 2, "context": "\u2022 In line with classical Montague theory Heim and Kratzer (1998), our parser is type-driven parsing instead of syntax-driven as in CCG-based efforts Zettlemoyer and Collins (2005); Kwiatkowski et al. (2011); Krishnamurthy and Mitchell (2014) (Section 2.", "startOffset": 41, "endOffset": 242}, {"referenceID": 2, "context": "\u2022 In line with classical Montague theory Heim and Kratzer (1998), our parser is type-driven parsing instead of syntax-driven as in CCG-based efforts Zettlemoyer and Collins (2005); Kwiatkowski et al. (2011); Krishnamurthy and Mitchell (2014) (Section 2.3). \u2022 We introduce parametric polymorphism into natural language semantics (Section 3), along with proper treatment of subtype polymorphism, and implement Hindley-Milner style type inference (Pierce, 2005, Chap. 10) during parsing (Section 3.2).1 \u2022 We adapt the latent-variable max-violation perceptron training from machine translation Yu et al. (2013), which is a perfect fit for semantic parsing due to its huge search space (Section 4).", "startOffset": 41, "endOffset": 607}, {"referenceID": 2, "context": "The simply typed lambda calculus Heim and Kratzer (1998); Lu and Ng (2011) augments the system with types, including base types (entities e, truth values t, or numbers i), and function types (e.", "startOffset": 33, "endOffset": 57}, {"referenceID": 2, "context": "The simply typed lambda calculus Heim and Kratzer (1998); Lu and Ng (2011) augments the system with types, including base types (entities e, truth values t, or numbers i), and function types (e.", "startOffset": 33, "endOffset": 75}, {"referenceID": 2, "context": "We also note that this typechecking mechanism, inspired by the classical type-driven theory in linguistics Heim and Kratzer (1998), eliminates the need for an explicit encoding of direction as in CCG, which makes our formalism much simpler than the synchronous syntactic-semantic ones in most other semantic parsing efforts Zettlemoyer and Collins (2005, 2007); Wong and Mooney (2007).", "startOffset": 107, "endOffset": 131}, {"referenceID": 2, "context": "We also note that this typechecking mechanism, inspired by the classical type-driven theory in linguistics Heim and Kratzer (1998), eliminates the need for an explicit encoding of direction as in CCG, which makes our formalism much simpler than the synchronous syntactic-semantic ones in most other semantic parsing efforts Zettlemoyer and Collins (2005, 2007); Wong and Mooney (2007).", "startOffset": 107, "endOffset": 385}, {"referenceID": 2, "context": "This is different from classical simply-typed Montague semantics Heim and Kratzer (1998) which defines such predicates as type top\u2192t so that city(mississippi : st) returns false.", "startOffset": 65, "endOffset": 89}, {"referenceID": 3, "context": "We follow the Latent Variable Violation-Fixing Perceptron framework Huang et al. (2012); Yu et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 3, "context": "We follow the Latent Variable Violation-Fixing Perceptron framework Huang et al. (2012); Yu et al. (2013) for the training.", "startOffset": 68, "endOffset": 106}, {"referenceID": 13, "context": "Following Yu et al. (2013), we then find the step i\u2217 with the maximal score difference between the best reference partial derivation and the Viterbi partial derivation: i\u2217 \u2206 = argmax i w \u00b7\u2206\u03a6(x, di (x, y), d \u2212 i (x, y)),", "startOffset": 10, "endOffset": 27}, {"referenceID": 1, "context": "Unlike syntactic incremental parsing, where the forced decoding can be done in polynomial time Goldberg et al. (2014), we do not have an algorithm designed for efficient forced decoding.", "startOffset": 95, "endOffset": 118}, {"referenceID": 14, "context": "For evaluation, we follow Zettlemoyer and Collins (2005) to use precision and recall, where", "startOffset": 26, "endOffset": 57}, {"referenceID": 14, "context": "Following the scheme of Zettlemoyer and Collins (2007), we use the first 600 sentences of Geo880 as the training set and the rest 280 sentences as the testing set.", "startOffset": 24, "endOffset": 55}, {"referenceID": 14, "context": "Following the splitting scheme of Zettlemoyer and Collins (2005), we use 500 sentences as training set and 140 sentences as testing set.", "startOffset": 34, "endOffset": 65}, {"referenceID": 14, "context": "Following the splitting scheme of Zettlemoyer and Collins (2005), we use 500 sentences as training set and 140 sentences as testing set. Table 1 shows that our algorithm achieves significantly higher recall than existing method of Zettlemoyer and Collins (2005), although our precision is not as high as theirs.", "startOffset": 34, "endOffset": 262}, {"referenceID": 5, "context": "Kwiatkowski et al. (2013) use \u201con-the-fly\u201d matching to fetch the most possible predicate in the dataset for some MR subexpression.", "startOffset": 0, "endOffset": 26}, {"referenceID": 5, "context": "Kwiatkowski et al. (2013) use \u201con-the-fly\u201d matching to fetch the most possible predicate in the dataset for some MR subexpression. The matching happens at the end of parsing, and is constrained by the type of the subexpression. We do matching and parsing jointly, both of which are constrained by the typing, and affect the typing, which is more similar to how human do semantic parsing, i.e., we parse part of the sentence and bind that part to some specific meaning, and continue parsing using grounded meaning. Wong and Mooney (2007) also use type information to help reduce unnecessary tree joining in decoding.", "startOffset": 0, "endOffset": 537}], "year": 2014, "abstractText": "Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GEOQUERY, JOBS and ATIS domains.", "creator": "LaTeX with hyperref package"}}}