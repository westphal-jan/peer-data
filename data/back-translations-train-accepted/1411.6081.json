{"id": "1411.6081", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2014", "title": "PU Learning for Matrix Completion", "abstract": "In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M, and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only \"likes\" or \"friendships\" are observed. The problem of learning from only positive and unlabeled examples, called PU (positive-unlabeled) learning, has been studied in the context of binary classification. We consider the PU matrix completion problem, where an underlying real-valued matrix M is first quantized to generate one-bit observations and then a subset of positive entries is revealed. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a \"shifted matrix completion\" method that recovers M using only a subset of indices corresponding to ones, while for the second case, we propose a \"biased matrix completion\" method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds --- if M is n by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotes the fraction of ones observed. This implies a sample complexity of O(n\\log n) ones to achieve a small error, when M is dense and n is large. We extend our methods and guarantees to the inductive matrix completion problem, where rows and columns of M have associated features. We provide efficient and scalable optimization procedures for both the methods and demonstrate the effectiveness of the proposed methods for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks.", "histories": [["v1", "Sat, 22 Nov 2014 04:37:15 GMT  (167kb,D)", "http://arxiv.org/abs/1411.6081v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA stat.ML", "authors": ["cho-jui hsieh", "nagarajan natarajan", "inderjit s dhillon"], "accepted": true, "id": "1411.6081"}, "pdf": {"name": "1411.6081.pdf", "metadata": {"source": "CRF", "title": "PU Learning for Matrix Completion", "authors": ["Cho-Jui Hsieh", "Nagarajan Natarajan", "Inderjit S. Dhillon"], "emails": [], "sections": [{"heading": null, "text": "This implies a sample complexity of O (n log n) to achieve a small error when M is dense and n large. We extend our methods and guarantees to the recently proposed problem of inductive matrix completion, where rows and columns of M have associated features. We provide efficient and scalable optimization procedures for both methods and demonstrate the effectiveness of the proposed methods for link prediction (in real networks consisting of over 2 million nodes and 90 million links) and semi-monitored cluster tasks."}, {"heading": "1 Introduction", "text": "In fact, it is not that one sees oneself in a position to abide by the rules that one has imposed on oneself, but that one is in a position to abide by the rules that one has imposed on oneself. (...) It is not that one is in a position to understand the rules. (...) It is not that one is in a position to understand the rules. (...) It is not that one sees oneself in a position to understand the rules. (...) It is not that one is in a position to understand them. (...) It is not that one is in a position to understand them. (...) \"(...).\" (...) It is. \"(...).\" (...). \"(...).\" (...). \"(...).\" It is. (...). \"(...).\" (...). \"It is. (...).\" (. \"It is. (...).\" (. \"It is. (...).\" It is. (...). (). \"It is. (. (...).\" It is. (). (...). (. \"It is. (). (). (.\" It is. (...). (. (). \"It is. (...). (. (). (). (It is. (. (.). (.). (It is. (.).). (It is. (. (.). (.). (. (.). (It is. (.). (.). (It is.). (. (.). (. (.). (.). (. (It is. (.). (. (It is. (It is.). (.). (It is.). (. (. (.). (. (.). (It is.) It is. (.). (. (It is. (.). It is. (.). (. (.). (It is. It is.). (. (.). (.). (. (.). (It is. (.). It is. (.). It is. It is. (. (."}, {"heading": "2 Problem Settings", "text": "We assume that the underlying matrix M-Rm-n has a limited nuclear standard, i.e. that t is a constant independent of m and n. If Mij-Rm-n is for all (i, j), specifying the problem of completing the PU matrix is straightforward: we observe only a subset randomly collected from {(i, j) | Mij = 1}, and the goal is to restore M on the basis of this \"one-sided\" scanning. We call this the \"default setting.\" However, in real-world applications, it is unlikely that the underlying matrix is binary. Below, we will consider two general settings that include the default as a special case."}, {"heading": "2.1 Non-deterministic setting", "text": "In the non-deterministic setting, we assume that Mij has limits and can assume without loss of the general public that Mij (0, 1) applies to all (i, j) by normalizing it. We then regard each entry as a probability distribution that produces a clean 0-1 observation Y (Rm). In our PU learning model, we assume that only a subset of positive entries is observed by Y. More specifically, we observe a subset of Y where a subset of Y (j) is captured."}, {"heading": "2.2 Deterministic setting", "text": "In the deterministic setting, we can only hope to recover the underlying 0-1 matrix Y from the given Y problem by considering the process of restoring the existing Y as useful: Yij = I (Mij > q), where I (\u00b7) is the indicator function and q \u2212 R is the threshold. Also, in our PU learning model, we only assume a subset of positive entries from Y, i.e., we observe \"1\" from Y, where \"1\" is evenly scanned from \"i, j\" and \"2.\" It is impossible to recover M even if we observe all of the entries from Y. A trivial example is that all the matrices that Y = eeT will give when \"q\" and we cannot recover from Y. \"In the deterministic setting, we can only hope to recover the underlying 0-1 matrix problem from the existing Y observation."}, {"heading": "3 Proposed Algorithms for PU Matrix Completion", "text": "In this section we present two algorithms: postponed matrix completion for non-deterministic PU matrix completion and distorted matrix completion for deterministic PU matrix completion. All evidence is reset to Appendix A."}, {"heading": "3.1 Shifted Matrix Completion for Non-deterministic Setting (ShiftMC)", "text": "We want to find a matrix X in such a way that the loss of M \u2212 X \u2212 2F is limited, using the noisy observation matrix A (1) generated from M. Note that the noise in Aij is asymmetric in the context of binary classification, i.e. P (Aij = 0 | Yij = 1) and P (Aij = 1 | Yij = 0. Asymmetric noise has been studied in the context of binary classification, and most recently Natarajan et al. [2013] proposed an impartial estimator method to tie the true loss using only noisy observations. In our case, we aim to find a matrix that defines the unbiased estimator, which leads to the following optimization problem: min X (Aij, Aij), so that we tie the true loss using noisy observations."}, {"heading": "3.2 Biased Matrix Completion for Deterministic Setting (BiasMC)", "text": "In the deterministic setting, we suggest solving the matrix completion problem with the marker dependent loss (Scott, 2012). Let's call \"(x, a) = (x \u2212 a) 2 the squared loss, for an error where 1a = 1, 1a = 0 are indicator functions. We then restore the basic truth by solving the following distorted matrix completion problem (biasMC): X = argmin X = 0 '(Xij, Aij) = argmin i, j'\u03b1 = argmin X = 0 indicator functions."}, {"heading": "X such that, for any matrix X,", "text": "Minimizing the \u03b1-weighted expected error in the partially observed situation is equivalent to minimizing the true recovery error R. By continuing to relate R\u03b1 *, \u03c1 (X) and R'\u03b1 *, \u03c1 (X), we can show: Theorem 5 (main result 2). If X is the minimizer of (17) and X is the threshold of 0-1 matrix of X, then with a probability of at least 1 \u2212 \u03b4, we have R (X) \u2264 2\u03b7 1 + \u03c1 (Ct \u221a n + 270 m + 4 \u221a smn + 3 \u221a log (2 / \u043c) \u221a mn (1 \u2212 \u03c1), where \u03b7 = max (1 / q2, 1 / (1 \u2212 q) 2) and C is a constant. The average error is in the order of O (1n (1 \u2212 2) if M'R n \u00d7 n, where 1 \u2212 2 is similar to the observed ratio of 1 the MC."}, {"heading": "4 PU Inductive Matrix Completion", "text": "In this section, we expand our approach to the problem of inductive matrix completion, where in addition to samples, row and column characteristics Fu-Rm-d, Fv-Rn-d, observations from the basic truth M-Rm-d are also used, and we want to restore M by solving the following optimization problem: min D-Rd-d-i, j-i-i (Aij \u2212 (FuDF Tv) ij) 2 + zi-D-D. (20) Matrix completion is a special case of inductive matrix completion when Fu = I, Fv = I. In the multi-marked learning problem, M represents the label matrix and Fu corresponds to the examples (typically Fv = I) [Yu et al., 2014, Xu et al., 2013]."}, {"heading": "4.1 Shifted Inductive Matrix Completion for Non-deterministic Setting", "text": "In the non-deterministic setting, we look at the inductive version of ShiftMC: min D-Rd-d-II, j-II (FuDF T v) ij, Aij) so that we are Fu, Fv \u2212 n orthogonal (otherwise we can perform a pre-processing step to normalize it), let us be the i-th row of Fu (the attribute for row i) and vj the j-th row of Fv. Fine constants Xu \u2212 n, Xv = maxj to normalize it, let us be the i-th row of Fu (the attribute for row i) and vj the j-th row of Fv."}, {"heading": "4.2 Biased Inductive Matrix Completion for Deterministic Setting", "text": "In the deterministic setting, we propose solving the inductive version of BiasMC: D-1 = arg min D-1-1 (FuDF T v), j: Aij = 1 ((FuDF T v) ij \u2212 1) 2 + (1 \u2212 \u03b1) \u2211 i, j: Aij = 0 (FuDF T v) 2 ij. (23) The clean 0-1 matrix Y can then be converted by Y-1 (FuD-F T v) ij > q).Y-1 if (FuD-F T v) ij \u2265 q0 if (FuD-F T v) ij < q. (24) Similar to the matrix completion, Lemma 2 shows that the expected 0-1 error R (X) and the \u03b1-weighted expected error in the silent observation R\u03b1, \u03c1 (X) can be converted by a linear transformation if we have no threshold."}, {"heading": "5 Optimization Techniques for PU Matrix Completion", "text": "In this section we show that BiasMC can be calculated very efficiently using a method or procedure where we have a large volume of data (millions of rows and columns), and that ShiftMC can be efficiently solved after relaxation. First, we consider the optimization problem for BiasMC: argmin X\u03b1 (1), which is equivalent to the limited problem (17). The typical proximal gradient update isX (X), where the learning rate and S of the soft threshold are based on singular values (Ji and Ye, 2009). The (approximate) SVD of G: = (X \u2212 2), which we can use efficiently."}, {"heading": "6 Experiments", "text": "We first use synthetic data to show that our boundaries make sense, and then demonstrate the effectiveness of our algorithms in the real world."}, {"heading": "6.1 Synthetic Data", "text": "We assume that the underlying matrix M-Rn-n is generated by UUT, where U-Rn-k is the orthogonal basis of a random Gaussian n-by-k matrix with mean 0 and variance 1. For the non-deterministic setting, we set M linearly to values in [0, 1] and then generate training samples according to Section 2. For the deterministic setting, we choose q so that Y has the same number of zeros and ones. We fix \u03c1 = 0.9 (so that only 10% of 1's are observed). From Lemma 2, \u03b1 = 0.95 is optimal. We fix k = 10 and test our algorithms with different sizes n. The results are shown in Figure 1 (a) - (b). Interestingly, the results reflect our theory: Errors of our estimators decrease with n; in particular errors of linear decays with n scaled in log-log-log-log diagrams, indicating a rate of 1 / 4, as in problem O and n."}, {"heading": "6.2 Parameter Selection", "text": "Before showing the experimental results on problems in the real world, we will discuss the selection of the parameter \u03c1 in our PU matrix completion model (see eq (1)). Note that \u03c1 indicates the noise rate when flipping a 1 to 0. If there is the same number of positive and negative elements in the underlying matrix Y, we have \u03c1 = 1 \u2212 2s, where s = (# positive entries) / (# total entries). In practice (e.g. link prediction problems), the number of 1s is usually smaller than the number of 0 in the underlying matrix, but we do not know the ratio. Therefore, in all experiments, we have \u03c1 selected from the set {1 \u2212 2s, 10 (1 \u2212 2s), 100 (1 \u2212 2s), 1000 (1 \u2212 2s)} based on a random validation set and use the corresponding \u03b1 in the optimization problems."}, {"heading": "6.3 Matrix completion for link prediction", "text": "One of the most important applications that have motivated our analysis in this work is the linknumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumbernumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumnumcatalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs # Catalogs #"}, {"heading": "6.4 Inductive matrix completion", "text": "We use the semi-monitored cluster problem to evaluate our PU inductive matrix completion methods. PU inductive matrix completion can be applied to many real problems, including recommendation systems with characteristics and 0-1 observations, and the semi-monitored cluster problem if we can only observe positive relationships. Here, we use the latter as an example to demonstrate the usefulness of our algorithm.In semi-monitored cluster problems, we obtain n samples with characteristics {xi} ni = 1 and paired relationships A-Rn, where Aij = 1, if two samples are in the same cluster, Aij = -1, if they are in different clusters, and Aij = 0, if the relationship is not observed. That the basic truth matrix M-1, \u2212 1} n-n-n has a simple structure and has a low date matrix, as well as a low trace standard matrix; it is demonstrated in [Yi-IMi] that we can re-establish the relationships."}, {"heading": "7 Conclusions", "text": "Motivated by modern matrix completion applications, our work attempts to bridge the gap between matrix completion theory and practice. We have shown that the underlying matrix can be accurately restored even with noise in the form of one-bit quantization and one-sided scanning process that reveals the measurements. We have considered two recovery scenarios, both of which are natural for PU learning, and have given similar recovery guarantees for both. Our error limits are strong and useful in practice. Our work serves to gain initial theoretical insights into the distorted matrix completion approach, which has been used in the past as heuristics for similar problems. Experimental results on synthetic data agree with our theory; the effectiveness of our methods are evident for the task of predicting links in real networks."}, {"heading": "A Proofs", "text": "(EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA)) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA)) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA)) (EA) (EA) (EA)) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) () (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) (EA) () () (EA) (EA) (EA) (EA) (EA) (EA) () (EA) (EA) (EA) (EA) (EA)"}], "references": [{"title": "Learning classifiers from only positive and unlabeled data", "author": ["C. Elkan", "K. Noto"], "venue": null, "citeRegEx": "Elkan and Noto.,? \\Q2012\\E", "shortCiteRegEx": "Elkan and Noto.", "year": 2012}, {"title": "Provable inductive matrix completion", "author": ["P. Jain", "I.S. Dhillon"], "venue": "CoRR, abs/1306.0626,", "citeRegEx": "2014", "shortCiteRegEx": "2014", "year": 2013}, {"title": "Some estimates of norms of random matrices", "author": ["R. Latala"], "venue": "Proceedings of the AMS,", "citeRegEx": "2003", "shortCiteRegEx": "2003", "year": 2005}, {"title": "Calibrated asymmetric surrogate losses", "author": ["C. Scott"], "venue": "Electronic J. of Stat.,", "citeRegEx": "2013", "shortCiteRegEx": "2013", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "In the context of classification, methods for learning in the presence of positive and unlabeled examples only, called positive-unlabeled (PU in short) learning, have been studied in the past [Elkan and Noto, 2008, Liu et al., 2003]. For matrix completion, can one guarantee recovery when only a subset of positive entries is observed? In this paper, we formulate the PU matrix completion problem and answer the question in the affirmative under different settings. Minimizing squared loss on the observed entries corresponding to 1\u2019s, subject to the low-rank constraints, yields a degenerate solution \u2014 the rank-1 matrix with all its entries equal to 1 achieves zero loss. In practice, a popular heuristic used is to try and complete the matrix by treating some or all of the missing observations as true 0\u2019s, which seems to be a good strategy when the underlying matrix has a small number of positive examples, i.e., small number of 1\u2019s. This motivates viewing the problem of learning from only positive samples as a certain noisy matrix completion problem. Existing theory for noise-tolerant matrix completion [Cand\u00e8s and Plan, 2009, Davenport et al., 2012] does not sufficiently address recoverability under PU learning (see Section 2). In our work, we assume that the true matrix M \u2208 Rm\u00d7n has a bounded nuclear norm \u2016M\u2016\u2217. The PU learning model for matrix completion is specified by a certain one-bit quantization process that generates a binary matrix Y from M and a one-sided sampling process that reveals a subset of positive entries of Y . In particular, we consider two recovery settings for PU matrix completion: The first setting is non-deterministic \u2014 M parameterizes a probability distribution which is used to generate the entries of Y . We show that it is possible to recover M using only a subset of positive entries of Y . The idea is to minimize an unbiased estimator of the squared loss between the estimated and the observed \u201cnoisy\u201d entries, motivated by the approach in Natarajan et al. [2013]. We recast the objective as a \u201cshifted matrix completion\u201d problem that facilitates in obtaining a scalable optimization algorithm.", "startOffset": 193, "endOffset": 2015}, {"referenceID": 0, "context": "In the context of classification, methods for learning in the presence of positive and unlabeled examples only, called positive-unlabeled (PU in short) learning, have been studied in the past [Elkan and Noto, 2008, Liu et al., 2003]. For matrix completion, can one guarantee recovery when only a subset of positive entries is observed? In this paper, we formulate the PU matrix completion problem and answer the question in the affirmative under different settings. Minimizing squared loss on the observed entries corresponding to 1\u2019s, subject to the low-rank constraints, yields a degenerate solution \u2014 the rank-1 matrix with all its entries equal to 1 achieves zero loss. In practice, a popular heuristic used is to try and complete the matrix by treating some or all of the missing observations as true 0\u2019s, which seems to be a good strategy when the underlying matrix has a small number of positive examples, i.e., small number of 1\u2019s. This motivates viewing the problem of learning from only positive samples as a certain noisy matrix completion problem. Existing theory for noise-tolerant matrix completion [Cand\u00e8s and Plan, 2009, Davenport et al., 2012] does not sufficiently address recoverability under PU learning (see Section 2). In our work, we assume that the true matrix M \u2208 Rm\u00d7n has a bounded nuclear norm \u2016M\u2016\u2217. The PU learning model for matrix completion is specified by a certain one-bit quantization process that generates a binary matrix Y from M and a one-sided sampling process that reveals a subset of positive entries of Y . In particular, we consider two recovery settings for PU matrix completion: The first setting is non-deterministic \u2014 M parameterizes a probability distribution which is used to generate the entries of Y . We show that it is possible to recover M using only a subset of positive entries of Y . The idea is to minimize an unbiased estimator of the squared loss between the estimated and the observed \u201cnoisy\u201d entries, motivated by the approach in Natarajan et al. [2013]. We recast the objective as a \u201cshifted matrix completion\u201d problem that facilitates in obtaining a scalable optimization algorithm. The second setting is deterministic \u2014 Y is obtained by thresholding the entries of M (modeling how the users vote), and then a subset of positive entries of Y is revealed. While recovery of M is not possible (see Section 2), we show that we can recover Y with low error. To this end, we propose a scalable biased matrix completion method where the observed and the unobserved entries of Y are penalized differently. Recently, an inductive approach to matrix completion was proposed [Jain and Dhillon, 2013] where the matrix entries are modeled as a bilinear function of real-valued features associated with the rows and the columns. We extend our methods under the two aforementioned settings to the inductive matrix completion problem and establish similar recovery guarantees. Our contributions are summarized below: 1. To the best of our knowledge, this is the first paper to formulate and study PU learning for matrix completion, necessitated by the applications of matrix completion. Furthermore, we extend our results to the recently proposed inductive matrix completion problem. 2. We provide strong guarantees for recovery; for example, in the non-deterministic setting, the error in recovering an n \u00d7 n matrix is O( 1 (1\u2212\u03c1)n) for our method compared to O( 1 (1\u2212\u03c1) \u221a n ) implied by the method in Davenport et al. [2012], where (1 \u2212 \u03c1) is the fraction of observed 1\u2019s.", "startOffset": 193, "endOffset": 3474}, {"referenceID": 3, "context": "[2013], Yi et al. [2013], we assume the features are good enough such that M = Fu(Fu) MFv(Fv) T .", "startOffset": 1, "endOffset": 25}, {"referenceID": 2, "context": "We compare with competing link prediction methods [Kiben-Nowell and Kleinberg, 2003] Common Neighbors, Katz, and SVD-Katz (compute Katz using the rank-k approximation, A \u2248 Uk\u03a3kVk). Note that the classical matrix factorization approach in this case is equivalent to SVD on the given 0-1 training matrix, and SVD-Katz slightly improves over SVD by further computing the Katz values based on the low rank approximation (see Kiben-Nowell and Kleinberg [2003]), so we omit the SVD results in the figures.", "startOffset": 79, "endOffset": 455}], "year": 2014, "abstractText": "In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M , and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only \u201clikes\u201d or \u201cfriendships\u201d are observed. The problem of learning from only positive and unlabeled examples, called PU (positive-unlabeled) learning, has been studied in the context of binary classification. We consider the PU matrix completion problem, where an underlying real-valued matrix M is first quantized to generate one-bit observations and then a subset of positive entries is revealed. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a \u201cshifted matrix completion\u201d method that recovers M using only a subset of indices corresponding to ones, while for the second case, we propose a \u201cbiased matrix completion\u201d method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds \u2014 if M \u2208 Rn\u00d7n, the Frobenius error is bounded as O ( 1 (1\u2212\u03c1)n ) , where 1\u2212\u03c1 denotes the fraction of ones observed. This implies a sample complexity of O(n log n) ones to achieve a small error, when M is dense and n is large. We extend our methods and guarantees to the recently proposed inductive matrix completion problem, where rows and columns of M have associated features. We provide efficient and scalable optimization procedures for both the methods and demonstrate the effectiveness of the proposed methods for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks.", "creator": "LaTeX with hyperref package"}}}