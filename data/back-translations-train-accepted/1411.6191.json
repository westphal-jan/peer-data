{"id": "1411.6191", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Kickback Cuts Backprop's Red-Tape: Biologically Plausible Credit Assignment in Neural Networks", "abstract": "Error backpropagation is an extremely effective algorithm for assigning credit in artificial neural networks. However, weight updates under Backprop depend on lengthy recursive computations and require separate output and error messages -- features not shared by biological neurons, that are perhaps unnecessary. In this paper, we revisit Backprop and the credit assignment problem. We first decompose Backprop into a collection of interacting learning algorithms; provide regret bounds on the performance of these sub-algorithms; and factorize Backprop's error signals. Using these results, we derive a new credit assignment algorithm for nonparametric regression, Kickback, that is significantly simpler than Backprop. Finally, we provide a sufficient condition for Kickback to follow error gradients, and show that Kickback matches Backprop's performance on real-world regression benchmarks.", "histories": [["v1", "Sun, 23 Nov 2014 04:58:22 GMT  (317kb,D)", "http://arxiv.org/abs/1411.6191v1", "7 pages. To appear, AAAI-15"]], "COMMENTS": "7 pages. To appear, AAAI-15", "reviews": [], "SUBJECTS": "cs.LG cs.NE q-bio.NC", "authors": ["david balduzzi", "hastagiri vanchinathan", "joachim m buhmann"], "accepted": true, "id": "1411.6191"}, "pdf": {"name": "1411.6191.pdf", "metadata": {"source": "CRF", "title": "Kickback cuts Backprop\u2019s red-tape: Biologically plausible credit assignment in neural networks", "authors": ["David Balduzzi", "Hastagiri Vanchinathan", "Joachim Buhmann"], "emails": ["david.balduzzi@vuw.ac.nz", "hastagiri@inf.ethz.ch", "jbuhmann@inf.ethz.ch"], "sections": [{"heading": "Introduction", "text": "This year it is more than ever before."}, {"heading": "Error Backpropagation", "text": "The current work has shown that reciprocal functions instead of sigmoids can significantly improve the performance of neural networks (= >). We limit ourselves to rectifying measures because empirically they work well (Jarrett et al. 2009; Nair and Hinton 2010), but are more realistic than the cortical neurons (Glorot, Bordes and Bengio 2011), and are universal functional approximations (Leshno et al. 1993).Denote the positive and negative rectifiers of P (a): = max (a) and N (a): \u2212 max (0, a). Rectifiers are continuous everywhere and everywhere except at 0."}, {"heading": "Biological relevance", "text": "There is a direct link between kickback and neurobiology provided by the Selectron: a > simplified model neurons (Balduzziand Besserve 2012); the Selectron derives from standard models of neuronal dynamics and learning - the Spike Response Model (SRM) and Spike-Timing Dependent Plasticity (STDP) - by taking the so-called \"Fast-Time Constant Limit\" to go from continuous to discrete time. Theorem 5 (selectron) The fast time limit of the SRM (Gerstner and Kistler 2002) is a node that issues the so-called \"Fast-Time Constant Limit\" when < w > 0 and 0 otherwise.Weight updates are in the fast time-constant limit of neuromodulated STDP (Song, Miller and Abbott 2000)."}, {"heading": "Experiments", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but a country, a country and a country."}, {"heading": "Conclusion", "text": "A necessary step in understanding how the brain allocates credit is to develop a minimal work model that fits basic constraints. Backprop solves the problem of lending. It is one of the simplest and most effective ways to learn representations. Combined with various tricks and optimizations, it continues to deliver cutting-edge performance. However, it disregards a fundamental constraint imposed by neurobiology: It requires that nodes produce error signals that differ from their outputs.Kickback is a stripped-down version of backprop that is motivated by theoretical (theorems 1-4) and biological (figs. 1 and theorem 5) considerations. Under Kickback, the use of nodes that differ from their output.Kickback activities - produced by the next layer - is a slimmed-down one. The sign of global error determines whether nodes follow the gradient downwards or upwards. Kickback is the first competitive algorithmically plausible lending algorithm."}], "references": [{"title": "Towards a learning-theoretic analysis of spike-timing dependent plasticity", "author": ["D. Balduzzi", "M. Besserve"], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Balduzzi and Besserve,? 2012", "shortCiteRegEx": "Balduzzi and Besserve", "year": 2012}, {"title": "Randomized co-training: from cortical neurons to machine learning and back again", "author": ["D. Balduzzi"], "venue": "Randomized Methods for Machine Learning Workshop, Neural Inf Proc Systems (NIPS).", "citeRegEx": "Balduzzi,? 2013", "shortCiteRegEx": "Balduzzi", "year": 2013}, {"title": "Cortical prediction markets", "author": ["D. Balduzzi"], "venue": "Proc. 13th Int Conf on Autonomous Agents and Multiagent Systems (AAMAS).", "citeRegEx": "Balduzzi,? 2014", "shortCiteRegEx": "Balduzzi", "year": 2014}, {"title": "Theano: A CPU and GPU Math Expression Compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proc. Python for Scientific Comp. Conf. (SciPy).", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Prediction, Learning and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press.", "citeRegEx": "Cesa.Bianchi and Lugosi,? 2006", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "The recent excitement about neural networks", "author": ["F. Crick"], "venue": "Nature 337(12):129\u2013132.", "citeRegEx": "Crick,? 1989", "shortCiteRegEx": "Crick", "year": 1989}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).", "citeRegEx": "Dahl et al\\.,? 2013", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Twenty-Five Lessons from Computational Neuromodulation", "author": ["P. Dayan"], "venue": "Neuron 76:240\u2013256.", "citeRegEx": "Dayan,? 2012", "shortCiteRegEx": "Dayan", "year": 2012}, {"title": "Spiking Neuron Models", "author": ["W. Gerstner", "W. Kistler"], "venue": "Cambridge University Press.", "citeRegEx": "Gerstner and Kistler,? 2002", "shortCiteRegEx": "Gerstner and Kistler", "year": 2002}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proc. 14th International Conference on Artificial Intelligence and Statistics (AISTATS).", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "The convex optimization approach to regret minimization", "author": ["E. Hazan"], "venue": "Sra, S.; Nowozin, S.; and Wright, S. J., eds., Optimization for machine learning. MIT Press.", "citeRegEx": "Hazan,? 2012", "shortCiteRegEx": "Hazan", "year": 2012}, {"title": "A Neuron as a Signal Processing Device", "author": ["T. Hu", "Z.J. Towfic", "C. Pehlevan", "A. Genkin", "D.B. Chklovskii"], "venue": "Asilomar Conference on Signals, Systems and Computers.", "citeRegEx": "Hu et al\\.,? 2013", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Neuromorphic silicon neuron circuits", "author": ["G. Indiveri", "B. Linares-Barranco", "T.J. Hamilton", "A. van Schaik", "R. Etienne-Cummings", "T. Delbruck", "S.-C. Liu", "P. Dudek", "P. H\u00e4fliger"], "venue": null, "citeRegEx": "Indiveri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Indiveri et al\\.", "year": 2011}, {"title": "What is the Best Multi-Stage Architecture for Object Recognition? In Proc", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "International Conference on Computer Vision (ICCV).", "citeRegEx": "Jarrett et al\\.,? 2009", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Gradient Weights help Nonparametric Regressors", "author": ["S. Kpotufe", "A. Boularias"], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Kpotufe and Boularias,? 2013", "shortCiteRegEx": "Kpotufe and Boularias", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The distinct modes of vision offered by feedforward and recurrent processing", "author": ["V. Lamme", "P. Roelfsema"], "venue": "Trends in Neurosci. 23(11):571\u2013579.", "citeRegEx": "Lamme and Roelfsema,? 2000", "shortCiteRegEx": "Lamme and Roelfsema", "year": 2000}, {"title": "Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function", "author": ["M. Leshno", "V.Y. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural Networks 6:861\u2013867.", "citeRegEx": "Leshno et al\\.,? 1993", "shortCiteRegEx": "Leshno et al\\.", "year": 1993}, {"title": "Rectifier Nonlinearities Improve Neural Network Acoustic Models", "author": ["A.L. Maas", "A.Y. Hannun", "A. Ng"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML).", "citeRegEx": "Maas et al\\.,? 2013", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML).", "citeRegEx": "Nair and Hinton,? 2010", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "A neuromorphic architecture for object recognition and motion anticipation using burst-STDP", "author": ["A. Nere", "U. Olcese", "D. Balduzzi", "G. Tononi"], "venue": "PLoS One 7(5):e36958.", "citeRegEx": "Nere et al\\.,? 2012", "shortCiteRegEx": "Nere et al\\.", "year": 2012}, {"title": "Attention-gated reinforcement learning of internal representations for classification", "author": ["P.R. Roelfsema", "A. van Ooyen"], "venue": "Neural Comput 17(10):2176\u20132214", "citeRegEx": "Roelfsema and Ooyen,? \\Q2005\\E", "shortCiteRegEx": "Roelfsema and Ooyen", "year": 2005}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature 323:533\u2013 536.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "A neural substrate of prediction and reward", "author": ["W. Schultz", "P. Dayan", "P. Montague"], "venue": "Science 275(1593-1599).", "citeRegEx": "Schultz et al\\.,? 1997", "shortCiteRegEx": "Schultz et al\\.", "year": 1997}, {"title": "Pandemonium: a paradigm for learning", "author": ["O.G. Selfridge"], "venue": "Mechanisation of Thought Processes: Proceedings of a Symposium Held at the National Physics Laboratory.", "citeRegEx": "Selfridge,? 1958", "shortCiteRegEx": "Selfridge", "year": 1958}, {"title": "Formal models and algorithms for decentralized decision making under uncertainty", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Auton Agent Multi-Agent Syst 17(2):190\u2013250.", "citeRegEx": "Seuken and Zilberstein,? 2008", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2008}, {"title": "Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission", "author": ["H.S. Seung"], "venue": "Neuron 40(10631073).", "citeRegEx": "Seung,? 2003", "shortCiteRegEx": "Seung", "year": 2003}, {"title": "Competitive Hebbian learning through spike-timing-dependent synaptic plasticity", "author": ["S. Song", "K.D. Miller", "L.F. Abbott"], "venue": "Nat Neurosci 3(9).", "citeRegEx": "Song et al\\.,? 2000", "shortCiteRegEx": "Song et al\\.", "year": 2000}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Motor Interaction", "author": ["R. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "Proc. 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS).", "citeRegEx": "Sutton et al\\.,? 2011", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Sleep and the Price of Plasticity: From Synaptic and Cellular Homeostasis to Memory Consolidation and Integration", "author": ["G. Tononi", "C. Cirelli"], "venue": "Neuron 81(1):12\u201334.", "citeRegEx": "Tononi and Cirelli,? 2014", "shortCiteRegEx": "Tononi and Cirelli", "year": 2014}, {"title": "A slow fraction of Mg2+ unblock of NMDA receptors limits their contribution to spike generation in cortical pyramidal neurons", "author": ["M. Vargas-Caballero", "H.P. Robinson"], "venue": "J Neurophysiol 89(5):2778\u201383.", "citeRegEx": "Vargas.Caballero and Robinson,? 2003", "shortCiteRegEx": "Vargas.Caballero and Robinson", "year": 2003}, {"title": "Reinforcement Learning via AIXI Approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "D. Silver"], "venue": "Proc. 24th AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Veness et al\\.,? 2010", "shortCiteRegEx": "Veness et al\\.", "year": 2010}, {"title": "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences", "author": ["P.J. Werbos"], "venue": "Ph.D. Dissertation, Harvard.", "citeRegEx": "Werbos,? 1974", "shortCiteRegEx": "Werbos", "year": 1974}, {"title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "author": ["R.J. Williams"], "venue": "Machine Learning 8:229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "On Rectified Linear Units for Speech Processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G. Hinton"], "venue": "IEEE Int Conf on Acoustics, Speech and Signal Proc (ICASSP).", "citeRegEx": "Zeiler et al\\.,? 2013", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 33, "context": "The discovery of error backpropagation was hailed as a breakthrough because it solved the main problem of distributed learning \u2013 the spatial credit assignment problem (Werbos 1974; Rumelhart, Hinton, and Williams 1986).", "startOffset": 167, "endOffset": 218}, {"referenceID": 28, "context": "number of layers and number of nodes); regularizers such as dropout (Srivastava et al. 2014); modifications to accelerate gradient descent; and unsupervised methods for pre-training to find better local optima.", "startOffset": 68, "endOffset": 92}, {"referenceID": 5, "context": "However, it was known from the start that Backprop is not biologically plausible (Crick 1989).", "startOffset": 81, "endOffset": 93}, {"referenceID": 16, "context": "Implementing Backprop requires that neurons produce two distinct signals \u2013 outputs and errors \u2013 whereas only one has been observed in cortex (Lamme and Roelfsema 2000; Roelfsema and van Ooyen 2005).", "startOffset": 141, "endOffset": 197}, {"referenceID": 0, "context": "We discuss Kickback\u2019s biological significance by relating it to a recently developed, discrete-time model neuron (Balduzzi and Besserve 2012).", "startOffset": 113, "endOffset": 141}, {"referenceID": 24, "context": "The idea of building learning algorithms out of individual learning agents dates back to at least (Selfridge 1958).", "startOffset": 98, "endOffset": 114}, {"referenceID": 34, "context": "More recent approaches include REINFORCE (Williams 1992), the hedonistic neurons in (Seung 2003), and the neurons modeled using online learning in (Hu et al.", "startOffset": 41, "endOffset": 56}, {"referenceID": 26, "context": "More recent approaches include REINFORCE (Williams 1992), the hedonistic neurons in (Seung 2003), and the neurons modeled using online learning in (Hu et al.", "startOffset": 84, "endOffset": 96}, {"referenceID": 11, "context": "More recent approaches include REINFORCE (Williams 1992), the hedonistic neurons in (Seung 2003), and the neurons modeled using online learning in (Hu et al. 2013).", "startOffset": 147, "endOffset": 163}, {"referenceID": 13, "context": "We restrict to rectifiers because they perform well empirically (Jarrett et al. 2009; Nair and Hinton 2010; Glorot, Bordes, and Bengio 2011; Krizhevsky, Sutskever, and Hinton 2012; Zeiler et al. 2013; Dahl, Sainath, and Hinton 2013; Maas, Hannun, and Ng 2013), are more realistic models of cortical neurons than sigmoid units (Glorot, Bordes, and Bengio 2011), and are universal function approximators (Leshno et al.", "startOffset": 64, "endOffset": 259}, {"referenceID": 19, "context": "We restrict to rectifiers because they perform well empirically (Jarrett et al. 2009; Nair and Hinton 2010; Glorot, Bordes, and Bengio 2011; Krizhevsky, Sutskever, and Hinton 2012; Zeiler et al. 2013; Dahl, Sainath, and Hinton 2013; Maas, Hannun, and Ng 2013), are more realistic models of cortical neurons than sigmoid units (Glorot, Bordes, and Bengio 2011), and are universal function approximators (Leshno et al.", "startOffset": 64, "endOffset": 259}, {"referenceID": 35, "context": "We restrict to rectifiers because they perform well empirically (Jarrett et al. 2009; Nair and Hinton 2010; Glorot, Bordes, and Bengio 2011; Krizhevsky, Sutskever, and Hinton 2012; Zeiler et al. 2013; Dahl, Sainath, and Hinton 2013; Maas, Hannun, and Ng 2013), are more realistic models of cortical neurons than sigmoid units (Glorot, Bordes, and Bengio 2011), and are universal function approximators (Leshno et al.", "startOffset": 64, "endOffset": 259}, {"referenceID": 17, "context": "2013; Dahl, Sainath, and Hinton 2013; Maas, Hannun, and Ng 2013), are more realistic models of cortical neurons than sigmoid units (Glorot, Bordes, and Bengio 2011), and are universal function approximators (Leshno et al. 1993).", "startOffset": 207, "endOffset": 227}, {"referenceID": 4, "context": "If the node fires then the rectilinear loss is the linear loss `L(w, \u03c6 \u00b7x) := \u3008w, \u03c6 \u00b7x\u3009, which has been extensively analyzed in online learning (Cesa-Bianchi and Lugosi 2006).", "startOffset": 144, "endOffset": 174}, {"referenceID": 10, "context": "The theorem follows from a well-known result on gradient descent for the linear loss, see (Hazan 2012).", "startOffset": 90, "endOffset": 102}, {"referenceID": 2, "context": "We thus have a framework for experimenting with alternate feedback signals (Balduzzi 2014).", "startOffset": 75, "endOffset": 90}, {"referenceID": 0, "context": "Biological relevance There is a direct link from Kickback to neurobiology provided by the selectron: a simplified model neuron (Balduzzi and Besserve 2012).", "startOffset": 127, "endOffset": 155}, {"referenceID": 8, "context": "The fast time-constant limit of the SRM (Gerstner and Kistler 2002) is a node that outputs 1 if \u3008w,x\u3009 > 0 and 0 otherwise.", "startOffset": 40, "endOffset": 67}, {"referenceID": 0, "context": "(Balduzzi and Besserve 2012).", "startOffset": 0, "endOffset": 28}, {"referenceID": 31, "context": "The kickback term, \u03c4j , corresponds to NMDA backconnections that have a multiplicative effect on synaptic updates, proportional to the weighted sum of downstream activity (Vargas-Caballero and Robinson 2003; Roelfsema and van Ooyen 2005).", "startOffset": 171, "endOffset": 237}, {"referenceID": 0, "context": "Indeed, limits on the physical size and metabolic budget of synapses suggest that synaptic weights may be constrained to an `1-ball (Balduzzi and Besserve 2012).", "startOffset": 132, "endOffset": 160}, {"referenceID": 0, "context": "The generalization bound for the selectron in (Balduzzi and Besserve 2012) assumes that inputs are i.", "startOffset": 46, "endOffset": 74}, {"referenceID": 3, "context": "Experiments were implemented in Theano (Bergstra et al. 2010).", "startOffset": 39, "endOffset": 61}, {"referenceID": 1, "context": "cause them to fire (Balduzzi 2013).", "startOffset": 19, "endOffset": 34}, {"referenceID": 14, "context": "(Kpotufe and Boularias 2013).", "startOffset": 0, "endOffset": 28}, {"referenceID": 12, "context": "Kickback\u2019s simplified signaling is suited to hardware implementations (Indiveri et al. 2011; Nere et al. 2012).", "startOffset": 70, "endOffset": 110}, {"referenceID": 20, "context": "Kickback\u2019s simplified signaling is suited to hardware implementations (Indiveri et al. 2011; Nere et al. 2012).", "startOffset": 70, "endOffset": 110}, {"referenceID": 25, "context": "An important outcome of the paper is a new formulation of Backprop in terms of interacting local learners, that may connect deep learning to recent developments in multi-agent systems (Seuken and Zilberstein 2008; Sutton et al. 2011) and mechanism design (Balduzzi 2014).", "startOffset": 184, "endOffset": 233}, {"referenceID": 29, "context": "An important outcome of the paper is a new formulation of Backprop in terms of interacting local learners, that may connect deep learning to recent developments in multi-agent systems (Seuken and Zilberstein 2008; Sutton et al. 2011) and mechanism design (Balduzzi 2014).", "startOffset": 184, "endOffset": 233}, {"referenceID": 2, "context": "2011) and mechanism design (Balduzzi 2014).", "startOffset": 27, "endOffset": 42}, {"referenceID": 7, "context": "Indeed, modeling the neuromodulatory system as producing scalar outputs is a vast oversimplification (Dayan 2012).", "startOffset": 101, "endOffset": 113}, {"referenceID": 32, "context": "Finally, reinforcement learning is a better model of how an agent adapts to its environment than supervised learning (Veness et al. 2010).", "startOffset": 117, "endOffset": 137}], "year": 2014, "abstractText": "Error backpropagation is an extremely effective algorithm for assigning credit in artificial neural networks. However, weight updates under Backprop depend on lengthy recursive computations and require separate output and error messages \u2013 features not shared by biological neurons, that are perhaps unnecessary. In this paper, we revisit Backprop and the credit assignment problem. We first decompose Backprop into a collection of interacting learning algorithms; provide regret bounds on the performance of these sub-algorithms; and factorize Backprop\u2019s error signals. Using these results, we derive a new credit assignment algorithm for nonparametric regression, Kickback, that is significantly simpler than Backprop. Finally, we provide a sufficient condition for Kickback to follow error gradients, and show that Kickback matches Backprop\u2019s performance on real-world regression benchmarks.", "creator": "TeX"}}}