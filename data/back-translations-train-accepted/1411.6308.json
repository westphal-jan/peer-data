{"id": "1411.6308", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "A Convex Formulation for Spectral Shrunk Clustering", "abstract": "Spectral clustering is a fundamental technique in the field of data mining and information processing. Most existing spectral clustering algorithms integrate dimensionality reduction into the clustering process assisted by manifold learning in the original space. However, the manifold in reduced-dimensional subspace is likely to exhibit altered properties in contrast with the original space. Thus, applying manifold information obtained from the original space to the clustering process in a low-dimensional subspace is prone to inferior performance. Aiming to address this issue, we propose a novel convex algorithm that mines the manifold structure in the low-dimensional subspace. In addition, our unified learning process makes the manifold learning particularly tailored for the clustering. Compared with other related methods, the proposed algorithm results in more structured clustering result. To validate the efficacy of the proposed algorithm, we perform extensive experiments on several benchmark datasets in comparison with some state-of-the-art clustering approaches. The experimental results demonstrate that the proposed algorithm has quite promising clustering performance.", "histories": [["v1", "Sun, 23 Nov 2014 22:12:52 GMT  (612kb)", "http://arxiv.org/abs/1411.6308v1", "AAAI2015"]], "COMMENTS": "AAAI2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "feiping nie", "zhigang ma", "yi yang", "xiaofang zhou"], "accepted": true, "id": "1411.6308"}, "pdf": {"name": "1411.6308.pdf", "metadata": {"source": "CRF", "title": "A Convex Formulation for Spectral Shrunk Clustering", "authors": ["Xiaojun Chang", "Feiping Nie", "Zhigang Ma", "Yi Yang", "Xiaofang Zhou"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 141 1.63 08v1 [cs.LG] 2 3N ov2 01"}, {"heading": "Introduction", "text": "It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) In fact it is. (...) Most people are able to understand themselves. (...) It is not that they feel able to understand themselves. (...) It is not. (...) It is not that they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is. (...). (...) It is. (...). (...) It is. (...) It is. (...) It is. (...) It is. (...). (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. (... It is. (...) It is. It is. It is. (... It is. (...) It is. (...) It is. It is. (... It is. It is. It is. (). It is. It is. (... It is. (...). It is. It is. (... It is. It is. It is. It is. It is. (). It is. It is. It is. (...). It is. It is. It is. It is. It is. (...). It is. It is. It is. (). It is. It is. It is. It is. It is. (). It is. It is. It is. It is. It is. It is. (). It is. (). It is. It is."}, {"heading": "Related Work", "text": "Our work is inspired by spectral clustering, so in this section we review the related work on spectral clustering."}, {"heading": "Basics of Spectral Clustering", "text": "To simplify the presentation, we first summarize the notations frequently used in this essay. In view of a dataset X = {x1,.., xn}, xi-Rd (1 \u2264 i \u2264 n) is the i-th date and n is the total number of data points. The goal of clustering is to partition into c clusters. Name the cluster indicator vector for the date xi. The j-th element of yi is 1 when xi is clustered to the j-th cluster, and 0 otherwise.Existing spectral cluster algorithms adopt a weighted graph to partition the data. Let us use G = {X, A} as a weighted graph with a vertex set and an affinity."}, {"heading": "Progress on Spectral Clustering", "text": "Chen et al. propose a \"Landmark-based Spectral Clustering\" (LSC) for large cluster problems (Chen and Cai 2011). Specifically, some representative data points are first selected as landmarks and the original data points are then presented as linear combinations of these landmarks. \"Spectral Clustering\" is performed on the landmark-based representation. Yang et al. propose to use a nonnegative constraint to loosen the elements of the cluster indicator matrix for spectral clustering (Yang et al. 2011). Liu et al. propose to compress the original graph for spectral clustering into a sparse two-part graph. Clustering is then performed on the basis of the two-part graph, which improves the efficiency of clustering (Liu et al. 2013)."}, {"heading": "The Proposed Algorithm", "text": "In this section we will present the details of the proposed algorithm. A fast iterative method is also proposed to solve the objective function."}, {"heading": "Problem Formulation", "text": "Our algorithms are not far from spectral clustering. (2) The shrinkage patterns located near us are able to satisfy the shrinkage patterns described by us. (1) The shrinkage patterns of n data patterns are able to process the shrinkage patterns described by us. (2) The shrinkage patterns of n data patterns are close to F of the original samples by minimizing the traditional spectral cluster algorithms of Tr (FTLnF), where Ln is a standardized Laplacian matrix. (1) The shrinkage patterns of the following requirements. (2) The shrinkage patterns should maintain consistency with the spectral patterns. (3) To be more specific, the shrinkage patterns should not be far away from the spectral clusters."}, {"heading": "Optimization", "text": "The proposed function refers to the l2,1 standard, which is difficult to solve in closed form. We propose to solve this problem in the following steps: Denote H = G \u2212 F and H = [h1, \u00b7 \u00b7 \u00b7, hd], where d is the dimension of the spectral embedding, the objective function can be rewritten as follows: min GTr (((G \u2212 F) TS (G \u2212 F)) + g \u2212 ijwij (9), where d is the dimension of the spectral embedding."}, {"heading": "Convergence Analysis", "text": "To prove the convergence of algorithm 1, we need the following problem (Nie et al q.).Lemma 1. For all non-zero vectors g, gt-Rc, the following inequality applies: Eq. (8). \u2212 SDAs 1. Algorithm 1. monotonously decreases the objective functional value of the problem in Eq. (8) The following theorem guarantees that the problem is in Eq. (8). \u2212 SDAs. \u2212 SDAs. (G) 1. Theorem 1. Algorithm 1. monotonically decreases the objective functional value of the problem in Eq. (8) In each iteration, making it converge to the global optimum. \u2212 Define f. (G \u2212 F) TS (G \u2212 F). According to algorithm 1. we know that we have the objective functioning of the problem in Eq. (1)."}, {"heading": "Datasets", "text": "The AR dataset (Lyons et al. 1997) consists of 213 images of different facial expressions from 10 different Japanese female models; the images are resized to 26 x 26 and represented by pixel values; the ORL dataset (Samaria and Harter 1994) consists of 40 different subjects, each with 10 images; we also resize each image to 32 x 32 and use pixel values to represent the images; the UMIST dataset (Graham and M 1998) consists of 564 images of 20 individuals of mixed race, gender and appearance; each individual is shown in a series of poses, from profiles to front views; the pixel value is used as a feature-resistant representation; the BinAlpha dataset contains 26 binary handwritten alphabets and we select images randomly from profile to front views."}, {"heading": "Setup", "text": "For parameters in all comparison algorithms, we adjust them in the range of {10 \u2212 6, 10 \u2212 3, 100, 103, 106} and report the best results. Note that the results of all cluster algorithms differ from the respective initialization. To reduce the influence of statistical variations, we repeat each cluster formation 50 times with random initialization and report the results that correspond to the best objective function values. For all dimensional reduction-based K averages and spectral clusters, we project the original data into a low-dimensional subspace of 10 to 150 and report the best results."}, {"heading": "Evaluation Metrics", "text": "After most clustering work, we use cluster accuracy (ACC) and normalized mutual information (NMI) as metrics in our experiments. Let Qi represent the cluster identifier resulting from a cluster algorithm, and Pi represent the corresponding truth identifier of any data point x. Then, ACC is defined as follows: ACC = \u2211 n = 1 \u03b4 (pi, map (qi) n, (19) where \u03b4 (x, y) = 1 if x = y and \u03b4 (x, y) = 0 otherwise. Map (qi) is the best mapping function that permutates cluster identifiers to match the KuhnMunkres algorithm. A larger ACC indicates better cluster performance. For two arbitrary variables P and Q, NMI is defined as follows (Strehl and Ghosh 2003): NMI = I (P, Q) (Q) (Q) (Q) (opposite class), Q (H) (Q) (where Ntl) and P (P) numbers."}, {"heading": "Experimental Results", "text": "The experimental results in Table 1 and Table 2. We can see from the two tables that our method is consistently the best algorithm using both evaluation metrics. We also observe the following: 1. The spectral cluster algorithm and its variants perform better than the classical k means and its variants. This observation suggests that it is advantageous to use the paired similarities between all data points from a weighted graph adjacency matrix containing helpful information for clustering. 2. PCA Kmeans and LDA Kmeans are better than K means, whereas PCA SC and LDA SC are better than SC. This shows that reducing dimensionality is helpful for improving cluster performance. 3. LDA Kmeans outperforms PCA Kmeans, while LDA SC outperforms PCA SC. This indicates that LDA is better able to maintain structural information in reducing dimensionality than PCA."}, {"heading": "Parameter Sensitivity", "text": "In this section, we will examine the sensitivity of our algorithm to the parameter \u03b3 in Equation (3). Figure 1 shows the accuracy (y-axis) of SSC for different \u03b3 values (x-axis) on all experimental datasets. Figure 1 shows that performance varies when different values of \u03b3 are used. However, with the exception of MSRA50 and USPS datasets, our method achieves the best / respectable performance when \u03b3 = 1. This indicates that our method has a consistent preference for setting parameters, making it easy to obtain optimal parameter values in practice."}, {"heading": "Convergence Study", "text": "As already mentioned, the proposed iterative approach in algorithm 1 monotonously reduces the objective functional value in Equation (3). In this experiment, we show the convergence curves of the iterative approach on different datasets in Figure 2. The parameter \u03b3 is set to 1, which is the mean of the coordinated range of parameters. It can be observed that the objective functional value converges quickly. The convergence experiment demonstrates the efficiency of our algorithm."}, {"heading": "Conclusion", "text": "The advantage of our method is threefold. Firstly, it is able to learn the manifold structure in low-dimensional subspace and not in the original space. This property contributes to more precise structural information for clustering based on low-dimensional space. Secondly, our method is able to reveal the manifold structure. In particular, the shrunken pattern it has learned through the proposed algorithm is not the orthogonal constraint that makes it more flexible to adapt the manifold structure. The knowledge acquired is particularly helpful in order to achieve a better clustering result. Thirdly, our algorithm is convex, which makes it easy to implement applications in the real world and very suitable. Extensive experiments on a variety of applications are given to show the effectiveness of the proposed algorithms. By comparing it with several state-of-the-art clustering approaches, we confirm the advantage of our references d.Chen and Li [Cai and Larala]."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Spectral clustering is a fundamental technique in the<lb>field of data mining and information processing. Most<lb>existing spectral clustering algorithms integrate dimen-<lb>sionality reduction into the clustering process assisted<lb>by manifold learning in the original space. However,<lb>the manifold in reduced-dimensional subspace is likely<lb>to exhibit altered properties in contrast with the orig-<lb>inal space. Thus, applying manifold information ob-<lb>tained from the original space to the clustering process<lb>in a low-dimensional subspace is prone to inferior per-<lb>formance. Aiming to address this issue, we propose a<lb>novel convex algorithm that mines the manifold struc-<lb>ture in the low-dimensional subspace. In addition, our<lb>unified learning process makes the manifold learning<lb>particularly tailored for the clustering. Compared with<lb>other related methods, the proposed algorithm results in<lb>more structured clustering result. To validate the effi-<lb>cacy of the proposed algorithm, we perform extensive<lb>experiments on several benchmark datasets in compar-<lb>ison with some state-of-the-art clustering approaches.<lb>The experimental results demonstrate that the proposed<lb>algorithm has quite promising clustering performance. Introduction<lb>Clustering has been widely used in many<lb>real-world applications (Jain and Dubes 1988;<lb>Wang, Nie, and Huang 2014). The objective of cluster-<lb>ing is to cluster the original data points into various<lb>clusters, so that data points within the same cluster<lb>are dense while those in different clusters are far away<lb>from each other (Filippone et al. 2008). Researchers have<lb>proposed a variety of clustering algorithms, such as K-<lb>means clustering and mixture models (Wang et al. 2014;<lb>Nie, Wang, and Huang 2014; Nie et al. 2011b), etc.<lb>The existing clustering algorithms, however, mostly work<lb>well when the samples\u2019 dimensionality is low. When par-<lb>titioning high-dimensional data, the performance of these<lb>algorithms is not guaranteed. For example, K-means clus-<lb>tering iteratively assigns each data point to the cluster This paper was partially supported by the ARC DECRA project<lb>DE130101311, UQ ECR (2013002401) and Tianjin Key Labo-<lb>ratory of Cognitive Computing and Application. Copyright c<lb>\u00a9<lb>2015, Association for the Advancement of Artificial Intelligence<lb>(www.aaai.org). All rights reserved.<lb>with the closest center based on specific distance/similarity<lb>measurement and updates the center of each cluster. But<lb>the distance/similarity measurements may be inaccurate<lb>on high-dimensional data, which tends to limit the clus-<lb>tering performance. As suggested by some researchers,<lb>many high-dimensional data may exhibit dense grouping<lb>in a low-dimensional subspace (Nie et al. 2009). Hence, re-<lb>searchers have proposed to first project the original data<lb>into a low-dimensional subspace via some dimensionality<lb>reduction techniques and then cluster the computed low-<lb>dimensional embedding for high-dimensional data cluster-<lb>ing. For instance, a popular approach is to use Princi-<lb>ple component analysis (PCA) to reduce the dimensional-<lb>ity of the original data followed by Kmeans for cluster-<lb>ing (PcaKm) (Xu and Wunsch 2005). Ding et al. present a<lb>clustering algorithm based on Linear discriminant analysis<lb>(LDA) method (Ding and Li 2007). Ye et al propose dis-<lb>criminative K-means (DisKmeans) clustering which uni-<lb>fies the iterative procedure of dimensionality reduction and<lb>K-means clustering into a trace maximization problem<lb>(Ye, Zhao, and Wu 2007). Another genre of clustering, i.e., spectral clustering<lb>(Shi and Malik 2000) integrates dimensionality reduction<lb>into its clustering process. The basic idea of spectral clus-<lb>tering is to find a clustering assignment of the data points<lb>by adopting the spectrum of similarity matrix that leverages<lb>the nonlinear manifold structure of original data. Spectral<lb>clustering has been shown to be easy to implement and of-<lb>tentimes it outperforms traditional clustering methods be-<lb>cause of its capacity of mining intrinsic geometric struc-<lb>tures, which facilitates partitioning data with more com-<lb>plicated structures. The benefit of utilizing manifold infor-<lb>mation has been demonstrated in many applications, such<lb>as image segmentation and web mining. Due to the ad-<lb>vantage of spectral clustering, different variants of spec-<lb>tral clustering algorithms have been proposed these years<lb>(Li et al. 2015). For example, local learning-based cluster-<lb>ing (LLC) (Wu and Schlkopf 2006) utilizes a kernel regres-<lb>sion model for label prediction based on the assumption<lb>that the class label of a data point can be determined by its<lb>neighbors. Self-tuning SC (Zelnik-Manor and Perona 2004)<lb>is able to tune parameters automatically in an unsupervised<lb>scenario. Normalized cuts is capable of balancing the vol-<lb>ume of clusters for the usage of data density information (Shi and Malik 2000).<lb>Spectral clustering is essentially a two-stage approach,<lb>i.e., manifold learning based in the original high-<lb>dimensional space and dimensionality reduction. To achieve<lb>proper clustering, spectral clustering assumes that two<lb>nearby data points in the high density region of the reduced-<lb>dimensional space have the same cluster label. However, this<lb>assumption does not always hold. More possibly, these near-<lb>est neighbors may be far away from each other in the original<lb>high-dimensional space due to the curse of dimensionality.<lb>That being said, the distance measurement of the original<lb>data could not precisely reflect the low-dimensional mani-<lb>fold structure, thus leading to suboptimal clustering perfor-<lb>mance.<lb>Intuitively, if the manifold structure in the low-<lb>dimensional space is precisely captured, the clustering<lb>performance could be enhanced when applied to high-<lb>dimensional data clustering. Aiming to achieve this goal, we<lb>propose a novel clustering algorithm that is able to mine the<lb>inherent manifold structure of the low-dimensional space for<lb>clustering. Moreover, compared to traditional spectral clus-<lb>tering algorithms, the shrunk pattern learned by the proposed<lb>algorithm does not have an orthogonal constraint, giving it<lb>more flexibility to fit the manifold structure. It is worthwhile<lb>to highlight the following merits of our work: \u2022 The proposed algorithm is more capable of uncovering the<lb>manifold structure. Particularly, the shrunk pattern does<lb>not have the orthogonal constraint, making it more flexi-<lb>ble to fit the manifold structure.<lb>\u2022 The integration of manifold learning and clustering makes<lb>the former particularly tailored for the latter. This is in-<lb>trinsically different from most state-of-the-art clustering<lb>algorithms.<lb>\u2022 The proposed algorithm is convex and converges to global<lb>optimum, which indicates that the proposed algorithm<lb>does not rely on the initialization. The rest of this paper is organized as follows. After re-<lb>viewing related work on spectral clustering in section 2, we<lb>detail the proposed algorithm in section 3. Extensive experi-<lb>mental results are given in section 4 and section 5 concludes<lb>this paper.", "creator": "LaTeX with hyperref package"}}}