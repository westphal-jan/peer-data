{"id": "1411.7974", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2014", "title": "Solving Games with Functional Regret Estimation", "abstract": "We propose a novel online learning method for minimizing regret in large extensive-form games. The approach learns a function approximator online to estimate the regret for choosing a particular action. A no-regret algorithm uses these estimates in place of the true regrets to define a sequence of policies.", "histories": [["v1", "Fri, 28 Nov 2014 18:45:50 GMT  (292kb)", "https://arxiv.org/abs/1411.7974v1", "AAAI Conference on Artificial Intelligence 2015"], ["v2", "Wed, 31 Dec 2014 23:45:22 GMT  (113kb)", "http://arxiv.org/abs/1411.7974v2", "AAAI Conference on Artificial Intelligence 2015"]], "COMMENTS": "AAAI Conference on Artificial Intelligence 2015", "reviews": [], "SUBJECTS": "cs.AI cs.GT cs.MA", "authors": ["kevin waugh", "dustin morrill", "james andrew bagnell", "michael h bowling"], "accepted": true, "id": "1411.7974"}, "pdf": {"name": "1411.7974.pdf", "metadata": {"source": "CRF", "title": "Solving Games with Functional Regret Estimation", "authors": ["Kevin Waugh", "Dustin Morrill", "J. Andrew Bagnell", "Michael Bowling"], "emails": ["waugh@cs.cmu.edu", "morrill@ualberta.ca", "dbagnell@ri.cmu.edu", "mbowling@ualberta.ca"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.79 74v2 [cs.AI] 31 Dec 201 4"}, {"heading": "Introduction", "text": "In fact, the fact is that most of them will be able to be in a position without being able to be in a position to be able to move."}, {"heading": "Regression Regret-Matching", "text": "Let's start by introducing a new, even simpler, algorithm for the standard online learning framework. (Let's introduce a set of N actions or experts). (Let's start by introducing a new, even simpler, algorithm for the standard online learning framework. (Let's introduce a set of N actions or experts). (Let's introduce a set of N actions or experts.) (Let's introduce a set of regrets (Let's), (Let's), (Let's introduce), (Let's), (Let), (Let), (Let), (Let), (Let), (Let), (Let), (Let), (Let), (Let), \"(Let),\" (Let), \"(Let),\" (Let), \"(Let,\" (Let), \"(Let),\" (Let, \"(Let),\" (Let, \"(Let),\" (Let, \"(Let),\" (Let, \"(Let),\" (Let), \"((Let),\" (((Let), \"R),\" (Let, \"), (((((),\"), \"), ((((((),),), (((),), ((((),),), ((((),), (((),), ((((),), (), ((), (), (((), (), (), ((((),), (((), (),), ((((),)), ((), (((),), ((((),)), ((((((),),)), ((((((),))), ((((()), (((()), ((())), (((((((),)))), ((((((()),)), ((((((),))), (((((()),)), ((((((())),))))), ((((((((("}, {"heading": "Extensive-form Games", "text": "A two-player zero-sum extensive form game is a tuple game = (H, p, \u03c3c, I, u) (Osborne and Rubinstein 1994). The set of history, H, forms a tree rooted in \u03c6, H, the empty story. A (h) is the set of actions available to h, H and ha, H. The subset Z (H) is the set of end history, i.e. if z \u00b2 Z then A (h) = c) is empty. p: H\\ Z \u2192 {1, c} is the player's selection function that determines whether nature or a player should act in a given story. For all stories h \u00b2 H in which chance acts (i.e., all h where p (h) = c), (\u00b7 h), A (h) is a probability distribution of the available random actions that define the strategy."}, {"heading": "Counterfactual Regret Minimization", "text": "Counterfactual Regret Minimization (CFR) is an algorithm for calculating an \u03b5 balance in large-scale regret games (Zinkevich et al. 2008). It employs several no-regret online learners, usually cases of regret matching, which minimize counterfactual regret in each set of information. Counterfactual benefit for taking action from information I take in due course is defined as asuti (a | I) to take swift action and then follow their current policies. Counterfactual benefit is defined as immediate counterfactual regret and cumulative counterfactual regret in taking action I take."}, {"heading": "Regression CFR", "text": "We are now ready to put the pieces together to form our new regret algorithm for sequential decision scenarios: Regression CFR (RCFR). The common regressor uses features that are a function of the information set-action pair, allowing the regressor to generalize similar actions and similar situations when building its regret estimates. As with the CFR, we can derive a regret limit from this, which in turn implies a worst-case limit resulting from self-play. Theorem 5. If the regressor generalizes similar actions and similar situations when building its regret estimates for each t-set."}, {"heading": "Relationship to Abstraction", "text": "As stated in the introduction, the problem we want to solve when we look at it without any structure is insoluble."}, {"heading": "Experimental Results", "text": "To demonstrate the practicality of the RCFR, we will test its performance in Leduc Hold'em, a simplified poker game. Our goal is to compare the strategies found by the RCFR with different regressors with strategies generated using conventional abstraction techniques. We will also examine the iterative behavior of the RCFR in comparison to the CFR."}, {"heading": "Leduc Hold\u2019em", "text": "Leduc Hold'em is a poker game based on Kuhn Poker (Southey et al. 2005). It provides a good testing environment, as common operations, such as best reaction and balance calculations, tractable and exact.The game has two betting rounds, the preflop and flop. At the start of the game, both players place a single chip in the pot and are dealt a single private card from a mixed deck of six cards - two Jacks, two Queens and two Kings. Then begins the preflop betting round, in which the first player can either check or bet. If the first player checks and passes the pot, the second player can also finish the round by checking or continue by betting. If the player stands in front of a bet, he can raise by placing two chips in the pot; call by matching the bet in the pot and finishing the round; or fold by giving the pot to the opponent a maximum of one bet per round, i.e. there is one increase."}, {"heading": "Features and Implementation", "text": "We use a regression tree that aims to minimize the average error square as a regressor. If the error improvement at a node is less than a threshold or no improvement can be achieved by a split, a sheet is inserted that predicts the average. It is this error threshold that we manipulate to control the complexity of the regressor - the size of the tree. All training data is held between iterations, as in Algorithm 1. Eight characteristics were selected in such a way that the set of characteristics would be small, enabling us to perform fast regression tree training that is performed on each RCFR iteration, but is still descriptive enough to have a unique feature extension for each sequence: 1) the expected hand strength (E [HS]) or the probability of winning the hand would be present, and the information about the available, and the pot on a uniform map; 3 and not the possible distribution of the error aimed at it; a regression tree, a) and a regression tree (3) and a regression tree (3)."}, {"heading": "Experiments", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "Future Work", "text": "In this paper, we have introduced RCFR, a technique that avoids the need for abstraction as a pre-processing step by using an online regression algorithm. The regressor essentially automatically learns and adjusts abstraction, as the algorithm greatly simplifies abstraction design. The experimental results show that this technique is quite promising. The next step is scaling to larger games, such as unlimited Texas hold'em, where abstraction is required. In this paper, we probably did not examine scanning with RCFR at all, but a number of interesting questions need to be answered along the way. First, CFR is the algorithm of choice for balance finding due to the powerful scanning schemes it has at its disposal. In this paper, we did not examine scanning with RCFR at all, but there are no obvious limitations prohibiting scanning, but we assume that lower scanning schemas is currently the preferred policy and second, the policy of ressing is very important."}, {"heading": "Acknowledgments", "text": "This work is supported by the ONR MURI grant N0001409-1-1052, the National Sciences and Engineering Research Council of Canada (NSERC) and Alberta Innovative Technology Futures (AITF). Thanks to Compute Canada for computing resources and the Computer Poker Research Group (CPRG) for supporting the software infrastructure."}], "references": [{"title": "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches", "author": ["Awerbuch", "B. Kleinberg 2004] Awerbuch", "R. Kleinberg"], "venue": "In ACM Symposium on Theory of Computing (STOC)", "citeRegEx": "Awerbuch et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch et al\\.", "year": 2004}, {"title": "Online implicit agent modelling", "author": ["Bard"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS)", "citeRegEx": "Bard,? \\Q2013\\E", "shortCiteRegEx": "Bard", "year": 2013}, {"title": "Gradient-based algorithms for finding Nash equilibria in extensive form games", "author": ["Gilpin"], "venue": "In International Workshop on Internet and Network Economics (WINE)", "citeRegEx": "Gilpin,? \\Q2007\\E", "shortCiteRegEx": "Gilpin", "year": 2007}, {"title": "Potential-aware automated abstraction of sequential games, and holistic equilib", "author": ["Sandholm Gilpin", "A. Sorensen 2007] Gilpin", "T. Sandholm", "T. Sorensen"], "venue": null, "citeRegEx": "Gilpin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gilpin et al\\.", "year": 2007}, {"title": "A simple adaptive procedure leading to correlated equilibrium", "author": ["Hart", "S. Mas-Colell 2000] Hart", "A. Mas-Colell"], "venue": null, "citeRegEx": "Hart et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hart et al\\.", "year": 2000}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Hazan"], "venue": "In Conference on Learning Theory (COLT)", "citeRegEx": "Hazan,? \\Q2006\\E", "shortCiteRegEx": "Hazan", "year": 2006}, {"title": "Accelerating best response calculation in large extensive games", "author": ["Johanson"], "venue": "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Johanson,? \\Q2011\\E", "shortCiteRegEx": "Johanson", "year": 2011}, {"title": "Monte carlo sampling for regret minimization in extensive games", "author": ["Lanctot"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lanctot,? \\Q2009\\E", "shortCiteRegEx": "Lanctot", "year": 2009}, {"title": "A Course On Game Theory", "author": ["Osborne", "M. Rubinstein 1994] Osborne", "A. Rubinstein"], "venue": null, "citeRegEx": "Osborne et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 1994}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Gordon Ross", "S. Bagnell 2011] Ross", "G.J. Gordon", "J.A. Bagnell"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Abstraction methods for game theoretic poker", "author": ["Shi", "J. Littman 2002] Shi", "M. Littman"], "venue": "In International Conference on Computers and Games (CG),", "citeRegEx": "Shi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2002}, {"title": "bluff: Opponent modelling in poker", "author": ["Southey"], "venue": "In Conference on Uncertainty in AI (UAI)", "citeRegEx": "Southey,? \\Q2005\\E", "shortCiteRegEx": "Southey", "year": 2005}, {"title": "Effective short-term opponent exploitation in simplified poker. Machine Learning 74(2):159\u2013189", "author": ["Hoehn Southey", "F. Holte 2009] Southey", "B. Hoehn", "R. Holte"], "venue": null, "citeRegEx": "Southey et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Southey et al\\.", "year": 2009}, {"title": "Abstraction pathologies in extensive games", "author": ["Waugh"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS)", "citeRegEx": "Waugh,? \\Q2008\\E", "shortCiteRegEx": "Waugh", "year": 2008}, {"title": "Regret minimization in games with incomplete information", "author": ["Zinkevich"], "venue": "Technical Report TR07-14,", "citeRegEx": "Zinkevich,? \\Q2007\\E", "shortCiteRegEx": "Zinkevich", "year": 2007}, {"title": "Regret minimization in games with incomplete information", "author": ["Zinkevich"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich,? \\Q2008\\E", "shortCiteRegEx": "Zinkevich", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "Theorem 4 (from Zinkevich et al. (2008)).", "startOffset": 16, "endOffset": 40}, {"referenceID": 3, "context": "Though other algorithms boast better asymptotic rates (Gilpin et al. 2007), CFR more rapidly converges to an acceptable solution in practice.", "startOffset": 54, "endOffset": 74}, {"referenceID": 14, "context": "The proof combines Theorem 2 together with the CFR convergence proof of Zinkevich et al. (2008). Now that we have presented RCFR, let us examine how it relates to modern abstraction techniques.", "startOffset": 72, "endOffset": 96}], "year": 2014, "abstractText": "We propose a novel online learning method for minimizing regret in large extensive-form games. The approach learns a function approximator online to estimate the regret for choosing a particular action. A noregret algorithm uses these estimates in place of the true regrets to define a sequence of policies. We prove the approach sound by providing a bound relating the quality of the function approximation and regret of the algorithm. A corollary being that the method is guaranteed to converge to a Nash equilibrium in selfplay so long as the regrets are ultimately realizable by the function approximator. Our technique can be understood as a principled generalization of existing work on abstraction in large games; in our work, both the abstraction as well as the equilibrium are learned during self-play. We demonstrate empirically the method achieves higher quality strategies than state-of-the-art abstraction techniques given the same resources.", "creator": "gnuplot 5.0 patchlevel rc2"}}}