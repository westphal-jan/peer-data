{"id": "1412.1443", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2014", "title": "Structure learning of antiferromagnetic Ising models", "abstract": "In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. We first observe that the notoriously difficult problem of learning parities with noise can be captured as a special case of learning graphical models. This leads to an unconditional computational lower bound of $\\Omega (p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree $d$, for the class of so-called statistical algorithms recently introduced by Feldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime required to exhaustively search over neighborhoods cannot be significantly improved without restricting the class of models.", "histories": [["v1", "Wed, 3 Dec 2014 19:08:55 GMT  (25kb)", "http://arxiv.org/abs/1412.1443v1", "15 pages. NIPS 2014"]], "COMMENTS": "15 pages. NIPS 2014", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["guy bresler", "david gamarnik", "devavrat shah"], "accepted": true, "id": "1412.1443"}, "pdf": {"name": "1412.1443.pdf", "metadata": {"source": "CRF", "title": "Structure learning of antiferromagnetic Ising models", "authors": ["Guy Bresler", "David Gamarnik", "Devavrat Shah"], "emails": ["gbresler@mit.edu", "gamarnik@mit.edu", "devavrat@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.14 43v1 [st at.M L] 3lower bound suggests that the exhaustive search time required by Bresler, Mossel and Sly's [2] cannot be significantly improved without limiting the class of models.Apart from structural assumptions about the graph such as tree, hypertree, treelike, etc., many recent structural learning studies assume that the model exhibits the correlation decay characteristic.In fact, Bento and Montanari [3], taking into account ferromagnetic issuing models, showed that all known low-complexity algorithms do not learn simple graphs when the interaction strength exceeds a number associated with the correlation decay threshold. Our second set of results gives a class of repellent (anti-ferromagnetic) models that exhibit the opposite behavior: very strong interaction enables efficient learning in time O (p2)."}, {"heading": "1 Introduction", "text": "For unstructured high-dimensional distributions, such as in social networks, biology, and finance, an important first step is to determine which graphic model to use. In this paper, we focus on the problem of structural learning: Accessing n independent and identically distributed samples \u03c3 (1),... \u03c3 (n) from an undirected graphic model, which represents a discrete random vector \u03c3 = (\u03c31,..., \u03c3p), the goal is to find the diagram G on which the model is based. Two basic questions are: 1) How many samples are needed? and 2) How high is the computational complexity? In this paper, we are mainly interested in the computational complexity of structural learning. First, we look at the problem of learning a general discrete, uncontrolled, limited-degree graphical model."}, {"heading": "1.1 Learning general graphical models", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "1.2 Correlation decay property", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "1.3 Repelling models", "text": "The antiferromagnetic Ising model has a negative interaction parameter, with adjacent nodes preferring to be in opposite states. Other popular antiferromagnetic models include the pot or dye model and the hard-core model. Antiferromagnetic models have the interesting property that correlations between neighbors can be zero due to cancellations. Therefore, algorithms based on cutting back neighborhoods with paired correlations, like the algorithm in [2] for models with correlation decay, are not suitable for antiferromagnetic models. To our knowledge, there are no previous results that improve the pd computational complexity for structural learning of antiferromagnetic models on general graphs with maximum degree. Our first learning algorithm, described in section 2, is for the hard-core model."}, {"heading": "2 Learning the graph of a hard-core model", "text": "The analysis in this section is simple, but serves as an example to emphasize the fact that the CDP is not a necessary condition for learning the structure. Given a graph G = (V, E) on | V | = p nodes, which from I (G) contains fewer arithmetic examples (G) less arithmetic examples (G) less arithmetic examples (G) less probability (G) p of the series of independent set indicators (G), for which at least one of the two indicators of the series of arithmetic examples of the series of independent vectors in I (G), withP (n) = \u03bb of the series of arithmetic examples I (G) zero for each edge {i, j} E (G). The number of arithmetic examples of the arithmetic examples is equal and Z = Z (G) is equal."}, {"heading": "3 Learning anti-ferromagnetic Ising models", "text": "In this section, we will look at the antiferromagnetic Ising model using a diagram G = (V, E).We parameterise the model in such a way that each configuration has probabilityP (\u03c3) = 1Z exp {H (\u03c3)}, where \u03b2 > 0, 1} p, (3.1) where H (\u03c3) = \u2212 \u03b2 (i, j), E\u03c3i\u03c3j + \u2211 i-Vhi\u03c3i. (3.2) Here are \u03b2 > 0 and {hi} i-V are real value parameters, and we assume that | hi | \u2264 h applies to all i. If we work with configurations in {0, 1} p and not with the more typical {\u2212 1, + 1} p, this boils down to a repair repair repair (which is without loss of universality, as shown for example in Appendix 1 of [25].If we set hi = h = ln \u00b2 for all i, we will return the hardcore model with uncertainty within the limit \u2192 \"that we think back.\""}, {"heading": "3.1 Strongly antiferromagnetic models", "text": "We begin by considering the situation in which the repulsion force \u03b2 is so great that we can modify the approach used for the hard core model. Let us define the empirical conditional probabilityP: (\u03c3a = 1 | \u03c3b = 1): = P: (\u03c3a = 1, \u03c3b = 1) P: (\u03c3b = 1) P: (\u03c3b = 1) P: (\u03c3b = 1), using for each quantity S: V and xS: 0, 1} | S:, P: (\u03c3V = xV) = 1nn: k = 11 {\u03c3 (k) V = xV}.The following problem shows that we have good estimates for P: (\u03c3a = 1 | \u03c3b = 1).Lemma 3.1. Let us suppose that P: (\u03c3b = 1) \u2265 q for all b: V. If the number of samples is n: (2 / q2\u04452), we log (8p2 / \u0445), then we have a threshold with at least a probability: 1 | P: a threshold."}, {"heading": "4: Output E\u0302", "text": "The proof is similar to that of Theorem 2.1, which replaces Lemma 2.3 of Lemma 3.3. Theorem 3.7, which is presented in the next section, assumes Proposition 3.2, so that we only consider the stronger theorem 3.7.Proposition 3.2, then the antiferromagnetic issuing model (3.2) is taken into account on a diagram G = (V, E) on p-nodes and with maximum degree of d. If\u03b2 (d + ln 2), then the algorithm komplexityn = O (22de2h) on p), i.e. these many samples are sufficient to reconstruct the diagram with probability of 1 \u2212 o (1). The computational complexity of StrongRepelling isO (np2) = O (22de2h).D"}, {"heading": "3.2 Weakly antiferromagnetic models", "text": "In this section we focus on weakly repulsive models and show a trade-off between computational complexity and the strength of repulsion. Remember that for strongly repulsive models (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive models) (with repulsive) (with repulsive models) (with repulsive models) ((with repulsive models) (with repulsive models) (with rep"}, {"heading": "4: Compute P\u0302G\\U (\u03c3a = 1|\u03c3b = 1)", "text": "5: We initially argue that all empirical probabilities P-G-U-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u-u"}, {"heading": "4 Statistical algorithms and proof of Theorem 1.1", "text": "We start with the description of the statistical algorithm framework introduced by [1]. In this section, it is convenient to work with variables that take values in {\u2212 1, + 1} and not in {0, 1}."}, {"heading": "4.1 Background on statistical algorithms", "text": "Let X = {\u2212 1} p denote the space of configurations and let D have a series of distributions over X. Let F be a set of solutions (in our case, graphs) and Z: D \u2192 2F is a map that will map any distribution D = D to a subset of solutions Z (D) F that are defined to find valid solutions for D (D), since any graphical model will be identifiable under our consideration, there is a single graph Z (D) that corresponds to any distribution. For n > 0, the distributional search problem Z via D and F that uses n samples to find a valid solution f (D), there is access to n random samples from an unknown D D. The class of algorithms that we refer to as unbiased statistical algorithms, defined by accessing an unbiased oracle. Other related classes of algorithms are defined in [1], and lower terms can be derived for them (4.e)."}, {"heading": "4.2 Soft parities", "text": "(4.2) Here c is a constant, and the partition function is Z = \u2211 xexp (c \u00b7 \u03c7S (x)) = 2 p \u2212 1 (ec + e \u2212 c). (4.3) Our distribution family D is derived from these soft parities over the subsets S [p] and | D | = (p d). Leave U the uniform distribution to {\u2212 1, + 1} p (ec + e \u2212 c). (4.3) For S 6 = T the correlation < pSU U \u2212 1 > is the same way."}, {"heading": "Acknowledgments", "text": "This work was partially supported by NSF grants CMMI-1335155 and CNS-1161964 as well as the MURI Award of the Army Research Office W911NF-11-1-0036."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "In this paper we investigate the computational complexity of learning the<lb>graph structure underlying a discrete undirected graphical model from i.i.d.<lb>samples. Our first result is an unconditional computational lower bound of<lb>\u03a9(p) for learning general graphical models on p nodes of maximum degree<lb>d, for the class of so-called statistical algorithms recently introduced by<lb>Feldman et al. [1]. The construction is equivalent to the notoriously difficult<lb>learning parities with noise problem in computational learning theory. Our<lb>lower bound suggests that the \u00d5(p) runtime required by Bresler, Mossel,<lb>and Sly\u2019s [2] exhaustive-search algorithm cannot be significantly improved<lb>without restricting the class of models.<lb>Aside from structural assumptions on the graph such as it being a tree,<lb>hypertree, tree-like, etc., many recent papers on structure learning assume<lb>that the model has the correlation decay property. Indeed, focusing on fer-<lb>romagnetic Ising models, Bento and Montanari [3] showed that all known<lb>low-complexity algorithms fail to learn simple graphs when the interaction<lb>strength exceeds a number related to the correlation decay threshold. Our<lb>second set of results gives a class of repelling (antiferromagnetic) models<lb>that have the opposite behavior: very strong interaction allows efficient<lb>learning in time \u00d5(p). We provide an algorithm whose performance in-<lb>terpolates between \u00d5(p) and \u00d5(p) depending on the strength of the<lb>repulsion.", "creator": "LaTeX with hyperref package"}}}