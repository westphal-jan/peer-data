{"id": "1412.2007", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2014", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "abstract": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English-&gt;German translation and almost as high performance as state-of-the-art English-&gt;French translation system.", "histories": [["v1", "Fri, 5 Dec 2014 14:26:27 GMT  (122kb,D)", "http://arxiv.org/abs/1412.2007v1", null], ["v2", "Wed, 18 Mar 2015 19:41:42 GMT  (124kb,D)", "http://arxiv.org/abs/1412.2007v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["s\u00e9bastien jean", "kyunghyun cho", "roland memisevic", "yoshua bengio"], "accepted": true, "id": "1412.2007"}, "pdf": {"name": "1412.2007.pdf", "metadata": {"source": "CRF", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "authors": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able, in which they are able to move, in which they are able, and in which they are able, and in which they are able, to change the world, in which they are able, to change the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to"}, {"heading": "2 Neural Machine Translation and", "text": "In this section, we briefly describe an approach to neural machine translation proposed recently in Bahdanau et al., 2014. Based on this description, we will explain the problem of limited vocabulary in neural machine translation."}, {"heading": "2.1 Neural Machine Translation", "text": "Neural machine translation is a recently proposed machine translation approach that uses a single neural network that is jointly trained to maximize translation performance (Forcada and N-eco, 1997; Kalchburner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014). Neural machine translation is often implemented as an encoder decoder network. (1) The encoder then reads the source sentence x = (x1,., xT) and encodes it into a sequence of hidden states h = (h1, \u00b7 \u00b7, hT): ht = f (xt, ht \u2212 1). (1) The encoder, another recursive neural network, generates a corresponding translation y = (y1, \u00b7, yT), maxict capability."}, {"heading": "2.1.1 Detailed Description", "text": "In this paper, we use a specific implementation of neural machine translation that uses an attention mechanism recently proposed in (Bahdanau et al., 2014). In (Bahdanau et al., 2014), the encoder in Eq. (1) is implemented by a bi-directional recursive neural network such as thatht = [\u2190 \u2212 h; \u2212 h t], where \u2190 \u2212 h = f (xt, \u2190 \u2212 h t + 1), \u2212 \u2192 h t = f (xt, \u2212 h t \u2212 1).They used a gated recurrent unit for f (see e.g. (Cho et al., 2014b).The decoder calculates the context vector at any time as the convex sum of the hidden states (h1,., hT) with the coefficients \u03b11,."}, {"heading": "2.2 Limited Vocabulary Issue and Conventional Solutions", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3 Approximate Learning Approach to Very Large Target Vocabulary", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Description", "text": "In this work, we propose a model-specific approach that allows us to learn a neural machine translation model with a very large target vocabulary. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "3.1.1 Informal Discussion on Consequence", "text": "The parameterization of the output probability in Equation (6) can be understood in such a way that the vectors associated with the target words are arranged in such a way that the point product is maximized between the most likely or correct target word vector and the current hidden state. Exposure followed by normalization is simply a process in which the point products are converted into correct probabilities. Therefore, as the learning progresses, the vectors of all probable target words tend to align with each other, but not with the other. This is achieved precisely by moving the vector of the correct word in the direction \u03c6 (yt \u2212 1, zt, ct), while pushing all other vectors away, which occurs when the gradient of the logarithm of the exact output probability in Equation (6) is maximized."}, {"heading": "3.2 Decoding", "text": "Once the model has been trained according to the proposed approach, we can use the full target vocabulary when we decipher a translation with a new source sentence. Although this is advantageous, as it allows the trained model to use the entire target vocabulary when creating a translation, it can be too expensive in arithmetical terms, for example for real-time applications. Since the training places the target word vectors in space so that they adapt well to the hidden state of the decoder only when it is likely to be a correct word, we can only use a subset of target words when decrypting, which is similar to what we do during the training, except that we do not have access to a set of correct targets during test time.The most naive way to select a subset of target words is to take only the most common K targets, which allows K to be adapted to the calculation requirement."}, {"heading": "3.3 Source Words for Unknown Words", "text": "In the experiments, we evaluate the proposed approach using the neural machine translation model called RNNsearch (Bahdanau et al., 2014) (see paragraph 2.1.1). In this model, as part of the decoding process, we obtain the matches between target words and source positions via the alignment model in equivalent. (5) We can use this feature to infer the source word to which each target word was most aligned (indicated by the largest \u03b1t in equivalent. (5)) This is particularly useful when the model has generated a [UNK] token. Once a translation has been generated by a source sentence, each [UNK] can be replaced by a translation-specific technique based on the aligned source word. For example, in the experiment, we try to replace each [UNK] token with the aligned source word or its most likely translation determined by a different alignment model based on the aligned source word."}, {"heading": "4 Experiments", "text": "We evaluated the proposed approach in English \u2192 French and English \u2192 German translation tasks. We trained the neural machine translation models only using the bilingual parallel corpora provided by WMT '14. For each pair of data sets used are: \u2022 English \u2192 French2: 2The pre-processed data can be processed from Europarl v7, Common Crawl, UN, News Commentary, Gigaword \u2022 English \u2192 German: Europarl v7, Common Crawl, News CommentaryTo ensure a fair comparison, the English \u2192 French corpus, which contains about 12 million sentences, is identical to the corpus used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) in a similar manner to the one used in (Peitz et al., 2014; Li et al., 2014; Sutskever et al., 2014)."}, {"heading": "4.1 Settings", "text": "As a starting point for English \u2192 French translation, we use the RNNsearch model, which was trained by (Bahdanau et al., 2014) with 30,000 source and target words. Another RNNsearch model is trained for English \u2192 German translation with 50,000 source and target words. For each language pair, we train another set of RNNsearch models with a much larger vocabulary of 500,000 source and target words using the proposed approach. We call these models RNsearch-LV. We vary the size of the shortlist used during the training (e.g. in Sec. 3.1). We tried 15,000 and 30,000 for English \u2192 French and 15,000 and 50,000 for English \u2192 German. We report the result later with the best performance on the development set. To stabilize parameters other than the word embeddings, we freeze the word embeddings and just adjust the other parameters for English."}, {"heading": "4.2 Translation Performance", "text": "In Table 1, we present the results achieved by the trained models with very large target vocabularies, and alongside them the previous results reported in (Sutskever et al., 2014), (Luong et al., 2014), (Buck et al., 2014) and (Durrani et al., 2014). It is clear that the RNNsearch-LV system, which uses a much larger vocabulary, surpasses the base model RNNsearch (Sec. 3,2-3,3). However, in the case of the English \u2192 French task, RNsearch-LV reached the performance level of the previous best single neural machine translation (NMT), even without translation-specific techniques (Sec. 3,2-3,3). However, in these, the RNsearch-LV outperformed it. The performance of the RNsearch-LV system is also better than that of a standard phrase-based translation system (Cho et al., 2014b)."}, {"heading": "4.3 Note on Ensembles", "text": "For each language pair, the seven models of the ensemble were collected at different times from only two training runs, some of which were tuned separately during the last training phase, when the word embedding was frozen, which probably makes the composition of our ensembles suboptimal. We indirectly confirm this suboptimality with the high cross-model BLEU values (Freitag et al., 2014). For the English \u2192 French development set, if we use the translations of the best individual models as a reference, we obtain, before the UNK replacement with the models from the training run of the reference model 65,69, 67.06 and 67,39 BLEU values. Meanwhile, with the models from the other training run, we obtain significantly lower values of 57.90, 57.99 and 58.11. A better translation performance can be achieved with more diverse models in the ensemble."}, {"heading": "4.4 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 Decoding Speed", "text": "Decoding RNNsearch-LV with the full target vocabulary is clearly the slowest. If we use a candidate list for decoding each translation, the speed of decoding greatly improves and approaches the base line RNNsearch. This slight decoding slowdown is less problematic for us, taking into account the significant improvement in translation performance achieved by RNNsearch-LV with candidate list over the base model. A potential problem when using a candidate list is that we have to rebuild a target vocabulary for each set of sources and then replace some of the parameters, which can easily become time consuming. We can solve this problem by, for example, creating a common candidate list for multiple sentences, but this can let the order of test sentences influence the translations generated."}, {"heading": "4.4.2 Decoding Target Vocabulary", "text": "For English \u2192 French, we evaluate the impact of the target vocabulary in the translation of test sentences by using a fixed set of 30,000 common words and (at most) K \u2032 likely candidates for each dictionary source word. Results are presented in Figure 1. With K \u2032 = 0 (not shown), the performance of the system is comparable to the basic performance if the unknown words are not replaced (30.12), but there are not as many improvements (31.07). Since the large vocabulary model [UNK] does not predict as much during training, it is less likely to generate them during decryption, which limits the effectiveness of the post-processing step in this case. K \u2032 = 1, which limits the variety of permitted unusual words, BLEU is not as good as with a moderately larger K \u2032, suggesting that our models can correctly choose between rare alternatives to some extent. If K \u2032 s very large, the performance may slowly deteriorate as the NT system may confuse too many alternatives."}, {"heading": "5 Conclusion", "text": "The proposed approach allows us to train a model with a much larger target vocabulary without substantially increasing computational complexity. It is based on the previous work in (Bengio and Se'ne \u0301 cal, 2008), where importance tests were used to reduce the complexity of calculating the normalization constant of the output word probability in neural language models.In English \u2192 French and English \u2192 German translation tasks, we observed that neural machine translation models trained with the proposed method performed better than those using only limited sentences of target words.The models trained with the very large target vocabulary proved to be just as good or sometimes better when only a select subset of target vocabulary was used during decoding, making the proposed learning algorithm more practical than those using limited sentences of target words.Measured by the EU, our translation models showed that the translation state is comparable to the English translation system."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and the following research funding and computer support agencies: NSERC, Calcul Que'bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Technical report, arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["eron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "eron et al\\.,? \\Q2012\\E", "shortCiteRegEx": "eron et al\\.", "year": 2012}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Yoshua Bengio", "Jean-S\u00e9bastien S\u00e9n\u00e9cal."], "venue": "IEEE Trans. Neural Networks, 19(4):713\u2013722.", "citeRegEx": "Bengio and S\u00e9n\u00e9cal.,? 2008", "shortCiteRegEx": "Bengio and S\u00e9n\u00e9cal.", "year": 2008}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings of the Python for Scientific", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen."], "venue": "Proceedings of the Language Resources and Evaluation Conference, Reykjav\u0131\u0301k, Iceland, May.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder\u2013Decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, October.", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Empiricial Methods in Nat-", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Edinburgh\u2019s phrase-based machine translation systems for WMT-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97\u2013104. Association for Computa-", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of IBM Model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Recursive hetero-associative memories for translation", "author": ["Mikel L. Forcada", "Ram\u00f3n P. \u00d1eco."], "venue": "Jos\u00e9 Mira, Roberto Moreno-D\u0131\u0301az, and Joan Cabestany, editors, Biological and Artificial Computation: From Neuroscience to Technology, volume 1240 of Lecture", "citeRegEx": "Forcada and \u00d1eco.,? 1997", "shortCiteRegEx": "Forcada and \u00d1eco.", "year": 1997}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyvarinen."], "venue": "Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201910).", "citeRegEx": "Gutmann and Hyvarinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyvarinen.", "year": 2010}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700\u20131709. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1,", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press, New York, NY, USA, 1st edition.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "The DCUICTCAS MT system at WMT 2014 on GermanEnglish translation task", "author": ["Liangyou Li", "Xiaofeng Wu", "Santiago Cortes Vaillo", "Jun Xie", "Andy Way", "Qun Liu."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "arXiv preprint arXiv:1410.8206.", "citeRegEx": "Luong et al\\.,? 2014", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "International Conference on Learning Representations: Workshops Track.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26,", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Strouds-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "The RWTH Aachen GermanEnglish machine translation system for WMT 2014", "author": ["Stephan Peitz", "Joern Wuebker", "Markus Freitag", "Hermann Ney."], "venue": "In", "citeRegEx": "Peitz et al\\.,? 2014", "shortCiteRegEx": "Peitz et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS\u20192014.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 98, "endOffset": 177}, {"referenceID": 0, "context": "Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 98, "endOffset": 177}, {"referenceID": 20, "context": "Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 98, "endOffset": 177}, {"referenceID": 20, "context": "The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 102, "endOffset": 149}, {"referenceID": 0, "context": "The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 102, "endOffset": 149}, {"referenceID": 12, "context": "Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system (Koehn et al., 2003).", "startOffset": 150, "endOffset": 170}, {"referenceID": 20, "context": "For instance, all of the models proposed in (Sutskever et al., 2014), (Bahdanau et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 0, "context": ", 2014), (Bahdanau et al., 2014) or (Kalchbrenner and Blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words.", "startOffset": 9, "endOffset": 32}, {"referenceID": 11, "context": ", 2014) or (Kalchbrenner and Blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words.", "startOffset": 11, "endOffset": 43}, {"referenceID": 0, "context": "A usual practice is to construct a target vocabulary of the k most frequent words (a so-called shortlist), where k is often in the range of 30, 000 (Bahdanau et al., 2014) to 80, 000 (Sutskever et al.", "startOffset": 148, "endOffset": 171}, {"referenceID": 20, "context": ", 2014) to 80, 000 (Sutskever et al., 2014).", "startOffset": 19, "endOffset": 43}, {"referenceID": 5, "context": "This approach works well when there are only a few unknown words in the target sentence, but it has been observed that the translation performance degrades rapidly as the number of unknown words increases (Cho et al., 2014a; Bahdanau et al., 2014).", "startOffset": 205, "endOffset": 247}, {"referenceID": 0, "context": "This approach works well when there are only a few unknown words in the target sentence, but it has been observed that the translation performance degrades rapidly as the number of unknown words increases (Cho et al., 2014a; Bahdanau et al., 2014).", "startOffset": 205, "endOffset": 247}, {"referenceID": 0, "context": "We compare the proposed algorithm against the baseline shortlist-based approach in the tasks of English\u2192French and English\u2192German translation using the NMT model introduced in (Bahdanau et al., 2014).", "startOffset": 176, "endOffset": 199}, {"referenceID": 0, "context": "In this section, we briefly describe an approach to neural machine translation proposed recently in (Bahdanau et al., 2014).", "startOffset": 100, "endOffset": 123}, {"referenceID": 9, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 11, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 6, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 20, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 0, "context": "Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 174, "endOffset": 296}, {"referenceID": 0, "context": "In this paper, we use a specific implementation of neural machine translation that uses an attention mechanism, as recently proposed in (Bahdanau et al., 2014).", "startOffset": 136, "endOffset": 159}, {"referenceID": 0, "context": "In (Bahdanau et al., 2014), the encoder in Eq.", "startOffset": 3, "endOffset": 26}, {"referenceID": 6, "context": ", (Cho et al., 2014b)).", "startOffset": 2, "endOffset": 21}, {"referenceID": 0, "context": "For the detailed description of the implementation, we refer the reader to the appendix of (Bahdanau et al., 2014).", "startOffset": 91, "endOffset": 114}, {"referenceID": 0, "context": "Recently proposed neural machine translation models, hence, use a shortlist of 30,000 to 80,000 most frequent words (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 116, "endOffset": 163}, {"referenceID": 20, "context": "Recently proposed neural machine translation models, hence, use a shortlist of 30,000 to 80,000 most frequent words (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 116, "endOffset": 163}, {"referenceID": 5, "context": "First of all, the performance of the model degrades heavily if the translation of a source sentence requires many words that are not included in the shortlist (Cho et al., 2014a).", "startOffset": 159, "endOffset": 178}, {"referenceID": 17, "context": "This has been used proposed recently in (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010).", "startOffset": 40, "endOffset": 90}, {"referenceID": 16, "context": "This has been used proposed recently in (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010).", "startOffset": 40, "endOffset": 90}, {"referenceID": 10, "context": ", 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010).", "startOffset": 46, "endOffset": 75}, {"referenceID": 15, "context": "of (Luong et al., 2014) proposed such an approach for neural machine translation.", "startOffset": 3, "endOffset": 23}, {"referenceID": 2, "context": "The proposed approach is based on the earlier work of (Bengio and S\u00e9n\u00e9cal, 2008).", "startOffset": 54, "endOffset": 80}, {"referenceID": 0, "context": "In the experiments, we evaluate the proposed approach with the neural machine translation model called RNNsearch (Bahdanau et al., 2014) (see Sec.", "startOffset": 113, "endOffset": 136}, {"referenceID": 13, "context": "Other techniques such as transliteration may also be used to further improve the performance (Koehn, 2010).", "startOffset": 93, "endOffset": 106}, {"referenceID": 11, "context": "To ensure fair comparison, the English\u2192French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 138, "endOffset": 217}, {"referenceID": 0, "context": "To ensure fair comparison, the English\u2192French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 138, "endOffset": 217}, {"referenceID": 20, "context": "To ensure fair comparison, the English\u2192French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 138, "endOffset": 217}, {"referenceID": 19, "context": "As for English\u2192German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences.", "startOffset": 75, "endOffset": 112}, {"referenceID": 14, "context": "As for English\u2192German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences.", "startOffset": 75, "endOffset": 112}, {"referenceID": 18, "context": "Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "As a baseline for English\u2192French translation, we use the RNNsearch model trained by (Bahdanau et al., 2014) with 30,000 source and target words.", "startOffset": 84, "endOffset": 107}, {"referenceID": 5, "context": "During beam search, we keep a set of 12 hypotheses and normalize probabilities by the length of the candidate sentences, as in (Cho et al., 2014a) 6.", "startOffset": 127, "endOffset": 146}, {"referenceID": 8, "context": "The bilingual dictionary is built using fast align (Dyer et al., 2013).", "startOffset": 51, "endOffset": 70}, {"referenceID": 20, "context": "1, we present the results obtained by the trained models with very large target vocabularies, and alongside them, the previous results reported in (Sutskever et al., 2014), (Luong et al.", "startOffset": 147, "endOffset": 171}, {"referenceID": 15, "context": ", 2014), (Luong et al., 2014), (Buck et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 4, "context": ", 2014), (Buck et al., 2014) and (Durrani et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": ", 2014) and (Durrani et al., 2014).", "startOffset": 12, "endOffset": 34}, {"referenceID": 6, "context": "The performance of the RNNsearch-LV is also better than that of a standard phrase-based translation system (Cho et al., 2014b).", "startOffset": 107, "endOffset": 126}, {"referenceID": 0, "context": "This is why the baseline score for English\u2192French differs from the one reported in (Bahdanau et al., 2014).", "startOffset": 83, "endOffset": 106}, {"referenceID": 0, "context": "RNNsearch is the model proposed in (Bahdanau et al., 2014), RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 20, "context": ", 2014), RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al., 2014).", "startOffset": 136, "endOffset": 160}, {"referenceID": 20, "context": "(?) (Sutskever et al., 2014), (\u25e6) (Luong et al.", "startOffset": 4, "endOffset": 28}, {"referenceID": 15, "context": ", 2014), (\u25e6) (Luong et al., 2014), (\u2022) (Durrani et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 7, "context": ", 2014), (\u2022) (Durrani et al., 2014), (\u2217) Standard Moses Setting (Cho et al.", "startOffset": 13, "endOffset": 35}, {"referenceID": 6, "context": ", 2014), (\u2217) Standard Moses Setting (Cho et al., 2014b), ( ) (Buck et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 4, "context": ", 2014b), ( ) (Buck et al., 2014).", "startOffset": 14, "endOffset": 33}, {"referenceID": 2, "context": "It is based on the earlier work in (Bengio and S\u00e9n\u00e9cal, 2008) which used importance sampling to reduce the complexity of computing the normalization constant of the output word probability in neural language models.", "startOffset": 35, "endOffset": 61}, {"referenceID": 15, "context": "On the English\u2192French task, a model trained with the proposed approach outperformed the best single neural machine translation (NMT) model from (Luong et al., 2014) by more than 1 BLEU point.", "startOffset": 144, "endOffset": 164}, {"referenceID": 7, "context": "3 BLEU points away from the best system (Durrani et al., 2014).", "startOffset": 40, "endOffset": 62}, {"referenceID": 4, "context": "67) reported in (Buck et al., 2014).", "startOffset": 16, "endOffset": 35}, {"referenceID": 3, "context": "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 57, "endOffset": 102}], "year": 2014, "abstractText": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English\u2192German translation and almost as high performance as state-of-the-art English\u2192French translation system.", "creator": "TeX"}}}