{"id": "1412.2302", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2014", "title": "Theano-based Large-Scale Visual Recognition with Multiple GPUs", "abstract": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.", "histories": [["v1", "Sun, 7 Dec 2014 01:12:10 GMT  (82kb,D)", "http://arxiv.org/abs/1412.2302v1", null], ["v2", "Thu, 1 Jan 2015 07:37:05 GMT  (82kb,D)", "http://arxiv.org/abs/1412.2302v2", null], ["v3", "Fri, 27 Feb 2015 04:53:46 GMT  (83kb,D)", "http://arxiv.org/abs/1412.2302v3", null], ["v4", "Mon, 6 Apr 2015 20:46:11 GMT  (83kb,D)", "http://arxiv.org/abs/1412.2302v4", "ICLR 2015 workshop camera-ready version"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiguang ding", "ruoyan wang", "fei mao", "graham taylor"], "accepted": true, "id": "1412.2302"}, "pdf": {"name": "1412.2302.pdf", "metadata": {"source": "CRF", "title": "TION WITH MULTIPLE GPUS", "authors": ["Weiguang Ding", "Ruoyan Wang", "Fei Mao"], "emails": ["ruoyanry}@uoguelph.ca", "feimao@sharcnet.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "In particular, AlexNet (Krizhevsky et al., 2012), a kind of revolutionary neural network (LeCun et al., 1998) (ConvNet), has significantly improved the performance of image classification by winning the ImageNet Large Scale Visual Recognition Challenge (Russakovsky et al., 2014) in 2012 (ILSVRC 2012). However, as deep learning has become more popular, many open source frameworks have emerged that are able to train deep ConvNets on datasets with over 1 million examples, including Caffe (Jia et al., 2014), Torch7 (Collobert et al., 2011) and cuda-convennet (Krizhevsky et al., 2012). However, the convenience of using them is limited to building \"standardized\" architectures. To experiment with entirely new architectures, researchers must derive the appropriate gradation functions and implement them (Krizhevsky, 2012)."}, {"heading": "2 METHODS", "text": "\"AlexNet\" is now a standard architecture known in the deep learning community and commonly used for benchmarks. It contains 5 revolutionary layers, 3 of which are max pooling1The code is open source at https: / / github.com / uoguelph-mlrg / theano _ alexnet. In addition, there is a toy example at https: / / github.com / uoguelph-mlrg / theano _ multi _ gpu.ar Xiv: 141 2.23 02v1 [cs.LG] 7 layers, 2 fully connected layers and 1 Softmax layer (Krizhevsky et al., 2012). In our AlexNet implementation we use convolution and max pooling operators from the Pylearn2 (Goodfellow et al., 2013)."}, {"heading": "2.1 PARALLEL DATA LOADING", "text": "Figure 1 illustrates the process of parallel training and data loading. Two processes run simultaneously, one for training and the other for loading minibatches. While the training process is working on the current minibatch, the loading process copies the next minibatch from the hard disk to the host memory, prepares it and copies it from the host memory to the GPU memory. After completing the training via the current minibatch, the data batch is moved \"instantaneously\" from the loading process to the training process because they access the same GPU."}, {"heading": "2.2 DATA PARALLELISM", "text": "In this implementation, 2 AlexNets are trained on 2 GPUs. They are initialized identically, and at each step they are updated on different minibatches, and then their parameters (weights, distortions) and impulses are exchanged and averaged. 2Pre-processing involves subtracting the center image, randomly cropping and flipping images (Krizhevsky et al., 2012).Figure 2 illustrates the steps required for training on a minibatch. For each weight3 matrix in the model, 2 common variables are assigned: one for updating and one for storing copied weights from the other GPU. The common variables for updating to 2 GPUs start right away. In the first step, they are updated separately on different data batches. In the second step, weights are exchanged between GPUs. In the third step, these weights (no longer the same) are averaged on both GPUs."}, {"heading": "3 RESULTS", "text": "Our experimental system consists of 2 Intel Xeon E5-2620 CPUs (6-core and 2.10 GHz each) and 3 Nvidia Titan Black GPUs. 2 of the GPUs are under the same PCI-E switch and are used for the 2-GPU implementation. We do not use the third GPU. For experiments on a single GPU, we use batch size 256. Likewise, we use batch size 128 for experiments on 2 GPUs. We have recorded the time to train 20 batches (5,120 images) under different settings and compared them with Caffe4 in Table 1. We can see that both parallelism of data on 2 GPUs bring significant speeds. Implementation of the 2-GPU & parallel charging is slightly faster than Caffe itself and slower than Caffe with cuDNN (Chetlur et al., 2014)."}, {"heading": "4 DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 ALTERNATIVE IMPLEMENTATION OF ALEXNET", "text": "A possible alternative to the kuda-convoked pylearn2 wrapper is to use cuDNN-accelerated Theano-built wave operations. However, it is difficult to replicate AlexNet accurately at this point, as Theano currently does not offer the \"same\" fill mode (where the input form before and the output form after wave formation are the same) that is used in two of its wavelengths. Another reason is that AlexNet's maximum pooling steps may differ from the maximum pooling filter sizes that are currently not supported by the built-in Theano operations 5. However, this limitation applies only to the exact replication of AlexNet. It does not prevent Theano's built-in functions from being used to build and deploy other large architectures."}, {"heading": "4.2 NATIVE THEANO MULTI-GPU SUPPORT", "text": "Theano native multi-GPU support is in development6. Our current implementation is a temporary workaround prior to its release and could also provide helpful communication components."}, {"heading": "4.3 RELATED WORK", "text": "Many multi-GPU frameworks have been proposed and implemented (Yadan et al., 2013; Zou et al., 2014; Paine et al., 2013; Krizhevsky, 2014), usually adopting mixed data and model parallelisms.3The same process is performed in distortions and pulsations.4The performance of Caffe is equivalent to http: / / caffe.berkeleyvision.org / performance _ hardware.html, where timing information is provided for CaffeNet. Since CaffeNet has similar structures, we consider this a rough reference.5Although pylearn2 provides this functionality, it is significantly slower. 6https: / / groups.google.com / d / msg / theano-users / vtR _ L0QltpE / Kp5hK1nFLtsJThis report implements only the data parallelism framework, but could potentially be extended to the parallelism model with a non-trial effort."}, {"heading": "4.4 LIMITATIONS", "text": "In order to use fast peer-to-peer GPU memory copy, the GPUs must be under the same PCI-E switch. Otherwise, communication must pass through the host memory, resulting in longer latency. Situations involving more GPUs are discussed in Krizhevsky (2014). Due to our current hardware limitation, we have only proposed and experimented with a 2-GPU implementation. This report and code will be updated as experiments with more GPUs are conducted."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Lev Givon for his helpful suggestions on how to use the PyCUDA library."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "arXiv preprint arXiv:1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Pycuda and pyopencl: A scripting-based approach to gpu run-time code generation", "author": ["Kl\u00f6ckner", "Andreas", "Pinto", "Nicolas", "Lee", "Yunsup", "Catanzaro", "Bryan", "Ivanov", "Paul", "Fasih", "Ahmed"], "venue": "Parallel Computing,", "citeRegEx": "Kl\u00f6ckner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kl\u00f6ckner et al\\.", "year": 2012}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["Krizhevsky", "Alex"], "venue": "arXiv preprint arXiv:1404.5997,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2014\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Gpu asynchronous stochastic gradient descent to speed up neural network training", "author": ["Paine", "Thomas", "Jin", "Hailin", "Yang", "Jianchao", "Lin", "Zhe", "Huang"], "venue": "arXiv preprint arXiv:1312.6186,", "citeRegEx": "Paine et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paine et al\\.", "year": 2013}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael"], "venue": "arXiv preprint arXiv:1409.0575,", "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Multi-gpu training of convnets", "author": ["Yadan", "Omry", "Adams", "Keith", "Taigman", "Yaniv", "Ranzato", "MarcAurelio"], "venue": "arXiv preprint arXiv:1312.5853,", "citeRegEx": "Yadan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yadan et al\\.", "year": 2013}, {"title": "Mariana: Tencent deep learning platform and its applications", "author": ["Zou", "Yongqiang", "Jin", "Xing", "Li", "Yi", "Guo", "Zhimao", "Wang", "Eryu", "Xiao", "Bin"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "Zou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "ABSTRACT In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs.", "startOffset": 60, "endOffset": 85}, {"referenceID": 5, "context": "Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU.", "startOffset": 76, "endOffset": 94}, {"referenceID": 8, "context": "In particular, AlexNet (Krizhevsky et al., 2012), a type of convolutional neural network (LeCun et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 9, "context": ", 2012), a type of convolutional neural network (LeCun et al., 1998) (ConvNet), has significantly improved the performance of image classification by winning the 2012 ImageNet Large Scale Visual Recognition Challenge (Russakovsky et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 11, "context": ", 1998) (ConvNet), has significantly improved the performance of image classification by winning the 2012 ImageNet Large Scale Visual Recognition Challenge (Russakovsky et al., 2014) (ILSVRC 2012).", "startOffset": 156, "endOffset": 182}, {"referenceID": 5, "context": "These include Caffe (Jia et al., 2014), Torch7 (Collobert et al.", "startOffset": 20, "endOffset": 38}, {"referenceID": 3, "context": ", 2014), Torch7 (Collobert et al., 2011) and cuda-convnet (Krizhevsky et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 8, "context": ", 2011) and cuda-convnet (Krizhevsky et al., 2012).", "startOffset": 25, "endOffset": 50}, {"referenceID": 1, "context": "Theano (Bergstra et al., 2010; Bastien et al., 2012), on the other hand, provides the automatic differentiation feature, which saves researchers from tedious derivations and can help in avoiding errors in such calculations.", "startOffset": 7, "endOffset": 52}, {"referenceID": 0, "context": "Theano (Bergstra et al., 2010; Bastien et al., 2012), on the other hand, provides the automatic differentiation feature, which saves researchers from tedious derivations and can help in avoiding errors in such calculations.", "startOffset": 7, "endOffset": 52}, {"referenceID": 8, "context": "layers, 2 fully connected layers, and 1 softmax layer (Krizhevsky et al., 2012).", "startOffset": 54, "endOffset": 79}, {"referenceID": 4, "context": "In our AlexNet implementation, we use convolution and max pooling operators from the Pylearn2 (Goodfellow et al., 2013) wrapper of cuda-convnet, the original implemenation of AlexNet.", "startOffset": 94, "endOffset": 119}, {"referenceID": 6, "context": "We also use functions in the PyCUDA library (Kl\u00f6ckner et al., 2012) to transfer Theano shared variables between different python processes for two tasks: 1.", "startOffset": 44, "endOffset": 67}, {"referenceID": 8, "context": "Preprocessing includes subtracting the mean image, randomly cropping and flipping images (Krizhevsky et al., 2012).", "startOffset": 89, "endOffset": 114}, {"referenceID": 2, "context": "The 2-GPU & parallel loading implementation is slightly faster than Caffe itself and slower than Caffe with cuDNN (Chetlur et al., 2014).", "startOffset": 114, "endOffset": 136}, {"referenceID": 12, "context": "3 RELATED WORK Many multi-GPU frameworks has been proposed and implemented (Yadan et al., 2013; Zou et al., 2014; Paine et al., 2013; Krizhevsky, 2014), usually adopting a mixed data and model parallelism.", "startOffset": 75, "endOffset": 151}, {"referenceID": 13, "context": "3 RELATED WORK Many multi-GPU frameworks has been proposed and implemented (Yadan et al., 2013; Zou et al., 2014; Paine et al., 2013; Krizhevsky, 2014), usually adopting a mixed data and model parallelism.", "startOffset": 75, "endOffset": 151}, {"referenceID": 10, "context": "3 RELATED WORK Many multi-GPU frameworks has been proposed and implemented (Yadan et al., 2013; Zou et al., 2014; Paine et al., 2013; Krizhevsky, 2014), usually adopting a mixed data and model parallelism.", "startOffset": 75, "endOffset": 151}], "year": 2017, "abstractText": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.", "creator": "LaTeX with hyperref package"}}}