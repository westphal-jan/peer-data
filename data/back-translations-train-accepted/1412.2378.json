{"id": "1412.2378", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2014", "title": "Learning Word Representations from Relational Graphs", "abstract": "Attributes of words and relations between two words are central to numerous tasks in Artificial Intelligence such as knowledge representation, similarity measurement, and analogy detection. Often when two words share one or more attributes in common, they are connected by some semantic relations. On the other hand, if there are numerous semantic relations between two words, we can expect some of the attributes of one of the words to be inherited by the other. Motivated by this close connection between attributes and relations, given a relational graph in which words are inter- connected via numerous semantic relations, we propose a method to learn a latent representation for the individual words. The proposed method considers not only the co-occurrences of words as done by existing approaches for word representation learning, but also the semantic relations in which two words co-occur. To evaluate the accuracy of the word representations learnt using the proposed method, we use the learnt word representations to solve semantic word analogy problems. Our experimental results show that it is possible to learn better word representations by using semantic semantics between words.", "histories": [["v1", "Sun, 7 Dec 2014 17:49:53 GMT  (211kb,D)", "http://arxiv.org/abs/1412.2378v1", "AAAI 2015"]], "COMMENTS": "AAAI 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["danushka bollegala", "takanori maehara", "yuichi yoshida", "ken-ichi kawarabayashi"], "accepted": true, "id": "1412.2378"}, "pdf": {"name": "1412.2378.pdf", "metadata": {"source": "CRF", "title": "Learning Word Representations from Relational Graphs", "authors": ["Danushka Bollegala", "Takanori Maehara", "Yuichi Yoshida", "Ken-ichi Kawarabayashi"], "emails": [], "sections": [{"heading": "Introduction", "text": "In fact, the fact is that most of them are able to move to another world, in which they are able, in which they are able to move, in which they are able, in which they are able to move, in which they are able, in which they are able, in which they are able to move, in which they are able to move, in which they are able, in which they are able, in which they are able, in which they are able to move."}, {"heading": "Related Work", "text": "In fact, it is that we see ourselves in a position to be in and that we are able to put ourselves in a position to be in the lead."}, {"heading": "Learning Word Representations from Relational Graphs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Relational Graphs", "text": "A pattern is a predicate of two arguments and expresses a semantic relationship between the two words. Formally, an edge e that connects two vertices u, v, V in the relational graph G, is a tuple (u, v, l (e), w (e)), where l (e) denotes the markup type that corresponds to the pattern that occurs in any connection with the two words u and v, and w (e) the frequency strength between l (e) and the word pair (u, v). Each word in the vocabulary is represented by a unique vertice in the relative graph, and each pattern is represented by a unique markup type. Due to this one-to-one correspondence between words and the word pair (u, v), a pattern that coincides with the two names (i.e., most of the names in the first and second rows) will match well with the names (i.e., the ones that we match in the second row)."}, {"heading": "X is a large Y [0.8]", "text": "Consider the relationship diagram shown in Figure 1. Suppose, for example, that we observed the context ostrich is a large bird living in a corpus in Africa. Then, we extract the lexical pattern X is a large Y between ostrich and bird from this context and incorporate it into the relationship diagram by adding two vertices for ostrich and bird and an edge from ostrich to bird. Such lexical patterns have been used for related tasks, such as measuring semantic similarity between words (Bollegala et al. 2007)."}, {"heading": "Learning Word Representations", "text": "The dimensionality d of the vector space is a predefined parameter of the method, and by adaptation it is possible to obtain word representations in different sizes. Consider two vertices u and v connected by an edge with labeling l (labeling l and weight w). We model the problem of optimal word representations x (u) and pattern representations G (l) as a solution for the following square loss minimizations x (u) and the designation l by a matrix G (l). We model the problem of optimal word representations x (u) and pattern representations G (l) as a solution of the following square loss minimization x (u)."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Creating Relational Graphs", "text": "We use the English ukWaC1 corpus in our experiments. ukWaC is a 2 billion token corpus constructed from the web to restrict the crawl and limit the number of words from the British National Corpus (BNC). The corpus is a pattern that selects the patterns of individual words in individual terms. In addition, MaltParser3 is used to create a dependent version of the ukWaC corpus.To create relational graphs, we first compute the cooccurrences of words in sentences in the ukWaC corpus. For two words u and v that occur in more than 100 sentences, we create two word pairs (v, u). Considering the scale of the ukWaC corpus, low co-acting words often represent misspellings or non-English terms.Next, for each generated word-pair, we retrieve the set of sets in which the scale of the ukWaC corpus, low-acting words."}, {"heading": "Evaluation", "text": "We use the semantic word analogy dataset, first proposed by Mikolov et al. (2013a) and used in many previous work to evaluate word representation methods. In contrast to syntactic word analogies such as the past tense or plural forms of verbs that can be accurately grasped using rule-based methods (Lpage 2000), semantic analogies are more difficult to detect by transforming them on a surface level. Therefore, we consider it appropriate to evaluate word representation methods using semantic word analogies (e.g. boy, girl vs. king, queen).The dataset contains 8869 word pairs that represent word analogies covering various semantic relationships, such as the capital of a country (e.g. Tokyo, Japan vs. Paris, France), and family relationships (e.g. boys, girls vs. king, queen).A word representation method is evaluated by its ability to correctly answer word analogies with word analogies, e.g. by using this word representation method."}, {"heading": "Results", "text": "In fact, it is that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world, \"he said."}, {"heading": "Conclusion", "text": "We have proposed a method that takes into account not only the randomness of two words, but also the semantic relationships in which they occur together to learn word representations. It can be applied to manually generated relational graphs such as ontologies, as well as to automatically extracted relational graphs from text corpora. We used the proposed method to learn word representations from three types of relational graphs. We used the learned word representations to answer semantic word analogy questions using a previously proposed data set. Our experimental results show that lexical patterns are particularly useful for learning good word representations and exceed several basic methods. We hope that our work will inspire future research in the field of word representation to take advantage of the rich semantic relationships between words that go beyond simple co-events."}], "references": [{"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics, 36(4):673 \u2013 721,", "citeRegEx": "Baroni and Lenci 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli. Nouns are vectors"], "venue": "EMNLP, pages 1183 \u2013 1193,", "citeRegEx": "Baroni and Zamparelli 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Don\u2019t count", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, pages 238\u2013247,", "citeRegEx": "Baroni et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Journal of Machine Learning Research", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin. A neural probabilistic language model"], "venue": "3:1137 \u2013 1155,", "citeRegEx": "Bengio et al. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798 \u2013 1828, March", "citeRegEx": "Bengio et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Websim: A web-based semantic similarity measure", "author": ["Danushka Bollegala", "Yutaka Matsuo", "Mitsuru Ishizuka"], "venue": "Proc. of 21st Annual Conference of the Japanese Society of Artitificial Intelligence, pages 757 \u2013 766,", "citeRegEx": "Bollegala et al. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Foundations and Trends in Machine Learning", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Exkstein. Distributed optimization", "statistical learning via the alternating direction method of multipliers"], "venue": "3(1):1 \u2013 122,", "citeRegEx": "Boyd et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "The Morgan Kaufmann Series in Artificial Intelligence", "author": ["Ronald Brachman", "Hector J. Levesque. Knowledge Representation", "Reasoning"], "venue": "Morgan Kaufmann Publishers Inc., June", "citeRegEx": "Brachman and Levesque 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuska"], "venue": "Journal of Machine Learning Research, 12:2493 \u2013 2537,", "citeRegEx": "Collobert et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Using relational similarity between word pairs for latent relational search on the web", "author": ["Duc"], "venue": "In IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,", "citeRegEx": "Duc,? \\Q2010\\E", "shortCiteRegEx": "Duc", "year": 2010}, {"title": "12:2121 \u2013 2159", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization. Journal of Machine Learning Research"], "venue": "July", "citeRegEx": "Duchi et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A synopsis of linguistic theory 1930-55", "author": ["John R. Firth"], "venue": "Studies in Linguistic Analysis, pages 1 \u2013 32,", "citeRegEx": "Firth 1957", "shortCiteRegEx": null, "year": 1957}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Edward Grefenstette"], "venue": "*SEM, pages 1 \u2013 10,", "citeRegEx": "Grefenstette 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In ACL", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng. Improving word representations via global context", "multiple word prototypes"], "venue": "pages 873 \u2013 882,", "citeRegEx": "Huang et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In ACL", "author": ["Yves Lepage. Languages of analogical strings"], "venue": "pages 488 \u2013 494,", "citeRegEx": "Lepage 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "CoRR", "author": ["Tomas Mikolov", "Kai Chen", "Jeffrey Dean. Efficient estimation of word representation in vector space"], "venue": "abs/1301.3781,", "citeRegEx": "Mikolov et al. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "In NIPS", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111 \u2013 3119,", "citeRegEx": "Mikolov et al. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "In NAACL", "author": ["Tomas Mikolov", "Wen tau Yih", "Geoffrey Zweig. Linguistic regularities in continous space word representations"], "venue": "pages 746 \u2013 751,", "citeRegEx": "Mikolov et al. 2013c", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu"], "venue": "NIPS,", "citeRegEx": "Mnih and Kavukcuoglu 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In ACL", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni. A practical", "linguistically-motivated approach to compositional distributional semantics"], "venue": "pages 90\u201399,", "citeRegEx": "Paperno et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Glove: global vectors for word representation", "author": ["Jeffery Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "EMNLP,", "citeRegEx": "Pennington et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Ng", "Chris Manning"], "venue": "ICML,", "citeRegEx": "Socher et al. 2011a", "shortCiteRegEx": null, "year": 2011}, {"title": "In EMNLP", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions"], "venue": "pages 151\u2013161,", "citeRegEx": "Socher et al. 2011b", "shortCiteRegEx": null, "year": 2011}, {"title": "In EMNLP", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng. Semantic compositionality through recursive matrix-vector spaces"], "venue": "pages 1201\u20131211,", "citeRegEx": "Socher et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In ACL", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y. Parsing with compositional vector grammars"], "venue": "pages 455\u2013465,", "citeRegEx": "Socher et al. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "NIPS,", "citeRegEx": "Socher et al. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Aritificial Intelligence Research, 37:141 \u2013 188,", "citeRegEx": "Turney and Pantel 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Similarity of semantic relations", "author": ["P.D. Turney"], "venue": "Computational Linguistics, 32(3):379\u2013416", "citeRegEx": "Turney 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In EMNLP", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu. Deep learning for Chinese word segmentation", "POS tagging"], "venue": "pages 647\u2013657,", "citeRegEx": "Zheng et al. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2014, "abstractText": "Attributes of words and relations between two words are central to numerous tasks in Artificial Intelligence such as knowledge representation, similarity measurement, and analogy detection. Often when two words share one or more attributes in common, they are connected by some semantic relations. On the other hand, if there are numerous semantic relations between two words, we can expect some of the attributes of one of the words to be inherited by the other. Motivated by this close connection between attributes and relations, given a relational graph in which words are interconnected via numerous semantic relations, we propose a method to learn a latent representation for the individual words. The proposed method considers not only the co-occurrences of words as done by existing approaches for word representation learning, but also the semantic relations in which two words co-occur. To evaluate the accuracy of the word representations learnt using the proposed method, we use the learnt word representations to solve semantic word analogy problems. Our experimental results show that it is possible to learn better word representations by using semantic semantics be-", "creator": "LaTeX with hyperref package"}}}