{"id": "1412.2404", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2014", "title": "Dimensionality Reduction with Subspace Structure Preservation", "abstract": "Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that $2K$ projection vectors are sufficient for the independence preservation of any $K$ class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving \\textit{state-of-the-art} results compared to popular dimensionality reduction techniques.", "histories": [["v1", "Sun, 7 Dec 2014 22:02:33 GMT  (104kb,D)", "http://arxiv.org/abs/1412.2404v1", "Published in NIPS 2014"], ["v2", "Sun, 31 May 2015 22:30:47 GMT  (104kb,D)", "http://arxiv.org/abs/1412.2404v2", "Published in NIPS 2014; v2: minor updates to the algorithm and added a few lines addressing application to large-scale/high-dimensional data"], ["v3", "Wed, 6 Apr 2016 23:11:46 GMT  (226kb,D)", "http://arxiv.org/abs/1412.2404v3", "Published in NIPS 2014; v2: minor updates to the algorithm and added a few lines addressing application to large-scale/high-dimensional data"]], "COMMENTS": "Published in NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["devansh arpit", "ifeoma nwogu", "venu govindaraju"], "accepted": true, "id": "1412.2404"}, "pdf": {"name": "1412.2404.pdf", "metadata": {"source": "CRF", "title": "Dimensionality Reduction with Subspace Structure Preservation", "authors": ["Devansh Arpit", "Ifeoma Nwogu"], "emails": ["devansha@buffalo.edu", "inwogu@buffalo.edu", "govind@buffalo.edu"], "sections": [{"heading": "1 Introduction", "text": "A number of real-world applications model data scanned from an association of independent sub-spaces. These applications include image representation and compression [6], system theory [12], image segmentation [15], motion segmentation [13], face grouping [7, 5], and texture segmentation [8], to name a few. Dimensionality reduction is generally used before applying these methods, because most of these algorithms optimize expensive loss functions such as core norm, '1 regulation, i.e. most of these applications simply apply techniques to reduce dimensionality from store theory or reduce images (in the case of image data) as a pre-processing step. The unification of independent subspace models can be thought of as a generalization of the traditional approach to representing a given set of data using a single low-dimensional subspace (e.g. reduction of dimensions or reduction of images (in the case of pre-processing data)."}, {"heading": "2 Preliminaries", "text": "We say that these K subspaces are independent if there is no non-zero vector in Si, which is a linear combination of vectors in the other K-1 subspaces. Let the columns of the matrix Bi-Rn-d denote the support of the ith subspace of the d dimensions. Then, each vector in this subspace can be represented as x = Biw-w-Rd. Now, we define the concept of the margin between two subspaces. Definition 1 (subspace margin) Subspaces Si and Sj are separated by margin."}, {"heading": "3 Proposed Approach", "text": "The number of vectors we need depends not only on the size of the dataset, but also on the number of vectors for the structure of the K-class that we are able to find a fixed number of vectors for the structure of the K-class dataset. This theorem is a useful property of each individual subspace.Theorem 2 Let's be the vectors v1 and v2 that we have for all two subspaces.Let's be the vectors v1 and v2, the vector vector vector pairs for all two discommon subspaces S1 and S2 in Rn."}, {"heading": "3.1 Implementation", "text": "A naive approach to searching for projection vectors (say, for a binary class case) would be to calculate the SVD of the matrix XT1 X2, where the columns of X1 and X2 contain vectors of class 1 and class 2. For large datasets, this would not only be mathematically expensive, but also unable to deal with noise. Therefore, although Theorem 3 guarantees any information about the structure of dataset X after projection using P as specified, it does not solve the problem of dimensionality reduction. The reason for this is that a labeled datasector from a union of independent subspaces, we have no information about the basis or even the dimensionality of the underlying subspaces, the construction of projection matrix P is specified as in Theorem 3 itself. To solve this problem, we project an algorithm that attempts to find the underlying principal vector pair between subspaces and others."}, {"heading": "3.2 Justification", "text": "The definition 1 for margin 1 between two sub-ranges S1 and S2 can be expressed equivalently. (w21 2-B1w1-B2w2-B2w2-B2w2-B2w2-B2w2), where the columns of B1 and B2-B2 are the basis of two sub-ranges S1 and S2 respectively so that BT1 B1 and B2 are both identity matrices.Proposition 5 LB1 and B2-B2 can be the basis of two sub-ranges S1 and S2. (w2-B2), w2 is the basis of two sub-ranges S1 and S2. (w2), wB1-B2 is the basis of two sub-ranges S1 and S2. (w2-B2), w2 is the basis of two sub-ranges S1 and S2."}, {"heading": "3.3 Complexity", "text": "Solving algorithm 1 requires solving an unrestricted quadratic program within a awhile loop. Suppose that we traverse this awhile loop for T-iterations, and that we solve the quadratic program in each iteration with conjugated gradient descent. Moreover, it is known that for each matrix A-Ra \u00b7 b and the vector b-Ra conjugate gradients applied to a problem of the form margin min w-Ax \u2212 b-2 (9), time O (ab-K) is required, where K is the condition number of ATA. Therefore, it is easy to recognize that for calculating the projection matrix for a K class problem in our case is O (KTnN-K), where n is the dimensionality of the attribute space, N is the total number of samples, and K is the condition number of the matrix (XTk + 1). Here the identity matrix is I."}, {"heading": "4 Empirical Analysis", "text": "In this section, we present empirical evidence for our theoretical analysis of our subspace learning approach. For real data, we use the following data sets: 1. Extended Yale data set B [2]: This data set consists of more than 2414 frontal facial images of 38 individuals (K = 38) with 64 images per person.These images were taken under limited but different illumination conditions.2. AR data set [9]: This data set consists of more than 4000 frontal facial images of 126 individuals with 26 images per person.These images were taken under different illumination, expression, and facial shroud.For our experiments, we use images of 100 individuals (K = 100) with 50 men and 50 women. Furthermore, we only use 14 images per class that correspond to illumination and expression changes, corresponding to 7 images from session 1 and 7 from session 2.3. PIE data set [11]: The pose, lighting, and expression (all PIE 68 subdatabase units) of the PIE-11 database."}, {"heading": "4.1 Qualitative Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Two Subspaces-Two Lines", "text": "We test both the claim of theorem 2 and the quality of the approximation achieved by algorithm 1 in this paragraph. We perform these tests on both synthetic and real data.1. Synthetic data: We create two random subspaces of dimensionality 20 and 30 in R1000 (note that these subspaces will be independent with probability 1).We randomly generate 100 data vectors from each subspace and normalize them to have a unit length. We then calculate the 1st main vector pair between the two subspaces using their base vectors by running SVD from BT1 B2, with B1 and B2 being the basis of the two subspaces. We orthonorize the vector pair to form the projection matrix Pa. Next, we use the marked data set of 200 points that is generated to form the projection matrix Pb."}, {"heading": "4.1.2 Multi-class separability", "text": "We analyze the separation between the K-classes of a given K-class dataset after the dimensionality reduction. First, we calculate the projection matrix for this dataset using our approach and project the data. Second, we calculate the uppermost main vector for each class separately from the projected data, resulting in K-vectors. If the columns of the matrix Z and R2K \u00d7 K contain these vectors, then to visualize the separability between the classes, we simply take the point product of the matrix Z with us, i.e. ZTZ. Figure 4 shows this visualization for the three face datasets. The diagonal elements represent the auto-point product, so the value is 1 (white). The non-diagonal elements represent the class point product, and these values are throughout small (dark) for all three datasets reflecting the separability between the classes."}, {"heading": "4.2 Quantitative Analysis", "text": "To evaluate Theorem 3, we perform a classification experiment for all three of the above real data sets by projecting the data vectors with different dimensionality techniques. We compare our quantitative results with PCA, Linear Discriminance Analysis (LDA), Regularized LDA and Random Projections (RP) 1. We use the sparse coding [14] for classification. For Extended Yale data set B, we use all 38 classes for evaluation with 50% -50% Tension Test Split 1 and 70% -30% Tension Test Split 2. Since our method is randomized, we perform 50 runs of the projection matrix with algorithm 1 and report the mean accuracy with standard deviations. Likewise for the RP, we generate 50 different random matrices and then perform the classification."}, {"heading": "5 Conclusion", "text": "We proposed a theoretical analysis to maintain independence between multiple subspaces. We show that for K-independent subspaces, 2K projection vectors are sufficient to maintain independence (theorem 3).This result is motivated by our observation that there is a two-dimensional plane for any number of subspaces of arbitrary dimensionality, so that after projection the entire subspace collapses on only two lines (theorem 2).As a result of this analysis, we proposed an efficient iterative algorithm (1) that attempts to use these properties to learn a projection matrix for dimensionality reduction that maintains independence between multiple subspaces. Our empirical results on three real data sets provide state-of-the-art results compared to common methods for dimensionality reduction."}], "references": [{"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Comput.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Locality preserving projections (lpp)", "author": ["X. He", "P. Niyogi"], "venue": "Proc. of the NIPS, Advances in Neural Information Processing Systems. Vancouver: MIT Press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Neighborhood preserving embedding", "author": ["Xiaofei He", "Deng Cai", "Shuicheng Yan", "Hong-Jiang Zhang"], "venue": "In Computer Vision,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Clustering appearances of objects under varying illumination conditions", "author": ["Jeffrey Ho", "Ming-Husang Yang", "Jongwoo Lim", "Kuang-Chih Lee", "David Kriegman"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Multiscale hybrid linear models for lossy image representation", "author": ["Wei Hong", "John Wright", "Kun Huang", "Yi Ma"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["Guangcan Liu", "Zhouchen Lin", "Yong Yu"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Segmentation of multivariate mixed data via lossy coding and compression", "author": ["Yi Ma", "Harm Derksen", "Wei Hong", "John Wright", "Student Member"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": "Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "The cmu pose, illumination, and expression (pie) database", "author": ["Terence Sim", "Simon Baker", "Maan Bsat"], "venue": "In Automatic Face and Gesture Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "An algebraic geometric approach to the identification of a class of linear hybrid systems", "author": ["Ren\u00e9 Vidal", "Stefano Soatto", "Yi Ma", "Shankar Sastry"], "venue": "In Decision and Control,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Multiframe motion segmentation with missing data using powerfactorization and gpca", "author": ["Ren\u00e9 Vidal", "Roberto Tron", "Richard Hartley"], "venue": "International Journal of Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Yi Ma"], "venue": "IEEEE TPAMI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Unsupervised segmentation of natural images via lossy data compression", "author": ["Allen Y Yang", "John Wright", "Yi Ma", "S Shankar Sastry"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "These applications include image representation and compression [6], systems theory [12], image segmentation [15], motion segmentation [13], face clustering [7, 5] and texture segmentation [8], to name a few.", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "These applications include image representation and compression [6], systems theory [12], image segmentation [15], motion segmentation [13], face clustering [7, 5] and texture segmentation [8], to name a few.", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "These applications include image representation and compression [6], systems theory [12], image segmentation [15], motion segmentation [13], face clustering [7, 5] and texture segmentation [8], to name a few.", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "These applications include image representation and compression [6], systems theory [12], image segmentation [15], motion segmentation [13], face clustering [7, 5] and texture segmentation [8], to name a few.", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "These applications include image representation and compression [6], systems theory [12], image segmentation [15], motion segmentation [13], face clustering [7, 5] and texture segmentation [8], to name a few.", "startOffset": 157, "endOffset": 163}, {"referenceID": 4, "context": "These applications include image representation and compression [6], systems theory [12], image segmentation [15], motion segmentation [13], face clustering [7, 5] and texture segmentation [8], to name a few.", "startOffset": 157, "endOffset": 163}, {"referenceID": 7, "context": "These applications include image representation and compression [6], systems theory [12], image segmentation [15], motion segmentation [13], face clustering [7, 5] and texture segmentation [8], to name a few.", "startOffset": 189, "endOffset": 192}, {"referenceID": 8, "context": "Although a number of existing dimensionality reduction techniques [10, 3, 1, 4] try to preserve the spacial geometry of any given data, no prior work has tried to explicitly preserve the independence between subspaces to the best of our knowledge.", "startOffset": 66, "endOffset": 79}, {"referenceID": 2, "context": "Although a number of existing dimensionality reduction techniques [10, 3, 1, 4] try to preserve the spacial geometry of any given data, no prior work has tried to explicitly preserve the independence between subspaces to the best of our knowledge.", "startOffset": 66, "endOffset": 79}, {"referenceID": 0, "context": "Although a number of existing dimensionality reduction techniques [10, 3, 1, 4] try to preserve the spacial geometry of any given data, no prior work has tried to explicitly preserve the independence between subspaces to the best of our knowledge.", "startOffset": 66, "endOffset": 79}, {"referenceID": 3, "context": "Although a number of existing dimensionality reduction techniques [10, 3, 1, 4] try to preserve the spacial geometry of any given data, no prior work has tried to explicitly preserve the independence between subspaces to the best of our knowledge.", "startOffset": 66, "endOffset": 79}, {"referenceID": 1, "context": "Extended Yale dataset B [2]: It consists of\u223c 2414 frontal face images of 38 individuals (K = 38) with 64 images per person.", "startOffset": 24, "endOffset": 27}, {"referenceID": 12, "context": "For our experiments, similar to [14], we use images from 100 individuals (K = 100) with 50 males and 50 females.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "PIE dataset [11]: The pose, illumination, and expression (PIE) database is a subset of CMU PIE dataset consisting of 11, 554 images of 68 people (K = 68).", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "We make use of sparse coding [14] for classification.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "We also used LPP (Locality Preserving Projections) [3], NPE (Neighborhood Preserving Embedding) [4], and Laplacian Eigenmaps [1] for dimensionality reduction on Extended Yale B dataset.", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "We also used LPP (Locality Preserving Projections) [3], NPE (Neighborhood Preserving Embedding) [4], and Laplacian Eigenmaps [1] for dimensionality reduction on Extended Yale B dataset.", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "We also used LPP (Locality Preserving Projections) [3], NPE (Neighborhood Preserving Embedding) [4], and Laplacian Eigenmaps [1] for dimensionality reduction on Extended Yale B dataset.", "startOffset": 125, "endOffset": 128}], "year": 2017, "abstractText": "Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that 2K projection vectors are sufficient for the independence preservation of any K class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving state-of-the-art results compared to popular dimensionality reduction techniques.", "creator": "LaTeX with hyperref package"}}}