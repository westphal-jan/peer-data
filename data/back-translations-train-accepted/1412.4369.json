{"id": "1412.4369", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2014", "title": "Incorporating Both Distributional and Relational Semantics in Word Representations", "abstract": "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.", "histories": [["v1", "Sun, 14 Dec 2014 15:18:18 GMT  (132kb,D)", "https://arxiv.org/abs/1412.4369v1", "Under review as a workshop contribution at ICLR2015 (long version of the paper)"], ["v2", "Thu, 18 Dec 2014 12:44:01 GMT  (132kb,D)", "http://arxiv.org/abs/1412.4369v2", "Under review as a workshop contribution at ICLR2015 (long version of the paper)"], ["v3", "Sat, 21 Mar 2015 13:21:20 GMT  (132kb,D)", "http://arxiv.org/abs/1412.4369v3", "This is the long version of a short paper accepted as a workshop contribution at ICLR2015"]], "COMMENTS": "Under review as a workshop contribution at ICLR2015 (long version of the paper)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel fried", "kevin duh"], "accepted": true, "id": "1412.4369"}, "pdf": {"name": "1412.4369.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Daniel Fried"], "emails": ["dfried@email.arizona.edu", "kevinduh@is.naist.jp"], "sections": [{"heading": "1 INTRODUCTION", "text": "We are interested in algorithms for learning vector representations of words, but recent work has shown that such representations, also known as word embedding, can successfully capture the semantic and syntactic regularities of words (Mikolov et al., 2013a) and improve the performance of various natural language processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), parsing (Socher et al., 2013a), and semantic role designation (Collobert et al., 2011). Although many types of representation learning algorithms have been proposed so far, they are all essentially based on the same premise of distribution semantics (Harris et al., 1954), embodied by J. R. Firth's dictate: \"You will know a word through the company that the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al."}, {"heading": "2 OBJECTIVES FOR REPRESENTATION LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 DISTRIBUTIONAL SEMANTICS OBJECTIVE", "text": "A standard method for implementing distributional semantics in representation learning is the Neural Language Model (NLM) by Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi-Rd, which concerns the embedding of the word. An n-length sequence of words (i1, i2,., in) is represented as a vector x by concatenating the vector embeddings for each word, x = [wi1; wi2.; win]. This vector x is then evaluated by being fed through a two-layer neural network of h-hidden nodes: SNLM (x) = u > (f (Ax + b)) (1), where A-Rh \u00d7 (nd) is the weight matrix and b-Rh is the weight matrix training, where the bias vector is the hidden layer for the layer, the vector for the layer, and the sigmot."}, {"heading": "2.2 RELATIONAL SEMANTICS OBJECTIVE", "text": "Methods for learning word representations based on relational semantics have recently been researched. First, we present a simple new goal based on WordNet diagrams (\u00a7 2.2.1), then we discuss two current proposals that directly model relationship types (\u00a7 2.2.2). While our goals focus on relational semantics in WordNet, they are expandable to other types of relational data, including knowledge databases such as Freebase."}, {"heading": "2.2.1 GRAPH DISTANCE", "text": "In this approach, we aim to train word embeddings so that the distance between word embeddings in vector space is a function of the distance between the corresponding entities in WordNet. The primary entities in WordNet are synonym sentences or synsets. Each synset is a group of words that represent a lexical concept. By treating these HYPERNYM relationships as undirected edges between synsets, we build a directed graph in which vertices are synsets and relationships are edges. The primary relationship is formed by the HYPERNYM relationship (Is-A). By treating these HYPERNYM relationships as undirected edges between synsets, we approach the semantic relationship between synsets as the length of the shortest path between two synsets in the graph. We add a common root node to the base of all synthesis trees, so that the synapse-graph relationship is the synthesis-graph-similarity function."}, {"heading": "2.2.2 EXISTING RELATIONAL OBJECTIVES", "text": "We know of two recent approaches from the knowledge base literature that, in addition to representing words (entities) with vector embedding, directly represent the relationships of a knowledge base as operations in the vector embedding space. These models both take as input a tuple (vl, R, vr) representing a possible relationship of type R between words vl and vr, and assign a score to the relationship space. TransE model by Bordes et al. (2013) represents relationships as translations in the vector embedding space. For two words vl and vr, if the relationship R holds, i.e. (vl, R, vr) the relationship between tuple + R and vr is applicable, then the corresponding embedding vl, vr-Rd should be close after translation by the relationship vector R-Rd. The score of a relationship level between tuple and vr is the similarity between the embedding model vr and vr."}, {"heading": "3 JOINT OBJECTIVE OPTIMIZATION BY ADMM", "text": "We aim to train a series of word embeddings that, together with the corresponding model parameters, must objectively map the two distribution modeling goals (Sec. 2.1) and one of the two relational modeling1As in the diagram distance. In each SGD iteration, a vocabulary (sl., sr) is scanned by WordNet so that synsets sl and sr words are included in the vocabulary. To produce the corrupt tuple, one of wl, wr, which is a substitute. (Sec. 2.2) We adopt the alternative method of multipliers (ADM)."}, {"heading": "4 DATA SETUP AND ANALYSIS", "text": "The distribution target LNLM is with 5-grams from the Google Books English corpus, distributed by UC Berkeley in Web 1T format2. This corpus contains more than 180 million different types and 31 billion 5-gram tokens. In our experiments, 5-grams per year are pre-processed to capture all terms. In each ADMM project, a block of 100,000 n-grams is evaluated according to their frequency. Each n-gram in the blocking is replaced with a token that has its own embedding vector space. In each ADMM project, a block of 100,000 n-grams is executed. Every n-gram in the blocking is evaluated. Every n-gram in the blocking is used by us."}, {"heading": "5 TASK-SPECIFIC EVALUATION", "text": "We now compare the learned embedding with different targets for three different tasks. For all experiments we use 50-dimensional embedding from iteration 1000 of the training, whereby \u03c1 = 0.05 applies to ADMM."}, {"heading": "5.1 KNOWLEDGE BASE COMPLETION", "text": "We use the methodology and datasets of Socher et al. (2013b) to evaluate the models \"ability to classify triplets as right or wrong. This relationship is useful for\" completing \"or\" expanding \"a knowledge base with new facts. The test set consists of correct relationship levels present in WordNet and incorrect tuples created from correct tuples by randomly switching entities. A development set is used to determine thresholds for each relationship that maximizes classification accuracy when tuples3On a 3.3Hz Xeon CPU, which took about 9 hours for ADMM, no more than the 7 hours for independent LNLM and 3 hours for independent LGD targets."}, {"heading": "5.2 ANALOGY TESTS FOR RELATIONAL SIMILARITY", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "5.3 DEPENDENCY PARSING", "text": "The goal is to see if the performance of a standard parser can be improved by simply using embedding as additional features. This can be seen as a kind of semi-supervised feature learning (Koo et al., 2008). Subsequently (Wu et al., 2013) we cluster the embedding with k-means first, then we integrate the cluster IDs as features. The reasoning is that discrete cluster IDs can be more easily integrated into existing parsers than conjunctions of features. We use the standard first-order MST parser7. For simplicity, we only try to integrate the embedding in k = 64 cluster IDs as features."}, {"heading": "6 CONCLUSIONS AND FUTURE WORK", "text": "Our first paper is an investigation of the ADMM algorithm, which flexibly combines multiple objectives. We show that ADMM converges quickly and is an effective method of combining multiple sources of information and linguistic intuitions in word representations. Note that other approaches to combining objectives besides ADMM, including direct gradient pedigree to the common goal, or concatenation of word representations that are individually optimized for independent objectives, are possible. Comparing different approaches to multi-objective optimization in word representations, where the optimization space is interspersed with local optimizations, is worthwhile as a future work.The second paper is a preliminary evaluation of three specific instances of ADMM, combining the NLM distribution objectives with graph distance, TransE, or NTN relation objectives."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by a Microsoft Research CORE Grant and JSPS KAKENHI Grant Number 26730121. D.F. was supported during this work by the Flinn Scholarship. We thank Haixun Wang, Jun'ichi Tsujii, Tim Baldwin, Yuji Matsumoto and several anonymous reviewers for helpful discussions at various stages of the project."}], "references": [{"title": "A neural probabilistic language models", "author": ["REFERENCES Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Boyd", "Stephen", "Parikh", "Neal", "Chu", "Eric", "Peleato", "Borja", "Eckstein", "Jonathan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Lexical Semantics", "author": ["Cruse", "Alan D"], "venue": null, "citeRegEx": "Cruse and D.,? \\Q1986\\E", "shortCiteRegEx": "Cruse and D.", "year": 1986}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester", "Scott", "Dumais", "Susan T", "Furnas", "George W", "Landauer", "Thomas K", "Harshman", "Richard"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Faruqui", "Manaal", "Dodge", "Jesse", "Jauhar", "Sujay Kumar", "Dyer", "Chris", "Hovy", "Eduard H", "Smith", "Noah A"], "venue": "CoRR, abs/1411.4166,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "In IJCAI,", "citeRegEx": "Gabrilovich and Markovitch,? \\Q2007\\E", "shortCiteRegEx": "Gabrilovich and Markovitch", "year": 2007}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Michael", "Hyv\u00e4rinen", "Aapo"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "Semeval-2012 task 2: Measuring degrees of relational similarity", "author": ["Jurgens", "David A", "Turney", "Peter D", "Mohammad", "Saif M", "Holyoak", "Keith J"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics,", "citeRegEx": "Jurgens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo", "Terry", "Carreras", "Xavier", "Collins", "Michael"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["Leacock", "Claudia", "Chodorow", "Martin"], "venue": "WordNet: An Electronic Lexical Database,", "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tom\u00e1s", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A lexical database for English", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Jeff", "Lapata", "Mirella"], "venue": "In ACL, pp", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["Mnih", "Andriy", "Kavukcuoglu", "Koray"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "Slav", "McDonald", "Ryan"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Continuous space language models", "author": ["Schwenk", "Holger"], "venue": "Computer Speech and Language,", "citeRegEx": "Schwenk and Holger.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk and Holger.", "year": 2007}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Smith", "Noah A", "Eisner", "Jason"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Socher", "Richard", "Bauer", "John", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: A simple and general method for semi-supervise learning", "author": ["Turian", "Joseph", "Ratinov", "Lev-Arie", "Bengio", "Yoshua"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Wang", "Mengqiu", "Manning", "Christopher D"], "venue": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Generalization of words for chinese dependency parsing", "author": ["Wu", "Xianchao", "Zhou", "Jie", "Sun", "Yu", "Liu", "Zhanyi", "Dianhai", "Hua", "Wang", "Haifeng"], "venue": "In Proceedings of the 13th International Conference on Parsing Technologies (IWPT),", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["Xu", "Chang", "Bai", "Yanlong", "Bian", "Jiang", "Gao", "Bin", "Wang", "Gang", "Liu", "Xiaoguang", "Tie-Yan"], "venue": "In Proceedings of the ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Mo", "Dredze", "Mark"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Combining heterogeneous models for measuring relational similarity", "author": ["Zhila", "Alisa", "Yih", "Wen-tau", "Meek", "Christopher", "Zweig", "Geoffrey", "Mikolov", "Tomas"], "venue": "In NAACL-HLT,", "citeRegEx": "Zhila et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhila et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 22, "context": ", 2013a) and improve the performance of various Natural Language Processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), parsing (Socher et al.", "startOffset": 118, "endOffset": 161}, {"referenceID": 3, "context": ", 2013a), and semantic role labeling (Collobert et al., 2011).", "startOffset": 37, "endOffset": 61}, {"referenceID": 0, "context": "\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.", "startOffset": 29, "endOffset": 138}, {"referenceID": 3, "context": "\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.", "startOffset": 29, "endOffset": 138}, {"referenceID": 2, "context": "We thus employ a general representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011) for jointly optimizing both distributional and relational objectives.", "startOffset": 123, "endOffset": 142}, {"referenceID": 3, "context": "A standard way to implement distributional semantics in representation learning is the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi \u2208 R, the word\u2019s embedding.", "startOffset": 118, "endOffset": 142}, {"referenceID": 1, "context": "The TransE model of Bordes et al. (2013) represents relationships as translations in the vector embedding space.", "startOffset": 20, "endOffset": 41}, {"referenceID": 2, "context": "We adopt the Alternating Direction Method of Multipliers (ADMM) approach (Boyd et al., 2011).", "startOffset": 73, "endOffset": 92}, {"referenceID": 2, "context": "Further, in accordance with previous works (Boyd et al., 2011), ADMM attains a reasonable objective value relatively quickly in a few iterations; our loss converges around 100 iterations.", "startOffset": 43, "endOffset": 62}, {"referenceID": 19, "context": "For comparison with the existing work of Socher et al. (2013b), we use their dataset, which contains training, development, and testing splits for 11 WordNet relationships (Table 1).", "startOffset": 41, "endOffset": 63}, {"referenceID": 20, "context": "We use the methodology and datasets of Socher et al. (2013b) to evaluate the models\u2019 ability to classify relationship triplets as correct or incorrect.", "startOffset": 39, "endOffset": 61}, {"referenceID": 9, "context": "SemEval-2012 Task 2 (Jurgens et al., 2012) is a relational similarity task similar to SAT-style analogy questions.", "startOffset": 20, "endOffset": 42}, {"referenceID": 9, "context": "SemEval-2012 Task 2 (Jurgens et al., 2012) is a relational similarity task similar to SAT-style analogy questions. The task is to score word pairs by the degree to which they belong in a relation class defined by a set of example word pairs. For example, the relation class REVERSE contains the example pairs (attack,defend) and (buy,sell). There are 69 testing relation categories, each with three or four example word pairs. In the evaluation, the model is shown a number of testing relation pairs in each category and scores each testing pair according to its similarity to the example relation pairs. These similarity scores are then compared to human similarity judgements. This is an useful task to test whether the positioning of learned embeddings in vector space leads to some meaningful semantics. Following Zhila et al. (2013), we evaluate the embeddings in this task on their ability to represent relations as translations in the vector space.", "startOffset": 21, "endOffset": 838}, {"referenceID": 9, "context": "(Jurgens et al., 2012).", "startOffset": 0, "endOffset": 22}, {"referenceID": 27, "context": "NLM achieves scores competitive with the recurrent neural language models used in (Zhila et al., 2013), which had a maximum accuracy of 0.", "startOffset": 82, "endOffset": 102}, {"referenceID": 10, "context": "This can be seen as a kind of semisupervised feature learning (Koo et al., 2008).", "startOffset": 62, "endOffset": 80}, {"referenceID": 24, "context": "Following (Wu et al., 2013), we first cluster the embeddings using k-means, then incorporate the cluster ids as features.", "startOffset": 10, "endOffset": 27}, {"referenceID": 25, "context": "To the best of our knowledge, some recent work (Xu et al., 2014; Yu & Dredze, 2014; Faruqui et al., 2014) explored similar motivations as ours.", "startOffset": 47, "endOffset": 105}, {"referenceID": 6, "context": "To the best of our knowledge, some recent work (Xu et al., 2014; Yu & Dredze, 2014; Faruqui et al., 2014) explored similar motivations as ours.", "startOffset": 47, "endOffset": 105}, {"referenceID": 25, "context": "gradient descent directly on the joint objective is used in (Xu et al., 2014; Yu & Dredze, 2014), while Faruqui et al.", "startOffset": 60, "endOffset": 96}, {"referenceID": 5, "context": "LSA, (Deerwester et al., 1990), ESA (Gabrilovich & Markovitch, 2007), SDS (Mitchell & Lapata, 2008)), the promise of recent learning-based approaches is that they enable a flexible definition of optimization objectives.", "startOffset": 5, "endOffset": 30}, {"referenceID": 5, "context": ", 2014; Yu & Dredze, 2014; Faruqui et al., 2014) explored similar motivations as ours. The main differences are in their optimization methods (i.e. gradient descent directly on the joint objective is used in (Xu et al., 2014; Yu & Dredze, 2014), while Faruqui et al. (2014) introduces a post-processing graph-based method) as well as alternate relational objectives, e.", "startOffset": 27, "endOffset": 274}, {"referenceID": 5, "context": ", 2014; Yu & Dredze, 2014; Faruqui et al., 2014) explored similar motivations as ours. The main differences are in their optimization methods (i.e. gradient descent directly on the joint objective is used in (Xu et al., 2014; Yu & Dredze, 2014), while Faruqui et al. (2014) introduces a post-processing graph-based method) as well as alternate relational objectives, e.g. Yu & Dredze (2014) formulate a skip-gram objective where word embeddings are trained to predict other words they share relations with.", "startOffset": 27, "endOffset": 391}], "year": 2015, "abstractText": "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.", "creator": "LaTeX with hyperref package"}}}