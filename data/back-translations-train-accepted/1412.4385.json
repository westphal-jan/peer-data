{"id": "1412.4385", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2014", "title": "Unsupervised Domain Adaptation with Feature Embeddings", "abstract": "Representation learning is the dominant technique for unsupervised domain adaptation, but existing approaches often require the specification of \"pivot features\" that generalize across domains, which are selected by task-specific heuristics. We show that a novel but simple feature embedding approach provides better performance, by exploiting the feature template structure common in NLP problems.", "histories": [["v1", "Sun, 14 Dec 2014 17:44:58 GMT  (145kb,D)", "http://arxiv.org/abs/1412.4385v1", null], ["v2", "Mon, 30 Mar 2015 19:35:12 GMT  (139kb,D)", "http://arxiv.org/abs/1412.4385v2", "For more details, please refer to the long version of this paper:this http URL"], ["v3", "Thu, 16 Apr 2015 01:44:48 GMT  (139kb,D)", "http://arxiv.org/abs/1412.4385v3", "For more details, please refer to the long version of this paper:this http URL"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yi yang", "jacob eisenstein"], "accepted": true, "id": "1412.4385"}, "pdf": {"name": "1412.4385.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yi Yang"], "emails": ["jacobe}@gatech.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Domain customization is critical if natural language processing is to be used successfully in high-impact applications such as social media, medical records, and historical texts. Unattended domain customization is particularly attractive because it does not require labeled data in the target domain. Some of the most successful approaches to unattended domain customization are based on representation learning: converting sparse high-dimensional surface features into dense vector representations, which are often more resilient to domain shifts (Blitzer et al., 2006; Glorot et al., 2011). However, these methods are computationally expensive (sometimes with thousands of dense features) and often require specific task-specific heuristics to select good \"pivot features.\" We present FEMA (Feature EMbeddings for Domain Adaptation), a novel approach to learning how to display domain customizations in structured feature spaces."}, {"heading": "2 LEARNING FEATURE EMBEDDINGS", "text": "We avoid this division directly through the appearance of information that we can use for the representation and downstream processing of information."}, {"heading": "3 EXPERIMENTS", "text": "We evaluate FEMA based on Part-of-Speech (POS): Adapting the English POS marker from message text to web text, as with the SANCL Shared Task (Petrov & McDonald, 2012)."}, {"heading": "3.1 EXPERIMENT SETUP", "text": "We use data from the SANCL collaborative task (Petrov & McDonald, 2012), which includes several web-related corporations (newsgroups, reviews, weblogs, replies, e-mails), as well as the WSJ portion of OntoNotes corpus (Hovy et al., 2006). Following Schnabel & Scheutze (2014), we use Sections 02-21 of WSJ for training and Section 22 for development, and use 100,000 unlabeled WSJ sentences from 1988 for learning representations. On the Web page, each of the five target domains has an unlabeled training set of 100,000 sentences, along with development and testing kits of about 1,000 labeled sentences each.SVM tagger While POS tagging is treated classically as a structured prediction problem, we follow Schnabel & Scheutze (2014) by applying a classification-based approach."}, {"heading": "3.2 RESULTS", "text": "As shown in Tables 1 and 2, FEMA performs better in all target areas than other systems, with the exception of REVIEW, where FLORS performs slightly better. FLORS uses more basic features than FEMA; these features could in principle be combined with feature embeddings to achieve better performance. Compared to other representation learning approaches, FEMA is on average about 1% better, representing a 10% error reduction. Training time is about 70 minutes on a 24-core machine, using a genesis-based implementation.2 This is slightly faster than SCL, though slower than mDA with structured failure noise."}, {"heading": "4 RELATED WORK", "text": "Visual learning approaches to domain customization target cross-domain representations that were initially triggered by additional prediction problems (Ando & Zhang, 2005), such as 1https: / / code.google.com / p / word2vec / 2http: / / radimrehurek.com / gensim / prediction of pivot features (Blitzer et al., 2006). In these approaches, as in later work on the denocialization of autoencoders (Chen et al., 2012), the key mechanism is to learn a function for predicting a subset of characteristics for each instance based on different characteristics of the instance. Word embedding can be considered a special case of visualization learning, where the goal is to learn representations for each word and then provide them instead of lexical characteristics (Turian et al., 2010)."}, {"heading": "5 CONCLUSION", "text": "Feature embedings can be used for domain customization for all problems with feature templates. They offer strong performance, avoid the practical disadvantages of alternative learning approaches in the field of representation, and are easy to learn with existing Word embedding methods."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Rie Kubota", "Zhang", "Tong"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer", "John", "McDonald", "Ryan", "Pereira", "Fernando"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Chen", "Minmin", "Z. Xu", "Weinberger", "Killian", "Sha", "Fei"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Ontonotes: the 90% solution", "author": ["Hovy", "Eduard", "Marcus", "Mitchell", "Palmer", "Martha", "Ramshaw", "Lance", "Weischedel", "Ralph"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In Proceedings of International Conference on Learning Representations,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "Slav", "McDonald", "Ryan"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["Ratnaparkhi", "Adwait"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Ratnaparkhi and Adwait,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi and Adwait", "year": 1996}, {"title": "Flors: Fast and simple domain adaptation for part-of-speech tagging", "author": ["Schnabel", "Tobias", "Sch\u00fctze", "Hinrich"], "venue": "Transactions of the Association of Computational Linguistics,", "citeRegEx": "Schnabel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2014}, {"title": "Linguistic structure prediction", "author": ["Smith", "Noah A"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "Smith and A.,? \\Q2011\\E", "shortCiteRegEx": "Smith and A.", "year": 2011}, {"title": "Word Representation: A Simple and General Method for Semi-Supervised Learning", "author": ["Turian", "Joseph", "Ratinov", "Lev", "Bengio", "Yoshua"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Under review as a workshop contribution at ICLR", "author": ["Yi", "Eisenstein", "Jacob"], "venue": null, "citeRegEx": "Yi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Some of the most successful approaches to unsupervised domain adaptation are based on representation learning: transforming sparse high-dimensional surface features into dense vector representations, which are often more robust to domain shift (Blitzer et al., 2006; Glorot et al., 2011).", "startOffset": 244, "endOffset": 287}, {"referenceID": 4, "context": "Some of the most successful approaches to unsupervised domain adaptation are based on representation learning: transforming sparse high-dimensional surface features into dense vector representations, which are often more robust to domain shift (Blitzer et al., 2006; Glorot et al., 2011).", "startOffset": 244, "endOffset": 287}, {"referenceID": 2, "context": "For example, both Structural Correspondence Learning (SCL; Blitzer et al., 2006) and Denoising Autoencoders (Chen et al.", "startOffset": 53, "endOffset": 80}, {"referenceID": 3, "context": ", 2006) and Denoising Autoencoders (Chen et al., 2012) learn to reconstruct a subset of \u201cpivot features\u201d, as shown in Figure 1(a).", "startOffset": 35, "endOffset": 54}, {"referenceID": 6, "context": "Our feature embeddings are based on the skip-gram model, trained with negative sampling (SGNS; Mikolov et al., 2013), which is a simple yet efficient method for learning word embeddings.", "startOffset": 88, "endOffset": 116}, {"referenceID": 1, "context": "Finally, since it has been shown that nonlinearity is important for generating robust representations (Bengio et al., 2013), we follow (Chen et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 3, "context": ", 2013), we follow (Chen et al., 2012) and apply the hyperbolic tangent function to the embeddings.", "startOffset": 19, "endOffset": 38}, {"referenceID": 5, "context": "Datasets We use data from the SANCL shared task (Petrov & McDonald, 2012), which contains several web-related corpora (newsgroups, reviews, weblogs, answers, emails) as well as the WSJ portion of OntoNotes corpus (Hovy et al., 2006).", "startOffset": 213, "endOffset": 232}, {"referenceID": 5, "context": "Datasets We use data from the SANCL shared task (Petrov & McDonald, 2012), which contains several web-related corpora (newsgroups, reviews, weblogs, answers, emails) as well as the WSJ portion of OntoNotes corpus (Hovy et al., 2006). Following Schnabel & Sch\u00fctze (2014), we use sections 02-21 of WSJ for training and section 22 for development, and use 100,000 unlabeled WSJ sentences from 1988 for learning representations.", "startOffset": 214, "endOffset": 270}, {"referenceID": 2, "context": "Competitive systems We consider two competitive unsupervised domain adaptation methods that require pivot features: Structural Correspondence Learning (SCL; Blitzer et al., 2006) and marginalized Denoising Autoencoders (mDA; Chen et al, 2012).", "startOffset": 151, "endOffset": 178}, {"referenceID": 2, "context": "Following Blitzer et al. (2006), we consider 6918 pivot features that appear more than 50 times in all the domains for SCL and mDA.", "startOffset": 10, "endOffset": 32}, {"referenceID": 2, "context": "Following Blitzer et al. (2006), we consider 6918 pivot features that appear more than 50 times in all the domains for SCL and mDA. The best parameters for SCL are dimensionality K = 50 and rescale factor \u03b1 = 5. For both FEMA and word2vec, the best embedding size is 100 and the best number of negative samples is 5. The noise distribution P (n) t is simply the unigram probability of each feature in the template t. Mikolov et al. (2013b) argue for exponentiating the unigram distribution, but we find it makes little difference here.", "startOffset": 10, "endOffset": 440}, {"referenceID": 2, "context": "prediction of pivot features (Blitzer et al., 2006).", "startOffset": 29, "endOffset": 51}, {"referenceID": 3, "context": "In these approaches, as well as in later work on denoising autoencoders (Chen et al., 2012), the key mechanism is to learn a function to predict a subset of features for each instance, based on other features of the instance.", "startOffset": 72, "endOffset": 91}, {"referenceID": 12, "context": "Word embeddings can be viewed as special case of representation learning, where the goal is to learn representations for each word, and then to supply these representations in place of lexical features (Turian et al., 2010).", "startOffset": 202, "endOffset": 223}], "year": 2017, "abstractText": "Representation learning is the dominant technique for unsupervised domain adaptation, but existing approaches often require the specification of \u201cpivot features\u201d that generalize across domains, which are selected by task-specific heuristics. We show that a novel but simple feature embedding approach provides better performance, by exploiting the feature template structure common in NLP problems.", "creator": "LaTeX with hyperref package"}}}