{"id": "1412.4659", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2014", "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions", "abstract": "We consider the problem of recovering the sparsest vector in a subspace $\\mathcal{S} \\subseteq \\mathbb{R}^p$ with $\\mathrm{dim}(\\mathcal{S}) = n &lt; p$. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $1/\\sqrt{n}$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $\\Omega(1)$. To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.", "histories": [["v1", "Mon, 15 Dec 2014 16:27:29 GMT  (787kb,D)", "http://arxiv.org/abs/1412.4659v1", "38 pages, 4 figures. Extended abstract appears in Advances in Neural Information Processing Systems (NIPS), 2014"], ["v2", "Tue, 24 Nov 2015 03:23:33 GMT  (390kb,D)", "http://arxiv.org/abs/1412.4659v2", "Submitted to IEEE Trans. Information Theory"], ["v3", "Wed, 20 Jul 2016 00:54:41 GMT  (750kb,D)", "http://arxiv.org/abs/1412.4659v3", "Accepted by IEEE Trans. Information Theory. The paper has been revised by the reviewers' comments. The proofs have been streamlined"]], "COMMENTS": "38 pages, 4 figures. Extended abstract appears in Advances in Neural Information Processing Systems (NIPS), 2014", "reviews": [], "SUBJECTS": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "authors": ["qing qu", "ju sun", "john wright"], "accepted": true, "id": "1412.4659"}, "pdf": {"name": "1412.4659.pdf", "metadata": {"source": "CRF", "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions", "authors": ["Qing Qu", "Ju Sun"], "emails": ["jw2966}@columbia.edu"], "sections": [{"heading": null, "text": "\u221a n. To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result presupposes a planted sparse model in which the sparse target vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in developing more sophisticated data models resulting, for example, from sparse dictionary learning."}, {"heading": "1 Introduction", "text": "Suppose we have a linear model of learning [SWW12], a high-dimensional model of learning [BM05] and the prony problem [BM05], a frugal PCA [ZHT06], a blind source of learning [SWW12], a high-quality model of learning [SWW12], a spectral estimate and prony problem [BM05], a frugal PCA [ZHT06], a blind dividing line [SWW12]."}, {"heading": "2 Problem Formulation and Global Optimality", "text": "We study the problem of restoring a sparse vector x0 6 = 0 (up to this point), which is an element of a known sub-space S-Rp of dimension n, provided there is an arbitrary orthonormal base Y-Rp \u00b7 n for S. Our starting point is the non-convex formulation (1,2). Both the objective and the constraint are not constant and therefore not easy to optimize. We relax (1,2) by replacing the \"0 norm with the\" 1 norm. For the constraint x 6 = 0, which is necessary to avoid a trivial solution, we force x to live on the unit."}, {"heading": "3 Algorithm based on Alternating Direction Method (ADM)", "text": "In order to develop an algorithm for the solution (2,2), it is useful to consider a slight relaxation of (2,2) q (2,2) in which we introduce an auxiliary variable x \u00b2 (1), although this version is much stronger and more practicable than appears in the conference version [QSW14]. Here, it is not difficult to see that this problem is equivalent to the minimization of the HuberM estimator via Yq. This relaxation makes it possible to apply the alternative direction method to this problem, starting from a starting point q (0), alternatively between optimization with respect to x and optimization with respect to q (k + 1) = arg min x12."}, {"heading": "4 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Main Results", "text": "In this section we describe our main theoretical result, which shows that the algorithm described in the previous section has a high probability of success.Theorem 4.1. Suppose S satisfies the planted sparse model and leaves the columns of Y an arbitrary orthonormal basis for the subspace S. Let us call y1,..., yp, yp, yp, q1,.., qp. Solve the linear program (3,6) with r = q1,., qp, around q = 1,.., q, p. Set i? arg, y1,., yp, around the results q1,."}, {"heading": "4.2 A Sketch of Analysis", "text": "The proof of our main result requires a fairly detailed technical analysis of the iteration-by-iteration properties of Y. In this section, as illustrated in Fig. 1, we can assume that we are working without loss of generality on the respective base Y = ZR defined in this section. To further streamline the presentation, we will outline the proof under the assumption that the proof under the assumption that Y = [x0 | g1] is correct, instead of the orthogonized version Y. If p is large, Y is already almost orthogonal, and hence Y is very close to Y. In fact, in our evidence we simply perform the argument for Y and then note that all the steps of the proof are still using Y."}, {"heading": "5 Experimental Results", "text": "In this section, we will show the performance of the proposed ADM algorithm on both synthetic and real data sets. On the synthetic data set, we will show the phase transition of our algorithm on both sparse vector and dictionary learning models; for the real data set, we will show how searching for sparse vectors can help discover interesting patterns."}, {"heading": "5.1 Phase Transition on Synthetic Data", "text": "For the planted sparse model, we generate for each pair of (k, p) the n > dimensional subspace pair S > Rp by a k sparse vector x0 with unequal entries equal to 1 and a random Gaussian matrix G > Rp \u00b7 (n \u2212 1) with Gij-Schmidt orthonormalization operator and U Rn \u00b7 n is an arbitrary orthogonal matrix. We fix the relationship between n and p as p = 5n log n, and set the regulation parameter to (3,1). We use all normalized rows of Y as initializations of q for the proposed AD algorithm, and run each program for 5000 iterations."}, {"heading": "5.2 Exploratory Experiments on Faces", "text": "It is well known that the appearance of convex objects only leads to an image capture that can be easily approximated by the low-dimensional space in the raw pixel space [BJ03]. We will play with face subspaces here. First, we extract face images of a person (65 images) under different lighting conditions. We apply the ADM algorithm to find the sparsest element in such a subspace by randomly selecting 10% rows as initializations for q. We judge conciseness in a '1 /' 2 sense, that is, the sparsest vector x-0 = Yq? we should find the smallest element in such subspace by selecting 10% rows as initializations for q."}, {"heading": "6 Discussion", "text": "Furthermore, we believe that the paradigm of the algorithm goes far beyond idealized models, as our preliminary experiments with facial data have clearly shown. In the case of the respective sparse model, the performance gap in terms of (p, n, \u03b8) between the empirical simulation and our result can probably be attributed to the analysis itself. Advanced techniques for binding the empirical process, such as decoupling [DlPG99] techniques, can be used instead of our raw material union, which will cover all iterates. On the application side, despite the cases mentioned above, the potential of finding sparse / structured elements in a subspace seems largely unexplored. We hope that this work can invite more application ideas. This paper is part of a recent wave of research on verifiable and practical non-convective approaches to estimating various types of low-dimensional structures, often in large-scale problems."}, {"heading": "Acknowledgement", "text": "JS would like to thank the Wei Family Private Foundation for their generous support. We thank Cun Mu, IEOR Department of Columbia University, for helpful discussions and suggestions regarding this work. This work has been supported in part by grants ONR N00014-13-1-0492, NSF 1343282 and grants from the Moore and Sloan Foundations."}, {"heading": "A Technical Tools and Preliminaries", "text": "s (x) and (x) the probability density function (pdf) and the cumulative distribution function (cdf) for the standard normal distribution: (standard normal pdf), (A.2), (A.2), (A.2), (A.2), (A.2), (A.2), (A.2), (A.2), (A.2), (x), (A.2), (A.2), (A.2), (A.2), (A.2), (A.2), (Pdf f3), (x), (x), (x), (x), (x)."}, {"heading": "B The Random Basis vs. Its Orthonormalized Version", "text": "We view Y as a model that translates the results for Y into quantitative statements. (*) We view Y as a model in which we transform the results for Y into quantitative statements. (*) We view Y as a model in which we transform the results for Y into quantitative statements. (*) We view Y as a model that we transform into quantitative statements about Y into quantitative statements. (*) We view Y as a model that we transform into quantitative statements about Y into quantitative statements. (*) We view Y as a model that we transform into quantitative statements about Y into quantitative statements. (*) We view Y as a model that we transform into quantitative statements about Y into quantitative statements. (*) We view Y as a model that we transform into quantitative statements about Y into quantitative statements. (*) We view Y as a model that we transform into quantitative statements."}, {"heading": "D Proof of Main Result", "text": "Rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally rally"}, {"heading": "E Good Initialization", "text": "Proposition E.1. Let y \"k\" for k = 1,., p \"be the transpose of the rows of the orthonormal bases Y\" defined in (B.3). If we use \"n\" and \"exp\" (n) for any constant C > 0, then it is true that at least one of our p \"initialization vectors is proposed in Section 3, say\" c \"n\" for some positive constants c \"and c.\" Proof. Since x0 is i.i.d. Bernoulli, with a probability of at least 1. \"(1), (E.1), p\" exp, \"exp\" n \"for some positive constants c\" and c. \"Proof."}, {"heading": "F Lower Bounding Finite Sample Gap G\u2032(q)", "text": "(1) (1) (2) (2) (2) (2) (2) (2) (2) (2) () () () () () () (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) () () () () () () () () (2) () () () () () (2) (2) (2) (2) (2) () () () () () () () () () () () () () (2) (2) () () () () () ()) () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () ()) () () () ()) () () () () ()) () ()) () ()) () () ()) () ()) () ()) () ()) () () ()) () () ())) () ()) () () () ()) () ()) () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () () (() () () (() () () () () () () () () () () (((() () () (() () () () () () () (() (() () () () () () ("}, {"heading": "H Bounding Iteration Complexity", "text": "Proposition H.1. There is a constant stating that the ADM algorithm in algorithm 1, with a probability of at least 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11"}, {"heading": "I Rounding to the Desired Solution", "text": "For convenience, we proceed from the notes we used in Appendix B. Then, the rounding scheme can be led to the desired solution with the same probability, regardless of the respective orthonormal base. Proposition I.1. Assume that the input base is defined in (B.3) and the ADM algorithm q-1 with q1 > 2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"\" q2 \"\" q2 \"\" q2 \"\" q2 \"q2\" q2 \"\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" and \"q2\" q2 \"q2\" q1 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"and\" q2 \"q2\" q1 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2\" q2 \"q2"}], "references": [{"title": "Learning sparsely used overcomplete dictionaries via alternating minimization", "author": ["Alekh Agarwal", "Animashree Anandkumar", "Prateek Jain", "Praneeth Netrapalli", "Rashish Tandon"], "venue": "arXiv preprint arXiv:1310.7991,", "citeRegEx": "AAJ13", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact recovery of sparsely used overcomplete dictionaries", "author": ["Alekh Agarwal", "Animashree Anandkumar", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1309.1952,", "citeRegEx": "AAN13", "shortCiteRegEx": null, "year": 2013}, {"title": "More algorithms for provable dictionary learning", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1401.0579,", "citeRegEx": "ABGM14", "shortCiteRegEx": null, "year": 2014}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "arXiv preprint arXiv:1308.6273,", "citeRegEx": "AGM13", "shortCiteRegEx": null, "year": 2013}, {"title": "When are overcomplete topic models identifiable? uniqueness of tensor tucker decompositions with structured sparsity", "author": ["AnimaAnandkumar", "Daniel Hsu", "Majid Janzamin", "ShamMKakade"], "venue": "Advances in Neural Information Processing Systems, pages 1986\u20131994,", "citeRegEx": "AHJK13", "shortCiteRegEx": null, "year": 2013}, {"title": "IEEE Transactions on", "author": ["Ronen Basri", "David W Jacobs. Lambertian reflectance", "linear subspaces. Pattern Analysis", "Machine Intelligence"], "venue": "25(2):218\u2013233,", "citeRegEx": "BJ03", "shortCiteRegEx": null, "year": 2003}, {"title": "Rounding sum-of-squares relaxations", "author": ["Boaz Barak", "Jonathan Kelner", "David Steurer"], "venue": "arXiv preprint arXiv:1312.6652,", "citeRegEx": "BKS13", "shortCiteRegEx": null, "year": 2013}, {"title": "Applied and Computational Harmonic Analysis", "author": ["Gregory Beylkin", "Lucas Monz\u00f3n. On approximation of functions by exponential sums"], "venue": "19(1):17\u201348,", "citeRegEx": "BM05", "shortCiteRegEx": null, "year": 2005}, {"title": "In Conference on Learning Theory", "author": ["Quentin Berthet", "Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection"], "venue": "pages 1046\u20131066,", "citeRegEx": "BR13", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust principal component analysis? Journal of the ACM", "author": ["Emmanuel Cand\u00e8s", "Xiaodong Li", "Yi Ma", "JohnWright"], "venue": "58(3), May", "citeRegEx": "CLMW11", "shortCiteRegEx": null, "year": 2011}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "arXiv preprint arXiv:1407.1065,", "citeRegEx": "CLS14", "shortCiteRegEx": null, "year": 2014}, {"title": "SIAM Journal on Algebraic Discrete Methods", "author": ["Thomas F Coleman", "Alex Pothen. The null space problem i. complexity"], "venue": "7(4):527\u2013537,", "citeRegEx": "CP86", "shortCiteRegEx": null, "year": 1986}, {"title": "IEEE Transactions on", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao. Decoding by linear programming. Information Theory"], "venue": "51(12):4203\u20134215,", "citeRegEx": "CT05", "shortCiteRegEx": null, "year": 2005}, {"title": "In Computer Vision and Pattern Recognition (CVPR)", "author": ["Yuchao Dai", "Hongdong Li", "Mingyi He. A simple prior-free method for non-rigid structurefrom-motion factorization"], "venue": "2012 IEEE Conference on, pages 2018\u20132025. IEEE,", "citeRegEx": "DLH12", "shortCiteRegEx": null, "year": 2012}, {"title": "Decoupling: from dependence to independence", "author": ["Victor De la Pena", "Evarist Gin\u00e9"], "venue": "Springer,", "citeRegEx": "DlPG99", "shortCiteRegEx": null, "year": 1999}, {"title": "Communications on pure and applied mathematics", "author": ["David L Donoho. For most large underdetermined systems of linear equations the minimal `-norm solution is also the sparsest solution"], "venue": "59(6):797\u2013829,", "citeRegEx": "Don06", "shortCiteRegEx": null, "year": 2006}, {"title": "Acta Mathematica", "author": ["Tadeusz Figiel", "Joram Lindenstrauss", "Vitali D Milman. The dimension of almost spherical sections of convex bodies"], "venue": "139(1):53\u201394,", "citeRegEx": "FLM77", "shortCiteRegEx": null, "year": 1977}, {"title": "volume 277", "author": ["Andrej Y Garnaev", "Efim D Gluskin. The widths of a euclidean ball. In Dokl. Akad. Nauk SSSR"], "venue": "pages 1048\u20131052,", "citeRegEx": "GG84", "shortCiteRegEx": null, "year": 1984}, {"title": "pages 131\u2013135", "author": ["E Gluskin", "VMilman. Note on the geometric-arithmetic mean inequality. In Geometric aspects of Functional analysis"], "venue": "Springer,", "citeRegEx": "GM03", "shortCiteRegEx": null, "year": 2003}, {"title": "On the provable convergence of alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "arXiv preprint arXiv:1312.0925,", "citeRegEx": "Har13", "shortCiteRegEx": null, "year": 2013}, {"title": "Recovering the sparsest element in a subspace", "author": ["Paul Hand", "Laurent Demanet"], "venue": "arXiv preprint arXiv:1310.1654,", "citeRegEx": "HD13", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of The 30th International Conference on Machine Learning", "author": ["Jeffrey Ho", "Yuchen Xie", "Baba Vemuri. On a nonlinear generalization of sparse coding", "dictionary learning"], "venue": "pages 1480\u20131488,", "citeRegEx": "HXV13", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 45th annual ACM symposium on Symposium on theory of computing", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi. Low-rank matrix completion using alternating minimization"], "venue": "pages 665\u2013674. ACM,", "citeRegEx": "JNS13", "shortCiteRegEx": null, "year": 2013}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi. Phase retrieval using alternating minimization"], "venue": "pages 2796\u20132804,", "citeRegEx": "NJS13", "shortCiteRegEx": null, "year": 2013}, {"title": "volume 94", "author": ["Gilles Pisier. The volume of convex bodies", "Banach space geometry"], "venue": "Cambridge University Press,", "citeRegEx": "Pis99", "shortCiteRegEx": null, "year": 1999}, {"title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions", "author": ["Qing Qu", "Ju Sun", "John Wright"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "QSW14", "shortCiteRegEx": null, "year": 2014}, {"title": "Complete dictionary learning over the sphere", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "preparation,", "citeRegEx": "SQW14", "shortCiteRegEx": null, "year": 2014}, {"title": "Exact recovery of sparsely-used dictionaries", "author": ["Daniel A Spielman", "Huan Wang", "John Wright"], "venue": "Proceedings of the 25th Annual Conference on Learning Theory,", "citeRegEx": "SWW12", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Ver10", "shortCiteRegEx": null, "year": 2010}, {"title": "Alternating minimization for mixed linear regression", "author": ["Xinyang Yi", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "arXiv preprint arXiv:1310.3745,", "citeRegEx": "YCS13", "shortCiteRegEx": null, "year": 2013}, {"title": "Applied Mathematics and Computation", "author": ["Yun-Bin Zhao", "Masao Fukushima. Rank-one solutions for homogeneous linear matrix equations over the positive semidefinite cone"], "venue": "219(10):5569\u2013 5583,", "citeRegEx": "ZF13", "shortCiteRegEx": null, "year": 2013}, {"title": "Journal of computational and graphical statistics", "author": ["Hui Zou", "Trevor Hastie", "Robert Tibshirani. Sparse principal component analysis"], "venue": "15(2):265\u2013286,", "citeRegEx": "ZHT06", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural computation", "author": ["Michael Zibulevsky", "Barak A Pearlmutter. Blind source separation by sparse decomposition in a signal dictionary"], "venue": "13(4):863\u2013882,", "citeRegEx": "ZP01", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 105, "endOffset": 111}, {"referenceID": 30, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 146, "endOffset": 152}, {"referenceID": 13, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 185, "endOffset": 192}, {"referenceID": 7, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 234, "endOffset": 240}, {"referenceID": 31, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 253, "endOffset": 260}, {"referenceID": 32, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 286, "endOffset": 292}, {"referenceID": 27, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 314, "endOffset": 321}, {"referenceID": 4, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 348, "endOffset": 356}, {"referenceID": 21, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 389, "endOffset": 396}, {"referenceID": 11, "context": "2) is NP-hard [CP86].", "startOffset": 14, "endOffset": 20}, {"referenceID": 27, "context": "[SWW12] introduced a relaxation which replaces the nonconvex problem (1.", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "Method Recovery Condition Total Complexity `1/`\u221e Relaxation[HD13] \u03b8 \u2208 O(1/\u221an) O(np) SDP Relaxation \u03b8 \u2208 O(1/\u221an) O(p) SOS Relaxation [BKS13] p \u2265 \u03a9(n), \u03b8 \u2208 O(1) high order poly(p) This work p \u2265 \u03a9(n log n), \u03b8 \u2208 O(1) O(np log n)", "startOffset": 59, "endOffset": 65}, {"referenceID": 6, "context": "Method Recovery Condition Total Complexity `1/`\u221e Relaxation[HD13] \u03b8 \u2208 O(1/\u221an) O(np) SDP Relaxation \u03b8 \u2208 O(1/\u221an) O(p) SOS Relaxation [BKS13] p \u2265 \u03a9(n), \u03b8 \u2208 O(1) high order poly(p) This work p \u2265 \u03a9(n log n), \u03b8 \u2208 O(1) O(np log n)", "startOffset": 131, "endOffset": 138}, {"referenceID": 20, "context": "3) also correctly recovers x0, provided the fraction of nonzeros in x0 scales as \u03b8 \u2208 O (1/\u221an) [HD13].", "startOffset": 94, "endOffset": 100}, {"referenceID": 31, "context": "4) and sparse PCA [ZHT06].", "startOffset": 18, "endOffset": 25}, {"referenceID": 8, "context": "In sparse PCA, there is a substantial gap between what can be achieved with efficient algorithms and the information theoretic optimum [BR13].", "startOffset": 135, "endOffset": 141}, {"referenceID": 6, "context": "introduced a new rounding technique for sum-ofsquares relaxations, and showed that the sparse vector x0 in the planted sparse model can be recovered when p \u2265 \u03a9 ( n ) and \u03b8 = \u03a9(1) [BKS13].", "startOffset": 179, "endOffset": 186}, {"referenceID": 27, "context": "Second, our theoretical results require a second, linear programming based rounding phase, which is similar to [SWW12].", "startOffset": 111, "endOffset": 118}, {"referenceID": 26, "context": "2 1This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (withb 6= 0), inwhich it is possible to handle very large fractions of nonzeros (say, \u03b8 = \u03a9(1/ logn), or even \u03b8 = \u03a9(1)) using a very simple `1 relaxation [CT05, Don06] 2In work currently in preparation [SQW14], we show that in the dictionary learning problem, efficient algorithms based on nonconvex", "startOffset": 301, "endOffset": 308}, {"referenceID": 32, "context": ", the work of [ZP01] in blind source separation for precedent.", "startOffset": 14, "endOffset": 20}, {"referenceID": 25, "context": "3Note that this version is much stronger and more practical than that appearing in the conference version [QSW14].", "startOffset": 106, "endOffset": 113}, {"referenceID": 28, "context": "5This is the common heuristic that \u201ctall random matrices are well conditioned\u201d [Ver10].", "startOffset": 79, "endOffset": 86}, {"referenceID": 6, "context": "\u2019s result [BKS13] in sampling complexity.", "startOffset": 10, "endOffset": 17}, {"referenceID": 27, "context": "Second, we consider the same dictionary learning model as in [SWW12].", "startOffset": 61, "endOffset": 68}, {"referenceID": 5, "context": "2 Exploratory Experiments on Faces It is well known in computer vision that appearance of convex objects only subject to illumination changes leads to image collection that can be well approximated by low-dimensional space in raw-pixel space [BJ03].", "startOffset": 242, "endOffset": 248}, {"referenceID": 9, "context": "Thenwe apply robust principal component analysis [CLMW11] to the data and get a low dimensional subspace of dimension 10, i.", "startOffset": 49, "endOffset": 57}, {"referenceID": 14, "context": "Advanced techniques to bound the empirical process, such as decoupling [DlPG99] techniques, can be deployed in place of our crude union bound to cover all iterates.", "startOffset": 71, "endOffset": 79}, {"referenceID": 26, "context": "Our forthcoming work [SQW14] on dictionary learning takes a more geometric approach, and proves global recovery via efficient algorithms, with arbitrary initialization.", "startOffset": 21, "endOffset": 28}, {"referenceID": 0, "context": "References [AAJ13] Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon.", "startOffset": 11, "endOffset": 18}, {"referenceID": 1, "context": "[AAN13] Alekh Agarwal, Animashree Anandkumar, and Praneeth Netrapalli.", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "[ABGM14] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.", "startOffset": 0, "endOffset": 8}, {"referenceID": 3, "context": "[AGM13] Sanjeev Arora, Rong Ge, and Ankur Moitra.", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "[AHJK13] AnimaAnandkumar, Daniel Hsu, Majid Janzamin, and ShamMKakade.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "[BJ03] Ronen Basri and David W Jacobs.", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[BKS13] Boaz Barak, Jonathan Kelner, and David Steurer.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[BM05] Gregory Beylkin and Lucas Monz\u00f3n.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[BR13] Quentin Berthet and Philippe Rigollet.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "[CLMW11] Emmanuel Cand\u00e8s, Xiaodong Li, Yi Ma, and JohnWright.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[CLS14] Emmanuel J.", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[CP86] Thomas F Coleman and Alex Pothen.", "startOffset": 0, "endOffset": 6}, {"referenceID": 12, "context": "[CT05] Emmanuel J Cand\u00e8s and Terence Tao.", "startOffset": 0, "endOffset": 6}, {"referenceID": 13, "context": "[DLH12] Yuchao Dai, Hongdong Li, and Mingyi He.", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "[DlPG99] Victor De la Pena and Evarist Gin\u00e9.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[Don06] David L Donoho.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[FLM77] Tadeusz Figiel, Joram Lindenstrauss, and Vitali D Milman.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[GG84] Andrej Y Garnaev and Efim D Gluskin.", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "[GM03] E Gluskin and VMilman.", "startOffset": 0, "endOffset": 6}, {"referenceID": 19, "context": "[Har13] Moritz Hardt.", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "[HD13] Paul Hand and Laurent Demanet.", "startOffset": 0, "endOffset": 6}, {"referenceID": 21, "context": "[HXV13] Jeffrey Ho, Yuchen Xie, and Baba Vemuri.", "startOffset": 0, "endOffset": 7}, {"referenceID": 22, "context": "[JNS13] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "[NJS13] Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "[Pis99] Gilles Pisier.", "startOffset": 0, "endOffset": 7}, {"referenceID": 25, "context": "[QSW14] Qing Qu, Ju Sun, and John Wright.", "startOffset": 0, "endOffset": 7}, {"referenceID": 26, "context": "[SQW14] Ju Sun, Qing Qu, and John Wright.", "startOffset": 0, "endOffset": 7}, {"referenceID": 27, "context": "[SWW12] Daniel A Spielman, Huan Wang, and John Wright.", "startOffset": 0, "endOffset": 7}, {"referenceID": 28, "context": "[Ver10] Roman Vershynin.", "startOffset": 0, "endOffset": 7}, {"referenceID": 29, "context": "[YCS13] Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 7}, {"referenceID": 30, "context": "[ZF13] Yun-Bin Zhao and Masao Fukushima.", "startOffset": 0, "endOffset": 6}, {"referenceID": 31, "context": "[ZHT06] Hui Zou, Trevor Hastie, and Robert Tibshirani.", "startOffset": 0, "endOffset": 7}, {"referenceID": 32, "context": "[ZP01] Michael Zibulevsky and Barak A Pearlmutter.", "startOffset": 0, "endOffset": 6}, {"referenceID": 28, "context": "13 (Spectrum of Gaussian Matrices, [Ver10]).", "startOffset": 35, "endOffset": 42}, {"referenceID": 18, "context": "Geometrically, this lemma roughly corresponds to thewell known almost spherical section theorem [FLM77, GG84], see also [GM03].", "startOffset": 120, "endOffset": 126}, {"referenceID": 15, "context": "A slight variant of this version has been proved in [Don06], borrowing ideas from [Pis99].", "startOffset": 52, "endOffset": 59}, {"referenceID": 24, "context": "A slight variant of this version has been proved in [Don06], borrowing ideas from [Pis99].", "startOffset": 82, "endOffset": 89}], "year": 2017, "abstractText": "We consider the problem of recovering the sparsest vector in a subspace S \u2286 R with dim (S) = n < p. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing andmachine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds 1/ \u221a n. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is \u03a9(1). To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.", "creator": "LaTeX with hyperref package"}}}