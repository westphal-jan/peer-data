{"id": "1412.4729", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2014", "title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "abstract": "Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.", "histories": [["v1", "Mon, 15 Dec 2014 19:21:50 GMT  (1859kb,D)", "https://arxiv.org/abs/1412.4729v1", null], ["v2", "Fri, 19 Dec 2014 00:58:38 GMT  (1859kb,D)", "http://arxiv.org/abs/1412.4729v2", "Corrected minor typos"], ["v3", "Thu, 30 Apr 2015 04:22:06 GMT  (6586kb,D)", "http://arxiv.org/abs/1412.4729v3", "NAACL-HLT 2015 camera ready"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["subhashini venugopalan", "huijuan xu", "jeff donahue", "marcus rohrbach", "raymond j mooney", "kate saenko"], "accepted": true, "id": "1412.4729"}, "pdf": {"name": "1412.4729.pdf", "metadata": {"source": "CRF", "title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "authors": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "emails": ["vsub@cs.utexas.edu", "hxu1@cs.uml.edu", "jdonahue@eecs.berkeley.edu", "rohrbach@eecs.berkeley.edu", "mooney@cs.utexas.edu", "saenko@cs.uml.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who work for the rights of women and men are working for the rights of men and women who work for the rights of women and men."}, {"heading": "2 Related Work", "text": "Most of the existing research in video description focuses on limited areas with limited vocabulary of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Ding et al., 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013). For example, Rohrbach et al. (2014) produce descriptions for videos of several people in the same kitchen. These approaches generate sentences by anticipating semantic role representation, for example, by using high-level concepts such as the actor, the action and the object."}, {"heading": "3 Approach", "text": "Figure 2 shows our model for sentence generation from videos. Our framework is based on deep image description models in Donahue et al. (2014); Vinyalset al. (2014) and expands them to generate sentences describing events in videos. These models work by first applying a functional transformation to an image to generate a fixed dimensional vector representation, then using a sequence model, in particular a recursive neural network (RNN), to \"decrypt\" the vector into a sentence (i.e. a word sequence). In this work, we apply the same principle of \"translating\" a visual vector into an English sentence and show that it works well to describe dynamic videos as well as static images. We identify the most likely description for a particular video by using a model to maximize the probability of the sentence S, given the corresponding video model V and model parameters."}, {"heading": "3.1 LSTMs for sequence generation", "text": "A Recurrent Neural Network (RNN) is a generalization of feed forward neural networks to sequences (1997). Standard RNNs learn to map a sequence of inputs (x1,.., xt) to a sequence of hidden states (h1,.., ht), and from the hidden states to a sequence of outputs (z1,.., zt) based on the following recurrences: ht = f (Wxhxt + Whhht \u2212 1) (3) zt = g (Wzhht) (4) where f and g are element-wise non-linear functions as a sigmoid or hyperbolic tangent, xt is a fixed length vector representation of the input, ht: RN is the hidden state with N units, Wij are the weight connecting the layer of neurons, and zt the output vector.RNNs can learn to map sequences for which the explication between the inputs and outputs is known time of Sutal problems (2014)."}, {"heading": "3.2 CNN-LSTMs for video description", "text": "The first steps in this direction are indeed very long, but there is still much to be done. \"The first steps in this direction have already been taken:\" We have not yet done enough. \"The first steps in the right direction have already been taken,\" we have not yet begun. \"The second steps in the right direction are not yet complete:\" We have not yet begun. \"The first steps in the right direction are to be taken in the right direction.\" We have not yet begun. \"The first steps in the right direction are not yet complete.\" We are not yet ready, \"he says,\" we are not yet ready. \"The second steps in the right direction are not yet complete.\" The first steps in the right direction have been taken in the right direction. \""}, {"heading": "3.3 Transfer Learning from Captioned Images", "text": "Since the training data available for the video description is quite limited (described in Section 4.1), we also use much larger sets of data available for captioning to train our LSTM model and then refine it on the video data set. Our LSTM model for images is the same as the one described above for individual video images (in Section 3.1 and 3.2). As with videos, we extract fc7 layer features (4096 dimensional vector) from the network (Section 3.2) for the images. This is the visual feature that is entered into the two-tiered LSTM description model. Vocabulary is the combined amount of words in the video and image data sets. After the model has been trained on the image data set, we use the weights of the trained model to initialize the LSTM model for the video description task. Additionally, we reduce the learning rate on our LSTM model so that it can adapt to the video data set."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "Video Data Record. We perform all our experiments on the Microsoft Research Video Description Corpus (Chen and Dolan, 2011). This video corpus is a collection of 1970 YouTube snippets. The duration of each clip is between 10 seconds and 25 seconds and typically depicts a single activity or a short sequence. The data set contains several man-made descriptions in a number of languages; we use the approximately 40 available English descriptions per video. This data set (or parts thereof) was used in several previous works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) for action data recognition and video description tasks. For our task we select 1200 videos that are used as training data, 100 videos for validation and 670 videos for testing, as they are used from previous work to video description data-data data-data data-data data-data-data description and video description tasks-data-data-data-data-data-data-data-data-data used as training tasks-data-data-data-data-data-data-data-data-used as-data-data-data-data-used from previous work to video description-and video description-data-and video description-data-data-and video description-data-data-data-data-and-data-data-and-data-data-used-and-data-data-data-data-and-data-and-and-data-and-data-data-used-data-and-and-data-data-to-data-data-to-to-be-used-used-data-data-and-data-data-and-used-data-data-to-and-data-and-data-to-to-be-used-data-data-used-and-data-data-data-to-data-to-be-used-and-used-data-data-data-used-data-to-data-data-and-data-used-to-and-data-data-to-as-and-data-as-data-and-as-as-data-as-data-data-and-as-as-data"}, {"heading": "4.2 Models", "text": "The model uses powerful visual detectors to predict trust across 45 subjects, 218 verbs and 241 objects. FGM (Thomason et al., 2014) also proposes a factor graph model (FGM), which combines knowledge gained from text corpora with visual certainties from the HVC model using a factor graph and performs probable conclusions to determine the most likely subject, verb, object and scene cube. Subsequently, they use a simple template to generate a sentence from the tuple. In this thesis, we compare the output of our model with the subject, verb, object word predicted by the HVC and FGM models and the sentences generated from the SVO triple.Our LSTM models We present four main models. LSTM-YT is our two-layered LSTM data model, which was trained on the YouTube video dataset, the Flickr model."}, {"heading": "4.3 Evaluation Metrics and Results", "text": "In fact, most of them are able to play by the rules that they have established in recent years, and they are able to play by the rules that they have imposed on themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "5 Discussion", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed a video description model that uses neural networks across the entire pipeline from pixels to sentences, potentially enabling the training and coordination of the entire network. In a comprehensive experimental evaluation, we have shown that our approach generates better sentences than related approaches. We have also shown that the use of image description data improves performance compared to the use of video description data. However, our approach falls short of making better use of time information in videos, which is a good direction for future work. We will publish our caffe-based implementation as well as the model and sentences generated."}, {"heading": "Acknowledgments", "text": "The authors thank Trevor Darrell for his valuable advice. We also thank the reviewers for their comments and suggestions. Marcus Rohrbach was supported by a scholarship within the FITworldwide Programme of the German Academic Exchange Service (DAAD), which was partially supported by ONR ATL Grant N00014-11-1-010, NSF Awards IIS-1451244 and IIS-1212798."}], "references": [{"title": "Generating image descriptions using dependency relational patterns", "author": ["Ahmet Aker", "Robert Gaizauskas."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Aker and Gaizauskas.,? 2010", "shortCiteRegEx": "Aker and Gaizauskas.", "year": 2010}, {"title": "Video2text: Learning to annotate video content", "author": ["H. Aradhye", "G. Toderici", "J. Yagnik."], "venue": "IEEE International Conference on Data Mining Workshops (ICDMW).", "citeRegEx": "Aradhye et al\\.,? 2009", "shortCiteRegEx": "Aradhye et al\\.", "year": 2009}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["Satanjeev Banerjee", "Alon Lavie."], "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Sum-", "citeRegEx": "Banerjee and Lavie.,? 2005", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Video in sentences out. In Association for Uncertainty in Artificial Intelligence (UAI)", "author": ["lian Wei", "Yifan Yin", "Zhiqi Zhang"], "venue": null, "citeRegEx": "Wei et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2012}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L. Chen", "William B. Dolan."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Translating related words to videos and back through latent topics", "author": ["P. Das", "R.K. Srihari", "J.J. Corso."], "venue": "Proceedings of Sixth ACM International Conference on Web Search and Data Mining (WSDM).", "citeRegEx": "Das et al\\.,? 2013a", "shortCiteRegEx": "Das et al\\.", "year": 2013}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["P. Das", "C. Xu", "R.F. Doell", "J.J. Corso."], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Das et al\\.,? 2013b", "shortCiteRegEx": "Das et al\\.", "year": 2013}, {"title": "Beyond audio and video retrieval: towards multimedia summarization", "author": ["D. Ding", "F. Metze", "S. Rawat", "P.F. Schulam", "S. Burger", "E. Younessian", "L. Bao", "M.G. Christel", "A. Hauptmann."], "venue": "Proceedings of the 2nd ACM International Conference on Multimedia", "citeRegEx": "Ding et al\\.,? 2012", "shortCiteRegEx": "Ding et al\\.", "year": 2012}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell."], "venue": "arXiv preprint arXiv:1310.1531.", "citeRegEx": "Donahue et al\\.,? 2013", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Kate Saenko", "Trevor Darrell."], "venue": "CoRR, abs/1411.4389.", "citeRegEx": "Saenko and Darrell.,? 2014", "shortCiteRegEx": "Saenko and Darrell.", "year": 2014}, {"title": "Comparing automatic evaluation measures for image description", "author": ["Desmond Elliott", "Frank Keller."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Elliott and Keller.,? 2014", "shortCiteRegEx": "Elliott and Keller.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest N. Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig."], "venue": "CoRR,", "citeRegEx": "Fang et al\\.,? 2014", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth."], "venue": "European Conference on Computer Vision (ECCV).", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Towards end-toend speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14).", "citeRegEx": "Graves and Jaitly.,? 2014", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot", "author": ["Sergio Guadarrama", "Niveda Krishnamoorthy", "Girish Malkarnenkar", "Subhashini Venugopalan", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "Guadarrama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guadarrama et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young Alice Lai Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics (TACL).", "citeRegEx": "Hodosh and Hockenmaier.,? 2014", "shortCiteRegEx": "Hodosh and Hockenmaier.", "year": 2014}, {"title": "A multi-modal clustering method for web videos", "author": ["Haiqi Huang", "Yueming Lu", "Fangwei Zhang", "Songlin Sun."], "venue": "Trustworthy Computing and Services. Springer.", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell."], "venue": "arXiv preprint arXiv:1408.5093.", "citeRegEx": "Jia et al\\.,? 2014", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Li Fei-Fei."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Karpathy et al\\.,? 2014", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Describing video contents in natural language", "author": ["Muhammad Usman Ghani Khan", "Yoshihiko Gotoh"], "venue": null, "citeRegEx": "Khan and Gotoh.,? \\Q2012\\E", "shortCiteRegEx": "Khan and Gotoh.", "year": 2012}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhuditnov", "Richard. S Zemel."], "venue": "arXiv preprint arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Natural language description of human activities from video images based on concept hierarchy of actions", "author": ["A. Kojima", "T. Tamura", "K. Fukunaga."], "venue": "International Journal of Computer Vision (IJCV), 50(2).", "citeRegEx": "Kojima et al\\.,? 2002", "shortCiteRegEx": "Kojima et al\\.", "year": 2002}, {"title": "Generating natural-language video descriptions using text-mined knowledge", "author": ["Niveda Krishnamoorthy", "Girish Malkarnenkar", "Raymond J. Mooney", "Kate Saenko", "Sergio Guadarrama."], "venue": "AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Krishnamoorthy et al\\.,? 2013", "shortCiteRegEx": "Krishnamoorthy et al\\.", "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg."], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Kulkarni et al\\.,? 2011", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Tamara L Berg", "UNC Chapel Hill", "Yejin Choi."], "venue": "Transactions of the Association for Computational Linguistics, 2(10).", "citeRegEx": "Kuznetsova et al\\.,? 2014", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "Save: A framework for semantic annotation of visual events", "author": ["M.W. Lee", "A. Hakeem", "N. Haering", "S.C. Zhu."], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Microsoft COCO: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1405.0312.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille."], "venue": "arXiv preprint arXiv:1410.1090.", "citeRegEx": "Mao et al\\.,? 2014", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Improving video activity recognition using object recognition and text mining", "author": ["Tanvi S. Motwani", "Raymond J. Mooney."], "venue": "Proceedings of the 20th European Conference on Artificial Intelligence (ECAI).", "citeRegEx": "Motwani and Mooney.,? 2012", "shortCiteRegEx": "Motwani and Mooney.", "year": 2012}, {"title": "TRECVID 2012 \u2013 an overview of the goals, tasks, data, evaluation mechanisms and metrics", "author": ["Paul Over", "George Awad", "Martial Michel", "Jonathan Fiscus", "Greg Sanders", "B Shaw", "Alan F. Smeaton", "Georges Qu\u00e9enot."], "venue": "Proceedings of TRECVID 2012.", "citeRegEx": "Over et al\\.,? 2012", "shortCiteRegEx": "Over et al\\.", "year": 2012}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan Thater", "Manfred Pinkal", "Bernt Schiele."], "venue": "IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Rohrbach et al\\.,? 2013", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Wei Qiu", "Annemarie Friedrich", "Manfred Pinkal", "Bernt Schiele."], "venue": "German Conference on Pattern Recognition (GCPR), September.", "citeRegEx": "Rohrbach et al\\.,? 2014", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["J. Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R.J. Mooney."], "venue": "Proceedings of the 25th International Conference on Computational", "citeRegEx": "Thomason et al\\.,? 2014", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "CoRR, abs/1411.4555.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Multimodal fusion for video search reranking", "author": ["Shikui Wei", "Yao Zhao", "Zhenfeng Zhu", "Nan Liu."], "venue": "IEEE Transactions on Knowledge and Data Engineering,, 22(8).", "citeRegEx": "Wei et al\\.,? 2010", "shortCiteRegEx": "Wei et al\\.", "year": 2010}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso."], "venue": "AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "I2t: Image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu."], "venue": "Proceedings of the IEEE, 98(8).", "citeRegEx": "Yao et al\\.,? 2010", "shortCiteRegEx": "Yao et al\\.", "year": 2010}, {"title": "Grounded language learning from videos described with sentences", "author": ["Haonan Yu", "Jeffrey Mark Siskind."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Yu and Siskind.,? 2013", "shortCiteRegEx": "Yu and Siskind.", "year": 2013}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1410.4615.", "citeRegEx": "Zaremba and Sutskever.,? 2014", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus."], "venue": "European Conference on Computer Vision (ECCV). Springer.", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 36, "context": ", (Barbu et al., 2012; Rohrbach et al., 2013), but generating descriptions for \u201cin-thewild\u201d videos such as the YouTube domain (Figure 1) remains an open challenge.", "startOffset": 2, "endOffset": 45}, {"referenceID": 15, "context": "a fixed set of semantic roles, such as subject, verb, and object (Guadarrama et al., 2013; Thomason et al., 2014), as an intermediate representation.", "startOffset": 65, "endOffset": 113}, {"referenceID": 40, "context": "a fixed set of semantic roles, such as subject, verb, and object (Guadarrama et al., 2013; Thomason et al., 2014), as an intermediate representation.", "startOffset": 65, "endOffset": 113}, {"referenceID": 9, "context": "Deep NNs can learn powerful features (Donahue et al., 2013; Zeiler and Fergus, 2014), but require a lot of supervised training data.", "startOffset": 37, "endOffset": 84}, {"referenceID": 47, "context": "Deep NNs can learn powerful features (Donahue et al., 2013; Zeiler and Fergus, 2014), but require a lot of supervised training data.", "startOffset": 37, "endOffset": 84}, {"referenceID": 27, "context": "2M+ images with category labels (Krizhevsky et al., 2012).", "startOffset": 32, "endOffset": 57}, {"referenceID": 18, "context": "and sequence of words is modeled by a recurrent (temporally invariant) deep network pre-trained on 100K+ Flickr (Hodosh and Hockenmaier, 2014) and COCO (Lin et al.", "startOffset": 112, "endOffset": 142}, {"referenceID": 31, "context": "and sequence of words is modeled by a recurrent (temporally invariant) deep network pre-trained on 100K+ Flickr (Hodosh and Hockenmaier, 2014) and COCO (Lin et al., 2014) images with associated sen-", "startOffset": 152, "endOffset": 170}, {"referenceID": 9, "context": "Our approach is inspired by recent breakthroughs reported by several research groups in image-to-text generation, in particular, the work by Donahue et al. (2014). They applied a version of their model to video-to-text generation, but stopped short of proposing an end-to-end single network, using an intermediate role representation instead.", "startOffset": 141, "endOffset": 163}, {"referenceID": 16, "context": "Inspired by their approach, we utilize a Long-Short Term Memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997) to", "startOffset": 96, "endOffset": 130}, {"referenceID": 39, "context": "The LSTM model, which has recently achieved state-ofthe-art results on machine translation tasks (French and English (Sutskever et al., 2014)), effectively", "startOffset": 117, "endOffset": 141}, {"referenceID": 15, "context": "models the sequence generation task without requiring the use of fixed sentence templates as in previous work (Guadarrama et al., 2013).", "startOffset": 110, "endOffset": 135}, {"referenceID": 38, "context": "Finally, the deep convnet, the winner of the ILSVRC2012 (Russakovsky et al., 2014) image classification competition, provides a strong visual representation of objects, actions and scenes depicted in the video.", "startOffset": 56, "endOffset": 82}, {"referenceID": 4, "context": "\u2022 We provide a detailed evaluation of our model on the popular YouTube corpus (Chen and Dolan, 2011) and demonstrate a significant improvement over the state of the art.", "startOffset": 78, "endOffset": 100}, {"referenceID": 25, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 30, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 22, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 8, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 22, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 7, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 6, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 36, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 45, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 6, "context": ", 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013). For example, Rohrbach et al. (2013), Rohrbach et al.", "startOffset": 30, "endOffset": 150}, {"referenceID": 6, "context": ", 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013). For example, Rohrbach et al. (2013), Rohrbach et al. (2014) produce descriptions for videos of several people cooking in the same kitchen.", "startOffset": 30, "endOffset": 174}, {"referenceID": 1, "context": "Most work on \u201cin-the-wild\u201d online video has focused on retrieval and predicting event tags rather than generating descriptive sentences; examples are tagging YouTube (Aradhye et al., 2009) and retriev-", "startOffset": 166, "endOffset": 188}, {"referenceID": 34, "context": "ing online video in the TRECVID competition (Over et al., 2012).", "startOffset": 44, "endOffset": 63}, {"referenceID": 42, "context": ", (Wei et al., 2010; Huang et al., 2013).", "startOffset": 2, "endOffset": 40}, {"referenceID": 19, "context": ", (Wei et al., 2010; Huang et al., 2013).", "startOffset": 2, "endOffset": 40}, {"referenceID": 33, "context": "ploy (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate", "startOffset": 5, "endOffset": 108}, {"referenceID": 26, "context": "ploy (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate", "startOffset": 5, "endOffset": 108}, {"referenceID": 15, "context": "ploy (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate", "startOffset": 5, "endOffset": 108}, {"referenceID": 40, "context": "ploy (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate", "startOffset": 5, "endOffset": 108}, {"referenceID": 40, "context": "We compare our method to the best-performing method of Thomason et al. (2014). A recent paper by Xu et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 40, "context": "We compare our method to the best-performing method of Thomason et al. (2014). A recent paper by Xu et al. (2015) extracts deep features from video and a continuous vector from language, and projects both to a joint semantic space.", "startOffset": 55, "endOffset": 114}, {"referenceID": 0, "context": "Predicting natural language desriptions of still images has received considerable attention, with some of the earliest works by Aker and Gaizauskas (2010), Farhadi et al.", "startOffset": 128, "endOffset": 155}, {"referenceID": 0, "context": "Predicting natural language desriptions of still images has received considerable attention, with some of the earliest works by Aker and Gaizauskas (2010), Farhadi et al. (2010), Yao et al.", "startOffset": 128, "endOffset": 178}, {"referenceID": 0, "context": "Predicting natural language desriptions of still images has received considerable attention, with some of the earliest works by Aker and Gaizauskas (2010), Farhadi et al. (2010), Yao et al. (2010), and", "startOffset": 128, "endOffset": 197}, {"referenceID": 32, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 21, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 12, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 23, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 41, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 29, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 24, "context": "In contrast to traditional statistical MT (Koehn, 2010), RNNs naturally combine with vector-based representations, such as those for images and video.", "startOffset": 42, "endOffset": 55}, {"referenceID": 9, "context": "Donahue et al. (2014) and Vinyals et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Donahue et al. (2014) and Vinyals et al. (2014) simultaneously proposed a multimodal", "startOffset": 0, "endOffset": 48}, {"referenceID": 9, "context": "Our approach to video to text generation is inspired by the work of Donahue et al. (2014), who also applied a variant of their model to video-to-text", "startOffset": 68, "endOffset": 90}, {"referenceID": 9, "context": "While Donahue et al. (2014) only showed results on a narrow domain of cooking videos with a small set of pre-defined objects and actors, we generate sentences for opendomain YouTube videos with a vocabulary of thousands of words.", "startOffset": 6, "endOffset": 28}, {"referenceID": 9, "context": "Our framework is based on deep image description models in Donahue et al. (2014);Vinyals m ea n p o o lin g Input Video Convolutional Net Recurrent Net Output", "startOffset": 59, "endOffset": 81}, {"referenceID": 14, "context": "In our work we use the highly successful Long Short-Term Memory (LSTM) net as the sequence model, since it has shown superior performance on tasks such as speech recognition (Graves and Jaitly, 2014), machine translation (Sutskever et al.", "startOffset": 174, "endOffset": 199}, {"referenceID": 39, "context": "In our work we use the highly successful Long Short-Term Memory (LSTM) net as the sequence model, since it has shown superior performance on tasks such as speech recognition (Graves and Jaitly, 2014), machine translation (Sutskever et al., 2014; Cho et al., 2014) and the more related task of generating sentence descriptions of images (Donahue et al.", "startOffset": 221, "endOffset": 263}, {"referenceID": 5, "context": "In our work we use the highly successful Long Short-Term Memory (LSTM) net as the sequence model, since it has shown superior performance on tasks such as speech recognition (Graves and Jaitly, 2014), machine translation (Sutskever et al., 2014; Cho et al., 2014) and the more related task of generating sentence descriptions of images (Donahue et al.", "startOffset": 221, "endOffset": 263}, {"referenceID": 41, "context": ", 2014) and the more related task of generating sentence descriptions of images (Donahue et al., 2014; Vinyals et al., 2014).", "startOffset": 80, "endOffset": 124}, {"referenceID": 39, "context": "RNNs can learn to map sequences for which the alignment between the inputs and outputs is known ahead of time (Sutskever et al., 2014) however it\u2019s unclear if they can be applied to problems where the inputs (xi) and outputs (zi) are of varying lengths.", "startOffset": 110, "endOffset": 134}, {"referenceID": 17, "context": "range dependencies (Hochreiter et al., 2001).", "startOffset": 19, "endOffset": 44}, {"referenceID": 16, "context": "However, LSTMs (Hochreiter and Schmidhuber, 1997), xt", "startOffset": 15, "endOffset": 49}, {"referenceID": 45, "context": "In our work we use the LSTM unit in Figure 3, described in Zaremba and Sutskever (2014), and Donahue et al.", "startOffset": 59, "endOffset": 88}, {"referenceID": 9, "context": "In our work we use the LSTM unit in Figure 3, described in Zaremba and Sutskever (2014), and Donahue et al. (2014). At the core of the LSTM model is a memory cell c", "startOffset": 93, "endOffset": 115}, {"referenceID": 20, "context": "For this we use a CNN, specifically the publicly available Caffe (Jia et al., 2014) reference model, a minor variant of AlexNet (Krizhevsky et al.", "startOffset": 65, "endOffset": 83}, {"referenceID": 27, "context": ", 2014) reference model, a minor variant of AlexNet (Krizhevsky et al., 2012).", "startOffset": 52, "endOffset": 77}, {"referenceID": 9, "context": "We use a two layer LSTM model for generating descriptions for videos based on experiments by Donahue et al. (2014) which suggest two LSTM layers are better than four and a single layer for image to text tasks.", "startOffset": 93, "endOffset": 115}, {"referenceID": 4, "context": "We perform all our experiments on the Microsoft Research Video Description Corpus (Chen and Dolan, 2011).", "startOffset": 82, "endOffset": 104}, {"referenceID": 33, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 26, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 15, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 40, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 43, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 15, "context": "For our task we pick 1200 videos to be used as training data, 100 videos for validation and 670 videos for testing, as used by the prior works on video description (Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015).", "startOffset": 164, "endOffset": 229}, {"referenceID": 40, "context": "For our task we pick 1200 videos to be used as training data, 100 videos for validation and 670 videos for testing, as used by the prior works on video description (Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015).", "startOffset": 164, "endOffset": 229}, {"referenceID": 43, "context": "For our task we pick 1200 videos to be used as training data, 100 videos for validation and 670 videos for testing, as used by the prior works on video description (Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015).", "startOffset": 164, "endOffset": 229}, {"referenceID": 39, "context": "lation (Sutskever et al., 2014) (12M sentences), we use data from the Flickr30k and COCO2014 datasets for training and learn to adapt to the video dataset by fine-tuning the image description models.", "startOffset": 7, "endOffset": 31}, {"referenceID": 18, "context": "The Flickr30k (Hodosh and Hockenmaier, 2014) dataset", "startOffset": 14, "endOffset": 44}, {"referenceID": 31, "context": "In addition to this, we use the recent COCO2014 (Lin et al., 2014) image description dataset consisting of", "startOffset": 48, "endOffset": 66}, {"referenceID": 40, "context": "HVC This is the Highest Vision Confidence model described in (Thomason et al., 2014).", "startOffset": 61, "endOffset": 84}, {"referenceID": 40, "context": "FGM (Thomason et al., 2014) also propose a factor graph model (FGM) that combines knowledge mined from text corpora with visual confidences from the HVC model using a factor graph and performs probabilistic inference to determine the most likely subject, verb, object and scene tuple.", "startOffset": 4, "endOffset": 27}, {"referenceID": 18, "context": "LSTMYTflickr is the model trained on the Flickr30k (Hodosh and Hockenmaier, 2014) dataset, and fine tuned on the YouTube dataset as descibed in Section 3.", "startOffset": 51, "endOffset": 81}, {"referenceID": 31, "context": "LSTM-YTcoco is first trained on the COCO2014 (Lin et al., 2014) dataset and then fine-tuned on the video dataset.", "startOffset": 45, "endOffset": 63}, {"referenceID": 26, "context": "Earlier works (Krishnamoorthy et al., 2013; Guadarrama et al., 2013) that reported results on the YouTube dataset compared their method", "startOffset": 14, "endOffset": 68}, {"referenceID": 15, "context": "Earlier works (Krishnamoorthy et al., 2013; Guadarrama et al., 2013) that reported results on the YouTube dataset compared their method", "startOffset": 14, "endOffset": 68}, {"referenceID": 43, "context": "The latter evaluation was also reported by (Xu et al., 2015), so we include it here for comparison.", "startOffset": 43, "endOffset": 60}, {"referenceID": 35, "context": "To evaluate the generated sentences we use the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores against all ground truth sentences.", "startOffset": 52, "endOffset": 75}, {"referenceID": 2, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) scores against all ground truth sentences.", "startOffset": 19, "endOffset": 45}, {"referenceID": 11, "context": "BLEU is the metric that is seen more commonly in image description literature, but a more recent study (Elliott and Keller, 2014) has shown METEOR to be a better evaluation metric.", "startOffset": 103, "endOffset": 129}, {"referenceID": 40, "context": "HVC (Thomason et al., 2014) 86.", "startOffset": 4, "endOffset": 27}, {"referenceID": 40, "context": "09 FGM (Thomason et al., 2014) 88.", "startOffset": 7, "endOffset": 30}, {"referenceID": 40, "context": "HVC (Thomason et al., 2014) 76.", "startOffset": 4, "endOffset": 27}, {"referenceID": 40, "context": "94 FGM (Thomason et al., 2014) 76.", "startOffset": 7, "endOffset": 30}, {"referenceID": 43, "context": "39 JointEmbed1(Xu et al., 2015) 78.", "startOffset": 14, "endOffset": 31}, {"referenceID": 40, "context": "FGM (Thomason et al., 2014) 13.", "startOffset": 4, "endOffset": 27}, {"referenceID": 40, "context": "FGM (Thomason et al., 2014) 2.", "startOffset": 4, "endOffset": 27}, {"referenceID": 41, "context": "Adding dropout as in (Vinyals et al., 2014) is likely to help prevent overfitting and improve performance.", "startOffset": 21, "endOffset": 43}, {"referenceID": 40, "context": "Additionally, when we ask Turkers to rate only the sentences (they are not provided the video) on grammatical correctness, the template based FGM (Thomason et al., 2014) achieves the highest ratings.", "startOffset": 146, "endOffset": 169}], "year": 2015, "abstractText": "Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.", "creator": "TeX"}}}