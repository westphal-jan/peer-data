{"id": "1412.4864", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2014", "title": "Learning with Pseudo-Ensembles", "abstract": "We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et. al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.", "histories": [["v1", "Tue, 16 Dec 2014 02:55:05 GMT  (321kb,D)", "http://arxiv.org/abs/1412.4864v1", "To appear in Advances in Neural Information Processing Systems 27 (NIPS 2014), Advances in Neural Information Processing Systems 27, Dec. 2014"]], "COMMENTS": "To appear in Advances in Neural Information Processing Systems 27 (NIPS 2014), Advances in Neural Information Processing Systems 27, Dec. 2014", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["philip bachman", "ouais alsharif", "doina precup"], "accepted": true, "id": "1412.4864"}, "pdf": {"name": "1412.4864.pdf", "metadata": {"source": "CRF", "title": "Learning with Pseudo-Ensembles", "authors": ["Philip Bachman", "Ouais Alsharif"], "emails": ["phil.bachman@gmail.com", "ouais.alsharif@gmail.com", "dprecup@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "Ensembles generally work by training multiple classifiers on disturbed input distributions, e.g. bagging randomly elides portions of the distribution for each model trained and increasing the reweighting of the distribution before training and adding each model to the ensemble. In recent years, dropout methods have achieved great empirical success in forming deep models by using a noise process that disrupts the model structure itself. However, there are not many analyses that relate this approach to classical ensemble methods or other approaches to learning more robust models. In this paper we formalize the notion of a pseudo-ensemble emerging from a parent model by interfering with any noise process. Sec. 2 defines pseudo-ensembles according to which Sec. 3 discusses the relationships between pseudo-ensembles and standard ensemble methods, as well as existing notations of robustness."}, {"heading": "2 What is a pseudo-ensemble?", "text": "Consider a data distribution model that we want to simulate using a parametric parent model. A pseudo-ensemble is a collection of pseudo-child models. (Pseudo-ensembles) Pseudo-ensembles is a collection of pseudo-child models. (Pseudo-ensembles) Pseudo-ensembles from a source network by accidentally masking the activity of subsets of their input / hidden layer nodes. The parameters shared by the subnetworks through their shared source network are learned to minimize the expected loss of the individual subnets. (Pseudo-ensembles: The source network is the parent model, each sampled subnetwork is a child model, and the noise process consists of sampling a node mask and using it to extract a subnetwork.) The noise process used to create a pseudo-ensemble can take fairly arbitrary forms."}, {"heading": "3 Related work", "text": "Pseudo-ensembles are closely linked to traditional ensemble methods, as well as to methods based on uncertainty."}, {"heading": "4 The Pseudo-Ensemble Agreement regularizer", "text": "We now present the pseudo-ensemble agreement (PEA) for regulation, which can be used in a fairly general class of computational graphs. To specify it, we present it in the case of deep, layered neural networks. PEA regulation works by controlling the distribution properties of random vectors {f2\u03b8 (x;),..., fd\u03b8 (x;)}, where f i\u03b8 (x;) indicates the activities of the ith layer of f\u03b8 in response to x when layers < i are disturbed by layer i, while layer i is left undisturbed. Fig. 1 illustrates the construction of these random vectors. We assume that layer d is the output layer, d.fd\u03b8 (x) gives the output of the undisturbed parent model in response to x and fdTB (x;) gives the response of the child model in dependence (x)."}, {"heading": "4.1 The effect of PEA regularization on feature co-adaptation", "text": "One of the original reasons for dropping out of the study was that it helps to prevent \"co-adaptation\" (max.), i.e., the drop-out encourages individual characteristics (i.e. hidden node activities) to remain helpful or at least not to become harmful when other characteristics are removed from their local context. We support this assertion by examining the following optimization objective 3: Minimize the E (x, y) -Pxy [L (fx), y)] + E x x x x x x x x x-px-px (d) -iVi (f i\u03b8 (x), f i\u03b8 (x))))), (3), in which the verified loss L depends only on the parent model and the pseudo-ensemble appears only in the PEA regularization concept."}, {"heading": "4.2 Relating PEA regularization to standard dropout", "text": "The authors of [21] show that, assuming a noise process designed so that E\u03b8 (f; q) = f (x), logistic regression under the influence of dropouts optimizes the following goal: n [i] = 1 E ['(f; xi), yi) = n (f) (f = 1' (f) (xi), and the regularization term is: R (f), n (f) + R (f), (4) where f\u03b8 (xi), '(f) (xi), yi) is the logistic regression loss, and the regularization term is: R (f)."}, {"heading": "4.3 PEA regularization for semi-supervised learning", "text": "The PEA regulation works as it does in a semi-supervised environment because the penalties Vi do not require label information. We train networks for semi-supervised learning in two ways, both of which apply the goal in Equation 1 to labeled examples and the PEA regulation to unlabeled examples. The first way applies a Tanh variance penalty Vt and the second way applies a xent variance penalty Vx, which we define as follows: Vt (y, y) = | tanh (y) \u2212 tanh (y) | 22, Vx (y, y) = xent (softmax (y, y)), (11) where y (y, y) represent the results of a pair of independently labeled child models, and tanh works elementally. The xent variance penalty can be extended further than: Vx (y, y) = DKL (soft, y)."}, {"heading": "5 Testing PEA regularization", "text": "We tested PEA regularization in three scenarios: Supervised Learning on MNIST digits, Semi-Supervised Learning on MNIST digits, and Semi-Supervised Transfer Learning on a dataset from the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models [13]. Full implementations of our methods, written with THEANO [3], and scripts / instructions for reproducing all results in this section are available online at: http: / / github.com / Philip-Bachman / Pseudo-ensembles."}, {"heading": "5.1 Fully-supervised MNIST", "text": "The MNIST dataset consists of 60k 28x28 grayscale images with handwritten numbers for the training and 10k images for the test. We used SGD hyperparameters roughly equivalent to those in [9] for the monitored tests. We trained networks with two hidden layers of 800 nodes each, using linear activations and a \"2-standard restriction\" of 3.5 to the incoming weights for each node. For both Standard Exposers (SDE) and PEA, we used Softmax \u2192 xent losses on the output layer. We initialized hidden layer distortions to 0.1, the output layer distorted to 0, and the interlayer weights to zero-mean-Gaussian noise to \u03c3 = 0.01. We trained all networks for 1000 epochs without premature stop (i.e. performance was measured for the final network state). SDE received 1.05% error averaged via five random initializations."}, {"heading": "5.2 Semi-supervised MNIST", "text": "We tested semi-supervised learning on MNIST following the protocol described in [23]. These tests split MNIST's 60k training samples into labeled / unlabeled subsets, with the labeled sets including nl, 100, 600, 1000, 3000} samples. For labeled sets of size 600, 1000, and 3000, the full training data was randomly split 10 times in labeled / unlabeled sets and results were averaged over the splits. For labeled sets of size 100, we averaged over 50 random splits. The labeled sets had the same number of examples for each class. We tested PEA regularization with and without denoising autoencoder pre-training [20] 4. Pre-trained networks were always PEA-regularized with penalty Vx4See our code for a perfect complete description of our pre-training.on the output layer and Vc on the hidden layer."}, {"heading": "5.3 Transfer learning challenge (NIPS 2011)", "text": "The organizers of the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models [13] proposed a challenge to improve performance in a target domain by using labeled and unlabeled data from two related source domains; the labeled data source was CIFAR-100 [11], which contained 50k 32x32 color images in 100 classes; the unlabeled data source was a collection of 100k 32x32 color images from Tiny Images [11]; the target domain consisted of 120 32x32 color images that were unevenly distributed among 10 classes; neither the classes nor the images in the target domain appeared in any of the source domains; the winner of this challenge used Convolutionary Spike and Slab Sparse Coding, followed by maximum pooling and a linear SVM on the pooled features [6]; labels on the source data were ignored; and the source data was used to prepare a large set of volume-related features."}, {"heading": "6 Improved sentiment analysis using pseudo-ensembles", "text": "This year it has come to the point that it is a purely reactionary project, in which it is a reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, etc."}, {"heading": "7 Discussion", "text": "We proposed the idea of a pseudo-ensemble that captures methods such as dropouts [9] and noise in linear models [5, 21] that have lately attracted a lot of attention. Using the conceptual framework of pseudo-ensembles, we developed and applied a regulator that performs well empirically and provides insights into the mechanisms underlying dropouts \"success. We also demonstrated how pseudo-ensembles can be used to improve the performance of an already powerful model on a competitive benchmark for the real world of mood analysis. We expect that this idea, which brings together several rapidly evolving lines of research, can be used to develop several new and successful algorithms, especially for semi-supervised learning."}], "references": [{"title": "Understanding dropout", "author": ["P. Baldi", "P. Sadowski"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "\u00c9. Thibodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": "[cs.LG],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Theano: A cpu and gpu math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Python for Scientific Computing Conference (SciPy),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Theory and applications of robust optimization", "author": ["D. Bertsimas", "D.B. Brown", "C. Caramanis"], "venue": "SIAM Review,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Learning with marginalized corrupted features", "author": ["L. Van der Maaten", "M. Chen", "S. Tyree", "K.Q. Weinberger"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Large-scale feature learning with spike-and-slab sparse coding", "author": ["I.J. Goodfellow", "A. Courville", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Semi-Supervised Learning, chapter Entropy Regularization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Elements of Statistical Learning II", "author": ["T. Hastie", "J. Friedman", "R. Tibshirani"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "[cs.NE],", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Workshop on challenges in learning hierarchical models: Transfer learning and optimization", "author": ["Q.V. Le", "M.A. Ranzato", "R.R. Salakhutdinov", "A.Y. Ng", "J. Tenenbaum"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks", "author": ["D.-H. Lee"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "On the difficulties of training recurrent neural networks", "author": ["R. Pacanu", "T. Mikolov", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Learning ordered representations with nested dropout", "author": ["O. Rippel", "M.A. Gelbart", "R.P. Adams"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Lectures on Stochastic Programming: Modeling and Theory", "author": ["A. Shapiro", "D. Dentcheva", "A. Ruszczynski"], "venue": "Society for Industrial and Applied Mathematics (SIAM),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In EMNLP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Dropout training as adaptive regularization", "author": ["S. Wager", "S. Wang", "P. Liang"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "An empirical analysis of dropout in piecewise linear networks", "author": ["D. Warde-Farley", "I.J. Goodfellow", "A. Courville", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "R. Collobert"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Robust regression and lasso", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Robustness and regularization of support vector", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "machines. JMLR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}], "referenceMentions": [{"referenceID": 8, "context": ", dropout [9] in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network.", "startOffset": 10, "endOffset": 13}, {"referenceID": 18, "context": "We provide a case study in which we transform the Recursive Neural Tensor Network of [19] into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "6 presents a case study in which we extend the Recursive Neural Tensor Network from [19] by converting it into a pseudo-ensemble.", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "Dropout [9] provides the clearest existing example of a pseudo-ensemble.", "startOffset": 8, "endOffset": 11}, {"referenceID": 16, "context": "For example, [17] develops a method for learning \u201cordered representations\u201d by applying dropout/masking noise in a deep autoencoder while enforcing a particular \u201cnested\u201d structure among the random masking variables in \u03be, and [2] relies heavily on random perturbations when training Generative Stochastic Networks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "For example, [17] develops a method for learning \u201cordered representations\u201d by applying dropout/masking noise in a deep autoencoder while enforcing a particular \u201cnested\u201d structure among the random masking variables in \u03be, and [2] relies heavily on random perturbations when training Generative Stochastic Networks.", "startOffset": 224, "endOffset": 227}, {"referenceID": 7, "context": "By optimizing the expected loss of individual ensemble members\u2019 outputs, rather than the expected loss of the joint ensemble output, pseudo-ensembles differ from boosting, which iteratively augments an ensemble to minimize the loss of the joint output [8].", "startOffset": 252, "endOffset": 255}, {"referenceID": 7, "context": "This distinguishes pseudo-ensembles from traditional \u201cindependent member\u201d ensemble methods, like bagging and random forests, which typically prefer diversity in the behavior of their members, as this provides bias and variance reduction when the outputs of their members are averaged [8].", "startOffset": 284, "endOffset": 287}, {"referenceID": 17, "context": "For example, the optimization community has produced a large body of theoretical and empirical work addressing \u201cstochastic programming\u201d [18] and \u201crobust optimization\u201d [4].", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "For example, the optimization community has produced a large body of theoretical and empirical work addressing \u201cstochastic programming\u201d [18] and \u201crobust optimization\u201d [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 23, "context": "For example, [24] shows that using Lasso (i.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "[25] shows that learning a standard SVM (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Supporting the notion that noise-robustness improves generalization, [25] prove many of the statistical guarantees that make SVMs so appealing directly from properties of their robust optimization equivalents, rather than using more complicated proofs involving, e.", "startOffset": 69, "endOffset": 73}, {"referenceID": 4, "context": "[5] shows how to efficiently learn a linear model that (globally) optimizes expected performance w.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "Particularly relevant to our work is [21], which studies dropout (applied to linear models) closely, and shows how its effects are well-approximated by a Tikhonov (i.", "startOffset": 37, "endOffset": 41}, {"referenceID": 20, "context": "The authors of [21] leveraged this label-agnosticism to achieve state-of-the-art performance on several sentiment analysis tasks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "This approximation holds reasonably well for many useful neural network architectures [1, 22].", "startOffset": 86, "endOffset": 93}, {"referenceID": 21, "context": "This approximation holds reasonably well for many useful neural network architectures [1, 22].", "startOffset": 86, "endOffset": 93}, {"referenceID": 8, "context": "One of the original motivations for dropout was that it helps prevent \u201cfeature co-adaptation\u201d [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 21, "context": "Also, by acting strictly to make the output of the parent model more robust to \u03be-perturbation, the performance of this regularizer rebuts the claim in [22] that noise-robustness plays only a minor role in the success of standard dropout.", "startOffset": 151, "endOffset": 155}, {"referenceID": 20, "context": "The authors of [21] show that, assuming a noise process \u03be such that E\u03be[f(x; \u03be)] = f(x), logistic regression under the influence of dropout optimizes the following objective:", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "Thus, V combines the KL-divergence penalty with an entropy penalty, which has been shown to perform well in a semi-supervised setting [7, 14].", "startOffset": 134, "endOffset": 141}, {"referenceID": 13, "context": "Thus, V combines the KL-divergence penalty with an entropy penalty, which has been shown to perform well in a semi-supervised setting [7, 14].", "startOffset": 134, "endOffset": 141}, {"referenceID": 12, "context": "We tested PEA regularization in three scenarios: supervised learning on MNIST digits, semi-supervised learning on MNIST digits, and semi-supervised transfer learning on a dataset from the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models [13].", "startOffset": 253, "endOffset": 257}, {"referenceID": 2, "context": "Full implementations of our methods, written with THEANO [3], and scripts/instructions for reproducing all of the results in this section are available online at: http://github.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "For the supervised tests we used SGD hyperparameters roughly following those in [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 22, "context": "We tested semi-supervised learning on MNIST following the protocol described in [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 19, "context": "We tested PEA regularization with and without denoising autoencoder pre-training [20]4.", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "The top row of filter blocks in (a) were the result of training a fixed network architecture on 600 labeled samples using: weight norm constraints only (RAW), standard dropout (SDE), standard dropout with PEA regularization on unlabeled data (PEA), and PEA preceded by pre-training as a denoising autoencoder [20] (PEA+PT).", "startOffset": 309, "endOffset": 313}, {"referenceID": 6, "context": "In the latter case, we gradually increased the \u03bbi over the course of training, as suggested by [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 22, "context": "E-NN (EmbedNN from [23]) uses a nearest-neighbors-based graph Laplacian regularizer to make predictions \u201csmooth\u201d with respect to the manifold underlying the data distribution px.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "MTC+ (the Manifold Tangent Classifier from [16]) regularizes predictions to be smooth with respect to the data manifold by penalizing gradients in a learned approximation of the tangent space of the data manifold.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "PL+ (the PseudoLabel method from [14]) uses the joint-ensemble predictions on unlabeled data as \u201cpseudo-labels\u201d, and treats them like \u201ctrue\u201d labels.", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "From left-to-right the methods are Transductive SVM , neural net, convolutional neural net, EmbedNN [23], Manifold Tangent Classifier [16], Pseudo-Label [14], standard dropout plus fuzzing [9], dropout plus fuzzing with pre-training, PEA, and PEA with pre-training.", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "From left-to-right the methods are Transductive SVM , neural net, convolutional neural net, EmbedNN [23], Manifold Tangent Classifier [16], Pseudo-Label [14], standard dropout plus fuzzing [9], dropout plus fuzzing with pre-training, PEA, and PEA with pre-training.", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "From left-to-right the methods are Transductive SVM , neural net, convolutional neural net, EmbedNN [23], Manifold Tangent Classifier [16], Pseudo-Label [14], standard dropout plus fuzzing [9], dropout plus fuzzing with pre-training, PEA, and PEA with pre-training.", "startOffset": 153, "endOffset": 157}, {"referenceID": 8, "context": "From left-to-right the methods are Transductive SVM , neural net, convolutional neural net, EmbedNN [23], Manifold Tangent Classifier [16], Pseudo-Label [14], standard dropout plus fuzzing [9], dropout plus fuzzing with pre-training, PEA, and PEA with pre-training.", "startOffset": 189, "endOffset": 192}, {"referenceID": 19, "context": "Methods with a \u201c+\u201d used contractive or denoising autoencoder pre-training [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 22, "context": "The testing protocol and the results left of MTC+ were presented in [23].", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "The organizers of the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models [13] proposed a challenge to improve performance on a target domain by using labeled and unlabeled", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "The labeled data source was CIFAR-100 [11], which contains 50k 32x32 color images in 100 classes.", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "The unlabeled data source was a collection of 100k 32x32 color images taken from Tiny Images [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "The winner of this challenge used convolutional Spike and Slab Sparse Coding, followed by max pooling and a linear SVM on the pooled features [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 18, "context": "We now show how the Recursive Neural Tensor Network (RNTN) from [19] can be adapted using pseudo-ensembles, and evaluate it on the Stanford Sentiment Treebank (STB) task.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "In the case of recursive/recurrent neural networks this may prove quite useful, as convolving the objective with a Gaussian reduces its curvature, thereby mitigating some problems stemming from ill-conditioned Hessians [15].", "startOffset": 219, "endOffset": 223}, {"referenceID": 18, "context": "RNTN is the original \u201cfull\u201d model presented in [19].", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "PV is the Paragraph Vector model in [12] and DCNN is the Dynamic Convolutional Neural Network model in [10].", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "PV is the Paragraph Vector model in [12] and DCNN is the Dynamic Convolutional Neural Network model in [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 18, "context": "Following the protocol suggested by [19], we measured root-level (i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 18, "context": "Using only `2 regularization on its parameters, our compact RNTN approached the performance of the full RNTN, roughly matching the performance of the second best method tested in [19].", "startOffset": 179, "endOffset": 183}, {"referenceID": 8, "context": "We proposed the notion of a pseudo-ensemble, which captures methods such as dropout [9] and feature noising in linear models [5, 21] that have recently drawn significant attention.", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "We proposed the notion of a pseudo-ensemble, which captures methods such as dropout [9] and feature noising in linear models [5, 21] that have recently drawn significant attention.", "startOffset": 125, "endOffset": 132}, {"referenceID": 20, "context": "We proposed the notion of a pseudo-ensemble, which captures methods such as dropout [9] and feature noising in linear models [5, 21] that have recently drawn significant attention.", "startOffset": 125, "endOffset": 132}], "year": 2014, "abstractText": "We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout [9] in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of [19] into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.", "creator": "LaTeX with hyperref package"}}}