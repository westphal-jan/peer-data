{"id": "1412.5068", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2014", "title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "abstract": "Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100% mis-classification for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial exam- ples, without a significant performance penalty.", "histories": [["v1", "Thu, 11 Dec 2014 23:03:49 GMT  (446kb)", "http://arxiv.org/abs/1412.5068v1", null], ["v2", "Wed, 17 Dec 2014 16:35:05 GMT  (390kb)", "http://arxiv.org/abs/1412.5068v2", null], ["v3", "Tue, 30 Dec 2014 14:14:24 GMT  (414kb)", "http://arxiv.org/abs/1412.5068v3", null], ["v4", "Thu, 9 Apr 2015 21:43:29 GMT  (414kb)", "http://arxiv.org/abs/1412.5068v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["shixiang gu", "luca rigazio"], "accepted": true, "id": "1412.5068"}, "pdf": {"name": "1412.5068.pdf", "metadata": {"source": "CRF", "title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "authors": ["Shixiang Gu"], "emails": ["shane.gu@us.panasonic.com", "luca.rigazio@us.panasonic.com"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.50 68v1 [cs.LG] 1 1D ec"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own without having to stray into another world."}, {"heading": "2 Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Generating Adversarial Examples", "text": "We follow the method outlined in Szegedy et al. (2014b) for generating counter-examples from the neural network of the classifier, with a slight change in computing power. By the same notation, we designate the classifier by f: Rm \u2212 \u2192 {1... k} and the associated continuous loss function by L: Rm \u00b7 {1... k} \u2212 \u2192 R +. Then, for a specific image, x: Rm, whose m pixels are normalized to [0, 1] and the target designation l: (1... k}, we approximate the minimum hostile noise r by optimizing the following problem, since c > 0 and are subject to x + r: minr c | 2 + L (x + r, l) (1). Instead of finding the minimum c by line search per example, we use constant c in evaluating a given dataset and model architecture. We find c so that we have found a sufficiently large data set in the entire order of magnitude of the subject + i, which is subjected to the 99 percent."}, {"heading": "2.2 Datasets and Model Architectures", "text": "We perform our experiments on the basis of the MNIST dataset, using a series of architectures LeCun and Cortes (1998), Krizhevsky and Hinton (2009). Table 1 summarizes the experimental settings, the baseline error rate, the error rate of opposing examples, and the average adversarial distortion. L2 weight decay is applied with \u03bb = 10 \u2212 3, except in Convolutionary layers. For MNIST, ConvNet has two revolutionary layers, a fully bonded layer, and a Softmax layer."}, {"heading": "3 Recovering from Adversarial Examples", "text": "To gain an insight into the properties of hostile noise, we will examine three preprocessing methods aimed at recovering from hostile noise, as illustrated in the following sections."}, {"heading": "3.1 Noise Injection", "text": "Given the minuscule nature of enemy noise, we are investigating a recovery strategy based on additional corruptions in the hope that we can move the input outside the \"blind spot\" of the network that we originally thought was small and localized. We are experimenting with additive Gaussian noise and Gaussian blur. For Gaussian noise, we have calculated predictions over 20 prediction runs to reduce the prediction variance. Gaussian additive noise results are summarized in Figure 1. Results for Gaussian noise and the compromises between the restored adverse examples and the clean examples are, however, classified as one to vary the amount of additive noise that is added only to the input layer or input layer plus all the hidden layers. Gaussian noise results and the compromises between the restored adverse examples and the clean examples are classified as one to vary the amount of additive noise that is added only to the input layer or input layer plus all the hidden layers."}, {"heading": "3.2 Autoencoder", "text": "To assess the structure of opposing noise, we trained a three-layered auto encoder (784- 256-128-256-784 neurons) that feeds the opposing examples back to the original data samples. An important detail is that we also train the model to trace the original training data back to itself, so that the auto encoder preserves the original data when the non-opposing data samples are fed in; this allows us to stack multiple auto encoders. We train the auto encoder using opposing examples from the training set, and test the generalization capabilities of opposing examples from the different model topologies. Table 3 shows the generalization performance of the auto encoders trained on conflicting examples from different models. Columns indicate whose conflicting data the auto encoder is being trained, and rows indicate whose conflicting data the opposing data is testing."}, {"heading": "3.3 Denoising Autoencoder", "text": "A PCS maps corrupt input data to clean input data. At each training session, each pixel in the input data is corrupted by adding independent Gaussian noise with 0 mean and \u03c3 standard deviation. Table 4 summarizes the results indicating that a standard autoencoder that denotes the autoencoder can still restore a significant portion of the hostile noise. In particular, a denosizing auto encoder with \u03c3 = 0.1 Gaussian noise could denode adverse examples almost as well as an auto encoder that denotes actual hostile noise, as shown in Table 3. However, this model also exhibits the same defect as in Section 3.2 that a stacked network is more susceptible to enemies. In this case, this deficiency is likely to arise from an imperfect formation of DAE itself."}, {"heading": "3.4 Discussion", "text": "Our experiments have shown that the opposing noise is relatively robust against local disturbances such as additive Gaussian noise, indicating that the size of the \"blind spots\" is relatively large. In the case of image data, the effect of opposing examples can be significantly reduced by low-pass filtering, such as Gaussian blur, indicating that the opposing noise is mostly in the radio frequency range. Furthermore, the success of the autoencoder and denoting autoencoder experiment shows that the opposing noise has a simple structure that can be easily exploited. A key observation of opposing examples is that they are inevitable and intrinsic characteristics of any forward-facing architecture. In any pre-processing, it is always possible to spread the error signal backwards through the additional functions and find new opposing examples, not only for deterministic pre-processing steps such as Gaussian blur and auto-coder, which result in us achieving the problem of distortion from even minor network outcomes."}, {"heading": "4 Deep Contractive Network", "text": "In this section, we formulate Deep Contractive Network, which imposes a layer-by-layer contractive penalty in an upstream neural network. Layered punishment roughly minimizes the deviation of network outputs in terms of input interference, allowing the trained model to achieve a \"flatness\" around the training points."}, {"heading": "4.1 Contractive autoencoder", "text": "The contractive autoencoder (CAE) is a variant of an auto encoder (AE) with an additional penalty for minimizing the quadratic standard of Jacobian's hidden representation with respect to the input data Rifai et al. (2011b). A standard AE consists of an encoder and a decoder. The encoder assigns the input data to the hidden representation, and the decoder attempts to reconstruct the input from the hidden representation. Formally, the input consists of x-rdx and the hidden representation h-rdh, the encoder parameterizes by dh-dx-matrix-We and the bias vector bh-Rdh-Rdh and the decoder of the decoder parameterized by dx-matrix-matrix-matrix-matrix-matrix-matrix-i-matrix-Wh functions."}, {"heading": "4.2 Deep Contractive Networks", "text": "A Deep Contractive Network (DCN) is a generalization of the contractive autoencoder (CAE) to a forward-facing neural network that outputs y-Rdy with a target t-Rdy. However, for a network with H-hidden layers, fi should denote the function for calculating hidden representation hi-Rdhi on hidden layer i: hi = fi (hi \u2212 1), i = 1... H + 1, h0 = x and hH + 1 = y. Ideally, the model should punish the following target: JDCN (\u03b8) = m-1 (L (i), y (i) + 1... H + 1 (i) x (i) 2) (5) However, such a penalty is mathematically expensive for calculating partial derivatives on each layer in the standard distribution framework."}, {"heading": "4.3 Experiments and results", "text": "The experiments involve applying a contractive penalty to the models in Table 1. The models were trained until they reached approximately the same accuracy as the original models without contractive punishment, and the contractive examples are generated using the method defined in Section 2.1.Table 5. Table 5 shows that contractive punishment successfully increases the minimal distortion of contractive noise. Table 6 shows the comparison of the Deep Contractive Network penalty against stochastic noise increase. Deep Contractive Networks are more robust than a normal neural network trained with Gauss input noise, and can be easily extended by adding Gauss input noise to further increase the minimum distortion of contractive noise."}, {"heading": "4.4 Discussions and future work", "text": "The results show that deep contractive networks can be successfully trained to spread contractivity around the input data through the deep architecture without significant losses in final accuracy. The model can be improved by increasing the layer-by-layer contractive penalty based on Higher-Order Contractive Autoencoders Rifai et al. (2011a) and the marginalized denoization of autoencoders Chen et al. (2014). While this paper uses deep contractive networks as a framework for mitigating effects based on the contradictory examples, they also provide an appropriate framework for studying the inventory display properties learned from deep neural networks. A further study should be conducted to determine the loss of performance due to layer-by-layer penalties as opposed to global contractive targets as defined in equality. 5 Furthermore, the research of non-euclidean counterexamples, such as small affine transformations on network, should lead to higher learning layers in order to previously varying the network characteristics."}, {"heading": "5 Conclusions", "text": "We tested several denocialization architectures to reduce the impact of the opposing examples, and concluded that while the simple and stable structure of the opposing examples makes it easy to remove them with autoencoders, the resulting stacked network is even more sensitive to new opposing examples. We conclude that the sensitivity of neural networks to opposing examples is more related to intrinsic deficiencies in the training sequence and objective function than to modeling the topology. The point of the problem is then to find a suitable training method and objective function that can efficiently let the network learn flat, invariant regions around the training data. We propose Deep Contractive Networks to explicitly learn invariant features at each level and show some positive initial results."}, {"heading": "Acknowledgments", "text": "The authors thank Nitish Srivastava for the publication of his DeepNet Library and Volodymyr Mnih for his CUDAMat Library Mnih (2009)."}], "references": [{"title": "What regularized auto-encoders learn from the data", "author": ["G. Alain", "Y. Bengio"], "venue": null, "citeRegEx": "Alain and Bengio,? \\Q2012\\E", "shortCiteRegEx": "Alain and Bengio", "year": 2012}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["M. Chen", "K. Weinberger", "F. Sha", "Y. Bengio"], "venue": "JMLR, 32(1):1476\u20131484.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30\u201342.", "citeRegEx": "Dahl et al\\.,? 2012", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Recurrent excitation in neocortical circuits", "author": ["R.J. Douglas", "C. Koch", "M. Mahowald", "K. Martin", "H.H. Suarez"], "venue": "Science, 269(5226):981\u2013985.", "citeRegEx": "Douglas et al\\.,? 1995", "shortCiteRegEx": "Douglas et al\\.", "year": 1995}, {"title": "Avoiding pathologies in very deep networks", "author": ["D. Duvenand", "O. Rippel", "R.P. Adams", "Z. Ghahramani"], "venue": "arXiv preprint arXiv:1402.5836.", "citeRegEx": "Duvenand et al\\.,? 2014", "shortCiteRegEx": "Duvenand et al\\.", "year": 2014}, {"title": "Distributed hierarchical processing in the primate cerebral cortex", "author": ["D.J. Felleman", "D.C. Van Essen"], "venue": "Cerebral cortex, 1(1):1\u201347.", "citeRegEx": "Felleman and Essen,? 1991", "shortCiteRegEx": "Felleman and Essen", "year": 1991}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN 2011, pages 44\u201351. Springer.", "citeRegEx": "Hinton et al\\.,? 2011", "shortCiteRegEx": "Hinton et al\\.", "year": 2011}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department, University of Toronto, Tech. Rep.", "citeRegEx": "Krizhevsky and Hinton,? 2009", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, volume 1, page 4.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes", "year": 1998}, {"title": "Cudamat: a cuda-based matrix class for python", "author": ["V. Mnih"], "venue": "Department of Computer Science, University of Toronto, Tech. Rep. UTML TR, 4.", "citeRegEx": "Mnih,? 2009", "shortCiteRegEx": "Mnih", "year": 2009}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "CoRR, abs/1406.6247.", "citeRegEx": "Mnih et al\\.,? 2014", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Higher order contractive auto-encoder", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 645\u2013660. Springer.", "citeRegEx": "Rifai et al\\.,? 2011a", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 833\u2013840.", "citeRegEx": "Rifai et al\\.,? 2011b", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["M. Stollenga", "J. Masci", "F. Gomez", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1407.3068.", "citeRegEx": "Stollenga et al\\.,? 2014", "shortCiteRegEx": "Stollenga et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842.", "citeRegEx": "Szegedy et al\\.,? 2014a", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "ICLR.", "citeRegEx": "Szegedy et al\\.,? 2014b", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1701\u20131708.", "citeRegEx": "Taigman et al\\.,? 2013", "shortCiteRegEx": "Taigman et al\\.", "year": 2013}, {"title": "Deep learning using support vector machines", "author": ["Y. Tang"], "venue": "arXiv preprint arXiv:1306.0239.", "citeRegEx": "Tang,? 2013", "shortCiteRegEx": "Tang", "year": 2013}, {"title": "Learning with recursive perceptual representations", "author": ["O. Vinyals", "Y. Jia", "L. Deng", "T. Darrell"], "venue": "Advances in Neural Information Processing Systems, pages 2825\u20132833.", "citeRegEx": "Vinyals et al\\.,? 2012", "shortCiteRegEx": "Vinyals et al\\.", "year": 2012}, {"title": "Panda: Pose aligned networks for deep attribute modeling", "author": ["N. Zhang", "M. Paluri", "M. Ranzato", "T. Darrell", "L. Bourdev"], "venue": "arXiv preprint arXiv:1311.5591.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "Deep neural networks have recently led to significant improvement in countless areas of machine learning, from speech recognition to computer vision Krizhevsky et al. (2012); Dahl et al.", "startOffset": 149, "endOffset": 174}, {"referenceID": 2, "context": "(2012); Dahl et al. (2012); Taigman et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 2, "context": "(2012); Dahl et al. (2012); Taigman et al. (2013); Zhang et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 2, "context": "(2012); Dahl et al. (2012); Taigman et al. (2013); Zhang et al. (2013). DNNs achieve high performance because deep cascades of nonlinear units allow to generalize non-locally, in data-specific manifolds Bengio (2009).", "startOffset": 8, "endOffset": 71}, {"referenceID": 2, "context": "(2012); Dahl et al. (2012); Taigman et al. (2013); Zhang et al. (2013). DNNs achieve high performance because deep cascades of nonlinear units allow to generalize non-locally, in data-specific manifolds Bengio (2009). While this ability to automatically learn non-local generalization priors from data is a strength of DNNs, it also creates counter-intuitive properties.", "startOffset": 8, "endOffset": 217}, {"referenceID": 2, "context": "(2012); Dahl et al. (2012); Taigman et al. (2013); Zhang et al. (2013). DNNs achieve high performance because deep cascades of nonlinear units allow to generalize non-locally, in data-specific manifolds Bengio (2009). While this ability to automatically learn non-local generalization priors from data is a strength of DNNs, it also creates counter-intuitive properties. In particular Szegedy et al. (2014b) showed in their seminal paper that one can engineer small perturbations to the input data, called adversarial examples, that make an otherwise high-performing DNN misclassify every example.", "startOffset": 8, "endOffset": 408}, {"referenceID": 2, "context": "(2012); Dahl et al. (2012); Taigman et al. (2013); Zhang et al. (2013). DNNs achieve high performance because deep cascades of nonlinear units allow to generalize non-locally, in data-specific manifolds Bengio (2009). While this ability to automatically learn non-local generalization priors from data is a strength of DNNs, it also creates counter-intuitive properties. In particular Szegedy et al. (2014b) showed in their seminal paper that one can engineer small perturbations to the input data, called adversarial examples, that make an otherwise high-performing DNN misclassify every example. For image datasets, such perturbations are often imperceptible to the human eye, thus creating potential vulnerabilities when deploying neural networks in real environments. As an example, one could envision situations where an attacker having knowledge of the DNN parameters could use adversarial examples to attack the system and make it fail consistently. Even worse, due to the cross-model, cross-dataset generalization properties of the adversarial examples Szegedy et al. (2014b) , the attacker might generate adversarial examples from independent models without full knowledge of the system and still be able to conduct a highly successful attack.", "startOffset": 8, "endOffset": 1084}, {"referenceID": 2, "context": "(2012); Dahl et al. (2012); Taigman et al. (2013); Zhang et al. (2013). DNNs achieve high performance because deep cascades of nonlinear units allow to generalize non-locally, in data-specific manifolds Bengio (2009). While this ability to automatically learn non-local generalization priors from data is a strength of DNNs, it also creates counter-intuitive properties. In particular Szegedy et al. (2014b) showed in their seminal paper that one can engineer small perturbations to the input data, called adversarial examples, that make an otherwise high-performing DNN misclassify every example. For image datasets, such perturbations are often imperceptible to the human eye, thus creating potential vulnerabilities when deploying neural networks in real environments. As an example, one could envision situations where an attacker having knowledge of the DNN parameters could use adversarial examples to attack the system and make it fail consistently. Even worse, due to the cross-model, cross-dataset generalization properties of the adversarial examples Szegedy et al. (2014b) , the attacker might generate adversarial examples from independent models without full knowledge of the system and still be able to conduct a highly successful attack. This indicates there is still a significant robustness gap between machine and human perception, despite recent results showing machine vision performance closing in on human performance Taigman et al. (2013). More formally, the challenge is: can we design and train a deep network that not only generalizes in abstract manifold space to achieve good recognition accuracy, but also retains local generalization in the input space?", "startOffset": 8, "endOffset": 1462}, {"referenceID": 14, "context": "A main result from Szegedy et al. (2014b) is that the smoothness assumption that underlies many kernel methods such as Support Vector Machines (SVMs) does not hold for deep neural networks trained through backpropagation.", "startOffset": 19, "endOffset": 42}, {"referenceID": 14, "context": "A main result from Szegedy et al. (2014b) is that the smoothness assumption that underlies many kernel methods such as Support Vector Machines (SVMs) does not hold for deep neural networks trained through backpropagation. This points to a possible inherent instability in all deterministic, feed-forward neural network architectures. In practice, SVMs can be used to replace the final softmax layer in classifier neural networks leading to better generalization Tang (2013), but applying SVM in the manifold space does not guarantee local generalization in the input space.", "startOffset": 19, "endOffset": 474}, {"referenceID": 4, "context": "Recently, Duvenand et al. (2014) categorize distributions of deep neural networks through deep Gaussian Process (GP) and show that in stacked architectures, the capacity of the network captures fewer degrees of freedom as the layers increase.", "startOffset": 10, "endOffset": 33}, {"referenceID": 12, "context": "A framework leveraging both approaches is Random Recursive SVM (RSVM) Vinyals et al. (2012), which recursively solves a SVM whose input combines input data and outputs from the previous SVM layer, randomly projected to the same dimension as the input data.", "startOffset": 70, "endOffset": 92}, {"referenceID": 12, "context": "A framework leveraging both approaches is Random Recursive SVM (RSVM) Vinyals et al. (2012), which recursively solves a SVM whose input combines input data and outputs from the previous SVM layer, randomly projected to the same dimension as the input data. RSVM avoids solving nonconvex optimization by recursively solving a SVM and demonstrates generalization on small datasets. However, performance is suboptimal compared to state-of-the-art DNNs, possibly due to lack of end-to-end training Vinyals et al. (2012); Tang (2013).", "startOffset": 70, "endOffset": 516}, {"referenceID": 12, "context": "(2012); Tang (2013). Another work inspired by the recursive nature of the human perceptual system is Deep Attention Selective Network (dasNet) Stollenga et al.", "startOffset": 8, "endOffset": 20}, {"referenceID": 11, "context": "Another work inspired by the recursive nature of the human perceptual system is Deep Attention Selective Network (dasNet) Stollenga et al. (2014), which dynamically fine-tunes the weight of each convolutional filter at recognition time.", "startOffset": 122, "endOffset": 146}, {"referenceID": 11, "context": "Another work inspired by the recursive nature of the human perceptual system is Deep Attention Selective Network (dasNet) Stollenga et al. (2014), which dynamically fine-tunes the weight of each convolutional filter at recognition time. We speculate that the robustness of human perception is due to complex hierarchies and recursions in the wirings of the human brain Felleman and Van Essen (1991); Douglas et al.", "startOffset": 122, "endOffset": 399}, {"referenceID": 3, "context": "We speculate that the robustness of human perception is due to complex hierarchies and recursions in the wirings of the human brain Felleman and Van Essen (1991); Douglas et al. (1995), since recursions provide multiple paths to input data and could retain locality information at multiple levels of representation.", "startOffset": 163, "endOffset": 185}, {"referenceID": 3, "context": "We speculate that the robustness of human perception is due to complex hierarchies and recursions in the wirings of the human brain Felleman and Van Essen (1991); Douglas et al. (1995), since recursions provide multiple paths to input data and could retain locality information at multiple levels of representation. Such an intuition is also partially supported by the recent state-ofthe-art models for object classification and detection involving multi-scale processing Szegedy et al. (2014a). Since modeling such recursions in DNNs is notoriously hard and often relies on additional techniques such as reinforcement learning Stollenga et al.", "startOffset": 163, "endOffset": 495}, {"referenceID": 3, "context": "We speculate that the robustness of human perception is due to complex hierarchies and recursions in the wirings of the human brain Felleman and Van Essen (1991); Douglas et al. (1995), since recursions provide multiple paths to input data and could retain locality information at multiple levels of representation. Such an intuition is also partially supported by the recent state-ofthe-art models for object classification and detection involving multi-scale processing Szegedy et al. (2014a). Since modeling such recursions in DNNs is notoriously hard and often relies on additional techniques such as reinforcement learning Stollenga et al. (2014); Mnih et al.", "startOffset": 163, "endOffset": 652}, {"referenceID": 3, "context": "We speculate that the robustness of human perception is due to complex hierarchies and recursions in the wirings of the human brain Felleman and Van Essen (1991); Douglas et al. (1995), since recursions provide multiple paths to input data and could retain locality information at multiple levels of representation. Such an intuition is also partially supported by the recent state-ofthe-art models for object classification and detection involving multi-scale processing Szegedy et al. (2014a). Since modeling such recursions in DNNs is notoriously hard and often relies on additional techniques such as reinforcement learning Stollenga et al. (2014); Mnih et al. (2014), we will at first investigate explicit inclusion of input generalization as an additional objective for the standard DNN training process.", "startOffset": 163, "endOffset": 672}, {"referenceID": 15, "context": "It is important to note that the adversarial examples are universal and unavoidable by their definition: one could always engineer an additive noise at input to make the model misclassify an example, and it is also a problem in shallow models such as logistic regression Szegedy et al. (2014b). The question is how much noise is needed to make the model misclassify an otherwise correct example.", "startOffset": 271, "endOffset": 294}, {"referenceID": 11, "context": "In this paper we investigate new training procedures such that the adversarial examples generated based on Szegedy et al. (2014b) have higher distortion, where distortion is measured by 1 n \u2211 (x\u2032i \u2212 xi) 2 where x, x \u2208 R are the adversarial data and original data respectively.", "startOffset": 107, "endOffset": 130}, {"referenceID": 11, "context": "In this paper we investigate new training procedures such that the adversarial examples generated based on Szegedy et al. (2014b) have higher distortion, where distortion is measured by 1 n \u2211 (x\u2032i \u2212 xi) 2 where x, x \u2208 R are the adversarial data and original data respectively. First, we investigate the structure of the adversarial examples, and show that contrary to their small distortion it is difficult to recover classification performance through additional perturbations, such as Gaussian additive noises and Gaussian blur. This suggests the size of \u201cblind-spots\u201d are in fact relatively large, in input space volume, and locally continuous. We also show that adversarial examples are quite similar Szegedy et al. (2014b), and an autoencoder (AE) trained to denoise adversarial examples from one network generalizes well to denoise adversarials generated from different architectures.", "startOffset": 107, "endOffset": 728}, {"referenceID": 10, "context": "We find that ideas from denoising autoencoder (DAE), contractive autoencoder (CAE), and most recently marginalized denoising autoencoder (mDAE) provide strong framework for training neural networks that are robust against adversarial noises Rifai et al. (2011b); Alain and Bengio (2012); Chen et al.", "startOffset": 241, "endOffset": 262}, {"referenceID": 0, "context": "(2011b); Alain and Bengio (2012); Chen et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 0, "context": "(2011b); Alain and Bengio (2012); Chen et al. (2014). We propose Deep Contractive Networks (DCNs), which incorporate a layer-wise contractive penalty, and show that adversarials generated from such networks have significantly higher distortion.", "startOffset": 9, "endOffset": 53}, {"referenceID": 15, "context": "We follow the procedure outlined in Szegedy et al. (2014b) for generating adversarial examples from the classifier neural network, with a slight modification for computational efficiency.", "startOffset": 36, "endOffset": 59}, {"referenceID": 8, "context": "We perform our experiements on the MNIST dataset, using a number of architectures LeCun and Cortes (1998); Krizhevsky and Hinton (2009).", "startOffset": 82, "endOffset": 106}, {"referenceID": 7, "context": "We perform our experiements on the MNIST dataset, using a number of architectures LeCun and Cortes (1998); Krizhevsky and Hinton (2009). Table 1 summarizes experimental settings, baseline error rate, adversarial examples\u2019 error rate and average adversarial distortion.", "startOffset": 107, "endOffset": 136}, {"referenceID": 12, "context": "Contractive autoencoder (CAE) is a variant of an autoencoder (AE) with an additional penalty for minimizing the squared norm of the Jacobian of the hidden representation with respect to input data Rifai et al. (2011b). A standard AE consists of an encoder and a decoder.", "startOffset": 197, "endOffset": 218}, {"referenceID": 0, "context": "5 Alain and Bengio (2012). However, such stochastic penalties require many passes of data to train the model effectively.", "startOffset": 2, "endOffset": 26}, {"referenceID": 0, "context": "5 Alain and Bengio (2012). However, such stochastic penalties require many passes of data to train the model effectively. For efficiency, we decided to employ a deterministic penalty instead Rifai et al. (2011b); Alain and Bengio (2012); Chen et al.", "startOffset": 2, "endOffset": 212}, {"referenceID": 0, "context": "5 Alain and Bengio (2012). However, such stochastic penalties require many passes of data to train the model effectively. For efficiency, we decided to employ a deterministic penalty instead Rifai et al. (2011b); Alain and Bengio (2012); Chen et al.", "startOffset": 2, "endOffset": 237}, {"referenceID": 0, "context": "5 Alain and Bengio (2012). However, such stochastic penalties require many passes of data to train the model effectively. For efficiency, we decided to employ a deterministic penalty instead Rifai et al. (2011b); Alain and Bengio (2012); Chen et al. (2014).", "startOffset": 2, "endOffset": 257}, {"referenceID": 11, "context": "The model can be improved by augmenting the layer-wise contractive penalty based on Higher-Order Contractive autoencoders Rifai et al. (2011a), and marginalized Denoising autoencoders Chen et al.", "startOffset": 122, "endOffset": 143}, {"referenceID": 6, "context": "For example, explicitly learning instantiation parameters as previously attempted by models such as Transforming Autoencoder Hinton et al. (2011).", "startOffset": 125, "endOffset": 146}, {"referenceID": 8, "context": "Recent progress in deep neural networks is driven by both end-to-end supervised training and various modes of unsupervised feature learning Krizhevsky et al. (2012); Bengio (2009), and thus we believe the merge of the two could likely enable new milestones in the field.", "startOffset": 140, "endOffset": 165}, {"referenceID": 8, "context": "Recent progress in deep neural networks is driven by both end-to-end supervised training and various modes of unsupervised feature learning Krizhevsky et al. (2012); Bengio (2009), and thus we believe the merge of the two could likely enable new milestones in the field.", "startOffset": 140, "endOffset": 180}, {"referenceID": 10, "context": "The authors thank Nitish Srivastava for releasing his DeepNet Library, and Volodymyr Mnih for his CUDAMat Library Mnih (2009).", "startOffset": 85, "endOffset": 126}], "year": 2017, "abstractText": "Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100% mis-classification for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. However, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial examples, without a significant performance penalty.", "creator": "LaTeX with hyperref package"}}}