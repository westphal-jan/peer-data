{"id": "1412.5335", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews", "abstract": "Sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document (typically a consumer review). In the simplest settings, we discriminate only between positive and negative sentiment, turning the task into a standard binary classification problem. We compare several ma- chine learning approaches to this problem, and combine them to achieve the best possible results. We show how to use for this task the standard generative lan- guage models, which are slightly complementary to the state of the art techniques. We achieve strong results on a well-known dataset of IMDB movie reviews. Our results are easily reproducible, as we publish also the code needed to repeat the experiments. This should simplify further advance of the state of the art, as other researchers can combine their techniques with ours with little effort.", "histories": [["v1", "Wed, 17 Dec 2014 11:02:04 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v1", null], ["v2", "Thu, 18 Dec 2014 14:17:16 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v2", null], ["v3", "Fri, 19 Dec 2014 11:36:14 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v3", null], ["v4", "Tue, 3 Feb 2015 20:03:35 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v4", null], ["v5", "Wed, 4 Feb 2015 05:17:55 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v5", null], ["v6", "Thu, 16 Apr 2015 14:26:14 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v6", null], ["v7", "Wed, 27 May 2015 06:40:09 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v7", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG cs.NE", "authors": ["gr\\'egoire mesnil", "tomas mikolov", "marc'aurelio ranzato", "yoshua bengio"], "accepted": true, "id": "1412.5335"}, "pdf": {"name": "1412.5335.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 141 2.53 35v1 [cs.CL] 1 7"}, {"heading": "1 INTRODUCTION", "text": "Mood analysis is one of the most popular, simple, and useful tasks in processing natural language. It aims to predict the setting of the text, typically a sentence or review. For example, movies or restaurants are often rated with a certain number of stars indicating the degree to which the reviewer was satisfied. In the simplest settings, this task can often be considered one of the simplest in NLP, because basic machine learning techniques can provide strong baselines (Wang & Manning, 2012), often exceeding more complex approaches (Socher et al., 2011). In the simplest settings, this task can be considered a binary classification between positive and negative sentimentalities. However, there are several challenges in achieving the best possible accuracy. It is not obvious how to represent variable length documents beyond simple word order information that lose word order information. Advanced machine learning techniques such as recurring neural networks (we use in 2010 and their ensembles) can be compared with different ensembles (we use them in 2010)."}, {"heading": "2 DESCRIPTION OF THE MODELS", "text": "In this section, we describe in detail the approaches we considered in our study, the novelty of which is to combine both generative and discriminatory models for predicting sentimentality."}, {"heading": "2.1 GENERATIVE MODEL", "text": "\"It's because,\" he says, \"we're able to be able to be able to be able to be able to be able to be able to hide.\""}, {"heading": "2.2 LINEAR CLASSIFICATION OF WEIGHTED N-GRAM FEATURES", "text": "Among the purely discriminatory methods, the most popular choice is a linear classifier above a Bagof-Word representation of the document. Input representation is usually a tf-idf weighted word count of the document. To maintain the local order of the words, a better representation would also take into account the location-independent n-gram count of the document (Bag-of-n-gram).In our overall ensemble we used a supervised reweighting of word counts as in the Naive Bayes Support Vector Machine (NB-SVM) approach (Wang & Manning, 2012).This approach calculates a log ratio vector between the average word counts extracted from positive documents and the average word counts extracted from negative documents. Input into the logistic regression classifier corresponds to the log ratio vector multiplied by the binary pattern for each word in the document sector. Note that Manning's regression performance can be improved by 0.1."}, {"heading": "2.3 SENTENCE VECTORS", "text": "The basic idea is to learn a compact representation of a word or paragraph by predicting nearby words in a fixed context window. Here, coexistence statistics are gathered and embedding words and paragraphs that capture rich semantics are learned. Synonyms and similar paragraphs are often surrounded by a similar context and are therefore mapped into nearby feature vectors (and vice versa). Such embedding can then be used to represent a new document (e.g. by averaging the representations of the paragraphs that make up the document) via a fixed-size vector. Authors then use such a document descriptor as an input into a single-layer neural network for mood discrimination."}, {"heading": "2.4 MODEL ENSEMBLE", "text": "In this thesis we combine the log probability values of the above models by linear interpolation. Formally, we define the general probability value as the weighted geometric mean of the base models: p (y = + + 1 | x) = p (y = + 1 | x) \u03b1k, with \u03b1k > 0.We find the best weight adjustment using Brute Force Grid search by quantifying the coefficient values in the interval [0, 1] in steps of 0.1. The search is evaluated using a validation set to avoid overfitting. We do not focus on a smarter method to find \u03b1, as we only consider 3 models in our approach and consider it outside the scope of this thesis. Using other models would make the use of this method prohibitive. For a larger number of models, a random search for \u03b1 coefficients or even Bayesian approaches could be considered, as these techniques offer a better runtime."}, {"heading": "3 RESULTS", "text": "In this section we report on the results of one of the largest publicly available sentiment analysis datasets, the IMDB Dataset of Movie Reviews. The dataset consists of 50,000 movie reviews categorized either positive or negative. We use 25,000 reviews for training and the rest for1https: / / github.com / mesnilgr / nbsvmEnsemble Accuracy RNN-LM + NB SVM Trigram 92.11% RNN-LM + Sentence Vectors 91.68% Sentence Vectors + NB-SVM Trigrams 92.46% All 92.77% State of the Art 92.58% Testing, using the same protocol proposed by (Maas et al., 2011). All experiments can be reproduced using the code available at https: / github.com / mesnilgr / iclr15.Table 2 Reports on the results of each model."}, {"heading": "4 CONCLUSION", "text": "We have proposed a very simple but powerful ensemble system for sentiment analysis. We combine three rather complementary and conceptually different basic models: one based on a generative approach (language models), one based on continuous sentence representations, and one based on a clever reweighting of the tf-idf wordbag representation of the document. Each such model contributes to the success of the overall system by achieving the new, state-of-the-art results on the sophisticated IMDB film review dataset. Code to reproduce our experiments is available at: https: / / github.com / mesnilgr / iclr15. We hope that researchers will use our code to incorporate their new findings into our ensemble and focus on improving the state of the art technology for sentiment analysis.2We have not been able to reproduce the accuracy reported by Le et al.Model Sentences (positively) that the film Nilopal Verma is truly realistic."}], "references": [{"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Reinhard", "Ney", "Hermann"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas", "Andrew L", "Daly", "Raymond E", "Pham", "Peter T", "Huang", "Dan", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Statistical language models based on neural networks", "author": ["Mikolov", "Tom\u00e1\u0161"], "venue": "PhD thesis,", "citeRegEx": "Mikolov and Tom\u00e1\u0161.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tom\u00e1\u0161.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "Foundations and trends in information retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher", "Richard", "Pennington", "Jeffrey", "Huang", "Eric", "Ng", "Andrew", "Manning", "Christopher D"], "venue": "Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Stolcke", "Andreas"], "venue": "In INTERSPEECH,", "citeRegEx": "Stolcke and Andreas,? \\Q2002\\E", "shortCiteRegEx": "Stolcke and Andreas", "year": 2002}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Wang", "Sida", "Manning", "Christopher D"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "This task is often considered as one of the simplest in NLP because basic machine learning techniques can yield strong baselines (Wang & Manning, 2012), often beating much more intricate approaches (Socher et al., 2011).", "startOffset": 198, "endOffset": 219}, {"referenceID": 4, "context": "One can use advanced machine learning techniques such as recurrent neural networks and their variations (Mikolov et al., 2010; Socher et al., 2011), however it is not clear if these provide any significant gain over simple bag-of-words and bag-of-ngram techniques (Pang & Lee, 2008; Wang & Manning, 2012).", "startOffset": 104, "endOffset": 147}, {"referenceID": 7, "context": "One can use advanced machine learning techniques such as recurrent neural networks and their variations (Mikolov et al., 2010; Socher et al., 2011), however it is not clear if these provide any significant gain over simple bag-of-words and bag-of-ngram techniques (Pang & Lee, 2008; Wang & Manning, 2012).", "startOffset": 104, "endOffset": 147}, {"referenceID": 4, "context": "In contrast with N-grams languages models, Recurrent neural networks (RNNs) (Mikolov et al., 2010) are parametric models that can address these issues.", "startOffset": 76, "endOffset": 98}, {"referenceID": 6, "context": "We know that in practice, the context window is limited due to exploding and vanishing gradients (Pascanu et al., 2012).", "startOffset": 97, "endOffset": 119}, {"referenceID": 2, "context": "testing, using the same protocol proposed by (Maas et al., 2011).", "startOffset": 45, "endOffset": 64}], "year": 2016, "abstractText": "Sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document (typically a consumer review). In the simplest settings, we discriminate only between positive and negative sentiment, turning the task into a standard binary classification problem. We compare several machine learning approaches to this problem, and combine them to achieve the best possible results. We show how to use for this task the standard generative language models, which are slightly complementary to the state of the art techniques. We achieve strong results on a well-known dataset of IMDB movie reviews. Our results are easily reproducible, as we publish also the code needed to repeat the experiments. This should simplify further advance of the state of the art, as other researchers can combine their techniques with ours with little effort.", "creator": "LaTeX with hyperref package"}}}