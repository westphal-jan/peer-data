{"id": "1412.5474", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Flattened Convolutional Neural Networks for Feedforward Acceleration", "abstract": "We present flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks has been extensively studied and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested flattened model on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained.", "histories": [["v1", "Wed, 17 Dec 2014 16:48:54 GMT  (136kb,D)", "https://arxiv.org/abs/1412.5474v1", null], ["v2", "Fri, 27 Feb 2015 20:36:05 GMT  (188kb,D)", "http://arxiv.org/abs/1412.5474v2", null], ["v3", "Mon, 6 Apr 2015 01:40:08 GMT  (183kb,D)", "http://arxiv.org/abs/1412.5474v3", "International Conference on Learning Representations (ICLR) 2015"], ["v4", "Fri, 20 Nov 2015 05:50:23 GMT  (183kb,D)", "http://arxiv.org/abs/1412.5474v4", "International Conference on Learning Representations (ICLR) 2015"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["jonghoon jin", "aysegul dundar", "eugenio culurciello"], "accepted": true, "id": "1412.5474"}, "pdf": {"name": "1412.5474.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["FEEDFORWARD ACCELERATION", "Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "emails": ["euge}@purdue.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recent success in the rapid implementation of conventional neural networks (CNNs), and new techniques such as dropout, allow researchers to train large networks that were previously impossible, and these large CNNs show great potential for visual and auditory understanding, which makes them useful for applications in autonomous robots, security systems, mobile phones, and wearables. These applications require networks with a high degree of accuracy, but also networks that can be run in real time. However, CNNs are very expensive and require high-performance servers (GPUs).To speed up the forward and reverse cycles of CNNs, there has been extensive work to efficiently implement CNNs on GPUs (Krizhevsky et al., 2014) and CPUs (GPUs)."}, {"heading": "2 RELATED WORK", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "3 FLATTENING CONVOLUTION FILTERS", "text": "Similar to the notation used in Denton et al. (2014), weights in CNNs can be described as 4- dimensional filters: W-RC \u00b7 X \u00b7 Y \u00b7 F, C is the number of input channels, X and Y are the spatial dimensions of the filter, and F is the number of filters or the number of output channels. The configuration for each channel output requires a filter W-RC \u00b7 X \u00b7 Y and is described as preload (x, y \u2212) Wf (c, x \u2212 \u00b2, y \u2212) Wf (c, x \u2212), x \u00b2, y \u2212 (1) assuming a step of f is an index of the output channel Y, I-RC \u00b7 N \u00b7 M \u00b7 F is the input channel filter, N \u00b7 M \u00b7 F \u00b7 f: the spatial dimensions of the input requirements."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "We tested the performance of the proposed model in accordance with a base model of CNNs in various classification tasks. In experiments, we used the Torch7 environment (Collobert et al., 2011) to demonstrate the performance of the model and perform custom gradient updates."}, {"heading": "4.1 TRAINING BASELINE MODEL", "text": "We choose a CNN model architecture such as the one used in Srivastava & Salakhutdinov (2013) with a smaller multi-layer perceptron. We keep the structure of the CNNs generic in order to minimize unwanted interruptions caused by hidden variables and to make comparison with flattened models transparent. The model consists of 3 convolutionary layers with 5 x 5 filters and two-level multi-layer perceptron. The number of revolutionary filters in each layer is 96, 128 and 256, each layer comprising a linear rectifier unit and a max pooling with sizes 2 and steps of 3. The two fully interconnected multi-layer perceptrons each have 256 units. The model is regulated with five failure layers with the probability of 0.1, 0.25, 0.5, 0.5 and 0.5 layers from lower to higher layer to prevent co-adjustment of the characteristics.In our tests we did not use data augmentation, 0.5, 25, 0.5, and 0.25, 0.25 respectively."}, {"heading": "4.2 TRAINING FLATTENED MODEL", "text": "USA, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "4.3 PARAMETER REDUCTION", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "4.5 CLASSIFICATION ACCURACY", "text": "Despite the reduced number of parameters, the flattened model does not suffer from loss of accuracy. We tested the model with datasets of 32 x 32 RGB images, which are often used as standard measurement to evaluate classification capability. CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) have 10 classes, while CIFAR-100 has 100 classes. Both datasets consist of 50,000 training images and 10,000 test images. Prior to use, datasets with contrast normalization per image are pre-processed, followed by ZCA brightening on the entire dataset as Goodfellow et al. (2013) Both models achieve the same performance, but in our experiments the accuracy of the flattened model slightly exceeds the data model as shown in Table 2.1The flattening is not applied to the first layer image."}, {"heading": "4.6 ACCELERATION", "text": "Due to the reduced parameters in 1D folding pipelines, the flattened structure reduces the computing requirements of the CNNs, thereby accelerating both the forward and backward computation of the CNNs. Profile results of the flattened and basal models for forward and backward propagation are shown in Figure 7. We performed the same tests on the CPU and GPU to check the performance of the flattened structure against the different degree of parallelism. Performance was measured on Intel i7 3.3GHz CPU and NVIDIA Tesla K-40 GPU. Different image sizes from 16 x 16 to 80 x 80 with 128 batch were applied to the models."}, {"heading": "5 CONCLUSION", "text": "In this thesis, we propose a flattening technique to reduce the number of parameters in Convolutionary Neural Networks for forward acceleration. We transform each of the revolutionary layers of the 3Dconvolution pipeline into a sequence of 1D turns across channels, vertical and horizontal directions during the training phase. We found that the model shaped in 1D structure is capable of successfully learning 1D filters and achieves approximately twice the speed of evaluation compared to the base model. Furthermore, the flattened Convolutionary Networks achieve similar or better accuracy on CIFAR-10, CIFAR-100 and MNIST with ten times fewer parameters. Furthermore, the proposed method does not require any effort in manual tuning or post-processing once the model is trained, and it circumvents the difficulties in solving the optimization problem to learn 1D filters."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by grants from the Office of Naval Research (ONR) 14PR02106-01 P00004 and MURI N000141010278. We are pleased to support NVIDIA Corporation by donating GPUs used for this research."}], "references": [{"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "arXiv preprint arXiv:1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, Neural Information Processing Systems Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "An analysis of the connections between layers of deep neural networks", "author": ["Culurciello", "Eugenio", "Jin", "Jonghoon", "Dundar", "Aysegul", "Bates", "Jordan"], "venue": "arXiv preprint arXiv:1306.0152,", "citeRegEx": "Culurciello et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Culurciello et al\\.", "year": 2013}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "de Freitas", "Nando"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily L", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-10),", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir D"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "An efficient implementation of deep convolutional neural networks on a mobile coprocessor", "author": ["Jin", "Jonghoon", "Gokhale", "Vinayak", "Dundar", "Aysegul", "Krishnamurthy", "Bharadwaj", "Martini", "Berin", "Culurciello", "Eugenio"], "venue": "In Circuits and Systems (MWSCAS),", "citeRegEx": "Jin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["Lebedev", "Vadim", "Ganin", "Yaroslav", "Rakhuba", "Maksim", "Oseledets", "Ivan V", "Lempitsky", "Victor S"], "venue": "arXiv preprint arXiv:1412.6553,", "citeRegEx": "Lebedev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Effiicient backprop. In Neural Networks: Tricks of the Trade, This Book is an Outgrowth", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "NIPS Workshop,", "citeRegEx": "LeCun et al\\.,? \\Q1996\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1996}, {"title": "Efficient sparse coding algorithms", "author": ["Lee", "Honglak", "Battle", "Alexis", "Raina", "Rajat", "Ng", "Andrew Y"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura", "B. Brezzo", "I. Vo", "S.K. Esser", "R. Appuswamy", "B. Taba", "A. Amir", "M.D. Flickner", "W.P. Risk", "R. Manohar", "D.S. Modha"], "venue": null, "citeRegEx": "Merolla et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merolla et al\\.", "year": 2014}, {"title": "Kernel analysis of deep networks", "author": ["Montavon", "Gr\u00e9goire", "Braun", "Mikio", "M\u00fcller", "Klaus-Robert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Montavon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Montavon et al\\.", "year": 2011}, {"title": "Learning separable filters", "author": ["R. Rigamonti", "A. Sironi", "V. Lepetit", "P. Fua"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Rigamonti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rigamonti et al\\.", "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Discriminative transfer learning with tree-based priors", "author": ["Srivastava", "Nitish", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Improving the speed of neural networks on CPUs", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 11, "context": "To accelerate forward and backward passes of CNNs, there has been extensive work for efficient implementation of CNNs on GPUs (Krizhevsky et al., 2012; Chetlur et al., 2014) and CPUs (Vanhoucke et al.", "startOffset": 126, "endOffset": 173}, {"referenceID": 0, "context": "To accelerate forward and backward passes of CNNs, there has been extensive work for efficient implementation of CNNs on GPUs (Krizhevsky et al., 2012; Chetlur et al., 2014) and CPUs (Vanhoucke et al.", "startOffset": 126, "endOffset": 173}, {"referenceID": 23, "context": ", 2014) and CPUs (Vanhoucke et al., 2011), including linear quantization of network weights and inputs.", "startOffset": 17, "endOffset": 41}, {"referenceID": 9, "context": "Because this is not the case always, custom architectures have been explored for power and speed efficient implementation of CNNs (Jin et al., 2014; Merolla et al., 2014).", "startOffset": 130, "endOffset": 170}, {"referenceID": 16, "context": "Because this is not the case always, custom architectures have been explored for power and speed efficient implementation of CNNs (Jin et al., 2014; Merolla et al., 2014).", "startOffset": 130, "endOffset": 170}, {"referenceID": 0, "context": ", 2012; Chetlur et al., 2014) and CPUs (Vanhoucke et al., 2011), including linear quantization of network weights and inputs. For mobile platforms, like smartphones, computation of these big networks is still demanding and takes place on off-site servers because of their limited computing power and battery life. However, that requires a necessity to a reliable connectivity between the mobile device and off-site servers. Because this is not the case always, custom architectures have been explored for power and speed efficient implementation of CNNs (Jin et al., 2014; Merolla et al., 2014). Another approach to speed up evaluation of CNNs is to reduce the number of parameters in the network representation. The work by Denil et al. (2013) is a good example to show that these networks have high redundancy in them.", "startOffset": 8, "endOffset": 745}, {"referenceID": 0, "context": ", 2012; Chetlur et al., 2014) and CPUs (Vanhoucke et al., 2011), including linear quantization of network weights and inputs. For mobile platforms, like smartphones, computation of these big networks is still demanding and takes place on off-site servers because of their limited computing power and battery life. However, that requires a necessity to a reliable connectivity between the mobile device and off-site servers. Because this is not the case always, custom architectures have been explored for power and speed efficient implementation of CNNs (Jin et al., 2014; Merolla et al., 2014). Another approach to speed up evaluation of CNNs is to reduce the number of parameters in the network representation. The work by Denil et al. (2013) is a good example to show that these networks have high redundancy in them. Considering that state of the art CNNs require hundreds of filters each layer and consist of three to five convolutional layers in general, finding essential representation with smaller parameters brings significant performance boost in terms of time and memory. Jaderberg et al. (2014); Denton et al.", "startOffset": 8, "endOffset": 1108}, {"referenceID": 0, "context": ", 2012; Chetlur et al., 2014) and CPUs (Vanhoucke et al., 2011), including linear quantization of network weights and inputs. For mobile platforms, like smartphones, computation of these big networks is still demanding and takes place on off-site servers because of their limited computing power and battery life. However, that requires a necessity to a reliable connectivity between the mobile device and off-site servers. Because this is not the case always, custom architectures have been explored for power and speed efficient implementation of CNNs (Jin et al., 2014; Merolla et al., 2014). Another approach to speed up evaluation of CNNs is to reduce the number of parameters in the network representation. The work by Denil et al. (2013) is a good example to show that these networks have high redundancy in them. Considering that state of the art CNNs require hundreds of filters each layer and consist of three to five convolutional layers in general, finding essential representation with smaller parameters brings significant performance boost in terms of time and memory. Jaderberg et al. (2014); Denton et al. (2014) exploit the redundancy within convolutional layer after training and could obtain speedup by keeping the accuracy within 1% of the original models.", "startOffset": 8, "endOffset": 1130}, {"referenceID": 18, "context": "A classic but powerful method to accelerate filtering operations is to condition separability on object function (Rigamonti et al., 2013) so as to force the network to learn separable filters.", "startOffset": 113, "endOffset": 137}, {"referenceID": 22, "context": "The importance of sparse connections in CNNs has been mentioned in the recent work (Szegedy et al., 2014).", "startOffset": 83, "endOffset": 105}, {"referenceID": 7, "context": "The proposed method does not require any manual tuning or changes in the structure once trained (Jaderberg et al., 2014; Denton et al., 2014), which simplify overall method.", "startOffset": 96, "endOffset": 141}, {"referenceID": 4, "context": "The proposed method does not require any manual tuning or changes in the structure once trained (Jaderberg et al., 2014; Denton et al., 2014), which simplify overall method.", "startOffset": 96, "endOffset": 141}, {"referenceID": 12, "context": "Sparse feature learning aligned with findings in V1 neurons is proposed by Lee et al. (2007). By iterating L1 and L2 regularizer, this work successfully finds sparse and essential basis filters in over-complete system.", "startOffset": 75, "endOffset": 93}, {"referenceID": 5, "context": "Recent works from Jaderberg et al. (2014); Denton et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 3, "context": "(2014); Denton et al. (2014) speed up CNNs evaluation time with low rank filter approximation.", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": "(2014); Denton et al. (2014) speed up CNNs evaluation time with low rank filter approximation. They compress convolutional layer of pre-trained networks by finding an appropriate low-rank approximation. Denton et al. (2014) extends the method to a largescale task.", "startOffset": 8, "endOffset": 224}, {"referenceID": 2, "context": "Also the connectivity previously was investigated by Culurciello et al. (2013), though many issues remain open for further research.", "startOffset": 53, "endOffset": 79}, {"referenceID": 2, "context": "Also the connectivity previously was investigated by Culurciello et al. (2013), though many issues remain open for further research. We apply structural constraints to conventional CNNs in order to learn 1D separated filters for feedforward acceleration. Our method does not alter training procedure of CNNs; backpropagating the error from output to the input along constrained paths. The approach bypasses difficulties in optimization problem witnessed in Rigamonti et al. (2013), but successfully learns 1D convolution filters.", "startOffset": 53, "endOffset": 481}, {"referenceID": 4, "context": "Similar to the notation used in Denton et al. (2014), weights in CNNs can be described as 4dimensional filters: W \u2208 RC\u00d7X\u00d7Y\u00d7F , C is the number of input channels, X and Y are the spatial dimensions of the filter, and F is the number of filters or the number of output channels.", "startOffset": 32, "endOffset": 53}, {"referenceID": 17, "context": "As the difficulty of classification problem increases, the more number of leading components is required to solve the problem (Montavon et al., 2011).", "startOffset": 126, "endOffset": 149}, {"referenceID": 1, "context": "In experiments, we used the Torch7 environment (Collobert et al., 2011) to demonstrate model performance as well as to handle customized gradient updates.", "startOffset": 47, "endOffset": 71}, {"referenceID": 11, "context": "1D separated filters with dimensions of C, X and Y contains only 5% of the parameters as in 3D filter in commonly used CNNs (Krizhevsky et al., 2012; Sermanet et al., 2013).", "startOffset": 124, "endOffset": 172}, {"referenceID": 19, "context": "1D separated filters with dimensions of C, X and Y contains only 5% of the parameters as in 3D filter in commonly used CNNs (Krizhevsky et al., 2012; Sermanet et al., 2013).", "startOffset": 124, "endOffset": 172}, {"referenceID": 6, "context": "(2013) demonstrates that 5% of essential parameters can predict the rest of parameters in the best case, such reduction in parameters accompanies accuracy loss (Gong et al., 2014; Lebedev et al., 2014).", "startOffset": 160, "endOffset": 201}, {"referenceID": 12, "context": "(2013) demonstrates that 5% of essential parameters can predict the rest of parameters in the best case, such reduction in parameters accompanies accuracy loss (Gong et al., 2014; Lebedev et al., 2014).", "startOffset": 160, "endOffset": 201}, {"referenceID": 3, "context": "While Denil et al. (2013) demonstrates that 5% of essential parameters can predict the rest of parameters in the best case, such reduction in parameters accompanies accuracy loss (Gong et al.", "startOffset": 6, "endOffset": 26}, {"referenceID": 15, "context": "Features have high contrast and edges are sparse as in Lee et al. (2007) without L1 penalty.", "startOffset": 55, "endOffset": 73}, {"referenceID": 11, "context": "Most layers of CNNs can benefit from this method since the number of channels does not decrease over layers and the ratio between channels usually resides between 1 and 3, other than the first layer (Krizhevsky et al., 2012; Sermanet et al., 2013; Szegedy et al., 2014; Simonyan & Zisserman, 2014).", "startOffset": 199, "endOffset": 297}, {"referenceID": 19, "context": "Most layers of CNNs can benefit from this method since the number of channels does not decrease over layers and the ratio between channels usually resides between 1 and 3, other than the first layer (Krizhevsky et al., 2012; Sermanet et al., 2013; Szegedy et al., 2014; Simonyan & Zisserman, 2014).", "startOffset": 199, "endOffset": 297}, {"referenceID": 22, "context": "Most layers of CNNs can benefit from this method since the number of channels does not decrease over layers and the ratio between channels usually resides between 1 and 3, other than the first layer (Krizhevsky et al., 2012; Sermanet et al., 2013; Szegedy et al., 2014; Simonyan & Zisserman, 2014).", "startOffset": 199, "endOffset": 297}, {"referenceID": 1, "context": "However, BLAS-friendly convolution routine is generally used to achieve the highest performance in time and adapted in scientific computing framework (Collobert et al., 2011; Jia et al., 2014; Chetlur et al., 2014).", "startOffset": 150, "endOffset": 214}, {"referenceID": 8, "context": "However, BLAS-friendly convolution routine is generally used to achieve the highest performance in time and adapted in scientific computing framework (Collobert et al., 2011; Jia et al., 2014; Chetlur et al., 2014).", "startOffset": 150, "endOffset": 214}, {"referenceID": 0, "context": "However, BLAS-friendly convolution routine is generally used to achieve the highest performance in time and adapted in scientific computing framework (Collobert et al., 2011; Jia et al., 2014; Chetlur et al., 2014).", "startOffset": 150, "endOffset": 214}, {"referenceID": 13, "context": "The MNIST dataset (LeCun et al., 1998a) consists of hand written digits of 0-9. This dataset contains 60, 000 training and 10, 000 testing images. We applied contrast normalization per image as Goodfellow et al. (2013) without ZCA whitening since hand-written images have low cross-correlation values as opposed to CIFAR datasets.", "startOffset": 19, "endOffset": 219}], "year": 2015, "abstractText": "We present flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks has been extensively studied and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested flattened model on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained.", "creator": "LaTeX with hyperref package"}}}