{"id": "1412.5673", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Entity-Augmented Distributional Semantics for Discourse Relations", "abstract": "Discourse relations bind smaller linguistic elements into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.", "histories": [["v1", "Wed, 17 Dec 2014 23:26:48 GMT  (42kb)", "https://arxiv.org/abs/1412.5673v1", "Three pages plus one page reference. Submit to the ICLR2015 workshop track for review"], ["v2", "Wed, 15 Apr 2015 23:17:48 GMT  (46kb)", "http://arxiv.org/abs/1412.5673v2", "Accepted as a workshop contribution at ICLR 2015"], ["v3", "Tue, 28 Apr 2015 14:14:44 GMT  (42kb)", "http://arxiv.org/abs/1412.5673v3", "Accepted as a workshop contribution at ICLR 2015"]], "COMMENTS": "Three pages plus one page reference. Submit to the ICLR2015 workshop track for review", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yangfeng ji", "jacob eisenstein"], "accepted": true, "id": "1412.5673"}, "pdf": {"name": "1412.5673.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jiyfeng@gatech.edu", "jacobe@gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 2,56 73v3 [cs.CL] 2 8A pr2 01"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is the case that most people who are able to determine themselves are able to determine themselves what they want and what they want. In fact, it is the case that they are able to determine themselves what they want and what they want. In fact, it is the case that they are able to determine themselves what they want and what they want. In fact, it is the case that they are able to determine themselves what they want and what they want."}, {"heading": "2 ENTITY AUGMENTED DISTRIBUTIONAL SEMANTICS FOR RELATION IDENTIFICATION", "text": "We briefly describe our approach to the entity-extended distribution semantics and the identification of discourse relations. Our model for the identification of relationships is called DISCO2, because it is a distributional compositional approach to discourse relations."}, {"heading": "2.1 ENTITY AUGMENTED DISTRIBUTIONAL SEMANTICS", "text": "The entity-enhanced distribution semantics comprises two passes in the composition procedure: the upward trend for the distribution representation of the sentence, while the downward trend for the distribution representation of the entities divided between the sentences runs upward. Upward distribution representations for sentences are calculated in a forward run-up: Each non-terminal in the binary syntactic parse tree has a K-dimensional distribution representation calculated from the distribution representations of its children, calculating the downward trajectory in the representations of individual words. We follow the recursive neural network (RNN) model proposed by Socher et al. (2011) Specifically, for a given parent node i, we refer to the left child as (i), and the right child as r (i). We compose their representations to obtain ui tanh (i); hyperbolic (h) is the element (h)."}, {"heading": "2.2 RELATION IDENTIFICATION MODEL", "text": "To predict the discourse relationship between a pair of sentences (m, n), the decision function is a sum of bilinear products, \u043d (y) = (u (m) 0) ayu (n) 0 + \u2211 i, j (m, n) (d (m) i) byd (n) j + \u03b2 y \u03c6 (m, n) + by, (1) where the predicted relationship is given by y = argmaxy, Y (y) and Ay, By RK \u00b7 K are the classification parameters for the relationship y. A scalar by is used as a preconceived term for the relationship y, and A (m, n) is the number of Korean entity mentions divided within the sentence pair (m, n). In cases where there are no Korean entities between two sentences, A (m, n) is used as the default for the relationship y, and the classification model only takes into account the upward vectors of the root."}, {"heading": "3 EXPERIMENTS", "text": "We evaluate our approach by identifying implicit discourse relationships in the Penn Discourse Treebank (PDTB), where the PDTB relationships can be explicit, meaning that they are signaled by discourse connections (e.g. because); alternatively, they can be implicit, meaning that the connecting element is missing; we focus on the more difficult problem of classifying implicit discourse relationships; in order to build a discourse sparser in the future, we follow the same experimental setting proposed by Lin et al. (2009) and evaluate our relationship model by relationship types at the second level; we conduct the Stanford Parser (Klein & Manning, 2003) and the Berkeley Co-Conference System (Durrett & Klein, 2013) to obtain syntactical trees or co-conference results; in the PDTB, any discourse relationship between two argument relaxations (Limplemention span, 2003) and the Berkeley-Korea System (2013) is commented on."}, {"heading": "4 CONCLUSION", "text": "Discourse relations are determined by the meaning of their arguments, and progress in discourse parsing therefore requires the calculation of representations of argumentation semantics. We present a compositional method to induce distributed representations not only of discourse arguments, but also of the units that run through the discourse. By learning the classification weights and the compositional operators together, this approach surpasses previous work based on handmade surface features. Further discussions and experimental results can be found in a forthcoming paper (Ji & Eisenstein, 2015)."}], "references": [{"title": "Frege in space: A program for compositional distributional semantics", "author": ["Baroni", "Marco", "Bernardi", "Raffaella", "Zamparelli", "Roberto"], "venue": "Linguistic Issues in Language Technologies,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax, volume 6 of Synthesis Lectures on Human Language Technologies", "author": ["Bender", "Emily M"], "venue": "doi: 10.2200/s00493ed1v01y201303hlt020. URL http://dx.doi.org/10.2200/s00493ed1v01y201303hlt020", "citeRegEx": "Bender and M.,? \\Q2013\\E", "shortCiteRegEx": "Bender and M.", "year": 2013}, {"title": "Easy Victories and Uphill Battles in Coreference Resolution", "author": ["Durrett", "Greg", "Klein", "Dan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "One vector is not enough: Entity-augmented distributional semantics for discourse relations. Conditionally accepted to Transactions of the Association for Computational Linguistics (TACL)", "author": ["Ji", "Yangfeng", "Eisenstein", "Jacob"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Dan", "Manning", "Christopher D"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "A data-driven methodology for motivating a set of coherence relations", "author": ["Knott", "Alistair"], "venue": "PhD thesis, The University of Edinburgh,", "citeRegEx": "Knott and Alistair.,? \\Q1996\\E", "shortCiteRegEx": "Knott and Alistair.", "year": 1996}, {"title": "Recognizing implicit discourse relations in the Penn Discourse Treebank", "author": ["Lin", "Ziheng", "Kan", "Min-Yen", "Ng", "Hwee Tou"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Automatically Evaluating Text Coherence Using Discourse Relations", "author": ["Lin", "Ziheng", "Ng", "Hwee Tou", "Kan", "Min-Yen"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Discourse indicators for content selection in summarization", "author": ["Louis", "Annie", "Joshi", "Aravind", "Nenkova", "Ani"], "venue": "In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Louis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Louis et al\\.", "year": 2010}, {"title": "Discourse structures for text generation", "author": ["Mann", "William"], "venue": "Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Mann and William.,? \\Q1984\\E", "shortCiteRegEx": "Mann and William.", "year": 1984}, {"title": "The Penn Discourse Treebank 2.0", "author": ["Prasad", "Rashmi", "Dinesh", "Nikhil", "Lee", "Alan", "Miltsakaki", "Eleni", "Robaldo", "Livio", "Joshi", "Aravind", "Webber", "Bonnie"], "venue": "In LREC,", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Manning", "Chris", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification", "author": ["Somasundaran", "Swapna", "Namata", "Galileo", "Wiebe", "Janyce", "Getoor", "Lise"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Somasundaran et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2009}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Peter D", "Pantel", "Patrick"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Discourse relations: A structural and presuppositional account using lexicalised tag", "author": ["Webber", "Bonnie", "Knott", "Alistair", "Stone", "Matthew", "Joshi", "Aravind"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL), pp", "citeRegEx": "Webber et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Webber et al\\.", "year": 1999}, {"title": "Dependency-based Discourse Parser for Single-Document Summarization", "author": ["Yoshida", "Yasuhisa", "Suzuki", "Jun", "Hirao", "Tsutomu", "Nagata", "Masaaki"], "venue": "In EMNLP,", "citeRegEx": "Yoshida et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yoshida et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "The high-level organization of text can be characterized in terms of discourse relations between adjacent spans of text (Knott, 1996; Mann, 1984; Webber et al., 1999).", "startOffset": 120, "endOffset": 166}, {"referenceID": 8, "context": "Identifying these relations has been shown to be relevant to tasks such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al.", "startOffset": 89, "endOffset": 131}, {"referenceID": 16, "context": "Identifying these relations has been shown to be relevant to tasks such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al.", "startOffset": 89, "endOffset": 131}, {"referenceID": 13, "context": ", 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al.", "startOffset": 28, "endOffset": 55}, {"referenceID": 7, "context": ", 2009), and coherence evaluation (Lin et al., 2011).", "startOffset": 34, "endOffset": 52}, {"referenceID": 10, "context": "While the Penn Discourse Treebank (PDTB) now provides a large dataset annotated for discourse relations (Prasad et al., 2008), the automatic identification of implicit discourse relations is a difficult task, with state-of-the-art performance at roughly 40% (Lin et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 6, "context": ", 2008), the automatic identification of implicit discourse relations is a difficult task, with state-of-the-art performance at roughly 40% (Lin et al., 2009).", "startOffset": 140, "endOffset": 158}, {"referenceID": 12, "context": "We address this issue by applying a discriminatively-trained model of compositional distributional semantics to discourse relation classification (Socher et al., 2013; Baroni et al., 2014).", "startOffset": 146, "endOffset": 188}, {"referenceID": 0, "context": "We address this issue by applying a discriminatively-trained model of compositional distributional semantics to discourse relation classification (Socher et al., 2013; Baroni et al., 2014).", "startOffset": 146, "endOffset": 188}, {"referenceID": 14, "context": "The meaning of each sentence is represented as a vector (Turney et al., 2010), which is computed through a series of compositional operations over the parse tree.", "startOffset": 56, "endOffset": 77}, {"referenceID": 6, "context": "In combination, our approach achieves a 3% improvement in accuracy over the best previous work (Lin et al., 2009) on the second-level discourse relation identification in the PDTB.", "startOffset": 95, "endOffset": 113}, {"referenceID": 6, "context": "In combination, our approach achieves a 3% improvement in accuracy over the best previous work (Lin et al., 2009) on the second-level discourse relation identification in the PDTB.1. Our model requires a syntactic parse tree, which is produced automatically from the Stanford CoreNLP parser Klein & Manning (2003). A reviewer asked whether it might be better to employ a left-to-right recurrent neural network, which would obviate the need for this language-specific resource.", "startOffset": 96, "endOffset": 314}, {"referenceID": 11, "context": "We follow the Recursive Neural Network (RNN) model proposed by Socher et al. (2011). Specifically, for a given parent node i, we denote the left child as l(i), and the right child as r(i).", "startOffset": 63, "endOffset": 84}, {"referenceID": 6, "context": "Lin et al. (2009) Yes 40.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Lin et al. (2009) Yes 40.2 Our work 2. Surface feature model Yes 39.69 3. DISCO2 No No 50 36.98 4. DISCO2 Yes No 50 37.63 5. DISCO2 No Yes 50 42.53 6. DISCO2 Yes Yes 50 43.56 \u2217 signficantly better than Lin et al. (2009) with p < 0.", "startOffset": 0, "endOffset": 220}, {"referenceID": 6, "context": "The results of Lin et al. (2009) are shown in line 1; the results for our reimplementation of this system are shown in line 2.", "startOffset": 15, "endOffset": 33}, {"referenceID": 6, "context": "Aiming to build a discourse parser in future, we follow the same experimental setting proposed by Lin et al. (2009), and evaluate our relation identification model on the second-level relation types.", "startOffset": 98, "endOffset": 116}, {"referenceID": 6, "context": "4% improvement over the accuracy reported by Lin et al. (2009) (p < .", "startOffset": 45, "endOffset": 63}], "year": 2015, "abstractText": "Discourse relations bind smaller linguistic elements into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.", "creator": "LaTeX with hyperref package"}}}