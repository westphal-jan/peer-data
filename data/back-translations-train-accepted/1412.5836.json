{"id": "1412.5836", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Incorporating Both Distributional and Relational Semantics in Word Representations", "abstract": "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.", "histories": [["v1", "Thu, 18 Dec 2014 12:30:55 GMT  (171kb,D)", "http://arxiv.org/abs/1412.5836v1", "Under review as a workshop contribution at ICLR2015 (short version of the paper). Long version at:arXiv:1412.4369"], ["v2", "Tue, 17 Feb 2015 17:18:34 GMT  (82kb,D)", "http://arxiv.org/abs/1412.5836v2", "Under review as a workshop contribution at ICLR2015 (short version of the paper). Long version at:arXiv:1412.4369"], ["v3", "Sat, 21 Mar 2015 13:27:28 GMT  (82kb,D)", "http://arxiv.org/abs/1412.5836v3", "Accepted as a workshop contribution at ICLR2015. Long version at:arXiv:1412.4369"]], "COMMENTS": "Under review as a workshop contribution at ICLR2015 (short version of the paper). Long version at:arXiv:1412.4369", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel fried", "kevin duh"], "accepted": true, "id": "1412.5836"}, "pdf": {"name": "1412.5836.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Daniel Fried"], "emails": ["dfried@email.arizona.edu", "kevinduh@is.naist.jp"], "sections": [{"heading": "1 INTRODUCTION", "text": "We are interested in algorithms to learn vector representations of words, and recent work has shown that such representations, also known as word embedding, can successfully capture the semantic and syntactic regularities of words (Mikolov et al., 2013a) and improve the performance of various natural language processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), all based on the same premise of distributional semantics (Harris et al., 2013a) and semantic role designation (Collobert et al., 2011). Although many types of representation learning algorithms have been proposed so far, they are essentially based on distributional semantics (Harris et al., 1954), embodied in J. R. Firth's dictum: \"You will know a word by the company that holds it.\" The models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al."}, {"heading": "2 DISTRIBUTIONAL AND RELATIONAL OBJECTIVES", "text": "The model is able to internalize distribution semantics (NLM). (D) We are able to internalize distribution semantics (NLM). (D) We are able to internalize distribution semantics (NLM). (D) We are able to internalize distribution semantics (NLR), distribution semantics (NLR), distribution semantics (NLR), distribution semantics (NLR), distribution semantics (RD), distribution semantics (NLR), distribution semantics (NLR), distribution semantics (NLR), distribution semantics (R), (R), (R), distribution semantics (R), (R), (R), distribution semantics (R), (R), (R), (R), distribution semantics (R), (R), (R), distribution semantics (R), (R), (R), distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R, distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R, distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R, distribution semantics (R), distribution semantics (R), distribution semantics (R), distribution semantics (R, (R), (R), distribution semantics (R),"}, {"heading": "3 PRELIMINARY EXPERIMENTS & DISCUSSIONS", "text": "The distribution object LNLM is trained with 5 grams from the Google books English corpus1, which contain over 180 million 5-gram types. To train LGD we pitch 100k words and calculate similarity with 5 other words per word in each ADMM iteration. For training LTransE and LNTN we use the data set from Socher et al. (2013b).We first offer an analysis of the behavior of ADMM on the training set to confirm that it effectively optimizes the common goal. Fig. 1 (left) plots the learning curve by training iteration for different values of the country hyperparameters. We see that ADMM achieves a reasonable objective value relatively quickly in 100 iterations. Fig. 1 (right) shows the average difference between the resulting sets of embedding w and v, which prove desirable."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by a Microsoft Research CORE Grant and JSPS KAKENHI Grant Number 26730121. D.F. was supported during this work by the Flinn Scholarship. We thank Haixun Wang, Jun'ichi Tsujii, Tim Baldwin, Yuji Matsumoto and several anonymous reviewers for helpful discussions at various stages of the project."}], "references": [{"title": "A neural probabilistic language models", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Boyd", "Stephen", "Parikh", "Neal", "Chu", "Eric", "Peleato", "Borja", "Eckstein", "Jonathan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Lexical Semantics", "author": ["Cruse", "Alan D"], "venue": null, "citeRegEx": "Cruse and D.,? \\Q1986\\E", "shortCiteRegEx": "Cruse and D.", "year": 1986}, {"title": "Semeval-2012 task 2: Measuring degrees of relational similarity", "author": ["Jurgens", "David A", "Turney", "Peter D", "Mohammad", "Saif M", "Holyoak", "Keith J"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics,", "citeRegEx": "Jurgens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["Leacock", "Claudia", "Chodorow", "Martin"], "venue": "WordNet: An Electronic Lexical Database,", "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tom\u00e1s", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A lexical database for English", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["Mnih", "Andriy", "Kavukcuoglu", "Koray"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "Slav", "McDonald", "Ryan"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Continuous space language models", "author": ["Schwenk", "Holger"], "venue": "Computer Speech and Language,", "citeRegEx": "Schwenk and Holger.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk and Holger.", "year": 2007}, {"title": "Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Socher", "Richard", "Bauer", "John", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: A simple and general method for semi-supervise learning", "author": ["Turian", "Joseph", "Ratinov", "Lev-Arie", "Bengio", "Yoshua"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Wang", "Mengqiu", "Manning", "Christopher D"], "venue": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": ", 2013a) and improve the performance of various Natural Language Processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), parsing (Socher et al.", "startOffset": 118, "endOffset": 161}, {"referenceID": 3, "context": ", 2013a), and semantic role labeling (Collobert et al., 2011).", "startOffset": 37, "endOffset": 61}, {"referenceID": 0, "context": "\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.", "startOffset": 29, "endOffset": 138}, {"referenceID": 3, "context": "\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.", "startOffset": 29, "endOffset": 138}, {"referenceID": 2, "context": "We employ a general representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011).", "startOffset": 118, "endOffset": 137}, {"referenceID": 3, "context": "Distributional Semantics Objective: We implement distributional semantics using the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi \u2208 R, the word\u2019s embedding.", "startOffset": 115, "endOffset": 139}, {"referenceID": 1, "context": "The TransE model of Bordes et al. (2013) represents relations as translations: if the relationship R holds for two words vl and vr, then their embeddings vl,vr \u2208 R should be close after translating vl by a relation vector R \u2208 R: STransE(vl, R, vr) = \u2212||vl +R\u2212 vr||2.", "startOffset": 20, "endOffset": 41}, {"referenceID": 1, "context": "The TransE model of Bordes et al. (2013) represents relations as translations: if the relationship R holds for two words vl and vr, then their embeddings vl,vr \u2208 R should be close after translating vl by a relation vector R \u2208 R: STransE(vl, R, vr) = \u2212||vl +R\u2212 vr||2. Socher et al. (2013b) introduce a Neural Tensor Network (NTN) that models interaction between embeddings using tensors.", "startOffset": 20, "endOffset": 289}, {"referenceID": 5, "context": "The SemEval2012 Analogy Test is a relational word similarity task similar to SAT-style analogy questions (Jurgens et al., 2012).", "startOffset": 105, "endOffset": 127}, {"referenceID": 12, "context": "For training LTransE and LNTN , we use the dataset of Socher et al. (2013b). We first provide an analysis of the behavior of ADMM on the training set, to confirm that it effectively optimizes the joint objective.", "startOffset": 54, "endOffset": 76}], "year": 2017, "abstractText": "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.", "creator": "LaTeX with hyperref package"}}}