{"id": "1412.6249", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Purine: A bi-graph based deep learning framework", "abstract": "In this paper, we introduce a novel deep learning framework, termed Purine. In Purine, a deep network is expressed as a bipartite graph (bi-graph), which is composed of interconnected operators and data tensors. With the bi-graph abstraction, networks are easily solvable with event-driven task dispatcher. We then demonstrate that different parallelism schemes over GPUs and/or CPUs on single or multiple PCs can be universally implemented by graph composition. Scheduled by the task dispatcher, memory transfers are fully overlapped with other computations, which greatly reduce the communication overhead and help us achieve approximate linear acceleration. This eases researchers from coding for various parallelization schemes, and the same dispatcher can be used for solving variant graphs.", "histories": [["v1", "Fri, 19 Dec 2014 08:20:10 GMT  (942kb,D)", "http://arxiv.org/abs/1412.6249v1", null], ["v2", "Mon, 22 Dec 2014 03:18:46 GMT  (943kb,D)", "http://arxiv.org/abs/1412.6249v2", null], ["v3", "Tue, 20 Jan 2015 02:17:39 GMT  (943kb,D)", "http://arxiv.org/abs/1412.6249v3", "Submitted to ICLR 2015 workshop"], ["v4", "Mon, 16 Mar 2015 16:13:46 GMT  (943kb,D)", "http://arxiv.org/abs/1412.6249v4", "Submitted to ICLR 2015 workshop"], ["v5", "Thu, 16 Apr 2015 13:09:33 GMT  (943kb,D)", "http://arxiv.org/abs/1412.6249v5", "Submitted to ICLR 2015 workshop"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["min lin", "shuo li", "xuan luo", "shuicheng yan"], "accepted": true, "id": "1412.6249"}, "pdf": {"name": "1412.6249.pdf", "metadata": {"source": "CRF", "title": "PURINE: A BI-GRAPH BASED DEEP LEARNING FRAME- WORK", "authors": ["Min Lin", "Shuo Li", "Xuan Luo", "Shuicheng Yan"], "emails": ["linmin@nus.edu.sg", "lishuo@nus.edu.sg", "luoxuan@nus.edu.sg", "eleyans@nus.edu.sg"], "sections": [{"heading": "1 INTRODUCTION", "text": "The need to train deep neural networks on large-scale datasets has motivated several research papers aimed at speeding up the training process by parallelizing training on multiple CPUs or GPUs. There are two different ways to parallelise training. (1) Parallelism of the model: The model is distributed among different computing nodes (Sutskever et al., 2014). (2) Parallelism of data: Different nodes train on different samples of the same model (Silde et al., 2014; Chilimbi et al., 2014). Some of the papers even use a hybrid of them (Krizhevsky et al., 2014; Dean et al., 2012; Le, 2013). For data parallelism, there are also two models of communication between equals. (1) The reduced approach, in which all updates from the equals are aggregated at the synchronization point and the average update is transmitted to the puzzle parallelism (2014)."}, {"heading": "2 BI-GRAPH ABSTRACTION", "text": "Purine abstracts the processing procedure of deep neural networks into directed bipartite graphs (bi-graphs). The bi-graph contains two types of vertices, tensors and operators. Directed edges are only between tensors and operators and there is no link within tensors or operators. Figure 1 illustrates the bi-graph for the forward-facing neural networks in Caffe.All forward-facing neural networks can be represented by a directed acyclic bipartite graph that can be solved by a universal task dispenser. There are several works that use similar abstractions. For example, the data flow graph in Dryad (Isard et al., 2007) and the pig latein (Olston et al., 2008) are the same as the bi-graph abstraction presented in this paper. Graphlab (Low et al., 2010) suggests that the data flow graph in Dryad (Isi al.), 2007, the Latin (Isi.) and the Latin (Isi."}, {"heading": "2.1 TASK DISPATCHER", "text": "Purine solves the bi-chart by scheduling the operators within the bi-chart with an event-driven task dispatcher. An operator is triggered when all incoming tensors are ready. A tensor is ready when all incoming operators have completed the calculation. Calculation of the bi-chart starts at the source of the chart and stops when all sinks are reached. This process is terminated by the task dispatcher."}, {"heading": "2.2 ITERATIONS", "text": "Although it was argued in (Low et al., 2010) that the directed acyclic graph could not effectively express iterative algorithms because the graph structure would depend on the number of iterations, we overcome this by iterating the graphs. Since the task dispatcher waits until all the sinks of the graph are reached, it acts as a synchronization point. Thus, parallelizable operations can be grouped into a single graph, while sequential tasks (iterations) are implemented by iterating graphs. A concrete example is shown in Figure 2."}, {"heading": "3 PARALLIZATION", "text": "The \"Location\" property uniquely identifies the computing resource (CPUs / GPUs) on which to assign a tensor / operator; the \"Location\" property consists of two fields: host name and device identifier; in a multi-machine cluster, the host name identifies the machine on which the vertice is located; the device identifier determines whether to assign the tensor / operator to the CPU or GPU; and the order identifier of the GPU when multiple GPUs are installed on a single computer. In addition to the \"Location\" property, the operators are assigned another property, \"Thread,\" as both the CPU and GPU support multi-threading; operators with the same thread ID will line up in the same thread, while operators with other threads decide on the allocation of resources whenever possible."}, {"heading": "3.1 COPY OPERATOR", "text": "In the multi-device setting, data located on one device is not directly accessible to operators on another device. Thus, a special \"copy\" operator is introduced to cross the boundary and connect parts of the bi-graph on individual devices. Copy operators, just like other operators, are terminated by the task dispatcher, so it is easy to overlap copying operations with other computing tasks by assigning them different threads."}, {"heading": "3.2 TASK DISPATCHER", "text": "For a single machine and multiple devices, only one dispatcher process is started. Operators are assigned to their threads and scheduled by the global dispatcher. For multiple machines and multiple devices, individual dispatcher processes are started on each machine. Copiers that copy data from Machine A to Machine B are sinks on Machine A and sources on Machine B. In this way, each machine only needs to plan its own subgraph, and no global disposition mechanism or communication between dispatchers is required."}, {"heading": "3.3 MODEL PARALLELISM", "text": "We will show how model parallelism can be implemented in purines, taking a two-tiered, fully connected neural network as an example. It can easily be expanded to deeper networks. As shown in Figure 3, the execution of the two-tiered network can be divided into three successive steps. They are referred to accordingly as A, B, C. To keep resources busy all the time, the network is replicated three times and executed sequentially."}, {"heading": "3.4 DATA PARALLELISM", "text": "Data parallelism is a simple but straightforward way to parallelise deep networks. In data parallelism, peers \"gradients are collected from the parameter server and the updated parameters are computed and copied back to all peers. A hybrid of data parallelism and model parallelism has already been proposed by Krizhevsky (2014), in which the folding layers use data parallelism and fully connected layers model parallelism based on the observation that the number of parameters is large and therefore the communication costs for fully connected layers are high. The hybrid approach significantly reduces communication costs. Another approach to reducing communication overheads is to overlap data transfer with calculations."}, {"heading": "4 RESULTS", "text": "We conducted experiments on the Purine framework with data parallelism on GoogLeNet (Szegedy et al., 2014). Data parallelism often leads to larger batch sizes, which are unfavorable to the convergence of SGD shown by previous studies. In this paper, we ignored the possible change in convergence rate, but instead studied how much more data per unit of time can be processed with parallelization. We compared the number of images processed per second for GoogLeNet with different numbers of GPUs for data parallelism. The batch size we use is 128 per GPU. As shown in Table 1, this batch increases linearly with more GPUs added when the number of GPUs is within 4 (on the same computer). With 8 GPUs on two computers, performance increases 7.3-fold. Note that the machines are connected by Gigabit-based Ethernet and thus have to PU."}], "references": [{"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In Proceedings of the 11th USENIX conference on Operating Systems Design and Implementation,", "citeRegEx": "Chilimbi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chilimbi et al\\.", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Dryad: distributed dataparallel programs from sequential building blocks", "author": ["Michael Isard", "Mihai Budiu", "Yuan Yu", "Andrew Birrell", "Dennis Fetterly"], "venue": "In ACM SIGOPS Operating Systems Review,", "citeRegEx": "Isard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Isard et al\\.", "year": 2007}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["Alex Krizhevsky"], "venue": "arXiv preprint arXiv:1404.5997,", "citeRegEx": "Krizhevsky.,? \\Q2014\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2014}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Le.,? \\Q2013\\E", "shortCiteRegEx": "Le.", "year": 2013}, {"title": "Graphlab: A new framework for parallel machine learning", "author": ["Yucheng Low", "Joseph Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin", "Joseph M Hellerstein"], "venue": "arXiv preprint arXiv:1006.4990,", "citeRegEx": "Low et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Low et al\\.", "year": 2010}, {"title": "Pig latin: a not-so-foreign language for data processing", "author": ["Christopher Olston", "Benjamin Reed", "Utkarsh Srivastava", "Ravi Kumar", "Andrew Tomkins"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Olston et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Olston et al\\.", "year": 2008}, {"title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns", "author": ["Frank Seide", "Hao Fu", "Jasha Droppo", "Gang Li", "Dong Yu"], "venue": "In Fifteenth Annual Conference of the International Speech Communication Association,", "citeRegEx": "Seide et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "(1) Model parallelism: the model is distributed to different computing nodes (Sutskever et al., 2014) (2) Data parallelism: different nodes train on different samples for the same model (Seide et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 8, "context": ", 2014) (2) Data parallelism: different nodes train on different samples for the same model (Seide et al., 2014; Chilimbi et al., 2014).", "startOffset": 92, "endOffset": 135}, {"referenceID": 0, "context": ", 2014) (2) Data parallelism: different nodes train on different samples for the same model (Seide et al., 2014; Chilimbi et al., 2014).", "startOffset": 92, "endOffset": 135}, {"referenceID": 4, "context": "Some of the works even use a hybrid of them (Krizhevsky, 2014; Dean et al., 2012; Le, 2013).", "startOffset": 44, "endOffset": 91}, {"referenceID": 1, "context": "Some of the works even use a hybrid of them (Krizhevsky, 2014; Dean et al., 2012; Le, 2013).", "startOffset": 44, "endOffset": 91}, {"referenceID": 5, "context": "Some of the works even use a hybrid of them (Krizhevsky, 2014; Dean et al., 2012; Le, 2013).", "startOffset": 44, "endOffset": 91}, {"referenceID": 8, "context": "(1) the allreduce approach where all updates from the peers are aggregated at the synchronization point and the averaged update is broadcasted back to the peers (Seide et al., 2014; Krizhevsky, 2014).", "startOffset": 161, "endOffset": 199}, {"referenceID": 4, "context": "(1) the allreduce approach where all updates from the peers are aggregated at the synchronization point and the averaged update is broadcasted back to the peers (Seide et al., 2014; Krizhevsky, 2014).", "startOffset": 161, "endOffset": 199}, {"referenceID": 1, "context": "(2) the parameter server approach handles the reads and writes of the parameters asynchronously (Dean et al., 2012; Le, 2013; Chilimbi et al., 2014).", "startOffset": 96, "endOffset": 148}, {"referenceID": 5, "context": "(2) the parameter server approach handles the reads and writes of the parameters asynchronously (Dean et al., 2012; Le, 2013; Chilimbi et al., 2014).", "startOffset": 96, "endOffset": 148}, {"referenceID": 0, "context": "(2) the parameter server approach handles the reads and writes of the parameters asynchronously (Dean et al., 2012; Le, 2013; Chilimbi et al., 2014).", "startOffset": 96, "endOffset": 148}, {"referenceID": 3, "context": "It is named \u201cPurine\u201d, which is an analog of caffeine in molecular structure, because we\u2019ve benefited a lot from the open source Caffe framework (Jia et al., 2014) in our research and the math functions used in Purine are ported from Caffe.", "startOffset": 144, "endOffset": 162}, {"referenceID": 2, "context": "For example, the dataflow graph in Dryad (Isard et al., 2007) and Pig Latin (Olston et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 7, "context": ", 2007) and Pig Latin (Olston et al., 2008) are the same as the Bi-Graph abstraction introduced in this paper.", "startOffset": 22, "endOffset": 43}, {"referenceID": 6, "context": "Graphlab (Low et al., 2010) proposed", "startOffset": 9, "endOffset": 27}, {"referenceID": 6, "context": "2 ITERATIONS Although it has been argued in (Low et al., 2010) that the directed acyclic graph could not effectively express iterative algorithms as the graph structure would depend on the number of iterations.", "startOffset": 44, "endOffset": 62}, {"referenceID": 4, "context": "A hybrid of data parallelism and model parallelism has previously been proposed by Krizhevsky (2014) in which the convolution layers use data parallelism and fully connected layers use model parallelism.", "startOffset": 83, "endOffset": 101}, {"referenceID": 4, "context": "A hybrid of data parallelism and model parallelism has previously been proposed by Krizhevsky (2014) in which the convolution layers use data parallelism and fully connected layers use model parallelism. This is based on the observation that the number of parameters is large and thus the communication cost is big for fully connected layers. The hybrid approach greatly reduces the communication cost. A different approach to reduce communication overhead is to overlap the data transfer with computations. Double buffering is proposed by Seide et al. (2014) to break a minibatch in half and exchange the gradients of the first half while doing computaion of the second half.", "startOffset": 83, "endOffset": 560}, {"referenceID": 10, "context": "4 RESULTS We carried out experiments on the Purine framework with data parallelism on GoogLeNet (Szegedy et al., 2014).", "startOffset": 96, "endOffset": 118}], "year": 2017, "abstractText": "In this paper, we introduce a novel deep learning framework, termed Purine. In Purine, a deep network is expressed as a bipartite graph (bi-graph), which is composed of interconnected operators and data tensors. With the bi-graph abstraction, networks are easily solvable with event-driven task dispatcher. We then demonstrate that different parallelism schemes over GPUs and/or CPUs on single or multiple PCs can be universally implemented by graph composition. Scheduled by the task dispatcher, memory transfers are fully overlapped with other computations, which greatly reduce the communication overhead and help us achieve approximate linear acceleration. This eases researchers from coding for various parallelization schemes, and the same dispatcher can be used for solving variant graphs.", "creator": "LaTeX with hyperref package"}}}