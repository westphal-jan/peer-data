{"id": "1412.6277", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "N-gram-Based Low-Dimensional Representation for Document Classification", "abstract": "The bag-of-words (BOW) model is the common approach for classifying documents, where words are used as feature for training a classifier. This generally involves a huge number of features. Some techniques, such as Latent Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been designed to summarize documents in a lower dimension with the least semantic information loss. Some semantic information is nevertheless always lost, since only words are considered. Instead, we aim at using information coming from n-grams to overcome this limitation, while remaining in a low-dimension space. Many approaches, such as the Skip-gram model, provide good word vector representations very quickly. We propose to average these representations to obtain representations of n-grams. All n-grams are thus embedded in a same semantic space. A K-means clustering can then group them into semantic concepts. The number of features is therefore dramatically reduced and documents can be represented as bag of semantic concepts. We show that this model outperforms LSA and LDA on a sentiment classification task, and yields similar results than a traditional BOW-model with far less features.", "histories": [["v1", "Fri, 19 Dec 2014 10:29:33 GMT  (170kb,D)", "https://arxiv.org/abs/1412.6277v1", "ICLR 2015 conference track"], ["v2", "Fri, 10 Apr 2015 13:53:40 GMT  (37kb)", "http://arxiv.org/abs/1412.6277v2", "Accepted as a workshop contribution at ICLR 2015"]], "COMMENTS": "ICLR 2015 conference track", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\\'emi lebret", "ronan collobert"], "accepted": true, "id": "1412.6277"}, "pdf": {"name": "1412.6277.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["remi@lebret.ch", "ronan@collobert.com"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.62 77v2 [cs.CL] 1 0A pr"}, {"heading": "1 INTRODUCTION", "text": "There are only a few terms that each have a binary value (present or not in the document) or a word-occurrence-frequency frequency frequency in the various terms and concepts. Some term weightings (e.g. the popular words-idf) have also been defined to reflect how discriminatory a word is for a document. These are considered characteristics for the training of a classifier. Naive Bayes (NB) and support Vector Machine (SVM) models are often the first decisions. One limitation of the bag-of-words model is that the discriminatory words are not usually the most common. A large dictionary of words needs to be defined to obtain a robust model."}, {"heading": "2 A BAG OF SEMANTIC CONCEPTS MODEL", "text": "The model is divided into three steps: (1) vector representations of n-Grammys are achieved by averaging pre-trained representations of its individual words; (2) n-Grammys are grouped into K-semantic concepts by performing K-mean clusters on all n-gram representations; (3) documents are represented by a bag of K-semantic concepts, each entry depending on the presence of n-grammars from the concepts defined in the previous step."}, {"heading": "2.1 N -GRAM REPRESENTATION", "text": "The first step of the model is to generate continuous vector representations xw for each word w within the dictionary D. Using newer models such as the Skip-gram (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014), they are efficiently trained over a large body of unlabeled data. Indeed, these models are highly parallelisable, which helps to obtain these representations very quickly. Word representations are then combined to create n-gram representations: 1nn \u2211 i = 1xwi. (1) These representations are vectors that hold the semantic information of n-grams with different n in the same dimensions, making distances between them calculable. It allows the use of a K-mean cluster to group all n-grams in K classes."}, {"heading": "2.2 K -MEANS CLUSTERING", "text": "K-Means is an unattended learning algorithm commonly used to automatically partition a dataset into K clusters. Taking into account a series of n-gram representations xi-Rm, the algorithm determines a set of K centroids \u03b3k-Rm to minimize the average distance from each representation to the next centroid: \u2211 i | | xi \u2212 \u03b3\u03c3i | | 2, where \u03c3i = argmink | xi \u2212 \u03b3k | | 2. (2) The limitation due to the size of the dictionary is therefore exceeded. By setting K to a low value, documents can also be represented by more compact vectors than with a sack-of-words model, preserving all meaningful information."}, {"heading": "2.3 DOCUMENT REPRESENTATION", "text": "The designation D = (d1, d2,.., dL) is a set of text documents, with each document di containing a set of n-grams. First, each n-gram is embedded in a common vector space by averaging its word vector representations. The resulting n-gram representations are assigned to clusters that use the centers defined by the K mean clustering. The set of text documents is then represented by a vector of K characteristics, fi-RK. Each entry fki usually corresponds to the frequency of n-grams from the k-th cluster within the document di. The set of text documents is then defined as D = (fi, yi) | fi-RK, yi-1} L i = 1.With NB characteristics. For certain document types, such as movie reviews, the use of Naive Bayes characteristics can improve overall performance (Wang & Manning, 2012)."}, {"heading": "3 EXPERIMENTS WITH SENTIMENT ANALYSIS", "text": "Sentiments can have a completely different meaning if n-grams are taken into account instead of words. A classifier could use a bigchart such as \"not good\" to classify a document as negative, whereas this would probably fail if only unigrams (words) were taken into account."}, {"heading": "3.1 IMDB MOVIE REVIEWS DATASETS", "text": "IMDB datasets have the nice characteristic of containing long documents, so it is valuable to consist of n-grams in such a framework. We experimented with small and large review collections, so we can analyze how good our model is for different dataset sizes compared to classic models."}, {"heading": "3.1.1 PANG & LEE (2004)", "text": "The collection consists of 1,000 positive and 1,000 negative reviews, so a random guess gives an accuracy of 50%. The authors only selected reviews where the rating was expressed with either stars or a certain numerical value. To avoid dominance of the corpus by a small number of productive reviewers, they set a limit of less than 20 reviews per author and sensation category. Since there is no test set, we used a 10-fold cross-validation."}, {"heading": "3.1.2 MAAS ET AL. (2011)", "text": "The collection consists of 100,000 reviews 2. It has been divided into three data sets: training and test sets (25,000 labeled reviews each) and 50,000 unlabeled training reviews. It does not allow more than 30 reviews per film. It contains an even number of positive and negative reviews, so that random guessing results in an accuracy of 50%. Only heavily polarized reviews were taken into account. A negative review has a score of \u2264 4 out of 10 and a positive review has a score of \u2265 7 out of 10."}, {"heading": "3.2 EXPERIMENTAL SETUP", "text": "We first learn word vector representations over a large body of unlabeled text. However, this step could be skipped by taking existing pre-trained word representations 3, rather than learning them from scratch. By following the three steps described in Section 2, movie reviews are then presented as bags full of semantic concepts, which are eventually used to form a linear SVM to classify feelings."}, {"heading": "3.2.1 LEARNING WORD REPRESENTATION OVER LARGE CORPORA", "text": "Our English corpus consists of the entire English Wikipedia4, the Reuters corpus, and the Wall Street Journal (WSJ) corpus. We look at lowercase letters and replace digits with a special token, and the resulting text is tokenized with the Stanford tokenizer. The final dataset is about 2 billion words. Our D dictionary consists of all words that occur at least a hundred times, resulting in a dictionary of 202,255 words. We then train a skip-gram model to obtain word representation in a 100-dimensional vector, which is intentionally quite small in order to accelerate clustering thereafter. Like other hyperparameters, we use a fixed learning rate of 0.01, a context size of 5 phrases, negative sampling with 5 negative samples for each positive sample, and a subsampling approach with a threshold of 10 \u2212 5."}, {"heading": "3.2.2 BAG OF SEMANTIC CONCEPTS FOR MOVIE REVIEWS", "text": "We consider n-gram representations up to n = 3. Only n-gram representations with words from our dictionary are taken into account for both datasets.5 This results in a set of 34,360 1-gram representations, 419,918 2-gram representations and thus 921,837 3-gram representations for the data set of Pang and Lee. And 67,847 1-gram representations, 1,842,461 2-gram representations and 5,724,871 3-gram representations for the data set of Maas et al. '. Since n-gram representations are calculated by means of average representations of his word, all n-gram representations are also calculated in a 100-dimensional vector. The division n-gram into semantic concepts. Since n-gram representations are represented in a common vector space, similarities between n-gram representations of different lengths can be calculated."}, {"heading": "3.2.3 COMPARISON WITH OTHER METHODS", "text": "We compare our models with two classical techniques for displaying text documents in a low-dimensional vector space: LSA and LDA. Both methods use the same 1 gram dictionaries as with the bag of semantic concepts model with K = {100, 200, 300}. Within the framework of the data set of Maas et al., LSA and LDA benefit from the large set of unlabeled evaluations: the latent sensory analysis (LSA) (Deerwester et al., 1990). Let X, R | D | \u00b7 L be a matrix in which each element Xi, j describes the log number ratio of words i in document j, with L the number of training documents and D the dictionary of words (i.e. 34,360 for Pang and Lees dataset, 67,847 for Maas et al dataset). By applying truncated SVD to the log number ratio matrix X, we obtain semantic representations in a 2003 film eponym K for the lognumber space (see SDA dimensional space)."}, {"heading": "3.2.4 CLASSIFICATION USING SVM", "text": "Based on the representation of film reviews in a K-dimensional vector, a classifier is trained to determine whether a given review is positive or negative. Considering the set of training materials D-fi-yi-f-i-R K, yi-i-1-1-L i-1, we chose a linear SVM as classifier trained with the help of the LIBLINEAR library (Fan et al., 2008): min w1 2 wTw + C-imax (0, 1 \u2212 yiw T f-i) 2, (5) with w-weight vector and C a penalty parameter."}, {"heading": "3.3 RESULTS", "text": "The overall results summarized in Table 1 show that the bag of semantic concepts approach outperforms traditional LDA and LSA approaches to present documents in a low-dimensional space. Good performance is achieved even with only 100 clusters, with LSA requiring more clusters to improve. We also point out that our approach performs well on a small dataset where LDA fails. Significant increases can be observed when using 2 grams instead of 1 gram. However, using only 3 grams harms performance, and the best results are obtained by combining n-grams, which confirms the utility of the method. This also means that word vector representations can be combined while retaining relevant semantic information. This is illustrated in Table 3, where semantically close n-grams are in the same cluster."}, {"heading": "3.4 COMPUTATION TIME", "text": "The most time-consuming and costly process step within the model is K-mean clustering, especially when dealing with millions of n-gram representations. However, this step can be performed very quickly with low memory by applying the K-mean method. Calculation times for generating 300-dimensional representations are reported in Table 2. All experiments were performed with single CPU core Intel i7 2600K 3.4 GHz. Despite the fact that a single CPU was used for this benchmark, the three steps of the model are highly parallelisable. Therefore, the recorded times could be divided by the number of available CPUs. We see that representations can be calculated in less than a minute using only a 1-gram dictionary."}, {"heading": "4 CONCLUSION", "text": "With newer techniques such as the Skip-gram model, text vector representations can be created quickly. N-grams with different lengths n can then be embedded in a homogeneous vector space, with a simple elemental addition, allowing the calculation of distances between n-grams, which can have many applications in processing natural language. We therefore proposed a bag of semantic concepts to represent documents in a low-dimensional space. These semantic concepts are achieved by performing a K-mean cluster that splits all n-grams into K-clusters. This model has several advantages over classical approaches to representing documents in a low-dimensional space: it uses semantic information derived from n-grams; it builds document representations with low resource consumption (time and memory); it can derive semantic concepts for invisible n-grams. Furthermore, we have shown that such a model is suitable for classifying documents."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the HASLER Foundation with the support of \"Information and Communication Technology for a Better World 2020\" (SmartWorld)."}], "references": [{"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["R. Fan", "K. Chang", "C. Hsieh", "X. Wang", "C. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Exploiting Wikipedia As External Knowledge for Document Clustering", "author": ["X. Hu", "X. Zhang", "C. Lu", "E.K. Park", "X. Zhou"], "venue": "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Word Embeddings through Hellinger PCA", "author": ["R. Lebret", "R. Collobert"], "venue": "In Proceedings of the EACL,", "citeRegEx": "Lebret and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2014}, {"title": "Phrase clustering for discriminative learning", "author": ["D. Lin", "X. Wu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lin and Wu,? \\Q2009\\E", "shortCiteRegEx": "Lin and Wu", "year": 2009}, {"title": "Learning Word Vectors for Sentiment Analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of ACL,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshp,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "In NIPS", "citeRegEx": "Mnih and Kavukcuoglu,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity", "author": ["B. Pang", "L. Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Pang and Lee,? \\Q2004\\E", "shortCiteRegEx": "Pang and Lee", "year": 2004}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Improving Document Clustering in a Learned Concept Space", "author": ["Pessiot", "J.-F", "Kim", "Y.-M", "Amini", "M.-R", "P. Gallinari"], "venue": "Information Processing & Management,", "citeRegEx": "Pessiot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pessiot et al\\.", "year": 2010}, {"title": "WordNet-based Text Document Clustering", "author": ["J. Sedding", "D. Kazakov"], "venue": "In Proceedings of the 3rd Workshop on RObust Methods in Analysis of Natural Language Data,", "citeRegEx": "Sedding and Kazakov,? \\Q2004\\E", "shortCiteRegEx": "Sedding and Kazakov", "year": 2004}, {"title": "Ontology-based Text Document Clustering", "author": ["S. Staab", "A. Hotho"], "venue": "In Intelligent Information Processing and Web Mining, Proceedings of the International IIS: IIPWM\u201903 Conference held in Zakopane,", "citeRegEx": "Staab and Hotho,? \\Q2003\\E", "shortCiteRegEx": "Staab and Hotho", "year": 2003}, {"title": "The use of bigrams to enhance text categorization", "author": ["C. Tan", "Y. Wang", "C. Lee"], "venue": "Journal of Information Processing and Management,", "citeRegEx": "Tan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2002}, {"title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "author": ["S.I. Wang", "C.D. Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Wang and Manning,? \\Q2012\\E", "shortCiteRegEx": "Wang and Manning", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "Latent Semantic Analysis (LSA) (Deerwester et al., 1990) uses the term-document matrix and a singular value decomposition (SVD) to represent terms and documents in a new low-dimensional space.", "startOffset": 31, "endOffset": 56}, {"referenceID": 0, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a generative probabilistic model of a corpus.", "startOffset": 34, "endOffset": 53}, {"referenceID": 3, "context": "Some have enriched document representations by integrating core ontologies as background knowledge (Staab & Hotho, 2003), or with Wikipedia concepts and category information (Hu et al., 2009).", "startOffset": 174, "endOffset": 191}, {"referenceID": 15, "context": "A collection of words cannot capture phrases or multi-word expressions, while n-grams have shown to be helpful features in several natural language processing tasks (Tan et al., 2002; Lin & Wu, 2009; Wang & Manning, 2012).", "startOffset": 165, "endOffset": 221}, {"referenceID": 11, "context": "Good word vector representations are obtained very quickly with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Lebret & Collobert, 2014; Pennington et al., 2014).", "startOffset": 97, "endOffset": 197}, {"referenceID": 8, "context": "Pessiot et al. (2010) also proposed probabilistic models for unsupervised dimensionality reduction in the context of document clustering.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Some have enriched document representations by integrating core ontologies as background knowledge (Staab & Hotho, 2003), or with Wikipedia concepts and category information (Hu et al., 2009). Part-of-speech tags have also been used to disambiguate words (Sedding & Kazakov, 2004). All these techniques are based on words alone, which raises another limitation. A collection of words cannot capture phrases or multi-word expressions, while n-grams have shown to be helpful features in several natural language processing tasks (Tan et al., 2002; Lin & Wu, 2009; Wang & Manning, 2012). N -gram features are not commonly used in text classification, probably because the dictionary D tends to grow exponentially with n. Phrase structure extraction can be used to identify only n-grams which are phrase patterns, and thus limit the dictionary size. However, this adds another step to the model, making it more complex. To overcome these barriers, we propose that documents be represented as a bag of semantic concepts, where n-grams are considered instead of only words. Good word vector representations are obtained very quickly with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Lebret & Collobert, 2014; Pennington et al., 2014). Mikolov et al. (2013a) also showed that simple vector addition can often produce meaningful results, such as king - man + woman \u2248 queen.", "startOffset": 175, "endOffset": 1290}, {"referenceID": 11, "context": ", 2013b) or GloVe (Pennington et al., 2014) models, they are trained over a large corpus of unlabeled data in an efficient manner.", "startOffset": 18, "endOffset": 43}, {"referenceID": 1, "context": "Latent Sentiment Analysis (LSA) (Deerwester et al., 1990).", "startOffset": 32, "endOffset": 57}, {"referenceID": 0, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003).", "startOffset": 34, "endOffset": 53}, {"referenceID": 0, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We train the K-topics LDA model using the code released by Blei et al. (2003)6.", "startOffset": 35, "endOffset": 133}, {"referenceID": 2, "context": "Given the set of training documents D\u0303 = {(\u0303fi, yi)| f\u0303i \u2208 R K , yi \u2208 {\u22121, 1}} L i=1, we picked a linear SVM as a classifier, trained using the LIBLINEAR library (Fan et al., 2008):", "startOffset": 162, "endOffset": 180}], "year": 2015, "abstractText": "The bag-of-words (BOW) model is the common approach for classifying documents, where words are used as feature for training a classifier. This generally involves a huge number of features. Some techniques, such as Latent Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been designed to summarize documents in a lower dimension with the least semantic information loss. Some semantic information is nevertheless always lost, since only words are considered. Instead, we aim at using information coming from n-grams to overcome this limitation, while remaining in a low-dimension space. Many approaches, such as the Skip-gram model, provide good word vector representations very quickly. We propose to average these representations to obtain representations of n-grams. All n-grams are thus embedded in a same semantic space. A K-means clustering can then group them into semantic concepts. The number of features is therefore dramatically reduced and documents are then represented as bag of semantic concepts. We show that this model outperforms LSA and LDA on a sentiment classification task, and yields similar results than a traditional BOW-model with far less features.", "creator": "LaTeX with hyperref package"}}}