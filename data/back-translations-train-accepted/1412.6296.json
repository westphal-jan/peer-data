{"id": "1412.6296", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Generative Modeling of Convolutional Neural Networks", "abstract": "The convolutional neural networks (CNNs) have proven to be a powerful tool for discriminative learning. Recently researchers have also started to show interest in the generative aspects of CNNs in order to gain a deeper understanding of what they have learned and how to further improve them. This paper investigates generative modeling of CNNs. The main contributions include: (1) We construct a generative model for the CNN in the form of exponential tilting of a reference distribution. (2) We propose a generative gradient for pre-training CNNs by a non-parametric importance sampling scheme, which is fundamentally different from the commonly used discriminative gradient, and yet has the same computational architecture and cost as the latter. (3) We propose a generative visualization method for the CNNs by sampling from an explicit parametric image distribution. The proposed visualization method can directly draw synthetic samples for any given node in a trained CNN by the Hamiltonian Monte Carlo (HMC) algorithm, without resorting to any extra hold-out images. Experiments on the challenging ImageNet benchmark show that the proposed generative gradient pre-training consistently helps improve the performances of CNNs, and the proposed generative visualization method generates meaningful and varied samples of synthetic images from a large-scale deep CNN.", "histories": [["v1", "Fri, 19 Dec 2014 11:34:37 GMT  (3478kb,D)", "http://arxiv.org/abs/1412.6296v1", null], ["v2", "Thu, 9 Apr 2015 15:07:06 GMT  (25616kb,D)", "http://arxiv.org/abs/1412.6296v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jifeng dai", "yang lu", "ying-nian wu"], "accepted": true, "id": "1412.6296"}, "pdf": {"name": "1412.6296.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jifeng Dai"], "emails": ["jifdai@microsoft.com", "ywu@stat.ucla.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, we have witnessed the triumphant return of feedback-forward neural networks, especially the Convolutionary Neural Networks (CNNs) (LeCun et al., 1989). Fueled by the availability of large, labeled data sets, increased computing power, and the inclusion of new types of nonlinear units, CNNs have proven to be a powerful tool for discriminatory learning. (1) Generative preschool education has the potential to lead the network to a better local optimum; (2) Samples of CNNs have not been thoroughly studied, but it can be very useful for the following reasons: (2) Generative preschool education has the potential to lead the network to a better local optimum."}, {"heading": "2 PAST WORK", "text": "The generative model we are examining is an energy-based model. Such models include field of expertise (Roth & Black, 2009), product of experts (Hinton, 2002), Boltzmann machines (Hinton et al., 2006a), model based on neural networks (Hinton et al., 2006b), etc. However, most of these generative models and learning algorithms have not been applied to learning deep CNNs. The relationship between generative models and discriminatory approaches has been extensively studied, perhaps starting from Efron (Efron, 1975), and more recently from (Jordan, 2002; Liang & Jordan, 2008), etc. Furthermore, the usefulness of generative pre-training for deep learning from us (Erhan et al, 2010), etc."}, {"heading": "3 GENERATIVE MODEL BASED ON CNN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 PROBABILITY DISTRIBUTIONS ON IMAGES", "text": "Let's say we observe images from many different object categories. Let x be an observed image from an object category y; let's consider the following probability distribution on x, py (x; w) = 1Zy (w) exp (fy (x; w)) q (x), (1) where q (x) is a reference distribution common to all categories, fy (x; w) is a scoring function for class x (x; x) dx is the normalizing constant or partition function. The distribution py (x; w) is in the form of an exponential inclination of the reference distribution q (x) and can be considered an energy-based model or an exponential family model."}, {"heading": "3.2 GENERATIVE GRADIENT", "text": "The gradient of the discriminatory Log probability is calculated on the basis of the following categories: (1), (2), (2), (2), (3), where \u03b1y is absorbed as mentioned above in w, (4), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5, (5), (5, 6, (5), (7, (7, 7, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6, (7, 6, 6, 6, 6, 6, (7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, (7), 6, 6, (7, 6, 6"}, {"heading": "3.3 BATCH TRAINING AND GENERATIVE LOSS LAYER", "text": "At first glance, the generative gradient appears due to the need to sample q (x). In fact, with q (x) as a collection of images from all categories, we can use each set of samples as an approximation to q (x) in discriminatory training mode. In the calculation of quantitative gradients (xi, yi) we can use (xi, yi) a series of training examples, and we try to maximize the cost of calculating the generative gradients. In the calculation of quantitative gradients (xi, w) / mps (xi, yj) nj (1) can be used as examples from q (x). In this way, the calculation costs of generative gradients are approximately the same as those of discriminatory gradients. In addition, the calculation of generative gradients can be arranged in the same way."}, {"heading": "3.4 GENERATIVE VISUALIZATION", "text": "Suppose we take care of the node on the top layer (U = 2 m). The idea can be applied to the nodes on any layer. We consider the generation of samples from py (x; w) that have already been learned through discriminatory training (or other methods). To this end, we must proceed from a parametric reference distribution q (x), such as Gaussian White Noise Distribution, which is the maximum entropy distribution or most feature distributions that correspond to the observed training images with normalized edge variances. After discriminatory learning (x; w) for all y distributions, we can proceed from the corresponding Py distribution (x; w) from Hamiltonian Monte Carlo (HMC). We can proceed from a parametric reference distribution q (x), such as the white noise distribution. We can include q (x) as a zero category for discriminatory learning."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 GENERATIVE PRE-TRAINING", "text": "In fact, most of them will be able to move to another world in which they are able to move, and in which they are able to move."}, {"heading": "4.2 GENERATIVE VISUALIZATION", "text": "In the generative visualization experiments, we visualize the nodes of the LeNet network and the AlexNet network realized by discriminatory gradients on MNIST and ImageNet ILSVRC-2012. Networks trained by generative gradients can also be visualized by the same algorithm. First, we visualize the nodes on the last fully connected layer of LeNet. In the experiments, we delete the failing layer to avoid unnecessary noise for the visualization. At the beginning of the visualization, x is initialized by q (x)."}, {"heading": "5 DISCUSSION", "text": "Generative modeling is a fundamental problem of statistical learning. A good learning machine should have strong discriminatory as well as generative abilities. In view of the recent successes of CNNs, it is worth exploring their generative aspects. In this work, we show that a simple generative model can be constructed based on CNN. The generative model helps to pre-train CNN. It also helps to visualize the knowledge of the CNN learned. The proposed scheme for visualizing a node by assuming a reference distribution of white noise can be easily converted into a learning algorithm where the expectation in the generative gradient (a) v1 (b) v2 (d) v2 (d) v3 (d) v4 (e) v5Figure 2: Samples from the nodes in the middle revolutionary layers (v1 to v5) can be explored in the fully trained AlexNet model. 2Figure 3: Samples from the middle revolutionary layers (5) fully synvolutionary (5) the chronous layers of this work."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "The convolutional neural networks (CNNs) have proven to be a powerful tool for<lb>discriminative learning. Recently researchers have also started to show interest in<lb>the generative aspects of CNNs in order to gain a deeper understanding of what<lb>they have learned and how to further improve them. This paper investigates gen-<lb>erative modeling of CNNs. The main contributions include: (1) We construct a<lb>generative model for the CNN in the form of exponential tilting of a reference<lb>distribution. (2) We propose a generative gradient for pre-training CNNs by a<lb>non-parametric importance sampling scheme, which is fundamentally different<lb>from the commonly used discriminative gradient, and yet has the same computa-<lb>tional architecture and cost as the latter. (3) We propose a generative visualization<lb>method for the CNNs by sampling from an explicit parametric image distribution.<lb>The proposed visualization method can directly draw synthetic samples for any<lb>given node in a trained CNN by the Hamiltonian Monte Carlo (HMC) algorithm,<lb>without resorting to any extra hold-out images. Experiments on the challeng-<lb>ing ImageNet benchmark show that the proposed generative gradient pre-training<lb>consistently helps improve the performances of CNNs, and the proposed gener-<lb>ative visualization method generates meaningful and varied samples of synthetic<lb>images from a large-scale deep CNN.", "creator": "LaTeX with hyperref package"}}}