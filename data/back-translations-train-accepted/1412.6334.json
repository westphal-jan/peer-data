{"id": "1412.6334", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Leveraging Monolingual Data for Crosslingual Compositional Word Representations", "abstract": "In this work, we present a novel neural network based architecture for inducing compositional crosslingual word representations. Unlike previously proposed methods, our method fulfills the following three criteria; it constrains the word-level representations to be compositional, it is capable of leveraging both bilingual and monolingual data, and it is scalable to large vocabularies and large quantities of data. The key component of our approach is what we refer to as a monolingual inclusion criterion, that exploits the observation that phrases are more closely semantically related to their sub-phrases than to other randomly sampled phrases. We evaluate our method on a well-established crosslingual document classification task and achieve results that are either comparable, or greatly improve upon previous state-of-the-art methods. Concretely, our method reaches a level of 91.5% and 84.0% accuracy for the English to German and German to English sub-tasks respectively. The latter being an absolute improvement upon the previous state of the art by 7.3% points of accuracy and an improvement of 31.3% in error reduction.", "histories": [["v1", "Fri, 19 Dec 2014 13:23:35 GMT  (370kb)", "https://arxiv.org/abs/1412.6334v1", null], ["v2", "Thu, 26 Feb 2015 07:44:39 GMT  (460kb)", "http://arxiv.org/abs/1412.6334v2", null], ["v3", "Tue, 31 Mar 2015 08:03:57 GMT  (460kb)", "http://arxiv.org/abs/1412.6334v3", null], ["v4", "Sat, 22 Aug 2015 15:22:26 GMT  (460kb)", "http://arxiv.org/abs/1412.6334v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hubert soyer", "pontus stenetorp", "akiko aizawa"], "accepted": true, "id": "1412.6334"}, "pdf": {"name": "1412.6334.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["soyer@nii.ac.jp", "pontus@stenetorp.se", "aizawa@nii.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 141 2,63 34v4 [cs.CL]"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to surpass themselves by embarking on a quest for new ways to conquer the world."}, {"heading": "2 MODEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 INDUCING CROSSLINGUAL WORD REPRESENTATIONS", "text": "Translation errors occur when translating representations between languages. Ideally, expressions of the same meaning (words, phrases, or documents) should be represented by the same vectors, regardless of the language in which they are expressed. The more different these representations are from language 1 (l1) to language 2 (l2), the greater the translation error. 2. Monolingual errors occur because the word, phrase, or document within the same language is not expressive enough. In the case of classification, for example, this would mean that the representations do not have enough discriminatory power to achieve high accuracy. The path to high performance in any task involving transmission errors and monolingual errors is to minimize both translation errors and monolingual representations that are both expressive and cross-border."}, {"heading": "2.2 CREATING REPRESENTATIONS FOR PHRASES AND DOCUMENTS", "text": "After the work of Klementiev et al. (2012); Hermann & Blunsom (2014); Gouws et al. (2014), we represent each word as a vector and use separate word representations for each language. Similar to Hermann & Blunsom (2014), we look up the vector representations for all words of a given sentence in the corresponding search table and apply a composition function to convert these word vectors into a sentence representation. To create document representations, we apply the same composition function again, this time to transform the representations of all sentences in a document into a document representation. To give an example of another possible candidate composition function that can be written as the sum of all word representations wi in a given phrase ([w1, w2, \u00b7 \u00b7, wl]) = l = i = 1wi (1) To give an example of another possible candidate composition function, we also use the Bigram-based addition function (Bi)."}, {"heading": "2.3 OBJECTIVE", "text": "Following Klementiev et al. (2012), we divide our goal into two sub-goals, a bilingual goal that minimizes transmission errors, and a monolingual goal that minimizes monolingual errors for l1 and l2. We formalize the loss over the entire training set asLtotal = Nbi \u2211 i = 1Lbi (v l1 i, vl2 i) + Nmono1 \u2211 i = 1Lmono (x l1 i) + Nmono2 \u2211 i = 1Lmono (y l2 i) + 1 Lmono (y l2 i), where Lbi is the bilingual loss for two aligned sentences, vi is one example of the series of nbi-aligned sentences in Languages 1 and 2, Lmono is the monolingual loss that we summarize via Nmono1 sentences xl1 i from corpora in Language 1 and Nmono2 i from corpora in Language 2."}, {"heading": "2.3.1 BILINGUAL OBJECTIVE", "text": "In view of a pair of aligned sentences, sl1 1 in l1 and s l2 1 in l2, we first calculate their vector representations vl1 1 and vl2 1 using the composition function. Since the sentences are either translations of each other or at least translations of very similar meaning, we demand that their vector representations be similar, and express this as minimizing the square Euclidean distance between vl11 and vl2. More formally, we write Lbi (v l1, vl2) = VL1 \u2212 vl2-2 (4) for any two vector representations vl1 and vl2 that correspond to the sentences of an aligned translation pair. The bilingual goal on its own is degenerated, since setting the vector representations of all sentences to the same value is a trivial solution. We therefore combine this bilingual goal with a monolingual goal."}, {"heading": "2.3.2 MONOLINGUAL OBJECTIVE", "text": "This year it is so far that it will only take a few days to reach an agreement."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 CROSSLINGUAL DOCUMENT CLASSIFICATION", "text": "Crosslingual document classification is a task in which a classifier is trained to classify documents in one language (l1) and later to apply them to documents in another language (l2), which requires either a transformation of the classifier itself to adapt to the new language, or a transformation / transmission of representations of the text for both languages. The crosslingual word and document representations triggered by the approach proposed in this paper are an intuitive method to address the crosslingual classification of documents (Lewis et al., 2004). We also evaluate our method with respect to the crosslingual classification task introduced by Klementiev et al. (2012) for documents in four categories: economics, government / social, markets or corporate. Maintaining the original setup, we train an average perception of the documents (Collins, 2002) for 10 iterations of representations of German documents (each used in a language) and the other documentation (Klev)."}, {"heading": "3.2 INDUCING CROSSLINGUAL WORD REPRESENTATIONS", "text": "In order to create representations using the method proposed in this thesis, we need at least one bilingual corpus of aligned sentences = 3.5 million. In addition, the representations can use monolingual data from one or both languages. As Klementiev et al. (2012), we select EuroParl v7 (Koehn, 2005) as our bilingual corpus and use the English and German parts of the RCV1 and RCV2 corpora as monolingual resources. To avoid test distortion, we exclude all documents that are part of the cross-border classification task. We recognize sentence boundaries based on pre-trained models of the dot tokenizer (Kiss & Strunk, 2006) that are shipped with NLTK1 and perform tokenization and praise casing with the scripts used with the cdec-Decoder2. We follow Turian et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al (2010, we remove the English paral.) and English parallels (we have them all in English and German equivalents)."}, {"heading": "4 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 CROSSLINGUAL DOCUMENT CLASSIFICATION", "text": "We compare our method with various architectures introduced in previous work. As these methods differ in their ability to handle monolingual data, we evaluate several versions of our model using different data sources and sizes for training. We also follow the lines of previous work and use 40-dimensional word representations. We report results using the first 500,000 sentence pairs of EuroParl (Euro500k), the full EuroParl corpus (EuroFullReuters), the first 500,000 sentence pairs of EuroParl and the German and English texts from the Reuters corpus as monolingual data (Euro500kReuters) and a version using the full EuroParl and Reuters corpus (EuroFullReuters). Table 2 shows results for all these configurations. The results table includes previous work as well as the glossed, machine translation and majority class baselines by Klementiev et al al al al al al al al al al al al al al al al al al al al. (2012) Our method achieves results that are more comparable or better than previous ones."}, {"heading": "4.2 INTERESTING PROPERTIES OF THE INDUCED CROSSLINGUAL WORD REPRESENTATIONS", "text": "For a bilingual word representation model that uses monolingual data, the most difficult cases to solve are words that appear in the monolingual data, but not in the bilingual data. Since the model has no direct signal about which translations these words should correspond to, their position in the vector space is entirely determined by how the monolingual target arranges them. Therefore, looking specifically at these difficult examples is a good way to get an idea of how well the monolingual and bilingual target should complement each other. In Table 3, we list some of the most common words that are present in the monolingual data but not in the bilingual data. The closest neighbors are topically strongly related to their corresponding queries."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "Our novel approach to learning monolingual representation naturally integrates with our bilingual goal, allowing us to draw on bilingual corporal sentence alignment as well as monolingual data. It is agnostic in its choice of composition function and allows for more complex (e.g., preservation of word order information) ways to compose phrase representation from word representation, and our models provide comparable or substantially improved results for classifying cross-border documents (Klementiev et al., 2012). To enhance the expressiveness of our method, we plan to study more complex composition functions, possibly based on folding, to obtain word order information. We consider the goal of monolingual inclusion on its own and will evaluate its performance relative to related methods when learning word representation from monolingual data."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by the Data Centric Science Research Commons Project at the Information and Systems Research Organization and the Japan Society for the Promotion of Science KAKENHI Grant Number 13F03041."}], "references": [{"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni", "Marco", "Dinu", "Georgiana", "Kruszewski", "Germ\u00e1n"], "venue": "In ACL, pp", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Julia: A Fast Dynamic Language for", "author": ["Bezanson", "Jeff", "Karpinski", "Stefan", "Shah", "Viral B", "Edelman", "Alan"], "venue": "Technical Computing. arXiv,", "citeRegEx": "Bezanson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bezanson et al\\.", "year": 2012}, {"title": "An Autoencoder Approach to Learning Bilingual Word Representations", "author": ["Chandar A P", "Sarath", "Lauly", "Stanislas", "Larochelle", "Hugo", "Khapra", "Mitesh", "Ravindran", "Balaraman", "Raykar", "Vikas C", "Saha", "Amrita"], "venue": "In NIPS,", "citeRegEx": "P et al\\.,? \\Q2014\\E", "shortCiteRegEx": "P et al\\.", "year": 2014}, {"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Collins", "Michael"], "venue": "In EMNLP, pp", "citeRegEx": "Collins and Michael.,? \\Q2002\\E", "shortCiteRegEx": "Collins and Michael.", "year": 2002}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML, pp", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "BilBOWA: Fast Bilingual Distributed Representations without", "author": ["Gouws", "Stephan", "Bengio", "Yoshua", "Corrado", "Greg"], "venue": "Word Alignments. arXiv,", "citeRegEx": "Gouws et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2014}, {"title": "Multilingual Models for Compositional Distributed Semantics", "author": ["Hermann", "Karl Moritz", "Blunsom", "Phil"], "venue": "In ACL, pp", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Unsupervised Multilingual Sentence", "author": ["Kiss", "Tibor", "Strunk", "Jan"], "venue": "Boundary Detection. CL,", "citeRegEx": "Kiss et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kiss et al\\.", "year": 2006}, {"title": "Inducing Crosslingual Distributed Representations of Words", "author": ["Klementiev", "Alexandre", "Titov", "Ivan", "Bhattarai", "Binod"], "venue": "In COLING, pp", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Koehn", "Philipp"], "venue": "In MT summit,", "citeRegEx": "Koehn and Philipp.,? \\Q2005\\E", "shortCiteRegEx": "Koehn and Philipp.", "year": 2005}, {"title": "RCV1: A New Benchmark Collection for Text Categorization", "author": ["Lewis", "David D", "Yang", "Yiming", "Rose", "Tony G", "Li", "Fan"], "venue": "Research. JMLR,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In ICLR Workshop,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting Similarities among Languages for Machine", "author": ["Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "Translation. arXiv,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew", "Potts", "Christopher"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian", "Joseph", "Ratinov", "Lev-Arie", "Bengio", "Yoshua"], "venue": "In ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Co-training for Cross-lingual Sentiment Classification", "author": ["Wan", "Xiaojun"], "venue": "In ACL, pp", "citeRegEx": "Wan and Xiaojun.,? \\Q2009\\E", "shortCiteRegEx": "Wan and Xiaojun.", "year": 2009}], "referenceMentions": [{"referenceID": 14, "context": "Dense vector representations (embeddings) of words and phrases, as opposed to discrete feature templates, have recently allowed for notable advances in the state of the art of Natural Language Processing (NLP) (Socher et al., 2013; Baroni et al., 2014).", "startOffset": 210, "endOffset": 252}, {"referenceID": 0, "context": "Dense vector representations (embeddings) of words and phrases, as opposed to discrete feature templates, have recently allowed for notable advances in the state of the art of Natural Language Processing (NLP) (Socher et al., 2013; Baroni et al., 2014).", "startOffset": 210, "endOffset": 252}, {"referenceID": 15, "context": "Unlike discrete feature templates, these representations allow supervised methods to readily make use of unlabeled data, effectively making them semi-supervised (Turian et al., 2010).", "startOffset": 161, "endOffset": 182}, {"referenceID": 9, "context": "Crosslingual representations are induced to represent words, phrases, or documents for more than one language, where the representations are constrained to preserve representational similarity or can be transformed between languages (Klementiev et al., 2012; Mikolov et al., 2013b; Hermann & Blunsom, 2014).", "startOffset": 233, "endOffset": 306}, {"referenceID": 9, "context": "Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classification (Klementiev et al., 2012).", "startOffset": 130, "endOffset": 155}, {"referenceID": 0, "context": ", 2013; Baroni et al., 2014). These representations are typically induced from large unannotated corpora by predicting a word given its context (Collobert & Weston, 2008). Unlike discrete feature templates, these representations allow supervised methods to readily make use of unlabeled data, effectively making them semi-supervised (Turian et al., 2010). A recent focus has been on crosslingual, rather than monolingual, representations. Crosslingual representations are induced to represent words, phrases, or documents for more than one language, where the representations are constrained to preserve representational similarity or can be transformed between languages (Klementiev et al., 2012; Mikolov et al., 2013b; Hermann & Blunsom, 2014). In particular, crosslingual representations can be helpful for tasks such as translation or to leverage training data in a source language when little or no training data is available for a target language. Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classification (Klementiev et al., 2012). Mikolov et al. (2013b) induced language-specific word representations, learned a linear mapping between the language-specific representations using bilingual word pairs and evaluated their approach \u2217Currently at the University College London.", "startOffset": 8, "endOffset": 1134}, {"referenceID": 7, "context": "Klementiev et al. (2012) used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "Klementiev et al. (2012) used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language. They also introduced a dataset for crosslingual document classification and evaluated their work on this task. Hermann & Blunsom (2014) introduced a method to induce compositional crosslingual word representations from sentence-aligned bilingual corpora.", "startOffset": 0, "endOffset": 360}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al.", "startOffset": 187, "endOffset": 239}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data.", "startOffset": 187, "endOffset": 294}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al.", "startOffset": 187, "endOffset": 605}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al.", "startOffset": 187, "endOffset": 629}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al. (2014) all learn their representations using a word-level monolingual objective.", "startOffset": 187, "endOffset": 650}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al. (2014) all learn their representations using a word-level monolingual objective. This effectively means that compositionality is not encouraged by the monolingual objective, which may be problematic when composing word representations for a phrase or document-level task. While the method of Hermann & Blunsom (2014) allows for arbitrary composition functions, they are limited to using sentence-aligned bilingual data and it is not immediately obvious how their method can be extended to make use of monolingual data.", "startOffset": 187, "endOffset": 960}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al. (2014) all learn their representations using a word-level monolingual objective. This effectively means that compositionality is not encouraged by the monolingual objective, which may be problematic when composing word representations for a phrase or document-level task. While the method of Hermann & Blunsom (2014) allows for arbitrary composition functions, they are limited to using sentence-aligned bilingual data and it is not immediately obvious how their method can be extended to make use of monolingual data. Lastly, while the method of Chandar A P et al. (2014) suffers from neither of the above issues, their method represents each sentence as a bag of words vector with the size of the whole vocabulary.", "startOffset": 187, "endOffset": 1216}, {"referenceID": 8, "context": "Following the work of Klementiev et al. (2012); Hermann & Blunsom (2014); Gouws et al.", "startOffset": 22, "endOffset": 47}, {"referenceID": 8, "context": "Following the work of Klementiev et al. (2012); Hermann & Blunsom (2014); Gouws et al.", "startOffset": 22, "endOffset": 73}, {"referenceID": 6, "context": "(2012); Hermann & Blunsom (2014); Gouws et al. (2014) we represent each word as a vector and use separate word representations for each language.", "startOffset": 34, "endOffset": 54}, {"referenceID": 6, "context": "(2012); Hermann & Blunsom (2014); Gouws et al. (2014) we represent each word as a vector and use separate word representations for each language. Like Hermann & Blunsom (2014), we look up the vector representations for all words of a given sentence in the corresponding lookup table and apply a composition function to transform these word vectors into a sentence representation.", "startOffset": 34, "endOffset": 176}, {"referenceID": 9, "context": "Following Klementiev et al. (2012) we split our objective into two sub-objectives, a bilingual objective minimizing the transfer errors and a monolingual objective minimizing the monolingual errors for l1 and l2.", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "Klementiev et al. (2012) use a neural language model to leverage monolingual data.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "Klementiev et al. (2012) use a neural language model to leverage monolingual data. However, this does not explicitly encourage compositionality of the word representations. Hermann & Blunsom (2014) achieve good results with a noise-contrastive objective, discriminating aligned translation pairs from randomly sampled pairs.", "startOffset": 0, "endOffset": 198}, {"referenceID": 5, "context": "Gouws et al. (2014) introduced BilBOWA combining a bilingual objective with the Skip-Gram model proposed by Mikolov et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Gouws et al. (2014) introduced BilBOWA combining a bilingual objective with the Skip-Gram model proposed by Mikolov et al. (2013a) which predicts the context of a word given the word itself.", "startOffset": 0, "endOffset": 131}, {"referenceID": 5, "context": "Gouws et al. (2014) introduced BilBOWA combining a bilingual objective with the Skip-Gram model proposed by Mikolov et al. (2013a) which predicts the context of a word given the word itself. They achieve high accuracy on the German \u2192 English sub-task of the crosslingual document classification task introduced by Klementiev et al. (2012). Chandar A P et al.", "startOffset": 0, "endOffset": 339}, {"referenceID": 2, "context": "Chandar A P et al. (2014) presented a bag-of-words auto-encoder model which is the current state of the art for the English \u2192 German sub-task for the same task.", "startOffset": 10, "endOffset": 26}, {"referenceID": 2, "context": "Chandar A P et al. (2014) presented a bag-of-words auto-encoder model which is the current state of the art for the English \u2192 German sub-task for the same task. Both the auto-encoder based model and BilBOWA require a sentencealigned bilingual corpus, but in addition are capable of leveraging monolingual data. However, due to their bag-of-words based nature, their architectures implicitly restrict how sentence representations are composed from word representations. We extend the idea of the noise-contrastive objective given by Hermann & Blunsom (2014) to the monolingual setting and propose a framework that, like theirs, is agnostic to the choice of composition function and operates on the phrase level.", "startOffset": 10, "endOffset": 557}, {"referenceID": 11, "context": "The goal is to correctly classify news articles taken from the English and German sections of the RCV1 and RCV2 corpus (Lewis et al., 2004) into one of four categories: Economics, Government/Social, Markets, or Corporate.", "startOffset": 119, "endOffset": 139}, {"referenceID": 9, "context": "Like previous work, we evaluate our method on the crosslingual document classification task introduced by Klementiev et al. (2012). The goal is to correctly classify news articles taken from the English and German sections of the RCV1 and RCV2 corpus (Lewis et al.", "startOffset": 106, "endOffset": 131}, {"referenceID": 9, "context": "Like previous work, we evaluate our method on the crosslingual document classification task introduced by Klementiev et al. (2012). The goal is to correctly classify news articles taken from the English and German sections of the RCV1 and RCV2 corpus (Lewis et al., 2004) into one of four categories: Economics, Government/Social, Markets, or Corporate. Maintaining the original setup, we train an averaged perceptron (Collins, 2002) for 10 iterations on representations of documents in one language (English/German) and evaluate its performance on representations of documents in the corresponding other language (German/English). We use the original data and the original implementation of the averaged perceptron used by Klementiev et al. (2012) to evaluate the document representations created by our method.", "startOffset": 106, "endOffset": 749}, {"referenceID": 5, "context": "To speed up the convergence of training we use AdaGrad (Duchi et al., 2011).", "startOffset": 55, "endOffset": 75}, {"referenceID": 1, "context": "Training our model3, implemented in a high-level, dynamic programming language (Bezanson et al., 2012), http://www.", "startOffset": 79, "endOffset": 102}, {"referenceID": 7, "context": "Like Klementiev et al. (2012) we choose EuroParl v7 (Koehn, 2005) as our bilingual corpus and leverage the English and German parts of the RCV1 and RCV2 corpora as monolingual resources.", "startOffset": 5, "endOffset": 30}, {"referenceID": 7, "context": "Like Klementiev et al. (2012) we choose EuroParl v7 (Koehn, 2005) as our bilingual corpus and leverage the English and German parts of the RCV1 and RCV2 corpora as monolingual resources. To avoid a testing bias, we exclude all documents that are part of the crosslingual classification task. We detect sentence boundaries using pre-trained models of the Punkt tokenizer (Kiss & Strunk, 2006) shipped with NLTK1 and perform tokenization and lowercasing with the scripts deployed with the cdec decoder2. Following Turian et al. (2010) we remove all English sentences (and their German correspondences in EuroParl) that have a lowercase nonlowercase ratio of less than 0.", "startOffset": 5, "endOffset": 533}, {"referenceID": 9, "context": "8 I-Matrix (Klementiev et al., 2012) EuroFullReuters 77.", "startOffset": 11, "endOffset": 36}, {"referenceID": 6, "context": "2 BilBOWA (Gouws et al., 2014) Euro500k 86.", "startOffset": 10, "endOffset": 30}, {"referenceID": 2, "context": "This can be compared to for example Chandar A P et al. (2014) which train their auto-encoder model for 3.", "startOffset": 46, "endOffset": 62}, {"referenceID": 9, "context": "The result table includes previous work as well as the Glossed, the machine translation and the majority class baselines from Klementiev et al. (2012). Our method achieves results that are comparable or improve upon the previous state of the art for all dataset configurations.", "startOffset": 126, "endOffset": 151}, {"referenceID": 9, "context": "The result table includes previous work as well as the Glossed, the machine translation and the majority class baselines from Klementiev et al. (2012). Our method achieves results that are comparable or improve upon the previous state of the art for all dataset configurations. It advances the state of the art for the EN \u2192 DE sub-task by 0.9% points of accuracy and greatly outperforms the previous state of the art for the DE \u2192 EN sub-task, where it yields an absolute improvement of 7.7% points of accuracy. The latter corresponds to an error reduction of 33.0% in comparison to the previous state of the art. An important observation is that including monolingual data is strongly beneficial for the classification accuracy. We found increases in performance to 80.6% for DE \u2192 EN and 88.6% accuracy for EN \u2192 DE, even when using as little as 5% of the monolingual data. We hypothesize that the key cause of this effect is domain adaptation. From this observation it is also worth pointing out that our method is on par with the previous state of the art for the DE \u2192 EN sub-task using no monolingual training data and would improve upon it using as little as 5% of the monolingual data. To show that our method achieves high accuracy even with a reduced vocabulary, we discard representations for infrequent terms and report results using our best setup with the same vocabulary size as Klementiev et al. (2012).", "startOffset": 126, "endOffset": 1415}, {"referenceID": 9, "context": "For crosslingual document classification (Klementiev et al., 2012) our models perform comparably or greatly improve upon previously reported results.", "startOffset": 41, "endOffset": 66}], "year": 2015, "abstractText": "In this work, we present a novel neural network based architecture for inducing compositional crosslingual word representations. Unlike previously proposed methods, our method fulfills the following three criteria; it constrains the wordlevel representations to be compositional, it is capable of leveraging both bilingual and monolingual data, and it is scalable to large vocabularies and large quantities of data. The key component of our approach is what we refer to as a monolingual inclusion criterion, that exploits the observation that phrases are more closely semantically related to their sub-phrases than to other randomly sampled phrases. We evaluate our method on a well-established crosslingual document classification task and achieve results that are either comparable, or greatly improve upon previous state-of-the-art methods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for the English to German and German to English sub-tasks respectively. The former advances the state of the art by 0.9% points of accuracy, the latter is an absolute improvement upon the previous state of the art by 7.7% points of accuracy and an improvement of 33.0% in error reduction.", "creator": "LaTeX with hyperref package"}}}