{"id": "1412.6550", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "FitNets: Hints for Thin Deep Nets", "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.", "histories": [["v1", "Fri, 19 Dec 2014 22:40:51 GMT  (124kb)", "http://arxiv.org/abs/1412.6550v1", null], ["v2", "Fri, 9 Jan 2015 20:56:15 GMT  (124kb)", "http://arxiv.org/abs/1412.6550v2", null], ["v3", "Fri, 27 Feb 2015 18:44:36 GMT  (126kb)", "http://arxiv.org/abs/1412.6550v3", null], ["v4", "Fri, 27 Mar 2015 11:52:28 GMT  (132kb)", "http://arxiv.org/abs/1412.6550v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["adriana romero", "nicolas ballas", "samira ebrahimi kahou", "antoine chassang", "carlo gatta", "yoshua bengio"], "accepted": true, "id": "1412.6550"}, "pdf": {"name": "1412.6550.pdf", "metadata": {"source": "CRF", "title": "FITNETS: HINTS FOR THIN DEEP NETS", "authors": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 141 2.65 50v1 [cs.LG] 1 9D ec2 01"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they want. (...) It is not so that people are able to determine for themselves what they want. (...) It is so that they do not want it to be done. (...) It is so that they do not want it to be done. (...) It is so. (...) It is so. (...) It is so. (...) \"\" \"It is so.\" (...). \"(...).\" (...). \"(...).\" (...). \"(.\"). \"(.\"). \"(.\"). \"(.\"). \"(.\"). \"(.\"). \"(.\"). \"(.\" (.). \"(.)\" (. \"(.).\" (.). \"(.)\" (. \").\" (. \").\" (. \").\" (. \").\" (. \").\" (. \").\" (. \").\" (. \").\" (. \").\" (. \").\" (. \").\"). \"(.\"). \"(.\"). \").\" (. \").\"). \"(.\"). \").\" (. \").\" (. \").\"). \"(.\"). \").\" (. \").\" (. \").\"). \"(.\"). \"(.\"). \"(.\"). \").\" (. \").\"). \"(.\"). \"(.\"). \").\" (. \")."}, {"heading": "2 METHOD", "text": "In this section, we explain the proposed student-teacher framework for training FitNet from flatter and wider nets. First, we look at the recently proposed KD. Second, we highlight the proposed Hints algorithm to guide the FitNet throughout the training process. Finally, we describe how the FitNet is trained step by step."}, {"heading": "2.1 REVIEW OF KNOWLEDGE DISTILLATION", "text": "To get a quicker conclusion, we explore the recently proposed compression system (Hinton > Dean, 2014) that forms a student network from the softened output of an ensemble of broader networks, teacher network. The idea is to allow the student network to capture not only the information provided by the true labels, but also the finer structure learned from the teacher network. The framework can be summarized as follows: Let's create a teacher network with an output softmax PT = softmax (aT), where aT is the vector of teacher pre-softmax activations, for example. In the case where the teacher model is a single network, aT represents the weighted sums of the output layer, while the teacher model is the result of an ensemble either PT or aT, achieved by averaging the outputs from different networks (each for arithmetic or geometric averaging)."}, {"heading": "2.2 HINT-BASED TRAINING", "text": "In order to advance the education of students, we must abide by the rules we have imposed on ourselves. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules."}, {"heading": "2.3 FITNET STAGE-WISE TRAINING", "text": "Based on a trained teacher network and a randomly initialized FitNet (Fig. 1 (a)), we add a regressor, which is parameterized by Wr on the FitNet leadership layer, and train the FitNet parameters WGuided up to the leadership layer WGuided to minimize the equation. (3) (Fig. 1 (b)))) Finally, we train the parameters of the entire FitNet leadership system WS from the pre-trained parameters to minimize the equation. (2) (Fig. 1 details the FitNet training process. Algorithm 1 FitNet Stage-Wise Training. The algorithm receives the parameters of the entire FitNet WS as input of the trained parameters WT of a teacher, the randomly initialized parameters WS."}, {"heading": "2.4 RELATION TO CURRICULUM LEARNING", "text": "In this section, we argue that our clue-based training with KD can be considered a specific form of curriculum learning (Bengio, 2009).The Learning Curriculum has proven to accelerate the convergence of education and potentially improve the generalization of the model by correctly selecting a sequence of distributions seen by the learner: from simple examples to more complex ones. A Curriculum Learning Extension (Gulcehre & Bengio, 2013) has also shown that by using guidance on an intermediate layer during the training, one could greatly simplify the training. However, Bengio (2009) uses hand-defined heuristics to mitigate the \"simplicity\" of an example in a sequence, and Gulcehre & Bengio (2013) the guidance of the guide requires some prior knowledge of the final task. Both of these curriculum learning strategies tend to be problem-specific. Our approach mitigates this problem by using a teacher model."}, {"heading": "3 RESULTS ON BENCHMARK DATASETS", "text": "In this section we show the results for different benchmark datasets. The architectures of all networks as well as the training details are presented in the supplementary material."}, {"heading": "3.1 CIFAR-10 AND CIFAR-100", "text": "The CIFAR-10 and CIFAR-100 datasets (Fellow & Hinton, 2009) consist of 32 x 32 pixel RGB images belonging to 10 and 100 different classes, respectively, and both contain 50K training images and 10K test images. CIFAR-10 has 1,000 samples per class, whereas CIFAR-100 has 100 samples per class. Like Goodfellow et al. (2013b), we have normalized the data sets for contrast normalization and ZCA whitening.CIFAR-10: To validate our approach, we have trained a teacher network of maxout revolutionary layers, as in Goodfellow et al. (2013b), and designed a FitNet with 17 revolutionary layers and roughly 1 / 3 of the parameters. To validate our approach, we have trained the 11th layer of the student network to mimic the 2nd layer of the teacher network."}, {"heading": "3.2 SVHN", "text": "The SVHN dataset (Netzer et al., 2011) consists of 32 x 32 color images of house numbers collected by Google Street View. There are 73,257 images in the training set, 26,032 images in the test set, and 531,131 less difficult examples. We follow the evaluation process of Goodfellow et al. (2013b) and use their maxout network as a teacher. We trained a 13-layer FitNet composed of 11 maxout revolutionary layers, a fully interconnected layer, and a Softmax layer. # params Misclass CompressionFitNet algorithm 1.5M 2.42% Teacher x 4.9M 2.38% State-of-the-art methods Maxout 2.47% Network in Network 2.35% Deeply-Supervised Networks 1.92% Table 3: SVHN errorAlgorithm # params Misclass CompressionTeacher x 361K 0.55% Standard prop vs. 30K Net-Net-Comparable Net40.39% Net-300.39% Net-Net-40.39%"}, {"heading": "3.3 MNIST", "text": "As a health check for the training process, we evaluated the proposed method using the MNIST dataset (LeCun et al., 1998). MNIST is a set of handwritten digits (from 0 to 9), consisting of 28x28 pixels of grayscale images, with 60K training images and 10K test images. We trained a teacher network of maxout Convolutionary Layers, as reported in Goodfellow et al. (2013b), and designed a FitNet that is twice as deep as the teacher network and contains approximately 8% of the parameters. The fourth layer of the student network was trained to mimic the second layer of the teacher network. Table 4 reports the results obtained. In order to verify the impact of using clues, we trained the FitNet architecture either with (1) standard backprop, (2) KD or (3) back-based training (HT). In case of the training, the Fitprop achieves less than the Fitpro51% error level while the standard backfill of 1.9% is lower."}, {"heading": "3.4 AFLW", "text": "AFLW (Koestinger et al., 2011) is a real face database containing 25K annotated images. In order to evaluate the proposed framework in a face recognition setting, we extracted positive samples by adjusting the annotated regions of the images to fit 16x16 pixel fields. Similarly, we extracted 25K 16x16 pixel fields that did not contain faces from ImageNet as negative samples (Russakovsky et al., 2014). We used 90% of the extracted patches to train the network.In this experiment, we aimed to evaluate the method on a different type of architecture. Therefore, we trained a teacher network of 3 ReLU revolutionary layers and a sigmoid output layer. We designed a first FitNet (FitNet 1) with 15 times less multiplications than the teacher network, and a second FitNet (FitNet 2) with 2.5 times less multiplications than the teacher network have both N7 and N7."}, {"heading": "4 ANALYSIS OF EMPIRICAL RESULTS", "text": "We examine empirically the benefits of our approach by comparing different networks trained using standard back propagation (cross-entropy w.r.t. labels), KD, or Hint-based Training (HT). Experiments are conducted using CIFAR-10 datasets (Krizhevsky & Hinton, 2009).We compare networks with increasing depth at a fixed computing budget. Each network consists of successive layers with a grain size of 3 x 3, followed by a maximum nonlinearity and a non-overlapping max pooling of 2 x 2. The last max pooling takes the maximum across all remaining spatial dimensions, resulting in a vector representation of 1 x 1. We only change the depth and number of channels per convolution between different networks, i.e. the number of channels per convolutionary layer decreases as the network depth increases to respect a given compilation budget."}, {"heading": "4.1 ASSISTING THE TRAINING OF DEEP NETWORKS", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "4.2 TRADE-OFF BETWEEN MODEL PERFORMANCE AND EFFICIENCY", "text": "To evaluate the effectiveness of FitNets, we measure the total time it takes to process CIFAR 10 test samples on a GPU. In this experiment, we retrain our FitNets to training plus validation sets as in Goodfellow et al. (2013b), for fair comparison with the teacher. FitNet 1, our smallest network, is an order of magnitude faster than the teacher at 2.7% of teacher capacity and shows only a slight 1.3% drop in performance. FitNet 2, which slightly increases capacity, now outperforms the teacher by 0.9%, while it is still a strong factor of 4.64 faster. By further increasing network capacity and depth in FitNet 3 and 4, we improve performance by up to 1.6% and still outperform the teacher."}, {"heading": "5 CONCLUSION", "text": "We proposed a novel framework to compress broad and deep networks into thin and deeper ones by introducing middle-level clues from the hidden levels of the teacher to guide the student's training process. We are able to use these clues to train very deep student models with fewer parameters that can generalize better and / or run faster than their teachers. Our experiments with benchmark data sets emphasize that low-capacity deep networks are able to extract feature representations that are comparable or even better than networks with up to 10 times more parameters. The evidence-based training suggests that more effort should be made to explore new training strategies to harness the power of deep networks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the developers of Theano (Bastien et al., 2012) and Pylearn2 (Goodfellow et al., 2013a) and the computing resources of Compute Canada and Calcul Que \u0301 bec. This work was partially supported by NSERC, CIFAR and Canada Research Chairs, Project TIN2013-41751, grant 2014-SGR-221 and Spanish MINECO grant TIN2012-38187-C03."}, {"heading": "A SUPPLEMENTARY MATERIAL: NETWORK ARCHITECTURES AND TRAINING PROCEDURES", "text": "In complementary matter, we describe all network architectures used throughout the paper world.A.1 CIFAR-10 / CIFAR-100In this section, we describe the Teacher and FitNet architectures used in the CIFAR-10 / CIFAR-100 environment.A.1.1 TEACHERWe used the maxout convolutional network reported in Goodfellow et al. (2013b) The teacher architecture has 3 convolutionally hidden layers of 96-192-192 units. Each convolutional layer is followed by a maxout non-linearity (with 2 linear pieces) and max-pooling operator with corresponding windows of 4x4, 4x4 and 2x2 pixels. The third convolutional layer is followed by a fully connected maxout non-linearity (with 2 linear pieces)."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": "In NIPS,", "citeRegEx": "Ba and Caruana,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana", "year": 2014}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS, pp", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["D. Erhan", "P.A. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "In AISTATS,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Pylearn2: a machine learning research", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Knowledge matters: Importance of prior information for optimization", "author": ["C. Gulcehre", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Gulcehre and Bengio,? \\Q2013\\E", "shortCiteRegEx": "Gulcehre and Bengio", "year": 2013}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Distilling knowledge in a neural network", "author": ["Hinton", "O.G. Vinyals", "J. Dean"], "venue": "In Deep Learning and Representation Learning Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization", "author": ["M. Koestinger", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "In First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies,", "citeRegEx": "Koestinger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koestinger et al\\.", "year": 2011}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In ICML, pp", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "In NIPS", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A. Ng"], "venue": "In Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "R. Collobert"], "venue": "In ICML,", "citeRegEx": "Weston et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2008}, {"title": "Teacher and FitNet architectures used in the MNIST experiments. We trained a teacher network of maxout convolutional layers as reported", "author": ["Goodfellow"], "venue": null, "citeRegEx": "Goodfellow,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow", "year": 2013}, {"title": "2013b), we added zero padding to the second convolutional layer. We designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters. The student architecture has 6 maxout convolutional hidden layers (with 2 linear pieces each) of 16-16-16-16-12-12 units, respectively. Max-pooling is only applied every second layer in regions of 4x4-4x4-2x2", "author": ["Goodfellow"], "venue": null, "citeRegEx": "Goodfellow,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Depth is a fundamental aspect of representation learning, since it encourages the re-use of features, and leads to more abstract and invariant representations at higher layers (Bengio et al., 2013).", "startOffset": 176, "endOffset": 197}, {"referenceID": 14, "context": "importance of depth has been verified (1) theoretically: deep representations are exponentially more expressive than shallow ones for some families of functions (Montufar et al., 2014); and (2) empirically: the two top-performers of ImageNet use deep convolutional networks with 19 and 22 layers, respectively (Simonyan & Zisserman, 2014) and (Szegedy et al.", "startOffset": 161, "endOffset": 184}, {"referenceID": 12, "context": "Nevertheless, training deep architectures has proven to be challenging (Larochelle et al., 2007; Erhan et al., 2009), since they are composed of successive non-linearities and, thus result in highly non-convex and non-linear functions.", "startOffset": 71, "endOffset": 116}, {"referenceID": 5, "context": "Nevertheless, training deep architectures has proven to be challenging (Larochelle et al., 2007; Erhan et al., 2009), since they are composed of successive non-linearities and, thus result in highly non-convex and non-linear functions.", "startOffset": 71, "endOffset": 116}, {"referenceID": 8, "context": "On the one hand, pre-training strategies, whether unsupervised (Hinton et al., 2006; Bengio et al., 2007) or supervised (Bengio et al.", "startOffset": 63, "endOffset": 105}, {"referenceID": 2, "context": "On the one hand, pre-training strategies, whether unsupervised (Hinton et al., 2006; Bengio et al., 2007) or supervised (Bengio et al.", "startOffset": 63, "endOffset": 105}, {"referenceID": 2, "context": ", 2007) or supervised (Bengio et al., 2007) train the network parameters in a greedy layerwise fashion in order to initialize the network parameters in a potentially good basin of attraction.", "startOffset": 22, "endOffset": 43}, {"referenceID": 18, "context": "Similarly, semisupervised embedding (Weston et al., 2008) provides guidance to an intermediate layer to help learn very deep networks.", "startOffset": 36, "endOffset": 57}, {"referenceID": 1, "context": "Alternatively, Curriculum Learning strategies (CL) (Bengio, 2009) tackle the optimization problem by modifying the training distribution, such that the learner network gradually receives examples of increasing and appropriate difficulty w.", "startOffset": 51, "endOffset": 65}, {"referenceID": 1, "context": "In this section, we argue that our hint-based training with KD can be seen as a particular form of Curriculum Learning (Bengio, 2009).", "startOffset": 119, "endOffset": 133}, {"referenceID": 1, "context": "In this section, we argue that our hint-based training with KD can be seen as a particular form of Curriculum Learning (Bengio, 2009). Curriculum learning has proven to accelerate the training convergence as well as potentially improve the model generalization by properly choosing a sequence of training distributions seen by the learner: from simple examples to more complex ones. A curriculum learning extension (Gulcehre & Bengio, 2013) has also shown that by using guidance hints on an intermediate layer during the training, one could considerably ease training. However, Bengio (2009) uses hand-defined heuristics to measure the \u201csimplicity\u201d of an example in a sequence and Gulcehre & Bengio (2013)\u2019s guidance hints require some prior knowledge of the end-task.", "startOffset": 120, "endOffset": 592}, {"referenceID": 1, "context": "In this section, we argue that our hint-based training with KD can be seen as a particular form of Curriculum Learning (Bengio, 2009). Curriculum learning has proven to accelerate the training convergence as well as potentially improve the model generalization by properly choosing a sequence of training distributions seen by the learner: from simple examples to more complex ones. A curriculum learning extension (Gulcehre & Bengio, 2013) has also shown that by using guidance hints on an intermediate layer during the training, one could considerably ease training. However, Bengio (2009) uses hand-defined heuristics to measure the \u201csimplicity\u201d of an example in a sequence and Gulcehre & Bengio (2013)\u2019s guidance hints require some prior knowledge of the end-task.", "startOffset": 120, "endOffset": 706}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening.", "startOffset": 5, "endOffset": 31}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters.", "startOffset": 5, "endOffset": 250}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al.", "startOffset": 5, "endOffset": 467}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training.", "startOffset": 5, "endOffset": 490}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 1 summarizes the obtained results. Our student model outperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is crucial to achieve potentially better representations. When compared to network compression methods, our algorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%, which is significantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requiring roughly 28 times fewer parameters.", "startOffset": 5, "endOffset": 993}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 1 summarizes the obtained results. Our student model outperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is crucial to achieve potentially better representations. When compared to network compression methods, our algorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%, which is significantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requiring roughly 28 times fewer parameters. Finally, compared to state-of-the-art methods, our algorithm matches the best performers. CIFAR-100: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and used the same FitNet architecture as in CIFAR-10.", "startOffset": 5, "endOffset": 1273}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 1 summarizes the obtained results. Our student model outperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is crucial to achieve potentially better representations. When compared to network compression methods, our algorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%, which is significantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requiring roughly 28 times fewer parameters. Finally, compared to state-of-the-art methods, our algorithm matches the best performers. CIFAR-100: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and used the same FitNet architecture as in CIFAR-10. As in Chen-Yu et al. (2014), we augmented the data with random flipping during training.", "startOffset": 5, "endOffset": 1355}, {"referenceID": 15, "context": "The SVHN dataset (Netzer et al., 2011) is composed by 32 \u00d7 32 color images of house numbers collected by GoogleStreet View.", "startOffset": 17, "endOffset": 38}, {"referenceID": 13, "context": "As a sanity check for the training procedure, we evaluated the proposed method on the MNIST dataset (LeCun et al., 1998).", "startOffset": 100, "endOffset": 120}, {"referenceID": 6, "context": "We trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters.", "startOffset": 75, "endOffset": 101}, {"referenceID": 10, "context": "AFLW (Koestinger et al., 2011) is a real-world face database, containing 25K annotated images.", "startOffset": 5, "endOffset": 30}, {"referenceID": 1, "context": "samples for which the teacher network is confident and, can lead to a smoother version of the training cost (Bengio, 2009).", "startOffset": 108, "endOffset": 122}, {"referenceID": 4, "context": "On the one hand, the proliferation of local minima and especially saddle points in highly non-linear functions such as very deep networks highlights the difficulty of finding a good starting point in the parameter space at random (Dauphin et al., 2014).", "startOffset": 230, "endOffset": 252}, {"referenceID": 1, "context": "samples for which the teacher network is confident and, can lead to a smoother version of the training cost (Bengio, 2009). Despite some optimization benefits, it is worth noticing that KD training still suffers from the increasing depth and reaches its limits for 7-layer networks. HT tends to ease these optimization issues and is able to train 13-layer networks of 30M multiplications. The only difference between HT and KD is the starting point in the parameter space: either random or obtained by means of the teacher\u2019s hint. On the one hand, the proliferation of local minima and especially saddle points in highly non-linear functions such as very deep networks highlights the difficulty of finding a good starting point in the parameter space at random (Dauphin et al., 2014). On the other hand, results in Figure 2(a) indicate that HT can guide the student to a better initial position in the parameter space, from which we can minimize the cost through stochastic gradient descent. Therefore, HT provides benefits from an optimization point of view. Networks trained with HT also tend to yield better test performances than the other training methods when we fix the capacity and number of layers. For instance, in Figure 2(b), the 7-layers network, trained with hints, obtains a +0.7% performance gain on the test set compared to the model that does not use any hints (the accuracy increases from 89.45% to 90.1%). As pointed by Erhan et al. (2009), pre-training strategies can act as regularizers.", "startOffset": 109, "endOffset": 1460}, {"referenceID": 6, "context": "In this experiment, we retrain our FitNets on training plus validation sets as in Goodfellow et al. (2013b), for fair comparison with the teacher.", "startOffset": 82, "endOffset": 108}], "year": 2017, "abstractText": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher\u2019s intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.", "creator": "LaTeX with hyperref package"}}}