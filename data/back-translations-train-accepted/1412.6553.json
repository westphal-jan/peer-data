{"id": "1412.6553", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition", "abstract": "We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process.", "histories": [["v1", "Fri, 19 Dec 2014 23:02:43 GMT  (720kb,D)", "http://arxiv.org/abs/1412.6553v1", null], ["v2", "Wed, 24 Dec 2014 21:57:37 GMT  (722kb,D)", "http://arxiv.org/abs/1412.6553v2", null], ["v3", "Fri, 24 Apr 2015 11:40:54 GMT  (763kb,D)", "http://arxiv.org/abs/1412.6553v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["vadim lebedev", "yaroslav ganin", "maksim rakhuba", "ivan oseledets", "victor lempitsky"], "accepted": true, "id": "1412.6553"}, "pdf": {"name": "1412.6553.pdf", "metadata": {"source": "CRF", "title": "SPEEDING-UP CONVOLUTIONAL NEURAL NETWORKS USING FINE-TUNED CP-DECOMPOSITION", "authors": ["Vadim Lebedev", "Yaroslav Ganin", "Maksim Rakhuba", "Ivan Oseledets", "Victor Lempitsky"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Over the past two years, neural networks (CNNs) have dominated the associated costs (LeCun et al., 1989), revolutionizing computer vision and becoming ubiquitous through a series of computer visions. In many ways, this breakthrough has been made possible by the acceptance of new computer-based tools, in particular GPUs (Krizhevsky et al., 2012), but also by CPU clusters (Dean et al., 2012) and FPGAs (Farabet et al., 2011). On the other hand, there is an increasing interest in the use of CNNs on low-end architectures, such as desktop / laptop CPUs, CPUs, mobile processors and CPUs in robotics. Such processors cost the application, but the mere formation of a CNN could pose a problem, especially when real-time operation is needed. The key layer of CNNs that distinguishes and enables them from other networks is their success."}, {"heading": "2 RELATED WORK", "text": "The composition of the individual groups within Y is independent (components are not divided). Jaderberg et al. (2014a) essentially evaluated the decomposition (Rigamonti et al.) into linear combinations of a common bank of divisible (degradable) filters Y. The decomposition of the filters within Y is independent (components are not divided). Jaderberg et al. (2014a) evaluated the decomposition (Rigamonti et al., 2013) in the context of CNNs and also suggested a more efficient decomposition of the filters within Y (components are not divided).Jaderberg et al."}, {"heading": "3 METHOD", "text": "Overall, our method is a conceptually simple two-step approach: (1) take a wavy layer and disassemble its core matrix using CP decomposition, (2) fine-tune the entire network using back propagation. If necessary, move on to another layer. In the following, we look at the CP decomposition that is at the heart of our method and give the details for the two steps of our approach."}, {"heading": "3.1 CP-DECOMPOSITION REVIEW", "text": "Recall that a decomposition of a low-ranking matrix A of size n \u00b7 m with rank R is given by: A (i, j) = R \u2211 r = 1 A1 (i, r) A2 (j, r), i = 1, n, m, (1) and leads to the idea of separating variables. The easiest way to separate variables in the case of many dimensions is to use canonical polyadic decomposition (CP decomposition, also referred to as the CANDECOMP / PARAFAC model) (Kolda & Bader, 2009). For a d-dimensional arrangement A of size n1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 nd a CP decomposition, the following formA (i1,) is used."}, {"heading": "3.2 KERNEL TENSOR APPROXIMATION", "text": "CNNs (LeCun et al., 1989) are multi-layered feed architectures that map the input images to specific output vectors (with a sequence of operations).The units within CNN are organized as a sequence of 3D tensors (\"stacks\") with two spatial dimensions and the third dimension corresponding to different \"cards\" or \"channels.\" The \"most important\" and time-consuming operation within modern CNNs is the generalized evolution that maps an input tensor U (\u00b7 \u00b7 \u00b7 Us) of size X \u00b7 Y \u00b7 S into an output tensor V (\u00b7 \u00b7 \u00b7, \u00b7) of the size (n \u2212 K) of the size (X \u2212 d + 1) \u00b7 T with the following linear figure: V (x, y, t) = x + x + x + x components of size Y \u00b7 S into an output tensor V \u00b7 Y \u00b7 S into an output tensor V \u00b7 x, Y \u00b7 S into an output tensor V (x \u2212 V), the following dimensions (x \u2212 V):"}, {"heading": "3.3 IMPLEMENTATION AND FINE-TUNING", "text": "Computing Us (\u00b7, \u00b7, \u00b7) of U (\u00b7, \u00b7, \u00b7) in (6) and V (\u00b7, \u00b7, \u00b7) of Usyx (\u00b7, \u00b7, \u00b7) in (9) represent so-called 1 \u00d7 1 turns (which are also used within the \"network-in-network\" approach (Lin et al., 2013), which essentially perform a pixel-by-pixel linear recombination of input cards. Computing Usy (\u00b7, \u00b7, \u00b7) of Us (\u00b7, \u00b7, \u00b7) and Usyx (\u00b7, \u00b7, \u00b7) of Usy (\u00b7, \u00b7, \u00b7) in (7) and (8) are \"standard\" windings with small cores that are \"flat\" in one of the two spatial dimensions. We use the popular Caffe package (Jia et al., 2014) to implement the resulting architecture, with standard convolution layers for (7) and (8) and one optimized (1) of the endangered layers (1) of the 1 \u00d7 6 and 1 (4) of the endangered layers."}, {"heading": "3.4 COMPLEXITY ANALYSIS", "text": "The initial folding operation is defined by STd2 parameters (number of elements in the kernel tensor) and requires the same number of \"multiplication + addition\" operations per pixel. For (Jaderberg et al., 2014a), this number changes to Rd (S + T), where R is the rank of decomposition (see Figure 1b and Jaderberg et al., 2014a). While the two numbers are not directly comparable, the scheme (Jaderberg et al., 2014a) results in a reduction in the order d compared to the initial convolution. Because (Denton et al., 2014) the required rank is comparable or many times smaller than S and T (e.g. takingR \u2248 STS + T), without bi-clustering, and in our approach, the complexity R (S + 2d + T) (again equal for both the number of parameters and the number of Jaental + additions) is high."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we test our approach on two network architectures, lowercase CNN and a larger network designed for the ILSVRC. Most of our experiments are devoted to the approach of individual layers when other layers remain intact besides fine tuning. We perform multiple measurements to evaluate our models. After the kernel tensor approaches the CP decomposition, we calculate the accuracy of this decomposition, i.e. we measure this decline before and after fine tuning CNN. In addition, we record the CPU timings for our models and report on the acceleration compared to the CPU timings of the original model (all timings are based on the caffe code executed in CPU mode)."}, {"heading": "4.1 CHARACTER-CLASSIFICATION CNN", "text": "We use CNN described in (Jaderberg et al., 2014b) for our experiments. The network has four conventional layers with maxout nonlinearity between them and a Softmax output. It has been trained to classify 24 \u00d7 24 image patches into one of 36 classes (10 digits plus 26 characters). Our caffe port of the publicly available pre-training model (based on CharNet) achieves 91.2% accuracy on test set (very similar to the original).As in (Jaderberg et al., 2014a) we consider only the second and third conventional layers, which represent more than 90% of the processing time. Layer 2 has input and 128 output channels and filters of size 9 \u00d7 9, Layer 3 has 64 input and 512 output channels, filter size is 8 \u00d7 8. The results of the separate approximation of layers 2 and 3 are shown in Figures 2a and 2b."}, {"heading": "4.2 ALEXNET", "text": "Following Denton et al. (2014), we also consider the second revolutionary layer of AlexNet Krizhevsky et al. (2012). As a starting point, we use a pre-trained model supplied with Caffe. We summarize various network characteristics for several different tiers of approximation in Figure 2c. It can be stated that to achieve proper performance, a much higher rank (compared to the CharNet experiment) is required for Conduc2 of the network under consideration. To achieve the 0.5% decrease in accuracy reported in Figure 2c, it is sufficient to take 200 components, which also results in a higher layer speed (3.6 x 2 x 2 x achieved by Scheme 2 of Jaderberg et al. (2014b)). The runtime of Conduc2 can be further reduced if we allow slightly more shifts: Rank 140 leads to 4.5 x acceleration at a cost of approximately 1% less accuracy than the results of Don et al (2014)."}, {"heading": "5 DISCUSSION", "text": "We have shown that a relatively straightforward application of a modern tensor decomposition method (low-rank NLS-based CP decomposition) in combination with discriminatory fine-tuning of the entire network can achieve significant acceleration with minimal loss of accuracy (or, for higher ranks without such loss). In the preliminary comparisons, this approach exceeds previous methods of (Denton et al., 2014; Jaderberg et al., 2014a). However, further comparisons are required, especially with a more closely related work by Denton et al. (2014). In particular, it must be determined whether biclustering is useful when using minimum nonlinear squares for CP decomposition. Another research incentive is layers with spatially varying nuclei, such as those used by Taigman et al. (2014)."}], "references": [{"title": "High performance convolutional neural networks for document processing", "author": ["Chellapilla", "Kumar", "Puri", "Sidd", "Simard", "Patrice"], "venue": "In Tenth International Workshop on Frontiers in Handwriting Recognition,", "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "Convnet-benchmarks. https://github.com/soumith/ convnet-benchmarks", "author": ["Chintala", "Soumith"], "venue": null, "citeRegEx": "Chintala and Soumith.,? \\Q2014\\E", "shortCiteRegEx": "Chintala and Soumith.", "year": 2014}, {"title": "Tensor rank and the ill-posedness of the best low-rank approximation problem", "author": ["De Silva", "Vin", "Lim", "Lek-Heng"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Silva et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2008}, {"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Large-scale FPGA-based convolutional networks", "author": ["Farabet", "Cl\u00e9ment", "LeCun", "Yann", "Kavukcuoglu", "Koray", "Culurciello", "Eugenio", "Martini", "Berin", "Akselrod", "Polina", "Talay", "Selcuk"], "venue": "Machine Learning on Very Large Data Sets,", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Deep features for text spotting", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Rev.,", "citeRegEx": "Kolda and Bader,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Fast training of convolutional networks through FFTs", "author": ["Mathieu", "Michael", "Henaff", "Mikael", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Learning separable filters", "author": ["Rigamonti", "Roberto", "Sironi", "Amos", "Lepetit", "Vincent", "Fua", "Pascal"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Rigamonti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rigamonti et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Taigman", "Yaniv", "Yang", "Ming", "Ranzato", "Marc\u2019Aurelio", "Wolf", "Lior"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "Under review as a conference", "author": ["Tomasi", "Giorgio", "Bro", "Rasmus"], "venue": "Comp. Stat. Data An.,", "citeRegEx": "Tomasi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tomasi et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Over the course of two years, Convolutional neural networks (CNNs) (LeCun et al., 1989) have revolutionized computer vision and became ubiquituous through a range of computer vision applications.", "startOffset": 67, "endOffset": 87}, {"referenceID": 10, "context": "In many ways, this breakthrough has become possible through the acceptance of new computational tools, most notably GPUs (Krizhevsky et al., 2012), but also CPU clusters (Dean et al.", "startOffset": 121, "endOffset": 146}, {"referenceID": 3, "context": ", 2012), but also CPU clusters (Dean et al., 2012) and FPGAs (Farabet et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 5, "context": ", 2012) and FPGAs (Farabet et al., 2011).", "startOffset": 18, "endOffset": 40}, {"referenceID": 12, "context": "Consequently, there is a strong interest to the task of improving the efficiency of this operation (Chintala, 2014; Mathieu et al., 2013; Chellapilla et al., 2006).", "startOffset": 99, "endOffset": 163}, {"referenceID": 0, "context": "Consequently, there is a strong interest to the task of improving the efficiency of this operation (Chintala, 2014; Mathieu et al., 2013; Chellapilla et al., 2006).", "startOffset": 99, "endOffset": 163}, {"referenceID": 4, "context": "Exploiting this tensor structure, previous works (Denton et al., 2014; Jaderberg et al., 2014a) have suggested different tensor decomposition schemes", "startOffset": 49, "endOffset": 95}, {"referenceID": 13, "context": "These schemes are applied to the kernel tensor and generalize previous 2D filter approximations in computer vision like (Rigamonti et al., 2013).", "startOffset": 120, "endOffset": 144}, {"referenceID": 13, "context": "Below, we first discuss previous works of (Rigamonti et al., 2013; Jaderberg et al., 2014a; Denton et al., 2014) and outline the differences between them and our approach in Section 2.", "startOffset": 42, "endOffset": 112}, {"referenceID": 4, "context": "Below, we first discuss previous works of (Rigamonti et al., 2013; Jaderberg et al., 2014a; Denton et al., 2014) and outline the differences between them and our approach in Section 2.", "startOffset": 42, "endOffset": 112}, {"referenceID": 8, "context": ", 2014b) and AlexNet from the Caffe package (Jia et al., 2014), follow in Section 4.", "startOffset": 44, "endOffset": 62}, {"referenceID": 13, "context": "In (Rigamonti et al., 2013), a bank of 2D or 3D filters X is decomposed into linear combinations of a shared bank of separable (decomposable) filters Y .", "startOffset": 3, "endOffset": 27}, {"referenceID": 13, "context": "(2014a) evaluated the decomposition (Rigamonti et al., 2013) in the context of CNNs and furthermore suggested a more efficient decomposition (Figure 1b) that effectively approximates the 4D kernel tensor as a composition (product) of two 3D tensors.", "startOffset": 36, "endOffset": 60}, {"referenceID": 13, "context": "(2014a) have demonstrated the advantage of this scheme over (Rigamonti et al., 2013).", "startOffset": 60, "endOffset": 84}, {"referenceID": 11, "context": "Using low-rank decomposition to accelerate convolution was suggested by Rigamonti et al. (2013) in the context of codebook learning.", "startOffset": 72, "endOffset": 96}, {"referenceID": 6, "context": "Jaderberg et al. (2014a) evaluated the decomposition (Rigamonti et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 6, "context": "Jaderberg et al. (2014a) evaluated the decomposition (Rigamonti et al., 2013) in the context of CNNs and furthermore suggested a more efficient decomposition (Figure 1b) that effectively approximates the 4D kernel tensor as a composition (product) of two 3D tensors. In the experiments, Jaderberg et al. (2014a) have demonstrated the advantage of this scheme over (Rigamonti et al.", "startOffset": 0, "endOffset": 312}, {"referenceID": 6, "context": "Jaderberg et al. (2014a) evaluated the decomposition (Rigamonti et al., 2013) in the context of CNNs and furthermore suggested a more efficient decomposition (Figure 1b) that effectively approximates the 4D kernel tensor as a composition (product) of two 3D tensors. In the experiments, Jaderberg et al. (2014a) have demonstrated the advantage of this scheme over (Rigamonti et al., 2013). In a sequel, when refering to (Jaderberg et al., 2014a) we imply this two-component decomposition. Once the decomposition is computed, Jaderberg et al. (2014a) perform \u201clocal\u201d fine-tuning that minimizes", "startOffset": 0, "endOffset": 550}, {"referenceID": 6, "context": "Jaderberg et al. (2014a) (b) approximate the initial convolution as a composition of two linear mappings with the intermediate map stack having R maps (where R is the rank of the decomposition).", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "CP-decompositions of the kernel tensor parts in (Denton et al., 2014) have been computed with the greedy approach1.", "startOffset": 48, "endOffset": 69}, {"referenceID": 4, "context": "In the work that is most related to ours, Denton et al. (2014) have suggested a scheme based on CP-decomposition of parts of the kernel tensor obtianed by biclustering (alongside with a different decompositions for the first decomposition layer and the fully-connected layer).", "startOffset": 42, "endOffset": 63}, {"referenceID": 4, "context": "In the work that is most related to ours, Denton et al. (2014) have suggested a scheme based on CP-decomposition of parts of the kernel tensor obtianed by biclustering (alongside with a different decompositions for the first decomposition layer and the fully-connected layer). Biclustering splits the two non-spatial dimensions into subgroups, and reduces the effective ranks in the CPdecomposition. CP-decompositions of the kernel tensor parts in (Denton et al., 2014) have been computed with the greedy approach1. Our approach essentially simplifies that of Denton et al. (2014) in that we do not perform biclustering and apply CP-decomposition directly to the full convolution kernel tensor.", "startOffset": 42, "endOffset": 581}, {"referenceID": 4, "context": "In the work that is most related to ours, Denton et al. (2014) have suggested a scheme based on CP-decomposition of parts of the kernel tensor obtianed by biclustering (alongside with a different decompositions for the first decomposition layer and the fully-connected layer). Biclustering splits the two non-spatial dimensions into subgroups, and reduces the effective ranks in the CPdecomposition. CP-decompositions of the kernel tensor parts in (Denton et al., 2014) have been computed with the greedy approach1. Our approach essentially simplifies that of Denton et al. (2014) in that we do not perform biclustering and apply CP-decomposition directly to the full convolution kernel tensor. On the other hand, we replace greedy computation of CP-decomposition with non-linear least squares. Finally, as discussed above, we fine-tune the complete network by backpropagation, whereas Denton et al. (2014) only fine-tunes the layers above the approximated one.", "startOffset": 42, "endOffset": 907}, {"referenceID": 4, "context": "Note that the alternating least squares process mentioned in (Denton et al., 2014) refers to computing the best next rank-1 tensor, but the outer process of adding rank-1 tensors is still greedy.", "startOffset": 61, "endOffset": 82}, {"referenceID": 4, "context": "Such NLS optimization is capable of obtaining much better approximations than the strategy of greedily finding best rank-1 approximation of the residual vectors used in Denton et al. (2014). We give a simple example highlighting this advantage of the NLS in the Appendix.", "startOffset": 169, "endOffset": 190}, {"referenceID": 11, "context": "2 KERNEL TENSOR APPROXIMATION CNNs (LeCun et al., 1989) are feed-forward multi-layer architectures that map the input images to certain output vectors using a sequence of operations.", "startOffset": 35, "endOffset": 55}, {"referenceID": 8, "context": "We use the popular Caffe package (Jia et al., 2014) to implement the resulting architecture, utilizing standard convolution layers for (7) and (8), and an optimized 1\u00d71 convolution layers for (6) and (9).", "startOffset": 33, "endOffset": 51}, {"referenceID": 4, "context": "For (Denton et al., 2014) in the absence of bi-clustering as well as in the case of our approach, the complexity is R(S + 2d + T ) (again, both for the number of parameters and for the number of \u201cmultiplications+additions\u201d per output pixel).", "startOffset": 4, "endOffset": 25}, {"referenceID": 4, "context": "The bi-clustering in (Denton et al., 2014) makes a \u201ctheoretical\u201d comparison with the complexity of our approach problematic, as on the one hand bi-clustering increases the number of tensors to be approximated, but on the other hand, reduces the required ranks considerably (so that assuming the same R would not be reasonable).", "startOffset": 21, "endOffset": 42}, {"referenceID": 5, "context": "For (Jaderberg et al., 2014a) this number changes to Rd(S + T ), where R is the rank of the decomposition (see Figure 1b and Jaderberg et al. (2014a)).", "startOffset": 5, "endOffset": 150}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe.", "startOffset": 10, "endOffset": 115}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.", "startOffset": 10, "endOffset": 514}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.6\u00d7 vs. 2\u00d7 achieved by Scheme 2 of Jaderberg et al. (2014b)).", "startOffset": 10, "endOffset": 660}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.6\u00d7 vs. 2\u00d7 achieved by Scheme 2 of Jaderberg et al. (2014b)). The running time of the conv2 can be further reduced if we allow for slightly more misclassifications: rank 140 approximation leads to 4.5\u00d7 speed-up at the cost of \u2248 1% accuracy loss surpassing the results of Denton et al. (2014). Along with conventional full-network fine-tuning we tried to refine the obtained tensor approximation by applying the data reconstruction approach from Jaderberg et al.", "startOffset": 10, "endOffset": 892}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.6\u00d7 vs. 2\u00d7 achieved by Scheme 2 of Jaderberg et al. (2014b)). The running time of the conv2 can be further reduced if we allow for slightly more misclassifications: rank 140 approximation leads to 4.5\u00d7 speed-up at the cost of \u2248 1% accuracy loss surpassing the results of Denton et al. (2014). Along with conventional full-network fine-tuning we tried to refine the obtained tensor approximation by applying the data reconstruction approach from Jaderberg et al. (2014b). Unfortunately, we failed to find a good SGD learning rate: larger values led to the exploding gradients, while the smaller ones did not allow to sensibly reduce the reconstruction loss.", "startOffset": 10, "endOffset": 1070}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.6\u00d7 vs. 2\u00d7 achieved by Scheme 2 of Jaderberg et al. (2014b)). The running time of the conv2 can be further reduced if we allow for slightly more misclassifications: rank 140 approximation leads to 4.5\u00d7 speed-up at the cost of \u2248 1% accuracy loss surpassing the results of Denton et al. (2014). Along with conventional full-network fine-tuning we tried to refine the obtained tensor approximation by applying the data reconstruction approach from Jaderberg et al. (2014b). Unfortunately, we failed to find a good SGD learning rate: larger values led to the exploding gradients, while the smaller ones did not allow to sensibly reduce the reconstruction loss. We suspect that this effect is due to the instability of the low-rank CP-decomposition De Silva & Lim (2008). One way to circumvent the issue would be to alternate the components learning (i.", "startOffset": 10, "endOffset": 1366}, {"referenceID": 4, "context": "In the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014; Jaderberg et al., 2014a).", "startOffset": 82, "endOffset": 128}, {"referenceID": 4, "context": "In the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014; Jaderberg et al., 2014a). However, more comparisons especially with a more related work of Denton et al. (2014) are needed.", "startOffset": 83, "endOffset": 216}, {"referenceID": 4, "context": "In the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014; Jaderberg et al., 2014a). However, more comparisons especially with a more related work of Denton et al. (2014) are needed. In particular, it is to be determined whether biclustering is useful, when non-linear least squares are used for CP-decomposition. Another avenue of research are layers with spatially-varying kernels, such as used e.g. in Taigman et al. (2014). Firstly, these layers would greatly benefit from the reduction in the number of parameters.", "startOffset": 83, "endOffset": 472}, {"referenceID": 4, "context": "In the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014; Jaderberg et al., 2014a). However, more comparisons especially with a more related work of Denton et al. (2014) are needed. In particular, it is to be determined whether biclustering is useful, when non-linear least squares are used for CP-decomposition. Another avenue of research are layers with spatially-varying kernels, such as used e.g. in Taigman et al. (2014). Firstly, these layers would greatly benefit from the reduction in the number of parameters. Secondly, the spatial variation of the kernel might be embedded into extra tensor dimensions, which may open up further speed-up possibilities. Finally, similarly to Denton et al. (2014), we note that low-rank decompositions seems to have a regularizing effects allowing to slightly improve the overall accuracy for higher rank.", "startOffset": 83, "endOffset": 752}], "year": 2017, "abstractText": "We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it yields larger CPU speedups at the cost of lower accuracy drops compared to previous approaches. For the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1% from 91% to 90%). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of 1% increase of the overall top-5 classification error.", "creator": "LaTeX with hyperref package"}}}