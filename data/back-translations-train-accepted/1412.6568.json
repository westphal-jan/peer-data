{"id": "1412.6568", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Improving zero-shot learning by mitigating the hubness problem", "abstract": "The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.", "histories": [["v1", "Sat, 20 Dec 2014 01:03:46 GMT  (1038kb,D)", "https://arxiv.org/abs/1412.6568v1", null], ["v2", "Tue, 10 Mar 2015 14:15:13 GMT  (1039kb,D)", "http://arxiv.org/abs/1412.6568v2", null], ["v3", "Wed, 15 Apr 2015 13:10:07 GMT  (1039kb,D)", "http://arxiv.org/abs/1412.6568v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["georgiana dinu", "angeliki lazaridou", "marco baroni"], "accepted": true, "id": "1412.6568"}, "pdf": {"name": "1412.6568.pdf", "metadata": {"source": "CRF", "title": "IMPROVING ZERO-SHOT LEARNING BY MITIGATING THE HUBNESS PROBLEM", "authors": ["Georgiana Dinu", "Angeliki Lazaridou", "Marco Baroni"], "emails": ["georgiana.dinu@unitn.it", "angeliki.lazaridou@unitn.it", "marco.baroni@unitn.it"], "sections": [{"heading": "1 INTRODUCTION", "text": "This, in turn, has led to the development of the so-called \"zeroshot learning paradigm,\" which is a way of addressing manual annotation in domains where other vector-based representations take place (e.g. images or brain signals) that must be associated with word labels. The idea behind this is to use the general mapping function of vectors in the domain where other vector-based representations exist (e.g. images or brain signals) associated with word labels. The idea is to learn a general mapping function of vectors in the domain of the word vectors, and then apply the induced function of vectors to new entities (which have not been seen)."}, {"heading": "2 HUBNESS IN ZERO-SHOT MAPPING", "text": "The zero solution is a problem that has a novel label: Dts = (xi)} ni = 1, yi = 1, yi = 1, yi = 1, yi = 1, yi = 1, Tts = 1, Tts = 1, Ttr = 1,. This is possible because Labels y vector representations y Rv.2, Training as a multivariate regression problem, Learning is a function that vectors (linguistic-space) vectors (linguistic-space) source domain."}, {"heading": "3 A GLOBALLY CORRECTED NEIGHBOUR RETRIEVAL METHOD", "text": "It is as if it were a reactionary project, able to retaliate."}, {"heading": "3.1 ENGLISH TO ITALIAN WORD TRANSLATION", "text": "First, we test our methods on bilingual lexicon-based representations (Baroni et al 2014).Since the amount of parallel data is limited, there has been a lot of work on the acquisition of translation words using vector space methods on monolingual corpora, along with a small seed lexicon (Haghighi et al., 2008; Klementiev et al., 2012; Koehn & Knight, 2002; Rapp et al., 2012; Rapp, 1999).However, this method is limited to vector spaces that use words as contextual features, and does not extend to vector-based word representations based on other types of dimensions, such as these neural language models, which have recently gone well beyond contextual representations."}, {"heading": "3.2 ZERO-SHOT IMAGE LABELING AND RETRIEVING", "text": "In this section, we test our proposed method in a crossmodal environment in which 5,000 images / labels are selected. \"We are the entire world.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\""}, {"heading": "4 CONCLUSION", "text": "In this paper, we have shown that the basic set-up in zero-shot experiments (using multivariate linear regression by a regulated error object with the smallest squares to learn a mapping across representative vector spaces) is negatively affected by strong hubness effects. We suggested a simple way to correct this by replacing traditional nearest queries with globally adapted ones, requiring only the availability of additional, unlabeled source space data in addition to the test instances. While more advanced methods for learning the mapping could be applied (e.g. the inclusion of strategies to avoid hubness in nonlinear functions or different learning goals), we have shown that consistent improvements in very different areas can already be achieved with our query-time correction of the basic learning structure, which is a popular and attractive one due to its simplicity, generality, and high performance. In future work, we plan to investigate how different uses of hubness affect the different types of space widely."}, {"heading": "5 ACKNOWLEDGMENTS", "text": "This work was supported by the ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES)."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni", "Marco", "Dinu", "Georgiana", "Kruszewski", "Germ\u00e1n"], "venue": "In Proceedings of ACL,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Vector space models of lexical meaning", "author": ["Clark", "Stephen"], "venue": "Handbook of Contemporary Semantics,", "citeRegEx": "Clark and Stephen.,? \\Q2015\\E", "shortCiteRegEx": "Clark and Stephen.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Lia-Ji", "Fei-Fei"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "How to make words with vectors: Phrase generation in distributional semantics", "author": ["Dinu", "Georgiana", "Baroni", "Marco"], "venue": "In Proceedings of ACL,", "citeRegEx": "Dinu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Ranzato", "Marc\u2019Aurelio", "Mikolov", "Tomas"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Haghighi", "Aria", "Liang", "Percy", "Berg-Kirkpatrick", "Taylor", "Klein", "Dan"], "venue": "In Proceedings of ACL,", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "The Elements of Statistical Learning, 2nd edition", "author": ["Hastie", "Trevor", "Tibshirani", "Robert", "Friedman", "Jerome"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia, MM", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Toward statistical machine translation without parallel corpora", "author": ["Klementiev", "Alexandre", "Irvine", "Ann", "Callison-Burch", "Chris", "Yarowsky", "David"], "venue": "In Proceedings of EACL,", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Learning a translation lexicon from monolingual corpora", "author": ["Koehn", "Philipp", "Knight", "Kevin"], "venue": "Proceedings of ACL Workshop on Unsupervised Lexical Acquisition,", "citeRegEx": "Koehn et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2002}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world", "author": ["Lazaridou", "Angeliki", "Bruni", "Elia", "Baroni", "Marco"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Lazaridou", "Angeliki", "Pham", "The Nghia", "Baroni", "Marco"], "venue": "In NIPS workshop on Learning Semantics,", "citeRegEx": "Lazaridou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for Machine Translation", "author": ["Mikolov", "Tomas", "Le", "Quoc", "Sutskever", "Ilya"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Predicting human brain activity associated with the meanings of nouns", "author": ["Mitchell", "Tom", "Shinkareva", "Svetlana", "Carlson", "Andrew", "Chang", "Kai-Min", "Malave", "Vincente", "Mason", "Robert", "Just", "Marcel"], "venue": "Science, 320:1191\u20131195,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Zero-shot learning with semantic output codes", "author": ["Palatucci", "Mark", "Pomerleau", "Dean", "Hinton", "Geoffrey E", "Mitchell", "Tom M"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Palatucci et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palatucci et al\\.", "year": 2009}, {"title": "On the existence of obstinate results in vector space models", "author": ["Radovanovi\u0107", "Milos", "Nanopoulos", "Alexandros", "Ivanovi\u0107", "Mirjana"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "Radovanovi\u0107 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Radovanovi\u0107 et al\\.", "year": 2010}, {"title": "Hubs in space: Popular nearest neighbors in high-dimensional data", "author": ["Radovanovi\u0107", "Milo\u0161", "Nanopoulos", "Alexandros", "Ivanovi\u0107", "Mirjana"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Radovanovi\u0107 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Radovanovi\u0107 et al\\.", "year": 2010}, {"title": "Automatic identification of word translations from unrelated english and german corpora", "author": ["Rapp", "Reinhard"], "venue": "In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,", "citeRegEx": "Rapp and Reinhard.,? \\Q1999\\E", "shortCiteRegEx": "Rapp and Reinhard.", "year": 1999}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Socher", "Richard", "Ganjoo", "Milind", "Manning", "Christopher", "Ng", "Andrew"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Socher", "Richard", "Le", "Quoc", "Manning", "Christopher", "Ng", "Andrew"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Parallel data, tools and interfaces in opus", "author": ["Tiedemann", "J\u00f6rg"], "venue": "In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912),", "citeRegEx": "Tiedemann and J\u00f6rg.,? \\Q2012\\E", "shortCiteRegEx": "Tiedemann and J\u00f6rg.", "year": 2012}, {"title": "The influence of hubness on nearest-neighbor methods in object recognition", "author": ["Tomasev", "Nenad", "Brehar", "Raluca", "Mladenic", "Dunja", "Nedevschi", "Sergiu"], "venue": "Intelligent Computer Communication and Processing (ICCP),", "citeRegEx": "Tomasev et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tomasev et al\\.", "year": 2011}, {"title": "A probabilistic approach to nearest-neighbor classification: naive hubness bayesian knn", "author": ["Tomasev", "Nenad", "Radovanovic", "Milos", "Mladenic", "Dunja", "Ivanovic", "Mirjana"], "venue": "In CIKM,", "citeRegEx": "Tomasev et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tomasev et al\\.", "year": 2011}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Peter", "Pantel", "Patrick"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston", "Jason", "Bengio", "Samy", "Usunier", "Nicolas"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Extensive research in computational linguistics and neural language modeling has shown that contextual co-occurrence patterns of words in corpora can be effectively exploited to learn high-quality vector-based representations of their meaning in an unsupervised manner (Collobert et al., 2011; Clark, 2015; Turney & Pantel, 2010).", "startOffset": 269, "endOffset": 329}, {"referenceID": 18, "context": ", images or brain signals) must be associated to word labels (Palatucci et al., 2009).", "startOffset": 61, "endOffset": 85}, {"referenceID": 17, "context": "This approach has originally been tested in neural decoding (Mitchell et al., 2008; Palatucci et al., 2009), where the task consists in learning a regression function from fMRI activation vectors to word representations, and then applying it to the brain signal of a concept outside the training set, in order to \u201cread the mind\u201d of subjects.", "startOffset": 60, "endOffset": 107}, {"referenceID": 18, "context": "This approach has originally been tested in neural decoding (Mitchell et al., 2008; Palatucci et al., 2009), where the task consists in learning a regression function from fMRI activation vectors to word representations, and then applying it to the brain signal of a concept outside the training set, in order to \u201cread the mind\u201d of subjects.", "startOffset": 60, "endOffset": 107}, {"referenceID": 6, "context": "In computer vision, zero-shot mapping of image vectors onto word space has been applied to the task of retrieving words to label images of objects outside the training inventory (Frome et al., 2013; Socher et al., 2013), as well as using the inverse language-to-vision mapping for image retrieval (Lazaridou et al.", "startOffset": 178, "endOffset": 219}, {"referenceID": 22, "context": "In computer vision, zero-shot mapping of image vectors onto word space has been applied to the task of retrieving words to label images of objects outside the training inventory (Frome et al., 2013; Socher et al., 2013), as well as using the inverse language-to-vision mapping for image retrieval (Lazaridou et al.", "startOffset": 178, "endOffset": 219}, {"referenceID": 6, "context": "For example, the system of Frome et al. (2013) returns the correct image label as top hit in less than 1% of cases in all zero-shot experiments (see their Table 2).", "startOffset": 27, "endOffset": 47}, {"referenceID": 19, "context": "It is known that the problem of hubness is related to concentration, the tendency of pairwise similarities between elements in a set to converge to a constant as the dimensionality of the space increases (Radovanovi\u0107 et al., 2010b). Radovanovi\u0107 et al. (2010a) show that this also holds for cosine similarity (which is used almost exclusively in linguistic applications): the expectation of pairwise similarities becomes constant and the standard deviation converges to 0.", "startOffset": 205, "endOffset": 260}, {"referenceID": 19, "context": "Methods for this have been proposed and evaluated, for example, by Radovanovi\u0107 et al. (2010a) and Tomasev et al.", "startOffset": 67, "endOffset": 94}, {"referenceID": 19, "context": "Methods for this have been proposed and evaluated, for example, by Radovanovi\u0107 et al. (2010a) and Tomasev et al. (2011a). We adopt a much simpler approach (similar in spirit to Tomasev et al.", "startOffset": 67, "endOffset": 121}, {"referenceID": 7, "context": "As the amount of parallel data is limited, there has been a lot of work on acquiring translation dictionaries by using vector-space methods on monolingual corpora, together with a small seed lexicon (Haghighi et al., 2008; Klementiev et al., 2012; Koehn & Knight, 2002; Rapp, 1999).", "startOffset": 199, "endOffset": 281}, {"referenceID": 10, "context": "As the amount of parallel data is limited, there has been a lot of work on acquiring translation dictionaries by using vector-space methods on monolingual corpora, together with a small seed lexicon (Haghighi et al., 2008; Klementiev et al., 2012; Koehn & Knight, 2002; Rapp, 1999).", "startOffset": 199, "endOffset": 281}, {"referenceID": 10, "context": "One of the most straightforward and effective methods is to represent words as high-dimensional vectors that encode co-occurrence only with the words in the seed lexicon and are therefore comparable cross-lingually (Klementiev et al., 2012; Rapp, 1999).", "startOffset": 215, "endOffset": 252}, {"referenceID": 0, "context": "However, this method is limited to vector spaces that use words as context features, and does not extend to vector-based word representations relying on other kinds of dimensions, such as those neural language models that have recently been shown to greatly outperform context-word-based representations (Baroni et al., 2014).", "startOffset": 304, "endOffset": 325}, {"referenceID": 0, "context": "However, this method is limited to vector spaces that use words as context features, and does not extend to vector-based word representations relying on other kinds of dimensions, such as those neural language models that have recently been shown to greatly outperform context-word-based representations (Baroni et al., 2014). The zero-shot approach, that induces a function from one space to the other based on paired seed element vectors, and then applies it to new data, works irrespective of the choice of vector representation. This method has been shown to be effective for bilingual lexicon construction by Mikolov et al. (2013b), with Dinu & Baroni (2014) reporting overall better performance than with the seed-word-dimension method.", "startOffset": 305, "endOffset": 637}, {"referenceID": 0, "context": "However, this method is limited to vector spaces that use words as context features, and does not extend to vector-based word representations relying on other kinds of dimensions, such as those neural language models that have recently been shown to greatly outperform context-word-based representations (Baroni et al., 2014). The zero-shot approach, that induces a function from one space to the other based on paired seed element vectors, and then applies it to new data, works irrespective of the choice of vector representation. This method has been shown to be effective for bilingual lexicon construction by Mikolov et al. (2013b), with Dinu & Baroni (2014) reporting overall better performance than with the seed-word-dimension method.", "startOffset": 305, "endOffset": 664}, {"referenceID": 15, "context": "Word representations The cbow method introduced by Mikolov et al. (2013a) induces vectorbased word representations by trying to predict a target word from the words surrounding it within a neural network architecture.", "startOffset": 51, "endOffset": 74}, {"referenceID": 8, "context": "We report results without regularization as well as with the regularization parameter \u03bb estimated by generalized cross-validation (GCV) (Hastie et al., 2009, p. 244). Both corrected methods achieve significant improvements over standard NN, ranging from 7% to 14%. For the standard method, the performance decreases as the training data size increases beyond 5K, probably due to the noise added by lower-frequency words. The corrected measures are robust against this effect: adding more training data does not help, but it does not harm them either. Regularization does not improve, and actually hampers the standard method, whereas it benefits the corrected measures when using a small amount of training data (1K), and does not affect performance otherwise. The results by frequency bin show that most of the improvements are brought about for the all-important medium- and low-frequency words. Although not directly comparable, the absolute numbers we obtain are in the range of those reported by Mikolov et al. (2013b), whose test data correspond, in terms of frequency, to those in our first 2 bins.", "startOffset": 137, "endOffset": 1024}, {"referenceID": 19, "context": "As pointed out by Radovanovi\u0107 et al. (2010b), hubness correlates with proximity to the test-set mean vector (the average of all test vectors).", "startOffset": 18, "endOffset": 45}, {"referenceID": 3, "context": "(2014b) containing 5,000 word labels, each associated to 100 ImageNet pictures (Deng et al., 2009).", "startOffset": 79, "endOffset": 98}, {"referenceID": 9, "context": "Images are represented by 4096dimensional vectors extracted using the Caffe toolkit (Jia et al., 2014) together with the pre-trained convolutional neural network of Krizhevsky et al.", "startOffset": 84, "endOffset": 102}, {"referenceID": 10, "context": "Experimental setting We use the data set of Lazaridou et al. (2014b) containing 5,000 word labels, each associated to 100 ImageNet pictures (Deng et al.", "startOffset": 44, "endOffset": 69}, {"referenceID": 3, "context": "(2014b) containing 5,000 word labels, each associated to 100 ImageNet pictures (Deng et al., 2009). Word representations are extracted from Wikipedia with word2vec in skip-gram mode. Images are represented by 4096dimensional vectors extracted using the Caffe toolkit (Jia et al., 2014) together with the pre-trained convolutional neural network of Krizhevsky et al. (2012). We use a random 4/1 train/test split.", "startOffset": 80, "endOffset": 373}, {"referenceID": 6, "context": "Note that, while there are differences between the setups, Frome et al. (2013) report accuracy results below 1% in all their zero-shot experiments, including those with chance levels comparable to ours.", "startOffset": 59, "endOffset": 79}, {"referenceID": 24, "context": "Prompted by a reviewer, we also performed preliminary experiments with a margin-based ranking objective similar to the one in WSABIE Weston et al. (2011) and DeViSE Frome et al.", "startOffset": 133, "endOffset": 154}, {"referenceID": 5, "context": "(2011) and DeViSE Frome et al. (2013) which is typically reported to outperform the l2 objective in Equation 1 (Socher et al.", "startOffset": 18, "endOffset": 38}, {"referenceID": 5, "context": "(2011) and DeViSE Frome et al. (2013) which is typically reported to outperform the l2 objective in Equation 1 (Socher et al. (2014)).", "startOffset": 18, "endOffset": 133}, {"referenceID": 5, "context": "We estimate W using stochastic gradient descent where per-parameter learning rates are tuned with Adagrad Duchi et al. (2011). Results on the En\u2192It task are at 38.", "startOffset": 106, "endOffset": 126}], "year": 2015, "abstractText": "The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.", "creator": "LaTeX with hyperref package"}}}