{"id": "1412.6575", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "abstract": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN and TransE, can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE when evaluated on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules from the KB. We demonstrate that embeddings trained from the bilinear objective can effectively capture relation composition via matrix multiplication. We also show that our embedding-based approach can extract rules that involve relation transitivity more effectively than a state-of-the-art rule mining approach that is tailored for large-scale KBs.", "histories": [["v1", "Sat, 20 Dec 2014 01:37:16 GMT  (1789kb,D)", "http://arxiv.org/abs/1412.6575v1", "13 pages, 4 figures"], ["v2", "Sat, 27 Dec 2014 00:18:17 GMT  (1627kb,D)", "http://arxiv.org/abs/1412.6575v2", "12 pages, 4 figures"], ["v3", "Fri, 10 Apr 2015 15:24:59 GMT  (1628kb,D)", "http://arxiv.org/abs/1412.6575v3", "12 pages, 4 figures"], ["v4", "Sat, 29 Aug 2015 15:08:45 GMT  (1628kb,D)", "http://arxiv.org/abs/1412.6575v4", "12 pages, 4 figures"]], "COMMENTS": "13 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bishan yang", "wen-tau yih", "xiaodong he", "jianfeng gao", "li deng"], "accepted": true, "id": "1412.6575"}, "pdf": {"name": "1412.6575.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "emails": ["bishan@cs.cornell.edu", "scottyih@microsoft.com", "xiaohe@microsoft.com", "jfgao@microsoft.com", "deng@microsoft.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, we have seen rapid growth in knowledge bases (KBs) such as Freebase1, DBPedia (Auer et al., 2007) and YAGO (Suchanek et al., 2007). These KBs store facts about real entities (e.g. people, places, and things) in the form of RDF triples2 (i.e.). Today's KBs are large in size. Thus, Freebase contains millions of entities and billions of facts (triples) that contain a wide variety of predicates (relationship types). Such multirelational data offers excellent potential for improving a wide range of tasks, information retrieval."}, {"heading": "2 RELATED WORK", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "3 MULTI-RELATIONAL REPRESENTATION LEARNING", "text": "In this section we will present a general neural network framework for multirelational representation learning. We will discuss various design options for the representation of units and relationships, which in Section 4.Given a KB represented as a list of relation triplets pe1, r, e2q (which denotes e1 (the subject) and e2 (the object) that are in a particular relationship r), we will learn representations for units and relationships, so that valid triplets get high values (or low energies); the embedding can be learned via a neural network architecture; the first layer projects a pair of input units (represented as \"one-hot\" index vectors or \"n-hot\" feature vectors) onto low-dimensional vectors, and the second layer combines these two vectors into a scalar for comparison via a scoring function with relation-specific parameters."}, {"heading": "3.1 ENTITY REPRESENTATIONS", "text": "Some use the text names associated with entities. For example, NTN (Socher et al., 2013) represents each entity as an average of its word vectors. This can be considered as adopting word bag vectors as input and learning a projection matrix consisting of word vectors. With xe1 and xe2, designate the input vectors for entity e1 and e2, respectively. With W, designate the first projection matrix. The projected entity vectors ye1 and ye2 can be written as 1 \"f\" Wxe1, ye2 \"f\" Wxe2, \"where f can be a linear or nonlinear function, and W is a parameter matrix that can be randomly initialized or initialized using pre-tracted word vectors (as in Socher et al., 2013) or pre-tracted vectors."}, {"heading": "3.2 RELATION REPRESENTATIONS", "text": "In fact, most of them will be able to play by the rules that they are able to play by, and they will have to play by the rules that they are able to play by, and they will have to play by the rules."}, {"heading": "3.3 PARAMETER LEARNING", "text": "The neural network parameters of all the models discussed above can be learned by minimizing a margin-based ranking objective4, which means that the values of positive relationships (triplets) are higher than the values of negative relationships (triplets). Normally, only positive triplets are observed in the data. In the face of a series of positive triplets T, we can construct a set of \"negative\" triplets T 1 by either corrupting one of the relation arguments, T 1 \"tpe11, r, e2q | e11 P E, pe11, r, e2q R T u Y tpe1, r, e12q | e12 P E, pe1, r, e12q R T, and so on. Call the scoring function for triplet pe1, r, e2q Spe1, r, e2q, e2q, e2q, e2q. The training goal is to minimize margin-based ranking losses."}, {"heading": "4 INFERENCE TASK I: LINK PREDICTION", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5 INFERENCE TASK II: RULE EXTRACTION", "text": "In this section, we focus on a complementary reasoning task, in which we use the learned embedding to effectively explore the rules of KB. Given, for example, that a person was born in New York and New York is a city of the United States, then the nationality of the person is that of the United States: BornInCitypa, bq ^ CityOfCountrypb, cq \u0445 HasNationalitypa, cq Such logical rules can serve four important purposes. Firstly, they can help to derive new facts and complete the existing KBs. Secondly, they can help to optimize data retention by storing only rules instead of large amounts of expanded data and generating facts only at the point in time of conclusion. Thirdly, they can support complex thinking. Finally, they can provide explanations for consequential results, e.g., we can conclude that movies are often in the language spoken by the director, and that the occupations of people are usually probing the specialization of the key field, the search for the KB rules, etc., that the rules enclosed by KB are the rules, etc."}, {"heading": "5.1 BACKGROUND AND NOTATIONS", "text": "Each relationship rpa, bq is a directed edge from node a to node b and with connection type r. We are interested in extracting horn rules consisting of a header relationship H and a sequence of body relations B1,..., Bn: B1pa1, a2q ^ B2pa2, a3q ^... ^ Bnpan, a \"1q \u00f9\u043c Hpa1, an\" 1q (4) where ai are variables that can be replaced by entities. A rule is instantiated when all variables are replaced by entities in KB. We restrict H, B1,..., Bn to be different types of relationships, just as we are interested in degrading the composition properties of different relationships. We also restrict B1,..., Bn to form a directed path in the graph and the header relationship H to be a directed edge: from start node to end node."}, {"heading": "5.2 EMBEDDING-BASED RULE EXTRACTION", "text": "For simplicity, we consider horn rules of length 2 (longer rules can easily be derived from this case): B1pa, bq ^ B2pb, cq \u0445 Hpa, cq (5) Note that the body can usually be considered a composition of relationships B1 and B2, which is a new relationship that has the property that entities a and c are in a relationship if and only if there is an entity b that simultaneously gives two relationships B1pa, bq and B2pb, cq, candidate relationship as a multiplication or addition of two relationships. Here, we focus on relations that are in the form of vectors (as in TRANSE) and matrices (as in BILINEAR and its variants). The composition of relationships in a new embedding system that lies in the same relationship."}, {"heading": "5.3 EXPERIMENTS", "text": "We evaluate our rule extraction method (referred to as EMBEDRULE) on the FB15k-401 dataset. In our experiments, we remove the equivalence relationships and relationships whose domains have cardinality 1, because rules containing these relationships are not interesting, resulting in training data containing 485,741 facts, 14,417 units and 373 relationships. Our EMBEDRULE algorithm identifies 60,020 possible length-2 relationship sequences and 2,156,391 possible length-3 relationship sequences. We then apply the threshold method described in Section 5.2 to further select the top \"3.9K length-2 rules and\" 2K length-3 rules. \"By default, all extracted rules are ranked by decreasing confidence point number: the ratio of the correct predictions (triplets included in the observed KB) to the total number of predictions where the predictions are three-dimensional."}, {"heading": "5.4 RESULTS", "text": "In assessing the various sets of rules for prediction methods, we calculate the estimated accuracy that the ratio of predictions to the totality of invisible predictions generated (triplets not included in the collected data). Note that accuracy is an estimate, since a prediction is not necessarily \"wrong\" if not seen. \"We have the best 30 invisible facts predicted by each method.\" We also remove rules in which the header relationships are difficult to justify due to dynamic factors, such as the current relationships in our datasets being non-functional."}, {"heading": "6 CONCLUSION", "text": "In this paper, we present a general framework for the representation of units and relationships in CBs. Within this framework, we evaluate empirically different embedding models for knowledge conclusions. We show that a simple formulation of a bilinear model can exceed current embedding models for link predictions on Freebase. Furthermore, we examine the embedding models learned by using them to extract rules from CBs. Starting from modelling the composition of relationships (e.g. the composition of BornInCity and CityInCountry implies nationality), we show that embedding models learned from the bilinear target can be used to extract rules more effectively than a state-of-the-art rule removal system tailored to large-scale CBs. For future work, we aim to use the deep structure within the neural network framework. Since learning representations that use deep networks have proven to be useful as an architectural aid in various applications, 2012 as a very successful Hinton, 2012 as a Vinal."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer", "S\u00f6ren", "Bizer", "Christian", "Kobilarov", "Georgi", "Lehmann", "Jens", "Cyganiak", "Richard", "Ives", "Zachary"], "venue": "In The semantic web,", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes", "Antoine", "Weston", "Jason", "Collobert", "Ronan", "Bengio", "Yoshua"], "venue": "In AAAI,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["Bordes", "Antoine", "Glorot", "Xavier", "Weston", "Jason", "Bengio", "Yoshua"], "venue": "Machine Learning,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In NIPS,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Can recursive neural tensor networks learn logical reasoning", "author": ["Bowman", "Samuel R"], "venue": "In ICLR,", "citeRegEx": "Bowman and R.,? \\Q2014\\E", "shortCiteRegEx": "Bowman and R.", "year": 2014}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Chang", "Kai-Wei", "Yih", "Wen-tau", "Yang", "Bishan", "Meek", "Chris"], "venue": "In EMNLP,", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["Deng", "Li", "G. Hinton", "B. Kingsbury"], "venue": "In in ICASSP,", "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Knowledge vault: A Web-scale approach to probabilistic knowledge fusion", "author": ["Dong", "Xin Luna", "K Murphy", "E Gabrilovich", "G Heitz", "W Horn", "N Lao", "Strohmann", "Thomas", "Sun", "Shaohua", "Zhang", "Wei"], "venue": "In KDD,", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Amie: association rule mining under incomplete evidence in ontological knowledge bases", "author": ["Gal\u00e1rraga", "Luis Antonio", "Teflioudi", "Christina", "Hose", "Katja", "Suchanek", "Fabian"], "venue": "In WWW,", "citeRegEx": "Gal\u00e1rraga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gal\u00e1rraga et al\\.", "year": 2013}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao", "Jianfeng", "Pantel", "Patrick", "Gamon", "Michael", "He", "Xiaodong", "Deng", "Li", "Shen", "Yelong"], "venue": "In EMNLP,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Introduction to Statistical Relational Learning", "author": ["Getoor", "Lise", "Taskar", "Ben (eds"], "venue": null, "citeRegEx": "Getoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Getoor et al\\.", "year": 2007}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Grefenstette", "Edward"], "venue": "In *SEM,", "citeRegEx": "Grefenstette and Edward.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette and Edward.", "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Hinton", "Geoff", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Sig. Proc. Mag.,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Learning deep structured semantic models for Web search using clickthrough data", "author": ["Huang", "Po-Sen", "He", "Xiaodong", "Gao", "Jianfeng", "Deng", "Li", "Acero", "Alex", "Heck", "Larry"], "venue": "In CIKM,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Tensor deep stacking networks", "author": ["B Hutchinson", "L. Deng", "D. Yu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hutchinson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2013}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Kemp", "Charles", "Tenenbaum", "Joshua B", "Griffiths", "Thomas L", "Yamada", "Takeshi", "Ueda", "Naonori"], "venue": "In AAAI,", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao", "Ni", "Mitchell", "Tom", "Cohen", "William W"], "venue": "In EMNLP,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Nickel", "Maximilian", "Tresp", "Volker", "Kriegel", "Hans-Peter"], "venue": "In ICML, pp", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["Nickel", "Maximilian", "Tresp", "Volker", "Kriegel", "Hans-Peter"], "venue": "In WWW, pp", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Learning distributed representations of concepts using linear relational embedding", "author": ["Paccanaro", "Alberto", "Hinton", "Geoffrey E"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Paccanaro et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Paccanaro et al\\.", "year": 2001}, {"title": "Low-dimensional embeddings of logic", "author": ["Rockt\u00e4schel", "Tim", "Bo\u0161njak", "Matko", "Singh", "Sameer", "Riedel", "Sebastian"], "venue": "In ACL Workshop on Semantic Parsing,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2014}, {"title": "Learning first-order horn clauses from web text", "author": ["Schoenmackers", "Stefan", "Etzioni", "Oren", "Weld", "Daniel S", "Davis", "Jesse"], "venue": "In EMNLP,", "citeRegEx": "Schoenmackers et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Schoenmackers et al\\.", "year": 2010}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Shen", "Yelong", "He", "Xiaodong", "Gao", "Jianfeng", "Deng", "Li", "Mesnil", "Gregoire"], "venue": "In CIKM,", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Learning semantic representations using convolutional neural networks for Web search", "author": ["Shen", "Yelong", "He", "Xiaodong", "Gao", "Jianfeng", "Deng", "Li", "Mesnil", "Gr\u00e9goire"], "venue": "In WWW,", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Relational learning via collective matrix factorization", "author": ["Singh", "Ajit P", "Gordon", "Geoffrey J"], "venue": "In KDD,", "citeRegEx": "Singh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2008}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher", "Richard", "Huval", "Brody", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["Suchanek", "Fabian M", "Kasneci", "Gjergji", "Weikum", "Gerhard"], "venue": "In WWW,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Modelling relational data using Bayesian clustered tensor factorization", "author": ["Sutskever", "Ilya", "Tenenbaum", "Joshua B", "Salakhutdinov", "Ruslan"], "venue": "In NIPS, pp", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Under review as conference paper at ICRL", "author": ["O. Vinyals", "Y. Jia", "L. Deng", "T. Darrell"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantic parsing for single-relation question", "author": ["NIPS", "2012. Yih", "Wen-tau", "He", "Xiaodong", "Meek", "Christopher"], "venue": null, "citeRegEx": "NIPS et al\\.,? \\Q2012\\E", "shortCiteRegEx": "NIPS et al\\.", "year": 2012}, {"title": "The deep tensor neural network with applications to large vocabulary", "author": ["D. Yu", "L. Deng", "F. Seide"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 0, "context": "Recent years have witnessed a rapid growth of knowledge bases (KBs) such as Freebase1, DBPedia (Auer et al., 2007), and YAGO (Suchanek et al.", "startOffset": 95, "endOffset": 114}, {"referenceID": 29, "context": ", 2007), and YAGO (Suchanek et al., 2007).", "startOffset": 18, "endOffset": 41}, {"referenceID": 19, "context": "(Nickel et al., 2011; 2012)) and neural-embedding-based models (e.", "startOffset": 0, "endOffset": 27}, {"referenceID": 28, "context": "(Bordes et al., 2013a;b; Socher et al., 2013)) are two popular kinds of approaches that learn to encode relational information using low-dimensional representations of entities and relations.", "startOffset": 0, "endOffset": 45}, {"referenceID": 28, "context": ", 2013b) and NTN (Socher et al., 2013) have shown state-of-the-art prediction performance compared to tensor factorization methods such as RESCAL (Nickel et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 20, "context": ", 2013) have shown state-of-the-art prediction performance compared to tensor factorization methods such as RESCAL (Nickel et al., 2012).", "startOffset": 115, "endOffset": 136}, {"referenceID": 28, "context": "(1) We present a general framework for multirelational learning that unifies most multi-relational embedding models developed in the past, including NTN (Socher et al., 2013) and TransE (Bordes et al.", "startOffset": 153, "endOffset": 174}, {"referenceID": 9, "context": "To evaluate this approach, we compare our embedding-based rule extraction approach with a state-of-the-art rule mining system AMIE (Gal\u00e1rraga et al., 2013) and demonstrate the advantages of our approach in terms of effectiveness and efficiency.", "startOffset": 131, "endOffset": 155}, {"referenceID": 17, "context": "Prior work can be categorized into three categories: (1) statistical relational learning (SRL) (Getoor & Taskar, 2007), such as Markov-logic networks (Richardson & Domingos, 2006), which directly encode multi-relational graphs using probabilistic models; (2) path ranking methods (Lao et al., 2011; Dong et al., 2014), which explicitly explore the large feature space of relations with random walk; and (3) representation learning methods, which embed multi-relational knowledge into low-dimensional representations of entities and relations via tensor/matrix factorization (Singh & Gordon, 2008; Nickel et al.", "startOffset": 280, "endOffset": 317}, {"referenceID": 7, "context": "Prior work can be categorized into three categories: (1) statistical relational learning (SRL) (Getoor & Taskar, 2007), such as Markov-logic networks (Richardson & Domingos, 2006), which directly encode multi-relational graphs using probabilistic models; (2) path ranking methods (Lao et al., 2011; Dong et al., 2014), which explicitly explore the large feature space of relations with random walk; and (3) representation learning methods, which embed multi-relational knowledge into low-dimensional representations of entities and relations via tensor/matrix factorization (Singh & Gordon, 2008; Nickel et al.", "startOffset": 280, "endOffset": 317}, {"referenceID": 19, "context": ", 2014), which explicitly explore the large feature space of relations with random walk; and (3) representation learning methods, which embed multi-relational knowledge into low-dimensional representations of entities and relations via tensor/matrix factorization (Singh & Gordon, 2008; Nickel et al., 2011; 2012), Bayesian clustering framework (Kemp et al.", "startOffset": 264, "endOffset": 313}, {"referenceID": 16, "context": ", 2011; 2012), Bayesian clustering framework (Kemp et al., 2006; Sutskever et al., 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al.", "startOffset": 45, "endOffset": 88}, {"referenceID": 30, "context": ", 2011; 2012), Bayesian clustering framework (Kemp et al., 2006; Sutskever et al., 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al.", "startOffset": 45, "endOffset": 88}, {"referenceID": 28, "context": ", 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al., 2013a;b; Socher et al., 2013).", "startOffset": 29, "endOffset": 100}, {"referenceID": 28, "context": "Existing neural embedding models (Bordes et al., 2013a;b; Socher et al., 2013) all represent entities as low-dimensional vectors and represent relations as operators that combine the representations of two entities.", "startOffset": 33, "endOffset": 78}, {"referenceID": 28, "context": "For instance, given two entity vectors, the model of Neural Tensor Network (NTN) (Socher et al., 2013) represents each relation as a bilinear tensor operator followed by a linear matrix operator.", "startOffset": 81, "endOffset": 102}, {"referenceID": 28, "context": "For example, NTN (Socher et al., 2013) represent entities as an average of word vectors and initializing word vectors with pre-trained vectors from large text corpora.", "startOffset": 17, "endOffset": 38}, {"referenceID": 1, "context": ", 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al., 2013a;b; Socher et al., 2013). Our work focuses on the study of neural-embedding models, where the representations of the involved entities and relations are learned using neural networks. Existing neural embedding models (Bordes et al., 2013a;b; Socher et al., 2013) all represent entities as low-dimensional vectors and represent relations as operators that combine the representations of two entities. The main difference among these models lies in the parametrization of the relation operators. For instance, given two entity vectors, the model of Neural Tensor Network (NTN) (Socher et al., 2013) represents each relation as a bilinear tensor operator followed by a linear matrix operator. The model of TransE (Bordes et al., 2013b), on the other hand, represents each relation as a single vector that linearly interacts with the entity vectors. There has not been work that closely examines the effectiveness of different design choices. Likewise, variations on entity representations also exist. For example, NTN (Socher et al., 2013) represent entities as an average of word vectors and initializing word vectors with pre-trained vectors from large text corpora. This idea is promising as pre-trained vectors tend to capture syntactic and semantic information from natural language and can assist in better generalization of entity embeddings. However, averaging word vectors may not always be appropriate, especially for entities denoted by non-compositional phrases (e.g. person names, movie names, etc.). Our work on embedding-based rule extraction presented in part of this paper is related to the earlier study on logical inference with learned continuous-space representations. Much existing work along this line focuses on learning logic-based representations for natural language sentences. For example, Socher et al. (2012) builds a neural network that recursively combines word representations based on parse tree structures and shows that such neural network can simulate the behavior of conjunction and negation.", "startOffset": 56, "endOffset": 1912}, {"referenceID": 1, "context": ", 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al., 2013a;b; Socher et al., 2013). Our work focuses on the study of neural-embedding models, where the representations of the involved entities and relations are learned using neural networks. Existing neural embedding models (Bordes et al., 2013a;b; Socher et al., 2013) all represent entities as low-dimensional vectors and represent relations as operators that combine the representations of two entities. The main difference among these models lies in the parametrization of the relation operators. For instance, given two entity vectors, the model of Neural Tensor Network (NTN) (Socher et al., 2013) represents each relation as a bilinear tensor operator followed by a linear matrix operator. The model of TransE (Bordes et al., 2013b), on the other hand, represents each relation as a single vector that linearly interacts with the entity vectors. There has not been work that closely examines the effectiveness of different design choices. Likewise, variations on entity representations also exist. For example, NTN (Socher et al., 2013) represent entities as an average of word vectors and initializing word vectors with pre-trained vectors from large text corpora. This idea is promising as pre-trained vectors tend to capture syntactic and semantic information from natural language and can assist in better generalization of entity embeddings. However, averaging word vectors may not always be appropriate, especially for entities denoted by non-compositional phrases (e.g. person names, movie names, etc.). Our work on embedding-based rule extraction presented in part of this paper is related to the earlier study on logical inference with learned continuous-space representations. Much existing work along this line focuses on learning logic-based representations for natural language sentences. For example, Socher et al. (2012) builds a neural network that recursively combines word representations based on parse tree structures and shows that such neural network can simulate the behavior of conjunction and negation. Bowman (2014) further demonstrates that recursive neural network can capture certain aspects of natural logical reasoning on examples involving quantifiers like some", "startOffset": 56, "endOffset": 2118}, {"referenceID": 22, "context": "Rockt\u00e4schel et al. (2014) further implements the idea by introducing a supervised objective that trains embeddings to be consistent with given logical rules.", "startOffset": 0, "endOffset": 26}, {"referenceID": 28, "context": "For example, NTN (Socher et al., 2013) represents each entity as an average of its word vectors.", "startOffset": 17, "endOffset": 38}, {"referenceID": 28, "context": "where f can be a linear or non-linear function, and W is a parameter matrix, which can be randomly initialized or initialized using pre-trained word vectors (as in (Socher et al., 2013)) or pre-trained entity vectors.", "startOffset": 164, "endOffset": 185}, {"referenceID": 1, "context": "Models Br Ar Scoring Function Distance (Bordes et al., 2011) ` QTr1  \u0301Q T r2 \u0306 \u0301||g r pye1 ,ye2q||1 Single Layer (Socher et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 28, "context": ", 2011) ` QTr1  \u0301Q T r2 \u0306 \u0301||g r pye1 ,ye2q||1 Single Layer (Socher et al., 2013) `", "startOffset": 60, "endOffset": 81}, {"referenceID": 28, "context": "\u0301p2g r pye1 ,ye2q  \u0301 2g rpye1 ,ye2q ` ||Vr||2q NTN (Socher et al., 2013) Tr ` Qr1 Q T r2 \u0306", "startOffset": 51, "endOffset": 72}, {"referenceID": 14, "context": "This general framework for relationship modeling also applies to the recent deep-structured semantic model (Huang et al., 2013; Shen et al., 2014a;b; Gao et al., 2014; Yih et al., 2014), which learns the relevance or a single relation between a pair of word sequences.", "startOffset": 107, "endOffset": 185}, {"referenceID": 10, "context": "This general framework for relationship modeling also applies to the recent deep-structured semantic model (Huang et al., 2013; Shen et al., 2014a;b; Gao et al., 2014; Yih et al., 2014), which learns the relevance or a single relation between a pair of word sequences.", "startOffset": 107, "endOffset": 185}, {"referenceID": 5, "context": "We consider Mean Reciprocal Rank (MRR) (an average of the reciprocal rank of an answered entity over all test triplets), HITS@10 (top-10 accuracy), and Mean Average Precision (MAP) (as used in (Chang et al., 2014)) as the evaluation metrics.", "startOffset": 193, "endOffset": 213}, {"referenceID": 28, "context": "We examine five embedding models in decreasing order of complexity: (1) NTN with 4 tensor slices as in (Socher et al., 2013); (2) Bilinear+Linear, NTN with 1 tensor slice and without the non-linear layer; (3) TransE, a special case of Bilinear+Linear (see Table 1); (4) Bilinear: using scoring function in Eq.", "startOffset": 103, "endOffset": 124}, {"referenceID": 14, "context": "Other objectives such as mutual information (as in (Huang et al., 2013)) and reconstruction loss (as in tensor decomposition approaches (Chang et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 5, "context": ", 2013)) and reconstruction loss (as in tensor decomposition approaches (Chang et al., 2014)) can also be applied.", "startOffset": 72, "endOffset": 92}, {"referenceID": 8, "context": "Training was implemented using mini-batch stochastic gradient descent with AdaGrad (Duchi et al., 2011).", "startOffset": 83, "endOffset": 103}, {"referenceID": 18, "context": "We focus on DISTMULT as our baseline and compare it with the two modifications DISTMULT-tanh (using f \u201c tanh for entity projection5) and DISTMULT-tanh-EVinit (initializing the entity parameters with the 1000-dimensional pre-trained phrase vectors released by word2vec (Mikolov et al., 2013)) on FB15k-401.", "startOffset": 268, "endOffset": 290}, {"referenceID": 28, "context": "We also reimplemented the word vector representation and initialization technique introduced in (Socher et al., 2013) \u2013 each entity is represented as an average of its word vectors and the word vectors are initialized using the 300-dimensional pretrained word vectors released by word2vec.", "startOffset": 96, "endOffset": 117}, {"referenceID": 5, "context": "Inspired by (Chang et al., 2014), we design a new evaluation setting where the predicted entities are automatically filtered according to \u201centity types\u201d (entities that appear as the subjects/objects of a relation have the same type defined by that relation).", "startOffset": 12, "endOffset": 32}, {"referenceID": 23, "context": "closed-paths in the graph) by pruning and filtering rules with low statistical significance and relevance (Schoenmackers et al., 2010).", "startOffset": 106, "endOffset": 134}, {"referenceID": 9, "context": "We also compare our approaches to AMIE (Gal\u00e1rraga et al., 2013), a state-of-the-art rule mining system that can efficiently search for horn rules in large-scale KBs by using novel measurements of support and confidence.", "startOffset": 39, "endOffset": 63}, {"referenceID": 9, "context": "Gal\u00e1rraga et al. (2013) propose to automatically identify incorrect predictions based on the functional property of relations.", "startOffset": 0, "endOffset": 24}], "year": 2017, "abstractText": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE when evaluated on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules from the KB. We demonstrate that embeddings trained from the bilinear objective can effectively capture relation composition via matrix multiplication. We also show that our embedding-based approach can extract rules that involve relation transitivity more effectively than a state-of-the-art rule mining approach that is tailored for large-scale KBs.", "creator": "LaTeX with hyperref package"}}}