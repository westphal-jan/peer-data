{"id": "1412.6583", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Discovering Hidden Factors of Variation in Deep Networks", "abstract": "We propose a method for learning latent representations of the factors of variation in data. By augmenting deep autoencoders with a supervised cost and an additional unsupervised cost, we create a semi-supervised model that can discover and explicitly represent factors of variation beyond those relevant for categorization. We use a novel unsupervised covariance penalty (XCov) to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Our model discovers additional high-level latent factors absent from the supervised signal.", "histories": [["v1", "Sat, 20 Dec 2014 02:52:03 GMT  (2795kb,D)", "http://arxiv.org/abs/1412.6583v1", "10 pages, 9 figures"], ["v2", "Fri, 27 Feb 2015 20:41:40 GMT  (2798kb,D)", "http://arxiv.org/abs/1412.6583v2", "12 pages, 9 figures"], ["v3", "Fri, 17 Apr 2015 17:15:02 GMT  (2798kb,D)", "http://arxiv.org/abs/1412.6583v3", "12 pages, 9 figures"], ["v4", "Wed, 17 Jun 2015 06:47:48 GMT  (3042kb,D)", "http://arxiv.org/abs/1412.6583v4", "Presented at International Conference on Learning Representations 2015 Workshop"]], "COMMENTS": "10 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["brian cheung", "jesse a livezey", "arjun k bansal", "bruno a olshausen"], "accepted": true, "id": "1412.6583"}, "pdf": {"name": "1412.6583.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DEEP NETWORKS", "Brian Cheung", "Jesse A. Livezey", "Arjun K. Bansal", "Bruno A. Olshausen"], "emails": ["bcheung@berkeley.edu", "jesse.livezey@berkeley.edu", "baolshausen@berkeley.edu", "arjun@nervanasys.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most people are able to survive themselves, most people in the world have not been able to survive themselves, most people in the world have not been able to survive themselves, most people in the world have been able to survive themselves, most people in the world have not been able to survive themselves, most people in the world have been able to survive themselves, most people in the world have been able to survive themselves, most people in the world have not been able to survive themselves, most people in the world have been able to survive themselves, most people in the world have been able to survive themselves, most people in the world have been able to survive themselves, most people in the world have been able to survive themselves."}, {"heading": "2 MODEL", "text": "In view of a data sample x-RD and its corresponding class name y-1,..., L) for a given data set D, our model can be divided into an encoding and a decoding level."}, {"heading": "2.1 ENCODING", "text": "The encoder F (x; \u03b8) consists of two functions with the parameters \u03b8 = {\u03b8h, \u03b8z, \u03b8y}.y = q (x; \u03b8y, \u03b8h) z = r (x; \u03b8z, \u03b8h) {y, z} = F (x; \u03b8) \u2261 {q (x), r (x)} (1) As shown in Figure 1, the encoder first transforms the input x into a swab of variables: {y, z}. The label prediction y has a coding process y = q (x), which is functionally equivalent to a monitored feedback network. z is an additional latent representation of x, similar to the representation generated in each sufficiently deep layer during the feedback process of an autoencoder. To keep our current formulation simple, z is at the same depth as y."}, {"heading": "2.2 DECODING", "text": "The decoder G (y, z; \u03c6) transforms the tuple provided by the encoder into a reconstruction x. the combination of the encoding and decoding steps defines the forward trajectory of our model: x, y, z; \u03c6) = G (F (x; \u03b8); \u03c6) (2) The decoder must generate a reconstruction x that comes as close as possible to the attribute representation y and z. The term y often represents a lossy transformation of the input as the class designation. In order to properly reconstruct x, the latent variable z must take into account the remaining variation of x. For example, the class designation \"5\" provided by y would not be sufficient to provide the decoder with sufficient information to properly reconstruct the image of a certain digit \"5.\" In this scenario, z would encode the characteristics of the digit such as style, slope, width, etc. to provide the image with sufficient decoder information."}, {"heading": "2.3 LEARNING", "text": "With the forward-looking process of the network defined, the objective function for training the network can be described. (3) U (x, x) is a reconstruction cost (x, y) is a supervised cost of the encoder, and C (y, y, z) is the XCov cost that unravels the supervised and unsupervised latent variables of the encoder. (3) This objective function naturally fits into a semi-supervised learning framework. For unlabeled data, the multiplier \u03b2 for the supervised cost S is simply set to zero. Moreover, if the goal, coding and decoding functions are differentiable, this model can descend with stochastic gradients."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "We evaluate our model using three sets of data of increasing complexity. The network is trained using AdaDelta (Zeiler, 2012) with gradients from the standard backpropagation. Models have been implemented in a modified version of Pylearn2 (Goodfellow et al., 2013a)."}, {"heading": "3.1 MNIST HANDWRITTEN DIGITS DATABASE", "text": "The MNIST database of handwritten digits (LeCun & Cortes, 1998) consists of 60,000 training images and 10,000 test images with handwritten digits 0-9. Following previous work (Goodfellow et al., 2013b), we have divided the training set into 50,000 samples for training and 10,000 samples as a validation kit for model selection."}, {"heading": "3.2 TORONTO FACES DATABASE", "text": "The Toronto Faces Database (Susskind et al., 2010) consists of 102,236 grayscale facial images measuring 48x48, of which 4,178 are labeled with 1 of 7 different expressions (anger, disgust, fear, happiness, sadness, surprise and neutrality). Examples are shown in Figure 2. The dataset also contains 3,784 identity tags not used in this essay. The dataset includes 5 folds of training, validation and testing examples."}, {"heading": "3.3 MULTI-PIE DATASET", "text": "The Multi-PIE datasets (Gross et al., 2010) consist of 754,200 high-resolution color images from 337 subjects. Each subject was recorded under 15 camera positions: 13 at a distance of 15 degrees at head height and 2 above the subject. For each of these cameras, subjects were imaged under 19 lighting conditions and a variety of facial expressions. We discarded images from the two overhead cameras due to discrepancies in the image alignment of these two cameras. Camera positions and illumination data were retained as verified labels. Only a small subset of images has facial information for each camera position. To perform a weak registration to approximately locate the facial region, we calculate the maximum bounding box created by all available face key point coordinates for a given camera position. The bounding box is applied to all images for these camera positions."}, {"heading": "3.4 EXPLORING DEEP SPACE", "text": "We begin our analysis with the MNIST dataset. After training the MNIST model described in Table 1, the function of hidden units in different layers can be investigated. As shown in Figure 4, the unattended latent variables z assume an approximate normal distribution with a mean zero and standard deviation."}, {"heading": "3.4.1 UNSUPERVISED LATENT VARIABLES", "text": "To visualize the transformations that the unattended variables learn, the decoder can be used to generate images with different values for z. We select latent variable values by setting zs to a linear range, with y applying uniform vectors, as in Figure 4. At the center of the z-space (0,0) we find the canonical MNIST digits. As we move further away from the center, the digits become more stylized and also less likely."}, {"heading": "3.4.2 MOVING FROM LATENT SPACE TO IMAGE SPACE", "text": "After the monitored and unsupervised latent variables, there are two additional activation layers before the model is output to the image space. To visualize the function of these layers, we calculate the Jacobin value of the output image, x, in relation to the activation of hidden units, hk, in a particular layer, 0 x, ki = \u2202 x, i, hkj, hj. (8) Here, i is the index of a pixel in the output of the network, j is the index of a hidden unit, and k is the layer number. We remove hidden units with zero activation from the Jacobin, as their derivatives are not meaningful. A summary of the results is shown in Figure 5.For the Jacobins in relation to the z units, we draw the result in the image space. As expected, the Jacobins locally reflect the transformations in relation to the z units, which can be seen in Figure 4.Inspired by Rifaal. Thus, for the next two, we create the corresponding layer for the first three of the image (2011)."}, {"heading": "3.5 GENERATING EXPRESSION TRANSFORMATIONS", "text": "We demonstrate the semi-supported capabilities of our model using the TFD, which contains much more complex images than MNIST and has far fewer labeled examples. We are able to modify the expression while retaining the identity of faces that have never been seen by the model. First, we initialize {y, z} with an example from the test set. Then, we discard y and fill it with an expression label and keep it. Figure 6 shows the results of this process. Expressions can be modified while other facial features remain largely intact. This is not possible if the XCov costs are removed, because the expression information is distributed to all 793-z variables in addition to the 7-y variables. Changes to y significantly affect the representation provided to the decoder. In addition, y can be set to values that far exceed the values that the encoder could output with a gentle maximum activation during training. We vary the expression variable, which is given to the decoder, to the decoder, to the decoder, or to the decoded structure when given to large or to negative."}, {"heading": "3.6 MANIPULATING MULTIPLE FACTORS OF VARIATION", "text": "For Multi-PIE, we use two sets of monitored latent factors (camera position and illumination) to train our model. As shown in Table 1, we have two Softmax layers at the end of the encoder, the first encoding the camera position of the input image and the second the illumination condition. Due to the increased complexity of this data set, we have this network much deeper (9 layers) than the previous models. Due to the additional gradient information injected by the monitored and covariance costs at the end of the encoder, it was possible to train this auto-encoder without prior training, similar to Szegedy et al. (2014). In Figure 8, we show the images generated by the decoder during iteration through each camera position. The network was tied to the illumination and unattended latent variables of images from the test set. Although the generated images are blurred, we preserve the illumination and identity of the subject (i.e., the skin color, the hair style)."}, {"heading": "4 CONCLUSIONS", "text": "We show that the model can use labeled and unlabeled data simultaneously. In addition, our model's decoder implicitly learns to create new manipulations of images on multiple sets of transformation variables. We show that deep feedback-forward networks are able to learn higher-order variation factors beyond the monitored labels without explicitly defining these higher-order interactions."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Nervana Systems for supporting Brian Cheung during the summer of this project and for continuing to work together. We thank NVIDIA Corporation for supporting the Tesla K40 GPUs used for this research. Bruno Olshausen was supported by NSF funding IIS-1111765. APPENDIXCLASSIFICATION PERFORMACETable 2 shows classification results for the MNIST and TFD networks described in Table 3. With this model it is possible to trade classification accuracy for reconstruction accuracy. Our methods are susceptible to different non-linearities such as maxout (Goodfellow et al., 2013b) and different training algorithms such as SGD mit Dynamik, SFO (Sohl-Dickstein et al., 2014) or Dropout (Srivastava et al., 2014)."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Restoring an image taken through a window covered with dirt or rain", "author": ["Eigen", "David", "Krishnan", "Dilip", "Fergus", "Rob"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Eigen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Eigen et al\\.", "year": 2013}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Bilinear sparse coding for invariant vision", "author": ["Grimes", "David B", "Rao", "Rajesh PN"], "venue": "Neural computation,", "citeRegEx": "Grimes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Grimes et al\\.", "year": 2005}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Le", "Quoc V"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Le and V.,? \\Q2013\\E", "shortCiteRegEx": "Le and V.", "year": 2013}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Lee", "Honglak", "Pham", "Peter", "Largman", "Yan", "Ng", "Andrew Y"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Bilinear models of natural images", "author": ["Olshausen", "Bruno A", "Cadieu", "Charles", "Culpepper", "Jack", "Warland", "David K"], "venue": "In Electronic Imaging", "citeRegEx": "Olshausen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 2007}, {"title": "Learning to disentangle factors of variation with manifold interaction", "author": ["Reed", "Scott", "Sohn", "Kihyuk", "Zhang", "Yuting", "Lee", "Honglak"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Reed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2014}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Disentangling factors of variation for facial expression recognition", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal", "Mirza", "Mehdi"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["Sohl-Dickstein", "Jascha", "Poole", "Ben", "Ganguli", "Surya"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "The toronto face database", "author": ["J. Susskind", "A. Anderson", "G.E. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Separating style and content with bilinear models", "author": ["Tenenbaum", "Joshua B", "Freeman", "William T"], "venue": "Neural computation,", "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "One of the goals of representation learning is to find an efficient representation of input data that simplifies tasks such as object classification (Krizhevsky et al., 2012) or image restoration (Eigen et al.", "startOffset": 149, "endOffset": 174}, {"referenceID": 1, "context": ", 2012) or image restoration (Eigen et al., 2013).", "startOffset": 29, "endOffset": 49}, {"referenceID": 10, "context": "identity using higher-order restricted Boltzmann machines (Reed et al., 2014).", "startOffset": 58, "endOffset": 77}, {"referenceID": 11, "context": "Autoencoder models have been shown to be useful for a variety of machine learning tasks (Rifai et al., 2011; Vincent et al., 2010; Le, 2013).", "startOffset": 88, "endOffset": 140}, {"referenceID": 18, "context": "Autoencoder models have been shown to be useful for a variety of machine learning tasks (Rifai et al., 2011; Vincent et al., 2010; Le, 2013).", "startOffset": 88, "endOffset": 140}, {"referenceID": 3, "context": "For example, Kingma et al. (2014) utilized a variational autoencoder in a semi-supervised learning paradigm which learned to separate content and style in data.", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": "Bengio et al. (2007) showed that greedy layerwise pre-training can speed up the convergence and improve generalization capabilities of deep networks.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "Rifai et al. (2012) proposed a similar penalty in Contractive Discriminant Analysis method which penalized the cross-derivatives between sets of supervised and unsupervised latent variables with respect to the input.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "2 TORONTO FACES DATABASE The Toronto Faces Database (Susskind et al., 2010) consists of 102,236 grayscale face images of size 48x48.", "startOffset": 52, "endOffset": 75}, {"referenceID": 11, "context": "Inspired by Rifai et al. (2011), for the next two layers, we plot the singular value spectrum.", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "It was possible to train this autoencoder without pre-training because of the additional gradient information injected by the supervised and covariance cost at the end of the encoder similar to Szegedy et al. (2014). In Figure 8, we show the images generated by the decoder while iterating through each camera pose.", "startOffset": 194, "endOffset": 216}, {"referenceID": 13, "context": ", 2013b) and different training algorithms such as SGD with momentum, SFO (Sohl-Dickstein et al., 2014), or Dropout (Srivastava et al.", "startOffset": 74, "endOffset": 103}, {"referenceID": 2, "context": "Our methods are amenable to different nonlinearities such as maxout (Goodfellow et al., 2013b) and different training algorithms such as SGD with momentum, SFO (Sohl-Dickstein et al., 2014), or Dropout (Srivastava et al., 2014). Performance of a fully connectedl maxout network used for the encoder similar to Goodfellow et al. (2013b) is also shown.", "startOffset": 69, "endOffset": 336}], "year": 2017, "abstractText": "We propose a method for learning latent representations of the factors of variation in data. By augmenting deep autoencoders with a supervised cost and an additional unsupervised cost, we create a semi-supervised model that can discover and explicitly represent factors of variation beyond those relevant for categorization. We use a novel unsupervised covariance penalty (XCov) to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Our model discovers additional high-level latent factors absent from the supervised signal.", "creator": "LaTeX with hyperref package"}}}