{"id": "1412.6598", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Automatic Discovery and Optimization of Parts for Image Classification", "abstract": "Part-based representations have been shown to be very useful for image classification. Learning part-based models is often viewed as a two-stage problem. First, a collection of informative parts is discovered, using heuristics that promote part distinctiveness and diversity, and then classifiers are trained on the vector of part responses. In this paper we unify the two stages and learn the image classifiers and a set of shared parts jointly. We generate an initial pool of parts by randomly sampling part candidates and selecting a good subset using L1/L2 regularization. All steps are driven \"directly\" by the same objective namely the classification loss on a training set. This lets us do away with engineered heuristics. We also introduce the notion of \"negative parts\", intended as parts that are negatively correlated with one or more classes. Negative parts are complementary to the parts discovered by other methods, which look only for positive correlations.", "histories": [["v1", "Sat, 20 Dec 2014 04:25:34 GMT  (5085kb,D)", "https://arxiv.org/abs/1412.6598v1", "19 pages"], ["v2", "Sat, 11 Apr 2015 20:13:40 GMT  (8936kb,D)", "http://arxiv.org/abs/1412.6598v2", "19 pages, template changed to camera ready version, 1 reference added, 1 reference fixed, Fig. 3, 4 updated (larger text)"]], "COMMENTS": "19 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["sobhan naderi parizi", "rea vedaldi", "rew zisserman", "pedro felzenszwalb"], "accepted": true, "id": "1412.6598"}, "pdf": {"name": "1412.6598.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sobhan Naderi Parizi", "Andrea Vedaldi", "Andrew Zisserman", "Pedro Felzenszwalb"], "emails": ["sobhan@brown.edu", "az}@robots.ox.ac.uk", "pff@brown.edu"], "sections": [{"heading": null, "text": "Part-based representations prove to be very useful for image classification. Learning part-based models is often considered a two-step problem. First, a collection of informative parts is discovered, using heuristics that promote the distinctness and diversity of parts, and then classifiers are trained on the vector of partial reactions. In this essay, we unify the two stages and jointly learn the image classifiers and a set of split parts. We create an initial pool of parts by randomly selecting partial candidates and selecting a good subset using \"1 / '2 regularization.\" All steps are driven directly by the same goal, namely the loss of classification on a training set. This lets us get away from technical heuristics. We also introduce the notion of negative parts, which are thought of parts negatively correlated to one or more classes. Negative parts supplement the parts discovered by other methods that only look for positive correlations."}, {"heading": "1 INTRODUCTION", "text": "There are at least three main reasons why parts of this system are useful to represent objects or scenes. A second reason is that parts of it can be recombined in a model to express a combinatorial number of variants of an object or scene. For example, parts of it can be realized in a scene, in the form of objects."}, {"heading": "1.1 RELATED WORK", "text": "The general pipeline in all of these approaches is a two-step process that preforms a set of parts, but includes a set of discriminatory parts, followed by the formation of a classifier on top of the vector of partial answers. The differences in these methods lie in the details of how parts are discovered. Each approach uses a different heuristic method to find a collection of parts, so that each part is highly rated on a subset of categories (and therefore discriminatory) and, collectively, they cover a large area of an image after maximum merge (and are therefore descriptive). Our goal is similar, but we achieve partial diversity, distinctiveness, and coverage as natural by-products of optimizing the \"right\" objective function, i.e. the final image classification performance."}, {"heading": "2 PART-BASED MODELS AND NEGATIVE PARTS", "text": "We assume that an image class with part of the image is treated as a latent variable that spreads over that image. (zj) We assume that an image class with part of the image does not exist. (zj) We assume that an image class with part of the image does not exist. (zj) We assume that an image class with part of the image does not exist. (zj) We assume that an image class with part of the image does not exist. (zj) We assume that an image class with part of the image does not exist. (zj) We assume that an image class with part of the image does not exist. (zj) We assume that an image class with part of the image does not exist. (zj) We assume that an image class with part of the image does not exist. (zj) We assume that an image class with part of the image does not exist. (zj) We assume that an image class with part of the image does not exist."}, {"heading": "2.1 NEGATIVE PARTS IN MULTI-CLASS SETTING", "text": "In the previous section, we showed that certain one-on-all part-based classifiers, including D\u03b2M, cannot capture counter-evidence of negative parts, as long as we allow a positive part to be used for a negative part for another class, using more general models with two sets of parameters \u03b2 = (u, w) and a scoring function f\u03b2 (x) = u \u00b7 r (x, w). Now, we consider the case of a multi-class classifier in which partial answers are weighted differently for each category, but all categories share the same set of sub-filters. A natural consequence of the part is that a positive part can be used for one class as a negative part for another class."}, {"heading": "3 JOINT TRAINING", "text": "In this section, we propose an approach for a common training of all parameters \u03b2 = (w, u) from a multidisciplinary, part-based model. Training is directly driven by loss of classification. Note that a loss of classification is sufficient to promote the diversity of parts. In particular, a joint training encourages partial filters to complement each other. We have found that a joint training leads to a significant improvement in performance (see Section 5). The use of classification losses also leads to a simple framework that is not based on multiple heuristics. Let D = (xi, yi)} ki = 1 denote a series of training examples mentioned above. We train with \"2 regulatory measures for both partial filters w and the partial weights u (we think of each as a single vector) and the multi-class hinges loss, which results in the objective function: O (u, w) = 1 + 2 + 2 + 2."}, {"heading": "4 PART GENERATION AND SELECTION", "text": "The common training target in (6) is non-convex, making Algorithm 1 sensitive to initialization. Thus, the selection of the initial parts may be critical in training models that perform well in practice. We dedicate the first two steps of our pipeline to finding good initial parts (Figure 2). We then use these parts to randomly generate a large pool of initial parts in the first step of our pipeline. Generating one part involves selecting a random training image (regardless of image categories) and extracting features from a random partial window of the image followed by brightening (Hariharan et al. (2012)). To lighten a feature vector when using a random vector \u2212 1 (f \u2212 \u00b5), where \u00b5 and \u03a3 are the mean and covariance of all patches in all training images. We estimate that the digits of 300,000 random patches are different from the norm used to estimate the distinctness of a patch."}, {"heading": "5 EXPERIMENTS", "text": "We evaluate our methods on the MIT Indoor Database (Quattoni & Torralba). We compare the performance of models with randomly generated parts, selected parts and jointly trained parts. We also compare the performance of HOG and CNN features. However, there are a large number of parts (between 3350 and 13400) based on this database. We reduce images (while maintaining aspect ratio) to have 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs) (2005); Felzenszwalb et al al al al al al al al al al. (2010) on multiple scales. Our HOG pyramid has 11000 patches per part."}, {"heading": "5.1 VISUALIZATION OF THE MODEL", "text": "Figure 5 shows the partial weight matrix after joint training, a model with 52 parts on the full MITindoor dataset. This model uses 60 PCA coefficients from the HP CNN features. Figure 6 shows top scoring patches for a few parts before and after joint training. The parts correspond to the model shown in Figure 5. The benefits of joint training are clear. The parts are consistent and \"clean\" after joint training. The majority of evidence of part 25 before joint training are seats. Joint training filters most of the space (mostly from bed and sofa) in this part."}, {"heading": "6 CONCLUSIONS", "text": "All model parameters are jointly trained within our framework, including common part filters and class-specific part weights. All levels of our training pipeline are driven directly by the same goal, which is classification performance on a training set. Specifically, our framework is not based on ad hoc heuristics to select discriminatory and / or diverse parts. We also introduced the concept of \"negative parts\" for part-based models. Models based on our randomly generated parts perform better than almost all previously published work, despite the profound simplicity of the method. Using CNN features and random parts, we obtain 77.1% accuracy on the MIT indoor data set, thus improving the state of the art. We also demonstrated that part selection and joint training can be used to train a model that achieves better or the same level of performance as a system with randomly generated parts using significantly fewer parts."}, {"heading": "B PART SELECTION", "text": "As mentioned in section 4 of the paper, we use group spareness to select useful parts from a pool of randomly selected parts. We use the same formulation as in Sun & Ponce (2013).The partial selection is made by optimizing the following objective function: \u03bb m \u00b2 j = 1 \u00b2 + k \u00b2 i = 1 max {0, max y 6 = yi (uy \u2212 uyi) \u00b7 r (xi, w) + 1 (17), where \u03c1j = 270 \u00b0 y \u00b2 y \u00b2 y \u00b2 y \u00b2 y, j is the \"2 standard of the column of u corresponding to part j. This objective function is convex. We minimize it by stochastic gradient departure, for which we must repeatedly take a small step in the opposite direction of a sub-gradient of the function. LetRg (u) = determination m j = 1 \u00b2 s \u00b2, which we define as such. The partial derivative Rg \u00b2 uy = uy \u00b2 jumps to zero."}, {"heading": "C MORE ON VISUALIZATION OF THE MODEL", "text": "We supplement section 5.1 of the paper by including more visualizations from part 1 to part 1 of yellow-colored parts at the garment level.Figure 9 shows the partial filters and weight matrix after joint training a model with 52 parts on the 10-grade subset of the MIT indoor record. This model uses HOG characteristics. The partial weight matrix determines whether a part is positive or negative in relation to two categories. Example: Part 42 is positive for bookstore and library in relation to laundromat. Part 29 is positive for laundromat in relation to bookstore and library. Part 37 is positive for library in relation to bookstore, so that it can be used in combination with the other two parts to distinguish between all three categories bookstore, library and laundromat. Figure 10 illustrates the top evaluation fields for these three parts. Figure 11 supplements Figure 7 of the paper so that the paper indices per column 5 coincide with those in the first column."}, {"heading": "D PROCESSING TIME", "text": "The first step is a standard multi-level system in which several persons are at the head at the same time. (2) The second step is the capture of individuals. (3) The second step is the capture of individuals. (3) The third step is the capture of individuals. (4) The third step is the capture of individuals. (4) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the capture of individuals. (5) The third step is the individual persons. (5) The third step is the individual persons. (5) The third step is the individual persons."}], "references": [{"title": "Painting-to-3D model alignment via discriminative visual elements", "author": ["Aubry", "Mathieu", "Russell", "Bryan", "Sivic", "Josef"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "Aubry et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Aubry et al\\.", "year": 2013}, {"title": "Histograms of oriented gradients for human detection", "author": ["Dalal", "Navneet", "Triggs", "Bill"], "venue": "In CVPR,", "citeRegEx": "Dalal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dalal et al\\.", "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Mid-level visual element discovery as discriminative mode seeking", "author": ["Doersch", "Carl", "Gupta", "Abhinav", "Efros", "Alexei"], "venue": "In NIPS,", "citeRegEx": "Doersch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2013}, {"title": "Learning collections of part models for object recognition", "author": ["Endres", "Ian", "Shih", "Kevin", "Jiaa", "Johnston", "Hoiem", "Derek"], "venue": "In CVPR,", "citeRegEx": "Endres et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Endres et al\\.", "year": 2013}, {"title": "Object detection with discriminatively trained part based models", "author": ["Felzenszwalb", "Pedro", "Girshick", "Ross", "McAllester", "David", "Ramanan", "Deva"], "venue": null, "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Training deformable part models with decorrelated features", "author": ["Girshick", "Ross", "Malik", "Jitendra"], "venue": "In ICCV,", "citeRegEx": "Girshick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2013}, {"title": "Discriminative decorrelation for clustering and classication", "author": ["Hariharan", "Bharath", "Malik", "Jitendra", "Ramanan", "Deva"], "venue": "In ECCV,", "citeRegEx": "Hariharan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Cutting-plane training of structural svms", "author": ["Joachims", "Thorsten", "Finley", "Thomas", "Yu", "Chun-Nam John"], "venue": "Machine Learning,", "citeRegEx": "Joachims et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2009}, {"title": "Blocks that shout: Distinctive parts for scene classification", "author": ["Juneja", "Mayank", "Vedaldi", "Andrea", "C.V. Jawahar", "Zisserman", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Juneja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Juneja et al\\.", "year": 2013}, {"title": "Imagenet classication with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Beyond bag of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["Lazebnik", "Svetlana", "Schmid", "Cordelia", "Ponce", "Jean"], "venue": "In CVPR,", "citeRegEx": "Lazebnik et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2006}, {"title": "Hierarchical joint max-margin learning of mid and top level representations for visual recognition", "author": ["Lobel", "Hans", "Vidal", "Rene", "Soto", "Alvaro"], "venue": "In ICCV,", "citeRegEx": "Lobel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lobel et al\\.", "year": 2013}, {"title": "Reconfigurable models for scene recognition", "author": ["Naderi", "Sobhan", "Oberlin", "John", "Felzenszwalb", "Pedro"], "venue": "In CVPR,", "citeRegEx": "Naderi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Naderi et al\\.", "year": 2012}, {"title": "Recognizing indoor scenes", "author": ["Quattoni", "Ariadna", "Torralba", "Antonio"], "venue": "In CVPR,", "citeRegEx": "Quattoni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2009}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["Razavian", "Ali Sharif", "Azizpour", "Hossein", "Sullivan", "Josephine", "Carlsson", "Stefan"], "venue": "In CVPR DeepVision workshop,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Unsupervised discovery of mid-level discriminative patches", "author": ["Singh", "Saurabh", "Gupta", "Abhinav", "Efros", "Alexei"], "venue": "In ECCV,", "citeRegEx": "Singh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2012}, {"title": "Learning discriminative part detectors for image classification and cosegmentation", "author": ["Sun", "Jian", "Ponce", "Jean"], "venue": "In ICCV,", "citeRegEx": "Sun et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "The concave-convex procedure", "author": ["Yuille", "Alan", "Rangarajan", "Anand"], "venue": "In NIPS,", "citeRegEx": "Yuille et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yuille et al\\.", "year": 2003}, {"title": "Learning deep features for scene recognition using places database", "author": ["Zhou", "Bolei", "Lapedriza", "Agata", "Xiao", "Jianxiong", "Torralba", "Antonio", "Oliva", "Aude"], "venue": "In NIPS,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Discovering good parts is a difficult problem that has recently raised considerable interest (Juneja et al. (2013); Doersch et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 3, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)).", "startOffset": 8, "endOffset": 30}, {"referenceID": 3, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)).", "startOffset": 8, "endOffset": 50}, {"referenceID": 3, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)). The quality of a part can be defined in different ways. Methods such as (Juneja et al. (2013); Doersch et al.", "startOffset": 8, "endOffset": 146}, {"referenceID": 3, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)). The quality of a part can be defined in different ways. Methods such as (Juneja et al. (2013); Doersch et al. (2013)) decouple learning parts and image classifiers by optimizing an intermediate objective that is only heuristically related to classification.", "startOffset": 8, "endOffset": 169}, {"referenceID": 9, "context": "To address this methods such as (Juneja et al. (2013); Endres et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 4, "context": "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model.", "startOffset": 8, "endOffset": 29}, {"referenceID": 4, "context": "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)).", "startOffset": 8, "endOffset": 650}, {"referenceID": 4, "context": "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts.", "startOffset": 8, "endOffset": 1107}, {"referenceID": 4, "context": "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts.", "startOffset": 8, "endOffset": 1127}, {"referenceID": 4, "context": "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts. This translates directly to test time speedup. We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al.", "startOffset": 8, "endOffset": 1322}, {"referenceID": 4, "context": "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts. This translates directly to test time speedup. We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al. (2012)) features and improve the state-of-the-art results on the MIT-indoor dataset (Quattoni & Torralba (2009)) using CNN features.", "startOffset": 8, "endOffset": 1357}, {"referenceID": 4, "context": "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts. This translates directly to test time speedup. We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al. (2012)) features and improve the state-of-the-art results on the MIT-indoor dataset (Quattoni & Torralba (2009)) using CNN features.", "startOffset": 8, "endOffset": 1462}, {"referenceID": 13, "context": "Related ideas in part learning have been recently explored in (Singh et al. (2012); Juneja et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 9, "context": "(2012); Juneja et al. (2013); Sun & Ponce (2013); Doersch et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 9, "context": "(2012); Juneja et al. (2013); Sun & Ponce (2013); Doersch et al.", "startOffset": 8, "endOffset": 49}, {"referenceID": 3, "context": "(2013); Sun & Ponce (2013); Doersch et al. (2013)).", "startOffset": 28, "endOffset": 50}, {"referenceID": 3, "context": "(2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classifier on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to find a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the \u201ccorrect\u201d objective function, i.e. the final image classification performance. Reconfigurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classification.", "startOffset": 28, "endOffset": 873}, {"referenceID": 3, "context": "(2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classifier on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to find a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the \u201ccorrect\u201d objective function, i.e. the final image classification performance. Reconfigurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classification. RBoW uses latent variables to define a mapping from image regions to part models. In contrast, the latent variables in our model define a mapping from parts to image regions. It has been shown before (Girshick & Malik (2013)) that joint training is important for the success of part-based models in object detection.", "startOffset": 28, "endOffset": 1157}, {"referenceID": 3, "context": "(2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classifier on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to find a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the \u201ccorrect\u201d objective function, i.e. the final image classification performance. Reconfigurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classification. RBoW uses latent variables to define a mapping from image regions to part models. In contrast, the latent variables in our model define a mapping from parts to image regions. It has been shown before (Girshick & Malik (2013)) that joint training is important for the success of part-based models in object detection. Differently from them, however, we share parts among multiple classes and define a joint optimization in which multiple classifiers are learned concurrently. In particular, the same part can vote strongly for a subset of the classes and against another subset. The most closely related work to ours is (Lobel et al. (2013)).", "startOffset": 28, "endOffset": 1572}, {"referenceID": 3, "context": "(2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classifier on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to find a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the \u201ccorrect\u201d objective function, i.e. the final image classification performance. Reconfigurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classification. RBoW uses latent variables to define a mapping from image regions to part models. In contrast, the latent variables in our model define a mapping from parts to image regions. It has been shown before (Girshick & Malik (2013)) that joint training is important for the success of part-based models in object detection. Differently from them, however, we share parts among multiple classes and define a joint optimization in which multiple classifiers are learned concurrently. In particular, the same part can vote strongly for a subset of the classes and against another subset. The most closely related work to ours is (Lobel et al. (2013)). Their model has two sets of parameters; a dictionary of visual words \u03b8 and a set of weights u that specifies the importance the visual words in each category. Similar to what we do here, Lobel et al. (2013) trains u and \u03b8 jointly (visual words would be the equivalent of part filters in our terminology).", "startOffset": 28, "endOffset": 1781}, {"referenceID": 11, "context": "In practice, filter responses are pooled within several distinct spatial subdivisions (Lazebnik et al. (2006)) to encode weak geometry.", "startOffset": 87, "endOffset": 110}, {"referenceID": 5, "context": "DPMs (Felzenszwalb et al. (2010)) also use binary classifiers to detect objects of each class.", "startOffset": 6, "endOffset": 33}, {"referenceID": 7, "context": "Generating a part involves picking a random training image (regardless of the image category labels) and extracting features from a random subwindow of the image followed by whitening (Hariharan et al. (2012)).", "startOffset": 185, "endOffset": 209}, {"referenceID": 0, "context": "Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts.", "startOffset": 12, "endOffset": 32}, {"referenceID": 0, "context": "Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts. Our experimental results with HOG features (Figure 3) show that randomly generated parts using the procedure described here perform better than or comparable to previous methods that are much more involved (Juneja et al. (2013); Doersch et al.", "startOffset": 12, "endOffset": 357}, {"referenceID": 0, "context": "Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts. Our experimental results with HOG features (Figure 3) show that randomly generated parts using the procedure described here perform better than or comparable to previous methods that are much more involved (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)).", "startOffset": 12, "endOffset": 380}, {"referenceID": 0, "context": "Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts. Our experimental results with HOG features (Figure 3) show that randomly generated parts using the procedure described here perform better than or comparable to previous methods that are much more involved (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)).", "startOffset": 12, "endOffset": 400}, {"referenceID": 6, "context": "Recent part-based methods that do well on this dataset (Juneja et al. (2013); Doersch et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 2, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400).", "startOffset": 8, "endOffset": 30}, {"referenceID": 2, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400).", "startOffset": 8, "endOffset": 50}, {"referenceID": 2, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400). HOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al.", "startOffset": 8, "endOffset": 252}, {"referenceID": 2, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400). HOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al. (2010)) at multiple scales.", "startOffset": 8, "endOffset": 280}, {"referenceID": 2, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400). HOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al. (2010)) at multiple scales. Our HOG pyramid has 3 scales per octave. This yields about 11,000 patches per image. Each part filter wj models a 6\u00d76 grid of HOG features, so wj and \u03c8(x, zj) are both 1152-dimensional. CNN features: We extract CNN features at multiple scales from overlapping patches of fixed size 256\u00d7256 and with stride value 256/3 = 85. We resize images (maintaining aspect ratio) to have about 5M pixels in the largest scale. We use a scale pyramid with 2 scales per octave. This yields about 1200 patches per image. We extract CNN features using Caffe (Jia et al. (2014)) and the hybrid neural network from (Zhou et al.", "startOffset": 8, "endOffset": 861}, {"referenceID": 2, "context": "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400). HOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al. (2010)) at multiple scales. Our HOG pyramid has 3 scales per octave. This yields about 11,000 patches per image. Each part filter wj models a 6\u00d76 grid of HOG features, so wj and \u03c8(x, zj) are both 1152-dimensional. CNN features: We extract CNN features at multiple scales from overlapping patches of fixed size 256\u00d7256 and with stride value 256/3 = 85. We resize images (maintaining aspect ratio) to have about 5M pixels in the largest scale. We use a scale pyramid with 2 scales per octave. This yields about 1200 patches per image. We extract CNN features using Caffe (Jia et al. (2014)) and the hybrid neural network from (Zhou et al. (2014)).", "startOffset": 8, "endOffset": 917}, {"referenceID": 2, "context": "The hybrid network is pre-trained on images from ImageNet (Deng et al. (2009)) and PLACES (Zhou et al.", "startOffset": 59, "endOffset": 78}, {"referenceID": 2, "context": "The hybrid network is pre-trained on images from ImageNet (Deng et al. (2009)) and PLACES (Zhou et al. (2014)) datasets.", "startOffset": 59, "endOffset": 110}, {"referenceID": 2, "context": "The hybrid network is pre-trained on images from ImageNet (Deng et al. (2009)) and PLACES (Zhou et al. (2014)) datasets. We use the output of the 4096 units in the penultimate fully connected layer of the network (fc7). We denote these features by HP in our plots. Part-based representation: Our final image representation is an mR-dimensional vector of part responses wherem is the number of shared parts andR is the number of spatial pooling regions. We use R = 5 pooling regions arranged in a 1\u00d71 + 2\u00d72 grid. To make the final representation invariant to horizontal image flips we average the mR-dimensional vector of part responses for image x and its right-to-left mirror image x\u2032 to get [r(x,w) + r(x\u2032, w)] /2 as in (Doersch et al. (2013)).", "startOffset": 59, "endOffset": 745}, {"referenceID": 9, "context": "On the full dataset, random parts already outperform the results from Juneja et al. (2013), flip invariance boosts the performance beyond Sun & Ponce (2013).", "startOffset": 70, "endOffset": 91}, {"referenceID": 9, "context": "On the full dataset, random parts already outperform the results from Juneja et al. (2013), flip invariance boosts the performance beyond Sun & Ponce (2013). Joint training dominates other methods.", "startOffset": 70, "endOffset": 157}, {"referenceID": 3, "context": "However, we could not directly compare with the best performance of Doersch et al. (2013) due to the very large number of parts they use.", "startOffset": 68, "endOffset": 90}, {"referenceID": 3, "context": "However, we could not directly compare with the best performance of Doersch et al. (2013) due to the very large number of parts they use. Figure 4 shows performance of CNN features on MIT-indoor dataset. As a baseline we extract CNN features from the entire image (after resizing to 256\u00d7256 pixels) and train a multi-class linear SVM. This obtains 72.3% average performance. This is a strong baseline. Razavian et al. (2014) get 58.", "startOffset": 68, "endOffset": 425}, {"referenceID": 3, "context": "However, we could not directly compare with the best performance of Doersch et al. (2013) due to the very large number of parts they use. Figure 4 shows performance of CNN features on MIT-indoor dataset. As a baseline we extract CNN features from the entire image (after resizing to 256\u00d7256 pixels) and train a multi-class linear SVM. This obtains 72.3% average performance. This is a strong baseline. Razavian et al. (2014) get 58.4% using CNN trained on ImageNet. They improve the result to 69% after data augmentation. We applied PCA on the 4096 dimensional features to make them more compact. This is essential for making the joint training tractable both in terms of running time and memory footprint. Figure 4-left shows the effect of PCA dimensionality reduction. It is surprising that we lose only 1% in accuracy with 160 PCA coefficients and only 3.5% with 60 PCA coefficients. We also show how performance changes when a random subset of dimensions is used. For joint training we use 60 PCA coefficients. Figure 4-right shows performance of our part-based models using CNN features. For comparison with HOG features we also plot result of Doersch et al. (2013). Note that part-based representation improves over the CNN extracted on the entire image.", "startOffset": 68, "endOffset": 1171}, {"referenceID": 20, "context": "HP denotes the hybrid features from Zhou et al. (2014). Left: the effect of dimensionality reduction on performance of the CNN features extracted from the entire image.", "startOffset": 36, "endOffset": 55}], "year": 2015, "abstractText": "Part-based representations have been shown to be very useful for image classification. Learning part-based models is often viewed as a two-stage problem. First, a collection of informative parts is discovered, using heuristics that promote part distinctiveness and diversity, and then classifiers are trained on the vector of part responses. In this paper we unify the two stages and learn the image classifiers and a set of shared parts jointly. We generate an initial pool of parts by randomly sampling part candidates and selecting a good subset using `1/`2 regularization. All steps are driven directly by the same objective namely the classification loss on a training set. This lets us do away with engineered heuristics. We also introduce the notion of negative parts, intended as parts that are negatively correlated with one or more classes. Negative parts are complementary to the parts discovered by other methods, which look only for positive correlations.", "creator": "LaTeX with hyperref package"}}}