{"id": "1412.6599", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Hot Swapping for Online Adaptation of Optimization Hyperparameters", "abstract": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.", "histories": [["v1", "Sat, 20 Dec 2014 04:36:28 GMT  (2681kb,D)", "https://arxiv.org/abs/1412.6599v1", "Submission to ICLR 2015"], ["v2", "Sat, 28 Feb 2015 05:18:18 GMT  (2682kb,D)", "http://arxiv.org/abs/1412.6599v2", "Submission to ICLR 2015"], ["v3", "Mon, 13 Apr 2015 23:28:01 GMT  (2682kb,D)", "http://arxiv.org/abs/1412.6599v3", "Submission to ICLR 2015"]], "COMMENTS": "Submission to ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kevin bache", "dennis decoste", "padhraic smyth"], "accepted": true, "id": "1412.6599"}, "pdf": {"name": "1412.6599.pdf", "metadata": {"source": "CRF", "title": "OPTIMIZATION HYPERPARAMETERS", "authors": ["Kevin M. Bache"], "emails": ["kbache@ics.uci.edu", "smyth@ics.uci.edu", "ddecoste@ebay.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In this paper, we present a new stochastic gradient method with adaptive learning rate selection based on the recognition that optimization hyperparameters can be freely \"swapped\" in the middle of the learning process. Where existing adaptive learning rate algorithms are based on ongoing curvature estimates of local loss area (Schaul et al., 2012; Zeiler, 2012), we present a method that transforms learning rate selection into an explorative-exploitative problem that can be addressed with existing solutions to multi-armed bandit problems. This method is easy to implement, retains the runtime characteristics and memory footprint of the stochastic gradient, and surpasses existing methods in a common benchmark task."}, {"heading": "2 ALGORITHM", "text": "The basis of the proposed algorithm selection or \"dynamic algorithm configuration problems\" (Gagliber and Schmidhuber, 2006) is the observation that optimization problems such as learning rate and impulse can be considered freely \"hot swapped\" during optimization runs, in contrast to modeling hyperparameters that set optimization hyperparameters in an outer loop and treat learning as an inner loop (Bergstra and Bengio, 2012; Snoek et al., 2012). Instead, we suggest observing the optimization process under a variety of hyperparameter settings and preferably continuing to use them. We do this1The notion of model-based hot swapping of algorithms or their parameters has previously been considered a study of \"dynamic algorithm selection\" or \"dynamic algorithm configuration problems.\""}, {"heading": "3 INITIAL RESULTS", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "4 CONCLUSION", "text": "Preliminary results indicate that the proposed method consistently outperforms competing methods on multiple metrics. Numerous enhancements to this basic method are possible, including the use of different meta-models, swapping new optimization or regulation parameters, swapping multiple parameters at once, and reducing the frequency of line searching for speed. We are currently working to test these and other hot-swapping methods on a variety of issues."}, {"heading": "ACKNOWLEDGMENTS", "text": "This material is based on work partially supported by a National Science Foundation Graduate Fellowship (KB), NSF under premium number IIS-1320527 (PS) and Office of Naval Research under MURI premium N00014-08-1-1015 (PS). Part of this work was done by KB during a summer internship at eBay Research Labs."}], "references": [{"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research, 13(1):281\u2013305.", "citeRegEx": "Bergstra and Bengio,? 2012", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Learning dynamic algorithm portfolios", "author": ["M. Gagliolo", "J. Schmidhuber"], "venue": "Annals of Mathematics and Artificial Intelligence, 47(3-4):295\u2013328.", "citeRegEx": "Gagliolo and Schmidhuber,? 2006", "shortCiteRegEx": "Gagliolo and Schmidhuber", "year": 2006}, {"title": "On upper-confidence bound policies for non-stationary bandit problems", "author": ["A. Garivier", "E. Moulines"], "venue": "arXiv:0805.3415 [math, stat]. arXiv: 0805.3415.", "citeRegEx": "Garivier and Moulines,? 2008", "shortCiteRegEx": "Garivier and Moulines", "year": 2008}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 265\u2013272.", "citeRegEx": "Ngiam et al\\.,? 2011", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F.R. Bach"], "venue": "Advances in Neural Information Processing Systems, pages 2663\u20132671. 00059.", "citeRegEx": "Roux et al\\.,? 2012", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "No more pesky learning rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "arXiv preprint arXiv:1206.1106.", "citeRegEx": "Schaul et al\\.,? 2012", "shortCiteRegEx": "Schaul et al\\.", "year": 2012}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q., editors, Advances in Neural Information Processing Systems 25, pages 2951\u20132959. Curran Associates, Inc.", "citeRegEx": "Snoek et al\\.,? 2012", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler,? 2012", "shortCiteRegEx": "Zeiler", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "Where existing adaptive learning rate algorithms are based on running curvature estimates of the local loss surface (Schaul et al., 2012; Zeiler, 2012), we present a procedure which recasts learning rate selection as an explore-exploit problem which can be addressed using existing solutions to multi-armed bandit problems.", "startOffset": 116, "endOffset": 151}, {"referenceID": 7, "context": "Where existing adaptive learning rate algorithms are based on running curvature estimates of the local loss surface (Schaul et al., 2012; Zeiler, 2012), we present a procedure which recasts learning rate selection as an explore-exploit problem which can be addressed using existing solutions to multi-armed bandit problems.", "startOffset": 116, "endOffset": 151}, {"referenceID": 0, "context": "This approach can also be contrasted to traditional hyperparameter search strategies such as grid search, random search, or Bayesian optimization which set optimization hyperparameters in an outer loop and treat learning as an inner loop (Bergstra and Bengio, 2012; Snoek et al., 2012).", "startOffset": 238, "endOffset": 285}, {"referenceID": 6, "context": "This approach can also be contrasted to traditional hyperparameter search strategies such as grid search, random search, or Bayesian optimization which set optimization hyperparameters in an outer loop and treat learning as an inner loop (Bergstra and Bengio, 2012; Snoek et al., 2012).", "startOffset": 238, "endOffset": 285}, {"referenceID": 1, "context": "We do this The notion of model-based hot swapping of algorithms or their parameters has been previously considered the study of the \u2018dynamic algorithm selection\u2019 or \u2018dynamic algorithm configuration\u2019 problems (Gagliolo and Schmidhuber, 2006), of which learning rate selection may be considered a specific instantiation.", "startOffset": 208, "endOffset": 240}, {"referenceID": 2, "context": "The upper confidence bound algorithm is a common choice for tackling exploreexploit problems, and its discounted form achieves the optimal regret bound up to a logaritmic factor for rapidly shifting reward distributions (Garivier and Moulines, 2008).", "startOffset": 220, "endOffset": 249}, {"referenceID": 3, "context": "Minibatch line search is not without precedent (Ngiam et al., 2011; Roux et al., 2012), though it has received little direct attention in the past This experiment was conducted using Theano, PyLearn2, and a cluster of computers with NVIDIA GRID K520 GPUs", "startOffset": 47, "endOffset": 86}, {"referenceID": 4, "context": "Minibatch line search is not without precedent (Ngiam et al., 2011; Roux et al., 2012), though it has received little direct attention in the past This experiment was conducted using Theano, PyLearn2, and a cluster of computers with NVIDIA GRID K520 GPUs", "startOffset": 47, "endOffset": 86}, {"referenceID": 7, "context": "We also compare against AdaDelta, another widely used adaptive learning rate algorithm, using the hyperparameters described in Zeiler (2012) for MNIST: = 10\u22126 and decay rate factor of 0.", "startOffset": 127, "endOffset": 141}], "year": 2015, "abstractText": "We describe a general framework for online adaptation of optimization hyperparameters by \u2018hot swapping\u2019 their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.", "creator": "LaTeX with hyperref package"}}}