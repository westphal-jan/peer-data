{"id": "1412.6618", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Permutohedral Lattice CNNs", "abstract": "This paper presents a convolutional layer that is able to process sparse input features. As an example, for image recognition problems this allows an efficient filtering of signals that do not lie on a dense grid (like pixel position), but of more general features (such as color values). The presented algorithm makes use of the permutohedral lattice data structure. The permutohedral lattice was introduced to efficiently implement a bilateral filter, a commonly used image processing operation. Its use allows for a generalization of the convolution type found in current (spatial) convolutional network architectures.", "histories": [["v1", "Sat, 20 Dec 2014 07:08:54 GMT  (29kb)", "https://arxiv.org/abs/1412.6618v1", null], ["v2", "Thu, 26 Feb 2015 14:16:58 GMT  (31kb)", "http://arxiv.org/abs/1412.6618v2", null], ["v3", "Sun, 3 May 2015 11:26:34 GMT  (32kb)", "http://arxiv.org/abs/1412.6618v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["martin kiefel", "varun jampani", "peter v gehler"], "accepted": true, "id": "1412.6618"}, "pdf": {"name": "1412.6618.pdf", "metadata": {"source": "CRF", "title": "PERMUTOHEDRAL LATTICE CNNS", "authors": ["Martin Kiefel", "Varun Jampani"], "emails": ["peter.gehler}@tuebingen.mpg.de"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.66 18v3 [cs.CV] 3 May 201 5"}, {"heading": "1 INTRODUCTION", "text": "In this paper, we take a more signal-theoretical standpoint of conventional operation and present an algorithm that allows us to process even sparse input data (Adams et al., 2010) Although the approach is more general, the following two scenarios are instructive (Aurich & Weule, 1995; Smith & Brady, 1997) and generalize it to the use of conventional architectures."}, {"heading": "2 PERMUTOHEDRAL LATTICE CONVOLUTION", "text": "We propose a folding operation of a d-dimensional input space based entirely on a grating. Input data are a tuple (fi, vi) of characteristics fi-Rd and corresponding signal values vi-R. Importantly, this does not assume that the characteristic types fi are queried on a regular grating, for example fi can be position and RGB value. We then map the input signal to a regular structure, the so-called permutodral grating. A folding then operates on the constructed grating and the result is mapped back to the output space, for example, the entire operation can consist of three stages (see Figure 1): The mapping on the grating space, the folding and the disc (the mapping back from the grating) This strategy has already been used to implement fast Gaussian filtering (Paris & Durand, 2009; Adams et al, 2010; 2009). Here we generalize it to arbitrary constellations."}, {"heading": "3 SPARSE CNNS AND ENCODING INVARIANTS", "text": "Permutohedral folding can be used as a new building block in a CNN architecture. We will omit the derivation of gradients for the filter elements in relation to the output of such a new layer due to space constraints. We will discuss two possible application scenarios. Firstly, as already mentioned, only two possible scenarios are available to change the sampling of the input signal of a grid-based folding. The sampling choice is problematic. Missing measurements or domain-specific sampling techniques that collect more information in highly discriminatory areas are only two possible scenarios. Furthermore, as we will show in our experiments, the method is robust in cases where the sampling of the train time and the sampling of the test time do not coincide. Secondly, the proposed method provides a tool for coding additional data invariances in a principled manner. A common technique for incorporating domain knowledge is complementing the transformations, such as complementing the educational system, such as the transformations, and letting the transformations with the transformations."}, {"heading": "4 EXPERIMENTS", "text": "We are investigating the performance and flexibility of the proposed method in two types of experiments: the first setup compares the permutohedral convolution with a spatial convolution combined with a bilinear interpolation; the second part adds a denoising experiment to the modeling strength of the permutohedral convolution.It is natural to ask how a bilinear interpolation combined with an interpolative convolution to a permutohedral neural network (PCNN) can be used. The proposed convolution is particularly advantageous in cases where samples are addressed in a higher dimensional space. Nevertheless, a bilinear interpolation prior to a spatial convolution can be used for a two-dimensional positioning. We are taking a reference implementation of LeNet (LeCun et al), which is part of the caffe project (Jia et al., 2014) on the MNIST dataset as the starting point for the following experiments."}, {"heading": "5 CONCLUSION", "text": "Consider signals that are naturally represented as measurements, rather than as images, such as MRI scans. Permutohedrale grid filtering avoids pre-processing the assembly into a dense image, it is possible to directly process the measured sparse signal. Another promising use of this filter is the coding of scale invariance, typically encoded by presenting multiple scaled versions of an image to several branches of a network. The folding presented here can be defined in the continuous range of image scales without endless sub-selection. In summary, this technique allows coding of foreknowledge of the observed signal to define the domain of convolution. The typical spatial filter of CNNs is a certain type of prior knowledge, we generalize this to save signals."}], "references": [{"title": "Gaussian kd-trees for fast high-dimensional filtering", "author": ["Adams", "Andrew", "Gelfand", "Natasha", "Dolson", "Jennifer", "Levoy", "Marc"], "venue": "In ACM SIGGRAPH 2009 Papers,", "citeRegEx": "Adams et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2009}, {"title": "Fast high-dimensional filtering using the permutohedral lattice", "author": ["Adams", "Andrew", "Baek", "Jongmin", "Davis", "Myers Abraham"], "venue": "Comput. Graph. Forum,", "citeRegEx": "Adams et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2010}, {"title": "Contour detection and hierarchical image segmentation", "author": ["Arbelez", "Pablo", "Maire", "Michael", "Fowlkes", "Charless", "Malik", "Jitendra"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Arbelez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arbelez et al\\.", "year": 2011}, {"title": "Non-linear Gaussian filters performing edge preserving diffusion", "author": ["Aurich", "Volker", "Weule", "J\u00f6rg"], "venue": "Mustererkennung", "citeRegEx": "Aurich et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Aurich et al\\.", "year": 1995}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Group invariant scattering", "author": ["Mallat", "St\u00e9phane"], "venue": "Communications in Pure and Applied Mathematics,", "citeRegEx": "Mallat and St\u00e9phane.,? \\Q2012\\E", "shortCiteRegEx": "Mallat and St\u00e9phane.", "year": 2012}, {"title": "A fast approximation of the bilateral filter using a signal processing approach", "author": ["Paris", "Sylvain", "Durand", "Fr\u00e9do"], "venue": "International Journal of Compututer Vision,", "citeRegEx": "Paris et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Paris et al\\.", "year": 2009}, {"title": "SUSAN \u2013 a new approach to low level image processing", "author": ["Smith", "Stephen M", "Brady", "J. Michael"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "Smith et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Smith et al\\.", "year": 1997}, {"title": "Bilateral filtering for gray and color images", "author": ["Tomasi", "Carlo", "Roberto", "Manduchi"], "venue": "In Proceedings of the Sixth International Conference on Computer Vision,", "citeRegEx": "Tomasi et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Tomasi et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 1, "context": "This work is inspired by the use of special data structures (Adams et al., 2010) for bilateral filters (Aurich & Weule, 1995; Smith & Brady, 1997; Tomasi & Roberto, 1998) and generalizes it for the use of convolutional architectures.", "startOffset": 60, "endOffset": 80}, {"referenceID": 1, "context": "This strategy has already been used to implement fast Gaussian filtering (Paris & Durand, 2009; Adams et al., 2010; 2009).", "startOffset": 73, "endOffset": 121}, {"referenceID": 5, "context": "We compare the LeNet (LeCun et al., 1998) implementation that is part of Caffe (Jia et al.", "startOffset": 21, "endOffset": 41}, {"referenceID": 4, "context": ", 1998) implementation that is part of Caffe (Jia et al., 2014) to the network with the first layer replaced by a permutohedral convolution layer (PCNN).", "startOffset": 45, "endOffset": 63}, {"referenceID": 2, "context": "(b) PSNR results of a denoising task using the BSDS500 dataset (Arbelez et al., 2011).", "startOffset": 63, "endOffset": 85}, {"referenceID": 5, "context": "We take a reference implementation of LeNet (LeCun et al., 1998) that is part of the caffe project (Jia et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 4, "context": ", 1998) that is part of the caffe project (Jia et al., 2014) on the MNIST dataset as a starting point for the following experiments.", "startOffset": 42, "endOffset": 60}, {"referenceID": 2, "context": "All experiments compare the performance of a PCNN to a common CNN with images from the BSDS500 dataset (Arbelez et al., 2011).", "startOffset": 103, "endOffset": 125}], "year": 2015, "abstractText": "This paper presents a convolutional layer that is able to process sparse input features. As an example, for image recognition problems this allows an efficient filtering of signals that do not lie on a dense grid (like pixel position), but of more general features (such as color values). The presented algorithm makes use of the permutohedral lattice data structure. The permutohedral lattice was introduced to efficiently implement a bilateral filter, a commonly used image processing operation. Its use allows for a generalization of the convolution type found in current (spatial) convolutional network architectures.", "creator": "LaTeX with hyperref package"}}}