{"id": "1412.6623", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Word Representations via Gaussian Embedding", "abstract": "Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.", "histories": [["v1", "Sat, 20 Dec 2014 07:42:40 GMT  (316kb,D)", "http://arxiv.org/abs/1412.6623v1", "12 pages, in submission to ICLR 2015"], ["v2", "Tue, 10 Mar 2015 14:24:28 GMT  (183kb,D)", "http://arxiv.org/abs/1412.6623v2", "12 pages, in submission to ICLR 2015"], ["v3", "Thu, 23 Apr 2015 18:19:11 GMT  (183kb,D)", "http://arxiv.org/abs/1412.6623v3", "12 pages, published as conference paper at ICLR 2015"], ["v4", "Fri, 1 May 2015 10:14:58 GMT  (184kb,D)", "http://arxiv.org/abs/1412.6623v4", "12 pages, published as conference paper at ICLR 2015"]], "COMMENTS": "12 pages, in submission to ICLR 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["luke vilnis", "rew mccallum"], "accepted": true, "id": "1412.6623"}, "pdf": {"name": "1412.6623.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["GAUSSIAN EMBEDDING", "Luke Vilnis", "Andrew McCallum"], "emails": ["luke@cs.umass.edu,", "mccallum@cs.umass.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, the number of unemployed minor infants inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior inferior."}, {"heading": "2 RELATED WORK", "text": "This work builds on a long series of papers on both distributed and distributional semantic word vectors, including distributive semantics, neural language models, numerical models-based language models, and, more generally, the field of representation formation. Related work in probabilistic matrix factorization (Mnih & Salakhutdinov, 2007) embeds rows and columns as Gaussians, and some forms of it provide each row and column with their own variance (Salakhutdinov & Mnih, 2008). Given the parallels between the embedding of models and matrix factorization (Deerwester et al, 1990; Riedel et al., 2014; Levy & Goldberg, 2014), this is relevant to our approach. However, these Bayesian methods apply Bayes to observed data to deduce latent distributions, while our model directly processes and discriminates in the space of probability distributions."}, {"heading": "3 BACKGROUND", "text": "In fact, it is as if most people are able to identify themselves, understand and understand how they have behaved. (...) In fact, it is as if they are able to identify themselves. (...) It is not as if they do. (...) It is as if they do. (...) It is as if they do. (...) It is as if they do. (...) It is as if they do not want to. (...) It is as if they do. (...) It is as if they do. (...) It is as if they do. (...) It is as if they do. (...) It is as if they do not want to. (...) It is as if they want to. (...) It is as if they want to. (...)"}, {"heading": "4 WARMUP: EMPIRICAL COVARIANCES", "text": "In view of a pre-trained series of word embeddings brought up from contexts, there is an easy way to construct variances using the empirical variance of the context vectors of a word type. is\u0448w = 1NW N \u2211 i W \u0445 j (c (w) ij \u2212 w) (c (w) ij \u2212 w) > For a word w with N word embeddings {c (w) i}, which represents the words found in its context and the window size W, the empirical variance is\u0448w = 1NW N \u2211 i W \u0445 j (c (w) ij \u2212 w) (c (w) ij \u2212 w) > This is an estimate of the covariance of a distribution assuming that the mean value is fixed at w. In practice, it is also necessary to add a small ridge term \u03b4 > 0 to the diagonal of the matrix in order to regulate and avoid numerical problems in the inversion."}, {"heading": "5 ENERGY-BASED LEARNING OF GAUSSIANS", "text": "As discussed in Section 3, our architecture learns Gaussian distribution embedding to predict words in context given the current word, and assigns these negatively sampled words. We present two energy functions to train these embedding."}, {"heading": "5.1 SYMMETRIC SIMILARITY: EXPECTED LIKELIHOOD OR PROBABILITY PRODUCT KERNEL", "text": "The logical next choice for a symmetric similarity function would be to take the inner product between the distributions themselves. Let's remember that for two (well-behaved) functions f, g and g.This idea seems very natural, and indeed the idea of mapping data in Kasesw into probability distributions (often via their contexts) and comparing them via integrals has resulted in a story under the name of the expected probability product (Jebara et al., 2004)."}, {"heading": "5.2 ASYMMETRIC SIMILARITY: KL DIVERGENCE", "text": "The formation of vectors by KL divergence to encode their context distributions or even to include more explicit guidelines is also a sensible objective choice. We optimize the following energy function (which for Gaussians has a similarly traceable solution in closed form): E (Pi, Pj) = DKL (Nj | | Ni) = x-x-x-x-x-x-x-i) Log N (x; \u00b5j) Log N (x; i-j) KL divergence is a natural energy function for displaying states between concepts - a low KL divergence from x to y indicates that we can easily generate them as x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-s if this divergence is a natural energy function for displaying states between concepts - a low KL divergence from x to y-x-x-x-x."}, {"heading": "5.3 UNCERTAINTY OF INNER PRODUCTS", "text": "Another advantage of embedding objects as probability distributions is that we can look at the distribution of point products between vectors derived from two Gaussian representations (Brown & Rutemiller, 1977).For the distribution P (z = x > y) we have a finite mean and a variance with a simple structure if the two Gaussian representations are assumed to be independent (Brown & Rutemiller, 1977).For the distribution P (z = x > y) we have a lower or upper limit on the point products of random samples from these distributions, which should hold a certain percentage of the time.If we parameterise this energy by a certain number of standard deviations c, we can also find a range for the Dot product as follows: \u00b5 > x \u00b5y \u00b1 c citib."}, {"heading": "5.4 LEARNING", "text": "To learn our model, we must select an energy function (EL or KL), a loss function (max margin), and a number of positive and negative training pairs. As the landscape is highly non-convex, it is also helpful to add a few regulations.We regulate the means and covariances differently, since they are different types of geometrical objects; the means should not become too large, so we can add a simple hard constraint to the '2 standard: the covariance matrices must be both positively defined and adequately dimensioned. This is achieved by adding a hard constraint that the eigenvalues are within the hypercube [m, M] d for constants m and M. mI."}, {"heading": "6 EVALUATION", "text": "We evaluate the presentation learning algorithms against several qualitative and quantitative tasks, including modeling asymmetric and linguistic relationships, uncertainty, and word similarity. All Gaussian experiments are performed with 50-dimensional vectors, with diagonal deviations, unless otherwise noted. Unsupervised embedding is learned at the chained ukWaC and WaCkypedia corpora (Baroni et al., 2009), which consists of approximately 3 billion tokens, corresponding to the experimental setup used by Baroni et al. (2012), except that the small British National Corpus is not publicly available and contains only 100 million tokens. Any word types occurring less than 100 times in the training set are dropped, creating a vocabulary of approximately 280 thousand word types. In the training word2vec Skip-Gram embedding for baseline, we follow the above training setup (50-dimensional insertion types), whereas we consider the difference between our own 2-dimensional modules for only."}, {"heading": "6.1 SPECIFICITY AND UNCERTAINTY OF EMBEDDINGS", "text": "In Figure 2, we examine some of the 100 closest neighbors of multiple query words as we sort from the largest to the smallest variance measured by the determinant of the covariance matrix using diagonal Gaussian embeddings. Note that more specific words, such as merriment and electrocollision, have less variance, while polysemic words, or those that denote more general concepts, have greater variances, such as mixture, mind, and graphics. This is not just an artifact of higher frequency of words that get more variance - when we sort by words that vary most in order and rank by variance, we see that genres with names like Chillout, Avant, and Shoegaze over-index their variance compared to their frequency, as they occur in different contexts. Similarly, common emotion words such as sadness and sincerity exhibit less variance than their frequency would predict because they have fixed meanings."}, {"heading": "6.2 ENTAILMENT", "text": "In fact, it is not that we are able to abide by the rules that we have imposed on ourselves. (...) It is not that we are able to abide by the rules. (...) It is not that we will abide by the rules. (...) It is not that we will abide by the rules. (...) It is not that we will abide by the rules. (...) It is not that we will abide by the rules. (...) It is not that we will abide by the rules. (...) It is not that we will abide by the rules. (...) It is not that we will abide by the rules. (...) It is not that we will abide by the rules. (...) It is not that we will abide by the rules. (...) It is not that we will abide by the rules."}, {"heading": "6.3 DIRECTLY LEARNING ASYMMETRIC RELATIONSHIPS", "text": "In Figure 4 we see the results of the direct embedding of simple tree hierarchies as Gaussian nodes. We embed nodes as Gaussian nodes with diagonal deviations into two-dimensional space by exploiting the KL divergence between parents and children by means of gradient descent. We create a Gaussian node for each node in the tree and randomly initialize means. Negative contexts come from randomly selected nodes that are neither ancestors nor descendants, while positive contexts originate from ancestors or descendants who use the corresponding directional KL divergence. Our training process captures the hierarchical relationships, although siblings at leaf level are not distinguished from each other by this objective function."}, {"heading": "6.4 WORD SIMILARITY BENCHMARKS", "text": "We evaluate the embedding in seven different standard word similarity benchmarks (Rubenstein & Goodenough, 1965; Szumlanski et al., 2013; Hill et al., 2014; Miller & Charles, 1991; Bruni et al., 2014; Yang & Powers, 2006; Finkelstein et al., 2001).A comparison with all current word embedding numbers for the different dimensions as in (Baroni et al., 2014) is outside the scope of this evaluation. However, we would point out that the overall performance of our 50-dimensional embedding matches or exceeds that of the numbers on these data sets for the 80-dimensional skip-gram vectors."}, {"heading": "7 CONCLUSION AND FUTURE WORK", "text": "In this work, we have introduced a method for embedding word types in the space of Gaussian distributions, and are learning the embedding directly in that space. This allows us to present words not as low-dimensional vectors, but as densities across a latent space that directly represent terms of uncertainty and allow for richer geometry in our embedded space. We hope to transfer the effectiveness of these embedding to a linguistic task that requires asymmetric comparisons, standard word similarity benchmarks, learning synthetic hierarchies, and multiple qualitative studies. In future work, we will move beyond spherical or diagonal codes and disregard combinations of low rankings and diagonal matrices. Efficient updates and scalable learning are still possible due to the Sherman-Woodbury-Morrison formula."}], "references": [{"title": "Theoretical foundations of the potential function method in pattern recognition learning", "author": ["M.A. Aizerman", "E.A. Braverman", "L. Rozonoer"], "venue": "In Automation and Remote Control,,", "citeRegEx": "Aizerman et al\\.,? \\Q1964\\E", "shortCiteRegEx": "Aizerman et al\\.", "year": 1964}, {"title": "The wacky wide web: a collection of very large linguistically processed web-crawled corpora", "author": ["Baroni", "Marco", "Bernardini", "Silvia", "Ferraresi", "Adriano", "Zanchetta", "Eros"], "venue": "Language resources and evaluation,", "citeRegEx": "Baroni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2009}, {"title": "Entailment above the word level in distributional semantics", "author": ["Baroni", "Marco", "Bernardi", "Raffaella", "Do", "Ngoc-Quynh", "Shan", "Chung-chieh"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni", "Marco", "Dinu", "Georgiana", "Kruszewski", "Germ\u00e1n"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Bengio", "Yoshua", "Schwenk", "Holger", "Sen\u00e9cal", "Jean-S\u00e9bastien", "Morin", "Fr\u00e9deric", "Gauvain", "JeanLuc"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Means and variances of stochastic vector products with applications to random linear models", "author": ["Brown", "Gerald G", "Rutemiller", "Herbert C"], "venue": "Management Science,", "citeRegEx": "Brown et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1977}, {"title": "Textual similarity with a bag-of-embedded-words model", "author": ["Clinchant", "St\u00e9phane", "Perronnin", "Florent"], "venue": "In Proceedings of the 2013 Conference on the Theory of Information Retrieval,", "citeRegEx": "Clinchant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Clinchant et al\\.", "year": 2013}, {"title": "Aggregating continuous word embeddings for information retrieval", "author": ["Clinchant", "St\u00e9phane", "Perronnin", "Florent"], "venue": "ACL", "citeRegEx": "Clinchant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Clinchant et al\\.", "year": 2013}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R.A. Harshman"], "venue": "Journal of the American Society for Information Science", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Representing words as regions in vector space", "author": ["Erk", "Katrin"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Erk and Katrin.,? \\Q2009\\E", "shortCiteRegEx": "Erk and Katrin.", "year": 2009}, {"title": "Community evaluation and exchange of word vectors at wordvectors.org", "author": ["Faruqui", "Manaal", "Dyer", "Chris"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Finkelstein", "Lev", "Gabrilovich", "Evgeniy", "Matias", "Yossi", "Rivlin", "Ehud", "Solan", "Zach", "Wolfman", "Gadi", "Ruppin", "Eytan"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["Geffet", "Maayan", "Dagan", "Ido"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Geffet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Geffet et al\\.", "year": 2005}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill", "Felix", "Reichart", "Roi", "Korhonen", "Anna"], "venue": "arXiv preprint arXiv:1408.3456,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Probability product kernels", "author": ["Jebara", "Tony", "Kondor", "Risi", "Howard", "Andrew"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Jebara et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jebara et al\\.", "year": 2004}, {"title": "Optimizing search engines using clickthrough data", "author": ["Joachims", "Thorsten"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Joachims and Thorsten.,? \\Q2002\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 2002}, {"title": "Fundamentals of Statistical Signal Processing, Volume III: Practical Algorithm Development", "author": ["S.M. Kay"], "venue": "Fundamentals of Statistical Signal Processing. Prentice-Hall PTR,", "citeRegEx": "Kay,? \\Q2013\\E", "shortCiteRegEx": "Kay", "year": 2013}, {"title": "A multiplicative model for learning distributed text-based attribute representations", "author": ["Kiros", "Ryan", "Zemel", "Richard", "Salakhutdinov", "Ruslan R"], "venue": "In NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Distribution representation of the meaning of words and phrases by a gaussian distribution", "author": ["Kiyoshiyo", "Shimaoka", "Masayasu", "Muraoka", "Futo", "Yamamoto", "Watanabe", "Yotaro", "Okazaki", "Naoaki", "Inui", "Kentaro"], "venue": "In Language Processing Society 20th Annual Conference (In Japanese),", "citeRegEx": "Kiyoshiyo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiyoshiyo et al\\.", "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Lanckriet", "Gert RG", "Cristianini", "Nello", "Bartlett", "Peter", "Ghaoui", "Laurent El", "Jordan", "Michael I"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Omer", "Goldberg", "Yoav"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["Miller", "George A", "Charles", "Walter G"], "venue": "Language and cognitive processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2008}, {"title": "Probabilistic matrix factorization", "author": ["Mnih", "Andriy", "Salakhutdinov", "Ruslan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "The matrix cookbook", "author": ["Petersen", "Kaare Brandt"], "venue": null, "citeRegEx": "Petersen and Brandt.,? \\Q2006\\E", "shortCiteRegEx": "Petersen and Brandt.", "year": 2006}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Riedel", "Sebastian", "Yao", "Limin", "McCallum", "Andrew", "Marlin", "Benjamin M"], "venue": null, "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "Herbert", "Goodenough", "John B"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hintont", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "A new set of norms for semantic relatedness measures", "author": ["Szumlanski", "Sean R", "Gomez", "Fernando", "Sims", "Valerie K"], "venue": "In ACL,", "citeRegEx": "Szumlanski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szumlanski et al\\.", "year": 2013}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston", "Jason", "Bengio", "Samy", "Usunier", "Nicolas"], "venue": "In IJCAI,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Xing", "Eric P", "Jordan", "Michael I", "Russell", "Stuart", "Ng", "Andrew Y"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Verb similarity on the taxonomy of wordnet", "author": ["Yang", "Dongqiang", "Powers", "David M. W"], "venue": "In In the 3rd International WordNet Conference (GWC-06), Jeju Island, Korea,", "citeRegEx": "Yang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 20, "context": "In recent years there has been a surge of interest in learning compact distributed representations or embeddings for many machine learning tasks, including collaborative filtering (Koren et al., 2009), image retrieval (Weston et al.", "startOffset": 180, "endOffset": 200}, {"referenceID": 33, "context": ", 2009), image retrieval (Weston et al., 2011), relation extraction (Riedel et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 28, "context": ", 2011), relation extraction (Riedel et al., 2013), word semantics and language modeling (Bengio et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 4, "context": ", 2013), word semantics and language modeling (Bengio et al., 2006; Mnih & Hinton, 2008; Mikolov et al., 2013), and many others.", "startOffset": 46, "endOffset": 110}, {"referenceID": 23, "context": ", 2013), word semantics and language modeling (Bengio et al., 2006; Mnih & Hinton, 2008; Mikolov et al., 2013), and many others.", "startOffset": 46, "endOffset": 110}, {"referenceID": 0, "context": "This paper advocates moving beyond vector point representations to potential functions (Aizerman et al., 1964), or continuous densities in latent space.", "startOffset": 87, "endOffset": 110}, {"referenceID": 8, "context": "Given the parallels between embedding models and matrix factorization (Deerwester et al., 1990; Riedel et al., 2013; Levy & Goldberg, 2014), this is relevant to our approach.", "startOffset": 70, "endOffset": 139}, {"referenceID": 28, "context": "Given the parallels between embedding models and matrix factorization (Deerwester et al., 1990; Riedel et al., 2013; Levy & Goldberg, 2014), this is relevant to our approach.", "startOffset": 70, "endOffset": 139}, {"referenceID": 21, "context": "This allows us to go beyond the Bayesian approach and use arbitrary (and even asymmetric) training criteria, and is more similar to methods that learn kernels (Lanckriet et al., 2004) or function-valued neural networks such as mixture density networks (Bishop, 1994).", "startOffset": 159, "endOffset": 183}, {"referenceID": 18, "context": "Other work in multiplicative tensor factorization for word embeddings (Kiros et al., 2014) and metric learning (Xing et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 34, "context": ", 2014) and metric learning (Xing et al., 2002) learns some combinations of representations, clusters, and a distance metric jointly; however, it does not effectively learn a distance function per item.", "startOffset": 28, "endOffset": 47}, {"referenceID": 8, "context": "Given the parallels between embedding models and matrix factorization (Deerwester et al., 1990; Riedel et al., 2013; Levy & Goldberg, 2014), this is relevant to our approach. However, these Bayesian methods apply Bayes\u2019 rule to observed data to infer the latent distributions, whereas our model works directly in the space of probability distributions and discriminatively trains them. This allows us to go beyond the Bayesian approach and use arbitrary (and even asymmetric) training criteria, and is more similar to methods that learn kernels (Lanckriet et al., 2004) or function-valued neural networks such as mixture density networks (Bishop, 1994). Other work in multiplicative tensor factorization for word embeddings (Kiros et al., 2014) and metric learning (Xing et al., 2002) learns some combinations of representations, clusters, and a distance metric jointly; however, it does not effectively learn a distance function per item. Fitting Gaussian mixture models on embeddings has been done in order to apply Fisher kernels to entire documents (Clinchant & Perronnin, 2013b;a). Preliminary concurrent work from Kiyoshiyo et al. (2014) describes a significantly different model similar to Bayesian matrix factorization, using a probabilistic Gaussian graphical model to define a distribution over pairs of words, and they lack quantitative experiments or evaluation.", "startOffset": 71, "endOffset": 1144}, {"referenceID": 23, "context": "In the word2vec Skip-Gram (Mikolov et al., 2013) word embedding model, the energy function takes the form of a dot product between the vectors of an observed word and an observed contextw>c.", "startOffset": 26, "endOffset": 48}, {"referenceID": 30, "context": "Backpropagating (Rumelhart et al., 1986) this loss to the word vectors trains them to be predictive of their contexts, achieving the desired effect (words in similar contexts have similar vectors).", "startOffset": 16, "endOffset": 40}, {"referenceID": 33, "context": "We chose a max-margin ranking objective, similar to that used in Rank-SVM (Joachims, 2002) or Wsabie (Weston et al., 2011), which pushes scores of positive pairs above negatives by a margin: Lm(w, cp, cn) = max(0,m\u2212 E(w, cp) + E(w, cn)) In this terminology, the contribution of our work is a pair of energy functions for training Gaussian distributions to represent word types.", "startOffset": 101, "endOffset": 122}, {"referenceID": 15, "context": "This idea seems very natural, and indeed has appeared before - the idea of mapping data casesw into probability distributions (often over their contexts), and comparing them via integrals has a history under the name of the expected likelihood or probability product kernel (Jebara et al., 2004).", "startOffset": 274, "endOffset": 295}, {"referenceID": 17, "context": "This is a consequence of the broader fact that the convolution of two Gaussian random variables is another Gaussian, and is sometimes called the reproducing property (Kay, 2013).", "startOffset": 166, "endOffset": 177}, {"referenceID": 9, "context": "We optimize the parameters using AdaGrad (Duchi et al., 2011) and stochastic gradients in small minibatches containing 20 sentences worth of tokens and contexts.", "startOffset": 41, "endOffset": 61}, {"referenceID": 1, "context": "Unsupervised embeddings are learned on the concatenated ukWaC and WaCkypedia corpora (Baroni et al., 2009), consisting of about 3 billion tokens.", "startOffset": 85, "endOffset": 106}, {"referenceID": 23, "context": "We train both models with one pass over the data, using separate embeddings for the input and output contexts, 1 negative sample per positive example, and the same subsampling procedure as in the word2vec paper (Mikolov et al., 2013).", "startOffset": 211, "endOffset": 233}, {"referenceID": 1, "context": "Unsupervised embeddings are learned on the concatenated ukWaC and WaCkypedia corpora (Baroni et al., 2009), consisting of about 3 billion tokens. This matches the experimental setup used by Baroni et al. (2012), aside from leaving out the small British National Corpus, which is not publicly available and contains only 100 million tokens.", "startOffset": 86, "endOffset": 211}, {"referenceID": 1, "context": "Model Test Similarity Best F1 AP Baroni et al. (2012) E balAPinc 75.", "startOffset": 33, "endOffset": 54}, {"referenceID": 1, "context": "E is the dataset of Baroni et al. (2012). Measures of similarity are symmetric (cosine between means) and asymmetric (KL) divergence for Gaussians.", "startOffset": 20, "endOffset": 41}, {"referenceID": 1, "context": "We evaluate quantitatively on the Entailment dataset of Baroni et al. (2012). Our setup is essentially the same as theirs but uses slightly less data, as mentioned in the beginning of this section.", "startOffset": 56, "endOffset": 77}, {"referenceID": 1, "context": "We evaluate quantitatively on the Entailment dataset of Baroni et al. (2012). Our setup is essentially the same as theirs but uses slightly less data, as mentioned in the beginning of this section. We evaluate with Average Precision and best F1 score. We include the best F1 score (by picking the optimal threshold at test) because this is used by Baroni et al. (2012), but we believe AP is better to demonstrate the correlation of various asymmetric and symmetric measures with the entailment data.", "startOffset": 56, "endOffset": 369}, {"referenceID": 32, "context": "We evaluate the embeddings on seven different standard word similarity benchmarks (Rubenstein & Goodenough, 1965; Szumlanski et al., 2013; Hill et al., 2014; Miller & Charles, 1991; Bruni et al., 2014; Yang & Powers, 2006; Finkelstein et al., 2001).", "startOffset": 82, "endOffset": 248}, {"referenceID": 14, "context": "We evaluate the embeddings on seven different standard word similarity benchmarks (Rubenstein & Goodenough, 1965; Szumlanski et al., 2013; Hill et al., 2014; Miller & Charles, 1991; Bruni et al., 2014; Yang & Powers, 2006; Finkelstein et al., 2001).", "startOffset": 82, "endOffset": 248}, {"referenceID": 12, "context": "We evaluate the embeddings on seven different standard word similarity benchmarks (Rubenstein & Goodenough, 1965; Szumlanski et al., 2013; Hill et al., 2014; Miller & Charles, 1991; Bruni et al., 2014; Yang & Powers, 2006; Finkelstein et al., 2001).", "startOffset": 82, "endOffset": 248}, {"referenceID": 3, "context": "A comparison to all of the state of the art word-embedding numbers for different dimensionalities as in (Baroni et al., 2014) is out of the scope of this evaluation.", "startOffset": 104, "endOffset": 125}, {"referenceID": 28, "context": "In other domains, we want to extend the use of potential function representations to other tasks requiring embeddings, such as relational learning with the universal schema (Riedel et al., 2013).", "startOffset": 173, "endOffset": 194}], "year": 2017, "abstractText": "Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.", "creator": "LaTeX with hyperref package"}}}