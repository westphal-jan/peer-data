{"id": "1412.6632", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.", "histories": [["v1", "Sat, 20 Dec 2014 08:10:04 GMT  (404kb,D)", "https://arxiv.org/abs/1412.6632v1", "arXiv admin note: substantial text overlap witharXiv:1410.1090"], ["v2", "Fri, 26 Dec 2014 08:24:11 GMT  (485kb,D)", "http://arxiv.org/abs/1412.6632v2", "Add performance comparisons with different variants of m-RNN model to show the importance of different components. arXiv admin note: substantial text overlap witharXiv:1410.1090"], ["v3", "Tue, 10 Mar 2015 04:17:48 GMT  (486kb,D)", "http://arxiv.org/abs/1412.6632v3", "Revision according to the comments and suggestions from ICLR 2015 reviewers (e.g. Add some details for section 5, 6; Fix typos). Add performance comparisons with different variants of m-RNN model to show the importance of different components. (Comments from last version)"], ["v4", "Fri, 10 Apr 2015 21:03:35 GMT  (486kb,D)", "http://arxiv.org/abs/1412.6632v4", "Add results on MS COCO test set; Compare results using beam search and using greedy word selection on the test set; Camera ready version for ICLR 2015. arXiv admin note: substantial text overlap witharXiv:1410.1090"], ["v5", "Thu, 11 Jun 2015 15:26:58 GMT  (2213kb,D)", "http://arxiv.org/abs/1412.6632v5", "Add a simple strategy to boost the performance of image captioning task significantly. More details are shown in Section 8 of the paper. The code and related data are available atthis https URL;. arXiv admin note: substantial text overlap witharXiv:1410.1090"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1410.1090", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["junhua mao", "wei xu", "yi yang", "jiang wang", "zhiheng huang", "alan yuille"], "accepted": true, "id": "1412.6632"}, "pdf": {"name": "1412.6632.pdf", "metadata": {"source": "CRF", "title": "DEEP CAPTIONING WITH MULTIMODAL RECURRENT NEURAL NETWORKS (M-RNN)", "authors": ["Junhua Mao", "Zhiheng Huang"], "emails": ["mjhustc@ucla.edu", "wei.xu@baidu.com", "yangyi05@baidu.com", "wangjiang03@baidu.com", "huangzhiheng@baidu.com", "yuille@stat.ucla.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Obtaining sentence descriptions for images becomes an important task, and it has many applications, such as early childhood education, image acquisition and navigation for the blind. Thanks to the rapid development of computer vision and natural language processing technologies, the recent work has made significant progress in this task (see a brief review in Section 2). Many earlier methods treat it as a retrieval task. They learn a common embedding to map the properties of the two sentences and images into the same semantic space. These methods generate captions by retrieving them from a sentence database. Therefore, they lack the ability to generate novel sentences or describe images that contain novel combinations of objects and scenarios. In this work, we propose a multi-modal recurrent neural network (m-RNN) Model 2 to address both the task of generating novel sentences and the task of describing images."}, {"heading": "2 RELATED WORK", "text": "The methods are based on the deep neural network that has developed rapidly in recent years, both in computer vision and in natural language. For computer vision, Krizhevsky et al. (2012) allows for a deep conventional 8-layer neural network structure (CNN) (referred to as AlexNet), surpassing previous methods by a wide margin in the task of image classification of ImageNet. (2014) This network structure is widely used in computer vision, e.g. Girshick et al. (2014) Design of an object recognition framework (RCNN) based on this work. Recently, Simonyan & Zisserman and Zisserman have a CNN with more than 16 layers than VggNet and perform much better than AlexNet. For natural language, the Recurrent Neural Network (RNN) shows state-of-art art art performance in many tasks, such as speech recognition and word processing (Mikolov; 2010)."}, {"heading": "3 MODEL ARCHITECTURE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 SIMPLE RECURRENT NEURAL NETWORK", "text": "We briefly introduce the simple Recurrent Neural Network (RNN) or Elman Network (Elman (1990). Its architecture is shown in Figure 2 (a). It has three types of levels in each timeframe: the input word layer w, the recurring layer r, and the output layer y. activating input, repeat, and output layers at a time t is referred to as w (t), r (t), and y (t), or as a word size with only one non-zero element. Mikolov et al. (2010). y (t) can be described as a simple 1-of-N encoding h (t) (i.e. the most uniform representation that is binary and has the same dimension as the word size with only one non-zero element."}, {"heading": "3.2 OUR M-RNN MODEL", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4 TRAINING THE M-RNN", "text": "To train our m-RNN model, we use a log likelihood cost function. It is related to the perplexity of the sentences in the training set based on their corresponding images. Perplexity is a standard measure for evaluating the language model. The perplexity for a word sequence (i.e. a sentence) w1: L is calculated as follows: log2 PPL (w1: L | I) = \u2212 1L L L L \u2211 n = 1 log2 P (wn | w1: n \u2212 1, I) (5), where L is the length of the word sequence, PPL (w1: L | I) represents the perplexity of the sentence w1: L in view of the image I. P (wn | w1: n \u2212 1, I) is the probability of generating the word wn given I and previous words w1: n \u2212 1. It corresponds to the activation of the SoftMax layer of our model. The cost function of our model is the average probability that the words given wn and given I wwn (1) are given \u2212 n."}, {"heading": "5 SENTENCE GENERATION, IMAGE RETRIEVAL AND SENTENCE RETRIEVAL", "text": "We use the trained m-RNN model for three tasks: 1) Sentence generation, 2) Sentence generation process is simple (retrieving the most relevant images for the given sentence), 3) Sentence capture (retrieving the most relevant sentences for the given image).The process of sentence generation is simple (retrieving the starting character wstart or the arbitrary number of reference words (e.g. we can enter the first K-words in the reference sentence to the model and then start generating new words), our model can calculate the probability distribution of the next word: P (wn | w1: n \u2212 1, I). Then we can select the next K-word from this probability distribution and then determine that the selection of the word with the maximum probability is slightly better than the sampling. After that, we enter the selected word into the model and continue the process until the end.For the query tasks, we use our model to calculate the probability."}, {"heading": "6 LEARNING OF SENTENCE AND IMAGE FEATURES", "text": "The architecture of our model enables us to propagate the gradients of the loss function back to both the part of the speech modeling (i.e. the word that embeds layers and the recurring layer) and the part of the vision (e.g. AlexNet or VggNet). For the linguistic part, as mentioned above, we randomly initialize the speech modeling levels and learn their parameters. For the visual part, we use the pre-trained AlexNet (Krizhevsky et al. (2012) or the VggNet (Simonyan & Zisserman (2014)) on ImageNet data sets (Russakovsky et al. (2014). Recently, Karpathy et al. (2014) show that the results of the object recognition of RCNN (Girshick et al al al al al. (2014)) combined with the AlexNet functions work better than the image as a whole. In the experiments, we show that our method works much better than Karpathy et al (2014)."}, {"heading": "7 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 DATASETS", "text": "We test our method using four benchmark datasets with annotations at the set level: IAPR TC-12 (Grubinger et al. (2006), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)), and MS COCO (Lin et al. (2014).IAPR TC-12. This dataset consists of approximately 20,000 images taken from different locations around the world. It contains images of different sports and actions, people, animals, cities, landscapes, etc. For each image, it provides at least one sentence comment. On average, there are about 1.7 sentence comments for one image. We adopt the standard separation of training and test components from previous work (Guillaumin et al. (2010); Kiros et al. (2014b)) with 17,665 images for training and 1962 date comments for testing. Flickr8K. This dataset consists of 8,000 images extracted from patch."}, {"heading": "7.2 EVALUATION METRICS", "text": "Sentence Generation. According to previous work, we use the phrases perplexity (see Equ. 5) and BLEU values (i.e. B-1, B-2, B-3 and B-4) (Papineni et al. (2002)) as yardsticks. BLEU values were originally designed for automatic machine translations, where they evaluate the quality of a translated sentence based on multiple reference sentences. Likewise, we can treat the sentence generation task as a \"translation\" of the content of images into sentences. BLEU remains the standard yardstick for sentence generation methods for images, although it has drawbacks. For some images, the reference sentences may not contain all possible descriptions in the image, and BLEU may punish some correctly generated sentences. Please see more details of the calculation of the BLEU values for this task in the supplementary material evaluation, with mentality retrieval and image retrieval being the highest rating."}, {"heading": "7.3 RESULTS ON IAPR TC-12", "text": "The results of task6 sentence generation are in Table 1. Our RNN base serves as the base method for our m-RNN model, which has the same architecture as m-RNN except that it does not have the image representation. To make a fair comparison, we follow the same experimental settings of Kiros et al. (2014b) to calculate the BLEU values and the perplexity. These two evaluation metrics are not necessarily correlated with each other for the following reasons. As mentioned in Section 4, perplexity is calculated based on the conditional probability of the word in a sentence that contains all of its previous reference words. Therefore, a strong language model that successfully captures the distributions of words in sentences may have low perplexity without the image content, but the content of the generated sentences is maximized by maximizing the images."}, {"heading": "7.4 RESULTS ON FLICKR8K", "text": "This dataset has been widely used as a benchmark dataset for retrieving images and sets. R @ K and Med r of different methods are presented in Table 3. We compare our model with several state-of-the-art methods: SDT-RNN (Socher et al. (2014), DeViSE (Frome et al. (2013), DeepFE (Karpathy et al. (2014))) with different images. Our model significantly exceeds these methods when using the same image representation (e.g. AlexNet). We also list the performance of methods that use more complex features in Table 3. \"-avg-RCNN\" refers to methods with features of average CNN activation of all objects above a detection trust threshold. DeepFE-RCNN Karpathy et al. (2014) uses a fragment mapping strategy to better exploit the results of object detection. Results show that using these features improves performance."}, {"heading": "MS COCO", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.5 RESULTS ON FLICKR30K AND MS COCO", "text": "We compare our method with several very recent methods in these two recently published datasets. (Note that the last six methods appear very new, we use the results in their papers): DeViSE (Frome et al. (2013), DeepFE (Karpathy et al. (2014), MNLM (Kiros et al. (2014), DMSM (Fang et al. (2014))), NIC (Vinyals et al. (2014), LRCN (Donahue et al. (2014), RVR (Chen & Zitnick), and DeepVS (Karpathy et-Fei). Results of retrival tasks and sentence generation task 7 are shown in Table 4 and Table 5."}, {"heading": "8 NEAREST NEIGHBOR AS REFERENCE", "text": "Recently, Devlin et al. (2015b) proposed an approach that retrieves the captions of the nearest k images in the training set, arranges these captions according to the consensus of the caption w.r.t. with the other captions, and returns the best placed ones. Inspired by this method, we first adopt the m-RNN model with the transposed weight distribution strategy (Mao et al. (2015), which is called the m-RNN split) to generate n hypotheses using a beam search scheme. Specifically, we keep the n best candidates in the sentence generation process until the model generates the end-point turn. These n best candidates are approximately the n most likely sentences generated by the model, and can be treated as n hypotheses. In our experiments, we use n = 10 because it gives us a diversified set of hypotheses without too many outliers on our validation group. These n are approximately the n most likely sentences generated by the model, and can be treated as n hypotheses. In our experiments, we use n = 10 because it gives us a diversified set of hypotheses without too many outliers on our validation group."}, {"heading": "8.1 IMAGE FEATURES FOR THE NEAREST NEIGHBOR IMAGE SEARCH", "text": "We try two types of image characteristics for the closest image search 9. The first is the original image characteristics extracted from VggNet (Simonyan & Zisserman (2014). First, we shrink the image so that its short page is 256 pixels. Then, we extract the characteristics to ten 224 x 224 Windows8 If we output the top hypotheses generated by the model directly, n = 5 gives us the best performance. But if we want to recalculate the hypotheses, n = 10 gives us a better result at the validation stage. 9We publish both types of features on MS COCO 2014 Train, VAL and test kits. Please refer to the readme file at http: / / github.com / mjhucla / mRNN-CR to see how to download and use them (the four corners, the middle and their mirrored versions) on the modified image."}, {"heading": "8.2 CONSENSUS RERANKING", "text": "Suppose we have the nearest neighbor pictures in the training set as a reference. We follow Devlin et al. (2015a) to calculate the consensus value of a hypothesis, the difference being that Devlin et al. (2015a) treat the captions of the nearest neighbor pictures as hypotheses, while our hypotheses are generated by the m-RNN model. Specifically, for each hypothesis we calculate the mean similarity between this hypothesis and all captions of the nearest neighbor pictures. The consensus value of this hypothesis is the mean similarity value of the m-next captions. The similarity between a hypothesis and one of its nearest neighbor reference pictures is defined by a set-level BLEU score (Papineni et al. (2002) or a set-level CIDEr (Vedantam et al. (2014))."}, {"heading": "8.3 EXPERIMENTS", "text": "We show the results of our model on our validation set and the MS COCO test server in Table 8. For BLEU-based consensus ranking, we get an improvement of 3.5 points over our validation set and 3.3 points over the MS COCO test 2014 in terms of BLEU4 score. For CIDER-based consensus ranking, we get an improvement of 9.4 points over our validation set and 9.8 points over the MS COCO test 2014 in terms of CIDER."}, {"heading": "8.4 DISCUSSION", "text": "We show the rank of the ten hypotheses before and after the re-ranking in Figure 4. Although the hypotheses are similar, there are some discrepancies between them (e.g. some of them capture more details of the images, some of which may be partially incorrect).The re-ranking process can improve the rank of good captions. We also show the oracle performance of the ten hypotheses, which is the upper limit of the re-ranking consensus. More specifically, for each image in our validation theorem we have recalculated the hypotheses according to the values (BLEU or CIDER) w.r.t for the basic captions.The results of this re-ranking oracle are shown in Table 8 (see lines with \"-oracle\").The oracle performance is surprisingly high, indicating that there is room for improvement both for the m-RNN model itself and for the re-ranking strategy."}, {"heading": "9 CONCLUSION", "text": "We propose a state-of-the-art multimodal Recurrent Neural Network (m-RNN) framework that works in three areas: record generation, record retrieval on a given query image, and image repetition on a given query set. The model consists of a deep RNN, a deep CNN, and these two subnetworks that interact on a multimodal level. Our m-RNN is powerful at combining images and sets, and is flexible at integrating more complex image representations and more complex language models."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Andrew Ng, Kai Yu, Chang Huang, Duohao Qin, Haoyuan Gao, Jason Eisner for useful discussions and technical support. We also thank the comments and suggestions of anonymous reviewers of the ICLR 2015 and the NIPS 2014 Deep Learning Workshop. We acknowledge the Center for Minds, Brains and Machines (CBMM), which was partially funded by the NSF STC-Award CCF-1231216 and ARO 62250-CS."}, {"heading": "10.1 EFFECTIVENESS OF THE DIFFERENT COMPONENTS OF THE M-RNN MODEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 SUPPLEMENTARY MATERIAL", "text": "This year, the number of unemployed has tripled compared to the previous year, and the number of unemployed has increased by 2.1 percent compared to the previous year."}, {"heading": "10.2 ADDITIONAL RETRIEVAL PERFORMANCE COMPARISONS ON IAPR TC-12", "text": "For the retrieval results in this dataset, in addition to the R @ K and Med r, we use exactly the same evaluation metrics as Kiros et al. (2014b) and plot the mean number of matches of the retrieved basic truths or images in terms of the percentage of retrieved sentences or images for the test set. For the retrieval task, Kiros et al. (2014b) use a shortlist of 100 images that are the closest neighbors of the retrieval image in the feature space. This shortlist makes the task more difficult because similar images may have similar descriptions and it is often more difficult to find subtle differences between sentences and select the most appropriate one. The retrieval accuracy curves relating to the percentage of retrieved images (sentence retrieval task) or sentences (sentence retrieval task) are shown in Figure 6."}, {"heading": "10.3 THE CALCULATION OF BLEU SCORE", "text": "The BLEU score was proposed by Papineni et al. (2002) and originally used as a benchmark for machine translation. To calculate the BLEU-N score (i.e. B-N in the work, in the N = 1,2,3,4), we first calculate the modified n-gram precision (Papineni et al. (2002), pn. Then we calculate the geometric mean from pn to length N and multiply it by an abbreviation penalty BP: BP = min (1, e1 \u2212 r c) (8) B-N = BP \u00b7 e 1N \u2211 Nn = 1 log pn (9), where r is the length of the reference set and c is the length of the generated sentence. We use the same strategy as Fang et al. (2014), where pn, r and c are calculated over the entire test corpus. For multiple reference sets, the length of the reference set closest to the length of the candidate is used (shorter or shorter)."}], "references": [{"title": "Microsoft coco captions: Data collection and evaluation", "author": ["X. Chen", "H. Fang", "TY Lin", "R. Vedantam", "S. Gupta", "P. Dollr", "C.L. Zitnick"], "venue": "server. arXiv preprint arXiv:1504.00325,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["Chen", "Xinlei", "Zitnick", "C Lawrence"], "venue": "arXiv preprint arXiv:1411.5654,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Devlin", "Jacob", "Cheng", "Hao", "Fang", "Gupta", "Saurabh", "Deng", "Li", "He", "Xiaodong", "Zweig", "Geoffrey", "Mitchell", "Margaret"], "venue": "arXiv preprint arXiv:1505.01809,", "citeRegEx": "Devlin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["Devlin", "Jacob", "Gupta", "Saurabh", "Girshick", "Ross", "Mitchell", "Margaret", "Zitnick", "C Lawrence"], "venue": "arXiv preprint arXiv:1505.04467,", "citeRegEx": "Devlin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue", "Jeff", "Hendricks", "Lisa Anne", "Guadarrama", "Sergio", "Rohrbach", "Marcus", "Venugopalan", "Subhashini", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "From captions to visual concepts and back", "author": ["Fang", "Hao", "Gupta", "Saurabh", "Iandola", "Forrest", "Srivastava", "Rupesh", "Deng", "Li", "Doll\u00e1r", "Piotr", "Gao", "Jianfeng", "He", "Xiaodong", "Mitchell", "Margaret", "Platt", "John"], "venue": "arXiv preprint arXiv:1411.4952,", "citeRegEx": "Fang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Farhadi", "Ali", "Hejrati", "Mohsen", "Sadeghi", "Mohammad Amin", "Young", "Peter", "Rashtchian", "Cyrus", "Hockenmaier", "Julia", "Forsyth", "David"], "venue": "In ECCV, pp", "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Mikolov", "Tomas"], "venue": "In NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "The iapr tc-12 benchmark: A new evaluation resource for visual information systems", "author": ["Grubinger", "Michael", "Clough", "Paul", "M\u00fcller", "Henning", "Deselaers", "Thomas"], "venue": "In International Workshop OntoImage,", "citeRegEx": "Grubinger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Multiple instance metric learning from automatically labeled bags of faces", "author": ["Guillaumin", "Matthieu", "Verbeek", "Jakob", "Schmid", "Cordelia"], "venue": "In ECCV,", "citeRegEx": "Guillaumin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Guillaumin et al\\.", "year": 2010}, {"title": "From image annotation to image description", "author": ["Gupta", "Ankush", "Mannem", "Prashanth"], "venue": "In ICONIP,", "citeRegEx": "Gupta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2012}, {"title": "Choosing linguistics over vision to describe images", "author": ["Gupta", "Ankush", "Verma", "Yashaswi", "Jawahar", "CV"], "venue": "In AAAI,", "citeRegEx": "Gupta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["Hodosh", "Micah", "Young", "Peter", "Hockenmaier", "Julia"], "venue": "metrics. JAIR,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Jia", "Yangqing", "Salzmann", "Mathieu", "Darrell", "Trevor"], "venue": "In ICCV, pp", "citeRegEx": "Jia et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2011}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In EMNLP, pp. 1700\u20131709,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Karpathy", "Andrej", "Joulin", "Armand", "Fei-Fei", "Li"], "venue": "In arXiv:1406.5679,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["Kiros", "Ryan", "R Zemel", "Salakhutdinov", "Ruslan"], "venue": "In ICML,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["Kulkarni", "Girish", "Premraj", "Visruth", "Dhar", "Sagnik", "Li", "Siming", "Choi", "Yejin", "Berg", "Alexander C", "Tamara L"], "venue": "In CVPR,", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["Kuznetsova", "Polina", "Ordonez", "Vicente", "Berg", "Tamara L", "Choi", "Yejin"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Kuznetsova et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "arXiv preprint arXiv:1405.0312,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Yuille", "Alan L"], "venue": "NIPS DeepLearning Workshop,", "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Huang", "Zhiheng", "Yuille", "Alan"], "venue": "arXiv preprint arXiv:1504.06692,", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Mikolov", "Tomas", "Kombrink", "Stefan", "Burget", "Lukas", "JH Cernocky", "Khudanpur", "Sanjeev"], "venue": "In ICASSP,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["Mitchell", "Margaret", "Han", "Xufeng", "Dodge", "Jesse", "Mensch", "Alyssa", "Goyal", "Amit", "Berg", "Alex", "Yamaguchi", "Kota", "Tamara", "Stratos", "Karl", "Daum\u00e9 III", "Hal"], "venue": "In EACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In ICML, pp", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In ACL, pp", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk. In NAACL-HLT workshop", "author": ["Rashtchian", "Cyrus", "Young", "Peter", "Hodosh", "Micah", "Hockenmaier", "Julia"], "venue": null, "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Socher", "Richard", "Q Le", "C Manning", "A. Ng"], "venue": "TACL,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Srivastava", "Nitish", "Salakhutdinov", "Ruslan"], "venue": "In NIPS, pp", "citeRegEx": "Srivastava et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In NIPS, pp", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Vedantam", "Ramakrishna", "Zitnick", "C Lawrence", "Parikh", "Devi"], "venue": "arXiv preprint arXiv:1411.5726,", "citeRegEx": "Vedantam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Young", "Peter", "Lai", "Alice", "Hodosh", "Micah", "Hockenmaier", "Julia"], "venue": "In ACL,", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "2014b) and plot the mean number of matches", "author": ["Kiros"], "venue": null, "citeRegEx": "Kiros,? \\Q2014\\E", "shortCiteRegEx": "Kiros", "year": 2014}, {"title": "2014b)) are all multimodal deep models. Our m-RNN model significantly outperforms", "author": ["Kiros"], "venue": null, "citeRegEx": "Kiros,? \\Q2014\\E", "shortCiteRegEx": "Kiros", "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "1090 (Mao et al. (2014)).", "startOffset": 6, "endOffset": 24}, {"referenceID": 11, "context": "In the experiments, we validate our model on four benchmark datasets: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al.", "startOffset": 82, "endOffset": 106}, {"referenceID": 11, "context": "In the experiments, we validate our model on four benchmark datasets: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al.", "startOffset": 82, "endOffset": 144}, {"referenceID": 11, "context": "In the experiments, we validate our model on four benchmark datasets: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al.", "startOffset": 82, "endOffset": 178}, {"referenceID": 11, "context": "In the experiments, we validate our model on four benchmark datasets: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al. (2014)).", "startOffset": 82, "endOffset": 210}, {"referenceID": 17, "context": "For computer vision, Krizhevsky et al. (2012) propose a deep Convolutional Neural Networks (CNN) with 8 layers (denoted as AlexNet) and outperform previous methods by a large margin in the image classification task of ImageNet challenge (Russakovsky et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 17, "context": "For computer vision, Krizhevsky et al. (2012) propose a deep Convolutional Neural Networks (CNN) with 8 layers (denoted as AlexNet) and outperform previous methods by a large margin in the image classification task of ImageNet challenge (Russakovsky et al. (2014)).", "startOffset": 21, "endOffset": 264}, {"referenceID": 8, "context": "Girshick et al. (2014) design a object detection framework (RCNN) based on this work.", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "Girshick et al. (2014) design a object detection framework (RCNN) based on this work. Recently, Simonyan & Zisserman (2014) propose a CNN with over 16 layers (denoted as VggNet) and performs substantially better than the AlexNet.", "startOffset": 0, "endOffset": 124}, {"referenceID": 8, "context": "Girshick et al. (2014) design a object detection framework (RCNN) based on this work. Recently, Simonyan & Zisserman (2014) propose a CNN with over 16 layers (denoted as VggNet) and performs substantially better than the AlexNet. For natural language, the Recurrent Neural Network (RNN) shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning (Mikolov et al. (2010; 2011; 2013)). Recently, RNNs have been successfully applied to machine translation to extract semantic information from the source sentence and generate target sentences (e.g. Kalchbrenner & Blunsom (2013), Cho et al.", "startOffset": 0, "endOffset": 625}, {"referenceID": 2, "context": "Kalchbrenner & Blunsom (2013), Cho et al. (2014) and Sutskever et al.", "startOffset": 31, "endOffset": 49}, {"referenceID": 2, "context": "Kalchbrenner & Blunsom (2013), Cho et al. (2014) and Sutskever et al. (2014)).", "startOffset": 31, "endOffset": 77}, {"referenceID": 2, "context": "Kalchbrenner & Blunsom (2013), Cho et al. (2014) and Sutskever et al. (2014)). Image-sentence retrieval. Many previous methods treat the task of describing images as a retrieval task and formulate the problem as a ranking or embedding learning problem (Hodosh et al. (2013); Frome et al.", "startOffset": 31, "endOffset": 274}, {"referenceID": 2, "context": "Kalchbrenner & Blunsom (2013), Cho et al. (2014) and Sutskever et al. (2014)). Image-sentence retrieval. Many previous methods treat the task of describing images as a retrieval task and formulate the problem as a ranking or embedding learning problem (Hodosh et al. (2013); Frome et al. (2013); Socher et al.", "startOffset": 31, "endOffset": 295}, {"referenceID": 2, "context": "Kalchbrenner & Blunsom (2013), Cho et al. (2014) and Sutskever et al. (2014)). Image-sentence retrieval. Many previous methods treat the task of describing images as a retrieval task and formulate the problem as a ranking or embedding learning problem (Hodosh et al. (2013); Frome et al. (2013); Socher et al. (2014)).", "startOffset": 31, "endOffset": 317}, {"referenceID": 2, "context": "Kalchbrenner & Blunsom (2013), Cho et al. (2014) and Sutskever et al. (2014)). Image-sentence retrieval. Many previous methods treat the task of describing images as a retrieval task and formulate the problem as a ranking or embedding learning problem (Hodosh et al. (2013); Frome et al. (2013); Socher et al. (2014)). They first extract the word and sentence features (e.g. Socher et al. (2014) uses dependency tree Recursive Neural Network to extract sentence features) as well as the image features.", "startOffset": 31, "endOffset": 396}, {"referenceID": 2, "context": "Kalchbrenner & Blunsom (2013), Cho et al. (2014) and Sutskever et al. (2014)). Image-sentence retrieval. Many previous methods treat the task of describing images as a retrieval task and formulate the problem as a ranking or embedding learning problem (Hodosh et al. (2013); Frome et al. (2013); Socher et al. (2014)). They first extract the word and sentence features (e.g. Socher et al. (2014) uses dependency tree Recursive Neural Network to extract sentence features) as well as the image features. Then they optimize a ranking cost to learn an embedding model that maps both the sentence feature and the image feature to a common semantic feature space. In this way, they can directly calculate the distance between images and sentences. Recently, Karpathy et al. (2014) show that object level image features based on object detection results can generate better results than image features extracted at the global level.", "startOffset": 31, "endOffset": 776}, {"referenceID": 23, "context": "They parse the sentence and divide it into several parts (Mitchell et al. (2012); Gupta & Mannem (2012)).", "startOffset": 58, "endOffset": 81}, {"referenceID": 23, "context": "They parse the sentence and divide it into several parts (Mitchell et al. (2012); Gupta & Mannem (2012)).", "startOffset": 58, "endOffset": 104}, {"referenceID": 18, "context": "Kulkarni et al. (2011) uses a Conditional Random Field model and Farhadi et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model).", "startOffset": 49, "endOffset": 71}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)).", "startOffset": 49, "endOffset": 347}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al.", "startOffset": 49, "endOffset": 589}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al.", "startOffset": 49, "endOffset": 631}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al. (2011)).", "startOffset": 49, "endOffset": 650}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al. (2011)). They generate sentences with richer and more flexible structure than the first group. The probability of generating sentences using the model can serve as the affinity metric for retrieval. Our method falls into this category. More closely related to our tasks and method is the work of Kiros et al. (2014b), which is built on a Log-BiLinear model (Mnih & Hinton (2007)) and use AlexNet to extract visual features.", "startOffset": 49, "endOffset": 960}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al. (2011)). They generate sentences with richer and more flexible structure than the first group. The probability of generating sentences using the model can serve as the affinity metric for retrieval. Our method falls into this category. More closely related to our tasks and method is the work of Kiros et al. (2014b), which is built on a Log-BiLinear model (Mnih & Hinton (2007)) and use AlexNet to extract visual features.", "startOffset": 49, "endOffset": 1022}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al. (2011)). They generate sentences with richer and more flexible structure than the first group. The probability of generating sentences using the model can serve as the affinity metric for retrieval. Our method falls into this category. More closely related to our tasks and method is the work of Kiros et al. (2014b), which is built on a Log-BiLinear model (Mnih & Hinton (2007)) and use AlexNet to extract visual features. It needs a fixed length of context (i.e. five words), whereas in our model, the temporal context is stored in a recurrent architecture, which allows arbitrary context length. Shortly after Mao et al. (2014), several papers appear with record breaking results (e.", "startOffset": 49, "endOffset": 1274}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al. (2011)). They generate sentences with richer and more flexible structure than the first group. The probability of generating sentences using the model can serve as the affinity metric for retrieval. Our method falls into this category. More closely related to our tasks and method is the work of Kiros et al. (2014b), which is built on a Log-BiLinear model (Mnih & Hinton (2007)) and use AlexNet to extract visual features. It needs a fixed length of context (i.e. five words), whereas in our model, the temporal context is stored in a recurrent architecture, which allows arbitrary context length. Shortly after Mao et al. (2014), several papers appear with record breaking results (e.g. Kiros et al. (2014a); Karpathy & Fei-Fei (2014); Vinyals et al.", "startOffset": 49, "endOffset": 1353}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al. (2011)). They generate sentences with richer and more flexible structure than the first group. The probability of generating sentences using the model can serve as the affinity metric for retrieval. Our method falls into this category. More closely related to our tasks and method is the work of Kiros et al. (2014b), which is built on a Log-BiLinear model (Mnih & Hinton (2007)) and use AlexNet to extract visual features. It needs a fixed length of context (i.e. five words), whereas in our model, the temporal context is stored in a recurrent architecture, which allows arbitrary context length. Shortly after Mao et al. (2014), several papers appear with record breaking results (e.g. Kiros et al. (2014a); Karpathy & Fei-Fei (2014); Vinyals et al.", "startOffset": 49, "endOffset": 1380}, {"referenceID": 6, "context": "(2011) uses a Conditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the retrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al. (2011)). They generate sentences with richer and more flexible structure than the first group. The probability of generating sentences using the model can serve as the affinity metric for retrieval. Our method falls into this category. More closely related to our tasks and method is the work of Kiros et al. (2014b), which is built on a Log-BiLinear model (Mnih & Hinton (2007)) and use AlexNet to extract visual features. It needs a fixed length of context (i.e. five words), whereas in our model, the temporal context is stored in a recurrent architecture, which allows arbitrary context length. Shortly after Mao et al. (2014), several papers appear with record breaking results (e.g. Kiros et al. (2014a); Karpathy & Fei-Fei (2014); Vinyals et al. (2014); Donahue et al.", "startOffset": 49, "endOffset": 1403}, {"referenceID": 5, "context": "(2014); Donahue et al. (2014); Fang et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 5, "context": "(2014); Donahue et al. (2014); Fang et al. (2014); Chen & Zitnick (2014)).", "startOffset": 8, "endOffset": 50}, {"referenceID": 5, "context": "(2014); Donahue et al. (2014); Fang et al. (2014); Chen & Zitnick (2014)).", "startOffset": 8, "endOffset": 73}, {"referenceID": 30, "context": "the one-hot representation, which is binary and has the same dimension as the vocabulary size with only one non-zero element) Mikolov et al. (2010). y(t) can be calculated as follows: x(t) = [w(t) r(t\u2212 1)]; r(t) = f1(U \u00b7 x(t)); y(t) = g1(V \u00b7 r(t)); (1) where x(t) is a vector that concatenates w(t) and r(t\u22121), f1(.", "startOffset": 126, "endOffset": 148}, {"referenceID": 30, "context": "the one-hot representation, which is binary and has the same dimension as the vocabulary size with only one non-zero element) Mikolov et al. (2010). y(t) can be calculated as follows: x(t) = [w(t) r(t\u2212 1)]; r(t) = f1(U \u00b7 x(t)); y(t) = g1(V \u00b7 r(t)); (1) where x(t) is a vector that concatenates w(t) and r(t\u22121), f1(.) and g1(.) are element-wise sigmoid and softmax function respectively, and U, V are weights which will be learned. The size of the RNN is adaptive to the length of the input sequence. The recurrent layers connect the sub-networks in different time frames. Accordingly, when we do backpropagation, we need to propagate the error through recurrent connections back in time (Rumelhart et al. (1988)).", "startOffset": 126, "endOffset": 712}, {"referenceID": 18, "context": "Most of the sentence-image multimodal models (Karpathy et al. (2014); Frome et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Socher et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Socher et al. (2014); Kiros et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Socher et al. (2014); Kiros et al. (2014b)) use pre-computed word embedding vectors as the initialization of their model.", "startOffset": 8, "endOffset": 72}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Socher et al. (2014); Kiros et al. (2014b)) use pre-computed word embedding vectors as the initialization of their model. In contrast, we randomly initialize our word embedding layers and learn them from the training data. We show that this random initialization is sufficient for our architecture to generate the state-of-the-art result. We treat the activation of the word embedding layer II (see Figure 2(b)) as the final word representation, which is one of the three direct inputs of the multimodal layer. After the two word embedding layers, we have a recurrent layer with 256 dimensions. The calculation of the recurrent layer is slightly different from the calculation for the simple RNN. Instead of concatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation at time t\u2212 1 (denoted as r(t\u2212 1)), we first map r(t\u2212 1) into the same vector space as w(t) and add them together: r(t) = f2(Ur \u00b7 r(t\u2212 1) +w(t)); (2) where \u201c+\u201d represents element-wise addition. We set f2(.) to be the Rectified Linear Unit (ReLU), inspired by its the recent success when training very deep structure in computer vision field (Nair & Hinton (2010); Krizhevsky et al.", "startOffset": 8, "endOffset": 1200}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Socher et al. (2014); Kiros et al. (2014b)) use pre-computed word embedding vectors as the initialization of their model. In contrast, we randomly initialize our word embedding layers and learn them from the training data. We show that this random initialization is sufficient for our architecture to generate the state-of-the-art result. We treat the activation of the word embedding layer II (see Figure 2(b)) as the final word representation, which is one of the three direct inputs of the multimodal layer. After the two word embedding layers, we have a recurrent layer with 256 dimensions. The calculation of the recurrent layer is slightly different from the calculation for the simple RNN. Instead of concatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation at time t\u2212 1 (denoted as r(t\u2212 1)), we first map r(t\u2212 1) into the same vector space as w(t) and add them together: r(t) = f2(Ur \u00b7 r(t\u2212 1) +w(t)); (2) where \u201c+\u201d represents element-wise addition. We set f2(.) to be the Rectified Linear Unit (ReLU), inspired by its the recent success when training very deep structure in computer vision field (Nair & Hinton (2010); Krizhevsky et al. (2012)).", "startOffset": 8, "endOffset": 1226}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Socher et al. (2014); Kiros et al. (2014b)) use pre-computed word embedding vectors as the initialization of their model. In contrast, we randomly initialize our word embedding layers and learn them from the training data. We show that this random initialization is sufficient for our architecture to generate the state-of-the-art result. We treat the activation of the word embedding layer II (see Figure 2(b)) as the final word representation, which is one of the three direct inputs of the multimodal layer. After the two word embedding layers, we have a recurrent layer with 256 dimensions. The calculation of the recurrent layer is slightly different from the calculation for the simple RNN. Instead of concatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation at time t\u2212 1 (denoted as r(t\u2212 1)), we first map r(t\u2212 1) into the same vector space as w(t) and add them together: r(t) = f2(Ur \u00b7 r(t\u2212 1) +w(t)); (2) where \u201c+\u201d represents element-wise addition. We set f2(.) to be the Rectified Linear Unit (ReLU), inspired by its the recent success when training very deep structure in computer vision field (Nair & Hinton (2010); Krizhevsky et al. (2012)). This differs from the simple RNN where the sigmoid function is adopted (see Section 3.1). ReLU is faster, and harder to saturate or overfit the data than non-linear functions like the sigmoid. When the backpropagation through time (BPTT) is conducted for the RNN with sigmoid function, the vanishing or exploding gradient problem appears since even the simplest RNN model can have a large temporal depth 3. Previous work (Mikolov et al. (2010; 2011)) use heuristics, such as the truncated BPTT, to avoid this problem. The truncated BPTT stops the BPTT after k time steps, where k is a hand-defined hyperparameter. Because of the good properties of ReLU, we do not need to stop the BPTT at an early stage, which leads to better and more efficient utilization of the data than the truncated BPTT. After the recurrent layer, we set up a 512 dimensional multimodal layer that connects the language model part and the vision part of the m-RNN model (see Figure 2(b)). This layer has three inputs: the word-embedding layer II, the recurrent layer and the image representation. For the image representation, here we use the activation of the 7th layer of AlexNet (Krizhevsky et al. (2012)) or 15 layer of VggNet (Simonyan & Zisserman (2014)), though our framework can use any image features.", "startOffset": 8, "endOffset": 2410}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Socher et al. (2014); Kiros et al. (2014b)) use pre-computed word embedding vectors as the initialization of their model. In contrast, we randomly initialize our word embedding layers and learn them from the training data. We show that this random initialization is sufficient for our architecture to generate the state-of-the-art result. We treat the activation of the word embedding layer II (see Figure 2(b)) as the final word representation, which is one of the three direct inputs of the multimodal layer. After the two word embedding layers, we have a recurrent layer with 256 dimensions. The calculation of the recurrent layer is slightly different from the calculation for the simple RNN. Instead of concatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation at time t\u2212 1 (denoted as r(t\u2212 1)), we first map r(t\u2212 1) into the same vector space as w(t) and add them together: r(t) = f2(Ur \u00b7 r(t\u2212 1) +w(t)); (2) where \u201c+\u201d represents element-wise addition. We set f2(.) to be the Rectified Linear Unit (ReLU), inspired by its the recent success when training very deep structure in computer vision field (Nair & Hinton (2010); Krizhevsky et al. (2012)). This differs from the simple RNN where the sigmoid function is adopted (see Section 3.1). ReLU is faster, and harder to saturate or overfit the data than non-linear functions like the sigmoid. When the backpropagation through time (BPTT) is conducted for the RNN with sigmoid function, the vanishing or exploding gradient problem appears since even the simplest RNN model can have a large temporal depth 3. Previous work (Mikolov et al. (2010; 2011)) use heuristics, such as the truncated BPTT, to avoid this problem. The truncated BPTT stops the BPTT after k time steps, where k is a hand-defined hyperparameter. Because of the good properties of ReLU, we do not need to stop the BPTT at an early stage, which leads to better and more efficient utilization of the data than the truncated BPTT. After the recurrent layer, we set up a 512 dimensional multimodal layer that connects the language model part and the vision part of the m-RNN model (see Figure 2(b)). This layer has three inputs: the word-embedding layer II, the recurrent layer and the image representation. For the image representation, here we use the activation of the 7th layer of AlexNet (Krizhevsky et al. (2012)) or 15 layer of VggNet (Simonyan & Zisserman (2014)), though our framework can use any image features.", "startOffset": 8, "endOffset": 2462}, {"referenceID": 26, "context": ") is the element-wise scaled hyperbolic tangent function (LeCun et al. (2012)):", "startOffset": 58, "endOffset": 78}, {"referenceID": 21, "context": "This is equivalent to the perplexity-based image retrieval in Kiros et al. (2014b).", "startOffset": 62, "endOffset": 83}, {"referenceID": 21, "context": "To solve this problem, Kiros et al. (2014b) uses the perplexity of a sentence conditioned on the averaged image feature across the training set as the reference perplexity to normalize the original perplexity.", "startOffset": 23, "endOffset": 44}, {"referenceID": 21, "context": "To solve this problem, Kiros et al. (2014b) uses the perplexity of a sentence conditioned on the averaged image feature across the training set as the reference perplexity to normalize the original perplexity. Different from them, we use the normalized probability where the normalization factor is the marginal probability of w 1:L: P (w 1:L|I)/P (w 1:L); P (w 1:L) = \u2211 I\u2032P (w D 1:L|I \u2032 ) \u00b7 P (I \u2032 ) (7) where w 1:L denotes the sentence in the dataset, I Q denotes the query image, and I \u2032 are images sampled from the training set. We approximate P (I \u2032 ) by a constant and ignore this term. This strategy leads to a much better performance than that in Kiros et al. (2014b) in the experiments.", "startOffset": 23, "endOffset": 676}, {"referenceID": 20, "context": "For the vision part, we use the pre-trained AlexNet (Krizhevsky et al. (2012)) or the VggNet (Simonyan & Zisserman (2014)) on ImageNet dataset (Russakovsky et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 20, "context": "For the vision part, we use the pre-trained AlexNet (Krizhevsky et al. (2012)) or the VggNet (Simonyan & Zisserman (2014)) on ImageNet dataset (Russakovsky et al.", "startOffset": 53, "endOffset": 122}, {"referenceID": 20, "context": "For the vision part, we use the pre-trained AlexNet (Krizhevsky et al. (2012)) or the VggNet (Simonyan & Zisserman (2014)) on ImageNet dataset (Russakovsky et al. (2014)).", "startOffset": 53, "endOffset": 170}, {"referenceID": 18, "context": "Recently, Karpathy et al. (2014) show that using the RCNN object detection results (Girshick et al.", "startOffset": 10, "endOffset": 33}, {"referenceID": 10, "context": "(2014) show that using the RCNN object detection results (Girshick et al. (2014)) combined with the AlexNet features performs better than simply treating the image as a whole frame.", "startOffset": 58, "endOffset": 81}, {"referenceID": 10, "context": "(2014) show that using the RCNN object detection results (Girshick et al. (2014)) combined with the AlexNet features performs better than simply treating the image as a whole frame. In the experiments, we show that our method performs much better than Karpathy et al. (2014) when the same image features are used, and is better than or comparable to their results even when they use more sophisticated features based on object detection.", "startOffset": 58, "endOffset": 275}, {"referenceID": 11, "context": "We test our method on four benchmark datasets with sentence level annotations: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al.", "startOffset": 91, "endOffset": 115}, {"referenceID": 11, "context": "We test our method on four benchmark datasets with sentence level annotations: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al.", "startOffset": 91, "endOffset": 153}, {"referenceID": 11, "context": "We test our method on four benchmark datasets with sentence level annotations: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al.", "startOffset": 91, "endOffset": 187}, {"referenceID": 11, "context": "We test our method on four benchmark datasets with sentence level annotations: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al. (2014)).", "startOffset": 91, "endOffset": 219}, {"referenceID": 11, "context": "We test our method on four benchmark datasets with sentence level annotations: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al. (2014)). IAPR TC-12. This dataset consists of around 20,000 images taken from different locations around the world. It contains images of different sports and actions, people, animals, cities, landscapes, etc. For each image, it provides at least one sentence annotation. On average, there are about 1.7 sentence annotations for one image. We adopt the standard separation of training and testing set as previous works (Guillaumin et al. (2010); Kiros et al.", "startOffset": 91, "endOffset": 657}, {"referenceID": 11, "context": "We test our method on four benchmark datasets with sentence level annotations: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al. (2014)). IAPR TC-12. This dataset consists of around 20,000 images taken from different locations around the world. It contains images of different sports and actions, people, animals, cities, landscapes, etc. For each image, it provides at least one sentence annotation. On average, there are about 1.7 sentence annotations for one image. We adopt the standard separation of training and testing set as previous works (Guillaumin et al. (2010); Kiros et al. (2014b)) with 17,665 images for training and 1962 images for testing.", "startOffset": 91, "endOffset": 679}, {"referenceID": 19, "context": "We follow the previous work (Karpathy et al. (2014)) which used 1,000 images for testing.", "startOffset": 29, "endOffset": 52}, {"referenceID": 31, "context": "B-1, B-2, B-3, and B-4) (Papineni et al. (2002)) as the evaluation metrics.", "startOffset": 25, "endOffset": 48}, {"referenceID": 31, "context": "B-1, B-2, B-3, and B-4) (Papineni et al. (2002)) as the evaluation metrics. BLEU scores were originally designed for automatic machine translation where they rate the quality of a translated sentences given several reference sentences. Similarly, we can treat the sentence generation task as the \u201ctranslation\u201d of the content of images to sentences. BLEU remains the standard evaluation metric for sentence generation methods for images, though it has drawbacks. For some images, the reference sentences might not contain all the possible descriptions in the image and BLEU might penalize some correctly generated sentences. Please see more details of the calculation of BLEU scores for this task in the supplementary material section 10.3 5. Sentence Retrieval and Image Retrieval. We adopt the same evaluation metrics as previous works (Socher et al. (2014); Frome et al.", "startOffset": 25, "endOffset": 859}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Karpathy et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Karpathy et al. (2014)) for both the tasks of sentences retrieval and image retrieval.", "startOffset": 8, "endOffset": 52}, {"referenceID": 9, "context": "(2014); Frome et al. (2013); Karpathy et al. (2014)) for both the tasks of sentences retrieval and image retrieval. We use R@K (K = 1, 5, 10) as the measurement. R@K is the recall rate of a correctly retrieved groundtruth given top K candidates. Higher R@K usually means better retrieval performance. Since we care most about the top-ranked retrieved results, the R@K scores with smaller K are more important. The Med r is another metric we use, which is the median rank of the first retrieved groundtruth sentence or image. Lower Med r usually means better performance. For IAPR TC-12 datasets, we use additional evaluation metrics to conduct a fair comparison with previous work (Kiros et al. (2014b)).", "startOffset": 8, "endOffset": 703}, {"referenceID": 21, "context": "To conduct a fair comparison, we follow the same experimental settings of Kiros et al. (2014b) to calculate the BLEU scores and perplexity.", "startOffset": 74, "endOffset": 95}, {"referenceID": 0, "context": "html The BLEU outputted by our implementation is slightly lower than the recently released MS COCO caption evaluation toolbox (Chen et al. (2015)) because of different tokenization methods of the sentences.", "startOffset": 127, "endOffset": 146}, {"referenceID": 0, "context": "html The BLEU outputted by our implementation is slightly lower than the recently released MS COCO caption evaluation toolbox (Chen et al. (2015)) because of different tokenization methods of the sentences. We reevaluate our method using the toolbox in the current version of the paper. Kiros et al. (2014b) further improved their results after the publication.", "startOffset": 127, "endOffset": 308}, {"referenceID": 19, "context": "064 MLBLB-AlexNet, Kiros et al. (2014b) 9.", "startOffset": 19, "endOffset": 40}, {"referenceID": 19, "context": "064 MLBLB-AlexNet, Kiros et al. (2014b) 9.86 0.393 0.211 0.112 MLBLF-AlexNet, Kiros et al. (2014b) 9.", "startOffset": 19, "endOffset": 99}, {"referenceID": 13, "context": "115 Gupta et al. (2012) - 0.", "startOffset": 4, "endOffset": 24}, {"referenceID": 13, "context": "115 Gupta et al. (2012) - 0.15 0.06 0.01 Gupta & Mannem (2012) - 0.", "startOffset": 4, "endOffset": 63}, {"referenceID": 21, "context": "We also adopt additional evaluation metrics to compare our method with Kiros et al. (2014b), see supplementary material Section 10.", "startOffset": 71, "endOffset": 92}, {"referenceID": 37, "context": "We compare our model with several state-ofthe-art methods: SDT-RNN (Socher et al. (2014)), DeViSE (Frome et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 9, "context": "(2014)), DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 9, "context": "(2014)), DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al. (2014)) with various image representations.", "startOffset": 17, "endOffset": 70}, {"referenceID": 9, "context": "(2014)), DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al. (2014)) with various image representations. Our model outperforms these methods by a large margin when using the same image representation (e.g. AlexNet). We also list the performance of methods using more sophisticated features in Table 3. \u201c-avg-RCNN\u201d denotes methods with features of the average CNN activation of all objects above a detection confidence threshold. DeepFE-RCNN Karpathy et al. (2014) uses a fragment mapping strategy to better exploit the object detection results.", "startOffset": 17, "endOffset": 466}, {"referenceID": 7, "context": "We compare our method with several state-of-the-art methods in these two recently released dataset (Note that the last six methods appear very recently, we use the results reported in their papers): DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al.", "startOffset": 207, "endOffset": 227}, {"referenceID": 7, "context": "We compare our method with several state-of-the-art methods in these two recently released dataset (Note that the last six methods appear very recently, we use the results reported in their papers): DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al. (2014)), MNLM (Kiros et al.", "startOffset": 207, "endOffset": 260}, {"referenceID": 7, "context": "We compare our method with several state-of-the-art methods in these two recently released dataset (Note that the last six methods appear very recently, we use the results reported in their papers): DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al. (2014)), MNLM (Kiros et al. (2014a)), DMSM (Fang et al.", "startOffset": 207, "endOffset": 289}, {"referenceID": 6, "context": "(2014a)), DMSM (Fang et al. (2014)), NIC (Vinyals et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 6, "context": "(2014a)), DMSM (Fang et al. (2014)), NIC (Vinyals et al. (2014)), LRCN (Donahue et al.", "startOffset": 16, "endOffset": 64}, {"referenceID": 5, "context": "(2014)), LRCN (Donahue et al. (2014)), RVR (Chen & Zitnick (2014)), and DeepVS (Karpathy & Fei-Fei (2014)).", "startOffset": 15, "endOffset": 37}, {"referenceID": 5, "context": "(2014)), LRCN (Donahue et al. (2014)), RVR (Chen & Zitnick (2014)), and DeepVS (Karpathy & Fei-Fei (2014)).", "startOffset": 15, "endOffset": 66}, {"referenceID": 5, "context": "(2014)), LRCN (Donahue et al. (2014)), RVR (Chen & Zitnick (2014)), and DeepVS (Karpathy & Fei-Fei (2014)).", "startOffset": 15, "endOffset": 106}, {"referenceID": 0, "context": "We also validate our method on the test set of MS COCO by their evaluation server (Chen et al. (2015)).", "startOffset": 83, "endOffset": 102}, {"referenceID": 3, "context": "Recently, Devlin et al. (2015b) proposed a nearest neighbor approach that retrieves the captions of the k nearest images in the training set, ranks these captions according to the consensus of the caption w.", "startOffset": 10, "endOffset": 32}, {"referenceID": 3, "context": "Recently, Devlin et al. (2015b) proposed a nearest neighbor approach that retrieves the captions of the k nearest images in the training set, ranks these captions according to the consensus of the caption w.r.t. to the rest of the captions, and output the top ranked one. Inspired by this method, we first adopt the m-RNN model with the transposed weight sharing strategy (Mao et al. (2015), denoted as m-RNN-shared) to generate n hypotheses using a beam search scheme.", "startOffset": 10, "endOffset": 391}, {"referenceID": 3, "context": "Recently, Devlin et al. (2015b) proposed a nearest neighbor approach that retrieves the captions of the k nearest images in the training set, ranks these captions according to the consensus of the caption w.r.t. to the rest of the captions, and output the top ranked one. Inspired by this method, we first adopt the m-RNN model with the transposed weight sharing strategy (Mao et al. (2015), denoted as m-RNN-shared) to generate n hypotheses using a beam search scheme. Specifically, we keep the n best candidates in the sentence generation process until the model generates the end sign wend. These n best candidates are approximately the n most probable sentences generated by the model, and can be treated as the n hypotheses. In our experiments, we set n = 10 since it gives us a diversified set of hypotheses without too much outliers on our validation set. 8 After generating the hypotheses of a target image, we retrieve its nearest neighbors in the image feature space on the training set (see details in Section 8.1). Then we calculate the \u201cconsensus\u201d scores (Devlin et al. (2015a)) of the hypotheses w.", "startOffset": 10, "endOffset": 1091}, {"referenceID": 3, "context": "We follow Devlin et al. (2015a) to calculate the consensus score of a hypotheses.", "startOffset": 10, "endOffset": 32}, {"referenceID": 3, "context": "We follow Devlin et al. (2015a) to calculate the consensus score of a hypotheses. The difference is that Devlin et al. (2015a) treat the captions of the k nearest neighbor images as the hypotheses while our hypotheses are generated by the m-RNN model.", "startOffset": 10, "endOffset": 127}, {"referenceID": 36, "context": "The similarity between a hypothesis and one of its nearest neighbor reference captions is defined by a sentence-level BLEU score (Papineni et al. (2002)) or a sentence-level CIDEr (Vedantam et al.", "startOffset": 130, "endOffset": 153}, {"referenceID": 36, "context": "The similarity between a hypothesis and one of its nearest neighbor reference captions is defined by a sentence-level BLEU score (Papineni et al. (2002)) or a sentence-level CIDEr (Vedantam et al. (2014)).", "startOffset": 130, "endOffset": 204}], "year": 2015, "abstractText": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated according to this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/ \u0303junhua.mao/m-RNN.html. 1", "creator": "LaTeX with hyperref package"}}}