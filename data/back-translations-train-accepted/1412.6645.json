{"id": "1412.6645", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Weakly Supervised Multi-Embeddings Learning of Acoustic Models", "abstract": "We trained a Siamese network with multi-task same/different information on a speech dataset, we found that learning a good representation for one task helps in the other too, and joint-learning of both tasks gave the best results. The first task was to discriminate between two same or different words, and the second was to discriminate between two same or different talkers.", "histories": [["v1", "Sat, 20 Dec 2014 11:54:41 GMT  (994kb,D)", "https://arxiv.org/abs/1412.6645v1", "5 pages, 3 figures"], ["v2", "Tue, 24 Feb 2015 10:09:09 GMT  (1207kb,D)", "http://arxiv.org/abs/1412.6645v2", "6 pages, 3 figures"], ["v3", "Mon, 20 Apr 2015 12:35:32 GMT  (1206kb,D)", "http://arxiv.org/abs/1412.6645v3", "6 pages, 3 figures"]], "COMMENTS": "5 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["gabriel synnaeve", "emmanuel dupoux"], "accepted": true, "id": "1412.6645"}, "pdf": {"name": "1412.6645.pdf", "metadata": {"source": "CRF", "title": "LEARNING OF ACOUSTIC MODELS", "authors": ["Gabriel Synnaeve", "Emmanuel Dupoux"], "emails": ["gabriel.synnaeve@gmail.com", "emmanuel.dupoux@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In theory, algorithms that operate unattended or poorly supervised are plausible models for language acquisition in human infants (Vallabha et al., 2007); in practice, they can be used for languages with limited resources (Park & Glass, 2008); and, building on the fact that infants can recognize some words (Bergelson & Swingley, 2012) and distinguish between speakers (Johnson et al., 2011) before they represent adult phonemes, we suggest testing a neural network architecture in which word and speaker identity are used as ancillary information to help learn an acoustic model (telephone embedding); previous work has shown that the same page information can be used for metric learning (Xing et al., 2003); and Synnaeve et al. (2014) have shown that they can be used with the help of Deep Neural Network (DNN) to learn telephone embedding."}, {"heading": "2 MODEL", "text": "We used the architecture of a Siamese network (Bromley et al., 1993), as shown in Fig. 3.1. It is a duplicated feedback-forward neural network with two parallel inputs. Each of the inputs consists of 11 stacked frames with 40 coefficients, which are log-compressed. Each network contains 3 hidden layers of 500 units with sigmoid activation and two output embedded with 100 dimensions each. One of the embedded frames is the one in which we calculate the similarity between the two inputs according to the same / different \"word type\" specification, while the other considers the same / different \"speaker display.\" Formally: xA and xB: R11 x 40; yA, W, yA, yB, W and yB, S: R100The loss function we use (for two inputs xA and xB) is a simple sum of COS2 losses in each embedding."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 DATASET", "text": "We used about 1 / 3 (12 loudspeakers) of the Buckeye corpus1, where we performed a Dynamic Time Contraction (DTW) of pairs of words in the attribute space (filter banks), just like in (Synnaeve et al., 2014).This involved making 76407 pairs of long \"same\" words (a total of 1057 species), with 1 / 4 of the time spoken by the same speakers (we subsampled \"the same word and different pairs of speakers\").During the training, we also tried pairs of tokens derived from different words (often referred to as negative sampling) with a ratio of pairs of same / different words of 1: 1, giving about 5 million frames for pairs of same words and 4.3 million frames for sampled pairs of different words."}, {"heading": "3.2 RESULTS", "text": "We trained the model with Adadelta (Zeiler, 2012), a variant of stochastic gradients that do not depend on an adaptive learning method that corrects the magnitude of updates, with an accumulation of past gradients (on a sliding window) and a local disadvantage of the speaker. We compared three network setups, we used the combined loss function, which includes both the word and the speaker losses. In the case-by-case setup constellation, we use only one of the losses (word or speaker), although the topology of the network remains the same. This means that the weights of only one of the two embedding are updated, the others remain in their initial state, resulting in the implementation of a random projection from the last hidden layer."}, {"heading": "3.3 DISCUSSION", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "4 CONCLUSION", "text": "We have shown that a Siamese network can perform both phonemes and loudspeaker discrimination with only a moderate amount of ancillary information (specifying the same / different speaker for only \u2248 1000 word types and 12 loudspeakers). Further work is needed to investigate the effect of the amount of information and whether the received loudspeaker embedding could replace or complement i-vectors. Finally, the embedding of the phone should be evaluated as a first step in a subsequent word recognition mechanism or language model adapted to this type of representation."}, {"heading": "ACKNOWLEDGMENTS", "text": "This project is partly funded by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX0001-02 PSL *), the Fondation de France, the Ecole de Neurosciences de Paris and the Ile de France Region (DIM cerveau et pense)."}], "references": [{"title": "Speaker diarization: A review of recent research", "author": ["Anguera Miro", "Xavier", "Bozonnet", "Simon", "Evans", "Nicholas", "Fredouille", "Corinne", "Friedland", "Gerald", "Vinyals", "Oriol"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on,", "citeRegEx": "Miro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Miro et al\\.", "year": 2012}, {"title": "At 6\u20139 months, human infants know the meanings of many common nouns", "author": ["Bergelson", "Elika", "Swingley", "Daniel"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Bergelson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergelson et al\\.", "year": 2012}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Bromley", "Jane", "Bentz", "James W", "Bottou", "L\u00e9on", "Guyon", "Isabelle", "LeCun", "Yann", "Moore", "Cliff", "S\u00e4ckinger", "Eduard", "Shah", "Roopak"], "venue": "Internat. Journ. of Pattern Recog. and Artific. Intell.,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["Carlin", "Michael A", "Thomas", "Samuel", "Jansen", "Aren", "Hermansky", "Hynek"], "venue": "In INTERSPEECH,", "citeRegEx": "Carlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Carlin et al\\.", "year": 2011}, {"title": "Front-end factor analysis for speaker verification", "author": ["Dehak", "Najim", "Kenny", "Patrick", "R\u00e9da", "Dumouchel", "Pierre", "Ouellet"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on,", "citeRegEx": "Dehak et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dehak et al\\.", "year": 2011}, {"title": "Towards spoken term discovery at scale with zero resources", "author": ["Jansen", "Aren", "Church", "Kenneth", "Hermansky", "Hynek"], "venue": "In INTERSPEECH,", "citeRegEx": "Jansen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jansen et al\\.", "year": 2010}, {"title": "Infant ability to tell voices apart rests on language experience", "author": ["Johnson", "Elizabeth K", "Westrek", "Ellen", "Nazzi", "Thierry", "Cutler", "Anne"], "venue": "Developmental Science,", "citeRegEx": "Johnson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2011}, {"title": "Understanding how deep belief networks perform acoustic modelling", "author": ["Mohamed", "Abdel-rahman", "Hinton", "Geoffrey", "Penn", "Gerald"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mohamed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2012}, {"title": "Unsupervised pattern discovery in speech", "author": ["Park", "Alex S", "Glass", "James R"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Park et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Park et al\\.", "year": 2008}, {"title": "Evaluating speech features with the minimal-pair abx task: Analysis of the classical mfc/plp pipeline", "author": ["Schatz", "Thomas", "Peddinti", "Vijayaditya", "Bach", "Francis", "Jansen", "Aren", "Hynek", "Hermansky", "Dupoux", "Emmanuel"], "venue": null, "citeRegEx": "Schatz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schatz et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Phonetics embedding learning with side information", "author": ["Synnaeve", "Gabriel", "Schatz", "Thomas", "Dupoux", "Emmanuel"], "venue": "In IEEE SLT,", "citeRegEx": "Synnaeve et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Synnaeve et al\\.", "year": 2014}, {"title": "Unsupervised learning of vowel categories from infant-directed speech", "author": ["Vallabha", "Gautam K", "McClelland", "James L", "Pons", "Ferran", "Werker", "Janet F", "Amano", "Shigeaki"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Vallabha et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vallabha et al\\.", "year": 2007}, {"title": "Distance metric learning", "author": ["Eric P", "Ng", "Andrew Y", "Jordan", "Michael I", "Russell", "Stuart"], "venue": null, "citeRegEx": "P et al\\.,? \\Q2015\\E", "shortCiteRegEx": "P et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Theoretically, algorithms performing unsupervised or weakly supervised discovery of linguistic structure represent plausible models of language acquisition in the human infant (Vallabha et al., 2007).", "startOffset": 176, "endOffset": 199}, {"referenceID": 6, "context": "Building on the fact that infant can recognize some words (Bergelson & Swingley, 2012) and discriminate between speakers (Johnson et al., 2011) before they have constructed adult-like phoneme representations, we propose to test a neural network architecture where word and talker identity are used as side information to help learning an acoustic model (phone embedding).", "startOffset": 121, "endOffset": 143}, {"referenceID": 5, "context": "As this paper is a feasibility study, we used gold same-different labels, and leave it to further work to derive them in an unsupervised fashion using spoken term discovery (Jansen et al., 2010) and talker diarization (Anguera Miro et al.", "startOffset": 173, "endOffset": 194}, {"referenceID": 4, "context": "Building on the fact that infant can recognize some words (Bergelson & Swingley, 2012) and discriminate between speakers (Johnson et al., 2011) before they have constructed adult-like phoneme representations, we propose to test a neural network architecture where word and talker identity are used as side information to help learning an acoustic model (phone embedding). Previous work has shown that same-different side information can be used for metric learning (Xing et al., 2003), and Synnaeve et al. (2014) demonstrated that it can be used with Deep Neural Network (DNN) architecture for learning phone embeddings.", "startOffset": 122, "endOffset": 513}, {"referenceID": 2, "context": "We used the architure of a Siamese network (Bromley et al., 1993), as shown in Fig.", "startOffset": 43, "endOffset": 65}, {"referenceID": 2, "context": "We used the architure of a Siamese network (Bromley et al., 1993), as shown in Fig. 3.1. It is a duplicated feedforward neural network taking two inputs in parallel. Each of the inputs consists in 11 stacked frames of 40 coefficients log-compressed Mel-filterbanks. Each network contains 3 hidden layers of 500 units with sigmoid activations, and two output embeddings each of 100 dimensions. One of the embeddings is the one in which we compute the similarity between the two inputs according to the same/different \u201cword type\u201d indication, while the other looks at the same/different \u201cspeaker\u201d indication. More formally: xA and xB \u2208 R11\u00d740 ; yA,W , yA,S , yB,W and yB,S \u2208 R The loss function that we use (for two inputs xA and xB) is a simple sum of the COSCOS losses in each of the embeddings (see Synnaeve et al. (2014) for a comparison with other loss functions): L(A,B) = LW (A,B) + LS(A,B) with W \u2208 {0, 1} (different or same word) and S \u2208 {0, 1} (different or same speaker), both losses are similar (here for speakers): LS(A,B) = S \u00d7 (1\u2212 cos(yA,S , yB,S)) + (1\u2212 S)\u00d7 (cos(yA,S , yB,S))", "startOffset": 44, "endOffset": 822}, {"referenceID": 11, "context": "1 DATASET We used about 1/3rd (12 speakers) of the Buckeye corpus1, on which we performed a dynamic timewarping (DTW) alignment of pairs of same words, in the features space (filterbanks), exactly as in (Synnaeve et al., 2014).", "startOffset": 203, "endOffset": 226}, {"referenceID": 3, "context": "Here, we follow the lead of Carlin et al. (2011) and Schatz et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "Here, we follow the lead of Carlin et al. (2011) and Schatz et al. (2013) who propose to use instead a discrimination task, which makes no assumption about the shape of the coded categories (phone-like, Gaussian, linearly separable, etc.", "startOffset": 28, "endOffset": 74}, {"referenceID": 4, "context": "This last result meshes well with the fact that speaker identification depends not so much on raw acoustic features, but on small deformations relative to a background pronunciation distribution (as encoded in i-vectors, Dehak et al. (2011)).", "startOffset": 221, "endOffset": 241}, {"referenceID": 7, "context": "In order to understand the nature of information encoding in a multi-layer network, it can be useful to inspect the hidden layers in details (Mohamed et al., 2012).", "startOffset": 141, "endOffset": 163}], "year": 2015, "abstractText": "We trained a Siamese network with multi-task same/different information on a speech dataset, and found that it was possible to share a network for both tasks without a loss in performance. The first task was to discriminate between two same or different words, and the second was to discriminate between two same or different talkers.", "creator": "LaTeX with hyperref package"}}}