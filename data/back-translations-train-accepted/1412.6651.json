{"id": "1412.6651", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Deep learning with Elastic Averaging SGD", "abstract": "We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameter vectors they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the theoretical analysis of the synchronous variant in the quadratic case and prove it achieves the highest possible asymptotic rate of convergence for the center variable. We additionally propose the momentum-based version of the algorithm that can be applied in both synchronous and asynchronous settings. An asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.", "histories": [["v1", "Sat, 20 Dec 2014 13:22:23 GMT  (2167kb,D)", "https://arxiv.org/abs/1412.6651v1", "submitted to iclr2015"], ["v2", "Mon, 29 Dec 2014 20:50:02 GMT  (2167kb,D)", "http://arxiv.org/abs/1412.6651v2", "submitted to iclr2015"], ["v3", "Mon, 5 Jan 2015 01:18:40 GMT  (4782kb,D)", "http://arxiv.org/abs/1412.6651v3", "submitted to iclr2015. The plots of ADOWNPOUR method in Figure 1 turned out to be the DOWNPOUR method. The current (3rd) version fixes this"], ["v4", "Wed, 25 Feb 2015 19:00:29 GMT  (1123kb,D)", "http://arxiv.org/abs/1412.6651v4", "submitted to iclr2015, version after rebuttal"], ["v5", "Wed, 29 Apr 2015 11:56:24 GMT  (1123kb,D)", "http://arxiv.org/abs/1412.6651v5", "submitted to iclr2015, workshop version"], ["v6", "Sat, 6 Jun 2015 00:20:58 GMT  (1289kb,D)", "http://arxiv.org/abs/1412.6651v6", "NIPS2015 Submission"], ["v7", "Sat, 8 Aug 2015 02:52:48 GMT  (1290kb,D)", "http://arxiv.org/abs/1412.6651v7", "NIPS2015 Submission(r)"], ["v8", "Sun, 25 Oct 2015 12:12:52 GMT  (3648kb,D)", "http://arxiv.org/abs/1412.6651v8", "NIPS2015 camera-ready version"]], "COMMENTS": "submitted to iclr2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["sixin zhang", "anna choromanska", "yann lecun"], "accepted": true, "id": "1412.6651"}, "pdf": {"name": "1412.6651.pdf", "metadata": {"source": "CRF", "title": "Deep learning with Elastic Averaging SGD", "authors": ["Sixin Zhang", "Anna Choromanska"], "emails": ["zsx@cims.nyu.edu", "achoroma@cims.nyu.edu", "yann@cims.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "One of the most difficult problems in the field of machine learning is the parallelism of the formation of large-scale models using a form of stochastic gradient descent (SGD) [1]. Attempts have been made to parallelise SGD-based training for a large number of CPUs, including the Google Distbelief system. But practical image recognition systems consist of large-scale revolutionary neural networks sitting on a few GPU cards [3, 4]. The biggest challenge is to develop parallel SGD algorithms to develop large-scale, deep learning models that result in significant acceleration for multiple GPU cards. In this paper, we present the elastic averaging SGD method (EASGD) and its variants."}, {"heading": "2 Problem setting", "text": "In this paper, we focus on the stochastic optimization problem of the following form x F (x): = E [f (x)], (1) where x is the model parameter to be estimated and vice versa is a random variable that follows the probability distribution P over x, so that F (x) = x (x) P (dB) follows. However, the optimization problem in Equation 1 can be reformulated as a follow-up parameter x1,..., xp, x [f (xi) = 1 E [f (xi)]] + 2 [xi] x) P (dB), (2), with everyone following the same distribution P (hence we assume that every worker can scan scan scan scan scan the entire dataset). In the paper, we point to xs as a local variable and we point to x [f (xi)] as a central variable. The problem with the equivalence of these two problems is the problem that the variables are not generally known in literature 11 and literature."}, {"heading": "3 EASGD update rule", "text": "The EASGD-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-"}, {"heading": "3.1 Asynchronous EASGD", "text": "We discussed the synchronous update of the EASGD algorithm in the previous section. In this section, we propose its asynchronous variant. Local workers are still responsible for updating the local variable xi's, while the master updates the middle variable x '. Each worker maintains his own clock ti, which starts at 0 and is increased by 1 after each stochastic gradient update xi's, as in Algorithm 1. The master performs an update whenever the local workers have finished their gradient updates, calling ourselves the communication period. As can be seen in Algorithm 1, the ith worker divides the local clock of the ith worker, communicates with the master and requests the current value of the central variable x \u2212 The worker then waits until the master sends back the requested parameter value, and calculates the elastic difference vix x x x) (this entire procedure is recorded in step a) in algorithm 1. The elastic difference is then sent back to the master."}, {"heading": "3.2 Momentum EASGD", "text": "Momentum EASGD (EAMSGD) is a variant of our algorithm 1 and is recorded in algorithm 2. It is based on the impulse scheme of Nesterov [24, 25, 26], in which the update of the local worker of the form included in Equation 3 is replaced by the following updatevit + 1 = \u03b4v i t \u2212 \u03b7git (xit + \u03b4vit) (7) xit + 1 = x i + v i t + 1 \u2212 \u03b7\u03c1 (xit \u2212 x \u0442t), where \u03b4 is the impulse term. Note that we are restoring the original EASGD algorithm. As we are interested in reducing the communication effort in the parallel computing environment, where the parameter vector is very large, we will examine the asynchronous EASGD algorithm and its dynamically-based variant in the relatively large regulation regime (less frequent communication) in the experimental section."}, {"heading": "4 Stability analysis of EASGD and ADMM in the round-robin scheme", "text": "In this section we will examine the stability of the asynchronous EASGD and ADMM methods in the circular scheme (20). We will first indicate the updates of both algorithms in this setting and then we will study their stability. We will show that in the one-dimensional quadratic case the ADMM algorithm can exhibit chaotic behavior, leading to exponential divergence. The analytical condition for the ADMM algorithm to be stable is still unknown, while for the EASGD algorithm it is very simple1.Analyzing the synchronous EASGD algorithm, including its convergence rate, and its average property, in the quadratic and strongly convex case, to the supplement.In our setting, the ADMM method [9, 27, 28] includes the solution of the following minimax problems 2, max."}, {"heading": "5 Experiments", "text": "In this section we compare the performance of EASGD and EAMSGD with the parallel method DOWNPOUR and the sequential method SGD, as well as their constant mean and impulse variants. \u2022 All parallel comparison methods are listed hereinafter: \u2022 DOWNPOUR [2], the pseudo-code for implementing DOWNPOUR in this essay is included in the supplement. \u2022 Momentum DOWNPOUR (MDOWNPOUR), where the Nesterov dynamic scheme is applied to updating the master (note that it is unclear how it should be applied to the local workers or to the case when the pseudo-code is included in the supplement. \u2022 A method we call ADOWNPOUR, where we calculate the average over time variable x: MUzt + 1 = (1 \u2212 \u03b1t + 1) aims at a variable learning rate."}, {"heading": "5.1 Experimental setup", "text": "For all our experiments, we use a GPU cluster connected to InfiniBand. Each node has 4 titanium GPU processors, with each local worker corresponding to a GPU processor. The master's mean variable is stored and updated on the centralized parameter server [2] 6.To describe the architecture of the Convolutionary Neural Network, we will first introduce notation. let (c, y) denotes the size of the input image at each level, where c is the number of color channels and y is both the horizontal and vertical dimension of the input. Let C denotes the fully connected Convolutionary Operator and let P \u2212 denotes the maximum Pooling Operator, D denotes the linear Operator with a failure rate of 0.5 and S denotes the linear Operator with a softmax output without linearity (AR)."}, {"heading": "5.2 Experimental results", "text": "For all experiments in this section we use EASGD with \u03b2 = 0.98, for all impulse \u03b2-based methods we set the impulse term \u03b4 = 0.99 and finally for MVADOWNPOUR we set the moving rate to \u03b1 = 0.001. We start with the experiment on the CIFAR dataset with p = 4 local workers running on a single computing node. For all methods we investigated the communication periods from the following sentence. = {1, 4, 16, 64} For comparison we also report on the performance of MSGD that exceeded SGD, ASGD and MVASGD as shown in Figure 6 in the supplement. For each method we examined a wide range of learning rates that were investigated in all experiments. Summarized in Table 1, 2, 3 in the supplement. The CIFAR experiment was conducted three times independently of the same initialization and for each method."}, {"heading": "6 Conclusion", "text": "In this paper we describe a new algorithm called EASGD and its variants for the formation of deep neural networks in the stochastic environment when the calculations are parallelized across multiple GPUs. Experiments show that this new algorithm quickly achieves an improvement in test errors compared to more common basic approaches such as DOWNPOUR and its variants. We show that our approach is very stable and plausible under communication constraints. We provide the stability analysis of the asynchronous EASGD in the round robin scheme and demonstrate the theoretical advantage of the method over ADMM. The different behavior of the EASGD algorithm compared to its momentary-based variant EAMSGD is fascinating and will be investigated in future work. 9For the ImageNet experiment, the training loss is measured using a subset of training data of size 50,000."}, {"heading": "Acknowledgments", "text": "The authors would like to thank R. Power, J. Li for the guidance on implementation, J. Bruna, O. Henaff, C. Farabet, A. Szlam, Y. Bachtin for the helpful discussion, P. L. Combettes, S. Bengio and the referees for valuable feedback."}, {"heading": "7 Additional theoretical results and proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Quadratic case", "text": "The analysis concentrates on the convergence of the mean variables to the local optimum. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "7.1.2 Condition in Equation 17", "text": "We are going to show that the number of workers who are able, is able, is able, is able to stay, is able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able to be able, to be able to be able to be able, to be able to be able to be able, to be able to be able to be able, to be able to be able to be able to be able, to be able to be able to be able, to be able to be able to be able to be able, to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able"}, {"heading": "7.2 Generalization to multidimensional case", "text": "The next problem (Lemma 7.2) shows that the EASGD algorithm achieves the highest possible convergence rate when we look at the double mean sequence (similar to [6]). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "7.3 Strongly convex case", "text": "We now extend the above evidence ideas to analyze the strongly convex case in which the noise gradient Git (x) = 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) \u2212 42.0 (x) (x)."}, {"heading": "8 Additional pseudo-codes of the algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 DOWNPOUR pseudo-code", "text": "Algorithm 3: DOWNPOUR: Processing by worker i and the master Input: learning rate \u03b7, communication period \u043d N Initialize: x = x is randomly initialized, xi = x, vi = 0, ti = 0 Repeatif (\u03c4 divides ti) then x-ig x-ig + vi xi-ig x-vi \u2190 0 end xi-\u03b7giti (xi) vi-\u03b7giti (xi) ti \u2190 ti + 1Bis in all eternity"}, {"heading": "8.2 MDOWNPOUR pseudo-code", "text": "Algorithm 4 shows the behavior of each local worker and algorithm 5 shows the behavior of the master. Algorithm 4: MDOWNPOUR: Processing by worker i Initialize: xi = x \"RepeatReceive x\" from the master: xi \u2190 x \"Compute gradient gi = gi (xi) Send gi to the masterUntil foreverAlgorithm 5: MDOWNPOUR: Processing by the master Input: Learning rate \u03b7, DynamikTerm \u043c Initialize: x = 0, RepeatReceive gi v\" v.... \""}, {"heading": "9 Experiments - additional material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Data preprocessing", "text": "For the ImageNet experiment, we enlarge each RGB image so that the smallest dimension is 256 pixels. In addition, we scale each pixel value to the interval [0, 1]. Then, we extract random sections (and their horizontal flips) of the size 3 x 221 x 221 pixels and present them to the network in mini-batches of the size 128. For the CIFAR experiment, we use the original RGB image of the size 3 x 32 x 32. As before, we scale each pixel value to the interval [0, 1]. Then, we extract random sections (and their horizontal flips) of the size 3 x 28 x 28 pixels and present them to the network in mini-batches of the size 128. Training and test losses and test errors are calculated only from the middle patch (3 x 28 x 28) for the CIFAR experiment and the middle patch (3 x 221 x 221) for the ImageNet experiment."}, {"heading": "9.2 Data prefetching (Sampling the dataset by the local workers)", "text": "The general parallel loading scheme for data on a single machine is as follows: We use k-CPUs, where k = 8 to load the data in parallel; each data loader reads from the mapped (mmap) file a portion of the raw c images (preprocessing was described in the previous subsection) and their labels (for CIFAR c = 512 and for ImageNet c = 64); for CIFAR, the maps file of each data loader contains the entire data set; for ImageNet, each map file contains different 1 / k fractions of the entire data set; a block of data is always sent by one of the data loaders to the first worker requesting the data; the next worker requesting the data from the same data loader receives the next chunk."}, {"heading": "9.3 Learning rates", "text": "In Table 1, we summarize the learning rates \u03b7 (we used constant learning rates) examined for each method shown in Figure 2. For all values of the p, the same set of learning rates was examined for each method. In Table 2, we summarize the initial rates we use for each method shown in Figure 4 (we used constant learning rates). For all values of the p, the same set of learning rates was examined for each method. We also used the rule of thumb to reduce the initial rate twice, the first time we divided it by 5 and the second time by 2, when we observed that the decrease in online prediction (training) loss saturation for each method was investigated."}, {"heading": "9.5 Dependence of the learning rate", "text": "This section discusses the dependence of the trade-off between exploration and exploitation on the learning rate. We compare the performance of EAMSGD and EASGD for different learning rates \u03b7 when p = 16 and \u03c4 = 10 are applied to the CIFAR experiment. In Figure 8, we observe that higher learning rates \u03b7 lead to better test performance for the EAMSGD algorithm, which can potentially be justified by maintaining higher fluctuations of the local workforce. We suspect that higher fluctuations lead to more exploration and, at the same time, force higher regulation. However, this picture appears to be reversed for the EASSGD algorithm, where higher learning rates affect the performance of the method and lead to overadjustment. Interestingly, in this experiment, the learning rate at which both the EASGD and EAMSGD algorithms achieved the best training performance resulted in the worst test performance."}, {"heading": "9.6 Dependence of the communication period", "text": "This section discusses the dependence of the trade-off between exploration and exploitation on the communication period. We observed in the CIFAR experiment that the EASGD algorithm exhibits a very similar convergence behavior when \u03c4 = 1 to even \u03c4 = 1000, while the EAMSGD can remain trapped at lower energy (loss) rates. This behavior of the EAMSGD is most likely due to the non-convexity of the objective function. Fortunately, it can be avoided by gradually decreasing the learning rate, i.e. increasing the penalty period, as shown in Figure 9. In contrast, the EASGD algorithm does not appear to be trapped along its entire trajectory. EASGD's performance is less sensitive to the extension of the communication period compared to EAMSGD, whereas for the EAMSGD the careful choice of the learning rate for large communication periods seems to be crucial."}, {"heading": "9.7 Breakdown of the wallclock time", "text": "Furthermore, in Table 4 we give the distribution of the total runtime for EASGD if \u03c4 = 10 (the time distribution for EAMSGD is almost identical) and DOWNPOUR if \u03c4 = 1 in computation time, data loading time and communication time of the parameters. For the CIFAR experiment, the reported time corresponds to the processing of 400 x 128 data samples, while for the ImageNet experiment it corresponds to the processing of 1024 x 128 data samples."}, {"heading": "9.8 Time speed-up", "text": "In Figure 10 and 11, we summarize the time of the wall clock required to achieve the same level of test error for all methods of the CIFAR and ImageNet experiment depending on the number of local workers. For CIFAR (Figure 10), we examined the following levels: {21%, 20%, 19%, 18%} and for the ImageNet (Figure 11), which we examined: {49%, 47%, 45%, 43%. If a method for a certain level of test error does not appear on the figure, this indicates that this method has never reached this level. For the CIFAR experiment, we note that the EASGD, DOWNPOUR and MDOWNPOUR methods take less time to reach a certain level of test error. We note that with a higher p, each of these methods does not necessarily take less time to achieve the same test error level, although the learning rate is selected for the methods used for the SAMRM, which are lower than the SAMRR values for the SAMRM."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>We study the problem of stochastic optimization for deep learning in the paral-<lb>lel computing environment under communication constraints. A new algorithm<lb>is proposed in this setting where the communication and coordination of work<lb>among concurrent processes (local workers), is based on an elastic force which<lb>links the parameters they compute with a center variable stored by the parameter<lb>server (master). The algorithm enables the local workers to perform more explo-<lb>ration, i.e. the algorithm allows the local variables to fluctuate further from the<lb>center variable by reducing the amount of communication between local workers<lb>and the master. We empirically demonstrate that in the deep learning setting, due<lb>to the existence of many local optima, allowing more exploration can lead to the<lb>improved performance. We propose synchronous and asynchronous variants of<lb>the new algorithm. We provide the stability analysis of the asynchronous vari-<lb>ant in the round-robin scheme and compare it with the more common parallelized<lb>method ADMM. We show that the stability of EASGD is guaranteed when a simple<lb>stability condition is satisfied, which is not the case for ADMM. We additionally<lb>propose the momentum-based version of our algorithm that can be applied in both<lb>synchronous and asynchronous settings. Asynchronous variant of the algorithm<lb>is applied to train convolutional neural networks for image classification on the<lb>CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm<lb>accelerates the training of deep architectures compared to DOWNPOUR and other<lb>common baseline approaches and furthermore is very communication efficient.", "creator": "LaTeX with hyperref package"}}}