{"id": "1412.6980", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Adam: A Method for Stochastic Optimization", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based an adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also ap- propriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice when experimentally compared to other stochastic optimization methods.", "histories": [["v1", "Mon, 22 Dec 2014 13:54:29 GMT  (280kb,D)", "http://arxiv.org/abs/1412.6980v1", "initial"], ["v2", "Sat, 17 Jan 2015 20:26:06 GMT  (283kb,D)", "http://arxiv.org/abs/1412.6980v2", "initial"], ["v3", "Fri, 27 Feb 2015 21:04:48 GMT  (289kb,D)", "http://arxiv.org/abs/1412.6980v3", "revision2"], ["v4", "Tue, 3 Mar 2015 17:51:27 GMT  (289kb,D)", "http://arxiv.org/abs/1412.6980v4", null], ["v5", "Thu, 23 Apr 2015 16:46:07 GMT  (289kb,D)", "http://arxiv.org/abs/1412.6980v5", null], ["v6", "Tue, 23 Jun 2015 19:57:17 GMT  (958kb,D)", "http://arxiv.org/abs/1412.6980v6", null], ["v7", "Mon, 20 Jul 2015 09:43:23 GMT  (519kb,D)", "http://arxiv.org/abs/1412.6980v7", "Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015"], ["v8", "Thu, 23 Jul 2015 20:27:47 GMT  (526kb,D)", "http://arxiv.org/abs/1412.6980v8", "Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015"], ["v9", "Mon, 30 Jan 2017 01:27:54 GMT  (490kb,D)", "http://arxiv.org/abs/1412.6980v9", "Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015"]], "COMMENTS": "initial", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["diederik p kingma", "jimmy ba"], "accepted": true, "id": "1412.6980"}, "pdf": {"name": "1412.6980.pdf", "metadata": {"source": "CRF", "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION", "authors": ["Diederik P. Kingma", "Jimmy Lei Ba"], "emails": ["dpkingma@uva.nl", "jimmy@psi.utoronti.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "Many problems in these areas can be considered as the optimization of some scalar parameterized objective functions, which require a relatively efficient method of optimization, since the calculation of partial derivatives of the first order can be made more efficient in this case by merely evaluating the function. Often, objective functions are stochastical. For example, many objective functions are composed of a sum of subfunctions that are evaluated on different subsamples of data; in this case, optimization can be made more efficient by taking gradient steps. Individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD has proven to be an efficient and effective optimization method that has been central to many machine learning successes, such as recent advances in deep learning processes."}, {"heading": "2 ALGORITHM", "text": "See algorithm 1 for the pseudo-code of our proposed Adam algorithm. Let f (\u03b8) be a noisy objective function: a stochastic scalar function that can be distinguished from other parameters. \u2212 We are interested in minimizing the expected value of this function, E [f (\u03b8)] w.r.t., of its parameters. \u2212 With f1 (\u03b8),..., fT (\u03b8) we denote the realizations of the stochastic function at subsequent points in time 1,..., T. Stochasticity could, for example, come from the evaluation of randomly selected sub-samples (minibatches) of data points or from inherent function noise. \u2212 With gt = occurrence (\u03b8) we denote the gradient, i.e. the vector of partial derivatives of ft, w.r.t that change at points in time (points in time)."}, {"heading": "2.1 ADAM\u2019S UPDATE RULE", "text": "An important feature of Adam's updating rule is the careful selection of the step sizes. The effective step taken in the parameter space approximately in the time span is \u00b7 t = \u03b1 \u00b7 m \u00b7 t / \u221a v. The effective step size has strong upper and lower limits: \u2212 \u03b1 \u00b7 \u03b21 / \u03b22 \u2264 t \u2264 + \u03b1 \u00b7 \u03b21 / \u03b22. The extremities \u00b1 \u00b7 \u03b1 \u00b7 \u03b21 / \u03b22 happen only in the most serious case of the step size: if a gradient has been zero in all the time span except in the current time span. For less sparse cases, the effective step size will be smaller. If \u03b21 = \u03b22 we have this step size, we will not have this step size."}, {"heading": "3 INITIALIZATION BIAS CORRECTION", "text": "As explained in section 2, Adam uses the terms of initialization distortion. Here, we derive the term for the second instant estimation; the derivative for the first moment is completely analogous. Let g be the gradient of the stochastic object f, and we would like to estimate its second raw moment (uncentered variance) using an exponential moving average of the squared gradient \u03b22. Let us initialize the exponential moving average of the squared gradient \u03b22, where g1,..., gT may be the gradients at subsequent times, each drawing from an underlying gradient distribution gt p (gt). Let us initialize the exponential moving average as v0 = 0 (a vector of the nulars). First, it should be noted that the update of the exponential moving average vt = \u03b22 \u00b7 g2t + (1 \u2212 \u03b22) \u00b7 vt \u2212 1 (where the quadrant is correct)."}, {"heading": "4 CONVERGENCE ANALYSIS", "text": "We analyze the convergence of Adam under the online learning framework proposed in Zinkevich (2003). We get an arbitrary, unknown sequence of convex cost functions f1, f2,..., fT. At any time we have to make a prediction for the parameter \u03b8t and evaluate it against a previously unknown cost function. Since we do not know the nature of the sequence in advance, we evaluate our algorithm using regret, which is the sum of the total previous difference between the online prediction ft (\u03b8t) and the best fixed point parameter ft (\u03b8) for all previous steps. Specifically, we have the regret as: R (T) = T = 1 [ft (\u03b8t) \u2212 ft (\u03b8) \u2212 ft (Adam) - FT = arg min."}, {"heading": "5 RELATED WORK", "text": "Optimization methods that are directly related to Adam include RProp Riedmiller & Braun (1992), RMSProp Tieleman & Hinton (2012), Graves et al. (2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both methods based on estimating the curvature from firststored information. Sum-of-Functions Optimizer (SFO) Sohl-Dickstein et al. (2014) is a quasi Newton method based on minibatches, but (unlike Adam) has a linear storage requirement in the number of data that is often available more than on memory-limited systems such as a GPU."}, {"heading": "6 EXPERIMENTS", "text": "To evaluate the proposed method empirically, we examined various popular models of machine learning, including logistic regression, fully networked neural networks, and deep Convolutionary Neural Networks. Using large models and data sets, we show how well Adam can solve practical deep learning problems. We use the same parameter initializations when comparing different optimization algorithms. Hyperparameters such as learning rate and impulse are searched for over a dense network, and the results are reported using the best hyperparameter settings."}, {"heading": "6.1 EXPERIMENT: LOGISTIC REGRESSION", "text": "Logistic regression classifies the class name directly on the image vectors of the 784 dimension. We compare Adam with accelerated SGD with Nesterov dynamics and AdaGrad with mini batch size of 128. According to Figure 1, we have found that Adam has a convergence similar to SGD with dynamics and both converge faster than AdaGrad. As discussed in Duchi et al. (2011), AdaGrad can efficiently handle sparse features and sequences as one of its most important theoretical outcomes, whereas SGD is low when learning rare features. We investigate the sparse feature problem using the IMDB Movie Rating Data Set Adag al. (2011), as we pre-process the IMDB movie as word reviews (where SGD is low when learning rare features."}, {"heading": "6.2 EXPERIMENT: MULTI-LAYER NEURAL NETWORKS", "text": "Multi-layer neural networks are powerful models with non-convex objective functions. Although our convergence analysis is not applicable to non-convex problems, we have found empirically that Adam often outperforms other methods in such cases. In our experiments, we have made model decisions consistent with previous publications in this area; a neural network model with two fully connected hidden layers, each containing 1000 hidden units, and ReLU activation will be used for this experiment with mini-batch size of 128. First, we investigate various optimizers that use the standard deterministic cross-entropy objective function with L2 weight decay on the parameters to prevent overmatch. The Sum of Functions (SFO) method (Sohl-Dickstein et al. (2014) is a recently proposed quasi-Newton method that works with minibatches of data and has shown good performance in optimizing multi-layer networks."}, {"heading": "6.3 EXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS", "text": "Unlike most fully networked neural networks, weight distribution in CNNs results in vastly different gradients in different layers. A lower learning rate for the folding layers is commonly applied in practice when applying SGD. We show the effectiveness of Adam in deep CNNs. Our CNN architecture has three alternating levels of 5x5 folding filters and 3x3 max pooling with steps of 2, followed by a fully connected layer of 1000 Reflected Linear Hidden Units (ReLUs). The input image is pre-processed by brightening, and failure noise is applied to the input layer and fully connected layers. Interestingly, both Adam and AdaGrad follow each other in the initial phase of training in Figure 3 (left), Adam and SGD as the first layer at a slower speed than in the input layer and fully connected layers."}, {"heading": "6.4 EXPERIMENT: BIAS-CORRECTION TERM", "text": "Empirically, we also evaluate the effect of the terms of bias correction explained in sections 2 and 3. Eliminating the terms of bias correction leads to a dynamic version of RMS Prop Tieleman & Hinton (2012) in section 5. We vary the \u03b21 and \u03b22 if we use a variable autoencoder (VAE) with the same architecture as in Kingma & Welling (2013) with a single hidden layer with 500 hidden units with soft plus nonlinearity and a 50-dimensional spherical gaussian latent variable. We iterated over a wide range of hyperparameter selections, i.e. \u03b21: (1, 0.1) and \u03b22: (0.001, 0.001, 0.001) and log10: (\u03b1)."}, {"heading": "7 CONCLUSION", "text": "We have introduced a simple and computationally efficient algorithm for gradient-based optimization of stochastic lens functions that targets machine learning problems with large datasets and / or high-dimensional parameter spaces. It combines the advantages of two recently popular optimization methods: AdaGrad's ability to handle sparse gradients and RMSProp's ability to handle non-stationary objects. It is easy to implement and requires little memory. Experiments confirm the analysis of convergence rates for convex problems. Empirically, we have found Adam to be robust and well suited to a wide range of non-convex optimization problems in machine learning."}, {"heading": "8 ACKNOWLEDGMENTS", "text": "Without the support of Google Deepmind, the associated collaborations and interesting conversations, this work would probably not have taken place. We would like to express our special thanks to Tom Schaul for coining the name Adam."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Amari", "Shun-Ichi"], "venue": "Neural computation,", "citeRegEx": "Amari and Shun.Ichi.,? \\Q1998\\E", "shortCiteRegEx": "Amari and Shun.Ichi.", "year": 1998}, {"title": "Recent advances in deep learning for speech research at microsoft", "author": ["Deng", "Li", "Jinyu", "Huang", "Jui-Ting", "Yao", "Kaisheng", "Yu", "Dong", "Seide", "Frank", "Seltzer", "Michael", "Zweig", "Geoff", "He", "Xiaodong", "Williams", "Jason"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Auto-Encoding Variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In The 2nd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas", "Andrew L", "Daly", "Raymond E", "Pham", "Peter T", "Huang", "Dan", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Rprop - a fast adaptive learning algorithm", "author": ["Riedmiller", "Martin", "Braun", "Heinrich"], "venue": "Technical report, Proc. of ISCIS VII), Universitat,", "citeRegEx": "Riedmiller et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 1992}, {"title": "No more pesky learning rates", "author": ["Schaul", "Tom", "Zhang", "Sixin", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1206.1106,", "citeRegEx": "Schaul et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2012}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["Sohl-Dickstein", "Jascha", "Poole", "Ben", "Ganguli", "Surya"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "Technical report,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Fast dropout training", "author": ["Wang", "Sida", "Manning", "Christopher"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": null, "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.", "startOffset": 168, "endOffset": 187}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al.", "startOffset": 168, "endOffset": 237}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al.", "startOffset": 168, "endOffset": 260}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al. (2013). Objectives may also have other sources of noise than data subsampling, such as dropout Hinton et al.", "startOffset": 168, "endOffset": 282}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al. (2013). Objectives may also have other sources of noise than data subsampling, such as dropout Hinton et al. (2012b) regularization.", "startOffset": 168, "endOffset": 392}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al. (2013). Objectives may also have other sources of noise than data subsampling, such as dropout Hinton et al. (2012b) regularization. For all such noisy objectives, efficient stochastic optimization techniques are required. The focus of this paper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In these cases, higher-order optimization methods are ill-suited, and discussion in this paper will be restricted to first-order methods. We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients and requires little memory. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation. Our method is designed to combine the advantages of two recently popular methods: AdaGrad Duchi et al. (2011), which works well with sparse gradients, and RMSProp Tieleman & Hinton (2012), which works well in on-line and non-stationary settings; important connections to these and other stochastic optimization methods are clarified in section 5.", "startOffset": 168, "endOffset": 1191}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al. (2013). Objectives may also have other sources of noise than data subsampling, such as dropout Hinton et al. (2012b) regularization. For all such noisy objectives, efficient stochastic optimization techniques are required. The focus of this paper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In these cases, higher-order optimization methods are ill-suited, and discussion in this paper will be restricted to first-order methods. We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients and requires little memory. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation. Our method is designed to combine the advantages of two recently popular methods: AdaGrad Duchi et al. (2011), which works well with sparse gradients, and RMSProp Tieleman & Hinton (2012), which works well in on-line and non-stationary settings; important connections to these and other stochastic optimization methods are clarified in section 5.", "startOffset": 168, "endOffset": 1269}, {"referenceID": 2, "context": "2 in Duchi et al. (2011). Their results of the expected value of E \u2211d i=1 \u2016g1:T,i\u20162 also apply to Adam.", "startOffset": 5, "endOffset": 25}, {"referenceID": 2, "context": "Optimization methods bearing a direct relation to Adam include RProp Riedmiller & Braun (1992), RMSProp Tieleman & Hinton (2012); Graves et al. (2013) and AdaGrad Duchi et al.", "startOffset": 130, "endOffset": 151}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below.", "startOffset": 19, "endOffset": 39}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information.", "startOffset": 19, "endOffset": 153}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information.", "startOffset": 19, "endOffset": 180}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information. The Sum-of-Functions Optimizer (SFO) Sohl-Dickstein et al. (2014) is a quasiNewton method based on minibatches, but (unlike Adam) has memory requirements linear in the number of minibatches of data, which is often more than available on memory-constrained systems such as a GPU.", "startOffset": 19, "endOffset": 323}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information. The Sum-of-Functions Optimizer (SFO) Sohl-Dickstein et al. (2014) is a quasiNewton method based on minibatches, but (unlike Adam) has memory requirements linear in the number of minibatches of data, which is often more than available on memory-constrained systems such as a GPU. Like natural gradient descent (NGD) Amari (1998), Adam employs a preconditioner that adapts to the geometry of the data, since v\u0302t is an approximation to the diagonal of the Fisher information matrix Pascanu & Bengio (2013); however, Adam\u2019s preconditioner (like AdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square root of the inverse of the diagonal Fisher information matrix approximation.", "startOffset": 19, "endOffset": 585}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information. The Sum-of-Functions Optimizer (SFO) Sohl-Dickstein et al. (2014) is a quasiNewton method based on minibatches, but (unlike Adam) has memory requirements linear in the number of minibatches of data, which is often more than available on memory-constrained systems such as a GPU. Like natural gradient descent (NGD) Amari (1998), Adam employs a preconditioner that adapts to the geometry of the data, since v\u0302t is an approximation to the diagonal of the Fisher information matrix Pascanu & Bengio (2013); however, Adam\u2019s preconditioner (like AdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square root of the inverse of the diagonal Fisher information matrix approximation.", "startOffset": 19, "endOffset": 760}, {"referenceID": 3, "context": "A version with momentum has sometimes been used Graves et al. (2013). RMSProp lacks a biascorrection term; this matters most in case of a small value \u03b22 (required in case of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and often divergence, as we also empirically demonstrate in section 6.", "startOffset": 48, "endOffset": 69}, {"referenceID": 2, "context": "AdaGrad: An algorithm that works well for sparse gradients is AdaGrad Duchi et al. (2011). Its basic version updates parameters as \u03b8t+1 = \u03b8t \u2212 \u03b1 \u00b7 gt/ \u221a\u2211t i=1 g 2 t .", "startOffset": 70, "endOffset": 90}, {"referenceID": 2, "context": "As discussed in Duchi et al. (2011), AdaGrad can efficiently deal with sparse features and gradients as one of its main theoretical results whereas SGD is low at learning rare features.", "startOffset": 16, "endOffset": 36}, {"referenceID": 2, "context": "As discussed in Duchi et al. (2011), AdaGrad can efficiently deal with sparse features and gradients as one of its main theoretical results whereas SGD is low at learning rare features. We examine the sparse feature problem using IMDB movie review dataset from Maas et al. (2011). We pre-process the IMDB movie reviews into bag-of-words (BoW) feature vectors including the first 10,000 most frequent words.", "startOffset": 16, "endOffset": 280}, {"referenceID": 2, "context": "As discussed in Duchi et al. (2011), AdaGrad can efficiently deal with sparse features and gradients as one of its main theoretical results whereas SGD is low at learning rare features. We examine the sparse feature problem using IMDB movie review dataset from Maas et al. (2011). We pre-process the IMDB movie reviews into bag-of-words (BoW) feature vectors including the first 10,000 most frequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As suggested in Wang & Manning (2013), 50% dropout noise can be applied to the BoW features during training to prevent over-fitting.", "startOffset": 16, "endOffset": 519}, {"referenceID": 12, "context": "The sum-of-functions (SFO) methodSohl-Dickstein et al. (2014) is a recently proposed quasi-Newton method that works with minibatches of data and has shown good performance on optimization of multi-layer neural networks.", "startOffset": 33, "endOffset": 62}, {"referenceID": 12, "context": "We compare with the sum-of-functions (SFO) optimizer Sohl-Dickstein et al. (2014)", "startOffset": 53, "endOffset": 82}], "year": 2014, "abstractText": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based an adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice when experimentally compared to other stochastic optimization methods.", "creator": "LaTeX with hyperref package"}}}