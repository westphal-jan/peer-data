{"id": "1412.7009", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Generative Class-conditional Autoencoders", "abstract": "Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain. The transition operator is typically unimodal, which limits its capacity to model complex data. In order to perform efficient sampling from conditional distributions, we extend this work, both theoretically and algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is able to generate convincing class-conditional samples when trained on both the MNIST and TFD datasets.", "histories": [["v1", "Mon, 22 Dec 2014 14:57:05 GMT  (2522kb,D)", "http://arxiv.org/abs/1412.7009v1", null], ["v2", "Sat, 28 Feb 2015 00:16:55 GMT  (2523kb,D)", "http://arxiv.org/abs/1412.7009v2", null], ["v3", "Thu, 9 Apr 2015 01:54:33 GMT  (2523kb,D)", "http://arxiv.org/abs/1412.7009v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["jan rudy", "graham taylor"], "accepted": true, "id": "1412.7009"}, "pdf": {"name": "1412.7009.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Graham Taylor"], "emails": ["jrudy@uoguelph.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "In the field of deep neural networks, purely monitored models formed on massive marked datasets have attracted a lot of attention in recent years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014). However, recent work has rekindled interest in generative data models (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013; Kingma & Welling, 2014; Goodfellow et al., 2014a).The recently proposed sampling method for denoizing autoencoders (Bengio et al., 2013) and their generalization to generative stochastic networks (Bengio & Thibodeau-Laufer, 2013) presents a novel training method that is distributable in a way, rather than attempting to maximize the probability of data within the model."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 AUTOENCODERS", "text": "An autoencoder is a forward-facing neural network that aims to minimize the reconstruction error of an input data vector via a latent representation, which can be interpreted as the composition of two learned functions, the encoder function f and the decoder function g. The encoder function f is a mapping of the input space to the representation space. Formally, the input vector is x-RnX, ar Xiv: 141 2,70 09v1 [cs.NE] 2 2D ec2 014Input weights W-RnH \u00b7 nX and hidden distortions bh-RnH, f (x) = sH (Wx + b) (1), where nH is the dimension of the hidden representation and sH is an activation function. Activation is a non-linear function, often from the sigmoidal family x-x."}, {"heading": "2.2 DENOISING AUTOENCODERS", "text": "If the dimension of the hidden representation nH is smaller than the dimension of the data space nX, the learning method encourages the model to learn the underlying structure of the data. Data representation can use the structure to compress the data to less dimensions than the original space. As such, any dimension of the representation space can be interpreted as a useful feature of the data. If, for example, you train on images of handwritten digits, these features can be interpreted as a stroke. However, if the autoencoder can achieve a perfect reconstruction without learning useful features in the data by simply learning the identity function, regulation is essential. Among the different types of regulated autoencoders, the denoizing autoencoder (DAE) can achieve a perfect reconstruction without learning the useful features in the data."}, {"heading": "2.3 DENOISING AUTOENCODERS AS GENERATIVE MODELS", "text": "Although DAEs are useful as a means of pre-training discriminatory models, especially when subsequently stacked into deep models (Vincent et al., 2010), the recent work of Bengio et al. (2013) has shown that DAEs and their variants locally characterize the data that generate density, which is an important link between DAEs and probabilistic generative models. We define observed data x such that x \u0445 P (x), where P (x) is the true data distribution, and we define C (x) as the conditional distribution of the corruption process. If such models are trained using a loss function that can be interpreted as log liquidity by predicting x given capabilities, the model will learn the conditional distribution of the data (x) (where the parameters of the model are represented).To generate samples from the model, simply form a Markov chain that alternately produces samples from learned distribution and corruption distribution."}, {"heading": "2.4 GATED AUTOENCODERS", "text": "rE \"s tis rf\u00fc ide rf\u00fc \u00fc \u00fc the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the r the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the ro the rf the rf the rf the rf the rf the rf the ro the rf the r"}, {"heading": "3 GATED AUTOENCODERS AS CLASS-CONDITIONAL GENERATIVE MODELS", "text": "Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure for-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Proposition-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure-Procedure) Procedure-Procedure for the Procedure-Procedure-Procedure-Procedure-Procedure) Procedure-Procedure-Procedure-Procedure-Procedure-Pro"}, {"heading": "4 EXPERIMENTS", "text": "We show the generative properties of the class GAE model on two datasets: binarized MNIST (LeCun & Cortes, 1998) and the Toronto Face Database (TFD) (Suskind et al., 2010). However, the MNIST database consists of 60,000 training examples and 10,000 test examples. Pixel intensities of 28 x 28 images each were scaled to [0, 1] and thresholds of 0.5, 1}. For the MNIST datasets, 1024 factors and 512 hidden units were trained, the hidden units were reactivated while the visible activation was the logistical function (i.e. sigmoid)."}, {"heading": "5 CONCLUSION", "text": "The class-related GAE can be interpreted as a generative model similar to the PCS. In fact, the GAE resembles the learning of a separate PCS model for each class, but with a significant weight distribution between the models. In this light, gating acts as a means of modulating the weights of the model depending on the class name. Therefore, the theoretical and practical considerations applied to PCS as generative models can also be applied to gated models. Future work will apply these techniques to richer conditional distributions, such as the task of image marking, as examined by Mirza & Osindero (2014)."}], "references": [{"title": "Gated autoencoders with tied input weights", "author": ["Alain", "Droniou", "Olivier", "Sigaud"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Alain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "\u00c9ric"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Phone recognition with the mean-covariance restricted boltzmann machine", "author": ["Dahl", "George E", "Marc\u2019Aurelio Ranzato", "Abdel-rahman Mohamed", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Dahl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2010}, {"title": "Binary coding of speech spectrograms using a deep auto-encoder", "author": ["Deng", "Li", "Seltzer", "Michael L", "Yu", "Dong", "Acero", "Alex", "Mohamed", "Abdel-Rahman", "Hinton", "Geoffrey E"], "venue": "In Interspeech,", "citeRegEx": "Deng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2010}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["Goodfellow", "Ian J", "Bulatov", "Yaroslav", "Ibarz", "Julian", "Arnoud", "Sacha", "Shet", "Vinay"], "venue": "In ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning to relate images. Pattern Analysis and Machine Intelligence", "author": ["Memisevic", "Roland"], "venue": "IEEE Transactions on,", "citeRegEx": "Memisevic and Roland.,? \\Q2013\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2013}, {"title": "Conditional generative adversarial nets", "author": ["Mirza", "Mehdi", "Osindero", "Simon"], "venue": "In NIPS 2014 Workshop on Deep Learning,", "citeRegEx": "Mirza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mirza et al\\.", "year": 2014}, {"title": "Multimodal transitions for generative stochastic networks", "author": ["Ozair", "Sherjil", "Yao", "Li", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1312.5578,", "citeRegEx": "Ozair et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ozair et al\\.", "year": 2013}, {"title": "The toronto face database", "author": ["Susskind", "Josh M", "Anderson", "Adam K", "Hinton", "Geoffrey E"], "venue": "Department of Computer Science,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain.", "startOffset": 15, "endOffset": 36}, {"referenceID": 3, "context": "In the field of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014).", "startOffset": 152, "endOffset": 263}, {"referenceID": 4, "context": "In the field of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014).", "startOffset": 152, "endOffset": 263}, {"referenceID": 8, "context": "In the field of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014).", "startOffset": 152, "endOffset": 263}, {"referenceID": 15, "context": "In the field of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014).", "startOffset": 152, "endOffset": 263}, {"referenceID": 1, "context": "However, recent work has rekindled interest in generative models of data (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013; Kingma & Welling, 2014; Goodfellow et al., 2014a).", "startOffset": 73, "endOffset": 177}, {"referenceID": 1, "context": "The recently proposed sampling procedure for denoising autoencoders (Bengio et al., 2013) and their generalization to Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) presents a novel training procedure which, instead of attempting to maximize the likelihood of the data under the model, amounts to learning the transition operator of a Markov chain (Bengio et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 1, "context": ", 2013) and their generalization to Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) presents a novel training procedure which, instead of attempting to maximize the likelihood of the data under the model, amounts to learning the transition operator of a Markov chain (Bengio et al., 2013).", "startOffset": 284, "endOffset": 305}, {"referenceID": 1, "context": "Although these models have shown both theoretically and empirically to have the capacity to model the underlying data generating distribution, the unimodal transition operator learned in (Bengio et al., 2013) and (Bengio & Thibodeau-Laufer, 2013) limits the types of distributions that can be modeled successfully.", "startOffset": 187, "endOffset": 208}, {"referenceID": 12, "context": "One way to address this issue is by adopting an alternative generative model such as the Neural Autoregressive Density Estimator (NADE) as the output distribution of the transition operator (Ozair et al., 2013).", "startOffset": 190, "endOffset": 210}, {"referenceID": 1, "context": "However, recent work has rekindled interest in generative models of data (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013; Kingma & Welling, 2014; Goodfellow et al., 2014a). The recently proposed sampling procedure for denoising autoencoders (Bengio et al., 2013) and their generalization to Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) presents a novel training procedure which, instead of attempting to maximize the likelihood of the data under the model, amounts to learning the transition operator of a Markov chain (Bengio et al., 2013). Although these models have shown both theoretically and empirically to have the capacity to model the underlying data generating distribution, the unimodal transition operator learned in (Bengio et al., 2013) and (Bengio & Thibodeau-Laufer, 2013) limits the types of distributions that can be modeled successfully. One way to address this issue is by adopting an alternative generative model such as the Neural Autoregressive Density Estimator (NADE) as the output distribution of the transition operator (Ozair et al., 2013). In this paper, we propose a simpler approach. When labeled data is available, we can use the label information in order to carve up the landscape of the data distribution. This work begins by presenting an overview of related work, including a treatment of autoencoders, denoising autoencoders, autoencoders as generative models and gated autoencoders. Next, we propose a class-conditional gated autoencoder along with training and sampling procedures based on the work of Bengio et al. (2013). Finally, we present experimental results of our model on two image-based datasets.", "startOffset": 74, "endOffset": 1589}, {"referenceID": 16, "context": "Among the various kinds of regularized autoencoders, the denoising autoencoder (DAE) (Vincent et al., 2008) is among the most popular and well-understood.", "startOffset": 85, "endOffset": 107}, {"referenceID": 16, "context": "Apart from preventing the model to learn a trivial representation of the input by acting as a form or regularization, the DAE can be interpreted as a means of learning the manifold of the underlying data generating distribution (Vincent et al., 2008).", "startOffset": 228, "endOffset": 250}, {"referenceID": 16, "context": "Under this interpretation the hidden representation can be interpreted as a coordinate system of manifolds (Vincent et al., 2008).", "startOffset": 107, "endOffset": 129}, {"referenceID": 17, "context": "Although DAEs are useful as a means of pre-training discriminative models, especially when stacked to form deep models (Vincent et al., 2010), recent work by Bengio et al.", "startOffset": 119, "endOffset": 141}, {"referenceID": 1, "context": ", 2010), recent work by Bengio et al. (2013) has shown that DAEs", "startOffset": 24, "endOffset": 45}, {"referenceID": 1, "context": "In order to generate samples from the model, one simply forms a Markov chain which alternately samples from learned distribution and the corruption distribution (Bengio et al., 2013).", "startOffset": 161, "endOffset": 182}, {"referenceID": 1, "context": "3, the asymptotic distribution of the generated samples converges to the datagenerating distribution (Bengio et al., 2013).", "startOffset": 101, "endOffset": 122}, {"referenceID": 1, "context": "A more efficient procedure called \u201cwalkback training\u201d is described in (Bengio et al., 2013) and bears resemblance to contrastive divergence training of restricted Boltzman machines.", "startOffset": 70, "endOffset": 91}, {"referenceID": 1, "context": "The sampling procedure proposed by Bengio et al. (2013) for classical denoising autoencoders can also be applied to a GAE.", "startOffset": 35, "endOffset": 56}, {"referenceID": 1, "context": "Bengio et al. (2013) provide a proof of the following theorem: Theorem 1.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "The arguments for consistency and ergoticity can also be made in the same manner as those in (Bengio et al., 2013).", "startOffset": 93, "endOffset": 114}, {"referenceID": 13, "context": "We demonstrate the generative properties of the class-conditional GAE model on two datasets: binarized MNIST (LeCun & Cortes, 1998) and the Toronto Face Database (TFD) (Susskind et al., 2010).", "startOffset": 168, "endOffset": 191}, {"referenceID": 14, "context": "For optimization, the model was trained using Nesterov\u2019s accelerated gradient (Sutskever et al., 2013) with a parameter of 0.", "startOffset": 78, "endOffset": 102}], "year": 2017, "abstractText": "Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain. The transition operator is typically unimodal, which limits its capacity to model complex data. In order to perform efficient sampling from conditional distributions, we extend this work, both theoretically and algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is able to generate convincing class-conditional samples when trained on both the MNIST and TFD datasets.", "creator": "LaTeX with hyperref package"}}}