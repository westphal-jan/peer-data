{"id": "1412.7028", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Joint RNN-Based Greedy Parsing and Word Composition", "abstract": "This paper introduces a greedy parser based on neural networks, which leverages a new compositional sub-tree representation. The greedy parser and the compositional procedure are jointly trained, and tightly depends on each-other. The composition procedure outputs a vector representation which sumarrizes syntactically (parsing tags) and semantically (words) sub-trees. Composition and tagging is achieved over continuous (word or tag) representations, and recurrent neural networks. We reach F1 performance on par with well-known existing parsers, while having the advantage of speed, thanks to the greedy nature of the parser.", "histories": [["v1", "Mon, 22 Dec 2014 15:40:31 GMT  (257kb,D)", "https://arxiv.org/abs/1412.7028v1", "Under review as a conference paper at ICLR 2015"], ["v2", "Thu, 25 Dec 2014 17:39:39 GMT  (260kb,D)", "http://arxiv.org/abs/1412.7028v2", "Under review as a conference paper at ICLR 2015"], ["v3", "Thu, 8 Jan 2015 15:04:34 GMT  (260kb,D)", "http://arxiv.org/abs/1412.7028v3", "Under review as a conference paper at ICLR 2015"], ["v4", "Fri, 10 Apr 2015 21:57:49 GMT  (273kb,D)", "http://arxiv.org/abs/1412.7028v4", "Published as a conference paper at ICLR 2015"]], "COMMENTS": "Under review as a conference paper at ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["jo\\\"el legrand", "ronan collobert"], "accepted": true, "id": "1412.7028"}, "pdf": {"name": "1412.7028.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jo\u00ebl Legrand", "Ronan Collobert"], "emails": ["joel.legrand@idiap.ch", "ronan@collobert.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In the linguistic world, Chomsky (1956) first formally introduced the parser task by defining the syntax of natural language as a set of context-free grammar rules (in the sense that it can indicate different interpretations of a given sentence). In this respect, syntactic parsing quickly became a key task in computer-aided linguistics. A parser tree not only carries syntax information, but could also embed some semantic information (in the sense that it can show different interpretations of a given sentence). In this regard, it was used as a key task in computer-aided linguistics."}, {"heading": "2 RELATED WORK", "text": "A wide range of parsers model syntactical grammar by computing statistics of simple grammar rules (via parsing tags) that occur in a training corpus. However, many linguistic ambiguities cannot be captured with simple tag-based PCFG rules. A key element of the success of PCFGs is the refinement of rules with a word lexicon. This is usually achieved by appending PCFGs with lexical information called a header. There are several variants of headwords, but they all rely on a deterministic process that uses clever linguistic knowledge. Parsing inferences is achieved largely by simple bottom-up chart parsers (Kasami, 1965; Earley, 1970; Kay, 1986). These methods represent a classic dilemma where the PCG rules are refined carefully enough to avoid refining the PCG rules."}, {"heading": "2.1 STATE-OF-THE-ART", "text": "The discriminatory approaches of Henderson (2004); Charniak & Johnson (2005) exceed the standard PCFG-based generative parsers, but only through discriminatory re-evaluation of the K-best predicted trees resulting from a generative parser. To our knowledge, the state of the art in syntactic parsing is still maintained by McClosky et al. (2006), which uses discriminatory re-evaluation and self-training over unlabeled corpus: A re-marker is trained using a generative model that is then used to label the unlabeled dataset, and the original parser is then retrained with this new \"labeled\" corpus. Petrov & Klein (2007) introduced a method to automatically replicate PCFG rules by splitting them iteratively, using an efficient coarse to fine such a method to speed up the decoding process."}, {"heading": "2.2 GREEDY PARSING", "text": "Many discriminatory parsers pursue a greedy strategy due to the lack (or intractability) of a global tree score for an entire derivative path that would combine independent node decisions; adopting a greedy strategy that maximizes local scores for individual decisions is then a solution worth exploring; one of the first successful discriminatory parsers (Ratnaparkhi, 1999) was based on MaxEnt classifiers (trained on a large number of different characteristics) and operated a greedy shiftreduction strategy; Henderson (2003) introduced a generative left-corner parser that approximated the likelihood of derivation taking into account derivative history with a simple synchronous network (SNN), a neural network specifically designed for processing structures; Turian & Melamed (2006) later proposed a greedy bottom-up algorithm that follows a left-right derivative or left-right derivative."}, {"heading": "2.3 PARSING WITH RECURRENT NEURAL NETWORKS", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "3 GREEDY RNN PARSING", "text": "Our approach is a bottom-up iterative procedure: The tree is constructed from the terminal nodes (sentence words) as shown in Figure 1. For each iteration, we search for all possible new tree nodes that merge input components (i.e. heads of previously predicted trees or leaves that have not yet been assembled), using a neural network (see Figure 3) that shifts the window tagger over input components X1,..., XN. Considering an arbitrary ruleA \u2192 Xi, Xi + 1,... XjDefinition of a new node with tag A, the tagger produces prefixed tags B-A, I-A,.. E-A, and for the components Xi + 1,.., XjDefinition of a new node with tag A, I-A, respectively."}, {"heading": "3.1 WORD EMBEDDINGS", "text": "Following the work of Collobert & Weston (2008) on various NLP tasks, our parser is based on raw words. Formally, each word in a finite dictionary is assigned a continuous vector representation to W. These representations, as all parameters of our architecture, are trained by backpropagation. Formally, each word is assigned a set of N-words, w1, w2,..., wN first embedded in a D-dimensional vector space by applying a search table procedure: LTW (wn) = Wwn, with the matrix W-RD \u00b7 W | representing the parameters trained in this search plane. Each column Wn-RD corresponds to the vector embedding of the ninth word in our dictionary W. In this work, two types of characteristics are used to feed the networks: words (or word compositions) and POS tags T (or parsing tags)."}, {"heading": "3.2 WORD-TAG COMPOSITION", "text": "At each step of the parsing procedure, we represent each node of the tree as a vector representation, summarizing both the syntax (predicted POS or parsing tags) and the semantic (words) of the subtree corresponding to the given node. As shown in Figure 2, vector representation is achieved by a simple, recurring procedure that includes several components: \u2022 word vector representations for the leaves (coming from a lookup table) (dimension D). \u2022 tag (POS for the leaves, otherwise predicted tags) vector representations (which also come from another lookup table, as explained in Section 3.1) (dimension T). \u2022 compositional networks Ck (). Each of them can represent a chunk of size k in a D-dimensional vector. Compositional networks that take both the merged node and the predicted tag representations."}, {"heading": "3.3 SLIDING WINDOW BIOES TAGGER", "text": "The tagging module of our architecture (see Figure 3) is a two-layer neural network that applies a slider window over the input constituent representations (as calculated in Section 3.2), as well as the input constituent tag representations. If we consider the input constituents X1,.., XN, if we assume that their respective representations were previously stored in reference books, the ninth window is asun = [LT (Xn \u2212 K \u2212 12),..., LT (Xn),..., LT (Xn + K \u2212 12)], where K is the window size. If P is the set of the BIOES preceded parsing tags from P, the module returns a vector of the values s (un) = [s1,..., s | P operations (where st is the value of the BIOESpreeSprefix parsing tag t-P for the constituent Xn), so a value of the Vector is (Speak VIOP), which is (Speak VIOP)."}, {"heading": "3.4 COHERENT BIOES PREDICTIONS", "text": "The next module in our architecture aggregates the BIOES-preceded parsing tags from our tagger module in a coherent manner. It is implemented as a viterbi decoding algorithm via a restricted graph G. It encodes all possible valid sequences of BIOES-preceded tags across components: B-A tags can only be followed by I-A or E-A tags for each parsing label A. Each node of the graph is simply assigned a score generated by the previous neural network module (score for each BIOES-preceded tag and for each word). Score S ([t] N1, [X] N 1, \u03b8) for a sequence of tags [t] N 1 in the grid G is achieved simply by summing the scores along the path ([X] N1 is the input sequence of components and the model)."}, {"heading": "3.5 TRAINING PROCEDURE", "text": "Both the composition network and the marker networks are made by maximizing a probability of the training data by means of stochastic gradient ascension. We performed all possible iterations of all the training sets of the greedy procedure depicted in Figure 1 \u2022 This leads to our training set of tree node sequences. While this procedure is similar to (Legrand & Collobert, 2014), it is worth noting that it implies that the system is trained only on correct tree node sequences. In this respect, it is not trained to recover from past mistakes it may have made during the recurring process. For each tree node, the sub-trees (structure and markers) are also extracted during this procedure. System training consists of repeating the following steps: \u2022 Choose a random sequence of nodes extracted in the training set as described above. Consider the associated sub-trees that are not a leaf of the following precursor."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 CORPUS", "text": "We adopted the classic structure with sections 02-21 for the turn, section 22 for the validation and section 23 for the test. The validation corpus was used to select our hyperparameters and best models. We processed the data only with a subset of operations achieved in standard parsers: (1) functional labels, traces were removed, (2) the PRT label was replaced by ADVP (Magerman, 1995). (3) We solved the problem of the simple chain - not terminals with a single non-terminal child - by merging the nodes and assigning the concatenation of the merged node tags as a tag, in order to avoid loop problems in the parsing algorithm (e.g. a node repeatedly marked with two different tags in our iterative process) and ensuring the convergence of the parsing process. Only linked tags that lead to at least 11 additional labels, are led to fewer than the original labels (30)."}, {"heading": "4.2 DETAILED SETUP", "text": "Our systems were trained using a stochastic gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient grade gradient grade gradient grade gradient gradient gradient gradient grade gradient gradient gradient gradient gradient grade gradient gradient gradient grade gradient gradient gradient gradient grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade gra"}, {"heading": "4.3 WORD EMBEDDING DROPOUT REGULARIZATION", "text": "We found that our system was slightly over-adjusted (the F1 score in training increased, while the validation curve eventually decreased, as shown in Figure 4).Since the capacity of our network is mainly based on words and tag embedding, we chose a failure control strategy for the failure tables (see Hinton et al., 2012).The basic idea of failure control is to randomly drop units (along with their connections) from the neural network during training, thereby preventing the units from adjusting too much. In our case, a \"failure mask\" is applied to the output of the failure tables during the training phase: each element of the output is set to 0, with a probability of 0.25. At test date, no patch is applied, but the output is reweighted and scaled by 0.75. We observed a good improvement in the performance of the F1 score, as shown in Figure 4."}, {"heading": "4.4 PERFORMANCE COMPARISON", "text": "We compared our system with a number of state-of-the-art parsers. In addition to the four main generative parsers, we report on the results of known parsers for reclassification (including the current state of the art by McClosky et al. (2006), as well as on two important purely discriminatory parsers. Detailed error analyses compared to a subset of these parsers are reported in Table 2 using the code provided by Kummerfeld et al. (2012). Performance in terms of record length is shown in Figure 5.We included a tuning process using multiple models based on different random initializations. The tuning process is achieved in the following way: Each iteration of the greedy parsing method, taking into account the input sequence of the components (1), the node representations for each model are calculated by assembling the sub-tree representations according to the given model and using its own J-tag values (2), the results of a network are calculated with other elements each."}, {"heading": "5 CONCLUSION", "text": "In this paper, we introduced a new parsing architecture that uses the RNN-based compositional representation of sub-trees, encoding both the syntactic (tags) and the semantic (words) information. The parsing method is tightly integrated with the composition operation and allows us to achieve the performance of well-known parsers, while (1) using a greedy and fast method (2) to avoid refined standard functions such as keywords."}, {"heading": "ACKNOWLEDGMENTS", "text": "Some of this work was supported by NEC Laboratories America."}], "references": [{"title": "On the usefulness of extracting syntactic dependencies for text indexing", "author": ["M.A. Alonso", "J. Vilares", "V.M. Darriba"], "venue": "In Artificial Intelligence and Cognitive Science", "citeRegEx": "Alonso et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2002}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. LeCun", "Y. Bengio"], "venue": "In Proc. of Computer Vision and Pattern Recognition,", "citeRegEx": "Bottou et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 1997}, {"title": "TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing", "author": ["X. Carreras", "M. Collins", "T. Koo"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Carreras et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "A maximum-entropy-inspired parser", "author": ["E. Charniak"], "venue": "In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference,", "citeRegEx": "Charniak,? \\Q2000\\E", "shortCiteRegEx": "Charniak", "year": 2000}, {"title": "Coarse-to-fine N-best parsing and MaxEnt discriminative reranking", "author": ["E. Charniak", "M. Johnson"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Charniak and Johnson,? \\Q2005\\E", "shortCiteRegEx": "Charniak and Johnson", "year": 2005}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Chen and Manning,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "Three models for the description of language", "author": ["N. Chomsky"], "venue": "IRE Transactions on Information Theory,", "citeRegEx": "Chomsky,? \\Q1956\\E", "shortCiteRegEx": "Chomsky", "year": 1956}, {"title": "Head-driven statistical models for natural language parsing", "author": ["M. Collins"], "venue": "Comput. Linguist.,", "citeRegEx": "Collins,? \\Q2003\\E", "shortCiteRegEx": "Collins", "year": 2003}, {"title": "Deep learning for efficient discriminative parsing", "author": ["R. Collobert"], "venue": "In AISTATS,", "citeRegEx": "Collobert,? \\Q2011\\E", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Towards incremental parsing of natural language using recursive neural networks", "author": ["F. Costa", "P. Frasconi", "V. Lombardo", "G. Soda"], "venue": null, "citeRegEx": "Costa et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Costa et al\\.", "year": 2002}, {"title": "Image segmentation and recognition", "author": ["J.S. Denker", "C.J.C. Burges"], "venue": "The Mathematics of Induction,", "citeRegEx": "Denker and Burges,? \\Q1995\\E", "shortCiteRegEx": "Denker and Burges", "year": 1995}, {"title": "An efficient context-free parsing algorithm", "author": ["J. Earley"], "venue": null, "citeRegEx": "Earley,? \\Q1970\\E", "shortCiteRegEx": "Earley", "year": 1970}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["J.L. Elman"], "venue": "Mach. Learn.,", "citeRegEx": "Elman,? \\Q1991\\E", "shortCiteRegEx": "Elman", "year": 1991}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["J.R. Finkel", "A. Kleeman", "C.D. Manning"], "venue": "In In Proc. ACL/HLT,", "citeRegEx": "Finkel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["J. Henderson"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume", "citeRegEx": "Henderson,? \\Q2003\\E", "shortCiteRegEx": "Henderson", "year": 2003}, {"title": "Discriminative training of a neural network statistical parser", "author": ["J. Henderson"], "venue": "In Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Henderson,? \\Q2004\\E", "shortCiteRegEx": "Henderson", "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "G E", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "An efficient recognition and syntax analysis algorithm for context-free languages", "author": ["T. Kasami"], "venue": "Technical report,", "citeRegEx": "Kasami,? \\Q1965\\E", "shortCiteRegEx": "Kasami", "year": 1965}, {"title": "Readings in natural language processing. chapter Algorithm Schemata and Data Structures in Syntactic Processing", "author": ["M. Kay"], "venue": null, "citeRegEx": "Kay,? \\Q1986\\E", "shortCiteRegEx": "Kay", "year": 1986}, {"title": "Parser showdown at the wall street corral: An empirical investigation of error types in parser output", "author": ["J.K. Kummerfeld", "D. Hall", "J.R. Curran", "D. Klein"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Kummerfeld et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Eighteenth International Conference on Machine Learning,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Word embeddings through hellinger PCA", "author": ["R. Lebret", "R. Collobert"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Lebret and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2014}, {"title": "Recurrent greedy parsing with neural networks", "author": ["J. Legrand", "R. Collobert"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Legrand and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Legrand and Collobert", "year": 2014}, {"title": "Statistical decision-tree models for parsing", "author": ["D.M. Magerman"], "venue": "Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Magerman,? \\Q1995\\E", "shortCiteRegEx": "Magerman", "year": 1995}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "In that respect, parsing it has been widely used as an input feature for several other NLP tasks such as machine translation (Zollmann & Venugopal, 2006), information retrieval (Alonso et al., 2002), or Semantic Role Labeling (Punyakanok et al.", "startOffset": 177, "endOffset": 198}, {"referenceID": 5, "context": "In the linguistic world, Chomsky (1956) first introduced formally the parsing task, by defining the natural language syntax as a set of context-free grammar rules (a particular type of formal grammar), combined with transformations rules.", "startOffset": 25, "endOffset": 40}, {"referenceID": 24, "context": "A wide range of parser were, and still are, based on Probabilistic context-free grammar (PCFGs) (Magerman, 1995; Collins, 2003; Charniak, 2000).", "startOffset": 96, "endOffset": 143}, {"referenceID": 7, "context": "A wide range of parser were, and still are, based on Probabilistic context-free grammar (PCFGs) (Magerman, 1995; Collins, 2003; Charniak, 2000).", "startOffset": 96, "endOffset": 143}, {"referenceID": 3, "context": "A wide range of parser were, and still are, based on Probabilistic context-free grammar (PCFGs) (Magerman, 1995; Collins, 2003; Charniak, 2000).", "startOffset": 96, "endOffset": 143}, {"referenceID": 18, "context": "Parsing inference is mostly achieved using simple bottom-up chart parser (Kasami, 1965; Earley, 1970; Kay, 1986).", "startOffset": 73, "endOffset": 112}, {"referenceID": 12, "context": "Parsing inference is mostly achieved using simple bottom-up chart parser (Kasami, 1965; Earley, 1970; Kay, 1986).", "startOffset": 73, "endOffset": 112}, {"referenceID": 19, "context": "Parsing inference is mostly achieved using simple bottom-up chart parser (Kasami, 1965; Earley, 1970; Kay, 1986).", "startOffset": 73, "endOffset": 112}, {"referenceID": 12, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser.", "startOffset": 31, "endOffset": 48}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser.", "startOffset": 49, "endOffset": 75}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser. To our knowledge, the state of the art in syntactic parsing is still held by McClosky et al. (2006), who leverages discriminative re-ranking, as well as self-training over unlabeled corpora: a re-ranker is trained over a generative model which is then used to label the unlabeled dataset.", "startOffset": 49, "endOffset": 328}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser. To our knowledge, the state of the art in syntactic parsing is still held by McClosky et al. (2006), who leverages discriminative re-ranking, as well as self-training over unlabeled corpora: a re-ranker is trained over a generative model which is then used to label the unlabeled dataset. The original parser is then re-trained with this new \u201clabeled\u201d corpus. Petrov & Klein (2007) introduced a method to automaticaly refine PCFG rules by iteratively spliting them.", "startOffset": 49, "endOffset": 610}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser. To our knowledge, the state of the art in syntactic parsing is still held by McClosky et al. (2006), who leverages discriminative re-ranking, as well as self-training over unlabeled corpora: a re-ranker is trained over a generative model which is then used to label the unlabeled dataset. The original parser is then re-trained with this new \u201clabeled\u201d corpus. Petrov & Klein (2007) introduced a method to automaticaly refine PCFG rules by iteratively spliting them. This method leverages an efficient coarse-to-fine procedure to speed up the decoding process. More recently, Finkel et al. (2008); Petrov & Klein (2008) proposed PCFG-based discriminative parsers reaching the performance of their generative counterparts.", "startOffset": 49, "endOffset": 824}, {"referenceID": 2, "context": "Discriminative approaches from Henderson (2004); Charniak & Johnson (2005) outperform standard PCFG-based generative parsers, but only by discriminatively re-ranking the K-best predicted trees coming out of a generative parser. To our knowledge, the state of the art in syntactic parsing is still held by McClosky et al. (2006), who leverages discriminative re-ranking, as well as self-training over unlabeled corpora: a re-ranker is trained over a generative model which is then used to label the unlabeled dataset. The original parser is then re-trained with this new \u201clabeled\u201d corpus. Petrov & Klein (2007) introduced a method to automaticaly refine PCFG rules by iteratively spliting them. This method leverages an efficient coarse-to-fine procedure to speed up the decoding process. More recently, Finkel et al. (2008); Petrov & Klein (2008) proposed PCFG-based discriminative parsers reaching the performance of their generative counterparts.", "startOffset": 49, "endOffset": 847}, {"referenceID": 2, "context": "Carreras et al. (2008) currently holds the state-of-the-art among the (non-reranking) discriminative parsers.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "Carreras et al. (2008) currently holds the state-of-the-art among the (non-reranking) discriminative parsers. Their parser leverages a global-linear model (instead of a CRF) with PCFGs, together with various new advanced features. Z. et al. (2010) showed that jointly using multiple self-trained grammars can achieve higher accuracy than an individual grammar.", "startOffset": 0, "endOffset": 248}, {"referenceID": 15, "context": "Henderson (2003) introduced a generative left-corner parser where the probability of a derivation given the derivation historic was approximated using a Simple Synchrony Networks (SNN), which is a neural network specifically designed for processing structures.", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "Henderson (2003) introduced a generative left-corner parser where the probability of a derivation given the derivation historic was approximated using a Simple Synchrony Networks (SNN), which is a neural network specifically designed for processing structures. Turian & Melamed (2006) later proposed a bottom-up greedy algorithm following a left-to-right or a right-to left strategy and using using a feature boosting approach.", "startOffset": 0, "endOffset": 285}, {"referenceID": 15, "context": "Henderson (2003) introduced a generative left-corner parser where the probability of a derivation given the derivation historic was approximated using a Simple Synchrony Networks (SNN), which is a neural network specifically designed for processing structures. Turian & Melamed (2006) later proposed a bottom-up greedy algorithm following a left-to-right or a right-to left strategy and using using a feature boosting approach. In this approach, greedy decisions regarding the tree construction are made using decision tree classifiers. Their model was nevertheless limited to short length sentences. Zhu et al. (2013) proposed a shift-reduce parser which achieves results comparable to their chartbased counterparts.", "startOffset": 0, "endOffset": 619}, {"referenceID": 13, "context": "Recurrent Neural Networks (RNNs) were seen very early (Elman, 1991) as a way to tackle the problem of parsing, as they can naturally recur along the parse tree.", "startOffset": 54, "endOffset": 67}, {"referenceID": 9, "context": "A first practical application of RNN on syntactic parsing were proposed by Costa et al. (2002). Their approach was based on a leftto-right incremental parser, where a recursive neural network was used to re-rank possible phrase attachments.", "startOffset": 75, "endOffset": 95}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner.", "startOffset": 0, "endOffset": 169}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner.", "startOffset": 0, "endOffset": 453}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner.", "startOffset": 0, "endOffset": 518}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner. However, this work did not tackle the full parse tree problem, but was restricted to unlabeled bracketing. Socher et al. (2013) introduced the Compositional Vector Grammar (CVG) which combines PCFGs with a Syntactically Untied Recursive Neural Network (SU-RNN).", "startOffset": 0, "endOffset": 698}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner. However, this work did not tackle the full parse tree problem, but was restricted to unlabeled bracketing. Socher et al. (2013) introduced the Compositional Vector Grammar (CVG) which combines PCFGs with a Syntactically Untied Recursive Neural Network (SU-RNN). Composition is performed over a binary tree, then used to score theK-best trees coming out of a generative parser. For a given (parent) node of the tree, the authors apply a composition operation over its child nodes, conditioned with their syntax information. In contrast, we compose phrases (not limited to two words). Both the words and syntax information of the child nodes are fed to each composition operation, leading to a vector representation of each tree node carrying both some semantic and syntactic information. We also do not rely on any generative parser as our model jointly trains the task of node prediction, and the task of node composition. Legrand & Collobert (2014) proposed a greedy RNN-based parser.", "startOffset": 0, "endOffset": 1520}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner. However, this work did not tackle the full parse tree problem, but was restricted to unlabeled bracketing. Socher et al. (2013) introduced the Compositional Vector Grammar (CVG) which combines PCFGs with a Syntactically Untied Recursive Neural Network (SU-RNN). Composition is performed over a binary tree, then used to score theK-best trees coming out of a generative parser. For a given (parent) node of the tree, the authors apply a composition operation over its child nodes, conditioned with their syntax information. In contrast, we compose phrases (not limited to two words). Both the words and syntax information of the child nodes are fed to each composition operation, leading to a vector representation of each tree node carrying both some semantic and syntactic information. We also do not rely on any generative parser as our model jointly trains the task of node prediction, and the task of node composition. Legrand & Collobert (2014) proposed a greedy RNN-based parser. The neural network was recurrent only in the sense it used previously predicted tags to produce next tree node tags. Contrary to Socher et al. (2013), it did not involve composing sub-tree representations.", "startOffset": 0, "endOffset": 1706}, {"referenceID": 8, "context": "Collobert (2011) proposed a purely discriminative parser based on neural networks. This model leveraged continuous vector representations from Collobert & Weston (2008), and builds the full parsing tree in a bottom-up manner. To deal with the recursive structure inherent to syntactic parsing, a very simple history was given to the network as a new vector feature (corresponding to the nearest tag spanning the word being tagged). Socher et al. (2011) also leveraged continuous vectors from Collobert & Weston (2008), combining them to build a tree in a greedy manner. However, this work did not tackle the full parse tree problem, but was restricted to unlabeled bracketing. Socher et al. (2013) introduced the Compositional Vector Grammar (CVG) which combines PCFGs with a Syntactically Untied Recursive Neural Network (SU-RNN). Composition is performed over a binary tree, then used to score theK-best trees coming out of a generative parser. For a given (parent) node of the tree, the authors apply a composition operation over its child nodes, conditioned with their syntax information. In contrast, we compose phrases (not limited to two words). Both the words and syntax information of the child nodes are fed to each composition operation, leading to a vector representation of each tree node carrying both some semantic and syntactic information. We also do not rely on any generative parser as our model jointly trains the task of node prediction, and the task of node composition. Legrand & Collobert (2014) proposed a greedy RNN-based parser. The neural network was recurrent only in the sense it used previously predicted tags to produce next tree node tags. Contrary to Socher et al. (2013), it did not involve composing sub-tree representations. Instead, head-words were used as a key feature. Our approach shares some similarities with (Legrand & Collobert, 2014), as it is also a greedy parser based on RNNs. However, instead of relying on head-words (which could be seen as a simplistic representations of sub-trees), we leverage compositional sub-tree vector representations trained jointly with the parser. This approach leads to much better parsing accuracy, while relying only on a few simple features (words and POS tags). Our model has also the ability of producing phrase embeddings, which may represent a valuable feature for other NLP tasks. Chen & Manning (2014) proposed a greedy transition-based dependency parser based on neural networks, fed with dense word and tag vector representations.", "startOffset": 0, "endOffset": 2392}, {"referenceID": 8, "context": "Following the work from Collobert & Weston (2008) on various NLP tasks, our parser relies on raw words.", "startOffset": 24, "endOffset": 50}, {"referenceID": 8, "context": "Lot of work on this domain has been done in recent year, including Collobert & Weston (2008), Mikolov et al.", "startOffset": 67, "endOffset": 93}, {"referenceID": 8, "context": "Lot of work on this domain has been done in recent year, including Collobert & Weston (2008), Mikolov et al. (2013). In this paper, we chose to use the representations from Lebret & Collobert (2014), obtained by a simple PCA on a matrix of word co-occurrences.", "startOffset": 67, "endOffset": 116}, {"referenceID": 8, "context": "Lot of work on this domain has been done in recent year, including Collobert & Weston (2008), Mikolov et al. (2013). In this paper, we chose to use the representations from Lebret & Collobert (2014), obtained by a simple PCA on a matrix of word co-occurrences.", "startOffset": 67, "endOffset": 199}, {"referenceID": 1, "context": "Similar training procedures have been proposed in the past for structured data (Denker & Burges, 1995; Bottou et al., 1997; Lafferty et al., 2001).", "startOffset": 79, "endOffset": 146}, {"referenceID": 21, "context": "Similar training procedures have been proposed in the past for structured data (Denker & Burges, 1995; Bottou et al., 1997; Lafferty et al., 2001).", "startOffset": 79, "endOffset": 146}, {"referenceID": 25, "context": "Experiments were conducted using the standard English Penn Treebank data set (Marcus et al., 1993).", "startOffset": 77, "endOffset": 98}, {"referenceID": 24, "context": "We pre-processed the data only with a subset of operations which are achieved in standard parsers: (1) functional labels, traces were removed, (2) the PRT label was replaced by ADVP (Magerman, 1995).", "startOffset": 182, "endOffset": 198}, {"referenceID": 8, "context": "We used the word embeddings obtained from Lebret & Collobert (2014) to initialize the word lookup-table.", "startOffset": 51, "endOffset": 68}, {"referenceID": 8, "context": "We used the word embeddings obtained from Lebret & Collobert (2014) to initialize the word lookup-table. These embeddings were then fine-tuned during the training process. We fixed the learning rate to \u03bb = 0.15 during the stochastic gradient procedure. As suggested in Plaut & Hinton (1987), the learning rate was divided by the size of the input vector of each layer.", "startOffset": 51, "endOffset": 291}, {"referenceID": 20, "context": "Detailed error analysis compared against a subset of these parsers is reported in Table 2, using the code provided by Kummerfeld et al. (2012). Performance with respect to sentence length is reported in Figure 5.", "startOffset": 118, "endOffset": 143}], "year": 2015, "abstractText": "This paper introduces a greedy parser based on neural networks, which leverages a new compositional sub-tree representation. The greedy parser and the compositional procedure are jointly trained, and tightly depends on each-other. The composition procedure outputs a vector representation which summarizes syntactically (parsing tags) and semantically (words) sub-trees. Composition and tagging is achieved over continuous (word or tag) representations, and recurrent neural networks. We reach F1 performance on par with well-known existing parsers, while having the advantage of speed, thanks to the greedy nature of the parser. We provide a fully functional implementation of the method described in this paper 1.", "creator": "LaTeX with hyperref package"}}}