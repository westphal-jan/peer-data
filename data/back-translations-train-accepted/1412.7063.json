{"id": "1412.7063", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Diverse Embedding Neural Network Language Models", "abstract": "We propose Diverse Embedding Neural Network (DENN) - a novel architecture for neural network language models (LMs). A DENN-LM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENN-LM. We also present an empirical analysis of the diverse word embeddings learned by DENN-LM.", "histories": [["v1", "Mon, 22 Dec 2014 17:19:56 GMT  (1520kb,D)", "https://arxiv.org/abs/1412.7063v1", null], ["v2", "Wed, 7 Jan 2015 21:33:46 GMT  (1533kb,D)", "http://arxiv.org/abs/1412.7063v2", null], ["v3", "Mon, 19 Jan 2015 19:53:21 GMT  (1589kb,D)", "http://arxiv.org/abs/1412.7063v3", null], ["v4", "Wed, 25 Feb 2015 21:55:15 GMT  (1484kb,D)", "http://arxiv.org/abs/1412.7063v4", "Under review as workshop contribution at ICLR 2015"], ["v5", "Wed, 15 Apr 2015 20:07:50 GMT  (1484kb,D)", "http://arxiv.org/abs/1412.7063v5", "Under review as workshop contribution at ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["kartik audhkhasi", "abhinav sethy", "bhuvana ramabhadran"], "accepted": true, "id": "1412.7063"}, "pdf": {"name": "1412.7063.pdf", "metadata": {"source": "CRF", "title": "DIVERSE EMBEDDING NEURAL NETWORK LANGUAGE MODELS", "authors": ["Kartik Audhkhasi", "Abhinav Sethy", "Bhuvana Ramabhadran"], "emails": ["kaudhkha@us.ibm.com", "asethy@us.ibm.com", "bhuvana@us.ibm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The problem of linguistic modeling, which aims to design predictive models of text, is not an exception. Multiple language models (LMs) have been proposed over the course of many years of research (Rosenfield (2000)). Simple N-grammars models estimate the conditional probability that the terms of the i-th word \"i-grammars\" are used in a sentence in which the terms \"i-grammars,\" \"\" i-grammars, \"\" i-grammars, \"\" \"i-grammars,\" \"\" i-grammars, \"\" \"\" i-grammars, \"\" \"\" i-grammars, \"\" \"\" \"\" i-grammars, \"\" \"\" \"\" i-grammars, \"\" \"\" \"i-grammars,\" \"\" \"in,\" \"i-grammars,\" \"\", \"\" \"i-grammars,\" \",\" \"\" i-grammars, \"\", \",\" \"\" \"i-grammars,\", \"\" \"\" in, \"\" \"\" i-grammars, \"\", \"\" \",\" \"\" \"i-grammars,\""}, {"heading": "2 FEED-FORWARD NEURAL NETWORK LANGUAGE MODEL (NNLM)", "text": "A forward-facing neural network LM (NNLM) converts the one-dimensional encoding of each word in history into a continuous low-dimensional vector representation (Bengio et al. (2006)). Figure 1 shows the schematic diagram of a typical NNLM. Let wi,.., wi \u2212 N + 1 denote the 1-in-V vectors of the history words. Let RH denote the D \u00b7 V matrix that projects a history word vector onto Ddimensional vectors RHwi., RHwi \u2212 N + 1. D is typically much smaller than the size V of the vocabulary with typical values such as V = 10, 000 and D = 500. This resulting (N \u2212 1) D-dimensional continuous representation of the input words leads into a neural network with a hidden layer that estimates the 1-in-V vector of the target wi."}, {"heading": "3 DIVERSE EMBEDDING NNLM", "text": "A diverse embedding of NNLM (DENNLM) aims to learn several different representations of the input words, rather than just a single representation. First, it is important to understand the intuition of a representation in the context of an NNLM. Given a series of N input vectors wi \u2212 1,.., wi \u2212 N + 1, we consider the set of D-dimensional vectors RHwi \u2212 1,.., RHwi \u2212 N + 1. The paired distances between the vectors of this sentence form the representation of the input words. A good representation captures the contextual and semantic similarity between word pairs. Similar words are close to each other in the representation, while unequal words are far apart. An NLM uses this representation of the input words to predict the next word. The most natural way to ensure the diversity of two NLMs is the diversity in the representation itself (representation diversity)."}, {"heading": "3.1 DIVERSITY BETWEEN NNLM EMBEDDINGS", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3.2 DENNLM ARCHITECTURE AND TRAINING LOSS FUNCTION", "text": "As discussed earlier, a DENNLM attempts to learn several different low-dimensional representations of the input word history = Nwi. Figure 2 shows the schematic diagram of a DENNLM with two different representations. The two representations run through two separate NNs and produce separate predictions of the next word. The model merges the two predictions to produce the final prediction. It is important to note that the DENNLM corresponds to a single NNLM with the following blocked prediction MatrixRH = R1HR2H 0 0 R2H and block diagonal connection weight matrices W = (W1 0 W2) RP = (R1P 0 R2P). The presence of zero entries in the above matrices implies that a DENNLM has a smaller number of parameters than a comparable NNLM. The equivalence between a DENLM function and a conventional NLM implem does not make the conventionally implementioning simple."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "We performed speech modeling experiments on a standard division of the Penn Treebank (UPenn) dataset (Marcus et al. (1993)) as used in previous work such as (Mikolov et al. (2011).The UPenn dataset has a vocabulary of 10K words and contains approximately 1M N-grams in the training set. We used the UPenn dataset because it was well studied for speech modeling and is also small enough to conduct several experiments to better understand the DENNLM model (Tieleman & Hinton (2012)), a variant of the DENNLM in Theano (Bergstra et al. (2011) and trained it by optimizing the augmented loss function in (4) using Root-Mean Square Propagation (RMSProp) (LSLM), a stochastic gradient lineage of NM."}, {"heading": "4.1 SENSITIVITY TO HYPERPARAMETERS", "text": "We further investigated the dependence of the performance of DENNLM on some groups of hyperparameters and listed our observations as follows: In order to further understand the influence of hyperparameters on DENNLM diversity and perplexity, we conducted a thorough search for \u03b2 and \u03b3 in (4), the learning rate of RMSProp and the weight of an L2 penalty on the DENNLM compound weights. We then calculated the log perplexity and the average cross-correlation between the posteriors of individual models in a DENNLM. Figure 3 shows the scatter plot between the average posterior cross-correlation and the percentage improvement of the DENNLM log perplexity compared to the perplexity of the best model. A strong negative correlation of \u2212 0.97 in this figure shows that more diversified models can achieve a greater improvement in log perplexity during interpolation, such as the fact that the perplexity of the formation of the NM emphasizes the large number of perplexity."}, {"heading": "5 CONCLUSION", "text": "In this work, we have introduced a neural network architecture and a training target function that encourages individual models to be diverse in terms of output distribution and underlying word representations. We have demonstrated their usefulness in the well-studied task of speech modeling by UPenn. The proposed training criterion is general enough and does not restrict the NNLM models to belong to a particular architecture. Given the promising results, our next step is to evaluate the improved speech models for speech recognition and speech recognition. We also plan to investigate the usefulness of these diverse representations for measuring semantic similarity and sentence completion, with word-based models proving effective."}, {"heading": "6 ACKNOWLEDGEMENT", "text": "This work has been supported by the Intelligence Advanced Research Projects Activity (IARPA) through the Department of Defense U.S. Army Research Laboratory (DoD / ARL) under contract number W911NF-12C-0012. The U.S. Government is authorized to reproduce and distribute copies for government purposes, regardless of copyright notices therein. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or notices of IARPA, DoD / ARL or the U.S. Government, neither express nor implied."}], "references": [{"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J. Sen\u00e9cal", "F. Morin", "J. Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Deep learning on gpus with python", "author": ["J. Bergstra", "F. Bastien", "O. Breuleux", "P. Lamblin", "R. Pascanu", "O. Delalleau", "G. Desjardins", "D. Warde-Farley", "I.J. Goodfellow", "A. Bergeron", "Bengio", "Y. Theano"], "venue": "In Big Learn workshop,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "A bit of progress in language modeling", "author": ["J.T. Goodman"], "venue": "Technical report,", "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "In Proc. ICASSP,", "citeRegEx": "Kneser and Ney,? \\Q1995\\E", "shortCiteRegEx": "Kneser and Ney", "year": 1995}, {"title": "Combining pattern classifiers: methods and algorithms", "author": ["L.I. Kuncheva"], "venue": null, "citeRegEx": "Kuncheva,? \\Q2004\\E", "shortCiteRegEx": "Kuncheva", "year": 2004}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernock\u1ef3"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Two decades of statistical language modeling: Where do we go from here", "author": ["R. Rosenfield"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rosenfield,? \\Q2000\\E", "shortCiteRegEx": "Rosenfield", "year": 2000}, {"title": "LSTM neural networks for language modeling", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "In Interspeech,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Diversity of systems trained to perform a given machine learning task is crucial to obtaining a performance improvement upon fusion (Kuncheva (2004)).", "startOffset": 133, "endOffset": 149}, {"referenceID": 4, "context": "Diversity of systems trained to perform a given machine learning task is crucial to obtaining a performance improvement upon fusion (Kuncheva (2004)). The problem of language modeling, that aims to design predictive models of text, is no exception. Several language models (LMs) have been proposed over many years of research (Rosenfield (2000)).", "startOffset": 133, "endOffset": 345}, {"referenceID": 0, "context": "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P\u0302NN(wi|wi\u2212N+1, .", "startOffset": 61, "endOffset": 82}, {"referenceID": 0, "context": "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P\u0302NN(wi|wi\u2212N+1, . . . , wi\u22121) = f(wi\u22121, . . . , wi\u2212N+1; \u0398) (2) parameterized by \u03982. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al.", "startOffset": 61, "endOffset": 319}, {"referenceID": 0, "context": "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P\u0302NN(wi|wi\u2212N+1, . . . , wi\u22121) = f(wi\u22121, . . . , wi\u2212N+1; \u0398) (2) parameterized by \u03982. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al. (2011)) often significantly improves performance.", "startOffset": 61, "endOffset": 342}, {"referenceID": 0, "context": "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P\u0302NN(wi|wi\u2212N+1, . . . , wi\u22121) = f(wi\u22121, . . . , wi\u2212N+1; \u0398) (2) parameterized by \u03982. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al. (2011)) often significantly improves performance. Table 1 shows the perplexity3 of 4-gram and NNLMs on a standard split of the Penn Treebank data set (Marcus et al. (1993)).", "startOffset": 61, "endOffset": 507}, {"referenceID": 0, "context": "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P\u0302NN(wi|wi\u2212N+1, . . . , wi\u22121) = f(wi\u22121, . . . , wi\u2212N+1; \u0398) (2) parameterized by \u03982. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al. (2011)) often significantly improves performance. Table 1 shows the perplexity3 of 4-gram and NNLMs on a standard split of the Penn Treebank data set (Marcus et al. (1993)). Interpolation of a NNLM with a 4-gram LM gives a 16.9% reduction in perplexity over a single NNLM even though the two LMs have relatively close perplexities. We use the correlation coefficient between the posterior probabilities of the predicted word over the test set from the two models as a simple measure to predict whether the models are diverse. If the posterior probabilities are highly correlated, then the models are less diverse and smaller gains are expected from fusion. The posteriors from the N-gram and NNLM have a correlation coefficient of 0.869 which is significantly lower than the correlation coefficient of 0.988 for a pair of randomly initialized NNLMs. This higher diversity of the NNLM and N-gram LM combination results in significant perplexity improvement upon interpolation. Almost all N-gram models are smoothed in various ways to assign non-zero probability estimates for word phrases unseen in training data. We use Kneser-Ney smoothing (Kneser & Ney (1995)) for all N-grams models in this paper.", "startOffset": 61, "endOffset": 1497}, {"referenceID": 6, "context": "Recurrent NNLM models of different topologies can be fused to get significant gains as well as demonstrated in (Mikolov et al. (2011)).", "startOffset": 112, "endOffset": 134}, {"referenceID": 0, "context": "A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)).", "startOffset": 153, "endOffset": 174}, {"referenceID": 0, "context": "A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)). Figure 1 shows the schematic diagram of a typical NNLM. Let wi, . . . ,wi\u2212N+1 denote the 1-in-V vectors of the history words. Let RH denote the D \u00d7 V matrix that projects a history word vectors onto Ddimensional vectors RHwi, . . . ,RHwi\u2212N+1. D is typically much smaller than the size V of the vocabulary with typical values being V = 10, 000 and D = 500. This resulting (N \u2212 1)D-dimensional continuous representation of input words feeds into a neural network with one hidden layer that estimates the 1-in-V vector of the target word wi. The hidden neuron activation function is hyperbolic tangent or logistic sigmoid while the output neuron activation function is a V -way softmax. The back-propagation algorithm (Rumelhart et al. (1986)) trains an NNLM, often using stochastic gradient descent where the gradient is computed over several random subsets or batches of N-grams from the input training data.", "startOffset": 153, "endOffset": 916}, {"referenceID": 0, "context": "A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)). Figure 1 shows the schematic diagram of a typical NNLM. Let wi, . . . ,wi\u2212N+1 denote the 1-in-V vectors of the history words. Let RH denote the D \u00d7 V matrix that projects a history word vectors onto Ddimensional vectors RHwi, . . . ,RHwi\u2212N+1. D is typically much smaller than the size V of the vocabulary with typical values being V = 10, 000 and D = 500. This resulting (N \u2212 1)D-dimensional continuous representation of input words feeds into a neural network with one hidden layer that estimates the 1-in-V vector of the target word wi. The hidden neuron activation function is hyperbolic tangent or logistic sigmoid while the output neuron activation function is a V -way softmax. The back-propagation algorithm (Rumelhart et al. (1986)) trains an NNLM, often using stochastic gradient descent where the gradient is computed over several random subsets or batches of N-grams from the input training data. Researchers have proposed several variants of the feed forward NNLM, especially to model a longer input word history. Prominent examples include the recurrent neural network LM (Mikolov et al. (2010)) and bidirectional long-short term memory (LSTM) LM (Sundermeyer et al.", "startOffset": 153, "endOffset": 1284}, {"referenceID": 0, "context": "A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)). Figure 1 shows the schematic diagram of a typical NNLM. Let wi, . . . ,wi\u2212N+1 denote the 1-in-V vectors of the history words. Let RH denote the D \u00d7 V matrix that projects a history word vectors onto Ddimensional vectors RHwi, . . . ,RHwi\u2212N+1. D is typically much smaller than the size V of the vocabulary with typical values being V = 10, 000 and D = 500. This resulting (N \u2212 1)D-dimensional continuous representation of input words feeds into a neural network with one hidden layer that estimates the 1-in-V vector of the target word wi. The hidden neuron activation function is hyperbolic tangent or logistic sigmoid while the output neuron activation function is a V -way softmax. The back-propagation algorithm (Rumelhart et al. (1986)) trains an NNLM, often using stochastic gradient descent where the gradient is computed over several random subsets or batches of N-grams from the input training data. Researchers have proposed several variants of the feed forward NNLM, especially to model a longer input word history. Prominent examples include the recurrent neural network LM (Mikolov et al. (2010)) and bidirectional long-short term memory (LSTM) LM (Sundermeyer et al. (2012)).", "startOffset": 153, "endOffset": 1363}, {"referenceID": 4, "context": "We conducted language modeling experiments on a standard split of the Penn Treebank (UPenn) data set (Marcus et al. (1993)), as used in previous works such as (Mikolov et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 4, "context": "We conducted language modeling experiments on a standard split of the Penn Treebank (UPenn) data set (Marcus et al. (1993)), as used in previous works such as (Mikolov et al. (2011)).", "startOffset": 102, "endOffset": 182}, {"referenceID": 1, "context": "We implemented the DENNLM in Theano (Bergstra et al. (2011)) and trained it by optimizing the augmented loss function in (4) using Root-Mean Square Propagation (RMSProp) (Tieleman & Hinton (2012)), a variant of stochastic gradient descent.", "startOffset": 37, "endOffset": 60}, {"referenceID": 1, "context": "We implemented the DENNLM in Theano (Bergstra et al. (2011)) and trained it by optimizing the augmented loss function in (4) using Root-Mean Square Propagation (RMSProp) (Tieleman & Hinton (2012)), a variant of stochastic gradient descent.", "startOffset": 37, "endOffset": 196}, {"referenceID": 1, "context": "We implemented the DENNLM in Theano (Bergstra et al. (2011)) and trained it by optimizing the augmented loss function in (4) using Root-Mean Square Propagation (RMSProp) (Tieleman & Hinton (2012)), a variant of stochastic gradient descent. We tuned all hyper-parameters on the standard UPenn heldout set. Table 4 shows the test set perplexities of the baseline and the DENNLMs. We kept the NN model size comparable by reducing the size of each component NNLM. Our results show a significant improvement in perplexity by using a DENNLM over the 4-gram model, a single NNLM, and interpolation of randomly initialized NNLMs. The posterior correlation coefficients are significantly less than 0.99, which is the correlation coefficient for randomly initialized NNLMs. Note that the DENNLM significantly outperforms a standard NNLM of size (600,800) which has similar number of parameters. It is also clearly better then a randomly initialized set of 4 NNLM models. The perplexity results in Table 4 for our diverse feed-forward NNLMs are especially encouraging given the fact that the more advanced recurrent neural network (RNN) LM gives a perplexity of 124.7 by itself and 105.7 upon interpolation with a 5-gram LM on the same task (Mikolov et al. (2011)).", "startOffset": 37, "endOffset": 1253}], "year": 2015, "abstractText": "We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higherdimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.", "creator": "LaTeX with hyperref package"}}}