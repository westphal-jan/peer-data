{"id": "1412.7091", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets", "abstract": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D x d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, this case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach that, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d^2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm is expected to yield an actual speedup of at least D/4d, i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.", "histories": [["v1", "Mon, 22 Dec 2014 18:51:08 GMT  (14kb)", "http://arxiv.org/abs/1412.7091v1", null], ["v2", "Sat, 11 Apr 2015 04:02:12 GMT  (14kb)", "http://arxiv.org/abs/1412.7091v2", null], ["v3", "Tue, 14 Jul 2015 01:27:13 GMT  (1498kb,D)", "http://arxiv.org/abs/1412.7091v3", "15 pages technical report version"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["pascal vincent", "alexandre de br\u00e9bisson", "xavier bouthillier"], "accepted": true, "id": "1412.7091"}, "pdf": {"name": "1412.7091.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["vincentp@iro.umontreal.ca"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.70 91v1 [cs.NE] 2 2D ec"}, {"heading": "1 INTRODUCTION", "text": "Many modern applications of neural networks have to deal with data that can be represented as very large sparse vectors. Such representations arise in natural language tasks where the dimensionD of this vector typically reflects (multiples) the size of the vocabulary, but also in the sparse matrix of collaborative filter applications. It is trivial to handle very large sparse inputs to a neural network in a computationally efficient way: the forward propagation and updating of the input weight matrix are correspondingly sparse. On the other hand, training with very large sparse predictive targets is problematic when the goal is sparse, calculating the equally large network performance and updating the input weight matrix."}, {"heading": "2 THE PROBLEM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 PROBLEM DEFINITION AND SETUP", "text": "It is about the question to what extent it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about which it is about a way, in which it is about a way, in which it is about which it is about a way, and in which it is about which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is"}, {"heading": "2.2 THE EASY PART: INPUT LAYER FORWARD PROPAGATION AND WEIGHT UPDATE", "text": "It is simple and simple to calculate the forward propagation efficiently, and to update the backward propagation and the weight part for the input layer (1) if we have a very large dindimensional, but K \u2212 sparse input vector x with reasonable sparse representation. In particular, we assume that x is represented as a pair of vectors u, v of length (at most) K, where u has integer indices and v the associated real output values of the elements of x such that xi = 0, if i / x u, and xuk = vk.Forward propagation through the input layer The sparse representation of x as the positions of the K elements together with their value makes it cheap to calculate W (1) Tx. Even if W (1) can be a huge complete Din \u00b7 d matrix, only K of its rows (which correspond to the non-zero entries of x), we must calculate."}, {"heading": "2.3 THE HARD PART: OUTPUT LAYER PROPAGATION AND WEIGHT UPDATE", "text": "Given some network inputs x, we assume that by forward propagation we can easily calculate the associated last hidden layer representation h = Rd. From that point on: \u2022 Calculating the final output o = Wh incurs prohibitive computational costs of O (Dd), since W is a complete D \u00b7 d matrix. Note that a priori there is no reason why the representation h should be sparse (e.g. with a sigmoid nonliability), but even if it were, this would not fundamentally change the problem, since it is D, which is extremely large, and we have already assumed a reasonable size. \u2022 Calculating the residual quantity (o \u2212 t) and the associated quadratic error loss o \u2212 t \u00b2 2 causes additional O (D) costs. \u2022 The gradient to h, which we need to propagate back to reach lower layers, it is not a great help that all these layers are updated (matrix \u2212 Ty, since it is not another O)."}, {"heading": "3 A COMPUTATIONALLY EFFICIENT ALGORITHM FOR PERFORMING THE EXACT ONLINE GRADIENT UPDATE", "text": "We propose a different approach that leads to exactly the same but efficient gradient update, remarkably without ever having to calculate major output o. The core of our approach is that instead of working directly with W, we will instead work with a factorized view of W = V U, where V is a D \u00b7 d matrix (the same shape as W) and U is a smaller d \u00b7 d invertable matrix that we will initialize to identity. This will allow us to implicitly update W instead of explicitly updating it by changing V and / or U, which, as we will see, we can achieve mathematically much more efficiently. We will also have to maintain current versions of two d \u00b7 d-d matrices: U \u2212 1 and Q = WTW = UTV TV TV U. This can, as we will see, be achieved cost-effectively after rank-one updates of V and / or U."}, {"heading": "3.1 COMPUTING THE SQUARED ERROR LOSS EFFICIENTLY", "text": "Suppose we have a hTQh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yTWh \u2212 yT)."}, {"heading": "3.3 EFFICIENT GRADIENT UPDATE OF W", "text": "The gradient of the quadratic error loss in relation to the output coating weight matrix W is: MP = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D"}, {"heading": "3.4 BOOKKEEPING: KEEPING AN UP-TO-DATE Q AND U\u2212T", "text": "We have already seen in Eq.6 how we can maintain an up-to-date U-T inexpensively after updating U. Similarly, after updating U and V, we must maintain an up-to-date Q = WTW required to efficiently calculate the loss L (Eq.1) and the gradient H (Eq.2). The updating of U and V in Eq.4 and 5 is equivalent to an implicit update of W as in Eq.3, and this translates into the following update of Q = WTW: z = Qh \u2212 UT (V T y) Qnew = Q \u2212 2\u03b7 (hz, T + z, hhhT) + (4throu2L) hT (7) The proof is straightforward, but not provided here due to space constraints."}, {"heading": "3.5 PUTTING IT ALL TOGETHER: ALGORITHM FOR COMPUTING THE COST L, GRADIENT ON", "text": "The following table describes the algorithmic steps we have compiled from the above-derived equations. Step # Operation Calculation Complexity Number of multipliers1: h = Qh O (d2) d2 2: y = UT (V T y) O (Kd + d2) Kd + d23: z = h = h-y-O (d) d 4: h = 2z-O (d) d 5: L = hT h \u2212 2hT y + yT y O (2d + K) 2d + K + 1 6: Unew = U \u2212 2h (Uh) hT O (d2) 2d2 + d 7: U \u2212 Tnew = U \u2212 T + 2d + 2d \u2212 K (2d + K) 2d + d2 (new) + d2 (new) + d2 (new) + 2) + 2 (new + d2) (new + 2) + 2 (new + d2) + 2 (new + 2) (new d2) + 2) (new d2) + 2 (new + 2) (new d2) (new + 2) (new d2) + 2) (new d2) (new + 2) (new + 2) (new d2)"}, {"heading": "4 DISCUSSION: EXPECTED BENEFITS, EXTENSIONS AND LIMITATIONS", "text": "If we take K \u00b2 d, we can say more precisely that the proposed algorithm for calculating loss and gradient updates requires about 12d2 operations, whereas the standard approach requires about 3Dd operations. So, overall, the proposed algorithm change corresponds to a computational acceleration of a factor of D4d. Thus, for D = 200 000 and d = 500, the expected Speeddup is 100. Note that the advantage lies not only in computational complexity, but also in memory access. For each example, the standard approach must access and modify all D \u2212 d elements of the matrix W, while the proposed approach only uses the much smaller number of K \u00b7 d elements of V and the three d \u00b7 d matrices U, U \u2212 T, and Q. So overall, we have a much faster algorithm that does this implicitly."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research is supported by the NSERC."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "In NIPS\u201900,", "citeRegEx": "Bengio et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "Large-scale learning of embeddings with reconstruction sampling", "author": ["Y. Dauphin", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine learning,", "citeRegEx": "Dauphin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2011}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyvarinen"], "venue": "In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201910)", "citeRegEx": "Gutmann and Hyvarinen,? \\Q2010\\E", "shortCiteRegEx": "Gutmann and Hyvarinen", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In International Conference on Learning Representations: Workshops Track", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih and Kavukcuoglu,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Morin and Bengio,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "This has been a practical problem ever since Bengio et al. (2001) first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications.", "startOffset": 45, "endOffset": 66}, {"referenceID": 2, "context": "(2011)and the use of Noise Contrastive Estimation (Gutmann and Hyvarinen, 2010) in Mnih and Kavukcuoglu (2013) and Mikolov et al.", "startOffset": 50, "endOffset": 79}, {"referenceID": 1, "context": "The reconstruction sampling of Dauphin et al. (2011)and the use of Noise Contrastive Estimation (Gutmann and Hyvarinen, 2010) in Mnih and Kavukcuoglu (2013) and Mikolov et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 1, "context": "The reconstruction sampling of Dauphin et al. (2011)and the use of Noise Contrastive Estimation (Gutmann and Hyvarinen, 2010) in Mnih and Kavukcuoglu (2013) and Mikolov et al.", "startOffset": 31, "endOffset": 157}, {"referenceID": 1, "context": "The reconstruction sampling of Dauphin et al. (2011)and the use of Noise Contrastive Estimation (Gutmann and Hyvarinen, 2010) in Mnih and Kavukcuoglu (2013) and Mikolov et al. (2013) fall under this category.", "startOffset": 31, "endOffset": 183}, {"referenceID": 5, "context": "\u2022 Hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013) imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 69}, {"referenceID": 3, "context": "\u2022 Hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013) imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 69}], "year": 2017, "abstractText": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D \u00d7 d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, this case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach that, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm is expected to yield an actual speedup of at least D 4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.", "creator": "LaTeX with hyperref package"}}}