{"id": "1412.7110", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Learning linearly separable features for speech recognition using convolutional neural networks", "abstract": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as, speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.", "histories": [["v1", "Mon, 22 Dec 2014 19:46:01 GMT  (215kb,D)", "http://arxiv.org/abs/1412.7110v1", "submitted for ICLR 2015 conference track"], ["v2", "Wed, 24 Dec 2014 13:46:10 GMT  (216kb,D)", "http://arxiv.org/abs/1412.7110v2", "Add references and correct typos. Submitted for ICLR 2015 conference track"], ["v3", "Fri, 23 Jan 2015 10:44:21 GMT  (217kb,D)", "http://arxiv.org/abs/1412.7110v3", "Revised Section 4.5. Add references and correct typos. Submitted for ICLR 2015 conference track"], ["v4", "Thu, 26 Feb 2015 19:51:35 GMT  (217kb,D)", "http://arxiv.org/abs/1412.7110v4", "Revisions according to reviews. Revised Section 4.5. Add references and correct typos. Submitted for ICLR 2015 conference track"], ["v5", "Fri, 27 Feb 2015 16:31:32 GMT  (218kb,D)", "http://arxiv.org/abs/1412.7110v5", "Revisions according to reviews. Revised Section 4.5. Add references and correct typos. Submitted for ICLR 2015 conference track"], ["v6", "Thu, 16 Apr 2015 08:29:14 GMT  (206kb,D)", "http://arxiv.org/abs/1412.7110v6", "Final version for ICLR 2015 Workshop; Revisions according to reviews. Revised Section 4.5. Add references and correct typos. Submitted for ICLR 2015 conference track"]], "COMMENTS": "submitted for ICLR 2015 conference track", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["dimitri palaz", "mathew magimai doss", "ronan collobert"], "accepted": true, "id": "1412.7110"}, "pdf": {"name": "1412.7110.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RAL NETWORKS", "Dimitri Palaz"], "emails": ["dimitri.palaz@idiap.ch", "mathew@idiap.ch,", "ronan@collobert.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In a first step, the data is transformed into characteristics that normally consist of a phase of dimensionality reduction and a phase of information selection based on the task-specific knowledge of the phenomena. These two phases have been carefully worked out by hand, resulting in state-of-the-art features such as CNN reception coefficients (MFCCs) or perceptual linear prediction cepstral characteristics (PLPs). In a second step, the probability of subword units such as phonemes is estimated on the basis of generative models or discriminatory models. In a final step, dynamic programming techniques are used to identify the word sequence based on lexical and syntactic relationships."}, {"heading": "2 MOTIVATION", "text": "In fact, it is as if most people who are able are able to understand things and understand what they are doing. (...) It is as if they were able to do the things that they are doing. (...) It is as if they were able to do the things that they are doing. (...) It is as if they were not able to do the things that they are doing. (...) It is as if they were not doing what they are doing. (...) It is as if they were doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is if they are doing it. (...) It is if they are doing it."}, {"heading": "3 CONVOLUTIONAL NEURAL NETWORKS", "text": "This section presents the architecture used in the paper, which is similar to that presented in (Palaz et al., 2013) and is presented here for the sake of clarity."}, {"heading": "3.1 ARCHITECTURE", "text": "Our network (see Figure 2) receives a sequence of raw input signals, divided into frames, and outputs a score for each class, for each frame. The network architecture consists of several filter extraction levels followed by a classification level. A filter extraction level comprises a wave layer followed by a temporal pooling layer and a nonlinearity. Processed signal coming from these levels is fed to a classification level, which in our case can be either a multi-layer perceptron (MLP) or a single linear layer (SLP). It outputs the conditional probabilities p (i | x) for each class i for each frame x."}, {"heading": "3.2 CONVOLUTIONAL LAYER", "text": "While \"classical\" linear layers in standard MLPs accept a fixed-size input vector, it is assumed that a folding layer is fed with a sequence of T-vectors / frames: X = {x1 x2... xT}. A folding layer applies the same linear transformation to all successive (or dW frames in between) windows of kW frames. For example, the transformation on frame t is formally written as follows: M xt \u2212 (kW \u2212 1) / 2... xt + (kW \u2212 1) / 2, (1) where M is a dout \u00b7 din matrix of parameters. In other words, dout filters (rows of matrix M) are applied to the input sequence."}, {"heading": "3.3 MAX-POOLING LAYER", "text": "These layers perform local temporal max operations via an input sequence. Formally, the transformation on frame t is described as follows: max t \u2212 (kW \u2212 1) / 2 \u2264 s \u2264 t + (kW \u2212 1) / 2xds \u0445 d (2) with x as input and d as dimension. These layers increase the robustness of the network up to minor time distortions in the input range."}, {"heading": "3.4 SOFTMAX LAYER", "text": "The Softmax layer (Bridle, 1990) interprets network output values fi (x) as conditional probabilities, for each class name i: p (i | x) = e fi (x) \u2211 jefj (x) (3)"}, {"heading": "3.5 NETWORK TRAINING", "text": "The network parameters are learned by maximizing the log probability L, which is given by: L (\u03b8) = N \u2211 n = 1log (p (in | xn, \u03b8)) (4) for each input x and label i, over the entire training set, in relation to the parameters of each network layer. If the log sumexp process is defined as: logsumexpi (zi) = log (\u2211 i e zi), the probability can be expressed as follows: L = log (p (i | x)) = fi (x) \u2212 logsumexp j (fj (x)) (5), where fi (x) describes the network score of input x and class i. Maximizing this probability is done by means of the stochastic gradient ascent algorithm (Bottou, 1991)."}, {"heading": "4 EXPERIMENTAL SETUP", "text": "In this section we present the two studies, the databases, the baselines and the hyperparameters of the networks."}, {"heading": "4.1 STUDIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 PHONEME RECOGNITION", "text": "As a first experiment, we propose a phoneme recognition study using the CNN-based system to estimate conditional probabilities of the phoneme class. The decoder is a standard HMM decoder with a limited duration of 3 states, where all phonemes are equally probable."}, {"heading": "4.1.2 LARGE VOCABULARY SPEECH RECOGNITION", "text": "We evaluate the scalability of the proposed system based on a large speech recognition task on the WSJ corpus. The CNN-based system is used to calculate the posterior probabilities of context-dependent phonemes. The decoder is an HMM. The scaled probabilities are estimated by dividing the posterior probabilities by the previous probability of each class estimated by counting on the training set. Hyperparameters such as language scaling factor and word insertion penalty are determined on the validation set."}, {"heading": "4.2 DATABASES", "text": "TIMIT's acoustic-phonetic corpus consists of 3,696 (sampled at 16 kHz) training expressions from 462 speakers, excluding the SA phrases; the set of cross-validation consists of 400 expressions from 50 speakers; the core test set was used to report the results; it contains 192 expressions from 24 speakers, excluding the set of validation; the 61 hand-labeled phonetic symbols are assigned to 39 phonemes, with an additional waste class, as illustrated in Lee & Hon, 1989; the set SI-284 of the Wall Street Journal (WSJ) corpus (Woodland et al., 1994) consists of the combination of data from WSJ0 and WSJ1 databases, as sampled in (Lee & Hon, 1989); the set contains 36416 sequences, representing approximately 80 hours of speech; and ten percent of the set was taken as a validation set."}, {"heading": "4.3 FEATURES", "text": "We also conducted several basic experiments in which MFCC served as an input marker. It was calculated (with HTK (Young et al., 2002)) using a 25 ms hamming window on the voice signal, with a displacement of 10 ms. The signal is represented by 13th order coefficients together with their first and second derivatives, which were calculated on the basis of a 9-frame context."}, {"heading": "4.4 BASELINE SYSTEMS", "text": "We compare our approach to the standard HMM / ANN system using ceptral features. We train a multi-layer perctron with a hidden layer called MLP and a linear single-layer perctron called SLP. The input to the MLPs is MFCC with multiple frames from the preceding and subsequent context. We do not train the network in advance."}, {"heading": "4.5 NETWORKS HYPER-PARAMETERS", "text": "The hyperparameters of the network are: the input window size win, according to the context taken with each example, the core width kWn and displacement dWn of the nth folding layer, the number of filters dout and the pooling width. We train the CNN-based system with several filter levels (consisting of folding and max pooling layers). We use between one and five filter levels. In the case of the linear classifier, the capacity of the system cannot be tuned directly. It depends on the size of the input of the classifier, which can be adjusted by manually tuning the hyperparameters. We therefore proposed two approaches to select the hyperparameters. In the first case, they were tuned by stopping the validation set early. Areas considered for grid search are shown in Table 2."}, {"heading": "5 RESULTS", "text": "The results for the task of phoneme detection based on the TIMIT corpus are presented in Table 3 for the best performance of the proposed system, along with the baseline. The number of parameters in the classifier and in the filter stages is also presented. Using a linear classifier, the proposed SLP-based system outperforms the MLP-based baseline with three or more filter stages. It can be noted that the performance of the proposed system appears to increase when most parameters are in the filter stages and not in the classification stage. In addition, our SLP-based system can almost match the performance of the MLP-based CNN system, with 60 times fewer parameters in the classifier. The results for the proposed system with a fixed number of parameters in Table 4, together with the base power and number of parameters in the classifier and filter stages. The proposed CNN-based system outperforms the SLP-based baseline with the same number of parameters in the classifier."}, {"heading": "6 DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 JOINT TRAINING", "text": "Traditionally, feature extraction and acoustic modeling are performed separately in the speech recognition system. Feature extraction is usually driven by knowledge, and the acoustic model is trained on the basis of the data. In the proposed approach, the steps of feature acquisition and acoustic modeling are trained jointly and fully on the basis of data. Such an approach allows the characteristics to be flexible, as they are efficiently learned with a particular classifier. In the presented studies, we show that it is possible to learn linearly separable characteristics and that the use of such characteristics can lead to similar or better results than the basic system. In this case, the capacity of the system lies largely at the learning level of the characteristics and not at the classification level. One potential application in language recognition for such a system with a simple classifier is language adaptation. In fact, training a classifier on a language with limited resources can be a difficult task, especially for training the acoustic model. The usual approach is to use it quickly, as a whole resource for a classifier with many languages, including English and very many starting languages."}, {"heading": "6.2 FEATURE LEARNING", "text": "In the proposed approach, the characteristics are learned through conventional neural networks; the entire architecture is designed empirically, i.e. by stopping the validation set early. We recognize that one of the most important hyperparameters is the core width of the first convolution, which takes a window of the temporal speech signal as input. We find that the best performance is achieved with a very narrow core, about 30 samples or 3 ms. In the case of conventional spectral-based characteristics as represented in Section 2, the time window width is about 30 ms. This width was selected on the basis of language production knowledge, whereas in our case the core width was found in a data-driven manner. In our previous studies, we showed that the first convolution layer can be viewed as a set of matching matching filters, each responding to different frequency bands (Palaz et al., 2013)."}, {"heading": "7 CONCLUSION", "text": "We have shown that the proposed system with a linear classifier performs better than an MLP-based system with conventional properties. These results suggest that CNNs are able to learn linearly separable characteristics in a data-driven manner, and that these characteristics can be as efficient as spectral-based characteristics. Furthermore, we show that the use of such an approach can lead to systems in which most of the capacity lies on the learning part of the characteristics, rather than on the classification part, which can be very simple. As future work, we plan to investigate the use of other types of classifiers, such as linear SVM, and examine language matching."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the HASLER Foundation (www.haslerstiftung.ch) with the grant \"Universal Spoken Term Detection with Deep Learning\" (DeepSTD) and the authors thank their colleague Ramya Rasipuram for providing the HMM / GMM baseline."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "G. Penn"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2012}, {"title": "Investigating deep neural network based transforms of robust audio features for lvcsr", "author": ["E. Bocchieri", "D. Dimitriadis"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Bocchieri and Dimitriadis,? \\Q2013\\E", "shortCiteRegEx": "Bocchieri and Dimitriadis", "year": 2013}, {"title": "Stochastic gradient learning in neural networks", "author": ["L. Bottou"], "venue": "In Proceedings of Neuro-Nmes", "citeRegEx": "Bottou,? \\Q1991\\E", "shortCiteRegEx": "Bottou", "year": 1991}, {"title": "Connectionist speech recognition: a hybrid approach, volume 247", "author": ["H. Bourlard", "N. Morgan"], "venue": null, "citeRegEx": "Bourlard and Morgan,? \\Q1994\\E", "shortCiteRegEx": "Bourlard and Morgan", "year": 1994}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["J.S. Bridle"], "venue": "In Neuro-computing: Algorithms, Architectures and Applications,", "citeRegEx": "Bridle,? \\Q1990\\E", "shortCiteRegEx": "Bridle", "year": 1990}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Learning a better representation of speech soundwaves using restricted boltzmann machines", "author": ["N. Jaitly", "G. Hinton"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Jaitly and Hinton,? \\Q2011\\E", "shortCiteRegEx": "Jaitly and Hinton", "year": 2011}, {"title": "Generalization and network design strategies", "author": ["Y. LeCun"], "venue": "Connectionism in Perspective,", "citeRegEx": "LeCun,? \\Q1989\\E", "shortCiteRegEx": "LeCun", "year": 1989}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "P. Pham", "Y. Largman", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["Lee", "K. F", "H.W. Hon"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "Lee et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1989}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Mohamed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2012}, {"title": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks", "author": ["D. Palaz", "R. Collobert", "M. Magimai.-Doss"], "venue": "In Proc. of Interspeech,", "citeRegEx": "Palaz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Palaz et al\\.", "year": 2013}, {"title": "Convolutional neural networks-based continuous speech recognition using raw speech signal", "author": ["D. Palaz", "M. Magimai.-Doss", "R. Collobert"], "venue": "Technical Report Idiap-RR-18-2014, Idiap Research Institute,", "citeRegEx": "Palaz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Palaz et al\\.", "year": 2014}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["T.N. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Convolutional neural networks for distant speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Signal Processing Letters,", "citeRegEx": "Swietojanski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Swietojanski et al\\.", "year": 2014}, {"title": "Acoustic modeling with deep neural networks using raw time signal for lvcsr", "author": ["Z. T\u00fcske", "P. Golik", "R. Schl\u00fcter", "H. Ney"], "venue": "In Interspeech,", "citeRegEx": "T\u00fcske et al\\.,? \\Q2014\\E", "shortCiteRegEx": "T\u00fcske et al\\.", "year": 2014}, {"title": "Large vocabulary continuous speech recognition using htk", "author": ["P.C. Woodland", "J.J. Odell", "V. Valtchev", "S.J. Young"], "venue": "In Proc. of ICASSP, volume ii,", "citeRegEx": "Woodland et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Woodland et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 6, "context": "Representations such as Mel filterbank output or log spectrum have been proposed in the context of deep neural networks (Hinton et al., 2012).", "startOffset": 120, "endOffset": 141}, {"referenceID": 13, "context": "In our recent study (Palaz et al., 2013), it was shown that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (LeCun, 1989) (CNNs).", "startOffset": 20, "endOffset": 40}, {"referenceID": 9, "context": ", 2013), it was shown that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (LeCun, 1989) (CNNs).", "startOffset": 172, "endOffset": 185}, {"referenceID": 14, "context": "We also showed that this system is scalable to large vocabulary speech recognition task (Palaz et al., 2014).", "startOffset": 88, "endOffset": 108}, {"referenceID": 15, "context": "(b) Typical CNN based pipeline using Mel filterbank (Sainath et al., 2013; Swietojanski et al., 2014)", "startOffset": 52, "endOffset": 101}, {"referenceID": 16, "context": "(b) Typical CNN based pipeline using Mel filterbank (Sainath et al., 2013; Swietojanski et al., 2014)", "startOffset": 52, "endOffset": 101}, {"referenceID": 7, "context": "In recent years, the deep neural network (DNN) and deep belief network (DBN) approaches have been proposed (Hinton et al., 2006), which yielded state-of-the-art results in speech recognition using neural networks composed of many hidden layers, initialized in an unsupervised manner.", "startOffset": 107, "endOffset": 128}, {"referenceID": 0, "context": "instance, Mel filterbank energies were used as input of convolutional neural networks based systems (Abdel-Hamid et al., 2012; Sainath et al., 2013).", "startOffset": 100, "endOffset": 148}, {"referenceID": 15, "context": "instance, Mel filterbank energies were used as input of convolutional neural networks based systems (Abdel-Hamid et al., 2012; Sainath et al., 2013).", "startOffset": 100, "endOffset": 148}, {"referenceID": 12, "context": "Deep neural network based systems using spectrum as input has also been proposed (Mohamed et al., 2012; Lee et al., 2009).", "startOffset": 81, "endOffset": 121}, {"referenceID": 10, "context": "Deep neural network based systems using spectrum as input has also been proposed (Mohamed et al., 2012; Lee et al., 2009).", "startOffset": 81, "endOffset": 121}, {"referenceID": 17, "context": "Extracting features from raw speech has also been investigated using DBNs (Jaitly & Hinton, 2011; T\u00fcske et al., 2014).", "startOffset": 74, "endOffset": 117}, {"referenceID": 13, "context": "In our recent studies (Palaz et al., 2013; 2014), it was shown that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (see Figure 1(c)).", "startOffset": 22, "endOffset": 48}, {"referenceID": 13, "context": "It is similar to the one presented in (Palaz et al., 2013), and is presented here for the sake of clarity.", "startOffset": 38, "endOffset": 58}, {"referenceID": 4, "context": "4 SOFTMAX LAYER The Softmax (Bridle, 1990) layer interprets network output scores fi(x) as conditional probabilities, for each class label i: p(i|x) = e fi(x) \u2211", "startOffset": 28, "endOffset": 42}, {"referenceID": 2, "context": "Maximizing this likelihood is performed using the stochastic gradient ascent algorithm (Bottou, 1991).", "startOffset": 87, "endOffset": 101}, {"referenceID": 18, "context": "The SI-284 set of the Wall Street Journal (WSJ) corpus (Woodland et al., 1994) is formed by combining data from WSJ0 and WSJ1 databases, sampled at 16 kHz.", "startOffset": 55, "endOffset": 78}, {"referenceID": 5, "context": "The experiments were implemented using the torch7 toolbox (Collobert et al., 2011).", "startOffset": 58, "endOffset": 82}, {"referenceID": 13, "context": "In our previous studies, we showed that the first convolution layer can be seen as a set of matching filters, each one responding to different frequency bands (Palaz et al., 2013).", "startOffset": 159, "endOffset": 179}, {"referenceID": 14, "context": "We later showed that these filters show some level of invariance across databases (Palaz et al., 2014).", "startOffset": 82, "endOffset": 102}, {"referenceID": 13, "context": "In our previous studies, we showed that the first convolution layer can be seen as a set of matching filters, each one responding to different frequency bands (Palaz et al., 2013). We later showed that these filters show some level of invariance across databases (Palaz et al., 2014). In the convolutional neural network framework, Swietojanski et al. (2014) used for instance Mel filterbank as input for the convolution and maxpooling layers.", "startOffset": 160, "endOffset": 359}], "year": 2014, "abstractText": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as, speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.", "creator": "LaTeX with hyperref package"}}}