{"id": "1412.7119", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Pragmatic Neural Language Modelling in Machine Translation", "abstract": "This paper presents an in-depth investigation on integrating neural language models in translation systems. Scaling neural language models is a difficult task, but crucial for real-world applications. This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques. We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. We explore the trade-offs between neural models and back-off n-gram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. We conclude with a set of recommendations one should follow to build a scalable neural language model for MT.", "histories": [["v1", "Mon, 22 Dec 2014 20:08:06 GMT  (448kb,D)", "https://arxiv.org/abs/1412.7119v1", null], ["v2", "Tue, 23 Dec 2014 02:17:28 GMT  (449kb,D)", "http://arxiv.org/abs/1412.7119v2", null], ["v3", "Fri, 20 Mar 2015 17:20:03 GMT  (218kb,D)", "http://arxiv.org/abs/1412.7119v3", "NAACL 2015"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["paul baltescu", "phil blunsom"], "accepted": true, "id": "1412.7119"}, "pdf": {"name": "1412.7119.pdf", "metadata": {"source": "CRF", "title": "Pragmatic Neural Language Modelling in Machine Translation", "authors": ["Paul Baltescu", "Phil Blunsom"], "emails": ["paul.baltescu@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The most popular language models are a \"back-off\" model with Kneser-Ney smoothing (Chen and Goodman, 1999); the \"back-off\" models are conceptually simple, very efficient in construction and query, and are considered extremely effective in translation; the neural language models are a newer class of language models (Bengio et al., 2003) that outperform the \"back-off\" models with intrinsic evaluations of perplexity (Chelba et al., 2013)."}, {"heading": "2 Model Description", "text": "As a basis for our study, we implement a probabilistic neural language model, as defined in Bengio et al. (2003). (1) For each word w in the vocabulary V, we learn two distributed representations qw and rw in the RD. The vector qw captures the syntactic and semantic role of the word w when w is part of a conditional context, while rw captures its predictive role. For some words wi in a given corpus, we leave hi the conditional context wi \u2212 1,., wi \u2212 n + 1. To find the conditional probability P (wi | hi), our model first calculates a context projection vector: p = f \u2212 1 x j = 1 Cjqhij, where Cj-RD \u00d7 D is context-specific transformation matrices and f is component-by-component accuracy of mathematical capabilities. (Our goal is to publish a scalable model for modeling the neural language: comparable / exle.com is a: http: / exle.com."}, {"heading": "2.1 Class Based Factorisation", "text": "The time complexity of the Softmax step is O (| V | \u00b7 D). One way to reduce this excessive calculation effort is to rely on a class-based factoring trick (Goodman, 2001). We break down the vocabulary into K classes {C1,.., CK} so that V = K i = 1 Ci and Ci-Cj = \u2205 1 \u2264 i < j \u2264 K.We define the conditional probabilities as: P (ci | hi) = P (ci | hi) P (wi | ci, hi), where ci is the class to which the word wi belongs, i.e. wi-Cci. We adjust the model definition to take into account the class probabilities P (ci | hi) as well. We associate a distributed representation sc and a bias term tc with each class c. The class-related probabilities are calculated using the projection vector p with a new scoring function soc (tc), + thi (tc)."}, {"heading": "2.2 Tree Factored Models", "text": "One can take the idea set out in the previous section a step further and construct a tree above the vocabulary V. The words in the vocabulary are used to label the leaves of the tree. Let n1,.., nk be the nodes on the way from the root (n1) to the leaf labeled with wi (nk). The probability of the word wi to follow the context is defined as: P (wi | hi) = k = 2P (nj | n1,.., nj \u2212 1, hi). We associate a distributed representation sn and the term tn for each node in the tree. The conditional probabilities are obtained by reusing the evaluation function: P (nj | n1,..) nj \u2212 1, hi) = exp (nj, hi) = exp (nj, hi) = np (nj, hi)))."}, {"heading": "2.3 Noise Contrastive Estimation", "text": "The new target is: J (D) hi = hi hi hi logical (H) j = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p: p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "3 Experimental Setup", "text": "In our experiments, we use data from the ACL workshop in Machine Translation.2 We train standard phrase-based translation systems for French \u2192 English \u2192 Czech and English \u2192 German using the Moses toolkit (Koehn et al., 2007).We used europarl and the news commentary corpora as parallel data for the training.2 Data is available here: http: / / www.statmt. org / wmt14 / translation-task.html.the translation systems. The parallel corpora were tokenized, lowercase and removed sentences using standard text processing.3 Table 2 contains statistics on the training companies after the pre-processing step. We adjusted the translation systems to the most recent data from 2013, using a minimal error rate in language training (Och, 2003) and we used the newer corpora to replace the unfilled EU linguistic commentary of 2007 BLS data from the training months, the training modules of 2007, and the training months."}, {"heading": "4 Normalisation", "text": "This year it is so far that it will only take a few weeks until it is so far again."}, {"heading": "5 Training", "text": "This section is about finding scalable training algorithms for neural language models. We examine the contrastive estimation of sounds as a much more efficient alternative to standard training with maximum probability of stochastic gradient descent. Class factor models allow us to do this research on a much larger scale than previous results (e.g. the WSJ corpus used by Mnih and Teh (2012) has a little more than 1M characters), thus gaining useful insights into how this method actually works on a scale. (In our experiments we use a 2B word corpus and a 100k vocabulary.) Table 9 summarizes our findings. We get a slightly better BLEU score with conventional gradient descent, but this is probably just noise from tuning the translation system with MERT. On the other hand, contrastive sound training reduces training time by a factor of 7.10 showing the amount of time needed to train each neural model in this essay."}, {"heading": "6 Diagonal Context Matrices", "text": "In this section, we examine diagonal context matrices as a source for reducing the calculation costs for calculating the projection vector. In the standard definition of a neural language model, these costs are dominated by the softmax step, but once tricks such as contrastive noise estimation or tree or class factorisations are used, this process becomes the main obstacle to training and querying the model. Using diagonal context matrices in calculating the projection layer reduces the time complexity from O (D2) to O (D). Similar optimization is achieved in the back propagation algorithm, as only O (D) context parameters need to be updated for each training step. Devlin et al. (2014) have also recognized the need to find a scalable solution for calculating the projection vector. Their approach is to cache the product between each word and each context matrix and look up these terms in a table as needed."}, {"heading": "7 Quality vs. Memory Trade-off", "text": "Neural speech models are a very attractive option for natural language applications, which are expected to run on mobile phones and raw material computers, where the typical amount of available memory is limited to 1-2 GB. Nowadays, it is increasingly common that these devices contain relatively powerful GPUs, which supports the idea that further scaling is possible if needed. On the other hand, adapting n-gram models to such devices is difficult because these models store the probability of each n-gram in the training data. In this section, we try to gain a further understanding of how these models operate under such conditions. In this analysis, we have used Heafield (2011) s drive-based implementation with quantization to build memory-efficient n-gram models. A 5-gram model trained on the English monolingual data introduced in Section 3 requires 12 GB of memory. We have randomly sampled sets with an acceptance ratio of 0.01 to construct smaller models and to observe their performance on a larger spectrum."}, {"heading": "8 Conclusion", "text": "This year, we are in a position to put ourselves at the top."}, {"heading": "Acknowledgments", "text": "This work was supported by the Xerox Foundation Award and the EPSRC grant number EP / K036580 / 1."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Oxlm: A neural language modelling framework for machine translation", "author": ["Paul Baltescu", "Phil Blunsom", "Hieu Hoang"], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Baltescu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baltescu et al\\.", "year": 2014}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Yoshua Bengio", "Jean-Sbastien Senecal"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio and Senecal.,? \\Q2008\\E", "shortCiteRegEx": "Bengio and Senecal.", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A. Botha", "Phil Blunsom"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML", "citeRegEx": "Botha and Blunsom.,? \\Q2014\\E", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "Chen and Goodman.,? \\Q1999\\E", "shortCiteRegEx": "Chen and Goodman.", "year": 1999}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Classes for fast maximum entropy training", "author": ["Joshua Goodman"], "venue": null, "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Kenlm: Faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT", "citeRegEx": "Heafield.,? \\Q2011\\E", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "A method for the construction of minimum-redundancy codes", "author": ["David A. Huffman"], "venue": "Proceedings of the Institute of Radio Engineers,", "citeRegEx": "Huffman.,? \\Q1952\\E", "shortCiteRegEx": "Huffman.", "year": 1952}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology,", "citeRegEx": "Liang.,? \\Q2005\\E", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "Strategies for training large scale neural network language models", "author": ["Tomas Mikolov", "Anoop Deoras", "Daniel Povey", "Lukas Burget", "Jan Cernocky"], "venue": "In Proceedings of the 2011 Automatic Speech", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom Mikolov", "Stefan Kombrink", "Luk Burget", "Jan ernock", "Sanjeev Khudanpur"], "venue": "In Proceedings of the 2011 IEEE International Conference on Acoustics,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech & Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Continuous-space language models for statistical machine translation", "author": ["Holger Schwenk"], "venue": "Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Schwenk.,? \\Q2010\\E", "shortCiteRegEx": "Schwenk.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "The most popular language model implementation is a back-off n-gram model with Kneser-Ney smoothing (Chen and Goodman, 1999).", "startOffset": 100, "endOffset": 124}, {"referenceID": 3, "context": "Neural language models are a more recent class of language models (Bengio et al., 2003) that have been shown to outperform back-off n-gram models using intrinsic evaluations of heldout perplexity (Chelba et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 6, "context": ", 2003) that have been shown to outperform back-off n-gram models using intrinsic evaluations of heldout perplexity (Chelba et al., 2013; Bengio et al., 2003), or when used in addition to traditional models in natural language systems such as speech recognizers (Mikolov et al.", "startOffset": 116, "endOffset": 158}, {"referenceID": 3, "context": ", 2003) that have been shown to outperform back-off n-gram models using intrinsic evaluations of heldout perplexity (Chelba et al., 2013; Bengio et al., 2003), or when used in addition to traditional models in natural language systems such as speech recognizers (Mikolov et al.", "startOffset": 116, "endOffset": 158}, {"referenceID": 17, "context": ", 2003), or when used in addition to traditional models in natural language systems such as speech recognizers (Mikolov et al., 2011a; Schwenk, 2007).", "startOffset": 111, "endOffset": 149}, {"referenceID": 20, "context": "It has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 4, "context": "It has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 1, "context": "It has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 18, "context": ", 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al., 2013).", "startOffset": 65, "endOffset": 99}, {"referenceID": 0, "context": ", 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al., 2013).", "startOffset": 65, "endOffset": 99}, {"referenceID": 5, "context": "and factoring the softmax layer using Brown clusters (Brown et al., 1992) provides the most pragmatic solution for fast training and decoding.", "startOffset": 53, "endOffset": 73}, {"referenceID": 3, "context": "As a basis for our investigation, we implement a probabilistic neural language model as defined in Bengio et al. (2003).1 For every word w in the vocabulary V , we learn two distributed representations qw and rw in RD.", "startOffset": 99, "endOffset": 120}, {"referenceID": 2, "context": "Several methods have been proposed to alleviate this problem: some applicable only during training (Mnih and Teh, 2012; Bengio and Senecal, 2008), while others may also speed up arbitrary queries to the language model (Morin and Bengio, 2005; Mnih and Hinton, 2009).", "startOffset": 99, "endOffset": 145}, {"referenceID": 15, "context": "Several methods have been proposed to alleviate this problem: some applicable only during training (Mnih and Teh, 2012; Bengio and Senecal, 2008), while others may also speed up arbitrary queries to the language model (Morin and Bengio, 2005; Mnih and Hinton, 2009).", "startOffset": 218, "endOffset": 265}, {"referenceID": 9, "context": "One option for reducing this excessive amount of computation is to rely on a class based factorisation trick (Goodman, 2001).", "startOffset": 109, "endOffset": 124}, {"referenceID": 15, "context": "Inducing high quality binary trees is a difficult problem which has received some attention in the research literature (Mnih and Hinton, 2009; Morin and Bengio, 2005).", "startOffset": 119, "endOffset": 166}, {"referenceID": 11, "context": "In our experiments, we use Huffman trees (Huffman, 1952) which do not have any linguistic motivation, but guarantee that a minimum number of nodes are accessed during training.", "startOffset": 41, "endOffset": 56}, {"referenceID": 14, "context": "Inducing high quality binary trees is a difficult problem which has received some attention in the research literature (Mnih and Hinton, 2009; Morin and Bengio, 2005). Results have been somewhat unsatisfactory, with the exception of Mnih and Hinton (2009), who did not release the code they used to construct their trees.", "startOffset": 120, "endOffset": 256}, {"referenceID": 20, "context": "This method has already been used for training neural language models for machine translation by Vaswani et al. (2013). The idea behind noise contrastive training is to transform a density estimation problem into a classification problem, by learning a classifier to discriminate between samples drawn from the data distribution and samples drawn for a known noise distribution.", "startOffset": 97, "endOffset": 119}, {"referenceID": 20, "context": "This method has already been used for training neural language models for machine translation by Vaswani et al. (2013). The idea behind noise contrastive training is to transform a density estimation problem into a classification problem, by learning a classifier to discriminate between samples drawn from the data distribution and samples drawn for a known noise distribution. Following Mnih and Teh (2012), we set the unigram distribution Pn(w) as the noise distribution and use k times more noise samples than data samples to train our models.", "startOffset": 97, "endOffset": 409}, {"referenceID": 16, "context": "We tuned the translation systems on the newstest2013 data using minimum error rate training (Och, 2003) and we used the newstest2014 corpora to report uncased BLEU scores averaged over 3 runs.", "startOffset": 92, "endOffset": 103}, {"referenceID": 10, "context": "To construct the back-off n-gram models, we used a compact trie-based implementation available in KenLM (Heafield, 2011), because otherwise we would have had difficulties with fitting these models in the main memory of our machines.", "startOffset": 104, "endOffset": 120}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al. (2014) alter their training objective to learn self-normalised models, i.", "startOffset": 0, "endOffset": 118}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al. (2014) alter their training objective to learn self-normalised models, i.e. models where the sum of the values in the output layer is (hopefully) close to 1. Vaswani et al. (2013) use noise contrastive estimation to speed up training, while Devlin et al.", "startOffset": 0, "endOffset": 291}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al. (2014) alter their training objective to learn self-normalised models, i.e. models where the sum of the values in the output layer is (hopefully) close to 1. Vaswani et al. (2013) use noise contrastive estimation to speed up training, while Devlin et al. (2014) train their models with standard gradient descent on a GPU.", "startOffset": 0, "endOffset": 373}, {"referenceID": 4, "context": "The second approach is to explicitly normalise the models, but to limit the set of words over which the normalisation is performed, either via class-based factorisation (Botha and Blunsom, 2014; Baltescu et al., 2014) or using a shortlist containing only the most frequent words in the vocabulary and scoring the remaining words with a back-off n-gram model (Schwenk, 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 1, "context": "The second approach is to explicitly normalise the models, but to limit the set of words over which the normalisation is performed, either via class-based factorisation (Botha and Blunsom, 2014; Baltescu et al., 2014) or using a shortlist containing only the most frequent words in the vocabulary and scoring the remaining words with a back-off n-gram model (Schwenk, 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 18, "context": ", 2014) or using a shortlist containing only the most frequent words in the vocabulary and scoring the remaining words with a back-off n-gram model (Schwenk, 2010).", "startOffset": 148, "endOffset": 163}, {"referenceID": 5, "context": "Table 7 compares two popular techniques for obtaining word classes: Brown clustering (Brown et al., 1992; Liang, 2005) and frequency binning (Mikolov et al.", "startOffset": 85, "endOffset": 118}, {"referenceID": 12, "context": "Table 7 compares two popular techniques for obtaining word classes: Brown clustering (Brown et al., 1992; Liang, 2005) and frequency binning (Mikolov et al.", "startOffset": 85, "endOffset": 118}, {"referenceID": 10, "context": "In this analysis, we used Heafield (2011)\u2019s triebased implementation with quantization for constructing memory efficient back-off n-gram models.", "startOffset": 26, "endOffset": 42}, {"referenceID": 19, "context": "We believe this result generalizes to other neural architectures such as neural translation models (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 99, "endOffset": 141}, {"referenceID": 8, "context": "We believe this result generalizes to other neural architectures such as neural translation models (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 99, "endOffset": 141}, {"referenceID": 5, "context": "We learn that the algorithm used for partitioning the vocabulary into classes has a strong impact on the overall quality and that Brown clustering (Brown et al., 1992) is a good choice.", "startOffset": 147, "endOffset": 167}], "year": 2015, "abstractText": "This paper presents an in-depth investigation on integrating neural language models in translation systems. Scaling neural language models is a difficult task, but crucial for real-world applications. This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques. We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. We explore the tradeoffs between neural models and back-off ngram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. We conclude with a set of recommendations one should follow to build a scalable neural language model for MT.", "creator": "LaTeX with hyperref package"}}}