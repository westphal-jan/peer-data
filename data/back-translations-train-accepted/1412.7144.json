{"id": "1412.7144", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Fully Convolutional Multi-Class Multiple Instance Learning", "abstract": "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, eliminating the need for object proposals based pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC 2011 segmentation data.", "histories": [["v1", "Mon, 22 Dec 2014 20:49:54 GMT  (2090kb,D)", "http://arxiv.org/abs/1412.7144v1", null], ["v2", "Sat, 24 Jan 2015 01:17:59 GMT  (2090kb,D)", "http://arxiv.org/abs/1412.7144v2", null], ["v3", "Sat, 7 Feb 2015 02:12:26 GMT  (2091kb,D)", "http://arxiv.org/abs/1412.7144v3", null], ["v4", "Wed, 15 Apr 2015 05:31:10 GMT  (2091kb,D)", "http://arxiv.org/abs/1412.7144v4", "in ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["deepak pathak", "evan shelhamer", "jonathan long", "trevor darrell"], "accepted": true, "id": "1412.7144"}, "pdf": {"name": "1412.7144.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["INSTANCE LEARNING", "Deepak Pathak", "Evan Shelhamer", "Jonathan Long", "Trevor Darrell"], "emails": ["pathak@cs.berkeley.edu", "shelhamer@cs.berkeley.edu", "jonlong@cs.berkeley.edu", "trevor@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Multiple Instance Learning (MIL) can reduce the need for costly annotations on tasks such as semantic segmentation by mitigating the level of monitoring required. We propose a novel MIL formulation of multi-level semantic segmentation learning through a fully revolutionary network. In this context, we are trying to learn a semantic segmentation model from only weak image-level labels, with end-to-end training to jointly optimize representation while rebutting the mapping of pixel-image labels. Full, revolutionary training accepts input of any size, eliminating the need for object pre-processing, and provides a pixel-by-pixel loss card for selecting latent instances. Our Multiclass MIL loss uses the further monitoring provided by multi-label images. We evaluate this approach through preliminary experiments based on the PASCAL VOC 2011 segmentation data."}, {"heading": "1 INTRODUCTION", "text": "Following the ILSVRC12 gain image classifiers from Krizhevsky et al. (2012), most in-depth learning methods point to heavily commented data that is very time consuming in these settings. Learning from weak supervision, though hard, would bypass the annotation costs to expand the available captions. In this paper, we propose a novel framework for multiple learning (MIL) with a fully revolutionary network (FCN). The task is to learn pixel-level semantic segmentation that only signals the presence or absence of an object. Images that are not centered on the object or contain multiple objects make the insight of semantic segmentation difficult."}, {"heading": "2 FULLY CONVOLUTIONAL MIL", "text": "A Fully Convolutionary Network (FCN) is a model designed for spatial prediction problems. Each layer of an FCN computes a local operation based on relative spatial coordinates. In this way, an FCN can make an input of any size and generate output of corresponding dimensions. For the purpose of poorly supervised MIL learning, the FCN allows efficient selection of training instances. The FCN predicts an output card for all pixels and has a corresponding loss card on each pixel. This loss card can be masked or otherwise manipulated to select instances for calculating the loss and back propagation for learning. We use the VGG network (Simonyan & Zisserman, 2014) and adapt it to the fully revolutionary form proposed in Long et al. (2014) to select and select semantic segmentations."}, {"heading": "3 MULTI-CLASS MIL LOSS", "text": "This selection is made possible by the output map created by FCN, i.e. for an image of any size, FCN outputs corresponding to large thermal images corresponding to each output class (including background). We identify the maximum score in the coarse thermal cameras of the classes present in the image and background, and the loss is then calculated only on these coarse pixels and propositioned backwards until startup. We ignore the loss of thermal cameras without maximum score, inspired by the alternating optimization in the binary MIL problem. The background class is analogous to the negative instances and competes with the other positive object classes. Let us leave the input image I, its label as LI (including background label), and p (x, y) as output of thermal cameras corresponding to the Lth label at the location (x, y), defining the loss as: (xl, IX, IX) IX."}, {"heading": "4 EXPERIMENTS", "text": "We validate our approach by preliminary experiments with PASCAL VOC 2011 segmentation dataset. The VGG-FCN network is refined using MIL loss from the model weights learned from ILSVRC12 with 1K categories. Long et al. (2014) refines itself only by means of weights adjusted by the representation layers, and not by means of the classification layer, as they have access to complete supervision. However, in our setting we observe significant improvements as we begin with adjusted classification layer weights for the two classes. Including this classification layer prevents degenerated solutions from all background layers. Table 1: Results on PASCAL VOC 2011 segmentation data. Fine-tuning with MIL loss achieves 58% relative improvement over baseline. Approach means IOUbaseline (no classifier) 3.52% baseline (w classifier) 12.96% FCN-MIL 20.46% encouraging results."}, {"heading": "5 DISCUSSION", "text": "We propose a novel model of collective learning of multiple instances and representation, inspired by the binary MIL, with pixel-by-pixel multiclass loss, which is learned end-to-end as a fully revolutionary network for the task of weakly monitored semantic segmentation. It eliminates the need to initialize instance hypotheses and any kind of suggestion mechanisms, with the advantage of much faster inference (\u2248 1 / 3 sec). These results are encouraging and can be further improved. Currently, we are using bilinear implementation to enlarge the picture that leads to rough results, as shown in Figure 1. These can be improved by conditional random fields using superpixel information (Achanta et al., 2012). Furthermore, controlling learning in the video network by manipulating the loss card in this way could have other applications such as hard negative mining."}], "references": [{"title": "Slic superpixels compared to state-of-the-art superpixel methods", "author": ["Achanta", "Radhakrishna", "Shaji", "Appu", "Smith", "Kevin", "Lucchi", "Aurelien", "Fua", "Pascal", "Susstrunk", "Sabine"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Achanta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Achanta et al\\.", "year": 2012}, {"title": "Confidence-rated multiple instance boosting for object detection", "author": ["K. Ali", "K. Saenko"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Ali and Saenko,? \\Q2014\\E", "shortCiteRegEx": "Ali and Saenko", "year": 2014}, {"title": "Support vector machines for multiple-instance learning", "author": ["Andrews", "Stuart", "Tsochantaridis", "Ioannis", "Hofmann", "Thomas"], "venue": "In Proc. NIPS, pp", "citeRegEx": "Andrews et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Andrews et al\\.", "year": 2002}, {"title": "Multi-fold mil training for weakly supervised object localization", "author": ["Cinbis", "Ramazan Gokberk", "Verbeek", "Jakob", "Schmid", "Cordelia"], "venue": "In CVPR,", "citeRegEx": "Cinbis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cinbis et al\\.", "year": 2014}, {"title": "Object detection with discriminatively trained part-based models", "author": ["Felzenszwalb", "Pedro F", "Girshick", "Ross B", "McAllester", "David", "Ramanan", "Deva"], "venue": "IEEE Tran. PAMI,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In In Proc. CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["Heckerman", "David"], "venue": "arXiv preprint arXiv:1304.1511,", "citeRegEx": "Heckerman and David.,? \\Q2013\\E", "shortCiteRegEx": "Heckerman and David.", "year": 2013}, {"title": "Detector discovery in the wild: Joint multiple instance and representation learning", "author": ["Hoffman", "Judy", "Pathak", "Deepak", "Darrell", "Trevor", "Saenko", "Kate"], "venue": "arXiv preprint arXiv:1412.1135,", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proc. NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1411.4038,", "citeRegEx": "Long et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Long et al\\.", "year": 2014}, {"title": "Weakly supervised object recognition with convolutional neural networks", "author": ["Oquab", "Maxime", "Bottou", "L\u00e9on", "Laptev", "Ivan", "Sivic", "Josef"], "venue": "In Proc. NIPS,", "citeRegEx": "Oquab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Weakly-supervised discovery of visual pattern configurations", "author": ["Song", "Hyun Oh", "Lee", "Yong Jae", "Jegelka", "Stefanie", "Darrell", "Trevor"], "venue": "In Proc. NIPS,", "citeRegEx": "Song et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Song et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "(2012), progress on detection (Girshick et al., 2014) and segmentation (Long et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 9, "context": ", 2014) and segmentation (Long et al., 2014) demonstrates that convnets can likewise address local tasks with structured output.", "startOffset": 25, "endOffset": 44}, {"referenceID": 3, "context": "MIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation.", "startOffset": 53, "endOffset": 93}, {"referenceID": 12, "context": "MIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation.", "startOffset": 53, "endOffset": 93}, {"referenceID": 2, "context": "Most MIL problems are framed as max-margin learning (Andrews et al., 2002; Felzenszwalb et al., 2010), while other approaches use boosting (Ali & Saenko, 2014) or Noisy-OR models (Heckerman, 2013).", "startOffset": 52, "endOffset": 101}, {"referenceID": 4, "context": "Most MIL problems are framed as max-margin learning (Andrews et al., 2002; Felzenszwalb et al., 2010), while other approaches use boosting (Ali & Saenko, 2014) or Noisy-OR models (Heckerman, 2013).", "startOffset": 52, "endOffset": 101}, {"referenceID": 3, "context": "Following the ILSVRC12-winning image classifier of Krizhevsky et al. (2012), progress on detection (Girshick et al.", "startOffset": 51, "endOffset": 76}, {"referenceID": 2, "context": "MIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation. Oquab et al. (2014) improve image classification by inferring latent object location, but do not evaluate the localization.", "startOffset": 54, "endOffset": 160}, {"referenceID": 2, "context": "MIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation. Oquab et al. (2014) improve image classification by inferring latent object location, but do not evaluate the localization. Hoffman et al. (2014) does MIL finetuning on top but relies on bounding box proposals and supervised data for representation learning.", "startOffset": 54, "endOffset": 286}, {"referenceID": 9, "context": "We use the VGG net (Simonyan & Zisserman, 2014) architecture and adapt it to the fully convolutional form as suggested in Long et al. (2014) for semantic segmentation.", "startOffset": 122, "endOffset": 141}, {"referenceID": 9, "context": "Long et al. (2014) fine-tunes only from the weights adapted from representation layers, and not classifier layer, as they have access to complete supervision.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained endto-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, eliminating the need for object proposals based pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC 2011 segmentation data.", "creator": "LaTeX with hyperref package"}}}