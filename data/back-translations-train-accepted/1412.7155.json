{"id": "1412.7155", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Learning Compact Convolutional Neural Networks with Nested Dropout", "abstract": "Recently, nested dropout has been shown as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in unsupervised learning. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity. Additionally, we hope that ordering parameters may provide additional insights into optimization of deep convolutional neural networks and how the network architecture impacts performance.", "histories": [["v1", "Mon, 22 Dec 2014 20:59:58 GMT  (134kb,D)", "https://arxiv.org/abs/1412.7155v1", null], ["v2", "Fri, 16 Jan 2015 01:47:57 GMT  (195kb,D)", "http://arxiv.org/abs/1412.7155v2", "Under review as a workshop paper at ICLR 2015"], ["v3", "Sat, 28 Feb 2015 00:07:59 GMT  (131kb,D)", "http://arxiv.org/abs/1412.7155v3", "Under review as a workshop paper at ICLR 2015"], ["v4", "Fri, 10 Apr 2015 06:11:22 GMT  (131kb,D)", "http://arxiv.org/abs/1412.7155v4", "4 pages, 2 figures. Accepted as a workshop contribution at ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["chelsea finn", "lisa anne hendricks", "trevor darrell"], "accepted": true, "id": "1412.7155"}, "pdf": {"name": "1412.7155.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["NESTED DROPOUT", "Chelsea Finn", "Lisa Anne Hendricks", "Trevor Darrell"], "emails": ["trevor}@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "Recently, a nested dropout was proposed as a method of arranging representation units in autoencoders based on their information content without reducing reconstruction costs (Rippel et al., 2014), but it was only used in training fully connected autoencoders in an unattended environment. We are investigating the effects of a nested dropout on the layers of a CNN trained by backpropagation and whether a nested dropout can provide a simple and systematic way to determine the optimal display size in terms of the desired accuracy and task and data complexity."}, {"heading": "1 MOTIVATION", "text": "One disadvantage of current such approaches, however, is that the selection of such architectures is largely optimized by hand, with researchers explicitly seeking architectural hyperparameters through cross-validation. To our knowledge, there is no deep visual network that bases its representation capacity on the complexity of the available data or tasks, and the recently proposed nested dropout method achieves this implicitly by exploring it in the context of learning optimal sparse models. To our knowledge, it is not possible to increase its representation capacity based on the complexity of the available data or tasks, and the recently proposed nested dropout method achieves this by learning deep units of representation in an incremental way. We examine whether such an approach is applicable to CNN visual models, and propose a visual CNN model that can scale its capacity according to complexity."}, {"heading": "2 NESTED DROPOUT ON CNNS", "text": "The nested failure algorithm for a convolutionary layer with n channels is as follows: For each sample in a mini-batch, we extract a number k from a geometric distribution and remove the latter n \u2212 k channels from the output of the layer. Nested failure layers can also be applied to several layers in a network by applying nested failure steps iteratively to each layer. First, the number of filters ni for layer i is determined by nested failure steps. After the number of filters ni in layer i is specified, nested failure steps can be used to determine the number of filters ni + 1 for layer i + 1. We trained our CNNs using stochastic gradient descension (SGD) and mini-batches of 100 samples. Since failed units are determined by dragging from a geometric distribution, low-index units are rarely omitted and therefore contract quickly, while the latter units are slowly and thus very frequently learned to fail."}, {"heading": "3 EXPERIMENTS", "text": "We apply nested dropouts to the first revolutionary layer of a CNN, which is trained to classify images in the CIFAR 10 dataset using the standard caffe architecture and training at a fixed learning rate. In Figure 1, we show the test accuracy of a single network trained with nested dropouts as a function of the number of convective filters. We report on the accuracy of k-filters by using the partially tracked network after the kth filter has converged, and test it against the last n \u2212 k filters that have failed. We compare two naive approaches to select the number of filters in convective filters. The first approach simply forms 32 separate networks that differ in the number of convective filters. The second approach trains a single network of 32 filters, but at test times a different number of convergence 1 filters are used with the remaining ones."}, {"heading": "4 DISCUSSION", "text": "In summary, we have developed a simple method for determining a more compact representation of the wavelayers. In our experiments, we learned a representation that achieved the same classification accuracy with 23 Conv1 filters and 25 Conv2 filters, instead of 32 Conv2 filters each within the same optimization framework. A major advantage of our method is that it allows the network to gradually increase network capacity during training. Furthermore, we hope that order parameters in the future will provide insights into the optimization of deep Convolutionary Neural Networks and how the network architecture affects performance."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was partially supported by the DARPA programs MSEE and SMISC, NSF awards IIS1427425, IIS-1212798 and IIS-1116411, Toyota, and the Berkeley Vision and Learning Center. Chelsea Finn was supported by a Berkeley EECS Fellowship, and Lisa Anne Hendricks by an NDSEG Fellowship."}], "references": [{"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A", "Howard", "Richard E", "Jackel", "Lawrence D"], "venue": "In NIPs,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Learning ordered representations with nested dropout", "author": ["Rippel", "Oren", "Gelbart", "Michael A", "Adams", "Ryan P"], "venue": "arXiv preprint arXiv:1402.0915,", "citeRegEx": "Rippel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2014}, {"title": "Efficient object localization using convolutional networks", "author": ["Tompson", "Jonathan", "Goroshin", "Ross", "Jain", "Arjun", "LeCun", "Yann", "Bregler", "Christoph"], "venue": "CoRR, abs/1411.4280,", "citeRegEx": "Tompson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tompson et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost (Rippel et al., 2014).", "startOffset": 170, "endOffset": 191}, {"referenceID": 0, "context": "This is traditionally applied to convolutional and fully-connected layers during training time, and has been shown to act as a regularizer, discouraging over-fitting to the training data (Hinton et al., 2012), though it has also been applied to entire channels of a convolutional layer output by Tompson et al.", "startOffset": 187, "endOffset": 208}, {"referenceID": 2, "context": "Empirically, when training large networks, such as those trained on ImageNet, drop out is necessary to avoid over fitting (Krizhevsky et al., 2012).", "startOffset": 122, "endOffset": 147}, {"referenceID": 4, "context": "When applied to a single layer semi-linear autoencoder, this technique has been proven to enforce an ordering of the units by their information capacity, while not decreasing the flexibility of the representation nor the quality of the resulting solution (Rippel et al., 2014).", "startOffset": 255, "endOffset": 276}, {"referenceID": 1, "context": "The primary contributions of this paper are to (1) demonstrate that nested dropout can successfully be applied to convolutional layers trained by back-propagation, (2) propose nested dropout as an advantageous method to learn CNNs that adapt to task and data complexity in a deep learning setting, and (3) provide our implementation in Caffe (Jia et al., 2014), a widely used deep learning framework, upon publication.", "startOffset": 342, "endOffset": 360}, {"referenceID": 0, "context": "Model selection for network architectures has been explored in the context of learning network connectivity dating back to Optimal Brain Damage from LeCun et al. (1989) and has been continued to be explored in the context of learning optimal sparse models.", "startOffset": 149, "endOffset": 169}, {"referenceID": 0, "context": "This is traditionally applied to convolutional and fully-connected layers during training time, and has been shown to act as a regularizer, discouraging over-fitting to the training data (Hinton et al., 2012), though it has also been applied to entire channels of a convolutional layer output by Tompson et al. (2014). Empirically, when training large networks, such as those trained on ImageNet, drop out is necessary to avoid over fitting (Krizhevsky et al.", "startOffset": 188, "endOffset": 318}], "year": 2015, "abstractText": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost (Rippel et al., 2014). However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.", "creator": "LaTeX with hyperref package"}}}