{"id": "1412.7272", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2014", "title": "Learning Non-deterministic Representations with Energy-based Ensembles", "abstract": "The goal of a generative model is to capture the distribution underlying the data, typically through latent variables. After training, these variables are often used as a new representation, more effective than the original features in a variety of learning tasks. However, the representations constructed by contemporary generative models are usually point-wise deterministic mappings from the original feature space. Thus, even with representations robust to class-specific transformations, statistically driven models trained on them would not be able to generalize when the labeled data is scarce. Inspired by the stochasticity of the synaptic connections in the brain, we introduce Energy-based Stochastic Ensembles. These ensembles can learn non-deterministic representations, i.e., mappings from the feature space to a family of distributions in the latent space. These mappings are encoded in a distribution over a (possibly infinite) collection of models. By conditionally sampling models from the ensemble, we obtain multiple representations for every input example and effectively augment the data. We propose an algorithm similar to contrastive divergence for training restricted Boltzmann stochastic ensembles. Finally, we demonstrate the concept of the stochastic representations on a synthetic dataset as well as test them in the one-shot learning scenario on MNIST.", "histories": [["v1", "Tue, 23 Dec 2014 07:06:55 GMT  (970kb,D)", "http://arxiv.org/abs/1412.7272v1", "9 pages, 3 figures, ICLR-15"], ["v2", "Wed, 22 Apr 2015 10:04:49 GMT  (970kb,D)", "http://arxiv.org/abs/1412.7272v2", "9 pages, 3 figures, ICLR-15 workshop contribution"]], "COMMENTS": "9 pages, 3 figures, ICLR-15", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["maruan al-shedivat", "emre neftci", "gert cauwenberghs"], "accepted": true, "id": "1412.7272"}, "pdf": {"name": "1412.7272.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Maruan Al-Shedivat"], "emails": ["maruan.shedivat@kaust.edu.sa", "nemre@ucsd.edu", "gert@ucsd.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "2 ENERGY-BASED STOCHASTIC ENSEMBLES", "text": "The distribution of the binary data vectors v \u00b2 0, 1 \u00b2 D can be encoded with the following energy-based model (EBM), which has some binary hidden variables h \u00b2 0, 1 \u00b2 K: P (v \u00b2, h \u00b2 E) = e \u2212 E (v \u00b2, h \u00b2 E) Z (\u03b8), (1), in which \u03b8 denotes the model parameters, E (v \u00b2 h \u00b2 E) is a parametric scalar energy function (LeCun et al., 2006), and Z (\u03b1) is the normalization coefficient (partition function). If we prescribe a distribution by the model parameters (i.e., disturbing properties according to any noise generation process), we obtain an energy-based stochastic ensemble (EBSE). To optimize the distribution over the models within the ensemble, we parameterise the process of noise generation with H \u00b2."}, {"heading": "2.1 LOG-LIKELIHOOD OPTIMIZATION", "text": "The gradient of the function of the Log probability (3) may be described as follows: LogP (v v; \u03b1) LogP (v; \u03b1) LogP (v; VI) VI (v) VI (v) VI (v; VI) VI (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV (v) IV) IV (v) IV) IV (v) IV (v) IV (v) IV) IV (v) IV) IV (V) IV (V) IV) IV (V) IV) IV (V) IV) IV (IV) IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV)) (IV) (IV) (IV) (IV) (IV)) (IV) (IV) (IV) (IV) IV) (IV) (IV) (IV) IV) (IV) (IV) IV) (IV) (IV) IV) IV (IV) IV) IV (IV) IV) IV (V) IV (IV) IV) IV (V) IV) IV (V) IV (V) IV) IV (V) IV (V) IV (IV) IV) IV (V) IV (V) IV (V) IV (V) IV) IV (IV) IV (IV) IV (IV) IV) IV (IV) IV (V) IV (IV) IV) IV (IV) IV (IV) IV (IV) IV (IV) IV) IV (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV) (IV (IV) (IV) (IV) (IV) (IV"}, {"heading": "2.2 MODEL STRUCTURE", "text": "The energy function of RBM is linear in each of the variables v, h, and \u03b8: E (v, h, \u03b8) = \u2212 (vTWh + bTv + cTh) = \u2212 (p, i, jWijvihj + p, ibivi, jcjhj), (7) where the parameters are represented by a tuple (W, b, c), andW, RD, b, RD, c, RK, c, RBM, is an undirected graphical model that can be represented by two layers of interconnected probability units (Figure 1b), which can be regarded as neurons with a probable sigmoid activation function, and the graphical model can be represented by a two-layer neural network."}, {"heading": "2.3 TRAINING", "text": "We propose the expectation maximization k-step contrastive divergence algorithm for the formation of RBSE (summarized in algorithm 1) with two different types of stochastic compounds - Bernoulli and Gauss - between the visible and the hidden layers. However, to perform the E-step, we have to calculate the expectations in (6). We use Monte Carlo estimates of these expectations: E [\u00b7] P (\u03b8; h; \u03b1) = [\u00b7 dvdh P (v, h; \u03b1)."}, {"heading": "3 EXPERIMENTS", "text": "We introduce the concept of stochastic representations by looking at a semi-supervised learning scenario in which a large amount of data is available, but only a few labeled examples per class. RBSE is a generative model, and therefore it can be trained unattended on the entire data set. Once the ensemble distribution P (\u03b8; \u03b1) is aligned with the data, we can conditionally select models \u03b8 \u00b2 P (\u03b8 | v) for each labeled training example, and then use each model to create a representation based on the activation of the hidden units. In other words, the generative stochastic ensembles can be used to store information about the entire data set and effectively increase the number of labeled examples by generating multiple stochastic representations, corresponding to the concept of corrupt features (Maaten et al., 2013; Wager et al., 2013). The main difference is that RBSEs can learn from the unlabeled part of the data sets how to corrupt the data."}, {"heading": "3.1 SYNTHETIC DATA", "text": "To visually test how a stochastic ensemble can manipulate training data, we created several simple two-dimensional datasets {x1, x2} and [0, 1] 2 (Figure 2). We trained an ordinary RBM with 2 visible and 8 hidden units and one RBSE of the same structure. Using these models, we mapped the test points multiple times to the latent space and then back into the original space. For RBM, we mapped the middle field activations for new representations: hnew = P (h | v) and vnew = P (v | hnew). It is not surprising that the two successive transformations, from visible to hidden and back into visible space, were always mapped for themselves by a properly trained RBM (Figure 2a). Note that this is not only true for RBM. Other point-by-point representation methods, such as autoencoders, have the same behavior in 2009."}, {"heading": "3.2 ONE-SHOT LEARNING OF MNIST DIGITS", "text": "We have a Bernoulli RBSE model with 784 visible and 400 hidden units on 50,000 unlabeled MNIST digits using the proposed algorithm 1 with a number of MCMC steps k = 1. The learned filters (i.e. the values of W-ij) and the Bernoulli connection probabilities (i.e., Pij) are presented on Figure 3. Also note that the connection probabilities encode a lot of structure (Figure 3b). For comparison purposes, we have trained an RBM of the same configuration on the same unlabeled MNIST data. Furthermore, we have sampled 1,000 objects (100 per class) from the remaining 20,000 MNIST digits: 1 training and 99 test examples per class."}, {"heading": "4 DISCUSSION", "text": "In this paper, we presented the concept of non-deterministic representations that can be learned from energy-based stochastic ensembles that are tuned to the data in an unsupervised manner; the stochasticity of the ensemble can capture the variance of the data and be used to adaptively regulate discriminatory models and improve their performance in semi-supervised environments; the actual learning of a correct disturbance of the model from the data is the conceptual difference from the previous work (Bachman et al., 2014; Maaten et al., 2013; Wager et al., 2013); we demonstrated the power of stochastic ensembles visually using synthetic two-dimensional data and quantitatively on one-time learning of the MNIST hand-written digitalis.The inspiration from synaptic stochasticity observed in biological nerve cells provides a set of insights and hypotheses for experimental neuroscience that we will report separately."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Cian O'Donnell for the helpful discussions. This work was partly funded by the National Science Foundation (NSF EFRI-1137279) and the Office of Naval Research (ONR MURI 14-13-1-0205)."}, {"heading": "A POSTERIOR BERNOULLI AND GAUSSIAN DISTRIBUTIONS", "text": "In this section we find three parameters: connecting strength between the visible and hidden levels W, RD, K and RK for visible and hidden units. If the equations for Wij, bi and cj have the same shape, we refer to these components as phenomena. For some fixed (v, h) we know that the energy of RBSE is a linear function of RBSE. Since the previous P, RBSE is responsible for all components of our definition, we can call these components phenomena."}, {"heading": "B EXPECTATIONS OVER THE POSTERIOR", "text": "Here we offer details for the analytical calculation of the expected stochastic course = Hi-K-course = Hi-K-course = Hi-K-course = Hi-K-course = Hi-K-course = Hi-K-course = Hi-K-course = Hi-K-course = Hi-K-course = Hi-K-course = Hi-K-course = Hi-K-course = K-K-course = K-K-course = Hi-K-K-course = K-K-course = K-K-K-course = K-K-K-course = K-K-course = K-K-course = K-K-course = K-K-course = K-K-course = K-K-course = K-K-course = K-K-course = K-K-course = K-K-K-course = Hi-K-K-course = Hi-K-K-K-course = Hi-K-K-K-course = Hi-K-K-K = Hi-K-K-K-K-course = Hi-K-K = Hi-K-K-K = Hi-K-K-K = Hi-K-K-K = Hi-K-K-K = Hi-K-K-K = Hi-K-K-K-K = Hi-K-K-K = Hi-K-K-K = Hi-K-K-K-K = Hi-K-K-K = Hi-K-K-K-K = Hi-K-K-K-K-K = Hi-K-K-K-K = Hi-K-K-K-K = Hi-K-K-K-K-K = Hi-K-K-K-K = Hi-K-K-K-K-K = Hi-K-K-K-K-K = Hi-K-K-K-K-K-K = Hi-K-K-K-K-K-Hi-K-K-K-K-K-K-K-K-K-K-K"}], "references": [{"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "Eric"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Pattern recognition and machine learning, volume 1", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "The probability of neurotransmitter release: variability and feedback control at single synapses", "author": ["Branco", "Tiago", "Staras", "Kevin"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "Branco et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branco et al\\.", "year": 2009}, {"title": "Untangling invariant object recognition", "author": ["DiCarlo", "James J", "Cox", "David D"], "venue": "Trends in cognitive sciences,", "citeRegEx": "DiCarlo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "DiCarlo et al\\.", "year": 2007}, {"title": "Probabilistic synaptic weighting in a reconfigurable network of VLSI integrate-and-fire neurons", "author": ["D.H. Goldberg", "G. Cauwenberghs", "A.G. Andreou"], "venue": "Neural Networks,", "citeRegEx": "Goldberg et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2001}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia", "M Ranzato", "F. Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Learning with marginalized corrupted features", "author": ["Maaten", "Laurens", "Chen", "Minmin", "Tyree", "Stephen", "Weinberger", "Kilian Q"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Maaten et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2013}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["Merolla", "Paul A", "Arthur", "John V", "Alvarez-Icaza", "Rodrigo", "Cassidy", "Andrew S", "Sawada", "Jun", "Akopyan", "Filipp", "Jackson", "Bryan L", "Imam", "Nabil", "Guo", "Chen", "Nakamura", "Yutaka"], "venue": null, "citeRegEx": "Merolla et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merolla et al\\.", "year": 2014}, {"title": "Event-driven contrastive divergence for spiking neuromorphic systems", "author": ["E. Neftci", "S. Das", "B. Pedroni", "K. Kreutz-Delgado", "G. Cauwenberghs"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "Neftci et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neftci et al\\.", "year": 2014}, {"title": "Changes in reliability of synaptic function as a mechanism for plasticity", "author": ["Stevens", "Charles F", "Wang", "Yanyan"], "venue": "Nature, 371(6499):704\u2013707,", "citeRegEx": "Stevens et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Stevens et al\\.", "year": 1994}, {"title": "Learning stochastic feedforward neural networks", "author": ["Tang", "Yichuan", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "Dropout training as adaptive regularization", "author": ["Wager", "Stefan", "Wang", "Sida", "Liang", "Percy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Maaten et al. (2013) recently demonstrated that the artificial data augmentation via feature corruption effectively plays the role of a data-adaptive regularization.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "Maaten et al. (2013) recently demonstrated that the artificial data augmentation via feature corruption effectively plays the role of a data-adaptive regularization. Wager et al. (2013) also showed that the dropout techniques applied to generalized linear models result into an adaptive regularization.", "startOffset": 0, "endOffset": 186}, {"referenceID": 8, "context": "The Dropout (Hinton et al., 2012) and DropConnect (Wan et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 16, "context": ", 2012) and DropConnect (Wan et al., 2013) techniques are successful examples of using a particular form of stochastic ensemble in the context of feedforward neural networks.", "startOffset": 24, "endOffset": 42}, {"referenceID": 10, "context": "Injection of arbitrary noise improves the robustness of the model to the corruption process (Maaten et al., 2013), but it does not necessarily capture the information about the generative process behind the actual data.", "startOffset": 92, "endOffset": 113}, {"referenceID": 8, "context": "The Dropout (Hinton et al., 2012) and DropConnect (Wan et al., 2013) techniques are successful examples of using a particular form of stochastic ensemble in the context of feedforward neural networks. A unified framework for a collection of perturbed models (which also encompasses the data corruption methods) was recently introduced by Bachman et al. (2014): An arbitrary noise process was used to perturb a parametric parent model to generate an ensemble of child models.", "startOffset": 13, "endOffset": 360}, {"referenceID": 9, "context": "where \u03b8 denotes the model parameters, E(v,h; \u03b8) is a parametric scalar energy function (LeCun et al., 2006), and Z(\u03b8) is the normalizing coefficient (partition function).", "startOffset": 87, "endOffset": 107}, {"referenceID": 16, "context": "This case is similar to DropConnect (Wan et al., 2013) technique but with adaptive distributions over the connections between visible and hidden layers in RBM.", "startOffset": 36, "endOffset": 54}, {"referenceID": 10, "context": "This is analogous to the concept of corrupted features (Maaten et al., 2013; Wager et al., 2013).", "startOffset": 55, "endOffset": 96}, {"referenceID": 15, "context": "This is analogous to the concept of corrupted features (Maaten et al., 2013; Wager et al., 2013).", "startOffset": 55, "endOffset": 96}, {"referenceID": 2, "context": "To test the concept, we implemented Bernoulli and a Gaussian RBSE using Theano library (Bergstra et al., 2010).", "startOffset": 87, "endOffset": 110}, {"referenceID": 10, "context": "The actual learning of a proper model perturbation from the data is the conceptual difference from the previous work (Bachman et al., 2014; Maaten et al., 2013; Wager et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 15, "context": "The actual learning of a proper model perturbation from the data is the conceptual difference from the previous work (Bachman et al., 2014; Maaten et al., 2013; Wager et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 0, "context": "Also, recently proposed Generative Stochastic Networks (GSNs), which are used to encode probabilities in their sampling behavior (Bengio & Thibodeau-Laufer, 2013; Bengio et al., 2013), can be naturally endorsed with non-deterministic connections and might potentially realize richer families of distributions.", "startOffset": 129, "endOffset": 183}, {"referenceID": 11, "context": "Interestingly, biological inspiration also suggests that neuromorphic computers that operate in a massively parallel fashion, while consuming a faction of the power of digital computers (Merolla et al., 2014) can be leveraged to carry out the operations necessary for RBSEs.", "startOffset": 186, "endOffset": 208}, {"referenceID": 6, "context": "They often natively support Bernoulli synaptic stochasticity (Goldberg et al., 2001), as well as neuromorphic variants of RBMs can be efficiently implemented and trained (Neftci et al.", "startOffset": 61, "endOffset": 84}, {"referenceID": 12, "context": ", 2001), as well as neuromorphic variants of RBMs can be efficiently implemented and trained (Neftci et al., 2014).", "startOffset": 93, "endOffset": 114}], "year": 2017, "abstractText": "The goal of a generative model is to capture the distribution underlying the data, typically through latent variables. After training, these variables are often used as a new representation, more effective than the original features in a variety of learning tasks. However, the representations constructed by contemporary generative models are usually point-wise deterministic mappings from the original feature space. Thus, even with representations robust to class-specific transformations, statistically driven models trained on them would not be able to generalize when the labeled data is scarce. Inspired by the stochasticity of the synaptic connections in the brain, we introduce Energy-based Stochastic Ensembles. These ensembles can learn non-deterministic representations, i.e., mappings from the feature space to a family of distributions in the latent space. These mappings are encoded in a distribution over a (possibly infinite) collection of models. By conditionally sampling models from the ensemble, we obtain multiple representations for every input example and effectively augment the data. We propose an algorithm similar to contrastive divergence for training restricted Boltzmann stochastic ensembles. Finally, we demonstrate the concept of the stochastic representations on a synthetic dataset as well as test them in the one-shot learning scenario on MNIST.", "creator": "LaTeX with hyperref package"}}}