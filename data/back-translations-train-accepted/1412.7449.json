{"id": "1412.7449", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2014", "title": "Grammar as a Foreign Language", "abstract": "Syntactic parsing is a fundamental problem in computational linguistics and Natural Language Processing. Traditional approaches to parsing are highly complex and problem specific. Recently, Sutskever et al. (2014) presented a domain-independent method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem. In this work, we show that precisely the same sequence-to-sequence method achieves results that are close to state-of-the-art on syntactic constituency parsing, whilst making almost no assumptions about the structure of the problem.", "histories": [["v1", "Tue, 23 Dec 2014 17:16:24 GMT  (114kb)", "http://arxiv.org/abs/1412.7449v1", null], ["v2", "Sat, 28 Feb 2015 03:16:54 GMT  (115kb)", "http://arxiv.org/abs/1412.7449v2", null], ["v3", "Tue, 9 Jun 2015 22:41:07 GMT  (113kb,D)", "http://arxiv.org/abs/1412.7449v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["oriol vinyals", "lukasz kaiser", "terry koo", "slav petrov", "ilya sutskever", "geoffrey e hinton"], "accepted": true, "id": "1412.7449"}, "pdf": {"name": "1412.7449.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["vinyals@google.com", "lukaszkaiser@google.com", "terrykoo@google.com", "slav@google.com", "ilyasu@google.com", "geoffhinton@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.74 49v1 [cs.CL] 2 3D ec2 014 Revised as conference contribution at ICLR 2015"}, {"heading": "1 INTRODUCTION", "text": "It has recently been shown that a recursive neural network can learn complex sequence-to-sequence mappings directly from raw data, as first demonstrated by the English-French translation task (Sutskever et al., 2014), but the same approach also works for the evaluation of short Python programs (Zaremba & Sutskever, 2014).In this thesis, we use the same type of recursive neural networks called Long Short-Term Memory (Hochreiter & Schmidhuber, 1997, LSTM).The LSTM model directly forms a variable input sequence to a large but fixed vector, which is then mapped to a variable length. It can therefore be used as a general functional learning mechanism: given the inputs x and the corresponding outputs of each serializable type, to learn a function f (x) = y, serialize only."}, {"heading": "2 LSTM PARSING MODEL", "text": "First, let's recall the sequence-to-sequence LSTM model. Hochreiter & Schmidhuber's (1997) long short-term memory model is defined as: Let xt, ht, and mt be the input, control state, and memory state at timestep t. Then, when a sequence of inputs (x1,., xT) is given, the LSTM calculates the h sequence (h1,.., hT) and the m sequence (m1,., mT) as followsit = sigm (W1xt + W2ht \u2212 1) i \u2032 t = tanh (W3xt + W4ht \u2212 1) ft = sigm (W5xt + W6ht \u2212 1) ot = sigm (W7xt \u2212 1) mt = mt \u2212 1)."}, {"heading": "2.1 ADAPTATIONS FOR PARSING", "text": "< M > W > W > W > W > W > W > W > W > W > W > W > W > W \"n\" W \"n\" n \"W\" n \"n\" W \"n\" n \"W\" n. W \"n\" n \"W\" n \"n\" W \"n\" n \"n. W\" n \"n\" n \"n\" W \"n\" n \"n\" w \"n\" n. W \"n\" n \"n\" n \"w\" n \"n\" n \"w\" n \"n\" n \"n\" n \"n. W\" n \"n\" n \"n. W\" n \"n\" n \"n. W\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"w\" n \"n\" n \"n\" n \"w\" n \"n\" n. W \"n\" n \"n\" n. W \"n\" n \"n\" n. W \"n\" n \"n\" n. W \"n\" n \"n. W\" n \"n\" n \"n\" n. W \"n\" n \"n\" n. W \"n\" n \"n\" n \"n\" n. W \"n\" n \"n\" n \"n\" n. W \"n\" n \"n\" n \"n\" n \"n\" n. W \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n n \"n\" n n n \"n\" n \"n. W\" n \"n\" n \"n\" n \"n\" n \"n. W\" n \"n\" n \"n\" n \"n\" n \"n\" n n \"n\" n \"n n\" n \"n\" n. W \"n\" n \"n\" n n \"n\" n \"n n\" n \"n\" n n \"n\" n \"n."}, {"heading": "3 EXPERIMENTS", "text": "We conducted a series of experiments with both the basic encoding and the stack encoding model as described above. We experimented with bound and unbound input and output LSTMs and measured the impact of using pre-trained word embedding and fine tuning on in-domain data."}, {"heading": "3.1 TRAINING DATA", "text": "In our experiments, we focus on English, but the model could easily be applied to other languages. Our goal is to build a robust and domain-independent parser that can be used to process texts from different genres. To this end, we train and evaluate by merging a number of publicly available tree banks. We use OntoNotes Corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov & McDonald, 2012), and the updated and corrected Question Treebank (Judge et al., 2006).1 Note that the popular section of Penn Treebank version 5 (Marcus et al., 1993) is used as part of OntoNotes Corpus. Overall, we train on 90K sets and rate the sets of the Treebank Union."}, {"heading": "3.2 MODELS AND PARAMETERS", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3.3 EFFECT OF SENTENCE LENGTH AND BEAM SIZE", "text": "The results shown in Figure 3 clearly show that the performance of the LSTM at long sets does not deteriorate compared to the performance of the BerkeleyParser. Regarding the effects of the beam size in the decoder, the performance of the LSTM decreases significantly compared to the performance of the BerkeleyParser. Regarding the effects of the beam size in the decoder, no beam search at all (i.e. beam size of 1) results in a significant reduction in the score. The following numbers show the F1 values on WSJ 22 for different beam sizes for the stack encoding 3x640 bound and finely tuned models.Beam size 1 2 3 6 9 20 F1 score 90.7 91.0 91.1 91.2 91.3"}, {"heading": "3.4 EFFECT OF PRE-TRAINING AND FINE-TUNING", "text": "In addition to the above results, where we only train on the common set of 7M + 90K phrases and with a vocabulary of size 50K, we experimented with two additional variations.Instead of feeding the network with tokens from a 50K vocabulary and learning an embedding for it, we tried to provide the network with already embedded words, which skip-gramUnder Review as a conference contribution at ICLR 2015, size 512 embedding was pre-trained with word2vec (Mikolov et al., 2013) on a 10B word body and fixed during the training of the other parameters of the network.In addition to pre-training, we also experimented with fine-tuning the model only on the 40K set of WSJ training sets. This gives the network the chance to correct certain errors, but it must not be stopped early to prevent overadjustment.We have the influence of these factors on a stment of the model stacked with size 940 STM-size 9J pre-school layers."}, {"heading": "3.5 FINAL RESULTS", "text": "It is difficult to directly compare our results with those of previous work because our training setup is different. To compare them with a publicly available state-of-the-art parser, we trained the BerkeleyParser (Petrov et al., 2006) on our experimental setup. Additionally, we compare with variants of the BerkeleyParser that use self-training on unlabeled data (Huang & Harper, 2009), or build a combination of multiple parsers (Petrov, 2010), or combine both. It is interesting to see that additional tree base data does not contribute much to Section 23 of the WSJ, but it does help a lot when parsing unlabeled data (Huang & Harper, 2009), or we include the best linear-time parsers in the literature (the Transition-based parser by Zhu et al. (2013). It is encouraging to see that our J-LM models are better when we optimize these sets at high speed."}, {"heading": "4 ANALYSIS", "text": "In fact, it is the case that most of us are able to go in search of a solution that puts them in the position they are in. In fact, it is the case that they are able to find a solution in which they can move. In fact, it is the case that they are able to find a solution in which they can move. In fact, it is the case that they are able to find a solution in which they can move. In fact, it is the case that they are able to find a solution in which they can move."}, {"heading": "5 RELATED WORK", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "6 CONCLUSIONS", "text": "In this paper, we demonstrated that the generic sequence-to-sequence approach of Sutskever et al. (2014) can achieve competitive results in syntactic parsing of components with relatively little effort or tuning. Our results underscore the importance of large datasets in using large deep neural networks that do not contain domain-specific, handmade knowledge. In the absence of prior knowledge, our system has not been able to learn a precise parser from the tree base alone (cf. first line of Table 1). Fortunately, there is a very simple way to benefit from hand engineering that flows into more conventional parsers: We use these parsers to generate additional training data, which allows us to benefit from the knowledge of other parsing systems without imposing any limitations on the form of the internal representations used by LSTM. The fact that the LSTM was able to meet the Berkeley parsers, the Net7M would be very sterile, even if the Net7M were to comment on the earlier sets, although it would be very expensive to do so."}, {"heading": "7 DETAILED EXPERIMENTAL SETUP", "text": "In this section we present details of our experimental setup using OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov & McDonald, 2012) and the updated and corrected questions Treebank (Judge et al., 2006), all of which are available through the Linguistic Data Consortium (LDC)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["Collins", "Michael"], "venue": "In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Collins and Michael.,? \\Q1997\\E", "shortCiteRegEx": "Collins and Michael.", "year": 1997}, {"title": "Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL\u201904)", "author": ["Collins", "Michael", "Roark", "Brian"], "venue": "Main Volume,", "citeRegEx": "Collins et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2004}, {"title": "Deep learning for efficient discriminative parsing", "author": ["Collobert", "Ronan"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Collobert and Ronan.,? \\Q2011\\E", "shortCiteRegEx": "Collobert and Ronan.", "year": 2011}, {"title": "A neural network for learning how to parse tree adjoining grammar", "author": ["Ghahramani", "Zoubin"], "venue": "B.S.Eng Thesis, University of Pennsylvania,", "citeRegEx": "Ghahramani and Zoubin.,? \\Q1990\\E", "shortCiteRegEx": "Ghahramani and Zoubin.", "year": 1990}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Sparser, better, faster gpu parsing", "author": ["Hall", "David", "Berg-Kirkpatrick", "Taylor", "Canny", "John", "Klein", "Dan"], "venue": "In ACL,", "citeRegEx": "Hall et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["Henderson", "James"], "venue": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Henderson and James.,? \\Q2003\\E", "shortCiteRegEx": "Henderson and James.", "year": 2003}, {"title": "Discriminative training of a neural network statistical parser. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL\u201904)", "author": ["Henderson", "James"], "venue": "Main Volume,", "citeRegEx": "Henderson and James.,? \\Q2004\\E", "shortCiteRegEx": "Henderson and James.", "year": 2004}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["Henderson", "James", "Titov", "Ivan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Henderson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ontonotes: The 90% solution", "author": ["Hovy", "Eduard", "Marcus", "Mitchell", "Palmer", "Martha", "Ramshaw", "Lance", "Weischedel", "Ralph"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL, Short Papers,", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Self-training PCFG grammars with latent annotations across languages", "author": ["Huang", "Zhongqiang", "Harper", "Mary"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Self-training with products of latent variable grammars", "author": ["Huang", "Zhongqiang", "Harper", "Mary", "Petrov", "Slav"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Questionbank: Creating a corpus of parse-annotated questions", "author": ["Judge", "John", "Cahill", "Aoife", "van Genabith", "Josef"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Judge et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Judge et al\\.", "year": 2006}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In EMNLP, pp. 1700\u20131709,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Dan", "Manning", "Christopher D"], "venue": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong", "Thang", "Sutskever", "Ilya", "Le", "Quoc V", "Vinyals", "Oriol", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Santorini", "Beatrice", "Marcinkiewicz", "Mary Ann"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Nivre", "Joakim"], "venue": "Comput. Linguist.,", "citeRegEx": "Nivre and Joakim.,? \\Q2008\\E", "shortCiteRegEx": "Nivre and Joakim.", "year": 2008}, {"title": "Products of random latent variable grammars", "author": ["Petrov", "Slav"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Petrov and Slav.,? \\Q2010\\E", "shortCiteRegEx": "Petrov and Slav.", "year": 2010}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "Slav", "McDonald", "Ryan"], "venue": "Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Petrov", "Slav", "Barrett", "Leon", "Thibaux", "Romain", "Klein", "Dan"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Petrov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "A linear observed time statistical parser based on maximum entropy models", "author": ["Ratnaparkhi", "Adwait"], "venue": "In Second Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ratnaparkhi and Adwait.,? \\Q1997\\E", "shortCiteRegEx": "Ratnaparkhi and Adwait.", "year": 1997}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Manning", "Chris", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Constituent parsing with incremental sigmoid belief networks", "author": ["Titov", "Ivan", "Henderson", "James"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Zhu", "Muhua", "Zhang", "Yue", "Chen", "Wenliang", "Min", "Jingbo"], "venue": "In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers),", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 27, "context": "Recently, Sutskever et al. (2014) presented a domainindependent method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem.", "startOffset": 10, "endOffset": 34}, {"referenceID": 27, "context": "This was first demonstrated on the English-to-French translation task (Sutskever et al., 2014), but the same approach also works for evaluating short python programs (Zaremba & Sutskever, 2014).", "startOffset": 70, "endOffset": 94}, {"referenceID": 18, "context": "It does, however, achieve high performance on a large scale machine translation task (Luong et al., 2014).", "startOffset": 85, "endOffset": 105}, {"referenceID": 24, "context": "Our main results are as follows: We train a deep LSTM model with 34M parameters on a dataset consisting of 90K sentences obtained from various treebanks and 7M sentences from the web that are automatically parsed with the BerkeleyParser (Petrov et al., 2006).", "startOffset": 237, "endOffset": 258}, {"referenceID": 27, "context": "A common criticism of the sequence-to-sequence approach of Sutskever et al. (2014) is that it is fundamentally incapable of dealing with long inputs and outputs, due to its need to store the entire input sequence in its short-term memory.", "startOffset": 59, "endOffset": 83}, {"referenceID": 27, "context": "Figure 1: A schematic outline of the sequence-to-sequence model of Sutskever et al. (2014). A deep input LSTM reads the input sequence A1, A2, A3 one token at a time and encodes it as its final hidden state vector.", "startOffset": 67, "endOffset": 91}, {"referenceID": 31, "context": "Inspired by shift-reduce transition-based parsers (Nivre, 2008; Zhu et al., 2013), we introduce a stack where the words are provided to the LSTM as additional inputs during decoding.", "startOffset": 50, "endOffset": 81}, {"referenceID": 12, "context": "We use the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov & McDonald, 2012) and the updated and corrected Question Treebank (Judge et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 15, "context": ", 2006), the English Web Treebank (Petrov & McDonald, 2012) and the updated and corrected Question Treebank (Judge et al., 2006).", "startOffset": 108, "endOffset": 128}, {"referenceID": 19, "context": "1 Note that the popular Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) as part of the OntoNotes corpus.", "startOffset": 73, "endOffset": 94}, {"referenceID": 24, "context": "To compare to an established baseline parser, we also train and test the publicly available BerkeleyParser (Petrov et al., 2006) on the Treebank union.", "startOffset": 107, "endOffset": 128}, {"referenceID": 27, "context": "We also found it useful to reverse the input sentences but not their parse trees, similarly to Sutskever et al. (2014), and we did it in all of our experiments.", "startOffset": 95, "endOffset": 119}, {"referenceID": 21, "context": "5 Petrov et al. (2006) WSJ only 90.", "startOffset": 2, "endOffset": 23}, {"referenceID": 21, "context": "5 Petrov et al. (2006) WSJ only 90.4 Ensemble of 10 basic encoding LSTMs 91.6 Petrov (2010) WSJ only ensemble 91.", "startOffset": 2, "endOffset": 92}, {"referenceID": 21, "context": "5 Petrov et al. (2006) WSJ only 90.4 Ensemble of 10 basic encoding LSTMs 91.6 Petrov (2010) WSJ only ensemble 91.8 Petrov et al. (2006) Treebank union only 90.", "startOffset": 2, "endOffset": 136}, {"referenceID": 21, "context": "5 Petrov et al. (2006) WSJ only 90.4 Ensemble of 10 basic encoding LSTMs 91.6 Petrov (2010) WSJ only ensemble 91.8 Petrov et al. (2006) Treebank union only 90.4 Huang & Harper (2009) semi-supervised 91.", "startOffset": 2, "endOffset": 183}, {"referenceID": 13, "context": "3 Huang et al. (2010) semi-supervised ensemble 92.", "startOffset": 2, "endOffset": 22}, {"referenceID": 13, "context": "3 Huang et al. (2010) semi-supervised ensemble 92.4 Zhu et al. (2013) WSJ only 90.", "startOffset": 2, "endOffset": 70}, {"referenceID": 13, "context": "3 Huang et al. (2010) semi-supervised ensemble 92.4 Zhu et al. (2013) WSJ only 90.4 Zhu et al. (2013) semi-supervised 91.", "startOffset": 2, "endOffset": 102}, {"referenceID": 20, "context": "embeddings of size 512 were pre-trained using word2vec (Mikolov et al., 2013) on a 10B-word corpus, and kept fix while training the other parameters of the network.", "startOffset": 55, "endOffset": 77}, {"referenceID": 24, "context": "To compare to a publicly available state-of-the-art parser, we trained the BerkeleyParser (Petrov et al., 2006) on our experimental setup.", "startOffset": 90, "endOffset": 111}, {"referenceID": 7, "context": "This is better than the speed reported in Figure 4 of (Hall et al., 2014) even though we run on sentences of all lengths (not only under 40), our model achieves better accuracy, and their code is highly optimized.", "startOffset": 54, "endOffset": 73}, {"referenceID": 22, "context": "To compare to a publicly available state-of-the-art parser, we trained the BerkeleyParser (Petrov et al., 2006) on our experimental setup. Table 2 shows performance on section 23 from the Penn Treebank when training on our setup on the left, and results from other papers on the right. Additionally, we compare to variants of the BerkeleyParser that use self-training on unlabeled data (Huang & Harper, 2009), or built an ensemble of multiple parsers (Petrov, 2010), or combine both techniques. It is interesting to see that additional treebank data does not help much on Section 23 of WSJ, but it helps a lot for parsing out-of-domain text. Finally, we include the best linear-time parser in the literature, the transition-based parser of Zhu et al. (2013). It is encouraging to see that our LSTM models are competitive with these highly optimized parsers that have received a lot of task specific tuning.", "startOffset": 91, "endOffset": 758}, {"referenceID": 20, "context": "The table contains the closest three neighbors for the embeddings learnt by the LSTM and contrasts them to the embeddings learnt by a skip-gram model by Mikolov et al. (2013). It is interesting to observe that the neighbors differ quite significantly between the two methods.", "startOffset": 153, "endOffset": 175}, {"referenceID": 23, "context": "Petrov et al. (2006) partially alleviate the heavy reliance on manual modeling of linguistic structure by using latent variables to learn a more articulated model.", "startOffset": 0, "endOffset": 21}, {"referenceID": 26, "context": "Socher et al. (2011) used a tree-structured neural network to score", "startOffset": 0, "endOffset": 21}, {"referenceID": 31, "context": "Relatedly, Zhu et al. (2013) present excellent parsing results with a single left-toright pass, but require a stack to explicitly delay making decisions and a parsing-specific transition strategy in order to achieve good parsing accuracies.", "startOffset": 11, "endOffset": 29}, {"referenceID": 18, "context": "(2014) because it is the simplest architecture that can solve general sequence-tosequence problems and because it achieves the best performance on a large scale machine translation task (Luong et al., 2014).", "startOffset": 186, "endOffset": 206}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al.", "startOffset": 7, "endOffset": 30}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition.", "startOffset": 7, "endOffset": 183}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition. Kalchbrenner & Blunsom (2013) used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used an RNN to produce the output sentence.", "startOffset": 7, "endOffset": 259}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition. Kalchbrenner & Blunsom (2013) used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used an RNN to produce the output sentence. Essentially the same model has been used by Vinyals et al. (2014) to successfully learn to generate image captions.", "startOffset": 7, "endOffset": 486}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition. Kalchbrenner & Blunsom (2013) used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used an RNN to produce the output sentence. Essentially the same model has been used by Vinyals et al. (2014) to successfully learn to generate image captions. Even though most of these models could be applied to parsing, we chose the model of Sutskever et al. (2014) because it is the simplest architecture that can solve general sequence-tosequence problems and because it achieves the best performance on a large scale machine translation task (Luong et al.", "startOffset": 7, "endOffset": 644}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition. Kalchbrenner & Blunsom (2013) used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used an RNN to produce the output sentence. Essentially the same model has been used by Vinyals et al. (2014) to successfully learn to generate image captions. Even though most of these models could be applied to parsing, we chose the model of Sutskever et al. (2014) because it is the simplest architecture that can solve general sequence-tosequence problems and because it achieves the best performance on a large scale machine translation task (Luong et al., 2014). It is also able to embed entire sentences in a continuous vector space. Finally, Ghahramani (1990) applied a similar recurrent neural network to the problem of syntactic parsing 20 years ago.", "startOffset": 7, "endOffset": 944}], "year": 2014, "abstractText": "Syntactic parsing is a fundamental problem in computational linguistics and natural language processing. Traditional approaches to parsing are highly complex and problem specific. Recently, Sutskever et al. (2014) presented a domainindependent method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem. In this work, we show that precisely the same sequence-to-sequence method achieves results that are close to state-of-the-art on syntactic constituency parsing, whilst making almost no assumptions about the structure of the problem.", "creator": "LaTeX with hyperref package"}}}