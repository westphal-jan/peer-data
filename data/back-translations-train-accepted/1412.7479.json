{"id": "1412.7479", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2014", "title": "Deep Networks With Large Output Spaces", "abstract": "Deep neural networks have been extremely successful at various image, speech, video recognition tasks because of their ability to model deep structures within the data. However, they are still prohibitively expensive to train and apply for problems containing millions of classes in the output layer. Based on the observation that the key computation common to most neural network layers is a vector/matrix product, we propose a fast locality-sensitive hashing technique to approximate the actual dot product enabling us to scale up the training and inference to millions of output classes. We evaluate our technique on three diverse large-scale recognition tasks and show that our approach can train large-scale models at a faster rate (in terms of steps/total time) compared to baseline methods.", "histories": [["v1", "Tue, 23 Dec 2014 19:22:59 GMT  (1202kb)", "https://arxiv.org/abs/1412.7479v1", null], ["v2", "Mon, 29 Dec 2014 18:45:36 GMT  (594kb)", "http://arxiv.org/abs/1412.7479v2", null], ["v3", "Sat, 28 Feb 2015 01:12:58 GMT  (709kb)", "http://arxiv.org/abs/1412.7479v3", null], ["v4", "Fri, 10 Apr 2015 19:53:21 GMT  (711kb)", "http://arxiv.org/abs/1412.7479v4", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sudheendra vijayanarasimhan", "jonathon shlens", "rajat monga", "jay yagnik"], "accepted": true, "id": "1412.7479"}, "pdf": {"name": "1412.7479.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["svnaras@google.com", "shlens@google.com", "rajatmonga@google.com", "jyagnik@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.74 79v4 [cs.NE] 1 0A pr2 015 Accepted as a workshop contribution at ICLR 2015"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them will be able to move to another world, in which they will be able to escape rather than to another world."}, {"heading": "2 RELATED WORK", "text": "Traditional methods such as logistic regression and softmax (multinomial regression) are known to have poor scaling properties with the number of classes (Dean et al., 2013), as the number of Dot products increases with the number of classes C to be considered. An advantage of this approach is that each step only requires calculations associated with crossing the tree to a single leaf. A second direction is to train a dense spatial representation and perform classifications by embedding k-next neighbors in this space instead."}, {"heading": "3 APPROACH", "text": "The aim of this work is to allow an approximate calculation of the matrix product from the parameters of a layer and its input activations, xTW, in a deep network, so that the number of output dimensions can be increased by several orders of magnitude. In the following sections, we show that a locally sensitive hash-based approach can provide such a solution without too much impairment of overall accuracy. In a first step, we apply this technique to enlarge the final classification layer, since the advantages of the bunny are easily discernible when the cardinality is quite large."}, {"heading": "3.1 SOFTMAX/LOGISTIC CLASSIFICATION", "text": "Softmax and logistic regression functions are two popular choices for the last layer of a deep network for multi-class and binary classification problems. Formally, the two functions are defined as Psoftmax (y = j | x) = exTwj \u2211 N k = 1 e xTwk (1) Plogistic (y = j | x) = 11 + e \u2212 (x Twj + \u03b2j) (2) (3), where P (y = j | x) is the probability of the jth class relative to the input vector x and {wj, j = 1... N} are different linear functions for the N classes. If the number of classes is large, not all classes are relevant to a particular input example. Therefore, in many situations we are only interested in the K classes with the highest probabilities. We could obtain the uppermost K classes by determining the K vectors, WK, which are the largest Dot products with the input vector, and calculating the probabilities for these cosmos with the highest probabilities."}, {"heading": "3.2 WINNER-TAKE-ALL HASHING (WTA)", "text": "Considering a vector x or w in Rd, its WTA hash is defined by permutation of its elements, which use different schemes and record the index of the maximum value of the first k elements (Yagnik et al., 2011). Each index can be represented compactly with log2k bits, which lead to P-Logic bits for the whole hash. The WTA hash has several desirable properties; since the only operation involved in the calculation of the hash is a comparison, it can be fully implemented using integer arithmetic, while the algorithm can be implemented efficiently without predicting the industry penalties.Furthermore, each WTA hash function defines an ordinal embedding and it has been shown in (Yagnik et al, 2011) that as P \u2192 d the dot product between two WTA hash bits to the ranking correlation between the underlying vectors is suitable as a hash basis for a hash similarity."}, {"heading": "3.3 INFERENCE", "text": "We can apply our proposed approach both during the model inference and during the training. Concluding, in the case of a learned model, we first calculate the hash codes of the Softmax / logistic regression layer parameter vectors and store the IDs of the corresponding classes in the hash table as described in Section 3.2. This is a one-time operation that is performed before executing inferences on any input examples. In an input example, we run them through all the layers leading to the classification layer as before, and compute the hash codes of the input activations on the classification layer. We then query the hash table to retrieve the uppermost K classes and calculate probabilities using Equation 3 only for these classes. Figure 1 shows a rough scheme of this procedure."}, {"heading": "3.4 TRAINING", "text": "We train the models using Rainfall SGD, an asynchronous stocastic gradient descent method that supports a large number of model replications proposed in (Dean et al., 2012). During reverse propagation, we propagate only gradients based on the topmost K classes retrieved during the forward pass of the model, and update the parameter vectors of only these retrieved classes using the error vector. In addition, we add all positive labels for an input example to the list of non-zero classes to always provide a positive gradient. In Section 4, we show empirical results of performing only these topmost K updates. These sparse gradients are much more computationally efficient and additionally fulfill the function of hard negative mining, as only the narrowest classes are updated to a specific example. While inference is easy to implement using WTA hashing methods, there are several challenges that need to be solved to make training more efficient."}, {"heading": "4 EXPERIMENTS", "text": "We evaluate our proposed method empirically on several large data sets with the aim of investigating the trade-off between the accuracy and time complexity of the WTA-based Softmax classifier compared to the basic approaches of Softmax (exhaustive) and Hierarchical Softmax."}, {"heading": "4.1 IMAGENET 21K", "text": "In fact, most of them will be able to play by the rules that they have established in recent years, and they will be able to play by the rules."}, {"heading": "4.2 SKIPGRAM DATASET", "text": "A popular application of deep networks is the construction and formation of models of language and hierarchy."}, {"heading": "4.3 VIDEO IDENTIFICATION", "text": "While the 21K problem is one of the largest tested for the Baseline softmax model, the benefits of hashings are best seen for problems of much greater cardinality. To illustrate this, we next consider a large-scale classification task for video identification. The task we propose is to predict the ID of a video based on its frames. We use the Sport 1M action datasets introduced in Karpathy et al., 2014. The Sport 1M datasets consist of roughly 1.2 million Youtube videos based on frames. We use the Sport 1M action datasets introduced in the US."}, {"heading": "5 CONCLUSIONS", "text": "Empirical evaluations of the proposed model on various large datasets show that the proposed approach allows significant acceleration compared to base models and can train such large models faster than alternatives such as hierarchical Softmax. Our approach is always advantageous when the number of classes considered is large or when batching is not possible. In the future, we would also like to extend this technique to intermediate layers, as the proposed method explicitly imposes austerity constraints that are desirable in hierarchical learning. Given the scaling properties of hashing, for example, our approach could be used to increase the number of filters used in the evolutionary layers from hundreds to tens of thousands, with several hundred active at any one time."}], "references": [{"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg S", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Le", "Quoc V", "Mao", "Mark Z", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Ng", "Andrew Y"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["Dean", "Thomas", "Ruzon", "Mark A", "Segal", "Mark", "Shlens", "Jonathon", "Vijayanarasimhan", "Sudheendra", "Yagnik", "Jay"], "venue": "In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Dean et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2013}, {"title": "Similarity search in high dimensions via hashing", "author": ["Gionis", "Aristides", "Indyk", "Piotr", "Motwani", "Rajeev"], "venue": "Proceedings of the 25th International Conference on Very Large Data Bases,", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Karpathy", "Andrej", "Toderici", "George", "Shetty", "Sanketh", "Leung", "Thomas", "Sukthankar", "Rahul", "Fei-Fei", "Li"], "venue": "In Proc. CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Proc. NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Le", "Quoc V", "Monga", "Rajat", "Devin", "Matthieu", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff", "Ng", "Andrew Y"], "venue": "In In International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In AISTATS\u201905,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Real-time large scale nearduplicate web video retrieval", "author": ["Shang", "Lifeng", "Yang", "Linjun", "Wang", "Fei", "Chan", "Kwok-Ping", "Hua", "Xian-Sheng"], "venue": "In Proceedings of the International Conference on Multimedia, MM", "citeRegEx": "Shang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2010}, {"title": "Multiple feature hashing for realtime large scale near-duplicate video retrieval", "author": ["Song", "Jingkuan", "Yang", "Yi", "Huang", "Zi", "Shen", "Heng Tao", "Hong", "Richang"], "venue": "In Proceedings of the 19th ACM International Conference on Multimedia, MM", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston", "Jason", "Bengio", "Samy", "Usunier", "Nicolas"], "venue": "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston", "Jason", "Bengio", "Samy", "Usunier", "Nicolas"], "venue": "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "The power of comparative reasoning", "author": ["Yagnik", "Jay", "Strelow", "Dennis", "Ross", "David A", "Lin", "Ruei-sung"], "venue": "In IEEE International Conference on Computer Vision. IEEE,", "citeRegEx": "Yagnik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yagnik et al\\.", "year": 2011}, {"title": "Near-duplicate keyframe identification with interest point matching and pattern learning", "author": ["Zhao", "Wan-Lei", "Ngo", "Chong-Wah", "Tan", "Hung-Khoon", "Wu", "Xiao"], "venue": "Trans. Multi.,", "citeRegEx": "Zhao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "Deep neural networks have proven highly successful at various image, speech, language and video recognition tasks (Krizhevsky et al., 2012; Mikolov et al., 2013; Karpathy et al., 2014).", "startOffset": 114, "endOffset": 184}, {"referenceID": 6, "context": "Deep neural networks have proven highly successful at various image, speech, language and video recognition tasks (Krizhevsky et al., 2012; Mikolov et al., 2013; Karpathy et al., 2014).", "startOffset": 114, "endOffset": 184}, {"referenceID": 3, "context": "Deep neural networks have proven highly successful at various image, speech, language and video recognition tasks (Krizhevsky et al., 2012; Mikolov et al., 2013; Karpathy et al., 2014).", "startOffset": 114, "endOffset": 184}, {"referenceID": 9, "context": "Duplicate video content identification (Shang et al., 2010; Song et al., 2011; Zhao et al., 2007) and video recommendations are also large-scale problems with millions of classes.", "startOffset": 39, "endOffset": 97}, {"referenceID": 10, "context": "Duplicate video content identification (Shang et al., 2010; Song et al., 2011; Zhao et al., 2007) and video recommendations are also large-scale problems with millions of classes.", "startOffset": 39, "endOffset": 97}, {"referenceID": 14, "context": "Duplicate video content identification (Shang et al., 2010; Song et al., 2011; Zhao et al., 2007) and video recommendations are also large-scale problems with millions of classes.", "startOffset": 39, "endOffset": 97}, {"referenceID": 3, "context": ", 2013; Karpathy et al., 2014). These networks typically have several layers of units connected in feedforward fashion between the input and output spaces. Each layer performs a specific function such as convolution, pooling, normalization, or plain matrix products in the case of fully connected layers followed by some form of non-linear activation such as sigmoid or rectified linear units. Despite their attractive qualities, and the relative efficiency of their local architecture, these networks are still prohibitively expensive to train and apply for large-scale problems containing millions of classes or nodes. There are several such problems proposed in the literature. The Imagenet dataset which is one of the largest datasets for image classification contains around 21000 classes. Wordnet, which is a superset of Imagenet, consists of 117, 00 synsets. Freebase, which is a community-curated database of well-known people, places, and things contains close to 20 million entities. Image models of text queries have ranged from 100000 queries in academic benchmarks Weston et al. (2011b) to several million in commercial search engines such as Google, Bing, Yahoo, etc.", "startOffset": 8, "endOffset": 1100}, {"referenceID": 13, "context": "Based on this observation, we exploit a fast locality-sensitive hashing technique (Yagnik et al., 2011) in order to approximate the actual dot product in the final output layer which enables us to scale up the training and inference to millions of output classes.", "startOffset": 82, "endOffset": 103}, {"referenceID": 6, "context": "Furthermore, using the same technique when training the models, we show that our approach can train large-scale models at a faster rate both in terms of number of steps and the total time compared to both the standard softmax layers and the more computationally efficient hierarchical softmax layer of (Mikolov et al., 2013).", "startOffset": 302, "endOffset": 324}, {"referenceID": 1, "context": "Traditional methods such as logistic regression and softmax (multinomial regression) are known to have poor scaling properties with the number of classes (Dean et al., 2013) as the number of dot products grows with the number of classes C that must be considered.", "startOffset": 154, "endOffset": 173}, {"referenceID": 6, "context": "One method for contending with this is hierarchical softmax whereby a tree is constructed of depth log2C in which the leaves are the individual classes which must be classified (Morin & Bengio, 2005; Mikolov et al., 2013).", "startOffset": 177, "endOffset": 221}, {"referenceID": 2, "context": "Locality sensitive hashing (LSH) (Gionis et al., 1999) provides a third alternative by providing methods to perform approximate nearest neighbor search in sub-linear time for various similarity metrics.", "startOffset": 33, "endOffset": 54}, {"referenceID": 13, "context": "An LSH scheme based on ordinal similarity is proposed in (Yagnik et al., 2011) which is used in (Dean et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 1, "context": ", 2011) which is used in (Dean et al., 2013) to speed-up filter based object detection.", "startOffset": 25, "endOffset": 44}, {"referenceID": 2, "context": "We note that this is equivalent to the problem of finding the approximate nearest neighbors of a vector based on cosine (dot product) similarity which has a rich literature beginning with the seminal work of (Gionis et al., 1999).", "startOffset": 208, "endOffset": 229}, {"referenceID": 13, "context": "In this work, we employ the subfamily of hash functions, winner-take-all (WTA) hashing introduced in (Yagnik et al., 2011), since it has been successfully applied for the similar task of scaling up filter-based object detection in (Dean et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 1, "context": ", 2011), since it has been successfully applied for the similar task of scaling up filter-based object detection in (Dean et al., 2013).", "startOffset": 116, "endOffset": 135}, {"referenceID": 13, "context": "Given a vector x or w in R, its WTA hash is defined by permuting its elements using P distinct permutations and recording the index of the maximum value of the first k elements (Yagnik et al., 2011).", "startOffset": 177, "endOffset": 198}, {"referenceID": 13, "context": "Furthermore, each WTA hash function defines an ordinal embedding and it has been shown in (Yagnik et al., 2011) that as P \u2192 d!, the dot product between two WTA hashes tends to the rank correlation between the underlying vectors.", "startOffset": 90, "endOffset": 111}, {"referenceID": 1, "context": "In this work, we employ the scheme used in (Dean et al., 2013) due to its simplicity and limited overhead.", "startOffset": 43, "endOffset": 62}, {"referenceID": 0, "context": "We train the models using downpour SGD, an asynchronous stocastic gradient descent procedure supporting a large number of model replicas, proposed in (Dean et al., 2012).", "startOffset": 150, "endOffset": 169}, {"referenceID": 5, "context": "We split the set into equal partitions of training and testing sets as done in (Le et al., 2012).", "startOffset": 79, "endOffset": 96}, {"referenceID": 1, "context": "We selected values of 16, 1000, 3000 for the k, M , P parameters of the WTA approach for all experiments based on results on a small set of images which agreed with the parameters mentioned in (Dean et al., 2013).", "startOffset": 193, "endOffset": 212}, {"referenceID": 4, "context": "We used the artichecture proposed in (Krizhevsky et al., 2012) (AlexNet) for all experiments replacing only the classification layer with the proposed approach.", "startOffset": 37, "endOffset": 62}, {"referenceID": 6, "context": "Recent work from (Mikolov et al., 2013; Q.V. Le, 2014) has demonstrated that a shallow, simple architecture can be trained efficiently by across language corpora.", "startOffset": 17, "endOffset": 54}, {"referenceID": 6, "context": "However, the WTA softmax produces underlying embedding vector representations that do not perform as well on analogy tasks as highlighted by (Mikolov et al., 2013).", "startOffset": 141, "endOffset": 163}, {"referenceID": 9, "context": "This task is modeled on Youtube\u2019s content ID classification problem which has also been addressed in several recent work under various settings Shang et al. (2010); Song et al.", "startOffset": 144, "endOffset": 164}, {"referenceID": 9, "context": "This task is modeled on Youtube\u2019s content ID classification problem which has also been addressed in several recent work under various settings Shang et al. (2010); Song et al. (2011); Zhao et al.", "startOffset": 144, "endOffset": 184}, {"referenceID": 9, "context": "This task is modeled on Youtube\u2019s content ID classification problem which has also been addressed in several recent work under various settings Shang et al. (2010); Song et al. (2011); Zhao et al. (2007).", "startOffset": 144, "endOffset": 204}, {"referenceID": 3, "context": "We use the Sports 1M action recognition dataset introduced in (Karpathy et al., 2014) for this problem.", "startOffset": 62, "endOffset": 85}], "year": 2015, "abstractText": "Deep neural networks have been extremely successful at various image, speech, video recognition tasks because of their ability to model deep structures within the data. However, they are still prohibitively expensive to train and apply for problems containing millions of classes in the output layer. Based on the observation that the key computation common to most neural network layers is a vector/matrix product, we propose a fast locality-sensitive hashing technique to approximate the actual dot product enabling us to scale up the training and inference to millions of output classes. We evaluate our technique on three diverse large-scale recognition tasks and show that our approach can train large-scale models at a faster rate (in terms of steps/total time) compared to baseline methods.", "creator": "LaTeX with hyperref package"}}}