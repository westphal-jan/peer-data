{"id": "1412.7580", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation", "abstract": "We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.", "histories": [["v1", "Wed, 24 Dec 2014 01:31:36 GMT  (1101kb)", "http://arxiv.org/abs/1412.7580v1", "Under review as a conference paper at ICLR2015"], ["v2", "Tue, 30 Dec 2014 16:55:04 GMT  (1100kb)", "http://arxiv.org/abs/1412.7580v2", "Under review as a conference paper at ICLR2015. Fixed performance anomaly for fbfft size 8 and updated graphs"], ["v3", "Fri, 10 Apr 2015 20:01:00 GMT  (1101kb)", "http://arxiv.org/abs/1412.7580v3", "Camera ready for ICLR2015"]], "COMMENTS": "Under review as a conference paper at ICLR2015", "reviews": [], "SUBJECTS": "cs.LG cs.DC cs.NE", "authors": ["nicolas vasilache", "jeff johnson", "michael mathieu", "soumith chintala", "serkan piantino", "yann lecun"], "accepted": true, "id": "1412.7580"}, "pdf": {"name": "1412.7580.pdf", "metadata": {"source": "CRF", "title": "FAST CONVOLUTIONAL NETS WITH fbfft : A GPU PERFORMANCE EVALUATION", "authors": ["Nicolas Vasilache", "Jeff Johnson", "Michael Mathieu"], "emails": ["ntv@fb.com", "jhj@fb.com", "myrhev@fb.com", "soumith@fb.com", "spiantino@fb.com", "yann@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.75 80v1 [cs.LG] 2 4D ec2 01 We examine the performance profile of Convolutional Neural Network (CNN) training courses on the current generation of NVIDIA Graphics Processing Units (GPUs). We present two new Fast Fourier Transform Convolution implementations: one based on NVIDIA's cuFFT library and another based on a Facebook-authored FFT implementation, fbfft, which provides significant acceleration over cuFFT (over 1.5 \u00d7) for entire CNNs. Both convolution implementations are available in open source and are faster than NVIDIA's cuDNN implementation for many common convolutions (up to 23.5 \u00d7 for a synthetic kernel configuration). We discuss different performance regimes of convolutions and compare areas where simple time domain configurations exceed frequency configurations."}, {"heading": "1 INTRODUCTION", "text": "A limiting factor for the use of revolutionary networks on large datasets has until recently been their computational effort. Krizhevsky et al. (2012) showed that training large CNNs with millions of weights and massive datasets is tractable if graphics processing units (GPUs) are used properly. Since then, renewed interest in CNNs has taken a fresh breath in various frameworks and implementations, including Torch (Collobert et al. (2011a)), Theano (Bergstra et al. (2010), cuda-connet (Krizhevsky (2014) and Caffe et al. (2014) Many of these frameUs are based on VIDA-NcoUs (2008)."}, {"heading": "2 CONVOLUTION", "text": "It is a question of whether it is at all possible for it to be a way in which it is part of a minibatch S, so that we have x (s, i) and y (s, j), i (f), i (f), i (s), i (s), i (s), i (s), i (s), i (s), i (s), i (s), i (s), i (s), i (s), i (s), i (s), i (s), i), i (s), i (s), i (s), i (s), i (s)."}, {"heading": "3 CUFFT CONVOLUTION IMPLEMENTATION", "text": "In this section, implementation strategies using the NVIDIA cuFFT libraries and their efficiency are discussed."}, {"heading": "3.1 FFT CONVOLUTION DETAILS", "text": "We described the general formulation for the three types of convolutions in Section 2. Here we borrow half of the FFT convention: Input for x (s, i); Weight forw (j, i); Output for y (s, j); GradOutput for XI (s, j); GradInput for X (s, i); and GradWeight for X (s, i). All are used as single precision 4-D tensors in the series of large layouts, and are stored in memory using the so-called BDHW formats."}, {"heading": "3.2 CUFFT DESIGN SPACE", "text": "Several factors influence the computational efficiency of FFTs: transform the size n, n's primary factor decomposition, and whether batch or iterated individual transformations are applied. In the field of deep learning, it is common to deal with small quantities, n 6 = 2k. If n exhibits undesirable properties, efficiency can decrease by an order of magnitude. 7cuFFT implements FFTs using the ubiquitous Cooley Tukey algorithm (Cooley & Tukey (1965)), which uses trigonometric equations to recursively decompress and reuse compressions. This is discussed further in the Supplement. Decomposition is based on specialized cores of fixed quantities corresponding to the primary factor decomposition of n."}, {"heading": "3.3 CUBLAS DESIGN SPACE", "text": "The cuBLAS library also comes with different implementations for batch and single-operation modes. We had the choice between 3 implementation options: \u2022 for larger batches via small matrices, the cublasCgemmBatch library calls; \u2022 for smaller batches via larger matrices, multiple cublasCgemm calls from the host; \u2022 for medium batch and matrix sizes, devices with computing capacity 3.5 and higher that support dynamic parallelism that allows CUDA cores to start other cores. This can be beneficial for many starts via small matrices. Note that the above discussion applies to multiplications after transposition. So the matrix size is either S \u00d7 f \u2032 or f \u00d7 f \u2032 and the number of such matrices is h \u00d7 W. These manufacturer libraries are usually optimized for throughput and not for latency, so we expect them to be more efficient for multiple dimensions (i.e., they are more critical for multiple dimensions)."}, {"heading": "3.4 AUTOTUNING", "text": "We combine the above implementation with a simple autotuning strategy. We design a strategy selection mechanism that is executed once for each problem size and caches the fastest strategy out of a few dozen strategies for later reuse. Autotuning strategy examines various possible Fourier base sizes, which can be broken down into potencies for which cuFFT has an efficient implementation. In other words, for a FFT dimension of size n we examine the sizes i- [n, 2-log2 n], where i = 2a3b5c7d. If the input size is a strength of 2, the search space is reduced to a single point. In addition to the Fourier base sizes, we weigh various cuBLAS calls and asynchronous modes.7http: / / docs.nvidia.com / cuda / cufft / index.html # Accuracy and performance."}, {"heading": "4 CUFFT CONVOLUTION PERFORMANCE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 PERFORMANCE VERSUS CUDNN: 8,232 CONFIGURATIONS", "text": "We compare our cuFFT results against the cuDNN evaluation table 2: 32. (2014), which contains one of the fastest, most universally applicable conversion methods for the GPU, because it essentially provides higher performance for many problem sizes, but also applies to other problems (especially text CNNs, Collobert et al. (2011b)). Thus, we limit ourselves to a 5-D problem domain {S, f, f, h = w), k (= kh =). Much of this space is not used in practice. (Some areas may be overemphasized) due to current technical concerns."}, {"heading": "4.2 CNN PERFORMANCE", "text": "In Table 3, we show performance for real CNNs, AlexNet (Krizhevsky et al. (2012)) and OverFeat fast (Sermanet al. (2014), compared to cuDNN and cuda-convnet2 (ccn2) kernels in Torch. The first layer uses cuDNN for cuFFT runs because it is striped, but all other layers use cuFFT. Timings encompass all the revolutionary layers of the network. 5 fbfft IMPLEMENTATION This section assumes familiarity with the GPU architecture, but all other layers use cuFFT. When designing high-performance libraries, several goals must be weighed against each other: memory latency / bandwidth compromise, maximized locality without sacrificing too much parallelism, good application mix, register matching, and mapping strategy for memory and compiled elements."}, {"heading": "5.1 LIMITATIONS OF CUFFT", "text": "(It is.). (It is.). (It is.). (It is.). (It is.). (It is.). (It is.). (It is.). (It is.). (It is.). (It is. (It is.). (It is.). (It is.). (It is.). (It is.). (It is.). (it is.). (It is.). (It is.). (It is. (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (It. (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it. (it.). (it. (it.).). (it.). (it. (it.). (it. (it.). (it.).). (it.). (it. (it. (it.). (it. (it.).). (it. (it.).). (it. (it. (it.). (it.). (it. (it. (it.).).). (it. (it. (it.). (it.). (it.). (it.). (it.).). (it. (it.).). (it.). (it.).). (it. (It.).).). (because. (it.).). (because. (it.). (it.).).). ("}, {"heading": "5.4 DISCUSSION", "text": "This year, the number of job-related redundancies has increased by more than 20%, and the number of job-related redundancies has increased by 20%, resulting in a 20% increase in the number of job-related redundancies."}, {"heading": "6 FUTURE WORK", "text": "\"We need to be able to establish a new system, in which we need to be able to establish a new system,\" he said. \"We need to be able to establish a new system.\" \"We need to be able to establish a new system.\" \"We need to be able to establish a new system.\" \"We need to be able to establish a new system.\" \"We need to be able to develop a new system.\" \"\" We need to be able to develop a new system. \"\" \"We need to be able to develop a new system.\" \"\" We need to be able to. \"\" \"We need to put ourselves in a position.\" \"\" We need to be able to develop a new system. \"\" \"We need to be able to.\" \"We need to be able to.\" \"We need to be able.\" \"We need to be able.\" \"We need to be able......\" We need to be able.... \"We need to be able...\" We need to be able... \"We need to be able...\" We need to be able.... \"We need to be able...\" We need to be able... \"We need to be able....\" We need to be able... \"We need to be able...\" We need to be able..... \"We need to be able...\" We need to be able... \"We need to be able....\" We need to be able... \"We need to be able....\" We need to be able.. \"We need to be able.\" We need to be able. \"We need to be able...\" We need to be able. \"We need to be able.\" We need to be able.... \"We need to be able.\" We need to be able. \"We need to be able..\" We need to be able.. \"We need to be able..\" We need to be able.. \"We need to be able.\" We need to be able. \"We need to be able.\" We need to be able..... \"We need to be able.\" We need to be able.. \"We need to be able"}, {"heading": "7 CONCLUSION", "text": "In summary, we are making significant progress on CNNs that use FFTs, with a cuFFT folding implementation achieving 1.4 x \u2212 14.5 x acceleration over cuDNN for common sizes. In response to cuFFT and cuBLAS constraints in the context of our specific field of application, we have developed our own FFT implementation fbfft, which is better suited for deep learning problem sizes (large lots, small functional levels). fbfft itself is \u2265 1.4 x faster than cuFFT transformations for these interesting problems. Even with folding problems, it is faster than cuFFT, with an average of 1.51 x for sizes we want to exploit. Given our new efficient primitives for 8-64 folding problems, we are continuing our work on bit twidging, transposition and pointedly multiplication optimizations, and continue to apply the comparative advantage of this size to larger folding problems."}, {"heading": "8 SUPPLEMENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 CUFFT CONVOLUTION PERFORMANCE BREAKDOWN", "text": "We show a breakdown of CuFFT folding performance for the steps given in Table 1. Timings do not add up to 100% of the reported performance in the previous table because we do not report additional copies for zero padding here. We also force additional synchronizations to isolate the contribution of each operation. Besides these details, the FFT and IFFT take up a significant amount of computing resources, which we address in Section 5.Table 5: CuFFT folding performance breakdown (K40m, ms) LAYER-FFT-TRANS."}, {"heading": "8.2 FFT : DECIMATION IN TIME VS FREQUENCY", "text": "A Fourier transformation projects R and C-weighted functions on a harmonious orthogonal basis. The discrete Fourier transformation of a vector {xk}, k [0, n \u2212 1] is the vector: {Xk} = n \u2212 1 [0, n \u2212 1], where wjn = e \u2212 2\u03c0ij / n is the jth n root of the unit. The traditional Radix-2 Cooley-Tukey algorithm decomposes the composition recursively between an odd and even component: {Xk} = (n \u2212 1) / 2 [n \u2212 1] / 2 [n \u2212 1] / 2 [f \u2212 1] / 2 [f \u2212 1] / 1w (2j + 1 + 1) n, k [1, n] This decomposition is called decomposition in time (DIT)."}, {"heading": "8.3 GPU PROGRAMMING", "text": "This year it has come to the point that it has never come as far as this year."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A linear filtering approach to the computation of discrete Fourier transform", "author": ["Bluestein", "Leo I"], "venue": "Audio and Electroacoustics, IEEE Transactions on,", "citeRegEx": "Bluestein and I.,? \\Q1970\\E", "shortCiteRegEx": "Bluestein and I.", "year": 1970}, {"title": "Fast fourier transforms, 2008. URL http://cnx.org/contents/ 16e8e5e8-4f22-4b53-9cd6-a15b14f01ce4@5.6:16/Fast_Fourier_ Transforms_(6x9_V", "author": ["Burrus", "C. Sidney"], "venue": null, "citeRegEx": "Burrus and Sidney.,? \\Q2008\\E", "shortCiteRegEx": "Burrus and Sidney.", "year": 2008}, {"title": "High Performance Convolutional Neural Networks for Document Processing", "author": ["Chellapilla", "Kumar", "Puri", "Sidd", "Simard", "Patrice"], "venue": "Tenth International Workshop on Frontiers in Handwriting Recognition, La Baule (France),", "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "CoRR, abs/1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "An algorithm for the machine calculation of complex fourier series", "author": ["Cooley", "James W", "Tukey", "John W"], "venue": "Mathematics of computation,", "citeRegEx": "Cooley et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Cooley et al\\.", "year": 1965}, {"title": "Parallel computing experiences with cuda", "author": ["Garland", "Michael", "Le Grand", "Scott", "Nickolls", "John", "Anderson", "Joshua", "Hardwick", "Jim", "Morton", "Phillips", "Everett", "Zhang", "Yao", "Volkov", "Vasily"], "venue": "IEEE Micro,", "citeRegEx": "Garland et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Garland et al\\.", "year": 2008}, {"title": "Course on cuda programming on nvidia gpus, lecture", "author": ["Giles", "Mike"], "venue": "URL http: //people.maths.ox.ac.uk/gilesm/cuda/lecs/lec3.pdf", "citeRegEx": "Giles and Mike.,? \\Q2014\\E", "shortCiteRegEx": "Giles and Mike.", "year": 2014}, {"title": "FLAME: formal linear algebra methods environment", "author": ["Gunnels", "John A", "Gustavson", "Fred G", "Henry", "Greg", "van de Geijn", "Robert A"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "Gunnels et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gunnels et al\\.", "year": 2001}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fast training of convolutional networks through ffts", "author": ["Mathieu", "Micha\u00ebl", "Henaff", "Mikael", "LeCun", "Yann"], "venue": "CoRR, abs/1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines", "author": ["Ragan-Kelley", "Jonathan", "Barnes", "Connelly", "Adams", "Andrew", "Paris", "Sylvain", "Durand", "Fr\u00e9do", "Amarasinghe", "Saman P"], "venue": "In ACM SIGPLAN Conference on Programming Language Design and Implementation,", "citeRegEx": "Ragan.Kelley et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ragan.Kelley et al\\.", "year": 2013}, {"title": "Optimizing matrix transpose in cuda", "author": ["Ruetsch", "Greg", "Micikevicius", "Paulius"], "venue": null, "citeRegEx": "Ruetsch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ruetsch et al\\.", "year": 2009}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Michael", "Fergus", "Rob", "LeCun", "Yann"], "venue": "In International Conference on Learning Representations (ICLR", "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Speeding up nek5000 with autotuning and specialization", "author": ["Shin", "Jaewook", "Hall", "Mary W", "Chame", "Jacqueline", "Chen", "Chun", "Fischer", "Paul F", "Hovland", "Paul D"], "venue": "In Proceedings of the 24th ACM International Conference on Supercomputing,", "citeRegEx": "Shin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2010}, {"title": "A bridging model for parallel computation", "author": ["Valiant", "Leslie G"], "venue": "Commun. ACM,", "citeRegEx": "Valiant and G.,? \\Q1990\\E", "shortCiteRegEx": "Valiant and G.", "year": 1990}, {"title": "Better performance at lower occupancy", "author": ["V. Volkov"], "venue": "In GPU Technology Conference,", "citeRegEx": "Volkov,? \\Q2010\\E", "shortCiteRegEx": "Volkov", "year": 2010}, {"title": "2008)) which we won\u2019t discuss in detail, but the implementation of fbfft", "author": ["tures (Garland"], "venue": null, "citeRegEx": ".Garland,? \\Q2008\\E", "shortCiteRegEx": ".Garland", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "Krizhevsky et al. (2012) demonstrated that training of large CNNs with millions of weights and massive data sets is tractable when graphics processing units (GPUs) are properly put to use.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Since then, renewed interest in CNNs insufflated a fresh breath in various frameworks and implementations, including Torch (Collobert et al. (2011a)), Theano (Bergstra et al.", "startOffset": 124, "endOffset": 149}, {"referenceID": 0, "context": "(2011a)), Theano (Bergstra et al. (2010)), cuda-convnet (Krizhevsky (2014)) and Caffe (Jia et al.", "startOffset": 18, "endOffset": 41}, {"referenceID": 0, "context": "(2011a)), Theano (Bergstra et al. (2010)), cuda-convnet (Krizhevsky (2014)) and Caffe (Jia et al.", "startOffset": 18, "endOffset": 75}, {"referenceID": 0, "context": "(2011a)), Theano (Bergstra et al. (2010)), cuda-convnet (Krizhevsky (2014)) and Caffe (Jia et al. (2014)).", "startOffset": 18, "endOffset": 105}, {"referenceID": 0, "context": "(2011a)), Theano (Bergstra et al. (2010)), cuda-convnet (Krizhevsky (2014)) and Caffe (Jia et al. (2014)). Many of these frameworks are based around codes for NVIDIA GPUs using CUDA (Garland et al. (2008)).", "startOffset": 18, "endOffset": 205}, {"referenceID": 0, "context": "(2011a)), Theano (Bergstra et al. (2010)), cuda-convnet (Krizhevsky (2014)) and Caffe (Jia et al. (2014)). Many of these frameworks are based around codes for NVIDIA GPUs using CUDA (Garland et al. (2008)). We discuss our contributions to convolution performance on these GPUs, namely using Fast Fourier Transform (FFT) implementations within the Torch framework. We summarize the theory behind training convolutional layers both in the time and frequency domain in Section 2. We then detail our implementations. The first is based on NVIDIA\u2019s cuFFT and cuBLAS libraries (Section 3). We evaluate our relative performance to NVIDIA\u2019s cuDNN library (Chetlur et al. (2014)) on over 8, 000 different configurations (Section 4).", "startOffset": 18, "endOffset": 670}, {"referenceID": 13, "context": "We quickly summarize these and their implementation, with a formulation mirroring Mathieu et al. (2013). Forward propagation (fprop) inputs are a set f of input feature planes xi, i \u2208 f .", "startOffset": 82, "endOffset": 104}, {"referenceID": 3, "context": "3 A popular convolution implementation is to unroll the data until the computation is in the form of a large matrix multiplication (Chellapilla et al. (2006)).", "startOffset": 132, "endOffset": 158}, {"referenceID": 3, "context": "3 A popular convolution implementation is to unroll the data until the computation is in the form of a large matrix multiplication (Chellapilla et al. (2006)). This is the strategy followed by many implementors, since matrix multiplication is a well-tuned linear algebra primitive available on virtually any platform. While it is possible to provide instances of direct calculation that are faster than matrix unrolling (e.g., for large S, Krizhevsky (2014)), it is challenging to provide an implementation that is faster for more than just a small subset of possible convolution problems.", "startOffset": 132, "endOffset": 458}, {"referenceID": 4, "context": "0 library (Chetlur et al. (2014)), which contains one of the fastest, general purpose convolution methods for the GPU, using matrix unrolling.", "startOffset": 11, "endOffset": 33}, {"referenceID": 4, "context": "0 library (Chetlur et al. (2014)), which contains one of the fastest, general purpose convolution methods for the GPU, using matrix unrolling. It has decent performance for many problem sizes thanks to heavy autotuning of cuBLAS codes for different problems. It is a strong baseline for this reason. Image CNNs to date have for the most part used square input images and filters, though rectangular filters are valid for other problems (notably text CNNs, Collobert et al. (2011b)).", "startOffset": 11, "endOffset": 481}, {"referenceID": 12, "context": "2 CNN PERFORMANCE In table 3, we show performance for real CNNs, AlexNet (Krizhevsky et al. (2012)) and OverFeat fast (Sermanet et al.", "startOffset": 74, "endOffset": 99}, {"referenceID": 12, "context": "2 CNN PERFORMANCE In table 3, we show performance for real CNNs, AlexNet (Krizhevsky et al. (2012)) and OverFeat fast (Sermanet et al. (2014)), comparing against cuDNN and cuda-convnet2 (ccn2) kernels in Torch.", "startOffset": 74, "endOffset": 142}, {"referenceID": 10, "context": "A key principle is to design a set of leaf kernels with well-tuned in-register performance and reduce the larger problem to a combination of these kernels by data and loop tiling (Irigoin & Triolet (1988)) and recursive decompositions (Gunnels et al. (2001)).", "startOffset": 236, "endOffset": 258}, {"referenceID": 10, "context": "A key principle is to design a set of leaf kernels with well-tuned in-register performance and reduce the larger problem to a combination of these kernels by data and loop tiling (Irigoin & Triolet (1988)) and recursive decompositions (Gunnels et al. (2001)). Since vendors have to sustain high performance for a large class of application domains, there exist parameter configurations for which a carefully tuned approach significantly outperforms vendor-tuned libraries (Shin et al. (2010)).", "startOffset": 236, "endOffset": 492}, {"referenceID": 14, "context": "This is an approach used in automatic code generation tools such as Halide (Ragan-Kelley et al. (2013)) and relies on aggressive if-conversion properties of the CUDA compiler.", "startOffset": 76, "endOffset": 103}], "year": 2014, "abstractText": "We examine the performance profile of Convolutional Neural Network (CNN) training on the current generation of NVIDIA Graphics Processing Units (GPUs). We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA\u2019s cuFFT library, and another based on a Facebook authored FFT implementation,fbfft, that provides significant speedups over cuFFT (over 1.5\u00d7) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA\u2019s cuDNN implementation for many common convolutional layers (up to 23.5\u00d7 for a synthetic kernel configuration). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}