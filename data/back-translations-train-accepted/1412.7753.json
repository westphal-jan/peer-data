{"id": "1412.7753", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "Learning Longer Memory in Recurrent Neural Networks", "abstract": "Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming kind of a longer term memory. We evaluate our model in language modeling experiments, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter &amp; Schmidhuber, 1997).", "histories": [["v1", "Wed, 24 Dec 2014 20:58:18 GMT  (222kb,D)", "http://arxiv.org/abs/1412.7753v1", null], ["v2", "Thu, 16 Apr 2015 23:37:58 GMT  (223kb,D)", "http://arxiv.org/abs/1412.7753v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["tomas mikolov", "armand joulin", "sumit chopra", "michael mathieu", "marc'aurelio ranzato"], "accepted": true, "id": "1412.7753"}, "pdf": {"name": "1412.7753.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu"], "emails": ["tmikolov@fb.com", "ajoulin@fb.com", "spchopra@fb.com", "myrhev@fb.com", "ranzato@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into another world, in which they move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live."}, {"heading": "2 MODEL", "text": "In this section, we describe the simple recurring network (SRN) model popularized by Elman (1990), and that is the cornerstone of this work. An SRN consists of an input layer, a hidden layer with a recurring connection, and an output layer (see Figure 1-a). The recurring connection allows propagation through the time of information about the state of the hidden layer. In the face of a sequence of tokens, an SRN takes xt of the current tokens as input of the most uniform encoding and predicts the likelihood yt of the next one."}, {"heading": "2.2 CONTEXT FEATURES", "text": "In this section, we propose an extension of the SRN by adding a hidden layer specifically designed to capture long-term dependencies. We design this layer according to two observations: (1) non-linearity can cause gradients to disappear, (2) a completely hidden layer changes its state completely at any time step.SRN, however, uses a fully connected recursive matrix that allows complex patterns to propagate over time, but suffers from the fact that the state of the hidden units changes rapidly at any time step. On the other hand, the use of a recurrent matrix equal to identity and the removal of non-linearity would keep the state of the hidden layer constant, and any change in state would have to come from external inputs. This should allow information to be retained for a longer period of time. More precisely, the rule would be: st = st \u2212 1 + Bxt (3), where the d \u00d7 s context cannot be trained to a matrix that is efficient solution."}, {"heading": "3 EXPERIMENTS", "text": "We evaluate our model using the language modeling task for two sets of data. The first data set is Penn Treebank Corpus1, which consists of 930K words in the training set. Pre-processing of data and subdivision into training, validation and test parts are the same as in (Mikolov et al., 2011). The state-of-the-art performance of this data set was achieved by Zaremba et al. (2014), using a combination of many large, regulated LSTM recurring neural network language models. LSTM networks were first introduced into language modeling by Sundermeyer et al. (2012). The second data set, which is moderately large, is called Text82. It is composed of a pre-processed version of the first 100 million characters from Wikipedia dump. We have divided it into a training part (first 99 M characters) and a development set (last 1M characters), which we use to report performance. After this, we have constructed the word set and replaced it with 44K."}, {"heading": "3.1 IMPLEMENTATION DETAILS.", "text": "We used the Torch library and implemented our proposed model according to the one shown in Figure 1-b. Note that according to the alternative interpretation of our model using the recursive matrix defined in Eq.8, our model could be implemented simply by modifying a standard SRN. We set \u03b1 to 0.95 unless otherwise specified. The number of backpropagation steps through time (BPTT) is set to 50 for our model and was selected by parameter search on the validation set. For normal SRN, we only use 10 BPTT steps because the gradients disappear faster. We perform a stochastic gradient descent after all 5 forward steps. Our model is trained with a size 32 stack gradient descent and a learning rate of 0.05. We divide the learning rate by 1.5 after each training period if the validation error does not decrease."}, {"heading": "3.2 RESULTS ON PENN TREEBANK CORPUS.", "text": "First, we report on the results of the Penn Treebank Corpus using both small and medium-sized models (in terms of the number of hidden units). Table 1 shows that our structurally constrained recursive network model (SCRN) can achieve performance comparable to LSTM models on small datasets with a relatively small number of parameters. It should be noted that the LSTM models have significantly more parameters for the same size of the hidden layer, which makes the comparison somewhat unfair - comparing SCRN with 40 hidden and 10 contextual units (test perplexity 127) versus SRN with the same size of the hidden layer shows that SCRN exceeds the SRN architecture even with much fewer parameters, as can be seen by comparing the performance of SCRN with 40 hidden and 10 contextual units (test perplexity 127) versus SRN with 300 hidden units (perplexity 129)."}, {"heading": "3.2.1 LEARNING SELF-RECURRENT WEIGHTS.", "text": "For the following experiments, we used a hierarchical soft-max method with 100 frequency-based classes at Penn Treebank Corpus to speed up the experiments. However, in Table 2, we show that learning diagonal weights is crucial when the hidden layer is small in size, and this result confirms the results of Pachitariu & Sahani (2013).However, since we are increasing the size of our model and using a sufficient number of hidden units, learning diagonal weights does not result in a significant improvement, suggesting that learning the weights of the contextual units allows these units to be used as a multi-scale representation of history, i.e. some contextual units may specialize in very recent history (for example, if the contextual units are close to 0, they would be part of a simple bigram-language model).With various learned, self-cursive mind units, we can consider the combination weights as indispensable if we consider the standard weights to be morally correct."}, {"heading": "3.3 RESULTS ON TEXT8.", "text": "Our next experiment concerns the Text8 corpus, which is significantly larger than the Penn Treebank. As this dataset contains several Wikipedia articles, longer-term information (such as the current topic) plays a greater role than in previous experiments. This is illustrated by the gains in adding the cache to the 5-gram base model: Perplexity drops from 309 to 229 (26% reduction).We report experiments with a number of model configurations with different numbers of hidden units. In Table 3, we show that increasing the capacity of standard SRNs by adding contextual features leads to better performance. For example, if we add contextual units with 100 hidden units to the SRN 40, the perplexity drops from 245 to 189 (23% reduction).Such a model is also much better than SRN with 300 hidden units (SCP 202).4 In table, we see that our STM model is better than the STM model, although the STM model is better than both RM models."}, {"heading": "4 CONCLUSION", "text": "In this paper, we have shown that learning longer-term patterns in real data using recurrent networks using the usual stochastic gradient lineage is quite feasible only by introducing structural constraints on the recursive weight matrix, and the model can then be interpreted as having fast-changing hidden layers focusing on short-term patterns, and slowly updating the context layer that retains longer-term information. Empirical comparison of SCRN with Long Short Term Memory (LSTM) shows very similar behavior in bilingual modeling tasks, with similar gains compared to simple recursive networks. This is somewhat surprising, since the LSTM structure is much more complex. We believe that these results will help other researchers better understand the problem of learning longer-term memory in sequential data. Our model simplifies analysis and implementation of long-term learning networks, which appears to be able to learn longer-term patterns in the above."}], "references": [{"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["Dahl", "George E", "Yu", "Dong", "Deng", "Li", "Acero", "Alex"], "venue": "IEEE Transactions on,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Classes for fast maximum entropy training", "author": ["Goodman", "Joshua"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Goodman and Joshua.,? \\Q2001\\E", "shortCiteRegEx": "Goodman and Joshua.", "year": 2001}, {"title": "A bit of progress in language modeling", "author": ["Goodman", "Joshua T"], "venue": "Computer Speech & Language,", "citeRegEx": "Goodman and T.,? \\Q2001\\E", "shortCiteRegEx": "Goodman and T.", "year": 2001}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Graves", "Alex", "Schmidhuber", "Juergen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Hochreiter", "Sepp"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1998}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Attractor dynamics and parallelism in a connectionist sequential machine", "author": ["Jordan", "Michael I"], "venue": "Proceedings of the Eighth Annual Conference of the Cognitive Science Society,", "citeRegEx": "Jordan and I.,? \\Q1987\\E", "shortCiteRegEx": "Jordan and I.", "year": 1987}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn", "Philipp", "Hoang", "Hieu", "Birch", "Alexandra", "Callison-Burch", "Chris", "Federico", "Marcello", "Bertoldi", "Nicola", "Cowan", "Brooke", "Shen", "Wade", "Moran", "Christine", "Zens", "Richard"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "A cache-based natural language model for speech recognition", "author": ["Kuhn", "Roland", "De Mori", "Renato"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Kuhn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 1990}, {"title": "Efficient backprop", "author": ["LeCun", "Yann", "Bottou", "Leon", "Orr", "Genevieve", "M\u00fcller", "Klaus"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Statistical language models based on neural networks", "author": ["Mikolov", "Tom\u00e1\u0161"], "venue": "PhD thesis,", "citeRegEx": "Mikolov and Tom\u00e1\u0161.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tom\u00e1\u0161.", "year": 2012}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Tomas", "Zweig", "Geoffrey"], "venue": "In SLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Extensions of recurrent neural network language model", "author": ["Mikolov", "Tomas", "Kombrink", "Stefan", "Burget", "Lukas", "JH Cernocky", "Khudanpur", "Sanjeev"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "A focused back-propagation algorithm for temporal pattern recognition", "author": ["Mozer", "Michael C"], "venue": "Complex systems,", "citeRegEx": "Mozer and C.,? \\Q1989\\E", "shortCiteRegEx": "Mozer and C.", "year": 1989}, {"title": "Neural net architectures for temporal sequence processing", "author": ["Mozer", "Michael C"], "venue": "In Santa Fe Institute Studies in The Sciences of Complexity,", "citeRegEx": "Mozer and C.,? \\Q1993\\E", "shortCiteRegEx": "Mozer and C.", "year": 1993}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Regularization and nonlinearities for neural language models: when are they needed", "author": ["Pachitariu", "Marius", "Sahani", "Maneesh"], "venue": "arXiv preprint arXiv:1301.5650,", "citeRegEx": "Pachitariu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pachitariu et al\\.", "year": 2013}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Lstm neural networks for language modeling", "author": ["Sundermeyer", "Martin", "Schl\u00fcter", "Ralf", "Ney", "Hermann"], "venue": "In INTERSPEECH,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Werbos", "Paul J"], "venue": "Neural Networks,", "citeRegEx": "Werbos and J.,? \\Q1988\\E", "shortCiteRegEx": "Werbos and J.", "year": 1988}, {"title": "Gradient-based learning algorithms for recurrent networks and their computational complexity", "author": ["Williams", "Ronald J", "Zipser", "David"], "venue": "Back-propagation: Theory, architectures and applications,", "citeRegEx": "Williams et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1995}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "This has been widely studied in the past with approaches taking their roots in a variety of fields (Goodman, 2001b; Young et al., 1997; Koehn et al., 2007).", "startOffset": 99, "endOffset": 155}, {"referenceID": 0, "context": "In particular, models based on neural networks have been very successful recently, obtaining state-of-the-art performances in automatic speech recognition (Dahl et al., 2012), language modeling (Mikolov, 2012) and video classification (Simonyan & Zisserman, 2014).", "startOffset": 155, "endOffset": 174}, {"referenceID": 19, "context": "Feedforward architectures such as time-delayed neural networks usually represent time explicitly with a fixed-length window of the recent history (Rumelhart et al., 1985).", "startOffset": 146, "endOffset": 170}, {"referenceID": 0, "context": "In particular, models based on neural networks have been very successful recently, obtaining state-of-the-art performances in automatic speech recognition (Dahl et al., 2012), language modeling (Mikolov, 2012) and video classification (Simonyan & Zisserman, 2014). These models are mostly based on two families of neural networks: feedforward neural networks and recurrent neural networks. Feedforward architectures such as time-delayed neural networks usually represent time explicitly with a fixed-length window of the recent history (Rumelhart et al., 1985). While this type of models work well in practice, fixing the window size makes long-term dependency harder to learn and can only be done at the cost of a linear increase of the number of parameters. The recurrent architectures, on the other hand, represent time recursively. For example, in the simple recurrent network (SRN) (Elman, 1990), the state of the hidden layer at a given time is conditioned on its previous state. This recursion allows the model to store complex signals for arbitrarily long time periods, as the state of the hidden layer can be seen as the memory of the model. In theory, this architecture could even encode a \u201cperfect\u201d memory by simply copying the state of the hidden layer over time. While theoretically powerful, these recurrent models were widely considered to be hard to train due to the so-called vanishing and exploding gradient problems (Hochreiter, 1998; Bengio et al., 1994). Mikolov (2012) showed how to avoid the exploding gradient problem by using simple, yet efficient strategy of gradient clipping.", "startOffset": 156, "endOffset": 1491}, {"referenceID": 11, "context": "This can be achieved by using secondorder methods designed for non-convex objective functions (see section 7 in LeCun et al. (1998)).", "startOffset": 112, "endOffset": 132}, {"referenceID": 14, "context": "We use a simple hierarchy with two levels, by binning the tokens into \u221a d clusters with same cumulative word frequency (Mikolov et al., 2011).", "startOffset": 119, "endOffset": 141}, {"referenceID": 19, "context": "The model is trained by using stochastic gradient descent method with back-propagation through time (Rumelhart et al., 1985; Williams & Zipser, 1995; Werbos, 1988).", "startOffset": 100, "endOffset": 163}, {"referenceID": 14, "context": "The pre-processing of data and division to training, validation and test parts are the same as in (Mikolov et al., 2011).", "startOffset": 98, "endOffset": 120}, {"referenceID": 13, "context": "The pre-processing of data and division to training, validation and test parts are the same as in (Mikolov et al., 2011). The state-ofthe-art performance on this dataset has been achieved by Zaremba et al. (2014), using combination of many big, regularized LSTM recurrent neural network language models.", "startOffset": 99, "endOffset": 213}, {"referenceID": 13, "context": "The pre-processing of data and division to training, validation and test parts are the same as in (Mikolov et al., 2011). The state-ofthe-art performance on this dataset has been achieved by Zaremba et al. (2014), using combination of many big, regularized LSTM recurrent neural network language models. The LSTM networks were first introduced to language modeling by Sundermeyer et al. (2012). The second dataset, which is moderately sized, is called Text82.", "startOffset": 99, "endOffset": 394}], "year": 2017, "abstractText": "Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming a kind of longer term memory. We evaluate our model on language modeling tasks on benchmark datasets, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997).", "creator": "LaTeX with hyperref package"}}}