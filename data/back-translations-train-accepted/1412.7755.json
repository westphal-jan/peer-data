{"id": "1412.7755", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "Multiple Object Recognition with Visual Attention", "abstract": "We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.", "histories": [["v1", "Wed, 24 Dec 2014 20:58:23 GMT  (618kb,D)", "http://arxiv.org/abs/1412.7755v1", null], ["v2", "Thu, 23 Apr 2015 16:49:23 GMT  (618kb,D)", "http://arxiv.org/abs/1412.7755v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["jimmy ba", "volodymyr mnih", "koray kavukcuoglu"], "accepted": true, "id": "1412.7755"}, "pdf": {"name": "1412.7755.pdf", "metadata": {"source": "CRF", "title": "VISUAL ATTENTION", "authors": ["Jimmy Lei Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "emails": ["jimmy@psi.utoronto.ca", "vmnih@google.com", "korayk@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "We have been working very successfully on a variety of recognition and classification processes in recent years (Krizhevsky et al., 2012; Goodfellow et al., 2013; Jaderberg et al., 2014a; Vinyals et al., 2014; Karpathy et al., 2014). One of the main drawbacks of Convolutionary Networks (ConvNets) is their poor scalability with increasing input image size, so efficient implementations of these models have become necessary on multiple GPUs (Krizhevsky et al., 2012) or even across multiple machines (Dean et al., 2012b). Applications of ConvNets to multi-object and sequence recognition from images have been avoided and instead focused on the use of ConvNets to detect characters or short sequence segments from image fields containing relatively narrowly cropped instances (Goodfellow et al., 2013; Jaderberg et al., 2014a)."}, {"heading": "2 RELATED WORK", "text": "Perhaps the most common approach to image-based classification of character sequences involves combining a sliding window detector with a character classifier (Wang et al., 2012; Jaderberg et al., 2014b). The detector and the classifier are typically trained separately, using different loss functions. Groundbreaking work on ConvNets of LeCun et al. (1998) introduced a graph transformer network architecture to detect a sequence of digits when reading checks, and also showed how the entire system could be trained end-to-end. However, this system relied on a number of ad-hoc components to extract candidate locations. More recently, ConvNets operating on truncated time sequences have achieved a state-of-art performance in the external recognition of Goodfellow et al (Goodfellow et al can be assigned to 2014)."}, {"heading": "3 DEEP RECURRENT VISUAL ATTENTION MODEL", "text": "In fact, most of us will be able to move to another world in which we are able to live, in which we want to live."}, {"heading": "3.1 LEARNING WHERE AND WHAT", "text": "In view of the fact that the learning rules can be derived from the inclusion of derivatives from the models mentioned above, learning can be formulated as an overarching classification problem. (W) The attention model predicts the class resolution for the individual locations. (W) The marginalized objective function can be learned by optimizing its variable free energy lower limit. (W) The learning model is optimized by optimizing the variable free energy lower limit F: log / I, W) p (Y, W) p (I, W) p (I) p (I) p (I) p (I) p (I) p (I) p (I) p (I) p (I) p (I) p (I) p (I) p (p) p (p) p (I) p (p) p (I p) p (I) p (I p) p (I p) p (I p) p (I p) p (I (I p) p (I p) p (I p) p (I p (I p) p (I p) p (I p) p (I p (I p) p (I p) p (I p (I p) p (I p) p (I p (p) p (I p) p (I p (p) p (I p) p (I p (p) p (I p (p) p (I p) p (I p (I p (I p) p (I p (I p) p (I p) p (I p (I p (I p (I p) p (I p (I p (I p) p) p (I p (I p (I p (I p (I p) p (I p (I p) p (I p (I p) p (I p (I p (I) p (I p (I p (I p) p (I p (I p (I p (I p (I p) p (I p) p (I p (I p) p (I p (I p) p (I p) p (I p (I p (I p (I p (I p) p (I p (I p) p (I p) p (I p (I p (I p) p (I p ("}, {"heading": "3.2 MULTI-OBJECT/SEQUENTIAL CLASSIFICATION AS A VISUAL ATTENTION TASK", "text": "Our proposed attention model can be easily expanded to solve multi-object classification problems. To train the deep-recurring attention model for the sequential recognition task, the multiple object labels for a particular image must be thrown into an ordered sequence {y1, y2, \u00b7 \u00b7 \u00b7, ys}. The low-recurring attention model then learns to predict an object at a time while it is sequentially exploring the image. We can use a simple fixed number of glances for each target in the sequence. In addition, a new class designation for the \"end of the sequence\" symbol is added to deal with variable number of objects in an image. We can stop the recurring attention model as soon as a terminal symbol is predicted. Specifically, the objective function for the sequential prediction islog p (y1, y2, \u00b7, yS | I, W s \u00b2 = S \u00b2 is repeated as a learning book, 1 = W (Logls I | p) is calculated as the number of the recurrent W (s)."}, {"heading": "4 EXPERIMENTS", "text": "To demonstrate the effectiveness of the deep recurrent attention model (DRAM), we first examine a number of multi-object classification tasks involving a variant of MNIST. Then we apply the proposed attention model to a real-world object detection task using the multi-digit SVHN dataset Netzer et al. (2011) and compare it with the state-of-the-art deep ConvNets. A description of the models and training protocols we use can be found in the appendix. Table 1: Error rates in the classification of the MNIST pairs task.Model Test Err. RAM Mnih et al. (2014) 9% DRAM w / o Context 7% DRAM 5% Table 2: Error rates in the double-digit addition task.Model Test Err. ConvNet 64-64-512 3.2% DRAM 2.5% Figure n: Links."}, {"heading": "4.1 LEARNING TO FIND DIGITS", "text": "First, we evaluate the effectiveness of the controller in the Deep Recurrent Attention Model using the MNIST handwritten digit dataset. We generated a dataset of randomly selected handwritten digits in a 100x100 image with distraction noise in the background. The task is to identify the 55 different combinations of the two digits as a classification problem. Attention models are given 4 insights before making a classification prediction. The aim of this experiment is to evaluate the ability of the controller and the recursive network to combine information from multiple impressions with minimal effort from the visual network.The results are presented in Table (4.1).The Context Network DRAM model clearly outperforms the other models."}, {"heading": "4.2 LEARNING TO DO ADDITION", "text": "For a more difficult task, we have designed another set of data with two MNIST digits on an empty 100x100 background, in which the task is to predict the sum of the two digits in the image as a classification problem with 19 targets. The model has to find out where each digit is located and add them up. If the two digits are scanned uniformly from all classes, the distribution of the names for the summation, where most of the probability mass is concentrated on about 10, is strongly unbalanced. In addition, there are many number combinations that can be assigned to the same target, for example [5,5] and [3,7]. In this task, the class designation provides a weaker association between the visual feature and the monitoring signal than in the number combination task. We used the same model as in the combination task. The low recurring attention model is able to detect a policy of looking at it in order to accomplish this task and to achieve a lower error rate in order to 2.5 and lower it."}, {"heading": "4.3 LEARNING TO READ HOUSE NUMBERS", "text": "The multivision house numbers (SVHN) available to the public are viewed from nature. (2011) It's about the images of people who are able to move in the world. (2013) It's about the way in which they move. (2013) It's about the way in which they move. (2012) It's about the way in which they move. (2012) It's about the way in which they move. (2013) It's about the way in which they move. (2013) It's about the way in which they move. (2013) It's about the way in which they move. (2013) It's about the way in which they move. (2013) It's about the way in which they move. (2013) It's about the way in which they move."}, {"heading": "5 DISCUSSION", "text": "In our experiments, the proposed Deep Recurring Attention Model (DRAM) outperforms the state-of-the-art Deep ConvNets on the standard SVHN sequence recognition task. In addition, the benefit of the Attention Model becomes more significant when we increase the image area around the house numbers or reduce the signal-to-noise ratio. In Table 5, we compare the calculation costs of our proposed Deep Recurring Attention Model with that of the Deep ConvNets in terms of the number of float pointing operations for the multi-digit SVHN models along with the number of parameters in each model. Recurring Attention Models, which handle only a selected subset of the input scale better than a ConvNet looking over an entire image, calculate the estimated cost of DRAM using the maximum sequence length in the dataset, with the expected computational costs in practice being much lower as most of the house numbers are 2-3 digits long."}, {"heading": "6 CONCLUSION", "text": "We have described a novel computer vision model that uses an attention mechanism to decide where to focus its calculation, and demonstrated how it can be trained end-to-end to classify multiple objects sequentially in an image. It has outperformed the state-of-the-art ConvNets in a multi-digit house number recognition task, using both fewer parameters and less calculation than the best ConvNets, demonstrating that attention mechanisms can improve both the accuracy and efficiency of ConvNets in a real-world task. Since our proposed deep recurring attention model is flexible, powerful, and efficient, we believe it could be a promising approach to tackle other demanding computer vision tasks."}, {"heading": "7 ACKNOWLEDGEMENTS", "text": "We would like to thank Geoffrey Hinton and Chris Summerfield for many helpful comments and discussions and also the developers of DistBelief (Dean et al., 2012a)."}, {"heading": "8 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 GENERAL TRAINING DETAILS", "text": "We found that ReLU units significantly accelerate training. We optimized the model parameters using stochastic gradient reduction using Nesterov pulse technology. A mini batch size of 128 was used to estimate the direction of the gradient. The pulse coefficient was set to 0.9 during training. In training, the learning rate \u03b7 scheduling was used to improve the convergence of the learning process. \u03b7 starts at 0.01 in the first epoch and was reduced exponentially by a factor of 0.97 after each epoch."}, {"heading": "8.2 DETAILS OF LEARNING TO FIND DIGITS", "text": "The unit width for the Cartesian coordinates has been set to 20 and the standard deviation for viewing angle detection to 0.03. There are 512 LSTM units and 256 hidden units in each fully connected layer of the model. We deliberately used a simple, fully connected network of 256 hidden units as a Gimage (\u00b7) in the visual network."}, {"heading": "8.3 DETAILS OF LEARNING TO READ HOUSE NUMBERS", "text": "Unlike in the MNIST experiment, the number of digits in each image varies and the digits have more variation due to natural backgrounds, lighting variations and highly variable resolution. For this task, we use a much larger, recurring attention model. It was crucial to have a powerful viewing network in order to achieve good performance. As described in Section 3, the viewing network consists of three sinuous layers with 5x5 filter cores in the first layer and 3x3 in the two later layers. The number of filters in these layers was {64, 64, 128}. There are 512 LSTM units in each layer of the recursive network. In addition, the completely hidden layers all have 1024 ReLU hidden units in each module listed in Section 3. Cartesian coordinate width was set at 12 pixels and the field of view is scanned by a fixed variance of 0.03."}], "references": [{"title": "Searching for objects driven by context", "author": ["Alexe", "Bogdan", "Heess", "Nicolas", "Teh", "Yee Whye", "Ferrari", "Vittorio"], "venue": "In NIPS,", "citeRegEx": "Alexe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alexe et al\\.", "year": 2012}, {"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["Goodfellow", "Ian J", "Bulatov", "Yaroslav", "Ibarz", "Julian", "Arnoud", "Sacha", "Shet", "Vinay"], "venue": "arXiv preprint arXiv:1312.6082,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A model of saliency-based visual attention for rapid scene analysis", "author": ["L. Itti", "C. Koch", "E. Niebur"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Itti et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Itti et al\\.", "year": 1998}, {"title": "Synthetic data and artificial neural networks for natural scene text recognition", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1406.2227,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Deep features for text spotting", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Karpathy", "Andrej", "Joulin", "Armand", "Li", "Fei Fei F"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoff"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Larochelle", "Hugo", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Larochelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Structured prediction with reinforcement learning", "author": ["Maes", "Francis", "Denoyer", "Ludovic", "Gallinari", "Patrick"], "venue": "Machine learning,", "citeRegEx": "Maes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maes et al\\.", "year": 2009}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1406.6247,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "End-to-end text recognition with convolutional neural networks", "author": ["Wang", "Tao", "Wu", "David J", "Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 9, "context": "Convolutional neural networks have recently been very successful on a variety of recognition and classification tasks (Krizhevsky et al., 2012; Goodfellow et al., 2013; Jaderberg et al., 2014a; Vinyals et al., 2014; Karpathy et al., 2014).", "startOffset": 118, "endOffset": 238}, {"referenceID": 3, "context": "Convolutional neural networks have recently been very successful on a variety of recognition and classification tasks (Krizhevsky et al., 2012; Goodfellow et al., 2013; Jaderberg et al., 2014a; Vinyals et al., 2014; Karpathy et al., 2014).", "startOffset": 118, "endOffset": 238}, {"referenceID": 15, "context": "Convolutional neural networks have recently been very successful on a variety of recognition and classification tasks (Krizhevsky et al., 2012; Goodfellow et al., 2013; Jaderberg et al., 2014a; Vinyals et al., 2014; Karpathy et al., 2014).", "startOffset": 118, "endOffset": 238}, {"referenceID": 8, "context": "Convolutional neural networks have recently been very successful on a variety of recognition and classification tasks (Krizhevsky et al., 2012; Goodfellow et al., 2013; Jaderberg et al., 2014a; Vinyals et al., 2014; Karpathy et al., 2014).", "startOffset": 118, "endOffset": 238}, {"referenceID": 9, "context": "One of the main drawbacks of convolutional networks (ConvNets) is their poor scalability with increasing input image size so efficient implementations of these models on multiple GPUs (Krizhevsky et al., 2012) or even spanning multiple machines (Dean et al.", "startOffset": 184, "endOffset": 209}, {"referenceID": 3, "context": "Applications of ConvNets to multi-object and sequence recognition from images have avoided working with big images and instead focused on using ConvNets for recognizing characters or short sequence segments from image patches containing reasonably tightly cropped instances (Goodfellow et al., 2013; Jaderberg et al., 2014a).", "startOffset": 274, "endOffset": 324}, {"referenceID": 16, "context": "Perhaps the most common approach to image-based classification of character sequences involves combining a sliding window detector with a character classifier (Wang et al., 2012; Jaderberg et al., 2014b).", "startOffset": 159, "endOffset": 203}, {"referenceID": 3, "context": "More recently, ConvNets operating on cropped sequences of characters have achieved state-of-theart performance on house number recognition (Goodfellow et al., 2013) and natural scene text recognition (Jaderberg et al.", "startOffset": 139, "endOffset": 164}, {"referenceID": 5, "context": "Our work builds on the long line of the previous attempts on attention-based visual processing (Itti et al., 1998; Larochelle & Hinton, 2010; Alexe et al., 2012), and in particular extends the recurrent attention model (RAM) proposed in Mnih et al.", "startOffset": 95, "endOffset": 161}, {"referenceID": 0, "context": "Our work builds on the long line of the previous attempts on attention-based visual processing (Itti et al., 1998; Larochelle & Hinton, 2010; Alexe et al., 2012), and in particular extends the recurrent attention model (RAM) proposed in Mnih et al.", "startOffset": 95, "endOffset": 161}, {"referenceID": 3, "context": ", 2012; Jaderberg et al., 2014b). The detector and the classifier are typically trained separately, using different loss functions. The seminal work on ConvNets of LeCun et al. (1998) introduced a graph transformer network architecture for recognizing a sequence of digits when reading checks, and also showed how the whole system could be trained end-to-end.", "startOffset": 8, "endOffset": 184}, {"referenceID": 2, "context": "More recently, ConvNets operating on cropped sequences of characters have achieved state-of-theart performance on house number recognition (Goodfellow et al., 2013) and natural scene text recognition (Jaderberg et al., 2014a). Goodfellow et al. (2013) trained a separate ConvNets classifier for each character position in a house number with all weights except for the output layer shared among the classifiers.", "startOffset": 140, "endOffset": 252}, {"referenceID": 2, "context": "More recently, ConvNets operating on cropped sequences of characters have achieved state-of-theart performance on house number recognition (Goodfellow et al., 2013) and natural scene text recognition (Jaderberg et al., 2014a). Goodfellow et al. (2013) trained a separate ConvNets classifier for each character position in a house number with all weights except for the output layer shared among the classifiers. Jaderberg et al. (2014a) showed that synthetically generated images of text can be used to train ConvNets classifiers that achieve state-of-the-art text recognition performance on realworld images of cropped text.", "startOffset": 140, "endOffset": 437}, {"referenceID": 0, "context": ", 1998; Larochelle & Hinton, 2010; Alexe et al., 2012), and in particular extends the recurrent attention model (RAM) proposed in Mnih et al. (2014). While RAM was shown to learn successful gaze strategies on cluttered digit classification tasks and on a toy visual control problem it was not shown to scale to real-world image tasks or multiple objects.", "startOffset": 35, "endOffset": 149}, {"referenceID": 0, "context": ", 1998; Larochelle & Hinton, 2010; Alexe et al., 2012), and in particular extends the recurrent attention model (RAM) proposed in Mnih et al. (2014). While RAM was shown to learn successful gaze strategies on cluttered digit classification tasks and on a toy visual control problem it was not shown to scale to real-world image tasks or multiple objects. Our approach of learning by maximizing variational lower bound is equivalent to the reinforcement learning procedure used in RAM and is related to the work of Maes et al. (2009) who showed how reinforcement learning can be used to tackle general structured prediction problems.", "startOffset": 35, "endOffset": 533}, {"referenceID": 13, "context": "We can reduce the variance in the estimator 10 by replacing the log p(y|l\u0303, I,W ) with a 0/1 discrete indicator function R and using a baseline technique used in Mnih et al. (2014).", "startOffset": 162, "endOffset": 181}, {"referenceID": 13, "context": "In fact, by using the 0/1 indicator function, the learning rule from equation 13 is equivalent to the REINFORCE (Williams, 1992) learning rule employed in Mnih et al. (2014) for training their attention model.", "startOffset": 155, "endOffset": 174}, {"referenceID": 14, "context": "We then apply the proposed attention model to a real-world object recognition task using the multi-digit SVHN dataset Netzer et al. (2011) and compare with the state-of-the-art deep ConvNets.", "startOffset": 118, "endOffset": 139}, {"referenceID": 13, "context": "RAM Mnih et al. (2014) 9% DRAM w/o context 7% DRAM 5% Table 2: Error rates on the MNIST two digit addition task.", "startOffset": 4, "endOffset": 23}, {"referenceID": 13, "context": "As suggested in Mnih et al. (2014), classification performance can be improved by having a glimpse network with two different scales.", "startOffset": 16, "endOffset": 35}, {"referenceID": 3, "context": "11 layer CNN Goodfellow et al. (2013) 3.", "startOffset": 13, "endOffset": 38}, {"referenceID": 13, "context": "The publicly available multi-digit street view house number (SVHN) dataset Netzer et al. (2011) consists of images of digits taken from pictures of house fronts.", "startOffset": 75, "endOffset": 96}, {"referenceID": 3, "context": "Following Goodfellow et al. (2013), we formed a validation set of 5000 images by randomly sampling images from the training set and the extra set, and these were used for selecting the learning rate and sampling variance for the stochastic glimpse policy.", "startOffset": 10, "endOffset": 35}, {"referenceID": 3, "context": "Following Goodfellow et al. (2013), we formed a validation set of 5000 images by randomly sampling images from the training set and the extra set, and these were used for selecting the learning rate and sampling variance for the stochastic glimpse policy. The models are trained using the remaining 200,000 training images. We follow the preprocessing technique from Goodfellow et al. (2013) to generate tightly cropped 64 x 64 images with multi-digits at the center and similar data augmentation is used to create 54x54 jittered images during training.", "startOffset": 10, "endOffset": 392}, {"referenceID": 3, "context": "Following Goodfellow et al. (2013), we formed a validation set of 5000 images by randomly sampling images from the training set and the extra set, and these were used for selecting the learning rate and sampling variance for the stochastic glimpse policy. The models are trained using the remaining 200,000 training images. We follow the preprocessing technique from Goodfellow et al. (2013) to generate tightly cropped 64 x 64 images with multi-digits at the center and similar data augmentation is used to create 54x54 jittered images during training. We also convert the RGB images to grayscale as we observe the color information does not affect the final classification performance. We trained a model to classify all the digits in an image sequentially with the objective function defined in equation 15. The label sequence ordering is chosen to go from left to right as the natural ordering of the house number. The attention model is given 3 glimpses for each digit before making a prediction. The recurrent model keeps running until it predicts a terminal label or until the longest digit length in the dataset is reached. In the SVHN dataset, up to 5 digits can appear in an image. This means the recurrent model will run up to 18 glimpses per image, that is 5 x 3 plus 3 glimpses for a terminal label. Learning the attention model took around 3 days on a GPU. The model performance is shown in table (4.3). We found that there is still a performance gap between the state-of-the-art deep ConvNet and a single DRAM that \u201creads\u201d from left to right, even with the Monte Carlo averaging. The DRAM often over predicts additional digits in the place of the terminal class. In addition, the distribution of the leading digit in real-life follows Benford\u2019s law. We therefore train a second recurrent attention model to \u201cread\u201d the house numbers from right to left as a backward model. The forward and backward model can share the same weights for their glimpse networks but they have different weights for their recurrent and their emission networks. The predictions of both forward and backward models can be combined to estimate the final sequence prediction. Following the observation that attention models often overestimate the sequence length, we can flip first k number of sequence prediction from the backwards model, where k is the shorter length of the sequence length prediction between the forward and backward model. This simple heuristic works very well in practice and we obtain state-of-the-art performance on the Street View house number dataset with the forward-backward recurrent attention model. Videos showing sample runs of the forward and backward models on SVHN test data can be found at http://www.psi.toronto.edu/ \u0303jimmy/dram/forward.avi and http://www.psi.toronto.edu/ \u0303jimmy/dram/backward.avi respectively. These visualizations show that the attention model learns to follow the slope of multi-digit house numbers when they go up or down. For comparison, we also implemented a deep ConvNet with a similar architecture to the one used in Goodfellow et al. (2013). The network had 8 convolutional layers with 128 filters in each followed by 2 fully connected layers of 3096 ReLU units.", "startOffset": 10, "endOffset": 3092}], "year": 2014, "abstractText": "We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.", "creator": "LaTeX with hyperref package"}}}