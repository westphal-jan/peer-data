{"id": "1412.8293", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2014", "title": "Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels", "abstract": "We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large datasets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead, where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem.", "histories": [["v1", "Mon, 29 Dec 2014 10:00:39 GMT  (2218kb)", "http://arxiv.org/abs/1412.8293v1", "A short version of this paper has been presented in ICML 2014"], ["v2", "Sun, 9 Aug 2015 07:20:00 GMT  (1935kb)", "http://arxiv.org/abs/1412.8293v2", "A short version of this paper has been presented in ICML 2014"]], "COMMENTS": "A short version of this paper has been presented in ICML 2014", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.NA stat.CO", "authors": ["jiyan yang", "vikas sindhwani", "haim avron", "michael w mahoney"], "accepted": true, "id": "1412.8293"}, "pdf": {"name": "1412.8293.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jiyan Yang", "Vikas Sindhwani", "Haim Avron", "Michael W. Mahoney"], "emails": ["jiyan@stanford.edu", "vsindhw@us.ibm.com", "haimav@us.ibm.com", "mmahoney@stat.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.82 93v1 [st at.M L] 29 Dec 201 4"}, {"heading": "1 Introduction", "text": "In this year it has come to the point that it is a kind of infinite vastness in which the greater people are able to decide whether they are able to change the world, or whether they are able to change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change, change."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "We use i both for subscripts and for the term \u221a \u2212 1, relying on context to distinguish between the two. We use y, z,.. to denote scalars. We use w, t, x. to denote vectors, and wi to denote the i-th coordinate of vectors w. Furthermore, in a sequence of vectors we use wi to denote the i-th element of the sequence, and we use wij to denote the j-th coordinate of the vector wi. Given x1,.., xn, the gram matrix matrix that produces K matrix is defined, where Kj = k (xi, xj) for i, j = 1,., n. We denounce the error function by erf (\u00b7), i.e., erf (z) = f matrix matrix is the definition of kernel (z-z2dz for z-C; see Weideman 1983 and Mori [more details]."}, {"heading": "2.2 Related Work", "text": "In this context, we must also mention the fact that the epidemics that have emerged in the United States in recent years and that have been getting older are a way in which people are able to put themselves and themselves at the centre. (...) In this sense, it is also necessary for people in the United States and in other parts of the world to be able to identify themselves. (...) In this sense, it is also necessary for people to be able to identify themselves. \"(...) In this sense, it is necessary for people in the United States and in other parts of the world to be able to identify themselves.\""}, {"heading": "2.3 Quasi-Monte Carlo Techniques: an Overview", "text": "In this section, we provide a self-contained overview of the quasi-Monte Carlo (QMC) techniques (QMC). Briefly, we will limit our discussion to the background necessary to understand the following sections. We refer to the excellent reviews by Caflisch [1998] and Dick et al. [2013], and the current book Leobacher und Pillichschammer [2014] for a much more detailed exposure. Consider the task of calculating an approximation of the following integral Id [f] = 0.1] d f (x) dx. (4) One can observe that x is a random vector uniformly distributed over [0, 1] d, then Id [f] = f (x). An empirical approximation of the expected value can be calculated by drawing a random point S = (w1)., ws) independent of [0, 1] d, and computing."}, {"heading": "3 QMC Feature Maps: Our Algorithm", "text": "We assume that the density function in (2) can be written as p (x) = p (d) = 1 pj (xj), where pj (\u00b7) is a univariate density function. Density functions associated with many shift-invariant cores, e.g. Gauss, Laplacian, and Cauchy, allow such a form. The QMC method is generally applicable to integrals via a unit cube. Typically, integrals of the form (2) are handled by first generating a low discrepancy sequence t1,..., ts [0, 1] d, and converting them into a sequence w1,. \u2212 ws to Rd instead of drawing the elements of the sequence of p (\u00b7) as in the MC method. To convert (2) into an integral diagram over the unit cube, a simple modification of the variables is sufficient."}, {"heading": "4 Theoretical Analysis and Average Case Error Bounds", "text": "The aim of this section is to develop a framework for analyzing the approximate quality of the QMC function boards described in the previous section (algorithm 1). We need to develop such a framework because the classic Koksma-Hlawka function cannot be applied to our setting, since the following position shows: Proposition 5 (x) = 1 pj (xj), where pj () is a univariate density function, Latvian \u2212 1 (t) = (t) = (t1),., p (d), p (D) = 1 pj (t) = e \u2212 iu Ti \u2212 1 (t), t (t). The Hardy-Krause variation of fu (\u00b7) is one of the integrals in the sum (6)."}, {"heading": "Discrepancy of Monte-Carlo Sequences.", "text": "We now derive an expression for the expected discrepancy of the Monte Carlo sequences and show that it decays as O (s \u2212 1 / 2). This is useful because we are guaranteed by an averaging argument that there are propositions for which the discrepancy O (s \u2212 1 / 2) behaves. Episode 14. Suppose t1,.., ts would be uniformly from [0, 1] d. Suppose wi = 1 (ti), for i = 1,.., s. Suppose it is from [0, 1] d. E [Ie, p (S) 2] = 1s."}, {"heading": "5 Learning Adaptive QMC Sequences", "text": "For the sake of simplicity, we assume in this section that p (\u00b7) is the density function of the q distribution with the mean zero. We also omit the subscript p from D p. Similar analyses and equations can be derived for other density functions. Error characterization using discrepancy measures such as (12) is typically used in the QMC literature to prescribe sequences whose discrepancy behaves favorably. It is clear that caste discrepancy requires a meticulous design for a high-quality sequence, and we leave this to future work. Instead, in this paper we use the fact that caste discrepancy is a smooth function of the sequence with a formula in closed form, allowing us to evaluate both different candidate sequences and select those with the lowest discrepancy, as well as learn a QMC discrepancy (4)."}, {"heading": "Global Adaptive Sequences.", "text": "The task is to minimize the box discrepancy function (12) over the space of the sequences of s vectors in Rd: S \u0443 = argminS = (w1... ws) \u0435Rds D (S).The gradient can be inserted into any first-order numerical solver for nonconvex optimization. We use a nonlinear conjugate gradient in our experiments (Section 6.2).The above learning mechanism can be extended in different directions. For example, QMC sequences for n-point-rank-one-lattice rules [Dick et al., 2013] are integral fractions of a grid defined by a single generator vector. This generating vector can be learned by local minimization of the box discrepancy."}, {"heading": "Greedy Adaptive Sequences.", "text": "Starting with S0 = \u2205, for t \u2265 1, leave St = {w1,.., wt}. At step t + 1, we solve the following optimization problem, wt + 1 = argminw \u0442 Rd D (St- {w}). (15) Set St + 1 = St-wt + 1) and repeat the above procedure. The gradient of the above target is also given in (14). Again, we use nonlinear conjugate gradients in our experiments (Section 6.2). The greedy adaptive procedure is closely related to the herding algorithm recently presented by Welling [2009]. Applying the herding algorithm to PWb and p (\u00b7), and using our notation, the points w1, w2,... are generated using the following iteration wt + 1, arg max w-Rd < zt (\u00b7), h-value."}, {"heading": "6 Experiments", "text": "In this section we report on experiments with both classic QMC sequences and adaptive sequences, which we learned from minimizing discrepancies between the speakers."}, {"heading": "6.1 Experiments With Classical QMC Sequences", "text": "We look at four sequences: Halton, Sobol ', Lattice Rules and Digital Nets. For Halton and Sobol', we use the implementation available in MATLAB3. For Lattice Rules and Digital Nets, we use scrambling and shifting techniques recommended in the QMC literature (see Dick et al. [2013] for details). For Sobol ', Lattice Rules and Digital Nets, scrambling results in randomization and sequence deviation. For Halton sequence, scrambling and shifting techniques recommended in the QMC literature (see Dick et al. [2013] for details. For Sobol', Lattice Rules and Digital Nets, scrambling results in randomization and sequence deviation. For Halton sequence, scrambling is deterministic, and there are no deviations. The generation of these sequences is extremely fast and fairly negligible if we use the time for a reasonable deviation."}, {"heading": "Quality of Kernel Approximation", "text": "rE \"s for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\""}, {"heading": "Behavior of Box Discrepancy", "text": "Next, we examine whether D is predictable for the quality of the approximation. We calculate the normalized deviations of the square box (i.e., \u03c0d = 1 bj) \u2212 1D (S) 2) for the different sequences with different sampling processes. Note that while the boundary field b is set on the basis of observed ranges of the characteristics in the dataset, the actual distribution of points X occurring within this box may be anything but uniform. In the diagrams, we see a strong correlation between the quality of the approximation and the deviations. In Figure 4, we see for cpu, if we limit the range of s to [0, 800] which is also the range of s forcpu in Figure 2, we can see a strong correlation between the quality of the approximation and the deviations. Interestingly, the deviations of the lattice rule sequences begin with a small discrepancy, but do not decrease with the approximation."}, {"heading": "6.2 Experiments With Adaptive QMC", "text": "We show that QMC sequences can be improved to achieve a better approximation to the gram matrix, and this can sometimes lead to an improved generalization error. Note that the runtime of learning adaptive sequences is less relevant in our experimental environment for the following reasons: Given the values of s, d, b and \u03c3, the optimization of a sequence has to be performed only once. There is some flexibility in these parameters: d can be adapted to the input by adding zero features or by PCA; you can use longer or shorter sequences; and the data can be forced to a particular sequence by (possibly unequal) scaling of the characteristics. (This in turn affects the choice of characteristics) Since the design of adaptive QMC sequences is data-independent and can be applied to a variety of downstream core methods."}, {"heading": "Integral Approximation", "text": "We begin by examining the integration error across the unit square using three different sequences, MC, Halton and adaptive QMC sequences. The integral consists of the form \u0442 [0,1] 2 e \u2212 iuT tdt, where the unit square is spanned. The error is illustrated in Figure 5. We see that MC sequences concentrate most of the error reduction near the origin. The halton sequence results in a significant improvement in the range of low integration errors. Adaptive QMC sequences result in a further improvement in the size of the integration error, which is now distributed across the entire unit square; the estimation of such sequences is \"aware\" of the complete integration region. In fact, adaptive sequences can be focused in a specified subfield by controlling the box size (see Figure b / 2), which may be helpful in generalizing if the actual data distribution is better represented by this subfield."}, {"heading": "Quality of Kernel Approximation", "text": "In Figure 6 and Figure 7, we examine how different metrics (discrepancy, maximum squared error, error norm) are developed on the gram matrix approximation during the optimization process for both adaptive sequences. Since learning adaptive sequences on datasets with low dimensional properties is more affordable, the experiment is performed on two such datasets, cpu and housing. This supports our hypothesis that performance evolves as the number of iterations. In Figure 6 (a), we examine the behavior on cpu. We see that all metrics go down as the iteration progresses."}, {"heading": "Generalization Error", "text": "We use the same two learning algorithms to learn adaptive sequences as the previous subsection and use them to approximate kernel burr regression; the burr parameter is set by the near-optimal value for both sequences in a 5x transverse validation on the training set. Table 2 summarizes the results.For CPU, adaptive sequences can yield lower test errors if the sampling size is small (since the test error is already low, about 3%, such an improvement in accuracy is not trivial).Greedy approach seems to show slightly better results.If s = 500 or even larger (not reported here), the performance of the sequences is very close. For census purposes, the adaptive sequences show no benefit up to s 1200. After that, we can see at least one of the two adaptive sequences, which yields significantly lower errors than Halton sequences for each sampling size. However, in some cases, adaptive sequences generate errors greater than the unoptimized part, before the whole sequence is adjusted to the central one."}, {"heading": "7 Conclusion and Future Work", "text": "Recent work on applying core methods to very large datasets has shown that they are capable of achieving state-of-the-art accuracies that sometimes coincide with those of Deep Neural Networks (DNN) [Huang et al., 2014] The key to these results is the ability to apply core methods to such datasets. An in-depth study of these empirical results shows that in order to achieve state-of-the-art accuracies, a very large number of random characteristics were required. For example, on TIMIT, a classical speech recognition dataset, over 200,000 random characteristics were used to compare DNN performance [Huang et al., 2014]. It is clear that improving the efficiency of random characteristics has a significant impact on our ability to extend core methodologies to potentially higher levels of accuracy."}, {"heading": "Acknowledgements", "text": "The authors thank Josef Dick for useful references to the literature to improve the QMC sequences; Ha Quang Minh for several discussions about Paley-Wiener rooms and the RKHS theory; the anonymous ICML reviewer for pointing out the link to herding and other helpful comments. This research was supported by the XDATA program of the Defense Advanced Research Projects Agency (DARPA), conducted under the Air Force Research Laboratory contract FA8750-12-C-0323, while J. Yang was a summer intern at IBM Research."}, {"heading": "A Technical Details", "text": "In this section we provide detailed evidence for the allegations in sections 4 and 5."}, {"heading": "A.1 Proof of Proposition 5", "text": "Remember that with every T-word, every T-word, every T-word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word, every word."}, {"heading": "A.2 Proof of Proposition 6", "text": "To see this, we need the following lemmas we have: # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "A.3 Proof of Theorem 9", "text": "We apply (9) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "A.4 Proof of Theorem 12", "text": "Let b > 0 be a scalar, and let u = \u00b2 (b) and i = \u00b2 (b). We have (b) and iux sin (b (x). (u / 2b) = e \u2212 iuz. In the formula above, rect is the function that is 1 on [\u2212 1 / 2, 1 / 2] and zero on [\u2212 2] and elsewhere. The last equality implies that for each u \u00b2 b and each x \u00b2 Rd we havefu (x) = x \u00b2 Rdfu (y, x) dy. That is, the reproducing property holds on fu (\u00b7) even if fu / i \u00b2 PWb.We now have for each u \u00b2 b and each x \u00b2 Rd \u00b2 S, x \u00b2 S = x \u00b2 Rdj \u00b2 S."}, {"heading": "A.5 Proof of Corollary 13", "text": "In this case p (x) = 1 pj (xj) = 1 pj (xj), where pj (\u00b7) is the density function of N (0, 1 / \u03c3j). The characteristic function associated with pj (\u00b7) is j (\u03b2) = e \u2212 \u03b2 2 2\u03c32 j. We apply (11) directly. In the first semester, since the second semester, we have \u2212 j (\u03b2) | 2d\u03b2 \u2212 j (\u03b2) \u2212 \u03b22d\u03b2 = d [2] j [2] j [2] y \u2212 y [3] 2 erf [4] (bj)."}, {"heading": "A.6 Proof of Proposition 16", "text": "Before calculating the derivative, we prove two auxiliary lemmas. Lemma 19. Let x-Rd be a variable and z-Rd a fixed vector. (21) We omit the proof, because it is a simple calculation, which is based on the definition of sincb.Lemma 20. The derivative of the scalar function f (x) = Re [e \u2212 ax 2 erf (c + idx) e (f), for real scalars a, c \u2212 \u2212 \u2212 x = \u2212 2axe \u2212 ax2 Re [erf (c + idx)] + 2d \u00b2 (f) x \u00b2 (f) x \u00b2 (c).Proof."}, {"heading": "A.7 Proof of Corollary 14", "text": "The proof is similar to the proof of Theorem 3.6 by Dick et al. [2013]. Note that since the Supx round h (x, x) < p = p (x, x) p (x) p (x) p (x) p (y) p (x) p (x) p (x) p (x) p (x) p (y) p (x) p (x) p (y) p (x) p (x) p (y) p (x) p (y) dxdy < p (9), haveDh, p (S) 2 = p) p (p) p (p) p) p (p) p) p (p) p) p (p) p (x) p) p (l) p (l) p (l) p) p (l) p (lemp) p (l) p (l) p (l) p (p) p) p."}], "references": [{"title": "Subspace embeddings for the polynomial kernel", "author": ["H. Avron", "H.L. Nguy \u0303\u0302en", "D.P. Woodruff"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Avron et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "Sharp analysis of low-rank kernel matrix approximations", "author": ["F. Bach"], "venue": "In Proceedings of the 26th Conference on Learning Theory (COLT),", "citeRegEx": "Bach.,? \\Q2013\\E", "shortCiteRegEx": "Bach.", "year": 2013}, {"title": "On the equivalence between herding and conditional gradient algorithms", "author": ["F. Bach", "S. Lacoste-Julien", "G. Obozinski"], "venue": "In Proceeding of the 29th International Conference in Machine Learning (ICML),", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Reproducing Kernel Hilbert Spaces in Probability and Statistics", "author": ["A. Berlinet", "C. Thomas-Agnan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Berlinet and Thomas.Agnan.,? \\Q2004\\E", "shortCiteRegEx": "Berlinet and Thomas.Agnan.", "year": 2004}, {"title": "Monotone funktionen, Stieltjes integrale und harmonische analyse", "author": ["S. Bochner"], "venue": "Math. Ann.,", "citeRegEx": "Bochner.,? \\Q1933\\E", "shortCiteRegEx": "Bochner.", "year": 1933}, {"title": "Hilbert space embeddings of predictive state representations", "author": ["B. Boots", "A. Gretton", "G.J. Gordon"], "venue": "In Conference Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Boots et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2013}, {"title": "Large-scale Kernel Machines", "author": ["L. Bottou", "O. Chapelle", "D. DeCoste", "J. Weston (Editors"], "venue": null, "citeRegEx": "Bottou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2007}, {"title": "Monte Carlo and Quasi-Monte Carlo methods", "author": ["R.E. Caflisch"], "venue": "Acta Numerica, 7:1\u201349,", "citeRegEx": "Caflisch.,? \\Q1998\\E", "shortCiteRegEx": "Caflisch.", "year": 1998}, {"title": "Super-samples from kernel herding", "author": ["Y. Chen", "M. Welling", "A. Smola"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "On the mathematical foundations of learning", "author": ["F. Cucker", "S. Smale"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "Cucker and Smale.,? \\Q2001\\E", "shortCiteRegEx": "Cucker and Smale.", "year": 2001}, {"title": "High-dimensional integration: The Quasi-Monte Carlo way", "author": ["J. Dick", "F.Y. Kuo", "I.H. Sloan"], "venue": "Acta Numerica, 22:133\u2013288,", "citeRegEx": "Dick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dick et al\\.", "year": 2013}, {"title": "Fast Randomized Kernel Methods With Statistical Guarantees", "author": ["A. El Alaoui", "M.W. Mahoney"], "venue": "ArXiv e-prints,", "citeRegEx": "Alaoui and Mahoney.,? \\Q2014\\E", "shortCiteRegEx": "Alaoui and Mahoney.", "year": 2014}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "In Proc. of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Gittens and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Gittens and Mahoney.", "year": 2013}, {"title": "Compact random feature maps", "author": ["R. Hamid", "A. Gittens", "Y. Xiao", "D. DeCoste"], "venue": "In Proc. of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Hamid et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hamid et al\\.", "year": 2014}, {"title": "Kernel-based methods for hypothesis testing: A unified view", "author": ["Z. Harchaoui", "F. Bach", "O. Cappe", "E. Moulines"], "venue": "Signal Processing Magazine,", "citeRegEx": "Harchaoui et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Harchaoui et al\\.", "year": 2013}, {"title": "Kernel methods match Deep Neural Networks on TIMIT", "author": ["P. Huang", "H. Avron", "T. Sainath", "V. Sindhwani", "B. Ramabhadran"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Random feature maps for dot product kernels", "author": ["P. Kar", "H. Karnick"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12),", "citeRegEx": "Kar and Karnick.,? \\Q2012\\E", "shortCiteRegEx": "Kar and Karnick.", "year": 2012}, {"title": "Building support vector machines with reduced classifier complexity", "author": ["S.S. Keerthi", "O. Chapelle", "D. DeCoste"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Keerthi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Keerthi et al\\.", "year": 2006}, {"title": "Fastfood \u2013 Approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "In Proc. of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Introduction to Quasi-Monte Carlo Integration and Applications", "author": ["G. Leobacher", "F. Pillichschammer"], "venue": null, "citeRegEx": "Leobacher and Pillichschammer.,? \\Q2014\\E", "shortCiteRegEx": "Leobacher and Pillichschammer.", "year": 2014}, {"title": "Random Fourier approximations for skewed multiplicative histogram kernels", "author": ["F. Li", "C. Ionescu", "C. Sminchisescu"], "venue": "Pattern Recognition,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Max-margin additive classifiers for detection", "author": ["S. Maji", "A.C. Berg"], "venue": "In IEEE 12th International Conference on Computer Vision (ICCV),", "citeRegEx": "Maji and Berg.,? \\Q2009\\E", "shortCiteRegEx": "Maji and Berg.", "year": 2009}, {"title": "A method for evaluation of the error function of real and complex variable with high relative accuracy", "author": ["M. Mori"], "venue": "Publ. RIMS, Kyoto Univ.,", "citeRegEx": "Mori.,? \\Q1983\\E", "shortCiteRegEx": "Mori.", "year": 1983}, {"title": "Random number generation and Quasi-Monte Carlo methods", "author": ["H. Niederreiter"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Niederreiter.,? \\Q1992\\E", "shortCiteRegEx": "Niederreiter.", "year": 1992}, {"title": "Statistical inference on time series by RKHS methods", "author": ["E. Parzen"], "venue": "In Proceedings of 12th Biennial Seminar Canadian Mathematical Congress on Time Series and Stochastic Processes: convexity and combinatorics,", "citeRegEx": "Parzen.,? \\Q1970\\E", "shortCiteRegEx": "Parzen.", "year": 1970}, {"title": "Classical spaces of holomorphic functions", "author": ["M.M. Peloso"], "venue": "Technical report, Universit\u2018 di Milano,", "citeRegEx": "Peloso.,? \\Q2011\\E", "shortCiteRegEx": "Peloso.", "year": 2011}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["N. Pham", "R. Pagh"], "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Pham and Pagh.,? \\Q2013\\E", "shortCiteRegEx": "Pham and Pagh.", "year": 2013}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Positive definite functions on spheres", "author": ["I.J. Schoenberg"], "venue": "Duke Mathematical Journal, 9(1):96\u2013108,", "citeRegEx": "Schoenberg.,? \\Q1942\\E", "shortCiteRegEx": "Schoenberg.", "year": 1942}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond", "author": ["B. Sch\u00f6lkopf", "A. Smola", "editors"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2002}, {"title": "High-performance kernel machines with implicit distributed optimization and randomization", "author": ["V. Sindhwani", "H. Avron"], "venue": "In JSM Proceedings, Tradeoffs in Big Data Modeling - Section on Statistical Computing,", "citeRegEx": "Sindhwani and Avron.,? \\Q2014\\E", "shortCiteRegEx": "Sindhwani and Avron.", "year": 2014}, {"title": "When are Quasi-Monte Carlo algorithms efficient for high dimensional integrals", "author": ["I.H. Sloan", "H. Wozniakowski"], "venue": "Journal of Complexity,", "citeRegEx": "Sloan and Wozniakowski.,? \\Q1998\\E", "shortCiteRegEx": "Sloan and Wozniakowski.", "year": 1998}, {"title": "Scholkopf. A hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song"], "venue": "In Algorithmic Learning Theory (ALT),", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Hilbert space embeddings of Hidden Markov Models", "author": ["L. Song", "B. Boots", "S. Siddiqi", "G. Gordon", "A. Smola"], "venue": "In Proceedings of the 30th International Conference in Machine Learning (ICML),", "citeRegEx": "Song et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Song et al\\.", "year": 2013}, {"title": "Generalized RBF feature maps for efficient detection", "author": ["V. Sreekanth", "A. Vedaldi", "C.V. Jawahar", "A. Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Sreekanth et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sreekanth et al\\.", "year": 2010}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Scholkopf", "G. Lanckriet"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "Breaking intractability", "author": ["J.F. Traub", "H. Wozniakowski"], "venue": "Scientific American,", "citeRegEx": "Traub and Wozniakowski.,? \\Q1994\\E", "shortCiteRegEx": "Traub and Wozniakowski.", "year": 1994}, {"title": "Core vector machines: Fast svm training on very large data sets", "author": ["I.W. Tsang", "J.T. Kwok", "P. Cheung"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Tsang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsang et al\\.", "year": 2005}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "Pattern Analysis and Machine Intellingence,", "citeRegEx": "Vedaldi and Zisserman.,? \\Q2011\\E", "shortCiteRegEx": "Vedaldi and Zisserman.", "year": 2011}, {"title": "Spline Models for Observational Data. Society for Industrial and Applied Mathematics, Philadelphia", "author": ["G. Wahba", "editor"], "venue": "PA, USA,", "citeRegEx": "Wahba and editor.,? \\Q1990\\E", "shortCiteRegEx": "Wahba and editor.", "year": 1990}, {"title": "Computation of the complex error function", "author": ["J.A.C. Weideman"], "venue": "SIAM Journal of Numerical Analysis,", "citeRegEx": "Weideman.,? \\Q1994\\E", "shortCiteRegEx": "Weideman.", "year": 1994}, {"title": "Herding dynamical weights to learn", "author": ["M. Welling"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Welling.,? \\Q2009\\E", "shortCiteRegEx": "Welling.", "year": 2009}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Average case complexity of multivariate integration", "author": ["H. Wozniakowski"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "Wozniakowski.,? \\Q1991\\E", "shortCiteRegEx": "Wozniakowski.", "year": 1991}, {"title": "Random Laplace feature maps for semigroup kernels on histograms", "author": ["J. Yang", "V. Sindhwani", "Q. Fan", "H. Avron", "M. Mahoney"], "venue": "In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Applications of Reproducing Kernel Hilbert Spaces - bandlimited signal models", "author": ["K. Yao"], "venue": "Inform. Control,", "citeRegEx": "Yao.,? \\Q1967\\E", "shortCiteRegEx": "Yao.", "year": 1967}, {"title": "Scholkopf. Kernel based conditional independence test and application in causal discovery", "author": ["K. Zhang", "J. Peters", "D. Janzing"], "venue": "In Confernece on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "These include nonlinear classification, regression, clustering, semi-supervised learning [Belkin et al., 2006], time-series analysis [Parzen, 1970], sequence modeling [Song et al.", "startOffset": 89, "endOffset": 110}, {"referenceID": 25, "context": ", 2006], time-series analysis [Parzen, 1970], sequence modeling [Song et al.", "startOffset": 30, "endOffset": 44}, {"referenceID": 34, "context": ", 2006], time-series analysis [Parzen, 1970], sequence modeling [Song et al., 2013], dynamical systems [Boots et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 6, "context": ", 2013], dynamical systems [Boots et al., 2013], hypothesis testing [Harchaoui et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 15, "context": ", 2013], hypothesis testing [Harchaoui et al., 2013], causal modeling [Zhang et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 47, "context": ", 2013], causal modeling [Zhang et al., 2011] and many more.", "startOffset": 25, "endOffset": 45}, {"referenceID": 26, "context": "In this paper, we revisit one of the most successful techniques, namely the randomized construction of a family of low-dimensional approximate feature maps proposed by Rahimi and Recht [2007]. These randomized feature maps, \u03a8\u0302 : X \u2192 Cs, provide low-distortion approximations for (complex-valued) kernel functions k : X \u00d7 X \u2192 C: k(x, z) \u2248 \u3008\u03a8\u0302(x), \u03a8\u0302(z)\u3009Cs (1) where Cs denotes the space of s-dimensional complex numbers with the inner product, \u3008\u03b1, \u03b2\u3009Cs = \u2211s i=1 \u03b1i\u03b2 \u2217 i , with z\u2217 denoting the conjugate of the complex number z (though Rahimi and Recht [2007] also define realvalued feature maps for real-valued kernels, our technical exposition is simplified by adopting the generality of complex-valued features).", "startOffset": 168, "endOffset": 192}, {"referenceID": 26, "context": "In this paper, we revisit one of the most successful techniques, namely the randomized construction of a family of low-dimensional approximate feature maps proposed by Rahimi and Recht [2007]. These randomized feature maps, \u03a8\u0302 : X \u2192 Cs, provide low-distortion approximations for (complex-valued) kernel functions k : X \u00d7 X \u2192 C: k(x, z) \u2248 \u3008\u03a8\u0302(x), \u03a8\u0302(z)\u3009Cs (1) where Cs denotes the space of s-dimensional complex numbers with the inner product, \u3008\u03b1, \u03b2\u3009Cs = \u2211s i=1 \u03b1i\u03b2 \u2217 i , with z\u2217 denoting the conjugate of the complex number z (though Rahimi and Recht [2007] also define realvalued feature maps for real-valued kernels, our technical exposition is simplified by adopting the generality of complex-valued features).", "startOffset": 168, "endOffset": 558}, {"referenceID": 15, "context": "This technique is immensely successful, and has been used in recent years to obtain state-of-the-art accuracies for some classical datasets [Huang et al., 2014, Sindhwani and Avron, 2014]. The starting point of Rahimi and Recht [2007] is a celebrated result that characterizes the class of positive definite functions: Definition 1.", "startOffset": 141, "endOffset": 235}, {"referenceID": 5, "context": "Theorem 2 (Bochner [1933]).", "startOffset": 11, "endOffset": 26}, {"referenceID": 28, "context": "Finally, empirical results (Section 6) clearly demonstrate the superiority of QMC techniques over the MC feature maps [Rahimi and Recht, 2007], the correctness of our theoretical analysis and the potential value of adaptive QMC techniques for large-scale kernel methods.", "startOffset": 118, "endOffset": 142}, {"referenceID": 16, "context": "This is motivated by recent work that showed that in order to obtain state-of-the-art accuracy on some important datasets, a very large number of random features is needed [Huang et al., 2014, Sindhwani and Avron, 2014]. Our point of departure from the work of Rahimi and Recht [2007] is the simple observation that when w1, .", "startOffset": 173, "endOffset": 285}, {"referenceID": 43, "context": "One dominant line of work constructs low-rank approximations of the Gram matrix, either using data-oblivious randomized feature maps to approximate the kernel function, or using sampling techniques such as the classical Nystr\u00f6m method [Williams and Seeger, 2001].", "startOffset": 235, "endOffset": 262}, {"referenceID": 30, "context": ", erf(z) = \u222b z 0 e \u2212z2dz for z \u2208 C; see Weideman [1994] and Mori [1983] for more details.", "startOffset": 40, "endOffset": 56}, {"referenceID": 18, "context": ", erf(z) = \u222b z 0 e \u2212z2dz for z \u2208 C; see Weideman [1994] and Mori [1983] for more details.", "startOffset": 60, "endOffset": 72}, {"referenceID": 12, "context": "More elaborate techniques exist, both randomized and deterministic; see Gittens and Mahoney [2013] for a thorough treatment.", "startOffset": 72, "endOffset": 99}, {"referenceID": 12, "context": "More elaborate techniques exist, both randomized and deterministic; see Gittens and Mahoney [2013] for a thorough treatment. More relevant to our work is the randomized feature mapping approach. Pioneered by the seminal paper of Rahimi and Recht [2007], the core idea is to construct, for a given kernel on a data domain X , a transformation \u03a8\u0302 : X \u2192 Cs such that k(x, z) \u2248 \u3008\u03a8\u0302(x), \u03a8\u0302(z)\u3009Cs .", "startOffset": 72, "endOffset": 253}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels.", "startOffset": 9, "endOffset": 537}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi).", "startOffset": 9, "endOffset": 654}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels.", "startOffset": 9, "endOffset": 766}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels. Sreekanth et al. [2010] developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(\u00b7) is a positive definite function, and D(\u00b7, \u00b7) is a distance metric.", "startOffset": 9, "endOffset": 840}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels. Sreekanth et al. [2010] developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(\u00b7) is a positive definite function, and D(\u00b7, \u00b7) is a distance metric. Kar and Karnick [2012] suggested feature maps for dot-product kernels.", "startOffset": 9, "endOffset": 1014}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels. Sreekanth et al. [2010] developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(\u00b7) is a positive definite function, and D(\u00b7, \u00b7) is a distance metric. Kar and Karnick [2012] suggested feature maps for dot-product kernels. The feature maps is based on the Maclaurin expansion, which is guaranteed to be non-negative due to a classical result of Schoenberg [1942]. Pham and Pagh [2013] suggested feature maps for the polynomial kernels.", "startOffset": 9, "endOffset": 1202}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels. Sreekanth et al. [2010] developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(\u00b7) is a positive definite function, and D(\u00b7, \u00b7) is a distance metric. Kar and Karnick [2012] suggested feature maps for dot-product kernels. The feature maps is based on the Maclaurin expansion, which is guaranteed to be non-negative due to a classical result of Schoenberg [1942]. Pham and Pagh [2013] suggested feature maps for the polynomial kernels.", "startOffset": 9, "endOffset": 1224}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014].", "startOffset": 151, "endOffset": 171}, {"referenceID": 38, "context": "Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem.", "startOffset": 56, "endOffset": 76}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms.", "startOffset": 152, "endOffset": 299}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, .", "startOffset": 152, "endOffset": 518}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, .", "startOffset": 152, "endOffset": 581}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small.", "startOffset": 152, "endOffset": 841}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function.", "startOffset": 152, "endOffset": 1683}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function. Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem. For a broader discussion of these methods, and others, see Bottou et al. [2007]. 2.", "startOffset": 152, "endOffset": 2093}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function. Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem. For a broader discussion of these methods, and others, see Bottou et al. [2007]. 2.3 Quasi-Monte Carlo Techniques: an Overview In this section we provide a self-contained overview of Quasi-Monte Carlo (QMC) techniques. For brevity, we restrict our discussion to background that is necessary for understanding subsequent sections. We refer the interested reader to the excellent reviews by Caflisch [1998] and Dick et al.", "startOffset": 152, "endOffset": 2418}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function. Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem. For a broader discussion of these methods, and others, see Bottou et al. [2007]. 2.3 Quasi-Monte Carlo Techniques: an Overview In this section we provide a self-contained overview of Quasi-Monte Carlo (QMC) techniques. For brevity, we restrict our discussion to background that is necessary for understanding subsequent sections. We refer the interested reader to the excellent reviews by Caflisch [1998] and Dick et al. [2013], and the recent book Leobacher and Pillichschammer [2014] for a much more detailed exposition.", "startOffset": 152, "endOffset": 2441}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function. Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem. For a broader discussion of these methods, and others, see Bottou et al. [2007]. 2.3 Quasi-Monte Carlo Techniques: an Overview In this section we provide a self-contained overview of Quasi-Monte Carlo (QMC) techniques. For brevity, we restrict our discussion to background that is necessary for understanding subsequent sections. We refer the interested reader to the excellent reviews by Caflisch [1998] and Dick et al. [2013], and the recent book Leobacher and Pillichschammer [2014] for a much more detailed exposition.", "startOffset": 152, "endOffset": 2499}, {"referenceID": 24, "context": ",ws}, the integration error is bounded above as follows, \u01ebS [f ] \u2264 D(S)VHK [f ] , where VHK is the Hardy-Krause variation of f (see Niederreiter [1992]), which is defined in terms of the following partial derivatives, VHK [f ] = \u2211", "startOffset": 132, "endOffset": 152}, {"referenceID": 11, "context": "However we mention that in addition to the Halton sequences, other notable members are Sobol\u2019 sequences, Faure sequences, Niederreiter sequences, and more (see Dick et al. [2013], Section 2).", "startOffset": 160, "endOffset": 179}, {"referenceID": 11, "context": "However we mention that in addition to the Halton sequences, other notable members are Sobol\u2019 sequences, Faure sequences, Niederreiter sequences, and more (see Dick et al. [2013], Section 2). We also mention that it is conjectured that the O((log s)d/s) rate for star discrepancy decay is optimal. The classical QMC theory, which is based on the Koksma-Hlawka inequality and low discrepancy sequences, thus achieves a convergence rate of O((log s)d/s). While this is asymptotically superior to O(s\u22121/2) for a fixed d, it requires s to be exponential in d for the improvement to manifest. As such, in the past QMC methods were dismissed as unsuitable for very high-dimensional integration. However, several authors noticed that QMC methods perform better than MC even for very highdimensional integration [Sloan and Wozniakowski, 1998, Dick et al., 2013]2. Contemporary QMC literature explains and expands on these empirical observations, by leveraging the structure of the space in which the integrand function lives, to derive more refined bounds and discrepancy measures, even when classical measures of variation such as (6) are unbounded. This literature has evolved along at least two directions: one, where worst-case analysis is provided under the assumption that the integrands live in a Reproducing Kernel Hilbert Space (RKHS) of sufficiently smooth and well-behaved functions (see Dick et al. [2013], Section 3) and second, where the analysis is done in terms of average-case error, under an assumed probability distribution over the integrands, instead of worst-case error [Wozniakowski, 1991, Traub and Wozniakowski, 1994].", "startOffset": 160, "endOffset": 1410}, {"referenceID": 42, "context": "The latter is closely connected to \u201cherding\u201d algorithms [Welling, 2009].", "startOffset": 56, "endOffset": 71}, {"referenceID": 11, "context": "For example, QMC sequences for n-point rank-one Lattice Rules [Dick et al., 2013] are integral fractions of a lattice defined by a single generating vector v.", "startOffset": 62, "endOffset": 81}, {"referenceID": 8, "context": "For example, QMC sequences for n-point rank-one Lattice Rules [Dick et al., 2013] are integral fractions of a lattice defined by a single generating vector v. This generating vector may be learnt via local minimization of the box discrepancy. Greedy Adaptive Sequences. Starting with S0 = \u2205, for t \u2265 1, let St = {w1, . . . ,wt}. At step t+1, we solve the following optimization problem, wt+1 = argminw\u2208Rd D (St \u222a {w}) . (15) Set St+1 = St \u222a {wt+1} and repeat the above procedure. The gradient of the above objective is also given in (14). Again, we use non-linear conjugate gradient in our experiments (Section 6.2). The greedy adaptive procedure is closely related to the herding algorithm, recently presented by Welling [2009]. Applying the herding algorithm to PWb and p(\u00b7), and using our notation, the points w1,w2, .", "startOffset": 63, "endOffset": 729}, {"referenceID": 7, "context": "Chen et al. [2010] showed that under some additional assumptions, the herding algorithm, when applied to a RKHS H, greedily minimizes \u2016\u03bch,p \u2212 \u03bc\u0302h,p,St\u20162H, which, recall, is equal to Dh,p(St).", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Chen et al. [2010] showed that under some additional assumptions, the herding algorithm, when applied to a RKHS H, greedily minimizes \u2016\u03bch,p \u2212 \u03bc\u0302h,p,St\u20162H, which, recall, is equal to Dh,p(St). Thus, under certain assumptions, herding and (15) are equivalent. Chen et al. [2010] also showed that under certain restrictions on the RKHS, herding will reduce the discrepancy in a ratio of O(1/t).", "startOffset": 0, "endOffset": 277}, {"referenceID": 1, "context": "Indeed, Bach et al. [2012] recently shown that these restrictions never hold for infinite-dimensional RKHS, as long as the domain is compact.", "startOffset": 8, "endOffset": 27}, {"referenceID": 11, "context": "For the low-discrepancy sequence, we use scrambling and shifting techniques recommended in the QMC literature (see Dick et al. [2013] for details).", "startOffset": 115, "endOffset": 134}, {"referenceID": 1, "context": "Worth mentioning in this regard, is the recent work by Bach [2013], which analyses the connection between Nystr\u00f6m approximations of the Gram matrix, and the regression error, and the work of El Alaoui and Mahoney [2014] on kernel methods with statistical guarantees.", "startOffset": 55, "endOffset": 67}, {"referenceID": 1, "context": "Worth mentioning in this regard, is the recent work by Bach [2013], which analyses the connection between Nystr\u00f6m approximations of the Gram matrix, and the regression error, and the work of El Alaoui and Mahoney [2014] on kernel methods with statistical guarantees.", "startOffset": 55, "endOffset": 220}, {"referenceID": 16, "context": "7 Conclusion and Future Work Recent work on applying kernel methods to very large datasets, has shown their ability to achieve stateof-the-art accuracies that sometimes match those attained by Deep Neural Networks (DNN) [Huang et al., 2014].", "startOffset": 220, "endOffset": 240}, {"referenceID": 31, "context": "The random features approach, originally due to Rahimi and Recht [2007], as emerged as a key technology for scaling up kernel methods [Sindhwani and Avron, 2014].", "startOffset": 134, "endOffset": 161}, {"referenceID": 16, "context": "For example, on TIMIT, a classical speech recognition dataset, over 200,000 random features were used in order to match DNN performance [Huang et al., 2014].", "startOffset": 136, "endOffset": 156}, {"referenceID": 16, "context": "7 Conclusion and Future Work Recent work on applying kernel methods to very large datasets, has shown their ability to achieve stateof-the-art accuracies that sometimes match those attained by Deep Neural Networks (DNN) [Huang et al., 2014]. Key to these results is the ability to apply kernel method to such datasets. The random features approach, originally due to Rahimi and Recht [2007], as emerged as a key technology for scaling up kernel methods [Sindhwani and Avron, 2014].", "startOffset": 221, "endOffset": 391}, {"referenceID": 11, "context": "6 of Dick et al. [2013]. Notice that since supx\u2208Rd h(x,x) < \u221e, we have \u222b Rd h(x,x)p(x)dx < \u221e.", "startOffset": 5, "endOffset": 24}], "year": 2017, "abstractText": "We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large datasets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead, where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem.", "creator": "LaTeX with hyperref package"}}}