{"id": "1501.00777", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2015", "title": "Sparse Deep Stacking Network for Image Classification", "abstract": "Sparse coding can learn good robust representation to noise and model more higher-order representation for image classification. However, the inference algorithm is computationally expensive even though the supervised signals are used to learn compact and discriminative dictionaries in sparse coding techniques. Luckily, a simplified neural network module (SNNM) has been proposed to directly learn the discriminative dictionaries for avoiding the expensive inference. But the SNNM module ignores the sparse representations. Therefore, we propose a sparse SNNM module by adding the mixed-norm regularization (l1/l2 norm). The sparse SNNM modules are further stacked to build a sparse deep stacking network (S-DSN). In the experiments, we evaluate S-DSN with four databases, including Extended YaleB, AR, 15 scene and Caltech101. Experimental results show that our model outperforms related classification methods with only a linear classifier. It is worth noting that we reach 98.8% recognition accuracy on 15 scene.", "histories": [["v1", "Mon, 5 Jan 2015 08:07:31 GMT  (224kb,D)", "http://arxiv.org/abs/1501.00777v1", "8 pages, 3 figures, AAAI-2015"]], "COMMENTS": "8 pages, 3 figures, AAAI-2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jun li 0027", "heyou chang", "jian yang 0003"], "accepted": true, "id": "1501.00777"}, "pdf": {"name": "1501.00777.pdf", "metadata": {"source": "CRF", "title": "Sparse Deep Stacking Network for Image Classification", "authors": ["Jun Li", "Heyou Chang", "Jian Yang"], "emails": [], "sections": [{"heading": "Introduction", "text": "In fact, it is a matter of a pure structure in which one is able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "Deep Stacking Network", "text": "The DSN architecture is originally presented in the literature (Deng and Yu 2011b). Deng and Yu explore an original strategy for building deep networks based on stacking layers of the basic SNNM modules that take the simplified form of multi-layered perception. We describe mathematically as follows: Let the target vectors ti = [t1i, \u00b7 \u00b7, tji, \u00b7 \u00b7 \u00b7, tCi] T be arranged to form the columns of T = [t1, \u00b7, \u00b7, tN]. Let the input data vectors xi = [x1i, \u00b7, \u00b7, \u00b7, \u00b7, \u00b7, \u00b7, \u00b7, \u00b7, \u00b7 form the output of T."}, {"heading": "Sparse Deep Stacking Network", "text": "The S-DSN is a sparse case of the DSN. Stacking operation of the S-DSN is the same as that of the DSN as described in Deng and Yu 2011b. The general paradigm is to use the output vector of the lower module and the original input vector to form the advanced \"input\" vector for the higher module of the DSN. The modular architecture of the S-DSN differs from that of the DSN. We consider the sigmoid function and the ReLU function; and the sparse penalties are added to the hidden units of the modular architecture."}, {"heading": "Sparse Module", "text": "The output of the top layer is Y = UT H, and the hidden layer output is as follows: H = \u03c6 (WT X), RL \u00b7 N (4), where \u03c6 (a) is the sigmoid activation function \u03c3 (a) or the ReLU activation function max (0, a). For the sake of simplicity, letH = {1, 2, \u00b7, L} is called the set of all hidden units. H is divided into G groups, where G is the number of groups. The gth group is called Gg, where H = G g = 1 Gg and Gg = 1 Gg = 1 Gg =. Thus, H groups are called [H-G1,: \u00b7 \u00b7 \u00b7; H-Gg; H-GG,:]. The parameters U and W are learned to minimize the least square units."}, {"heading": "Learning Weights- Algorithm", "text": "Once the lower weight matrix W is set, the lower weight matrix W (1) and the lower weight matrix W (2) are also set. (2) Then, the upper weight matrix U (2) is formulated as a convex optimization problem. (2) There are two algorithms for learning the lower weight matrix W. First, the upper weight matrix W (9) can be optimized using a closed form of algorithms. (3) The upper weight matrix W (9) can be optimized. (4) The upper weight matrix U (9) can be optimized using a gradient descent algorithm. (4) The lower weight matrix W (9) is used. (4)."}, {"heading": "The S-DSN Architecture", "text": "The SNNM replacement module described in the subsection above is used to construct the K-layer S-DSN architecture, where K is the number of layers. In the box replacement module, we denote input by Xk, hidden representations by H-DSN, output by Yk, labeling matrix by T, and weight matrix by Wk and Uk. Specified input data X and labeling T, if k = 1, X1 = X. Then, the general paradigm of S-DSN can be broken down into three phases: \u2022 Step 1: Train the kth sparse module to minimize the smallest square error between Yk and T. \u2022 Step 2: Generate the input Xk + 1 of the k + 1. Spark module by adding the output Yk of the kth sparse module. \u2022 Step 3: Iterate as in Step 1 and Step 2 to combine the S-DSN architecture by plugging the DSN into the optimization structure."}, {"heading": "Experiments", "text": "We present experimental results on four databases: the expanded YaleB database, the AR face database, Caltech101 and 15 scene categories. \u2022 Extended YaleB database: This database contains 2,414 frontal face images of 38 people. There are approximately 64 images for each person. The original images were cropped and normalized to 192 x 168 pixels. \u2022 The database consists of over 4,000 color images of 126 people. Each person has 26 face images taken during two sessions. These images include additional facial variations, including different expressions and different facial \"disguises\" (sunglasses and scarves). Following the standardized evaluation process, we use a subset of the database, which consists of 2,600 male subjects and 50 female subjects. Each face image has also been cropped and standardized to 165 x 120 pixels. \u2022 Caltech-101: This database contains 9144 images, which belong to 101 classes, with approximately 40 to 800 images per class."}, {"heading": "Sparseness Comparisons", "text": "Before presenting the classification results, we will first show how sparse S-DSN (sigm) and S-DSN (relu) are compared to DSN. To find out how sparse representations learned from S-DSN (sigm), S-DSN (relu) and DSN have good characteristics, located in the interval [0, 1] and on a normalized scale. Their value closer to 1 means that there are more zero components in the vector. We will perform comparisons to extended YaleB and AR databases, and the results will be in Table 1. The sparse results show that S-DSN (sigm) and S-DSN (relu) have a higher sparseness and detection accuracy. Table 1 compares the HSM network of the S-DSN (sigm) and the S-DSN (relu) image. We note that the sparse DSN layers of two DSN layers (DSN-105-relus) are average."}, {"heading": "Results", "text": "Face Recognition Extended YaleB = 1, G = 4, and \u03b2 = 0.01 are relativized in DSN. We randomly select half (32) of the images per category for training and the other half for testing. Parameters are selected as follows: in DSN1they can be from: http: / / www.umiacs.umd.edu / \u0445 zhuolin / projectlcksvd.html = 0.1 and \u03b1 = 0.5; in S-DSN (sigm) = 0.1, \u03b1 = 0.5, G = 2, and \u03b2-DSN = 0.01, \u03b1 = 2, DSDSDS2 = 2, and \u03b2 0.01. AR: For each person we randomly select 20 images for training and the other 6 for testing. In our experiments, = 0.1 and \u03b1 = 0.5, we are used in DSN; = 0.1, \u03b1 DSD = 0.5, and \u03b2 = 4, and \u03b2 = 0.001 are used in SDSN."}, {"heading": "Conclusion", "text": "In this paper, we present an improved DSN model, S-DSN, for image classification. S-DSN is built by stacking many sparse SNNM modules. In each sparse SNNM module, the lower layer weights and upper layer weights are solved by convex optimization and the grade-dissecting algorithm. We use the S-DSN to further extract the sparse representations from random facial features and spatial pyramid features for image classification. Experimental results show that S-DSN delivers very good classification results in four public databases with only one linear classifier."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the National Science Fund for Distinguished Young Scholars under grant numbers 61125305, 61472187, 61233011 and 61373063, the Chinese Ministry of Education key project under grant numbers 313030, the 973 Program No. 2014CB349303, Fundamental Research Funds for the Central Universities No. 30920140121005 and the Program for Changjiang Scholars and Innovative Research Team at University No. IRT13072."}], "references": [{"title": "Group sparse coding", "author": ["Bengio"], "venue": "In Proceedings of the Neural Info. Processing Systems,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Accelerated parallelizable neural network learning algorithm for speech recognition", "author": ["Deng", "L. Yu 2011a] Deng", "D. Yu"], "venue": "In Proceedings of the Interspeech,", "citeRegEx": "Deng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2011}, {"title": "Deep convex networks: A scalable architecture for speech pattern classification", "author": ["Deng", "L. Yu 2011b] Deng", "D. Yu"], "venue": "In Proceedings of the Interspeech,", "citeRegEx": "Deng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2011}, {"title": "Deep learning for signal and information processing. Foundations and Trends in Signal Processing 2-3:197\u2013387", "author": ["Deng", "L. Yu 2013] Deng", "D. Yu"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Deep stacking networks for information retrieval", "author": ["He Deng", "L. Gao 2013] Deng", "X. He", "J. Gao"], "venue": "In Proceedings of IEEE Conf. on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Scalable stacking and learning for building deep architectures", "author": ["Yu Deng", "L. Platt 2012] Deng", "D. Yu", "J. Platt"], "venue": "In Proceedings of IEEE Conf. on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Deng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2012}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue"], "venue": "In Proceedings of the Int\u2019l Conf. Machine Learning,", "citeRegEx": "Donahue,? \\Q2014\\E", "shortCiteRegEx": "Donahue", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "X. Bengio 2010] Glorot", "Y. Bengio"], "venue": "In Proceedings of the Int\u2019l Conf. Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Bordes Glorot", "X. Bengio 2011] Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the Int\u2019l Conf. Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Learning fast approximations of sparse coding", "author": ["Gregor", "K. LeCun 2010] Gregor", "Y. LeCun"], "venue": "In Proceedings of the Int\u2019l Conf. Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2010}, {"title": "Unsupervised feature learning by deep sparse coding", "author": ["He"], "venue": "In Proceedings of SIAM Int\u2019l Conf. on Data Mining,", "citeRegEx": "He,? \\Q2014\\E", "shortCiteRegEx": "He", "year": 2014}, {"title": "Supervised and projected sparse coding for image classification", "author": ["Huang"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Huang,? \\Q2013\\E", "shortCiteRegEx": "Huang", "year": 2013}, {"title": "Tensor deep stacking networks", "author": ["Deng Hutchinson", "B. Yu 2013] Hutchinson", "L. Deng", "D. Yu"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "Hutchinson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2013}, {"title": "Locality-constrained low-rank coding for image classification", "author": ["Guo Jiang", "Z. Peng 2014] Jiang", "P. Guo", "L. Peng"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Jiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2014}, {"title": "Label consistent k-svd learning a discriminative dictionary for recognition", "author": ["Lin Jiang", "Z. Davis 2013] Jiang", "Z. Lin", "L. Davis"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "Jiang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2013}, {"title": "Efficient sparse coding algorithms", "author": ["Lee"], "venue": "In Proceedings of the Neural Info. Processing Systems,", "citeRegEx": "Lee,? \\Q2007\\E", "shortCiteRegEx": "Lee", "year": 2007}, {"title": "Sparse deep belief net model for visual area v2", "author": ["Ekanadham Lee", "H. Ng 2008] Lee", "C. Ekanadham", "A. Ng"], "venue": "In Proceedings of the Neural Info. Processing Systems,", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Latent semantic representation learning for scene classification", "author": ["Li", "X. Guo 2014] Li", "Y. Guo"], "venue": "In Proceedings of the Int\u2019l Conf. Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Learning important spatial pooling regions for scene classification", "author": ["Lin"], "venue": "In Proceedings of the IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "Lin,? \\Q2014\\E", "shortCiteRegEx": "Lin", "year": 2014}, {"title": "Sparse group restricted boltzmann machines", "author": ["Luo"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Luo,? \\Q2011\\E", "shortCiteRegEx": "Luo", "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "V. Hinton 2010] Nair", "G. Hinton"], "venue": "In Proceedings of the Int\u2019l Conf. Machine Learning,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Information-theoretic dictionary learning for image classification", "author": ["Patel Qiu", "Q. Chellappa 2014] Qiu", "V. Patel", "R. Chellappa"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Ranzato"], "venue": "In Proceedings of the Neural Info. Processing Systems,", "citeRegEx": "Ranzato,? \\Q2007\\E", "shortCiteRegEx": "Ranzato", "year": 2007}, {"title": "Sparse feature learning for deep belief networks", "author": ["Boureau Ranzato", "M. LeCun 2008] Ranzato", "Y. Boureau", "Y. LeCun"], "venue": "In Proceedings of the Neural Info. Processing Systems,", "citeRegEx": "Ranzato et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2008}, {"title": "Locality-constrained linear coding for image classification", "author": ["Wang"], "venue": "In Proceedings of the IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "Wang,? \\Q2010\\E", "shortCiteRegEx": "Wang", "year": 2010}, {"title": "Robust face recognition via sparse representation", "author": ["Wright"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "Wright,? \\Q2009\\E", "shortCiteRegEx": "Wright", "year": 2009}, {"title": "Linear spatial pyramid matching using sparse coding for image", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2009\\E", "shortCiteRegEx": "Yang", "year": 2009}, {"title": "Beyond sparsity: The role of l1-optimizer in pattern classification", "author": ["Yang"], "venue": "Pattern Recognition", "citeRegEx": "Yang,? \\Q2012\\E", "shortCiteRegEx": "Yang", "year": 2012}, {"title": "Low-rank sparse coding for image classification", "author": ["Zhang"], "venue": "In Proceedings of the IEEE Int\u2019l Conf. Computer Vision,", "citeRegEx": "Zhang,? \\Q2013\\E", "shortCiteRegEx": "Zhang", "year": 2013}, {"title": "Supervised coupled dictionary learning with group structures for multi-modal retrieval", "author": ["Zhuang"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhuang,? \\Q2013\\E", "shortCiteRegEx": "Zhuang", "year": 2013}, {"title": "Learning discriminative and shareable features for scene classification", "author": ["Zuo"], "venue": "In Proceedings of the Euro. Conf. Computer Vision,", "citeRegEx": "Zuo,? \\Q2014\\E", "shortCiteRegEx": "Zuo", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Sparse coding can learn good robust representation to noise and model more higher-order representation for image classification. However, the inference algorithm is computationally expensive even though the supervised signals are used to learn compact and discriminative dictionaries in sparse coding techniques. Luckily, a simplified neural network module (SNNM) has been proposed to directly learn the discriminative dictionaries for avoiding the expensive inference. But the SNNM module ignores the sparse representations. Therefore, we propose a sparse SNNM module by adding the mixed-norm regularization (l1/l2 norm). The sparse SNNM modules are further stacked to build a sparse deep stacking network (S-DSN). In the experiments, we evaluate S-DSN with four databases, including Extended YaleB, AR, 15 scene and Caltech101. Experimental results show that our model outperforms related classification methods with only a linear classifier. It is worth noting that we reach 98.8% recognition accuracy on 15 scene.", "creator": "LaTeX with hyperref package"}}}