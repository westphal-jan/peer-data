{"id": "1501.02598", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2015", "title": "Combining Language and Vision with a Multimodal Skip-gram Model", "abstract": "We extend the effective SKIP-GRAM model of Mikolov et al. (2013) by taking visual information into account. Like S KIP - GRAM , our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMS KIP - GRAM models achieve excellent performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we also use them to improve image labeling and retrieval in the challenging zero-shot setup, where the test concepts are not seen in training. Finally, the MMS KIP - GRAM models discover intriguing vision-related properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.", "histories": [["v1", "Mon, 12 Jan 2015 10:48:32 GMT  (2994kb,D)", "https://arxiv.org/abs/1501.02598v1", "10 pages"], ["v2", "Tue, 13 Jan 2015 09:37:08 GMT  (2994kb,D)", "http://arxiv.org/abs/1501.02598v2", "10 pages"], ["v3", "Thu, 12 Mar 2015 09:47:33 GMT  (2537kb,D)", "http://arxiv.org/abs/1501.02598v3", "accepted at NAACL 2015, camera ready version, 11 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG", "authors": ["angeliki lazaridou", "nghia the pham", "marco baroni"], "accepted": true, "id": "1501.02598"}, "pdf": {"name": "1501.02598.pdf", "metadata": {"source": "CRF", "title": "Combining Language and Vision with a Multimodal Skip-gram Model", "authors": ["Angeliki Lazaridou"], "emails": ["angeliki.lazaridou@unitn.it", "thenghia.pham@unitn.it", "marco.baroni@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "2 Related Work", "text": "We focus here on a few representative systems that we effectively know. (Bruni et al. (2014) suggest a simple approach to MDSM induction, where text and image-based vectors for the same words are constructed independently and then \"mixed\" by applying multimodal image resolution to their concatenation. An empirically superior model was proposed by Silberer and Lapata (2014), who use more advanced visual representations that refer to images based on high-level \"visual attributes,\" and a multimodal fusion strategy based on stacked autocoders. Kiela and Bottou instead adopt a simple constellation strategy, but achieve empirical improvements by using state-of-the-art networks to extract visual attributes, and the skip-gram model for text."}, {"heading": "3 Multimodal Skip-gram Architecture", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Skip-gram Model", "text": "Let's start with the verification of the standard SKIP GRAM model by Mikolov et al. (2013a), in the version we use. In the face of a text corpus, SKIP-GRAM aims to produce word representations that are good at predicting the context words surrounding a target word. Mathematically, it maximizes the objective function: 1T \u2211 t = 1 \u2211 \u2212 c \u2264 j \u2264 c, j 6 = 0 log p (wt + j | wt) (1), where w1, w2,..., wT are words in the training corpus, and c is the size of the window around the target word, determining the amount of context words to be predicted by the induced representation of wt \u2032. Following Mikolov et al., we implement a subsampling option that randomly discards contextual words as an inverse function of their frequency, and c \u2032 s is controlled by the hyperparameter w \u00b2 W w \u00b2 (w \u00b2 probability w w) and w \u00b2 W W (W w \u00b2 probability)."}, {"heading": "3.2 Injecting visual knowledge", "text": "We now assume that word learning takes place in a situated context where, for a subset of the target words, the corpus contexts are accompanied by a cutesat on the mat CAT + = visual representation of the concepts they designate (just like in a conversation in which a linguistic utterance is often produced in a visual scene with some of the word referents).Visual representation is also encoded in a vector (we describe how we construct it in Section 4 below).Therefore, we make the skip gram \"multimodal\" by adding a second visual term to the original linguistic lens, that is, we expand Equation 1 as follows: 1T T T-T-t = 1 (Lling (wt) + Lvision (3), where Lling (wt) is the text-based skip gram lens, where the word Lling (wt) is also the text-based skip lens that has no visual modeling."}, {"heading": "3.3 Multi-modal Skip-gram Model A", "text": "One way to force word embedding to take visual representations into account is to try to directly increase the similarity (expressed, for example, through the cosine) between linguistic and visual representations, thus aligning the dimensions of the linguistic vector with those of the visual (remember that we induce the first, while the second is fixed), and to bring the linguistic representation of a concept closer to its visual representation. We maximize the similarity through a framework commonly used in models that combine language and vision (Weston et al., 2010; Frome et al., 2013). More precisely, we formulate the visual lens Lvision (wt) as: \u2212 ig w \u00b2 Pn (0, \u03b3 \u2212 cos (uwt, vwt) + cos (uwt, vw \u2032) + cos (4), where the minus sign turns a loss into a cost distribution, is the scope to make the target word visual wector (w), max (w), \u03b3 (t \u2212 cos) + cos (4), where the minus sign turns a loss into a cost distribution."}, {"heading": "3.4 Multi-modal Skip-gram Model B", "text": "The visual goal in MMSKIP-GRAM-A has the disadvantage that it provides a direct comparison between linguistic and visual representations, which means that they must be the same size. MMSKIP-GRAM-B removes this limitation by including an additional layer that mediates between linguistic and visual representations (see Figure 1 for a sketch of MMSKIP-GRAM-B. Learning this layer corresponds to an assessment of a crossmodal mapping matrix from linguistic to visual representations, which is induced together with linguistic word embeddings. The extension is simply implemented by replacing the word uwt = Mu \u2192 vuwt with force in Equation 4, where Mu \u2192 v is the crossmodal mapping matrix to be induced."}, {"heading": "4 Experimental Setup", "text": "The parameters of all models are estimated by backpropagating errors about stochastic gradient parentage. Our text corpus is a Wikipedia 2009 dump with about 800M tokens.1 To train the multimodal models, we add visual information to 5,100 words that have an entry in ImageNet (Deng et al., 2009), occur at least 500 times in the corpus, and have a concreteness value of \u2265 0.5 according to Turney et al. (2011). On average, about 5% tokens in the text corpus are associated with a visual representation. To construct the visual representation of a word, we try out 100 images from its ImageNet entry and extract from each image a 4096-dimensional vector using the Caffe toolkit (Jia et al., 2014), together with the pre-trained confessional neural networks of Krizhevsky et al. (2012).The vector corresponds to the activation of the network 7 in the upper level."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Approximating human judgments", "text": "In fact, it is not the case that one would embark on the search for a solution, as one imagines, but on the search for solutions that put oneself in a position, and on the search for solutions that get in one's way. (...) It is not the case that one should embark on the search for solutions. (...) It is not the case that one should embark on the search for solutions, but rather on the search for solutions. (...) It is not the case that one should embark on the search for solutions. (...) It is not the case that \"it is so.\" (...) \"It is not the case that\" it is so. \"(...) It is not so\" it is. \"(...)\" it is so \"so\" so \"so.\" (so \"so). (so.\" so. \"(so). (so.\" so. \"so.\" (so)"}, {"heading": "5.2 Zero-shot image labeling and retrieval", "text": "Considering that the quantitative and qualitative results gathered so far indicate that the models disseminate visual information about words, we apply them to image captions and retrievals in the challenging environment (see section 2 above).3We will refer here to a series of images that identify / retrieve the same object, but, as our visual vectors are summarized, the tasks we model consist, more precisely, in captioning a series of images that name the same object and find the corresponding set. Setup We take as a test 25% of the 5.1K words we have visual vectors. Multimodal models are retrained without visual vectors, using the same hyperparameters as above."}, {"heading": "5.3 Abstract words", "text": "We have done it. \"(...) We have done it.\" (...) We have done it. \"(...) We have done it.\" We have done it. \"(...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...) We have done it. (...\" We have done it. \"We have done it.\" We have done it. \"We have done it. (...\" We have done it. \"We have done it.\" We have done it. \""}, {"heading": "6 Conclusion", "text": "We have introduced two multimodal enhancements to SKIPGRAM: MMSKIP-GRAM-A is trained by directly optimizing the similarity of words to their visual representations, forcing maximum interaction between the two modalities. MMSKIP-GRAM-B includes an additional intermediary layer that acts as a cross-modal mapping component. Models \"ability to integrate and disseminate visual information has resulted in word representations that work well in both semantic and visual tasks and could be used as input in systems that benefit from earlier visual knowledge (e.g., the generation of captions). Our results with abstract words suggest that the models could also be useful in tasks such as metaphor even retrieval / generation of images of abstract concepts."}, {"heading": "Acknowledgments", "text": "We would like to thank Adam Liska, Tomas Mikolov, the reviewers and the audience of NIPS 2014 Learning Semantics. We were supported by the ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES)."}], "references": [{"title": "Strudel: A distributional semantic model based on properties and types", "author": ["Baroni et al.2010] Marco Baroni", "Eduard Barbu", "Brian Murphy", "Massimo Poesio"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of ACL,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Situating abstract concepts", "author": ["Barsalou", "Katja Wiemer-Hastings"], "venue": "Grounding Cognition: The Role of Perception and Action in Memory,", "citeRegEx": "Barsalou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Barsalou et al\\.", "year": 2005}, {"title": "Distributional semantics in Technicolor", "author": ["Bruni et al.2012] Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam Khanh Tran"], "venue": "In Proceedings of ACL,", "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Vector space models of lexical meaning", "author": ["Stephen Clark"], "venue": "Handbook of Contemporary Semantics,", "citeRegEx": "Clark.,? \\Q2015\\E", "shortCiteRegEx": "Clark.", "year": 2015}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["Deng et al.2009] Jia Deng", "Wei Dong", "Richard Socher", "Lia-Ji Li", "Li Fei-Fei"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Describing objects by their attributes", "author": ["Farhadi et al.2009] Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Farhadi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2009}, {"title": "Visual information in semantic representation", "author": ["Feng", "Lapata2010] Yansong Feng", "Mirella Lapata"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Feng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2010}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Greg Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Interpretable semantic vectors from a joint model of brain-and textbased meaning", "author": ["Fyshe et al.2014] Alona Fyshe", "Partha P Talukdar", "Brian Murphy", "Tom M Mitchell"], "venue": "Proceedings of ACL,", "citeRegEx": "Fyshe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fyshe et al\\.", "year": 2014}, {"title": "Symbol grounding and meaning: A comparison of high-dimensional and embodied theories of meaning", "author": ["Glenberg", "Robertson2000] Arthur Glenberg", "David Robertson"], "venue": "Journal of Memory and Language,", "citeRegEx": "Glenberg et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Glenberg et al\\.", "year": 2000}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad.,? \\Q1990\\E", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what I mean", "author": ["Hill", "Korhonen2014] Felix Hill", "Anna Korhonen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation. http://arxiv.org/abs/arXiv:1408.3456", "author": ["Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "A model of grounded language acquisition: Sensorimotor features improve lexical and grammatical learning", "author": ["Howell et al.2005] Steve Howell", "Damian Jankowicz", "Suzanna Becker"], "venue": "Journal of Memory and Language,", "citeRegEx": "Howell et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Howell et al\\.", "year": 2005}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia et al.2014] Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Armand Joulin", "Li Fei-Fei"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["Kiela", "Bottou2014] Douwe Kiela", "L\u00e9on Bottou"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Improving multimodal representations using image dispersion: Why less is sometimes more", "author": ["Kiela et al.2014] Douwe Kiela", "Felix Hill", "Anna Korhonen", "Stephen Clark"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel"], "venue": "In Proceedings of the NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Thought", "author": ["Lakoff", "Johnson1999] George Lakoff", "Mark Johnson"], "venue": null, "citeRegEx": "Lakoff et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lakoff et al\\.", "year": 1999}, {"title": "Is this a wampimuk? crossmodal mapping between distributional semantics and the visual world", "author": ["Elia Bruni", "Marco Baroni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Mao et al.2014] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan Yuille"], "venue": "In Proceedings of the NIPS Deep Learning and Representation", "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["Oquab et al.2014] Maxime Oquab", "Leon Bottou", "Ivan Laptev", "Josef Sivic"], "venue": "In Proceedings of CVPR", "citeRegEx": "Oquab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2014}, {"title": "Minds, Brains and Science", "author": ["John Searle"], "venue": null, "citeRegEx": "Searle.,? \\Q1984\\E", "shortCiteRegEx": "Searle.", "year": 1984}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Silberer", "Lapata2014] Carina Silberer", "Mirella Lapata"], "venue": "In Proceedings of ACL,", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["Sivic", "Zisserman2003] Josef Sivic", "Andrew Zisserman"], "venue": "In Proceedings of ICCV,", "citeRegEx": "Sivic et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sivic et al\\.", "year": 2003}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Milind Ganjoo", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["Turney et al.2011] Peter Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Turney et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2011}, {"title": "Labeling images with a computer game", "author": ["von Ahn", "Dabbish2004] Luis von Ahn", "Laura Dabbish"], "venue": "In Proceedings of CHI,", "citeRegEx": "Ahn et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2004}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Weston et al.2010] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine learning,", "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 26, "context": "We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account.", "startOffset": 33, "endOffset": 56}, {"referenceID": 5, "context": "DSMs have been very effectively applied to a variety of semantic tasks (Clark, 2015; Mikolov et al., 2013b; Turney and Pantel, 2010).", "startOffset": 71, "endOffset": 132}, {"referenceID": 13, "context": "However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems (Harnad, 1990; Searle, 1984), are severely impoverished, suffering of lack of grounding in extra-linguistic modalities (Glenberg and Robertson, 2000).", "startOffset": 118, "endOffset": 146}, {"referenceID": 30, "context": "However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems (Harnad, 1990; Searle, 1984), are severely impoverished, suffering of lack of grounding in extra-linguistic modalities (Glenberg and Robertson, 2000).", "startOffset": 118, "endOffset": 146}, {"referenceID": 3, "context": "MDSMs outperform state-of-the-art text-based approaches, not only in tasks that directly require access to visual knowledge (Bruni et al., 2012), but", "startOffset": 124, "endOffset": 144}, {"referenceID": 4, "context": "also on general semantic benchmarks (Bruni et al., 2014; Silberer and Lapata, 2014).", "startOffset": 36, "endOffset": 83}, {"referenceID": 26, "context": "The models build upon the very effective skip-gram approach of Mikolov et al. (2013a), that constructs vector representations by learning, incrementally, to predict the linguistic contexts in which target words occur in a corpus.", "startOffset": 63, "endOffset": 86}, {"referenceID": 3, "context": "Bruni et al. (2014) propose a straightforward approach to MDSM induction, where text- and image-based vectors for the same words are constructed independently, and then", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "These and related systems take a twostage approach to derive multimodal spaces (unimodal induction followed by fusion), and they are only tested on concepts for which both textual and visual labeled training data are available (the pioneering model of Feng and Lapata (2010) did learn from text and images jointly using Topic Models, but was shown to be empirically weak by Bruni et al. (2014)).", "startOffset": 374, "endOffset": 394}, {"referenceID": 7, "context": "timodal model based on simple recurrent networks (Elman, 1990), focusing on grounding propagation from early-acquired concrete words to a larger vocabulary.", "startOffset": 49, "endOffset": 62}, {"referenceID": 11, "context": "brain signal vectors (Fyshe et al., 2014).", "startOffset": 21, "endOffset": 41}, {"referenceID": 10, "context": "beling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) looks at image- and text-based vectors from a different perspective.", "startOffset": 7, "endOffset": 72}, {"referenceID": 24, "context": "beling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) looks at image- and text-based vectors from a different perspective.", "startOffset": 7, "endOffset": 72}, {"referenceID": 33, "context": "beling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) looks at image- and text-based vectors from a different perspective.", "startOffset": 7, "endOffset": 72}, {"referenceID": 26, "context": "We start by reviewing the standard SKIP-GRAM model of Mikolov et al. (2013a), in the version", "startOffset": 54, "endOffset": 77}, {"referenceID": 37, "context": "We maximize similarity through a max-margin framework commonly used in models connecting language and vision (Weston et al., 2010; Frome et al., 2013).", "startOffset": 109, "endOffset": 150}, {"referenceID": 10, "context": "We maximize similarity through a max-margin framework commonly used in models connecting language and vision (Weston et al., 2010; Frome et al., 2013).", "startOffset": 109, "endOffset": 150}, {"referenceID": 6, "context": "words that have an entry in ImageNet (Deng et al., 2009), occur at least 500 times in the corpus and have concreteness score \u2265 0.", "startOffset": 37, "endOffset": 56}, {"referenceID": 17, "context": "To construct the visual representation of a word, we sample 100 pictures from its ImageNet entry, and extract a 4096-dimensional vector from each picture using the Caffe toolkit (Jia et al., 2014), together with the pre-trained convolutional neural network of Krizhevsky et al.", "startOffset": 178, "endOffset": 196}, {"referenceID": 6, "context": "words that have an entry in ImageNet (Deng et al., 2009), occur at least 500 times in the corpus and have concreteness score \u2265 0.5 according to Turney et al. (2011). On average, about 5% tokens in the text corpus are associated to a visual representation.", "startOffset": 38, "endOffset": 165}, {"referenceID": 6, "context": "words that have an entry in ImageNet (Deng et al., 2009), occur at least 500 times in the corpus and have concreteness score \u2265 0.5 according to Turney et al. (2011). On average, about 5% tokens in the text corpus are associated to a visual representation. To construct the visual representation of a word, we sample 100 pictures from its ImageNet entry, and extract a 4096-dimensional vector from each picture using the Caffe toolkit (Jia et al., 2014), together with the pre-trained convolutional neural network of Krizhevsky et al. (2012). The vector corresponds", "startOffset": 38, "endOffset": 541}, {"referenceID": 3, "context": "Specifically, we test on general relatedness (MEN, Bruni et al. (2014), 3K pairs), e.", "startOffset": 51, "endOffset": 71}, {"referenceID": 14, "context": "larity (Simlex-999, Hill et al. (2014), 1K pairs; SemSim, Silberer and Lapata (2014), 7.", "startOffset": 20, "endOffset": 39}, {"referenceID": 14, "context": "larity (Simlex-999, Hill et al. (2014), 1K pairs; SemSim, Silberer and Lapata (2014), 7.", "startOffset": 20, "endOffset": 85}, {"referenceID": 14, "context": "larity (Simlex-999, Hill et al. (2014), 1K pairs; SemSim, Silberer and Lapata (2014), 7.5K pairs), e.g., pickles are similar to onions, as well as visual similarity (VisSim, Silberer and Lapata (2014), same", "startOffset": 20, "endOffset": 201}, {"referenceID": 29, "context": "lutional neural network (Oquab et al., 2014).", "startOffset": 24, "endOffset": 44}, {"referenceID": 3, "context": "We also test the vectors that performed best in the evaluation of Bruni et al. (2014), based on textual features", "startOffset": 66, "endOffset": 86}, {"referenceID": 0, "context": "pata (2014), obtained with a stacked-autoencoders architecture run on textual features extracted from Wikipedia with the Strudel algorithm (Baroni et al., 2010) and attribute-based visual features (Farhadi et al.", "startOffset": 139, "endOffset": 160}, {"referenceID": 8, "context": ", 2010) and attribute-based visual features (Farhadi et al., 2009) extracted from ImageNet.", "startOffset": 44, "endOffset": 66}, {"referenceID": 14, "context": "41 (Hill et al., 2014).", "startOffset": 3, "endOffset": 22}, {"referenceID": 0, "context": "8), achieved by Baroni et al. (2014) with a corpus 3 larger than ours and extensive tuning.", "startOffset": 16, "endOffset": 37}, {"referenceID": 4, "context": "also look similar (Bruni et al., 2014).", "startOffset": 18, "endOffset": 38}, {"referenceID": 22, "context": "To enforce strict zero-shot conditions, we exclude from the test fold labels occurring in the LSVRC2012 set that was employed to train the CNN of Krizhevsky et al. (2012), that we use to extract visual features.", "startOffset": 146, "endOffset": 171}, {"referenceID": 19, "context": "More precisely, we focused on the set of 200 words that were sampled across the USF norms concreteness spectrum by Kiela et al. (2014) (2 words had to be excluded for technical reasons).", "startOffset": 115, "endOffset": 135}, {"referenceID": 19, "context": "Table 5: Subjects\u2019 preference for nearest visual neighbour of words in Kiela et al. (2014) vs.", "startOffset": 71, "endOffset": 91}, {"referenceID": 16, "context": "For all these cases, we can borrow what Howell et al. (2005) say about", "startOffset": 40, "endOffset": 61}, {"referenceID": 19, "context": "Recently, Kiela et al. (2014) have proposed to measure abstractness by exploiting this very same intuition.", "startOffset": 10, "endOffset": 30}, {"referenceID": 19, "context": "sure the correlation of entropy and concreteness on the 200 words in the Kiela et al. (2014) set.", "startOffset": 73, "endOffset": 93}, {"referenceID": 19, "context": "(b) Spearman \u03c1 between concreteness and various measures on the Kiela et al. (2014) set.", "startOffset": 64, "endOffset": 84}], "year": 2015, "abstractText": "We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.", "creator": "LaTeX with hyperref package"}}}