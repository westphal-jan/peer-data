{"id": "1501.02714", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2015", "title": "From Visual Attributes to Adjectives through Decompositional Distributional Semantics", "abstract": "As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown... ) attracting most attention. By building on the recent \"zeroshot learning\" approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available. Our approach relies on two key observations. First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives. Second, objects and attributes come together in pictures (the same thing is a dog and it is brown). We can thus achieve better attribute (and object) label retrieval by treating images as \"visual phrases\", and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun. Our approach performs comparably to a method exploiting manual attribute annotation, it outperforms various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition.", "histories": [["v1", "Mon, 12 Jan 2015 16:48:19 GMT  (1157kb,D)", "http://arxiv.org/abs/1501.02714v1", "14 pages"], ["v2", "Tue, 24 Mar 2015 12:32:05 GMT  (1158kb,D)", "http://arxiv.org/abs/1501.02714v2", "accepted at Transactions of the Association for Computational Linguistics (TACL), 3/2015"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["angeliki lazaridou", "georgiana dinu", "adam liska", "marco baroni"], "accepted": true, "id": "1501.02714"}, "pdf": {"name": "1501.02714.pdf", "metadata": {"source": "CRF", "title": "From visual attributes to adjectives through decompositional distributional semantics", "authors": ["Angeliki Lazaridou", "Georgiana Dinu", "Adam Liska", "Marco Baroni"], "emails": ["angeliki.lazaridou@unitn.it", "georgiana.dinu@unitn.it", "adam.liska@unitn.it", "marco.baroni@unitn.it"], "sections": [{"heading": null, "text": "As automatic image analysis progresses, interest in a richer linguistic annotation of images grows, with the attributes of objects (e.g. furry, brown...) attracting the most attention. Building on the recent approach of \"zeroshot learning\" and taking into account the linguistic nature of attributes as noun modifiers, and especially adjectives, we show that it is possible to attach attributes to descriptive adjectives to images even when there is no training data available that includes the relevant annotations. Our approach is based on two key observations: First, objects can be considered bundles of attributes that are typically expressed as adjective modifiers (a dog is something furry, brown, etc.) and therefore a function that is trained to map visual representations of objects on nominal labels, implicitly learn to adjective attributes (a dog is the same attributes and is a dog)."}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2 General experimental setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Cross-Modal Mapping", "text": "Our approach is based on cross-modal mapping of a visual semantic space V, populated with vector-based representations of images, on a linguistic (distributional semantic) space W of word vectors. Mapping is performed by first searching for a function fproj: Rd1 \u2192 Rd2 of data points (vi, wi) in which vi [Rd1] is a vector representation of an image with an object or attribute (such as dog or metallic), and wi [Rd2] is the linguistic vector representation of the corresponding word. The mapping function can then be applied to any given image vi [V] to get its projection onto linguistic space: w \u2032 i = fproj (vi) Specifically we look at two mapping methods. In the RIDGE regression approach we learn a linear function Fproj [Rd2]."}, {"heading": "2.2 Representational Spaces", "text": "We construct distribution vectors from text using the method recently proposed by Mikolov et al. (2013), in which we feed a corpus of 2.8 billion words generated by concatenating English Wikipedia, ukWaC and BNC.4. Specifically, we used the CBOW algorithm, which triggers vectors by predicting a target word based on the words surrounding it. We construct vectors of 300 dimensions taking into account a context window of 5 words on both sides of the target, using the sub-sampling option to4http: / / wacky.sslmit.unibo.it, http: / / www.natcorp.ox.ac.uk1e-05 and the negative sampling parameter on 5.5 Visual Spaces According to common practice, images are presented as bags of visual words (BoVW) and Zisserman (Sivic and Zisserman, 2003)."}, {"heading": "2.3 Evaluation Dataset", "text": "For evaluation purposes, we use the data set, which consists of images with adjective-noun phrases introduced in Russakovsky and Fei-Fei (2010) and which relate to 384 WordNet / ImageNet synsets with 25 images per synset. Images have been manually provided with 25 attribute-indicating adjectives referring to texture, color, pattern, and shape, bearing in mind the limitations that one color must cover a substantial portion of the target object, and all other attributes must relate to the object as a whole (as opposed to parts). Table 1 lists the 25 attributes, and Table 2 illustrates sample notes. 8To enhance the quality of annotation, we consider only attributes with complete annotation consensus, for a total of 8,449 coming images, with an average of 2.7 attributes per image. In addition, to make linguistic annotation more natural and to avoid naming specific objects, we will name them in general problems."}, {"heading": "3 Experiment 1: Zero-shot attribute learning", "text": "In Section 1, we showed that there is a significant correlation between paired similarities of adjectives in a language-based semantic distribution space and those of visual characteristic vectors extracted from images labeled with the corresponding attributes. In the first experiment, we tested whether this match in the structure of attribute adjectives across modalities is sufficient to successfully apply zero-shot labeling. We learned a crossmodal function from a commented data set and used it to label images from a weighting data set with attributes outside the training set. We will call this approach DIRA, for direct retrieval using attribute annotation. Note that this is the first time that zero-shot techniques are used in the attribute domain. In the present evaluation, we distinguish DIRA-RIDGE and DIRA-NCCA modal according to the crossword function used in Section 2.2 above."}, {"heading": "3.1 Cross-modal function training and evaluation", "text": "To gather enough data to train an intermodal mapping function for attributes / adjectives, we combine the publicly available datasets of Farhadi et al. (2009) and Ferrari and Zisserman (2007) with attributes and associated images from MIRFLICKR (Huiskes and Lew, 2008).9 The resulting dataset contains 72 unique attributes and 2,300 images. Each image-attribute pair represents a training point (v, wadj), where v is the vector representation of the image, and wadj is the linguistic-spatial vector of the attribute (corresponding to an adjective). No information is required about the object depicted. To further maximize the number of training points, we perform an assessment where a single attribute is removed from the image, with the intermodal mapping function repeatedly learned from the training set on all 72 attributes."}, {"heading": "3.2 Results and discussion", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "4 Experiment 2: Learning attributes from objects and visual phrases", "text": "After showing that reasonably accurate annotations of invisible attributes can be achieved with zero-shot learning when a small amount of manual annotation is available, we are now trying to test the intuition that is provisionally supported by the data in Figure 1: Because objects are bundles of attributes, attributes are implicitly learned along with objects. We are therefore trying to create attribute-defining adjective names by evaluating only commonly available object-noun data. At the same time, building on the observation shown in Figure 2 that images of objects are images of visual phrases, we are experimenting with a vector decomposition model that treats images as a composition and collectively derives adjective and noun annotations. We are comparing it to standard zero-hot learning using direct label query, as well as a number of sophisticated alternatives that evaluate gold-standard information about the objects displayed."}, {"heading": "4.1 Cross-modal function training", "text": "The training data points have form (v, wnoun), where v is the vector representation of an image marked with an object and wnoun is the linguistic spatial vector of the corresponding noun. To ensure high imaging and diversity, we use as training object designations those that occur in the CIFAR 100 data set (Krizhevsky, 2009), combined with those previously used in the work of Farhadi et al. (2009), as well as the most common nouns in our corpus for which ImageNet data exist, for a total of 750 object nouns. For each object designation, we insert a maximum of 50 images from the corresponding ImageNet synset, resulting in approximately 23,000 training points. Images containing objects from the evaluation data set are excluded, so that both adjectives and nouns fall back on the zero-shot paradigm."}, {"heading": "4.2 Object-agnostic models", "text": "The only difference to DIRA (more precisely, DIRA-RIDGE), the zero-shot approach we tested above, is that the mapping function was trained only on object nomenclature data. DEC Dinu and Baroni (2014) recently proposed a general decomposition framework that, given a distribution vector that encodes the meaning of the phrase and the syntactical structure of that phrase, breaks it down into a series of vectors that are expected to express the semantics of the words from which the phrase is composed. In our setup, we are interested in a decomposition function fDec: Rd2 \u2192 R2d2, which, given a visual vector projected onto linguistic space, and the meaning of an adjective phrase."}, {"heading": "4.3 Object-informed models", "text": "In fact, it is so that we are in a time in which we are able to change the world, in a time in which we are able to assert ourselves, in a time in which we are still not able to change the world, in which we are able to change the world, in which we are able to change the world, in which we are able to change the world, in which we are able to change the world, in which we are able to change the world, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to think it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change"}, {"heading": "4.4 Results", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "5 Using DEC for attribute-based object classification", "text": "As discussed in the introduction, attributes can be effectively used for attribute-based object classification. In this section, we show that classifiers trained on attribute representations created with DEC - which does not require attribute annotated training data or training of a battery of attribute classifiers - perform better (and complement) standard-monitored object classifiers based on BoVW characteristics. We use a subset of Pascal VOC 2008 datasets. 14 In particular, according to Farhadi et al. (2009), we use the original VOC training set for training / validation, and the VOC validation set for testing. One-vs-all linear SVM classifiers are trained on all VOC objects, using 3 alternative image representations. First, we train directly on BoVW characteristics (PHOW, see Section 2.2), as in the classical pipeline of objects."}, {"heading": "5.1 Results", "text": "The confusion matrices in Figure 5 show that PHOW and DEC not only differ in quantitative performance, but also make different kinds of errors, some of which point to the different modalities that the two models tap into. PHOW, for example, tends to confuse cats with sofas, probably because the former often lie on monitors15 Given that the resulting representations are very dense, we save them by putting all adjective dimensions with Cosine below the global mean on zeros. DEC, on the other hand, tends to confuse chairs with television monitors, partly misguided by the taxonomic information encoded in language (both are pieces of furniture). In fact, the combined FUSED approach exceeds both representations by a large margin (35.81%), confirming that the linguistically enriched data for the DEC system is complemented to a direct extent (FUSED approach exceeds both representations by a multiple)."}, {"heading": "6 Conclusion", "text": "We extended zero-shot captioning beyond objects and showed that it is possible to add attribute-defining adjectives to images that were not seen during training. In some attributes, performance was comparable to that of attribute-monitored classifiers. We also showed that attributes are implicitly evoked when learning to map visual vectors of objects to their linguistic realizations as nouns, and that improvements in both attribute and noun are achieved by treating images as visual phrases whose linguistic representations need to be broken down into a coherent sequence of words. The resulting model outperformed a number of strong rivals. While the performance of the zero-shot decompositional approach to adjective-noun phrases alone may still be too low for practical applications, we showed that this model can already produce attribute-based representations that significantly improve the performance of a monitored object task when combined with standard identification."}], "references": [{"title": "Automatic attribute discovery and characterization from noisy Web data", "author": ["Tamara Berg", "Alexander Berg", "Jonathan Shih."], "venue": "ECCV, pages 663\u2013676, Crete, Greece.", "citeRegEx": "Berg et al\\.,? 2010", "shortCiteRegEx": "Berg et al\\.", "year": 2010}, {"title": "Image classification using random forests and ferns", "author": ["Anna Bosch", "Andrew Zisserman", "Xavier Munoz."], "venue": "Proceedings of ICCV, pages 1\u20138.", "citeRegEx": "Bosch et al\\.,? 2007", "shortCiteRegEx": "Bosch et al\\.", "year": 2007}, {"title": "Large language models in machine translation", "author": ["Thorsten Brants", "Ashok C Popat", "Peng Xu", "Franz J Och", "Jeffrey Dean."], "venue": "In Proceedings of EMNLP/CONLL, pages 858\u2013867.", "citeRegEx": "Brants et al\\.,? 2007", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research, 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Lia-Ji Li", "Li Fei-Fei."], "venue": "Proceedings of CVPR, pages 248\u2013255, Miami Beach, FL.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "How to make words with vectors: Phrase generation in distributional semantics", "author": ["Georgiana Dinu", "Marco Baroni."], "venue": "Proceedings of ACL, pages 624\u2013633, Baltimore, MD.", "citeRegEx": "Dinu and Baroni.,? 2014", "shortCiteRegEx": "Dinu and Baroni.", "year": 2014}, {"title": "Learning everything about anything: Weblysupervised visual concept learning", "author": ["Santosh Divvala", "Ali Farhadi", "Carlos Guestrin."], "venue": "Proceedings of CVPR.", "citeRegEx": "Divvala et al\\.,? 2014", "shortCiteRegEx": "Divvala et al\\.", "year": 2014}, {"title": "A flexible, corpus-driven model of regular and inverse selectional preferences", "author": ["Katrin Erk", "Sebastian Pad\u00f3", "Ulrike Pad\u00f3."], "venue": "Computational Linguistics, 36(4):723\u2013763.", "citeRegEx": "Erk et al\\.,? 2010", "shortCiteRegEx": "Erk et al\\.", "year": 2010}, {"title": "The Statistics of Word Cooccurrences", "author": ["Stefan Evert."], "venue": "Ph.D dissertation, Stuttgart University.", "citeRegEx": "Evert.,? 2005", "shortCiteRegEx": "Evert.", "year": 2005}, {"title": "Describing objects by their attributes", "author": ["Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth."], "venue": "Proceedings of CVPR, pages 1778\u20131785, Miami Beach, FL.", "citeRegEx": "Farhadi et al\\.,? 2009", "shortCiteRegEx": "Farhadi et al\\.", "year": 2009}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad A. Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth."], "venue": "Proceedings of ECCV, Crete, Greece.", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Learning visual attributes", "author": ["Vittorio Ferrari", "Andrew Zisserman."], "venue": "Advances in Neural Information Processing Systems, pages 433\u2013440.", "citeRegEx": "Ferrari and Zisserman.,? 2007", "shortCiteRegEx": "Ferrari and Zisserman.", "year": 2007}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R Hardoon", "Sandor Szedmak", "John ShaweTaylor."], "venue": "Neural Computation, 16(12):2639\u20132664.", "citeRegEx": "Hardoon et al\\.,? 2004", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "The Elements of Statistical Learning, 2nd edition", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman."], "venue": "Springer, New York.", "citeRegEx": "Hastie et al\\.,? 2009", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Concreteness and subjectivity as dimensions of lexical meaning", "author": ["Felix Hill", "Anna Korhonen."], "venue": "Proceedings of ACL, pages 725\u2013731, Baltimore, Maryland.", "citeRegEx": "Hill and Korhonen.,? 2014", "shortCiteRegEx": "Hill and Korhonen.", "year": 2014}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling."], "venue": "Biometrika, 28(3/4):321\u2013377.", "citeRegEx": "Hotelling.,? 1936", "shortCiteRegEx": "Hotelling.", "year": 1936}, {"title": "The mir flickr retrieval evaluation", "author": ["Mark J. Huiskes", "Michael S. Lew."], "venue": "Proceedings of MIR, New York, NY, USA.", "citeRegEx": "Huiskes and Lew.,? 2008", "shortCiteRegEx": "Huiskes and Lew.", "year": 2008}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Li Fei-Fei."], "venue": "Technical report, Stanford University.", "citeRegEx": "Karpathy et al\\.,? 2014", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proceedings of NIPS, pages 1097\u2013 1105, Lake Tahoe, Nevada.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky."], "venue": "Master\u2019s thesis.", "citeRegEx": "Krizhevsky.,? 2009", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg."], "venue": "Proceedings of CVPR, pages 1601\u20131608.", "citeRegEx": "Kulkarni et al\\.,? 2011", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling."], "venue": "Proceedings of CVPR, pages 951\u2013958.", "citeRegEx": "Lampert et al\\.,? 2009", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world", "author": ["Angeliki Lazaridou", "Elia Bruni", "Marco Baroni."], "venue": "Proceedings of ACL, pages 1403\u20131414, Baltimore, MD.", "citeRegEx": "Lazaridou et al\\.,? 2014", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["Svetlana Lazebnik", "Cordelia Schmid", "Jean Ponce."], "venue": "Proceedings of CVPR, pages 2169\u20132178, Washington, DC.", "citeRegEx": "Lazebnik et al\\.,? 2006", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2006}, {"title": "Integrating visual and linguistic information to describe properties of objects", "author": ["Calvin MacKenzie."], "venue": "Undergraduate Honors Thesis, Computer Science Department, University of Texas at Austin.", "citeRegEx": "MacKenzie.,? 2014", "shortCiteRegEx": "MacKenzie.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "http://arxiv.org/abs/ 1301.3781/.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["Margaret Mitchell", "Xufeng Han", "Jesse Dodge", "Alyssa Mensch", "Amit Goyal", "Alex Berg", "Kota Yamaguchi", "Tamara Berg", "Karl Stratos", "Hal Daum\u00e9 III."], "venue": "Proceedings of EACL, pages", "citeRegEx": "Mitchell et al\\.,? 2012", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "The Big Book of Concepts", "author": ["Gregory Murphy."], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Murphy.,? 2002", "shortCiteRegEx": "Murphy.", "year": 2002}, {"title": "From large scale image categorization to entry-level categories", "author": ["Vicente Ordonez", "Jia Deng", "Yejin Choi", "Alexander C Berg", "Tamara L Berg."], "venue": "Proceedings of ICCV, pages 1\u20138.", "citeRegEx": "Ordonez et al\\.,? 2013", "shortCiteRegEx": "Ordonez et al\\.", "year": 2013}, {"title": "The sun attribute database: Beyond categories for deeper scene understanding", "author": ["Genevieve Patterson", "Chen Xu", "Hang Su", "James Hays."], "venue": "International Journal of Computer Vision, 108(1-2):59\u201381.", "citeRegEx": "Patterson et al\\.,? 2014", "shortCiteRegEx": "Patterson et al\\.", "year": 2014}, {"title": "Large-scale syntactic language modeling with treelets", "author": ["Adam Pauls", "Dan Klein."], "venue": "Proceedings of ACL, pages 959\u2013968.", "citeRegEx": "Pauls and Klein.,? 2012", "shortCiteRegEx": "Pauls and Klein.", "year": 2012}, {"title": "Attribute learning in large-scale datasets", "author": ["Olga Russakovsky", "Li Fei-Fei."], "venue": "Proceedings of ECCV, pages 1\u201314.", "citeRegEx": "Russakovsky and Fei.Fei.,? 2010", "shortCiteRegEx": "Russakovsky and Fei.Fei.", "year": 2010}, {"title": "Recognition using visual phrases", "author": ["Mohammad Sadeghi", "Ali Farhadi."], "venue": "Proceedings of CVPR, pages 1745\u20131752, Colorado Springs, CO.", "citeRegEx": "Sadeghi and Farhadi.,? 2011", "shortCiteRegEx": "Sadeghi and Farhadi.", "year": 2011}, {"title": "Models of semantic representation with visual attributes", "author": ["Carina Silberer", "Vittorio Ferrari", "Mirella Lapata."], "venue": "Proceedings of ACL, pages 572\u2013582, Sofia, Bulgaria.", "citeRegEx": "Silberer et al\\.,? 2013", "shortCiteRegEx": "Silberer et al\\.", "year": 2013}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["Josef Sivic", "Andrew Zisserman."], "venue": "Proceedings of ICCV, pages 1470\u20131477, Nice, France.", "citeRegEx": "Sivic and Zisserman.,? 2003", "shortCiteRegEx": "Sivic and Zisserman.", "year": 2003}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Richard Socher", "Milind Ganjoo", "Christopher Manning", "Andrew Ng."], "venue": "Proceedings of NIPS, pages 935\u2013943, Lake Tahoe, Nevada.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research, 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["Peter Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen."], "venue": "Proceedings of EMNLP, pages 680\u2013690, Edinburgh, UK.", "citeRegEx": "Turney et al\\.,? 2011", "shortCiteRegEx": "Turney et al\\.", "year": 2011}, {"title": "Visualizing data using t-sne", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(2579-2605).", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Vlfeat \u2013 an open and portable library of computer vision algorithms", "author": ["Andrea Vedaldi", "Brian Fulkerson."], "venue": "Proceedings of ACM Multimedia, pages 1469\u20131472, Firenze, Italy.", "citeRegEx": "Vedaldi and Fulkerson.,? 2010", "shortCiteRegEx": "Vedaldi and Fulkerson.", "year": 2010}, {"title": "Understanding objects", "author": ["Andrea Vedaldi", "Siddarth Mahendran", "Stavros Tsogkas", "Subhransu Maji", "Ross B Girshick", "Juho Kannala", "Esa Rahtu", "Iasonas Kokkinos", "Matthew B Blaschko", "David Weiss", "Ben Taskar", "Karen Simonyan", "Naomi Saphra", "Sammy Mohamed"], "venue": null, "citeRegEx": "Vedaldi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vedaldi et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "gle words describing the depicted objects and their properties (Farhadi et al., 2009; Lampert et al., 2009) to richer expressions such as full-fledged image captions (Kulkarni et al.", "startOffset": 63, "endOffset": 107}, {"referenceID": 23, "context": "gle words describing the depicted objects and their properties (Farhadi et al., 2009; Lampert et al., 2009) to richer expressions such as full-fledged image captions (Kulkarni et al.", "startOffset": 63, "endOffset": 107}, {"referenceID": 22, "context": ", 2009) to richer expressions such as full-fledged image captions (Kulkarni et al., 2011; Mitchell et al., 2012).", "startOffset": 66, "endOffset": 112}, {"referenceID": 28, "context": ", 2009) to richer expressions such as full-fledged image captions (Kulkarni et al., 2011; Mitchell et al., 2012).", "startOffset": 66, "endOffset": 112}, {"referenceID": 9, "context": "tically richer representations of objects (Farhadi et al., 2009).", "startOffset": 42, "endOffset": 64}, {"referenceID": 23, "context": "1 Attribute-based methods achieve better generalization of object classifiers with less training data (Lampert et al., 2009), while at the same time producing semantic representations of visual concepts that more accurately model human se-", "startOffset": 102, "endOffset": 124}, {"referenceID": 5, "context": "Interestingly, Dinu and Baroni (2014) showed that the decomposition function we will adopt here can derive both adjective-noun and noun-PP phrases, suggesting that our approach could be seamlessly extended to visual attributes expressed by noun-modifying PPs.", "startOffset": 15, "endOffset": 38}, {"referenceID": 35, "context": "mantic intuition (Silberer et al., 2013).", "startOffset": 17, "endOffset": 40}, {"referenceID": 6, "context": "the basis for more accurate image search (for example in cases of visual sense disambiguation (Divvala et al., 2014), where a user disambiguates their query by searching for images of wooden cabinet as furniture and not just cabinet, which can also mean council).", "startOffset": 94, "endOffset": 116}, {"referenceID": 42, "context": "Classic attribute-centric image analysis requires, however, extensive manual and often domainspecific annotation of attributes (Vedaldi et al., 2014), or, at best, complex unsupervised imageand-text-mining procedures to learn them (Berg et al.", "startOffset": 127, "endOffset": 149}, {"referenceID": 0, "context": ", 2014), or, at best, complex unsupervised imageand-text-mining procedures to learn them (Berg et al., 2010).", "startOffset": 89, "endOffset": 108}, {"referenceID": 4, "context": "annotated datasets (Deng et al., 2009).", "startOffset": 19, "endOffset": 38}, {"referenceID": 38, "context": "The zero-shot approach relies on the possibility to extract, through distributional methods, semantically effective vector-based word representations from text corpora, on a large scale and without supervision (Turney and Pantel, 2010).", "startOffset": 210, "endOffset": 235}, {"referenceID": 31, "context": "(2010), Ferrari and Zisserman (2007) and Russakovsky and Fei-Fei (2010), containing annotations for 64, 7 and 25 attributes, respectively (this count excludes the SUN Attributes Database (Patterson et al., 2014), whose attributes characterize scenes rather than concrete objects).", "startOffset": 187, "endOffset": 211}, {"referenceID": 9, "context": "The attribute datasets we are aware of are the ones of Farhadi et al. (2010), Ferrari and Zisserman (2007) and Russakovsky and Fei-Fei (2010), containing annotations for 64, 7 and 25 attributes, respectively (this count excludes the SUN Attributes Database (Patterson et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 9, "context": "The attribute datasets we are aware of are the ones of Farhadi et al. (2010), Ferrari and Zisserman (2007) and Russakovsky and Fei-Fei (2010), containing annotations for 64, 7 and 25 attributes, respectively (this count excludes the SUN Attributes Database (Patterson et al.", "startOffset": 55, "endOffset": 107}, {"referenceID": 9, "context": "The attribute datasets we are aware of are the ones of Farhadi et al. (2010), Ferrari and Zisserman (2007) and Russakovsky and Fei-Fei (2010), containing annotations for 64, 7 and 25 attributes, respectively (this count excludes the SUN Attributes Database (Patterson et al.", "startOffset": 55, "endOffset": 142}, {"referenceID": 37, "context": "Given such paired training data, various algorithms (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014) can be used to induce a cross-modal projection of", "startOffset": 52, "endOffset": 117}, {"referenceID": 12, "context": "Given such paired training data, various algorithms (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014) can be used to induce a cross-modal projection of", "startOffset": 52, "endOffset": 117}, {"referenceID": 24, "context": "Given such paired training data, various algorithms (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014) can be used to induce a cross-modal projection of", "startOffset": 52, "endOffset": 117}, {"referenceID": 29, "context": "Inspired by linguistic and cognitive theories (Murphy, 2002) that characterize objects as attribute bundles, we hypothesize that, when we learn to project images of objects to the corresponding noun labels, we implicitly learn to", "startOffset": 46, "endOffset": 60}, {"referenceID": 34, "context": "We further observe that, as also highlighted by recent work in object recognition, any object in an image is, in a sense, a visual phrase (Sadeghi and Farhadi, 2011; Divvala et al., 2014), i.", "startOffset": 138, "endOffset": 187}, {"referenceID": 6, "context": "We further observe that, as also highlighted by recent work in object recognition, any object in an image is, in a sense, a visual phrase (Sadeghi and Farhadi, 2011; Divvala et al., 2014), i.", "startOffset": 138, "endOffset": 187}, {"referenceID": 5, "context": "Motivated by this observation, we turn to recent work in distributional semantics defining a vector decomposition framework (Dinu and Baroni, 2014) which, given a vector encoding the meaning of a phrase, aims at decoupling its constituents, producing vectors that can then be matched to a sequence of words best capturing the semantics of the phrase.", "startOffset": 124, "endOffset": 147}, {"referenceID": 17, "context": "3 Second, motivated by the success of Canonical Correlations Analysis (CCA) (Hotelling, 1936) in several vision-and-language tasks, such as image", "startOffset": 76, "endOffset": 93}, {"referenceID": 13, "context": "and caption retrieval (Gong et al., 2014; Hardoon et al., 2004; Hodosh et al., 2013), we adapt normalized Canonical Correlations Analysis (NCCA) to our setup.", "startOffset": 22, "endOffset": 84}, {"referenceID": 16, "context": "and caption retrieval (Gong et al., 2014; Hardoon et al., 2004; Hodosh et al., 2013), we adapt normalized Canonical Correlations Analysis (NCCA) to our setup.", "startOffset": 22, "endOffset": 84}, {"referenceID": 27, "context": "Linguistic Space We construct distributional vectors from text through the method recently proposed by Mikolov et al. (2013), to which we feed a cor-", "startOffset": 103, "endOffset": 125}, {"referenceID": 36, "context": "ages are represented as bags of visual words (BoVW) (Sivic and Zisserman, 2003).", "startOffset": 52, "endOffset": 79}, {"referenceID": 1, "context": "In our case, we use PHOW-color features, a variant of dense SIFT (Bosch et al., 2007), as lowlevel descriptors, and a visual vocabulary of 600 words.", "startOffset": 65, "endOffset": 85}, {"referenceID": 25, "context": "Spatial information is preserved with a twolevel spatial pyramid representation (Lazebnik et al., 2006), achieving a final dimensionality of 12,000.", "startOffset": 80, "endOffset": 103}, {"referenceID": 41, "context": "The entire pipeline is implemented using the VLFeat library (Vedaldi and Fulkerson, 2010), and its setup is identical to the basic recognition sample application there.", "startOffset": 60, "endOffset": 89}, {"referenceID": 8, "context": "7 We apply Positive Pointwise Mutual Information (Evert, 2005) to the BoVW counts, and", "startOffset": 49, "endOffset": 62}, {"referenceID": 33, "context": "ing of images annotated with adjective-noun phrases introduced in Russakovsky and Fei-Fei (2010), which pertains to 384 WordNet/ImageNet synsets with 25 images per synset.", "startOffset": 66, "endOffset": 97}, {"referenceID": 3, "context": "The parameters are tuned on the MEN word similarity dataset (Bruni et al., 2014).", "startOffset": 60, "endOffset": 80}, {"referenceID": 3, "context": "The parameters are tuned on the MEN word similarity dataset (Bruni et al., 2014). In future research, we might obtain a performance boost simply by using the more advanced visual features recently introduced by Krizhevsky et al. (2012). http://www.", "startOffset": 61, "endOffset": 236}, {"referenceID": 18, "context": "(2009) and Ferrari and Zisserman (2007) with attributes and associated images extracted from MIRFLICKR (Huiskes and Lew, 2008).", "startOffset": 103, "endOffset": 126}, {"referenceID": 9, "context": "To gather sufficient data to train a cross-modal mapping function for attributes/adjectives, we combine the publicly available datasets of Farhadi et al. (2009) and Ferrari and Zisserman (2007) with attributes and associated images extracted from MIRFLICKR (Huiskes and Lew, 2008).", "startOffset": 139, "endOffset": 161}, {"referenceID": 9, "context": "To gather sufficient data to train a cross-modal mapping function for attributes/adjectives, we combine the publicly available datasets of Farhadi et al. (2009) and Ferrari and Zisserman (2007) with attributes and associated images extracted from MIRFLICKR (Huiskes and Lew, 2008).", "startOffset": 139, "endOffset": 194}, {"referenceID": 33, "context": "9 1 DirA-Ridge Dec DirA-nCCA Russakovsky and Fei-Fei (2010)", "startOffset": 29, "endOffset": 60}, {"referenceID": 33, "context": "Figure 3: Performance of zero-shot attribute classification (as measured by AUC) compared to the supervised method of Russakovsky and Fei-Fei (2010), where available.", "startOffset": 118, "endOffset": 149}, {"referenceID": 33, "context": "Interestingly, for the last 4 attributes in this list, Russakovsky and Fei-Fei (2010) achieved", "startOffset": 55, "endOffset": 86}, {"referenceID": 33, "context": "Furthermore, Russakovsky and Fei-Fei (2010) excluded 5 attributes due to insufficient training data.", "startOffset": 13, "endOffset": 44}, {"referenceID": 21, "context": "To ensure high imageability and diversity, we use as training object labels those appearing in the CIFAR-100 dataset (Krizhevsky, 2009), combined with those previously used in the work of Farhadi et al.", "startOffset": 117, "endOffset": 135}, {"referenceID": 9, "context": "To ensure high imageability and diversity, we use as training object labels those appearing in the CIFAR-100 dataset (Krizhevsky, 2009), combined with those previously used in the work of Farhadi et al. (2009), as well as the most frequent nouns in our corpus for which ImageNet data exist, for a total of 750 objects-nouns.", "startOffset": 188, "endOffset": 210}, {"referenceID": 5, "context": "DEC Dinu and Baroni (2014) have recently proposed a general Decomposition framework that, given a distributional vector encoding a phrase meaning and the syntactic structure of that phrase, decomposes it into a set of vectors expected to express the semantics of the words that composed the phrase.", "startOffset": 4, "endOffset": 27}, {"referenceID": 5, "context": "We take fDec to be a linear function and, following Dinu and Baroni (2014), we use as training data vectors of adjective-", "startOffset": 52, "endOffset": 75}, {"referenceID": 14, "context": "The \u03bb parameter is tuned through generalized cross-validation (Hastie et al., 2009).", "startOffset": 62, "endOffset": 83}, {"referenceID": 32, "context": "LM We build a bigram Language Model by using the Berkeley LM toolkit (Pauls and Klein, 2012)11 on the one-trillion-token Google Web1T corpus12 and smooth probabilities with the \u201cStupid\u201d backoff technique (Brants et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 2, "context": "LM We build a bigram Language Model by using the Berkeley LM toolkit (Pauls and Klein, 2012)11 on the one-trillion-token Google Web1T corpus12 and smooth probabilities with the \u201cStupid\u201d backoff technique (Brants et al., 2007).", "startOffset": 204, "endOffset": 225}, {"referenceID": 26, "context": "MacKenzie (2014) recently introduced a similar model in a supervised setting, where it improved over standard attribute classifiers.", "startOffset": 0, "endOffset": 17}, {"referenceID": 7, "context": "SP The Selectional Preference model robustly captures semantic restrictions imposed by a noun on the adjectives modifying it (Erk et al., 2010).", "startOffset": 125, "endOffset": 143}, {"referenceID": 12, "context": "comparable, the results of DEC reported here are in the same range of state-of-the-art zero-shot learning models for object recognition (Frome et al., 2013).", "startOffset": 136, "endOffset": 156}, {"referenceID": 15, "context": "Following Hill and Korhonen (2014), we define the concreteness of an adjective as the average concreteness score of the nouns it modifies in our text corpus.", "startOffset": 10, "endOffset": 35}, {"referenceID": 39, "context": "Noun concreteness scores are taken, in turn, from Turney et al. (2011). For each test image and model, we obtain a concreteness score by averaging the concreteness of the top 5 adjectives that the model selected for the image.", "startOffset": 50, "endOffset": 71}, {"referenceID": 9, "context": "14 Specifically, following Farhadi et al. (2009), we use the original VOC training set for training/validation, and the VOC validation set for testing.", "startOffset": 27, "endOffset": 49}, {"referenceID": 9, "context": "(Farhadi et al., 2009), in which images are represented as distributions over attributes estimated with a set of ad hoc supervised attribute-specific classifiers.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "SVD (Bruni et al., 2014) (SVD dimensionality determined by cross-validation on the training set).", "startOffset": 4, "endOffset": 24}, {"referenceID": 9, "context": "The performance of our system is quite close to the one obtained by Farhadi et al. (2009) with ensembles of supervised attribute classifiers trained on manually annotated data (the most comparable accuracy from their Table 1 is at 34.", "startOffset": 68, "endOffset": 90}, {"referenceID": 19, "context": "More ambitiously, inspired by Karpathy et al. (2014), we would like to associate image fragments with phrases of arbitrary syntactic structures (e.", "startOffset": 30, "endOffset": 53}], "year": 2017, "abstractText": "As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown. . . ) attracting most attention. By building on the recent \u201czeroshot learning\u201d approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available. Our approach relies on two key observations. First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives. Second, objects and attributes come together in pictures (the same thing is a dog and it is brown). We can thus achieve better attribute (and object) label retrieval by treating images as \u201cvisual phrases\u201d, and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun. Our approach performs comparably to a method exploiting manual attribute annotation, it outperforms various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition.", "creator": "TeX"}}}