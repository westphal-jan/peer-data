{"id": "1501.03273", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2015", "title": "Classification with Low Rank and Missing Data", "abstract": "We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data.", "histories": [["v1", "Wed, 14 Jan 2015 08:16:50 GMT  (38kb,D)", "http://arxiv.org/abs/1501.03273v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad hazan", "roi livni", "yishay mansour"], "accepted": true, "id": "1501.03273"}, "pdf": {"name": "1501.03273.pdf", "metadata": {"source": "CRF", "title": "Classification with Low Rank and Missing Data", "authors": ["Elad Hazan", "Roi Livni", "Yishay Mansour"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The importance of dealing with the missing data is a fundamental and classic challenge in machine learning. There are many reasons why data may be missing. For example, one looks at the medical domain, some data may be missing because certain procedures have not been performed on a particular patient, other data may be missing because the patient does not want to disclose it, and even some data may be missing due to malfunctions of certain devices. Although it is definitely much better to always have complete and accurate data, this utopian need is not the reality many times. Therefore, we have to use the available data, even if some of it is wrong. Another, very different motive for missing data are recommendations. For example, a movie may have recommendations dataset user opinions on certain movies, which is the case, for example, that no user has seen all movies or even checked near them."}, {"heading": "2 Problem Setup and Main Result", "text": "The vector x represents the complete data and the sentence o represents the observed attributes. In the face of such a tuple, let us indicate by xo a vector in (R) d that (xo) i = {xi i).o elseThe task of learning a linear classifier with missing data is to return a target function via xo that competes with the best linear classifier via x. Specifically, a sequence of triplets {(i).o (i).o (i) elseThe task of learning a linear classifier with missing data is to provide a target function via xo that competes with the best linear classifier via x. Specifically, a sequence of triplets {(i).o (i).yi)} mi = 1 is iid according to any distribution D. An algorithm is provided with the sample S = (xioi, yi)."}, {"heading": "2.1 Main Result", "text": "Theorem 1 (main result). Suppose that \"an L-Lipschitz trick can actually bring about a big improvement in the margin.\" So let's do a regular distribution (see definition 1) (see definition 1). Let's give an example (see definition 2) that gets a sample (see definition 2). The algorithms run in time poly (| S |).Theorem 1 indicates an upper limit of the computational and sample complexity of learning. () A target function fS that is at least (1 \u2212) good with probability. The algorithms run in time poly (| S |).Theorem 1 indicates an upper limit of the learning and sample complexity of learning with missing data. Since the sample complexity is quasipolynomial, this has limited practical value in many situations."}, {"heading": "2.2 Regret minimization for joint subspace learning and classification", "text": "A significant technical contribution of this manuscript is the agnostic learning of a subspace coupled with a linear classifier. A subspace is represented by a projection matrix Q-Rd-d, which is Q2 = Q. Describe the following class of target functions F0 = {fw, Q: w, Q-Md-d, Q2 = Q, where fw, Q (xo) is the linear predictor defined by w over subspace as formally defined in the definition 2.In light of the above efficient kernel mapping method, we consider the following kernel-based online algorithm for classification called KARMA (Kernelized Algorithm for Risk-minimization with Missing Attributes)."}, {"heading": "3 Preliminaries and Notations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Notations", "text": "As discussed, we will consider a model in which a distribution D is fixed over Rd \u00b7 O \u00b7 Y, where O = 2d consists of all subsets of {1,.., d}. We will generally denote elements of Rd by x, w, v, u and elements of O by o. We will denote with Bd the unit sphere of Rd and with Bd (r) the sphere with the radius \u221a r.For a subset o, we will denote by Po: Rd \u2192 R | o | the projection onto the indices in o, i.e. if i1 \u2264 i2 \u2264 \u00b7 \u2264 ik are the elements of o in increasing order, then (Pox) j = xij. For a matrix A and a series of indices o, we will leave Ao, o = PoAP > o."}, {"heading": "3.2 Model Assumptions", "text": "Definition 1 (\u03bb regularity): We say that D with the associated subpsace E is regular if the following occurs with probability 1 (w.r.t the common random variables (x, o): 1. \u2022 smallpox assumption \u2264 1.2 x \u2022 E.3. ker (PoPE) = ker (PE) 4. If \u03bbo > 0 is a strictly positive single value of the matrix PoPE, then it is the assumption 1 (Low Rank Assumption). We say that D satisfies the low-level assumption with the permitted subspace E if it is with the associated subspace E for some \u03bb > 0.Note that in our setting we assume that the Pox value \u2264 1 a.s. If we have normalized the Pox value 1, then our assumption is weaker, then we assume that x is contained in a specified subspace E. Furthermore, the assumption can be verified on a sample with missing attributes. Also note that we have normalized both w and xo."}, {"heading": "4 Learning under low rank assumption and \u03bb-regularity.", "text": "Definition 2 (The class F0): We define the following class of target functionsF0 = {fw, Q: w-Bd (1), Q-Md \u00b7 d, Q2 = Q}, where fw, Q (xo) = (Pow) \u00b7 Q-o, o \u00b7 (Pox). (Here, M \u2020 denotes the pseudo-inversion of M.) In the following, Lemma states that the problem of linear learning with missing data under the assumption of low rank is reduced to the problem of learning in the class F0, in the sense that the hypothesis class F0 is no less expressive. Lemma 1. Let D be a distribution that satisfies the assumption of low rank. For each w-Rd, there is f-w, Q-F0 so that a.s: f-w, Q (xo) = w-x. In particular, Q = PE and w = P > Ew, where PE is the projection matrix on the subspace E."}, {"heading": "4.1 Approximating F0 under regularity", "text": "Next, we define a substitute class of target functions approaching the definition F0Definition 3 (The classes F\u03b3). For each \u03b3, we define the following classes F\u03b3 = {f\u03b3w, Q: w, Bd (1), Q: R d \u00b7 d, Q2 = Q}, where, f\u03b3w, Q (xo) = (Pow) \u00b7 t \u2212 1 \u00b2 j = 0 (Qo, o) j \u00b7 (Pox) Lemma 2. Let (x, o) be a sample drawn according to a \u03bb-regular distribution D with associated sub-space E. Allow Q = PE and vice versa \u2264 1 then a.s to: \u0435f\u03b3w, I \u2212 Q (xo) \u2212 fw, Q (xo) \u0432\u043e (1 \u2212 \u03bb).Conclusion 1. Let 'be an L-Lipschitz function."}, {"heading": "4.2 Improper learning of F\u03b3 and a kernel trick", "text": "Let G be the set of all finite, non-empty sequences of length not exceeding d at most. For each s \u00b2 G stands for the length of the sequence and sends the last element of the sequence. In view of a series of observations o, we write s \u00b2 o if all elements of the sequence s belong to o. We allow: (Rd \u00b7 O) \u2192 R\u044b will be the embedding: (1 \u00b2) s = {xsend s \u00b2 o 0 elseLemma 3. For each Q and w we have: f \u00b2 w, Q (xo) = Optimization s1 \u00b2 o ws1xs1 + Plane {s: s \u00b2, 2 \u00b2, ws1 \u00b7 Qs2 \u2212 Qs2, s4 \u00b7 s \u00b2 that we have such a product."}, {"heading": "5 Discussion and future work", "text": "We have described the first theoretically sound method for dealing with low-level missing data, resulting in a classification algorithm that yields competitive errors against the optimal linear classifier that has access to all data. Our improper agnostic framework for learning a low-level hidden subspace has proven guarantees, whereas heuristics based on separate data reconstruction and classification fail for certain scenarios of low-rank missing data and polynomial nuclei on core composition. General nuclei can be handled by polynomial approximation, but it is interesting to think about a more direct approach. It is possible to derive all our results for a less stringent condition than for an unevenness: instead of limiting the smallest eigenvalue of the hidden subspace, it is possible to bind only the ratio of largest to smallest eigenvalue."}, {"heading": "A Proofs of theorems and lemmas from main text", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Technical Claims", "text": "Remember: Im (A) = {v: u Au = v} and ker (A) = {v: Av = 0}. And this rank (A) is the size of the largest collection of linearly independent columns of A. The following statements are equivalent: 1. ker (PQ) = ker (Q).2. rank (PQ) = rank (QP >) = rank (PQP >) = rank (Q).3. Im (QP >) = Im (Q)."}, {"heading": "Proof.", "text": "1 \u21d2 2 Clear Rank (PQ) \u2264 Rank (Q). If rank (PQ) < rank (Q) = Q (Q) follows, we must have a collection of linear independent columns of Q that are linearly dependent on each other in PQ. This rank (PQ) = rank (QP >) = rank (PQP >) results from the fact that rank (A) = rank (A >) = rank (AA >) and from the fact that rank (PQ) = rank (Q) is a predictive matrix.2 \u21d2 3 We have this rank (QP >) Im (Q). The two subranges, Im (QP >) and Im (Q), are actually the linear range of the columns of QP > and Q."}, {"heading": "A.2 proof of Lemma 1", "text": "By definition, if Pox-Im (Qo, o) then Qo, o (Qo, o) \u2020 Pox = Pox. We claim that due to the assumption of low rank Pox-Im (Qo, o).In fact, we remember that Q = PE and x-E therefore Qx = x and Pox-Im (PoQ).According to claim 2 we have Im (Qo, o) = Im (PoQ), hence Pox-Po (ImQo, o).Next we have Pox-Pox > o (Qo, o) \u2020 Pox = Qo, o (Qo, o) \u2020 Pox = PoxAlternativelyPo (QP > o Q \u2020 o, oPox-x) = 0. (4) Again, since Qx = x-Pox-Pox = Q > Pox-Pox (P > Pox) = 0."}, {"heading": "A.3 proof of Lemma 2", "text": "Note first that (Io, o \u2212 Qo, o) = (I \u2212 Q) o, o and that Io \u2212 o is the identity matrix in R | o \u2212 | | o |.Let v1,.., vk be the normalized and orthogonal self-vectors of Qo, o with strictly positive eigenvalues.....,....... and since the spectral norm of Qo, o is smaller than the spectral norm of Q, we have this 1 \u2264 1st note that we use for each vj Q \u2020 o, ovj = 1 \u2012 j vj. Next, remember that Q = PE and x \u00b2 E hence Qx = x and Pox \u00b2 Im (PoQ) are smaller than the spectral norm of Q."}, {"heading": "A.4 Proof of Lemma 3", "text": "Let us let o1 \u2264 o2, \u2264. \u2264 o | o | the elements of o be ordered in increasing order. First, we have by definition the following: f\u03b3w, Q (xo) = \u03b3 \u2212 1 \u2211 j = 0 | o | \u2211 n, k = 1 won ((Qo, o) j) n, kxok = \u2211 i-wixi + \u03b3 \u2212 1 \u2211 j = 1 | o | \u2211 n, k = 1 won (((Qo, o) j) n, kxok (6) We also have by definition that for j \u2265 1: (Qo, o) j) n, k = 1 ((((Qo, o) j \u2212 1) n, s (Qo, o) s = 1 ((Qo, o) j \u2212 1 (Qo, o) j \u00b7 sok (sQo, o) n: (Qo, o) sQos, okBy induction we can show that: (now Qo, o), sq = 1 \u00b7 sq (o, sq), o, sq = 1, o, sq = 1."}, {"heading": "A.5 Proof of Corollary 2", "text": "Choosevs = {ws1 | = 1 ws1 \u00b7 Qs1, s2 \u00b7 Qs2, s3 \u00b7 \u00b7 Qs | \u2212 1, send | s | > 1It is clear from Lemma 3 that f\u03b3w, Q (xo) = v \u00b7 (xo). We only have to show that since Q2 = Q max. | Qi, j |) < 1. This results in | vs | ws1 | and: \u0438 v \u00b2 2 = \u0441ststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststst"}, {"heading": "A.6 Proof of Theorem 4", "text": "By definition, we have the following: \u03c6\u03b3 (x (1) o1) \u00b7 throughs (x (2) o2) = \u2211 s o1 o2 x (1) send \u00b7 x (2) send = \u03b3 (1) k o1 o2 s o1 o2 s s, send = k, | s | = l x (1) k \u00b7 x (2) k = 1 l = 1 s | = l \u2212 1, s o1 o1 o2 o2 x (1) k \u00b7 x (2) k = 1 l = 1 l = 1 s: 1, s l \u2212 1, s o1 o1 o2 (2) k = 1 o1 o2 k (1) o2 k (1) o2 k (1) o2 k (1)."}, {"heading": "A.7 Proof of Theorem 2", "text": "We assume that \"the L-Lipschitz and the L-Lipschitz (xo) and the L-Lipschitz (xo) and the L-Lipschitz (xo), the L-Lipschitz and the L-Lipschitz (xo), the L-Lipschitz and the L-Lipschitz (xo), the L-Lipschitz and the L-Lipschitz (xo), the L-Lipschitz and the L-Lipschitz (xo), the L-Lipschitz and the L-Lipschitz (xo), the L-Lipschitz and the L-Lipschitz."}, {"heading": "A.8 Proof of Theorem 1", "text": "Attach a sample S = {xioi} m i = 1 and \u03b3 \u2265 log 2L / \u03bb). Letter L (v) = E ('(v > \u03c6\u03b3 (xo), y) L (v) = 1' (v > \u03c6\u03b3 (x i oi), the expected and empirical losses of the vector v. Furthermore, label Fc (v) = 12C = 2 + L (v) F-c (v) = 12C-v (v) 2 + L-v (v) Leave C (m) Leave C (m) + O (m). Run Alg. 1 with T = m and leave v-v = 1 T-T i = 1 vt. By theorem 2, point 2 we get: F-C (m) (v-v) \u2264 min F-C (m) (v) + O (C-m) L2T () m) hint that we have none (xo)."}, {"heading": "A.9 Proof of Theorem 3", "text": "Before we prove the theory, we formally define the sequences for which the algorithm applies: a \"Q\" regular sequence is such that the even distribution over the sequence elements with the associated subspace. \"Proof of Theorem 3.\" Let us call the \"Q\" function \"Q\" and the associated \"Q\" projection. \"Since the sequence\" X \"(xt, ot, yt)\" x \"-regular is the\" X \"-rule.\" (D). \"(D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D. (D). (D. (D). (D). (D. (D). (D). (D. (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D. (D). (D. (D). (D). (D). (D. (D). (D. (D). (D). (D. (D). (D). (D). (D. (D. (D). (D). (D). (D. (D). (D). (D. (D). (D. (D). (D). (D). (D. (. (D). (D).). ("}], "references": [{"title": "Learning with restricted focus of attention", "author": ["S. Ben-David", "E. Dichterman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Ben.David and Dichterman,? \\Q1998\\E", "shortCiteRegEx": "Ben.David and Dichterman", "year": 1998}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Q. Berthet", "P. Rigollet"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Berthet and Rigollet,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet", "year": 2013}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Candes", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Candes and Recht,? \\Q2009\\E", "shortCiteRegEx": "Candes and Recht", "year": 2009}, {"title": "Efficient learning with partially observed attributes", "author": ["N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir"], "venue": "In Proceedings of the 27th international conference on Machine learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2010}, {"title": "Online learning of noisy data", "author": ["N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2011}, {"title": "Max-margin classification of data with absent features", "author": ["Chechik", "Gal", "Heitz", "Geremy", "Elidan", "Abbeel", "Pieter", "Koller", "Daphne"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Chechik et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2008}, {"title": "Learning to classify with missing and corrupted features", "author": ["Dekel", "Ofer", "Shamir", "Ohad", "Xiao", "Lin"], "venue": "Mach. Learn.,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Discrete chebyshev classifiers", "author": ["Eban", "Elad", "Mezuman", "Globerson", "Amir"], "venue": null, "citeRegEx": "Eban et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eban et al\\.", "year": 2014}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["Globerson", "Amir", "Roweis", "Sam"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Globerson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2006}, {"title": "Transduction with matrix completion: Three birds with one stone", "author": ["Goldberg", "Andrew B", "Zhu", "Xiaojin", "Recht", "Ben", "Xu", "Jun-Ming", "Nowak", "Robert D"], "venue": "In Proceedings of the 24th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Goldberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2010}, {"title": "Introduction to Online Convex Optimization", "author": ["Hazan", "Elad"], "venue": "URL http://ocobook.cs.princeton. edu/", "citeRegEx": "Hazan and Elad.,? \\Q2014\\E", "shortCiteRegEx": "Hazan and Elad.", "year": 2014}, {"title": "Linear regression with limited observation", "author": ["Hazan", "Elad", "Koren", "Tomer"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Near-optimal algorithms for online matrix prediction", "author": ["Hazan", "Elad", "Kale", "Satyen", "Shalev-Shwartz", "Shai"], "venue": "In COLT, pp", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["D.G. Horvitz", "D.J. Thompson"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Horvitz and Thompson,? \\Q1952\\E", "shortCiteRegEx": "Horvitz and Thompson", "year": 1952}, {"title": "Practical large-scale optimization for max-norm regularization", "author": ["J. Lee", "B. Recht", "R. Salakhutdinov", "N. Srebro", "J.A. Tropp"], "venue": "In NIPS, pp", "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "Statistical Analysis with Missing Data, 2nd Edition", "author": ["Little", "Roderick J. A", "Rubin", "Donald B"], "venue": null, "citeRegEx": "Little et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Little et al\\.", "year": 2002}, {"title": "Collaborative filtering in a non-uniform world: Learning with the weighted trace norm", "author": ["R. Salakhutdinov", "N. Srebro"], "venue": "In NIPS,", "citeRegEx": "Salakhutdinov and Srebro,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov and Srebro", "year": 2010}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew"], "venue": "Mathematical programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Collaborative filtering with the trace norm: Learning, bounding, and transducing", "author": ["O. Shamir", "S. Shalev-Shwartz"], "venue": "JMLR - Proceedings Track,", "citeRegEx": "Shamir and Shalev.Shwartz,? \\Q2011\\E", "shortCiteRegEx": "Shamir and Shalev.Shwartz", "year": 2011}, {"title": "Learning with Matrix Factorizations", "author": ["Srebro", "Nathan"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Srebro and Nathan.,? \\Q2004\\E", "shortCiteRegEx": "Srebro and Nathan.", "year": 2004}, {"title": "Fast rates for regularized objectives", "author": ["Sridharan", "Karthik", "Shalev-Shwartz", "Shai", "Srebro", "Nathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sridharan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "In the machine learning community, missing data was considered in the framework of limited attribute observability Ben-David & Dichterman (1998) and its many refinements Dekel et al. (2010); Cesa-Bianchi et al.", "startOffset": 170, "endOffset": 190}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data.", "startOffset": 8, "endOffset": 63}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)).", "startOffset": 8, "endOffset": 369}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)).", "startOffset": 8, "endOffset": 396}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)). In our case the learner has no control which attributes are observable in an example and the domain is fixed. The latter case is captured in the work of Chechik et al. (2008), who rescale inner-products according to the amount of missing data.", "startOffset": 8, "endOffset": 573}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)). In our case the learner has no control which attributes are observable in an example and the domain is fixed. The latter case is captured in the work of Chechik et al. (2008), who rescale inner-products according to the amount of missing data. Their method, however, does not entail theoretical gaurantees on reconstruction in the worst case, and gives rise to non-convex programs. A natural and intuitive methodology to follow is to treat the labels (both known and unknown) as an additional column in the data matrix and complete the data using a matrix completion algorithm, thereby obtaining the classification. Indeed, this exactly was proposed by Goldberg et al. (2010). Although this is a natural approach, we show that 1We remark that our model implicitly includes mean-imputation or 0-imputation method and therefore will always outperform them.", "startOffset": 8, "endOffset": 1074}, {"referenceID": 12, "context": "Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011).", "startOffset": 180, "endOffset": 198}, {"referenceID": 12, "context": "Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011).", "startOffset": 180, "endOffset": 229}, {"referenceID": 12, "context": "Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011). The only nonprobabilistic matrix completion algorithm in the online learning setting we are aware of is Hazan et al.", "startOffset": 180, "endOffset": 261}, {"referenceID": 11, "context": "The only nonprobabilistic matrix completion algorithm in the online learning setting we are aware of is Hazan et al. (2012), which we were not able to use for our purposes.", "startOffset": 104, "endOffset": 124}, {"referenceID": 9, "context": "Goldberg et al. (2010) Considered the following problem: minimize Z rank(Z) subject to: Zi,j = xi,j , (i, j) \u2208 \u03a9 , .", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "The optimization analysis is derived in a straightforward manner from the work of Shalev-Shwartz et al. (2011). Other optimization algorithms exist in the literature, and we chose this optimization method as it allows us to also derive regret bounds which are formally stronger (see Section 2.", "startOffset": 82, "endOffset": 111}], "year": 2015, "abstractText": "We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data.", "creator": "LaTeX with hyperref package"}}}