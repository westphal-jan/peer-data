{"id": "1501.03796", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2015", "title": "The Fast Convergence of Incremental PCA", "abstract": "We consider a situation in which we see samples in $\\mathbb{R}^d$ drawn i.i.d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and incrementally adjusts the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates for both.", "histories": [["v1", "Thu, 15 Jan 2015 20:08:49 GMT  (40kb)", "http://arxiv.org/abs/1501.03796v1", "NIPS 2013"]], "COMMENTS": "NIPS 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay balsubramani", "sanjoy dasgupta", "yoav freund"], "accepted": true, "id": "1501.03796"}, "pdf": {"name": "1501.03796.pdf", "metadata": {"source": "CRF", "title": "The Fast Convergence of Incremental PCA", "authors": ["Akshay Balsubramani"], "emails": ["abalsubr@cs.ucsd.edu", "dasgupta@cs.ucsd.edu", "yfreund@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 1.03 796v 1 [cs.L G] 15 Jan 20"}, {"heading": "1 Introduction", "text": "\"We have the possibility that there will be a problem like this in the next two years,\" he said. \"But it's not like there will be a problem like this.\" \"It's not like there will be a problem like this.\" \"It's not like there will be a problem like this.\" \"It's not like there will be a problem like this.\" \"It's not like there will be a problem like this.\" \"It's not like there will be a problem like this.\" \"It's not like there will be a problem like this.\" \"It's not like there will be a problem like this.\" \"It's not like there will be a problem like this.\" \"It's not like there will be a problem.\" \"It's not like there will be a problem like this.\" \"It's not like there will be a problem like this.\" \"It's not like there is a problem like this.\""}, {"heading": "1.1 The algorithm", "text": "We analyze the following procedure: 1. Set the start time. Set the clock to time no. 2. Initialization. Initialize Vno evenly randomly from the unit sphere in R d. 3. For the time n = no + 1, no + 2,..: (a) Receive the next data point, Xn. (b) Update step. Perform either the Krasulina or Oja update, the first step being similar to using a learning rate of the form \u03b3n = c / (n + no), as is often the case with stochastic gradient descents [1]. We adopted it because the initial sequence of updates is very loud: at this stage Vn moves wildly around and cannot be shown that progress is being made. It behaves better when the step size \u0421n gets smaller, that is, when n becomes larger than any suitable No. By setting the start time to no, we can simply accelerate the analysis to this moment."}, {"heading": "1.2 Initialization", "text": "One possible initialization is to set Vno to the first data point that arrives, or to the average of a few data points. This seems reasonable enough, but may fail dramatically in some situations. Let's assume X can only assume 2d possible values: \u00b1 e1, \u00b1 \u03c3e2,.., \u00b1 \u03c3ed, where the ei coordinate directions and 0 < \u03c3 < 1 are a small constant. Let's assume that the distribution of X by a single positive number p < 1: Pr (X = e1) = Pr (X = \u2212 e1) = p2Pr (X = \u03c3ei) = Pr (X = \u2212 \u03c3ei) = 1 \u2212 p2 (d \u2212 1) for i > 1Then X has an average zero and covariance diag (p, \u03c32 (1 \u2212 p) / (d \u2212 1)."}, {"heading": "1.3 The setting of the learning rate", "text": "For example, to get a sense of what convergence rates we can expect, we return to a random vector X with 2d possible values. In the Oja update Vn = Vn-1 + \u03b3nXTn Vn-1, we can ignore the normalization if we are only interested in the progress of the potential function. Since the Xn corresponds to the coordinate directions, each update changes only a coordinate of V: Xn = \u00b1 e1 = \u21d2 Vn, 1 = Vn -1,1 (1 + \u03b3n) Xn = \u00b1 \u03c3ei = \u21d2 Vn, i = Vn -1, i (1 + \u03c32\u03b3n) Remember that we initialize Vno to a random vector from the unity sphere \u2212 n. To make it easier, we simply assume that none = 0 and that this initial value is the sole vector (again Vn-1) (Vkec-2)."}, {"heading": "1.4 Nested sample spaces", "text": "Suppose Fn denotes the sigma field of all results up to and including time n: Fn = \u03c3 (Vno, Xno + 1,..., Xn). Let us begin by showing that the sigma field of all results up to and including time n: Fn = \u03c3 (Vno, Xno + 1,..., Xn). For example, suppose that the original Vno is uniformly randomly selected from the surface of the unit sphere in Rd, then we expect that no arguments 1 \u2212 1 / d. This means that the initial rate of decrease is very low because the term (1 \u2212 n \u2212 1). To deal with this, we divide the analysis into epochs: the first does not take arguments from 1 \u2212 1 / d to 1 \u2212 2 / d, the second from 1 \u2212 eg."}, {"heading": "1.5 Main result", "text": "We make the following assumptions: (A1) The Xn-Rd values are i.i.d. with mean zero and covariance A. (A2) The step sizes are so constant that we get the following convergence rate for the krasulina. Theorem 1.1. There are absolute constants Ao, A1 > 0 and 1 < a < 4 for which the following form applies. Choose any 0 < \u03b4 < 1, and any co > 2. Set the step sizes to \u03b3n = c / n, where c = co / (2 (1 \u2212 2) and set the start time to zero (AoB2c2d2 / 4) ln (1 / 4).Then there is a neconstant deviation from the deviation, where c = co / (2 (1 \u2212 2) and no deviation from the deviation (1 \u2212 2)."}, {"heading": "1.6 Related work", "text": "There is a wide range of work that analyzes PCA from a statistical perspective, characterizing the convergence of different estimators under certain conditions, including generative models of the data [5] and various assumptions about the covariance matrix spectrum [14, 4] and eigenvalue spacing [17]. Such work provides guarantees for finite samples, but applies only to the batch case and / or is computationally complex rather than considering an efficient incremental algorithm. Among incremental algorithms, Warmuth and Kuzmin [15] lack incremental analyses of finite samples. Recently, there have been attempts to address this situation by loosening the non-convexity inherent in the problem, or by generative assumptions [8]."}, {"heading": "2 Outline of proof", "text": "We will now sketch the proof for Theorem 1.1; almost all details are banished to the appendix. Remember that for n \u2265 no Fn we consider the sigma field of all results up to and including time n, i.e. Fn = \u03c3 (Vno, Xno + 1,..., Xn). An additional notation piece: We will use u to denote u / \u0441 u, the unit vector in the direction of u-Rd. Thus, for example, the Rayleigh quotient G (v) = v-TAv-Rd can be written."}, {"heading": "2.1 Expected per-step change in potential", "text": "Theorem 2.1. For each n > no we can write the expected improvement of n = n \u2212 n \u2212 1 + \u03b2n \u2212 Zn in each step of the krasulina or oja algorithms, where \u03b2n = {\u03b32nB2 / 4 (krasulina) 5\u03b32nB 2 + 2\u03b33nB 3 (oja) and where Zn is a Fn measurable random variable with the following properties: \u2022 E [Zn | Fn \u2212 1] = 2\u03b3n (V-n \u2212 1 \u00b7 v \u0445) 2 (\u03bb1 \u2212 G (Vn \u2212 1)) \u2265 2\u03b3n (\u03bb1 \u2212 \u03bb2) \u30fb n \u2212 1 (1 \u2212 \u041an \u2212 1) \u2265 0. \u2022 Zn | \u2264 4\u0445nB The theorem follows Lemmas A.4 and A.5 in the appendix. Its characterization of the two estimators is almost identical, and for simplicity we will henceforth stick to the results of the krasulina or oja method."}, {"heading": "2.2 A large deviation bound for \u03a8n", "text": "We know from theorem 2.1 that this growth is not big enough to achieve it. < n > Y, where \u03b2n is non-stochastical and Zn is a quantity of positive expectation value. \u2212 n In anticipation and modulo of a small additive expression, this magnitude decreases monotonously. However, the amount of decrease in the next time step can be arbitrarily small if it is close to 1. \u2212 n Therefore, we have to show that we are ultimately delimited from 1, i.e. there are some deviations from o > 0 and some time no such that we have no deviation for any nth time steps. \u2212 n Recall from the algorithm specification that we move the clock forward to skip the pre-no phase. What can we expect then to be lemmals? If the initial estimation Vno is a random unit vector, then we are a random vector, then E [= 1] > = 1, and Pb \u2212 n said."}, {"heading": "2.3 Intermediate epochs of improvement", "text": "We have seen that for appropriate solutions and no, it is likely that the probability that these measures will be taken is low. < nJ = > J = > J = > We are now defining a series of periods in which 1 \u2212 n will double in succession until they eventually fall below 1 / 2. < nJ and other intermediate goals (no, o), (n1, o), (n2, o), (n2, o),., (nJ, o), where no < n1 < nJ and o < nJ and o < nJ and o < (n1, o) < nJ = 1 / 2, with the intention that for all 0 \u2264 J, we sup n \u2264 n \u2264 1 \u2212 j. (2) Of course, this can only happen with some probability."}, {"heading": "2.4 The final epoch", "text": "Remember the definition of the intermediate goals in (2), (3). The last epoch is the period n \u2265 nJ, at which the values n \u2264 1 / 2. The following sequence of lemmas A.4 and 2.8 records the rate at which the values decrease in this phase. Lemma 2.11. For all n > nJ, En [n] \u2264 (1 \u2212 \u03b1n) En \u2212 1 [n \u2212 1] + \u03b2n, where \u03b1n = (1 \u2212 \u03bb2) \u03b3n and \u03b2n = (B2 / 4) \u03b32n.By solving this recurrent relationship and merging the different epochs, we obtain the overall result of theorem 1.1. Note that lemm2.11 is very similar to the return relationship, followed by the square L2 distance from the optimum of stochastic gradient lineage (SGD) on a strongly convex function [11]."}, {"heading": "3 Experiments", "text": "When performing PCA in practice with massive d and a large / growing dataset, an incremental method such as Krasulina or Oja remains practically practicable, even as square time and storage algorithms become increasingly impractical. Arora et al. [2] now have a broader discussion on the empirical necessity of using incremental PCA algorithms, including a version of Oja's method that is proving highly competitive in practice. As the efficiency benefits of these algorithms are well understood, we will instead focus on the impact of the learning rate on the performance of Oja's algorithm (the results for Krasulinas are extremely similar). We use the CMU-PIE faces [13], consisting of 11554 images of size 32 x 32, as a prototype example of a dataset with the largest part of its variance on the Figure 43010 recorded by some PCs."}, {"heading": "4 Open problems", "text": "Several basic questions remain unanswered: Firstly, the convergence rates of the two incremental schemes depend on the multiplier c in the learning rate \u03b3n. If it is too low, the convergence will be slower than O (1 / n). If it is too high, the convergence rate will be high. Is there a simple and practical scheme for setting c? Secondly, what can be said about the incremental estimation of the uppermost p eigenvectors, for p > 1? Both methods, which we consider easy, extend to this case [10]; the estimate at this point in time n is a d \u00b7 p matrix Vn, the columns of which correspond to the eigenvectors, with the invariant V Tn Vn = Ip always maintained. For example, in Oja's algorithm, when a new data point Xn \u2011 Rd arrives, the following update is performed: Wn = Vn \u2212 1 + \u03b3nX T n \u2212 1Vn = Orth (step where the Wn converges)."}, {"heading": "Acknowledgments", "text": "The authors thank the National Science Foundation for its support under the funding program IIS-1162581."}, {"heading": "A Expected per-step change in potential", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 The change in potential of Krasulina\u2019s update", "text": "Write Krasulina's updated equation asVn = Vn \u2212 1 part \u2212 n part \u00b7 n part \u00b7 n part \u00b7 n part \u00b7 n part (XnX T n \u2212 V \u2212 1XnXTn V \u2212 n) n part \u00b7 n part \u00b7 n part (Vn \u2212 n) n part (Vn \u2212 n) n part (Vn \u2212 n) n part (Vn \u2212 n) n part (n \u2212 n) n part \u2212 n (n) n part \u2212 n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n) n (n) n (n) n) n (n) n (n) n) n (n) n (n) n) n (n) n) n (n) n) n (n) n (n) n) n (n) n (n) n) n (n) n) n) n (n) n) n (n) n) n (n) n) n (n) n) n n (n) n) n (n) n) n (n) n) n (n) n) n (n) n (n) n) n (n) n (n) n (n) n) n (n) n (n) n (n) n) n (n) n (n) n (n) n) n (n) n (n) n) n (n (n) n (n) n) n (n) n (n) n n) n (n (n) n) n (n) n (n) n) n (n) n) n (n) n (n) n) n (n (n) n) n) n (n n"}, {"heading": "A.2 The change in potential of the Oja update", "text": "Remember the Oja update: Vn = Vn \u2212 1 + Vn \u2212 n, which depends on the length of Vn \u2212 n, and we can skip normalization and instead only look at the updated rule Vn \u2212 1 + Vn \u2212 n \u2212 n. \u2212 n The final limits, as well as many of the intermediate results, are almost identical to Krasulina's estimates. Here is the analogy to Lemma A.4. Lemma A.5. For each n \u2212 n we can n \u2212 n \u2212 n, where Zn + \u03b2n is the same as in Lemma A.4 and \u03b2n = 5g 2nB 2 (V2V) V2V (V2) 3.Proof. This is a series of calculations."}, {"heading": "B.1 Proof of Lemma 2.3", "text": "For each t > 0, E [etYn | Fn \u2212 1] \u2264 E [et (Yn \u2212 1 + \u03b2n \u2212 Zn) | Fn \u2212 1] = et (Yn \u2212 1 + \u03b2n) E [e \u2212 tZn | Fn \u2212 1] = et (Yn \u2212 1 + \u03b2n) E [e \u2212 tE [Zn | Fn \u2212 1] e \u2212 t (Zn \u2212 E [Zn | Fn \u2212 1]) | Fn \u2212 1] \u2264 et (Yn \u2212 1 + \u03b2n \u2212 E [Zn | Fn \u2212 1]) E [e \u2212 t (Zn \u2212 E [Zn | Fn \u2212 1]) | Fn \u2212 1]. We fixed the last expectation using the Hoeffding problem: E [etW] \u2264 et2 (b \u2212 a) 2 / 8 for each random variable W of mean zero and range [a, b]."}, {"heading": "B.2 Proof of Lemma 2.4", "text": "From Lemma 2,3, E [etYn | Fn \u2212 1] \u2264 exp (tYn \u2212 1 + t\u03b2n + t2\u04412n 8).Let us now define an appropriate martyrdom. Let us define a suitable martyrdom. Let us allow Mn = \u2211 > n (\u03b2 + t\u0445 2 / 8) and let us allow Mn = exp (t (Yn + \u03c4n). Thus, Mn Fn, andE [Mn | Fn \u2212 1] = E [etYn \u2212 1] exp (t\u03c4n) \u2264 exp (tYn \u2212 1 + t\u03b2n + t2\u0445 2n 8 + t\u03c4n) = Mn \u2212 1.Thus (Mn) is a positively rated supermartyrdom adapted to (Fn).A version of Doob's martyrdom inequality - see for example page 274 of [6] - then states that for every psur (this type of martyrm) \u2212 n."}, {"heading": "B.3 Proof of Lemma 2.5", "text": "It is generally known that V can be selected independently of the standard distribution by selecting the d-values Z = (Z1,.. \u2212 Z2dZ21 + (Z 2 + \u00b7 + Z2d) and then V = Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}, {"heading": "B.4 Proof of Theorem 2.2", "text": "Lemma A.4 (a) shows that the first deviation from Lemma 2.4 and E [Zn | Fn \u2212 1] \u2265 0 and the second deviation from Zn lies in a length interval of n = 8\u03b3nB. Therefore, we can directly apply the first deviation from Lemma 2.4. Since then, the first deviation from Lemma 2.4 is no longer possible."}, {"heading": "C.1 Proof of Lemma 2.7", "text": "Lemma A.4 determines an inequality of n \u2264 n \u2212 1 \u2212 Zn + \u03b2n as well as a lower limit of E [Zn | Fn \u2212 1], where Zn is a random variable that lies in a length interval of n = 8\u03b3nB. Starting from term 2.3, we then have E [et\u0430n | Fn \u2212 1] \u2264 exp (t (n \u2212 1 \u2212 E [Zn | Fn \u2212 1] + \u03b2n + t2n / 8) \u2264 exp (t (n \u2212 1 \u2212 2\u0441n (n \u2212 2), n \u2212 1 (1 \u2212 2 \u2212 1) + \u03b32nB2 (1 + 32t) / 4)) = exp (t (n \u2212 1 \u2212 co.n \u2212 1 (1 \u2212 n) / n + c2B2 (1 + 32t) / 4n2))."}, {"heading": "C.2 Proof of Lemma 2.8", "text": "Suppose j is the largest index, so nj < n. The value of n \u2212 1 (\u03c9) for n > 1 \u2212 1 \u2212 1 for n > 1 \u2212 j for n \u2212 1 \u2212 nThus, the expected value of g (n \u2212 1) over n is at best the expected value over n \u2212 1."}, {"heading": "C.3 Proof of Lemma 2.9", "text": "We start with the following Lemma.Lemma C.1. For all n > nj and all t > 0, En [e tn] \u2264 exp (t (1 \u2212 n) (nj + 1n + 1) co.j + tc2B2 (1 + 32t) 4 (1nj \u2212 1 n).Proof. Define n = 1 \u2212 (co.j / n) and n \u2212 n (t) = c2B2t (1 + 32t) / 4n2. Of Lemmas 2.7 and 2.8, for n > nj, En [e tn] \u2264 n \u2212 1 \u2212 1 (t) exp (n (t)) \u2264 n \u2212 1 \u2212 1 (tn \u2212 1 \u2212 1) exp (n \u2212 1 \u2212 1] exp (t (t (t)).By applying these inequalities repeatedly (1n (1n) (1p (and t shrinking at the same time), we get the exp (n))."}, {"heading": "C.4 Proof of Lemma 2.10", "text": "Choose any 0 < j \u2264 J. We will imitate the reasoning of theorem 2,2, taking care to define martyrdom only in the limited space and with the start time nj. ThenPnj (sup n \u2265 nj) > 1 \u2212 ej] exp (\u2212 t (1 \u2212 ej) + tc2B2 (1 + 32t) 4nj) \u2264 exp (\u2212 tc \u2212 1 + tc2B2 (1 + 32t) 4nj \u2212 1), with the second step Lemma 2.9.Finally, we choose t = (2 / eo) ln (4 / ej).The lower limit on the number is also a lower limit on nj \u2212 1 \u2212 1 and implies that tc2B2 (1 + 32t) / 4nj \u2212 1 \u2264 ej (2ej)."}, {"heading": "D The final epoch", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Proof of Lemma 2.11", "text": "By Lemma A.4, E [\u0441\u0435n | Fn \u2212 1] Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum-based Vacuum Vacuum-based Vacuum-based Vacuum-Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum Vacuum"}, {"heading": "D.2 Proof of Theorem 1.1", "text": "We define epochs (nj, j) that meet the conditions of theorems 2.6, with J = 1 / 2, and with j + 1 = 2 j whenever possible. Then, J = log2 1 / (o) andnJ + 1 = (no + 1) exp (5Jco) = (no + 1) (o) 5 / (co ln 2) = (no + 1) (\u2212 n) 5 / (co ln 2) 5 / (co ln 2). According to theorem 2.6, with probability > 1 \u2212 6, we have n \u2264 1 / 2 for all n nJ. More precisely, P (n) n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n (n n En n n n n n n n n n n n n n) \u2212 n n n \u2212 n n n (n n n n n n n n) \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 -n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 -n \u2212 n \u2212 n \u2212 -n \u2212 n \u2212 -n \u2212 n \u2212 n \u2212 -n \u2212 n \u2212 n \u2212 -n \u2212 n \u2212 -n \u2212 -n \u2212 n \u2212 n \u2212 n \u2212 -n \u2212 n \u2212 -n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n -n \u2212 -n -n \u2212 n -n, -n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 -n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 -n \u2212 -n \u2212 -n, -n, -n -n, -n -n), -n -n -n -n -n -n -n -n, -n, -n, -n -n -n, -n -n -n, -n -n -n, -n -n -n, -n -n -n, -n -n, -n -n -n, -n, -n -n -n -n, -n -n -n, -n -n -n, -n, -n, -n -n -n"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>We consider a situation in which we see samples Xn \u2208 R drawn i.i.d. from some<lb>distribution with mean zero and unknown covariance A. We wish to compute the<lb>top eigenvector of A in an incremental fashion with an algorithm that maintains<lb>an estimate of the top eigenvector in O(d) space, and incrementally adjusts the<lb>estimate with each new data point that arrives. Two classical such schemes are<lb>due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates<lb>for both.", "creator": "LaTeX with hyperref package"}}}