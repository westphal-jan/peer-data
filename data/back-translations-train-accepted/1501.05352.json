{"id": "1501.05352", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2015", "title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "abstract": "In binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is suboptimal. Recent work has applied alternating optimization to the objective over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that closes the loop and optimizes jointly over the hash functions and the binary codes. The resulting algorithm can be seen as a corrected, iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised and unsupervised datasets. In addition, the framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.", "histories": [["v1", "Wed, 21 Jan 2015 23:53:47 GMT  (64kb)", "https://arxiv.org/abs/1501.05352v1", "18 pages, 7 figures"], ["v2", "Fri, 5 Feb 2016 01:25:26 GMT  (230kb)", "http://arxiv.org/abs/1501.05352v2", "22 pages, 12 figures; added new experiments and references"]], "COMMENTS": "18 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV math.OC stat.ML", "authors": ["ramin raziperchikolaei", "miguel \u00e1 carreira-perpi\u00f1\u00e1n"], "accepted": true, "id": "1501.05352"}, "pdf": {"name": "1501.05352.pdf", "metadata": {"source": "CRF", "title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "authors": ["Ramin Raziperchikolaei", "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 1.05 352v 2 [cs.L G] 5F ebIn supervised binary hashing, you want to learn a function that maps a high-dimensional feature vector to a binary code vector for use in fast image retrieval. This typically leads to a difficult optimization problem, nonconvex and nonsmooth, due to the discrete variables involved. Much work has simply eased the problem during the training, solved a continuous optimization, and truncated the codes a posteriori. This yields reasonable results, but is quite suboptimal. Recent work has tried to optimize the target directly via the binary codes, and achieved better results, but the hash function has been learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions that use affinity-based loss functions that use auxiliary coordinates. This includes the loop and optimizes the hash functions collectively, so that they can be considered to coincide with better functions over time."}, {"heading": "1 Introduction", "text": "In practice, this is the case in several application areas, and a successful path in this direction is also in this area. For example, a user is interested in finding similar images for a query image. (This applies not only to the query image, but also to other applications where each relevant image is represented by a vector and then the closest points (the closest neighbors) to the vector for the query image can be found, according to a suitable distance (Shakhnarovich et al, 2006). For example, one can consider features such as SIFT (Lowe, 2004) or GIST (Oliva and Torralba, 2001) and the Euclidean distance for this purpose. Finding closest neighbors in a dataset of N images (where each vector of dimension D (typically in the hundreds of hundreds) is slow because exact algorithms essentially run in time O (ND) and space O () (for storing image data)."}, {"heading": "2 Nonlinear embedding and affinity-based loss functions for bi-", "text": "b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b"}, {"heading": "3 Learning codes and hash functions using auxiliary coordinates", "text": "Optimizing the loss L (h) in eq. (1) is difficult because of the threshold hash function, which appears as the argument of the loss function. We use the recently proposed auxiliary coordinate method (MAC) (Carreira-Perpin and Wang, 2012, 2014), which is a meta algorithm to construct optimization algorithms for nested functions. This proceeds in 3 stages. First, we introduce new variables (the \"auxiliary coordinates\") as equality constraints into the problem, with the aim of stripping the function. We can achieve this by introducing a binary vector zn (1) for each point."}, {"heading": "3.1 Stopping criterion, schedule over \u00b5 and path of optimal values", "text": "It is possible to prove that once Z = h (X) after a Z-step (regardless of \u00b5), the MAC algorithm does not make any further changes to Z or h, because then the constraints are met. This gives us a reliable stop criterion that is easy to verify, and the MAC algorithm will stop after a finite number of iterations (see below). It is also possible to prove that the path of the minimizers of LP via the continuous penalty parameter \u00b5 [0, \u221e) is actually discrete, with changes to (Z, h) occurring only on an finite number of values (see below). It is also possible to prove that the path of the minimizers of LP via the continuous penalty parameter \u00b5 [0, \u221e) is a first discrete, with changes to (Z, h) occurring only on a finite number of values 0 < \u00b51 < < < 1 < 1 < the starting point of < < < the use of < < < < < < < < the starting point of < < < < < the use of LP; < < < the use of the LP)."}, {"heading": "3.2 h step", "text": "Considering the binary codes z1,.., zN, since h does not occur in the first term of LP, it is simply a matter of finding a hash function h that minimizes zni-hN-n = 1-zn-h. Therefore, we can find b-bit hash functions in parallel and link them to the b-bit hash function. Each of these problems is a binary classification problem using the number of incorrectly classified patterns as loss. This allows us to use a regular classifier for h and even to use an easier replacement loss (such as the hinged loss), as this ultimately enforces the constraints (as a \u00b5 increase). For example, we can adjust an SVM by optimizing the margin plus the gaps and using a high penalty for the other lassified patterns."}, {"heading": "3.3 Z step", "text": "Although the MAC technique has significantly simplified the original problem, the step over Z is still complex. It involves finding the binary codes that produce the hash function h, and it is a NP-complete problem in Nb binary variables. Fortunately, some recent work has suggested practical approaches to this problem based on alternative optimization: a square surrogate method (Lin et al., 2013) and a GraphCut method (Lin et al., 2014). In both cases, this would be the first step in the two-step hash of Lin et al. (2013).In both the quadratic surrogate method and the GraphCut method, the starting point is to apply the alternate optimization over the ith bit of all remaining bits, fixed for all points (for i = 1,., b), and to solve the optimization over the ith bit."}, {"heading": "4 Experiments", "text": "We have tested our framework with several combinations of loss function, hash function, number of bits, datasets and comparisons with several state-of-the-art methods (Appendix A contains further experiments). We report on a representative subset to show the flexibility of the approach. We use the KSH functions (for each bit), we use linear SVMs (trained with LIBLINEAR; Fan et al., 2008) and kernel SVMs1We use the following highlighted datasets (all with the Euclidean distance in the feature space): (1) CIFAR, 2009) contains 60 000 images in 10 classes. We use D = 320 GIST features (Oliva and Torralba, 2001) of each image."}, {"heading": "4.1 The MAC algorithm finds better optima", "text": "The aim of this paper is to create a new affinity loss or function to construct algorithms that form a particular combination of them."}, {"heading": "4.2 Why does MAC learn better hash functions?", "text": "In both cases, the starting point is the \"free\" binary codes obtained by minimizing the loss of the codes without them being the output of a particular hash function, i.e., minimizing (4) without the \"zn = h (xn)\" constraints: min Z E (Z) = N \u00b2 n (zn, zm; ynm), z1,.. zN {\u2212 1} b The resulting free codes attempt to achieve good precision / retrieval regardless of whether a hash function can actually produce such codes."}, {"heading": "4.3 Runtime", "text": "The running time per iteration for our 10,000-point training sets with b = 48 bits and B = + = 100 and C \u2212 = 500 neighbors in a laptop is 2 'for both MACcut and MACquad. They stop after 10-20 iterations. Each iteration is comparable to a single section or quad run, as the Z step dominates the calculation. Iterations after the first iteration are faster because they are started warm."}, {"heading": "4.4 Comparison with binary hashing methods", "text": "We compare MACquad and MACcut with Two-Step Hashing (Quad) (Lin et al., 2013), FastHash (cut) (Lin et al., 2014), Kernel Hashing (KSH) (Liu et al., 2012), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Reconstructive Embeddings (BRE) (Kulis et al., 2009) and Self-Taught Hashing (STH) (Zhang et al., 2010).MACquad, MACcut, Quad and Cut all use the KSH loss function (3).The results show that MACcut (and MACquad) generally outperform all other methods, often only a large number of MACQ bits (MACcut, Quad and Cut) and in many cases have only a three-dimensional size."}, {"heading": "5 Discussion", "text": "Two-step approaches to the current hash function show that significant, consistent gains are achieved in both the loss of function and the loss of function. (Two-step approach of Two-Step Hashing (Lin et al., 2013) and FastHash (Lin et al., 2014) is a significant step forward in the search for good binary hashing codes, but it also causes a mismatch between the codes and the hash function, as the codes were learned without knowing which hash function they would use. Interaction between the loss and the hash function limits the quality of the results. For example, a linear hash function will have a harder time than a non-linear one when learning such codes. In our algorithm, this trade is gradually forced (as \u00b5 increases) in the Z step as a regulatory term (eq): it finds the best codes after the loss function, but ensures that they are achievable close to the current hash function."}, {"heading": "6 Conclusion", "text": "We have proposed a general framework for optimizing binary hashing by using affinity-based loss functions. It improves on earlier, two-step approaches based on learning binary code and then learning the hash function. Instead, it collectively optimizes the binary code and hash function in alternation so that the binary code eventually matches the hash function, resulting in a better local optimization of the affinity function. It has been possible to do this by introducing auxiliary variables that partially decouple the codes from the hash function, and gradually enforcing the corresponding constraints. Our framework makes it easy to design an optimization algorithm for a new choice of loss functions or hash functions, and the resulting algorithm is not much slower than the suboptimal two-step approach - it is similar to the iteration of the latter."}, {"heading": "A Additional experiments", "text": "However, the results of this study are indeed the same as for the monitored datasets, with small differences in the settings of the experiments. To construct an affinity-based objective function, we define neighbors as follows: For each point in the training set, we use the closest neighbors as positive (similar) neighbors, and we have 500 points among the remaining points as negative (dissimilar) neighbors. We report precision and precision for the test set queries using real neighbors and neighbors, and we have chosen 500 points among the remaining points as negative (dissimilar) neighbors."}, {"heading": "Acknowledgments", "text": "We thank Ming-Hsuan Yang, Yi-Hsuan Tsai and Mehdi Alizadeh (UC Merced) for useful discussions."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Comm. ACM,", "citeRegEx": "Andoni and Indyk.,? \\Q2008\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2008}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "Belkin and Niyogi.,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2003}, {"title": "Computing geodesics and minimal surfaces via graph cuts", "author": ["Y. Boykov", "V. Kolmogorov"], "venue": "In Proc. 9th Int. Conf. Computer Vision (ICCV\u201903),", "citeRegEx": "Boykov and Kolmogorov.,? \\Q2003\\E", "shortCiteRegEx": "Boykov and Kolmogorov.", "year": 2003}, {"title": "An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision", "author": ["Y. Boykov", "V. Kolmogorov"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov and Kolmogorov.,? \\Q2004\\E", "shortCiteRegEx": "Boykov and Kolmogorov.", "year": 2004}, {"title": "The elastic embedding algorithm for dimensionality reduction", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Proc. of the 27th Int. Conf. Machine Learning (ICML", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n.,? \\Q2010\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n.", "year": 2010}, {"title": "Hashing with binary autoencoders", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "R. Raziperchikolaei"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.,? \\Q2015\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.", "year": 2015}, {"title": "A fast, universal algorithm to learn parametric nonlinear embeddings", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "M. Vladymyrov"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Vladymyrov.,? \\Q2015\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Vladymyrov.", "year": 2015}, {"title": "Distributed optimization of deeply nested systems", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "W. Wang"], "venue": "Proc. of the 17th Int. Conf. Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.,? \\Q2014\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.", "year": 2014}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Graph cuts for supervised binary coding", "author": ["T. Ge", "K. He", "J. Sun"], "venue": "In Proc. 13th European Conf. Computer Vision (ECCV\u201914),", "citeRegEx": "Ge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2014}, {"title": "Iterative quantization: A Procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Learning binary hash codes for large-scale image search", "author": ["K. Grauman", "R. Fergus"], "venue": "Machine Learning for Computer Vision,", "citeRegEx": "Grauman and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Grauman and Fergus.", "year": 2013}, {"title": "Spherical hashing", "author": ["J.-P. Heo", "Y. Lee", "J. He", "S.-F. Chang", "S.-E. Yoon"], "venue": "In Proc. of the 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Heo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Heo et al\\.", "year": 2012}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "J\u00e9gou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "J\u00e9gou et al\\.", "year": 2011}, {"title": "The wrapper approach", "author": ["R. Kohavi", "G.H. John"], "venue": "Feature Extraction, Construction and Selection. A Data Mining Perspective. Springer-Verlag,", "citeRegEx": "Kohavi and John.,? \\Q1998\\E", "shortCiteRegEx": "Kohavi and John.", "year": 1998}, {"title": "What energy functions can be minimized via graph cuts", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kolmogorov and Zabih.,? \\Q2003\\E", "shortCiteRegEx": "Kolmogorov and Zabih.", "year": 2003}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, Dept. of Computer Science, University of Toronto, Apr", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kulis and Darrell.,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Darrell.", "year": 2009}, {"title": "Kernelized locality-sensitive hashing", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kulis and Grauman.,? \\Q2012\\E", "shortCiteRegEx": "Kulis and Grauman.", "year": 2012}, {"title": "A general two-step approach to learning-based hashing", "author": ["G. Lin", "C. Shen", "D. Suter", "A. van den Hengel"], "venue": "In Proc. 14th Int. Conf. Computer Vision (ICCV\u201913),", "citeRegEx": "Lin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Fast supervised hashing with decision trees for high-dimensional data", "author": ["G. Lin", "C. Shen", "Q. Shi", "A. van den Hengel", "D. Suter"], "venue": "In Proc. of the 2014 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "In Proc. of the 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Training invariant support vector machines using selective sampling", "author": ["G. Loosli", "S. Canu", "L. Bottou"], "venue": "Large Scale Kernel Machines, Neural Information Processing Series,", "citeRegEx": "Loosli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2007}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. J. Computer Vision,", "citeRegEx": "Lowe.,? \\Q2004\\E", "shortCiteRegEx": "Lowe.", "year": 2004}, {"title": "Numerical Optimization. Springer Series in Operations Research and Financial Engineering", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "Nocedal and Wright.,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D. Fleet"], "venue": "Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Norouzi and Fleet.,? \\Q2011\\E", "shortCiteRegEx": "Norouzi and Fleet.", "year": 2011}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "Int. J. Computer Vision,", "citeRegEx": "Oliva and Torralba.,? \\Q2001\\E", "shortCiteRegEx": "Oliva and Torralba.", "year": 2001}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science,", "citeRegEx": "Roweis and Saul.,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul.", "year": 2000}, {"title": "Nearest-Neighbor Methods in Learning and Vision. Neural Information Processing Series", "author": ["G. Shakhnarovich", "P. Indyk", "T. Darrell", "editors"], "venue": null, "citeRegEx": "Shakhnarovich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shakhnarovich et al\\.", "year": 2006}, {"title": "LDAHash: Improved matching with smaller descriptors", "author": ["C. Strecha", "A.M. Bronstein", "M.M. Bronstein", "P. Fua"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Strecha et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Strecha et al\\.", "year": 2012}, {"title": "Visualizing data using t-SNE", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "J. Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Partial-Hessian strategies for fast learning of nonlinear embeddings", "author": ["M. Vladymyrov", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.,? \\Q2012\\E", "shortCiteRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.", "year": 2012}, {"title": "Linear-time training of nonlinear low-dimensional embeddings", "author": ["M. Vladymyrov", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Proc. of the 17th Int. Conf. Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.,? \\Q2014\\E", "shortCiteRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.", "year": 2014}, {"title": "Semi-supervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Weiss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2009}, {"title": "Scalable optimization for neighbor embedding for visualization", "author": ["Z. Yang", "J. Peltonen", "S. Kaski"], "venue": "Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Multiclass spectral clustering", "author": ["S.X. Yu", "J. Shi"], "venue": "In Proc. 9th Int. Conf. Computer Vision (ICCV\u201903),", "citeRegEx": "Yu and Shi.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Shi.", "year": 2003}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. of the 33rd ACM Conf. Research and Development in Information Retrieval (SIGIR", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Algorithm 778: L-BFGS-B: FORTRAN subroutines for large-scale bound-constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM Trans. Mathematical Software,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 29, "context": "Computationally, this essentially involves defining a high-dimensional feature space where each relevant image is represented by a vector, and then finding the closest points (nearest neighbors) to the vector for the query image, according to a suitable distance (Shakhnarovich et al., 2006).", "startOffset": 263, "endOffset": 291}, {"referenceID": 24, "context": "For example, one can use features such as SIFT (Lowe, 2004) or GIST (Oliva and Torralba, 2001) and the Euclidean distance for this purpose.", "startOffset": 47, "endOffset": 59}, {"referenceID": 27, "context": "For example, one can use features such as SIFT (Lowe, 2004) or GIST (Oliva and Torralba, 2001) and the Euclidean distance for this purpose.", "startOffset": 68, "endOffset": 94}, {"referenceID": 11, "context": "In practice, this is approximated, and a successful way to do this is binary hashing (Grauman and Fergus, 2013).", "startOffset": 85, "endOffset": 111}, {"referenceID": 11, "context": "over the weights of a linear SVM), and L(\u00b7) is a loss function that compares the codes for two images (often through their Hamming distance \u2016h(xn)\u2212 h(xm)\u2016) with the ground-truth value ynm that measures the affinity in the original space between the two images xn and xm (distance, similarity or other measure of neighborhood; Grauman and Fergus, 2013).", "startOffset": 270, "endOffset": 351}, {"referenceID": 1, "context": "Examples of these objective functions (described below) include models developed for dimension reduction, be they spectral such as Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Supervised Hashing with Kernels (KSH) (Liu et al.", "startOffset": 151, "endOffset": 176}, {"referenceID": 28, "context": "Examples of these objective functions (described below) include models developed for dimension reduction, be they spectral such as Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Supervised Hashing with Kernels (KSH) (Liu et al.", "startOffset": 206, "endOffset": 229}, {"referenceID": 4, "context": "Examples of these objective functions (described below) include models developed for dimension reduction, be they spectral such as Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Supervised Hashing with Kernels (KSH) (Liu et al.", "startOffset": 274, "endOffset": 299}, {"referenceID": 22, "context": "Examples of these objective functions (described below) include models developed for dimension reduction, be they spectral such as Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Supervised Hashing with Kernels (KSH) (Liu et al., 2012), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) or Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al.", "startOffset": 463, "endOffset": 481}, {"referenceID": 17, "context": ", 2012), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) or Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 34, "context": ", 2012), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) or Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al., 2012).", "startOffset": 139, "endOffset": 158}, {"referenceID": 25, "context": "This would be guaranteed to converge to an optimum under mild conditions (for example, Wolfe conditions on the line search), which would be global if the objective is convex and local otherwise (Nocedal and Wright, 2006).", "startOffset": 194, "endOffset": 220}, {"referenceID": 35, "context": "Binarizing the codes has been done in different ways, from simply rounding them to {\u22121,+1} using zero as threshold (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011, 2012), to optimally finding a threshold (Liu et al.", "startOffset": 115, "endOffset": 179}, {"referenceID": 38, "context": "Binarizing the codes has been done in different ways, from simply rounding them to {\u22121,+1} using zero as threshold (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011, 2012), to optimally finding a threshold (Liu et al.", "startOffset": 115, "endOffset": 179}, {"referenceID": 21, "context": ", 2011, 2012), to optimally finding a threshold (Liu et al., 2011; Strecha et al., 2012), to rotating the continuous codes so that thresholding introduces less error (Yu and Shi, 2003; Gong et al.", "startOffset": 48, "endOffset": 88}, {"referenceID": 30, "context": ", 2011, 2012), to optimally finding a threshold (Liu et al., 2011; Strecha et al., 2012), to rotating the continuous codes so that thresholding introduces less error (Yu and Shi, 2003; Gong et al.", "startOffset": 48, "endOffset": 88}, {"referenceID": 37, "context": ", 2012), to rotating the continuous codes so that thresholding introduces less error (Yu and Shi, 2003; Gong et al., 2013).", "startOffset": 85, "endOffset": 122}, {"referenceID": 10, "context": ", 2012), to rotating the continuous codes so that thresholding introduces less error (Yu and Shi, 2003; Gong et al., 2013).", "startOffset": 85, "endOffset": 122}, {"referenceID": 0, "context": "Related work Although one can construct hash functions without training data (Andoni and Indyk, 2008; Kulis and Grauman, 2012), we focus on methods that learn the hash function given a training set, since they perform better, and our emphasis is in optimization.", "startOffset": 77, "endOffset": 126}, {"referenceID": 18, "context": "Related work Although one can construct hash functions without training data (Andoni and Indyk, 2008; Kulis and Grauman, 2012), we focus on methods that learn the hash function given a training set, since they perform better, and our emphasis is in optimization.", "startOffset": 77, "endOffset": 126}, {"referenceID": 22, "context": "These create an affinity matrix for a subset of training points based on their distances (unsupervised) or labels (supervised) and combine it with a loss function (Liu et al., 2012; Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Lin et al., 2013, 2014).", "startOffset": 163, "endOffset": 255}, {"referenceID": 17, "context": "These create an affinity matrix for a subset of training points based on their distances (unsupervised) or labels (supervised) and combine it with a loss function (Liu et al., 2012; Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Lin et al., 2013, 2014).", "startOffset": 163, "endOffset": 255}, {"referenceID": 26, "context": "These create an affinity matrix for a subset of training points based on their distances (unsupervised) or labels (supervised) and combine it with a loss function (Liu et al., 2012; Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Lin et al., 2013, 2014).", "startOffset": 163, "endOffset": 255}, {"referenceID": 17, "context": "For example, Binary Reconstructive Embeddings (Kulis and Darrell, 2009) use alternating optimization over the weights of the hash functions.", "startOffset": 46, "endOffset": 71}, {"referenceID": 22, "context": "Supervised Hashing with Kernels (Liu et al., 2012) learns hash functions sequentially by considering the difference between the inner product of the codes and the corresponding element of the affinity matrix.", "startOffset": 32, "endOffset": 50}, {"referenceID": 35, "context": "The codes can be found by relaxing the problem and binarizing its solution (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011), or by approximately solving for the binary codes using some form of alternating optimization (possibly combined with GraphCut), as in two-step hashing (Lin et al.", "startOffset": 75, "endOffset": 133}, {"referenceID": 38, "context": "The codes can be found by relaxing the problem and binarizing its solution (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011), or by approximately solving for the binary codes using some form of alternating optimization (possibly combined with GraphCut), as in two-step hashing (Lin et al.", "startOffset": 75, "endOffset": 133}, {"referenceID": 21, "context": "The codes can be found by relaxing the problem and binarizing its solution (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011), or by approximately solving for the binary codes using some form of alternating optimization (possibly combined with GraphCut), as in two-step hashing (Lin et al.", "startOffset": 75, "endOffset": 133}, {"referenceID": 9, "context": ", 2011), or by approximately solving for the binary codes using some form of alternating optimization (possibly combined with GraphCut), as in two-step hashing (Lin et al., 2013, 2014; Ge et al., 2014), or by using relaxation in other ways (Liu et al.", "startOffset": 160, "endOffset": 201}, {"referenceID": 22, "context": ", 2014), or by using relaxation in other ways (Liu et al., 2012; Norouzi and Fleet, 2011).", "startOffset": 46, "endOffset": 89}, {"referenceID": 26, "context": ", 2014), or by using relaxation in other ways (Liu et al., 2012; Norouzi and Fleet, 2011).", "startOffset": 46, "endOffset": 89}, {"referenceID": 4, "context": "A representative example is the elastic embedding (Carreira-Perpi\u00f1\u00e1n, 2010), where L(zn, zm; ynm) has the form: y nm \u2016zn \u2212 zm\u2016 2 + \u03bby nm exp (\u2212\u2016zn \u2212 zm\u2016 2 ), \u03bb > 0 (2) where the first term tries to project true neighbors (having y nm > 0) close together, while the second repels all non-neighbors\u2019 projections (having y nm > 0) from each other.", "startOffset": 50, "endOffset": 75}, {"referenceID": 1, "context": "Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000) result from replacing the second term above with a constraint that fixes the scale of Z, which results in an eigenproblem rather than a nonlinear optimization, but also produces more distorted embeddings.", "startOffset": 20, "endOffset": 45}, {"referenceID": 28, "context": "Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000) result from replacing the second term above with a constraint that fixes the scale of Z, which results in an eigenproblem rather than a nonlinear optimization, but also produces more distorted embeddings.", "startOffset": 75, "endOffset": 98}, {"referenceID": 4, "context": "Optimizing nonlinear embeddings is quite challenging, but much progress has been done recently (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 95, "endOffset": 241}, {"referenceID": 32, "context": "Optimizing nonlinear embeddings is quite challenging, but much progress has been done recently (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 95, "endOffset": 241}, {"referenceID": 36, "context": "Optimizing nonlinear embeddings is quite challenging, but much progress has been done recently (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 95, "endOffset": 241}, {"referenceID": 33, "context": "Optimizing nonlinear embeddings is quite challenging, but much progress has been done recently (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 95, "endOffset": 241}, {"referenceID": 35, "context": "Although these models were developed to produce continuous projections, they have been successfully used for binary hashing too by truncating their codes (Weiss et al., 2009; Zhang et al., 2010) or using the two-step approach of (Lin et al.", "startOffset": 154, "endOffset": 194}, {"referenceID": 38, "context": "Although these models were developed to produce continuous projections, they have been successfully used for binary hashing too by truncating their codes (Weiss et al., 2009; Zhang et al., 2010) or using the two-step approach of (Lin et al.", "startOffset": 154, "endOffset": 194}, {"referenceID": 17, "context": "Binary Reconstructive Embeddings (Kulis and Darrell, 2009) uses ( b \u2016zn \u2212 zm\u2016 2 \u2212 ynm) 2 where ynm = 1 2 \u2016xn \u2212 xm\u2016 2 .", "startOffset": 33, "endOffset": 58}, {"referenceID": 34, "context": "The exponential variant of SPLH (Wang et al., 2012) proposed by Lin et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 15, "context": "Binary Reconstructive Embeddings (Kulis and Darrell, 2009) uses ( b \u2016zn \u2212 zm\u2016 2 \u2212 ynm) 2 where ynm = 1 2 \u2016xn \u2212 xm\u2016 2 . The exponential variant of SPLH (Wang et al., 2012) proposed by Lin et al. (2013) (which we call eSPLH) uses exp(\u2212 1 b ynmz T nzn).", "startOffset": 34, "endOffset": 201}, {"referenceID": 25, "context": "Second, we solve the constrained problem using a penalty method, such as the quadratic-penalty or augmented Lagrangian (Nocedal and Wright, 2006).", "startOffset": 119, "endOffset": 145}, {"referenceID": 19, "context": "Here, we try two different approaches (Lin et al., 2013, 2014) with some modifications. \u2022 Optimize the hash function h given binary codes Z. This reduces to training b binary classifiers using X as inputs and Z as targets. This is very similar to the two-step (TSH) approach of Lin et al. (2013), except that the latter learns the codes Z in isolation, rather than given the current hash function, so iterating the two-step approach would change nothing, and it does not optimize the loss L.", "startOffset": 39, "endOffset": 296}, {"referenceID": 19, "context": "Fortunately, some recent works have proposed practical approaches for this problem based on alternating optimization: a quadratic surrogate method (Lin et al., 2013), and a GraphCut method (Lin et al.", "startOffset": 147, "endOffset": 165}, {"referenceID": 20, "context": ", 2013), and a GraphCut method (Lin et al., 2014).", "startOffset": 31, "endOffset": 49}, {"referenceID": 19, "context": "Solution using a quadratic surrogate method (Lin et al., 2013) This is based on the fact that any loss function that depends on the Hamming distance of two binary variables can be equivalently written as a quadratic function of those two binary variables (Lin et al.", "startOffset": 44, "endOffset": 62}, {"referenceID": 19, "context": ", 2013) This is based on the fact that any loss function that depends on the Hamming distance of two binary variables can be equivalently written as a quadratic function of those two binary variables (Lin et al., 2013).", "startOffset": 200, "endOffset": 218}, {"referenceID": 19, "context": "Fortunately, some recent works have proposed practical approaches for this problem based on alternating optimization: a quadratic surrogate method (Lin et al., 2013), and a GraphCut method (Lin et al., 2014). In both cases, this would correspond to the first step in the two-step hashing of Lin et al. (2013). In both the quadratic surrogate and the GraphCut method, the starting point is to apply alternating optimization over the ith bit of all points given the remaining bits are fixed for all points (for i = 1, .", "startOffset": 148, "endOffset": 309}, {"referenceID": 19, "context": "Lin et al. (2013) show that l(z1, z2) can be replaced by a binary quadratic function l(z1, z2) = 1 2z1z2 ( l \u2212 l )", "startOffset": 0, "endOffset": 18}, {"referenceID": 39, "context": "which we solve using L-BFGS-B (Zhu et al., 1997).", "startOffset": 30, "endOffset": 48}, {"referenceID": 20, "context": "Solution using a GraphCut algorithm (Lin et al., 2014) To optimize over the ith bit (given all the other bits are fixed), we have to minimize eq.", "startOffset": 36, "endOffset": 54}, {"referenceID": 15, "context": "We can apply the GraphCut algorithm (Boykov and Kolmogorov, 2003, 2004; Kolmogorov and Zabih, 2003), as proposed by the FastHash algorithm of Lin et al.", "startOffset": 36, "endOffset": 99}, {"referenceID": 2, "context": "We can apply the GraphCut algorithm (Boykov and Kolmogorov, 2003, 2004; Kolmogorov and Zabih, 2003), as proposed by the FastHash algorithm of Lin et al. (2014). This proceeds as follows.", "startOffset": 37, "endOffset": 160}, {"referenceID": 22, "context": "We use the KSH (3) (Liu et al., 2012) and eSPLH (Wang et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 34, "context": ", 2012) and eSPLH (Wang et al., 2012) loss functions.", "startOffset": 18, "endOffset": 37}, {"referenceID": 8, "context": "As hash functions (for each bit), we use linear SVMs (trained with LIBLINEAR; Fan et al., 2008) and kernel SVMs We use the following labeled datasets (all using the Euclidean distance in feature space): (1) CIFAR (Krizhevsky, 2009) contains 60 000 images in 10 classes.", "startOffset": 53, "endOffset": 95}, {"referenceID": 16, "context": ", 2008) and kernel SVMs We use the following labeled datasets (all using the Euclidean distance in feature space): (1) CIFAR (Krizhevsky, 2009) contains 60 000 images in 10 classes.", "startOffset": 125, "endOffset": 143}, {"referenceID": 27, "context": "We use D = 320 GIST features (Oliva and Torralba, 2001) from each image.", "startOffset": 29, "endOffset": 55}, {"referenceID": 23, "context": "(2) Infinite MNIST (Loosli et al., 2007).", "startOffset": 19, "endOffset": 40}, {"referenceID": 17, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Liu et al., 2012; Lin et al., 2013).", "startOffset": 131, "endOffset": 217}, {"referenceID": 26, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Liu et al., 2012; Lin et al., 2013).", "startOffset": 131, "endOffset": 217}, {"referenceID": 22, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Liu et al., 2012; Lin et al., 2013).", "startOffset": 131, "endOffset": 217}, {"referenceID": 19, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Liu et al., 2012; Lin et al., 2013).", "startOffset": 131, "endOffset": 217}, {"referenceID": 19, "context": "We compare 4 ways of optimizing the loss function: quad (Lin et al., 2013), cut (Lin et al.", "startOffset": 56, "endOffset": 74}, {"referenceID": 20, "context": ", 2013), cut (Lin et al., 2014), MACquad and MACcut.", "startOffset": 13, "endOffset": 31}, {"referenceID": 19, "context": "We compare MACquad and MACcut with Two-Step Hashing (quad) (Lin et al., 2013), FastHash (cut) (Lin et al.", "startOffset": 59, "endOffset": 77}, {"referenceID": 20, "context": ", 2013), FastHash (cut) (Lin et al., 2014), Hashing with Kernels (KSH) (Liu et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 22, "context": ", 2014), Hashing with Kernels (KSH) (Liu et al., 2012), Iterative Quantization (ITQ) (Gong et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 10, "context": ", 2012), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) and Self-Taught Hashing (STH) (Zhang et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 17, "context": ", 2013), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) and Self-Taught Hashing (STH) (Zhang et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 38, "context": ", 2013), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) and Self-Taught Hashing (STH) (Zhang et al., 2010).", "startOffset": 104, "endOffset": 124}, {"referenceID": 19, "context": "5 Discussion Two-step approaches vs the MAC algorithm for affinity-based loss functions The two-step approach of Two-Step Hashing (Lin et al., 2013) and FastHash (Lin et al.", "startOffset": 130, "endOffset": 148}, {"referenceID": 20, "context": ", 2013) and FastHash (Lin et al., 2014) is a significant advance in finding good codes for binary hashing, but it also causes a maladjustment between the codes and the hash function, since the codes were learned without knowledge of what hash function would use them.", "startOffset": 21, "endOffset": 39}, {"referenceID": 14, "context": "A similar, well-known situation arises in feature selection for classification (Kohavi and John, 1998).", "startOffset": 79, "endOffset": 102}, {"referenceID": 6, "context": "Finally, note that the method of auxiliary coordinates can be used also to learn an out-of-sample mapping for a continuous embedding (Carreira-Perpi\u00f1\u00e1n and Vladymyrov, 2015), such as the elastic embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008)\u2014rather than to learn hash functions for a discrete embedding, as is our case in binary hashing.", "startOffset": 133, "endOffset": 173}, {"referenceID": 4, "context": "Finally, note that the method of auxiliary coordinates can be used also to learn an out-of-sample mapping for a continuous embedding (Carreira-Perpi\u00f1\u00e1n and Vladymyrov, 2015), such as the elastic embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008)\u2014rather than to learn hash functions for a discrete embedding, as is our case in binary hashing.", "startOffset": 205, "endOffset": 230}, {"referenceID": 4, "context": "In particular, solving the Z step can be done efficiently with large datasets by using N -body methods and efficient optimization techniques (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 141, "endOffset": 287}, {"referenceID": 32, "context": "In particular, solving the Z step can be done efficiently with large datasets by using N -body methods and efficient optimization techniques (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 141, "endOffset": 287}, {"referenceID": 36, "context": "In particular, solving the Z step can be done efficiently with large datasets by using N -body methods and efficient optimization techniques (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 141, "endOffset": 287}, {"referenceID": 33, "context": "In particular, solving the Z step can be done efficiently with large datasets by using N -body methods and efficient optimization techniques (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 141, "endOffset": 287}, {"referenceID": 5, "context": "Binary autoencoder vs affinity-based loss, trained with MAC The method of auxiliary coordinates has also been applied in the context of binary hashing to a different objective function, the binary autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015): EBA(h, f) = N", "startOffset": 214, "endOffset": 260}, {"referenceID": 5, "context": "Neighborhood relations are only indirectly preserved by autoencoders (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), whose direct aim is to reconstruct its inputs and thus to learn the data manifold (imperfectly, because of the binary projection layer).", "startOffset": 69, "endOffset": 115}, {"referenceID": 13, "context": "We use the SIFT1M dataset (J\u00e9gou et al., 2011), which contains N = 1 000 000 training high-resolution color images and 10 000 test images, each represented by D = 128 SIFT features.", "startOffset": 26, "endOffset": 46}, {"referenceID": 10, "context": "In the second type, we use purely unsupervised methods (not based on similar/dissimilar affinities): thresholded PCA (tPCA), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al.", "startOffset": 154, "endOffset": 173}, {"referenceID": 5, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al.", "startOffset": 33, "endOffset": 79}, {"referenceID": 35, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al., 2009), AnchorGraph Hashing (AGH) (Liu et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 21, "context": ", 2009), AnchorGraph Hashing (AGH) (Liu et al., 2011), and Spherical Hashing (SPH) (Heo et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 12, "context": ", 2011), and Spherical Hashing (SPH) (Heo et al., 2012).", "startOffset": 37, "endOffset": 55}, {"referenceID": 4, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al., 2009), AnchorGraph Hashing (AGH) (Liu et al., 2011), and Spherical Hashing (SPH) (Heo et al., 2012). The results are again in general agreement with the conclusions in the main paper. Comparison using code utilization Fig. 12 shows the results (for all methods on SIFT1M) in effective number of bits beff. This is a measure of code utilization of a hash function introduced by Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei (2015), defined as the entropy of the code distribution.", "startOffset": 34, "endOffset": 541}, {"referenceID": 4, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al., 2009), AnchorGraph Hashing (AGH) (Liu et al., 2011), and Spherical Hashing (SPH) (Heo et al., 2012). The results are again in general agreement with the conclusions in the main paper. Comparison using code utilization Fig. 12 shows the results (for all methods on SIFT1M) in effective number of bits beff. This is a measure of code utilization of a hash function introduced by Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei (2015), defined as the entropy of the code distribution. That is, given the N codes z1, . . . , zN \u2208 {0, 1} b for the training set, we consider them as samples of a distribution over the 2 possible codes. The entropy of this distribution, measured in bits, is between 0 (when all N codes are equal) and min(b, log2 N) (when all N codes are distributed as uniformly as possible). We do the same for the test set. Although code utilization correlates to some extent with precision/recall when ranking different methods, a large beff does not guarantee a good hash function, and indeed, tPCA (which usually achieves a low precision compared to the state-of-theart) typically achieves the largest beff; see the discussion in Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei (2015). 15", "startOffset": 34, "endOffset": 1301}], "year": 2016, "abstractText": "In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as a corrected, iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.", "creator": "LaTeX with hyperref package"}}}