{"id": "1502.01176", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2015", "title": "Learning Local Invariant Mahalanobis Distances", "abstract": "For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance.", "histories": [["v1", "Wed, 4 Feb 2015 12:27:04 GMT  (112kb,D)", "http://arxiv.org/abs/1502.01176v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ethan fetaya", "shimon ullman"], "accepted": true, "id": "1502.01176"}, "pdf": {"name": "1502.01176.pdf", "metadata": {"source": "CRF", "title": "Learning Local Invariant Mahalanobis Distances", "authors": ["Ethan Fetaya", "Shimon Ullman"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is not that we are in a position to establish such a system. (...) It is not that we are getting involved in such a system. (...) It is not that we are getting involved in such a system. (...) It is not that we are getting involved in such a system. (...) It is not that we are getting involved in such a system. (...) It is that we are getting involved in such a system. (...) It is that we are getting involved in such a system. (...) It is that we are getting involved in such a system. (...) \"(...) It is that way.\" (...) It is that way. \"(...) It is that.\" (...) It is that way. \"(...) It is that. (...) It is that way."}, {"heading": "1 Related work", "text": "Metric learning is an active field of research with many algorithms generally divided into linear [21] learning a Mahalanobis distance, non-linear [14] learning a non-linear transformation and using L2 distance in transformed space, and local learning a metric per date. LMNN and MLMM [21] algorithms are considered the leading metric learning method. For a current comprehensive survey covering linear, non-linear and local methods, see [2].The copy SVM algorithm [18] can be considered a local measure of similarity. This is achieved by maximizing margins, using a linear model, and is poorly monitored like our work. Unlike copy SVM, we learn a Mahalanobis matrix and can learn an invariant metric. Another related work is PMLM [20], which also finds a local Mahalanobis metric for each data point."}, {"heading": "2 Local Mahalanobis", "text": "In this section we will show how a local Mahalanobis method can be applied very successfully in practice. (...) We will assume that we will obtain a uniform image belonging to a particular class, e.g. a person's face. (...) We will also receive a set of negative data that do not belong to this class, e.g. a set of facial images of different people. (...) We will use a local Mahalanobis metric for x0, M (x0) 0, where M 0 means M 0 is positively semi-defined. (...) For matricesM, N, we will name a set of facial images of different people. (...) We will use the pious standard of M 2 = 0, M 2 ij and of < M > the default product inner < M, N > = the MijNij. We will find a positive date x0 and the negative data x1."}, {"heading": "3 Local Invariant Mahalanobis", "text": "We will show how we can incorporate this knowledge into the local metrics that we are able to be resilient to the instability of pixel representation on these transformations. Various descriptors such as SIFT [17] offer a more robust representation and are highly successful in many computer applications. We will show in Section 4 that even if we use a relatively robust representation such as HOG, there is a significant impact.A natural path to mathematical transformation is that we are highly successful in many computer applications."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Running time", "text": "We compared the execution of the optimization with an SVM solver [5], with its solution as a semidefinitive problem and as a quadratic problem (loosening the semidefinitive constraint).The most important constraint when executing standard solvers is memory. Square or semidefinitive solvers require a constraint matrix, which in our case is a complete matrix of size n \u00b7 d2, where n is the number of samples and d is the data dimension.We tested all three approaches to solving problems such as square and semi-definitive programming for large matrices using only 5000 negative examples, as this has already led to a matrix of size 24.6 Gb. Currently, first-order methods such as ADMM [3] are the leading approaches to solving problems such as square and semi-definitive programming for large matrices. We used YALMIP for modeling and solving the square matrix [19].The time it took to execute this program as a definitive program as a semise4c program, which was semisese4\u00b1 SVM, and lasted the longest time of this program was 154se4sec."}, {"heading": "4.2 MNIST", "text": "The MNIST dataset is a well-known digital recognition dataset consisting of 28 x 28 grayscale images on which we perform the pre-processing twist. We calculated a local Mahalanobis distance and a local invariant Mahalanobis (using only negative examples) for each of the 60,000 training images. During the test period, we performed a Knn classification using k = 3, adding the Tanzformed images as positive training data. To show the importance of the invention goal, we also compare this with the SVM kernel, to which we add the transformed data as positive training data (as opposed to the way we use the moved data) and finally compared our results with the modern LMNN method (linear metric)."}, {"heading": "4.3 Labeling faces in the wild (LFW)", "text": "The LFW dataset is a sophisticated dataset containing 13,233 facial images of 5,749 different individuals with a high degree of variability.The LFW dataset is divided into 10 subsets when the task is to classify 600 pairs of images from one subset into equal / non-equal, using the other 9 subsets as training data. We perform the unattended LFW task where we do not use labels within the obtained training images, apart from the fact that they differ from the two test images. We used the aligned images [13] and displayed them using HOG features [8]. We compared our results with a cosmic similarity baseline, with example SVM and example SVM with shifts. We find that we cannot use LMNN or MLMNN on these data because we only have negative images with a single positive image."}, {"heading": "5 Summary", "text": "We demonstrated an efficient way to learn a local Mahalanobis metric taking into account a query date and a number of negative data points. We also demonstrated how to incorporate prior knowledge of our data, in particular the transformations to which it should be stable, and how to use it to learn locally invariant metrics. We demonstrated that our methods compete with leading methods, while applying them to other scenarios where methods such as LMNN and MLMNN cannot be used."}], "references": [{"title": "Learning a mahalanobis metric from equivalence constraints. JMLR", "author": ["A. Bar-Hillel", "T. Hertz", "N. Shental", "D. Weinshall"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "Chu", "B. Eric Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery", "author": ["C. Burges"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM Transactions on Intelligent Systems and Technology", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Learning optimally sparse support vector machines", "author": ["A. Cotter", "S. Shalev-Shwartz", "N. Srebro"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Information-theoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Image retrieval and classification using local distance functions", "author": ["A. Frome", "Y. Singer", "J. Malik"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval", "author": ["S. Hoi", "W. Liu", "Chang", "S.-F"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Learning to align from scratch", "author": ["G. Huang", "M.A. Mattar", "H. Lee", "E. Learned-Miller"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Non-linear metric learning", "author": ["D. Kedem", "S. Tyree", "K. Weinberger", "F. Sha"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Efficient learning of mahalanobis metrics for ranking", "author": ["D. Lim", "G. Lanckriet"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints. IJCV", "author": ["D. Lowe"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Ensemble of exemplar-svms for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A. Efros"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Operator splitting for conic optimization via homogeneous self-dual embedding", "author": ["B. O\u2019Donoghue", "E. Chu", "N. Parikh", "S. Boyd"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Parametric local metric learning for nearest neighbor classification", "author": ["J. Wang", "A. Woznica", "A. Kalousis"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Learning a mahalanobis distance metric for data clustering and classification", "author": ["S. Xiang", "F. Nie", "C. Zhang"], "venue": "Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 111, "endOffset": 118}, {"referenceID": 5, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 111, "endOffset": 118}, {"referenceID": 15, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "Various algorithms [21, 1, 9] differ by the objective through which they learn the matrix M from the data.", "startOffset": 19, "endOffset": 29}, {"referenceID": 0, "context": "Various algorithms [21, 1, 9] differ by the objective through which they learn the matrix M from the data.", "startOffset": 19, "endOffset": 29}, {"referenceID": 8, "context": "Various algorithms [21, 1, 9] differ by the objective through which they learn the matrix M from the data.", "startOffset": 19, "endOffset": 29}, {"referenceID": 9, "context": "One approach that can be used to overcome the first limitation is to use local distances [10] where we learn a unique distance function per training datum.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "In our current work we use a local approach inspired by the work on exemplar-SVM [18], that showed that using only negative examples can suffice for good performance.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "Metric learning is an active research field with many algorithms, generally divided into linear [21] which learn a Mahalanobis distance, non-linear [14] that learn a nonlinear", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "Metric learning is an active research field with many algorithms, generally divided into linear [21] which learn a Mahalanobis distance, non-linear [14] that learn a nonlinear", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "The LMNN and MLMM [21] algorithm are considered the leading metric learning method.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "For a recent comprehensive survey that covers linear, nonlinear and local methods see [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 17, "context": "The exemplar-SVM algorithm [18] can be seen as a local similarity measure.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "Another related work is PMLM [20], which also finds a local Mahalanobis metric for each data point.", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "The most common way to achieve invariance, or at least insensitivity, to a transformation in computer vision applications is by using hand-crafted descriptors such as SIFT [17] or HOG [8].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "The most common way to achieve invariance, or at least insensitivity, to a transformation in computer vision applications is by using hand-crafted descriptors such as SIFT [17] or HOG [8].", "startOffset": 184, "endOffset": 187}, {"referenceID": 14, "context": "Another way, used in convolutional networks [15], is by adding pooling and subsampling forcing the net to be insensitive to small translations.", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "Large margin methods have been very successful in metric learning [21], and more generally in machine learning, therefore, our algorithm will look for the PSD matrixM that maximizes the distance to the closest negative example", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "We will then see how this is equivalent to a kernel SVM problem with a quadratic kernel, and therefore can be solved easily with off-the-shelf SVM solvers such as LIBSVM [5].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "A well known observation arrising from the dual formulation of the SVM objective [4] is that the optimal solution M has the form", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "This bound on the rank can be improved by using sparse-SVM algorithms [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 16, "context": "Various descriptors such as SIFT [17] and HOG [8] offer a more robust representation, and have been highly successful in many computer vision applications.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "Various descriptors such as SIFT [17] and HOG [8] offer a more robust representation, and have been highly successful in many computer vision applications.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "1 Running time We compared running the optimization with an SVM solver [5], to solving it as a semidefinite problem and as a quadratic problem (relaxing the semidefinite constraint).", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "Currently first order methods, such as ADMM [3], are the leading approaches to solving problems such as quadratic and semidefinite programming for large matrices.", "startOffset": 44, "endOffset": 47}, {"referenceID": 18, "context": "We used YALMIP for modeling and solved using SCS [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "We used the aligned images [13] and represented using HOG features [8].", "startOffset": 27, "endOffset": 31}, {"referenceID": 7, "context": "We used the aligned images [13] and represented using HOG features [8].", "startOffset": 67, "endOffset": 70}], "year": 2015, "abstractText": "For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance. Metric learning is a machine learning task which learns a distance metric d(x, y) between data points, based on data instances. As distances play an important role in many machine learning algorithms, e.g. k-Nearest Neighbor and k-Means clustering, finding an appropriate metric for the task can improve performance considerably. This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few. A standard approach to metric learning is to learn a global Mahalanobis metric d(x, y)M = (x\u2212 y)M(x\u2212 y) (1) Where M is a positive semi-definite matrix (PSD). The PSD constraint only assures this is a pseudometric , but for simplicity we will not make this distinction. Various algorithms [21, 1, 9] differ by the objective through which they learn the matrix M from the data. As M is a PSD matrix, it can be written as M = LL and therefore d(x, y)M = (x\u2212 y)M(x\u2212 y) = ||x\u0303\u2212 \u1ef9||2 x\u0303 = Lx, \u1ef9 = Ly. This means that finding an optimal Mahalanobis distance is equivalent to finding the optimal linear transformation on the data, and then using L2 distance on the transformed data. This approach has two limitations, first it is limited to linear transformation. Second, it requires a large amount of labeled data. One approach that can be used to overcome the first limitation is to use local distances [10] where we learn a unique distance function per training datum. Local approaches do not produce, in general, a global metric (as they are usually not symmetrical) but are commonly considered metric learning nonetheless. These methods, in 1 ar X iv :1 50 2. 01 17 6v 1 [ cs .L G ] 4 F eb 2 01 5 general, need similar and dissimilar training data for each local metric. In our current work we use a local approach inspired by the work on exemplar-SVM [18], that showed that using only negative examples can suffice for good performance. The intuition behind this is that objects of the same class do not necessarily have to be similar, but objects from different classes must be dissimilar. We will show how to learn a local Mahalanobis distance that for each datum tries to keep the non-class as far away as possible. This approach can use a large amount of weakly supervised data, as in many cases negative examples are easier then positive examples to acquire. For example, if we are interested in face identification, we can learn a local metric around a query face image given a bank of train face images, which we only assume do not belong to the queried person. Unlike other metric learning methods, we will not need any labels on which image belongs to which person in the negative set. The intuition why Mahalanobis distances are the natural model for local metrics is simple. Assume we have some metric d(x, y) on the dataset and assume that it is smooth (at least continuously twice differentiable). From the metric properties we know that if we fix x and look at f(y) = d(y, x) then f has a global minimum at y = x. Applying second order Tylor approximation to f around x we get d(y, x) = f(y) \u2248 f(x) + (y \u2212 x)\u2207f(x)+ (y \u2212 x)\u2207f(x)(y \u2212 x) = (y \u2212 x)\u2207f(x)(y \u2212 x) (2) The equality holds since x is the global minimum with value f(x) = d(x, x) = 0, and this also implies that \u2207f(x) is positive semidefinite. While the Taylor approximation only holds for values of y close to x, as metric methods such as k-NN focus on similar objects the approximation should be good at the points of interest. This observation leads us to look for local matrices that are of the form of a Mahalanobis distance. We will first define our local Mahalanobis distance learning method as a semidefinite programming problem. We will then show how this problem can be solved efficiently without any costly matrix decompositions. This allows us to solve high dimensional problems that regular semidefinite solvers cannot handle. The second major contribution of this paper will be to show how invariant local matrices can be learned. In many cases we know there are simple transformations that our metric should not be sensitive to. For example, small translation and rotation on natural images. We know a priori that if x\u2032 = T (x), where T is the said transformation, then d(x, x\u2032) \u2248 0. We will show how this prior knowledge about our data can by incorporated by learning a local invariant metric. This also can be done in an efficient manner, and we will show that this improves performance in our experiments.", "creator": "LaTeX with hyperref package"}}}