{"id": "1502.02158", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2015", "title": "Learning Parametric-Output HMMs with Two Aliased States", "abstract": "In various applications involving hidden Markov models (HMMs), some of the hidden states are aliased, having identical output distributions. The minimality, identifiability and learnability of such aliased HMMs have been long standing problems, with only partial solutions provided thus far. In this paper we focus on parametric-output HMMs, whose output distributions come from a parametric family, and that have exactly two aliased states. For this class, we present a complete characterization of their minimality and identifiability. Furthermore, for a large family of parametric output distributions, we derive computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition and emission parameters. We illustrate our theoretical analysis by several simulations.", "histories": [["v1", "Sat, 7 Feb 2015 16:21:28 GMT  (128kb,D)", "http://arxiv.org/abs/1502.02158v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["roi weiss", "boaz nadler"], "accepted": true, "id": "1502.02158"}, "pdf": {"name": "1502.02158.pdf", "metadata": {"source": "CRF", "title": "Learning Parametric-Output HMMs with Two Aliased States", "authors": ["Roi Weiss", "Boaz Nadler"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "HMMs are a basic tool in the analysis of time series."}, {"heading": "2 Definitions & Problem Setup", "text": "In the second random variable Z, the quantitative variable Z denotes either Pr (Z = z = x), or the conditional density p (Z = z = x), or the conditional density p (Z = z = x), or the conditional density p (Z = z = x), depending on whether Z is discrete or continuous."}, {"heading": "3 Decomposing the transition matrix A", "text": "The most important tool in our analysis is a novel decomposition of the 2A-HMM transition part into its non-aliased and non-aliased parts. As shown in Lemma 1, the aliassociated part consists of three rank-one matrices corresponding to the dynamics of the output from, the input into and within the two aliassociated states. To this end, we introduce a pseudo-state n, which contains the two aliassociated states n \u2212 1 and n. We define the aliassociated states n \u2212 1 and n."}, {"heading": "4 Minimality and Identifiability", "text": "Two HMM H and H \u2032 are considered equivalent if their observed output sequences are not statistically distinguishable, namely PH \u2032 = PH. Likewise, an HMM H is minimal if there is no equivalent HMM with fewer states. Note that if H is not calibrated, assumptions (A1-A2) easily imply that it is also minimal [Leroux, 1992]. In this section, we present necessary and sufficient conditions for a 2A-HMM to be minimal and two minimum 2A-HMM to be equivalent. Finally, we derive necessary and sufficient conditions for a minimum 2A-HMM to be identifiable."}, {"heading": "4.1 Minimality", "text": "The minimality of an HMM is closely related to the concept of lumpability: Can hidden states be merged without affecting the distribution of PH? [Fredkin & Rice, 1986, White et al., 2000, Huang et al., 2014] In the following theorem, we give precise conditions for the minimality of a 2A-HMM.Theorem 1. Let H be a 2A-HMM satisfactory assumption (A1-A2) whose initial state X0 is distributed according to \u03c00 = (\u03c001, \u03c0 0 2,.) Then, (i) If there is a state X0 = 0 and \u03b20 6 = \u03b2 6 = \u03b2, then H is a minimum state X0 = 0."}, {"heading": "4.2 Identifiability", "text": "Recall that a HMMH has been studied in several papers (strictly) identifiable ifPH in two steps. First, we derive a novel geometric characterization of the set of all minimal HMMs equivalent to H, up to a permutation of the hidden states (theorem 2). Then, we specify necessary and sufficient conditions for H to be identifiable, namely for this set, which consists of only H itself (appendix C). In the process, we provide a simple procedure (algorithm 1) to determine whether a given minimum 2A HMM is identifiable or not.Equivalence between minimal 2A HMMs. Necessary and sufficient conditions for the equivalence of two minimal HMMs have been investigated in several papers."}, {"heading": "In addition, the set \u0393H is connected.", "text": "The feasible regions in the two possible cases (\u03b1n \u2212 1 \u2265 \u03b1n or \u03b1n \u2212 1 < \u03b1n) are presented in Appendix C, Fig.6.Strict identifiability. According to Lemma 2, the singleton group \"H = {(1, 0)}\" should be used for the strict identifiability of H. Due to the lack of space, sufficient and necessary prerequisites for this, as well as a corresponding simple procedure for determining whether a 2A-HMM is identifiable, Appendix C 2.Comment. While we go beyond the scope of this work, we note that instead of the strict identifiability of a given HMM, several papers have examined a different concept of generic identifiability [Allman et al., 2009], proving that under mild conditions the class of HMMs is generically identifiable. If we limit ourselves to the class of 2A-HMMs, our theorem 2 implies that this class is not generically identifiable."}, {"heading": "5 Learning a 2A-HMM", "text": "We assume that the HMM is either minimal and identifiable, as otherwise its parameters cannot be unambiguously determined. In this section, we examine the problems in determining whether the HMM will aliasing and restore its initial parameters and transition matrix A. We also assume that the HMM is either minimal and identifiable, as otherwise its parameters cannot be unambiguously determined. The proposed learning procedure consists of the following steps (see Fig.1): (i) Determine the number of initial parameters n \u2212 1 and estimate the initial parameters and transition matrix A, all with respect to (Yt) T \u2212 1t = 0. The proposed learning procedure consists of the following steps (see Fig.1): (i) Determine the number of initial parameters n \u2212 1 and estimate the n \u2212 1 unique initial parameters."}, {"heading": "5.1 Moments", "text": "To solve the problems (ii), (iii) and (iv) above, we first introduce the moment-based sizes that we will use (ii). (ii), (iii) and (iv). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (ij). (i.). (i.). (i.). (i.). (i.). (i.) (i. (i.) (i.) (i.) (i. (i.) (i.) (i. (i.) (i.) (i. (i.) (i. (i.) (i.) (i. (i.) (i.) (i. (i.) (i.) (i.) (i. (i.) (i.) (i. (i.) (i.) (i.) (i. (i.) (i.) (i. (i.) (i.) (i. (i.) (i.) (i.) (i.) (i.) (i. (i.) (i.) (i.) (i.) (i. (i.) (i.) (i. (i.) (i.) (i.). (i.). (i. (i.). (i.). (i.). (i. (i.).). (i.). (i. (i. (i.).). (i.). (i.). (i. (i. (i.). (i. (i.). (i.).). (i.). (i.). (i.). (i.). (i. (i.). (i.).). (i.). (i. (i."}, {"heading": "5.2 Detection of aliasing", "text": "We present this as a hypotheses test problem: H0: H is not compared with n \u2212 1 states \u2212 H1: H is compared with n states \u2212 H is compared with n states. We start with the following simple observation: Lemma 5. LetH becomes a minimal, unmediated HMM with n \u2212 1 states, satisfactory assumptions (A1-A3). Then R (2) = 0.In contrast, when H is then compared with 2 comparisons, we have R (2) = comparisons compared with T (2). Moreover, since the HMM is assumed to be minimal and assumes stationary distribution, Theorem 1 implies that both 6 = 0 and 6 = 0. Thus, R (2) is exactly a rank-1 matrix that we write."}, {"heading": "5.3 Identifying the aliased component \u03b8n\u22121", "text": "Assuming that the HMM has been recognized as a 2 aliasing, our next task, step (iv), is to identify the aliased component. Remember that we calculate the index i [n \u2212 1] of the aliased component by solving the following problem with the least squares: i = argmin i [n \u2212 1], c [n \u2212 1] and c [n \u2212 1]. The following result shows that this method is consistent. Lemma 7. Assumptions satisfactory for a minimal 2A HMM (A1-A3) with the aliased states n \u2212 1 and n, lim T \u00b2 Pr (i 6 = n \u2212 1) = 0."}, {"heading": "5.4 Learning the aliased transition matrix A", "text": "In view of the aliased component, we estimate the n \u00b7 n transition matrix A = \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 (8). Firstly, we must remember that by (22), R (2) = andre (andre) T = andre (andre). Since singular vectors are determined only up to scaling, we have the same result as \"andre\" and \"andre\" v, where \"andre\" is a still indeterminate constant. Considering that \"andre\" and \"andre\" of \"A\" take the form: A = C\u03b2A \"B + andre\" C\u03b2uc \"T\" and \"andre\" T\u03b2. \"(31) In view of the fact that\" andre \"andre\" andre \"andre\", u \"and\" v \"are known from previous steps, we must have the scalars\" andre, \"\u03b2\" and \"andre\" of \"Eq.\" (7). In relation to \"andre\" andre \"andre\" andre \"we have\" andre \"andre\" (3)."}, {"heading": "6 Numerical simulations", "text": "We present simulation results that illustrate the consistency of our methods. (W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W. (W = W = W = W). (W = W = W = W = W). (W = W = W = W = W). (W = W = W = W = W). (W = W = W = W = W = W). (W = W = W = W = W = W = W). (W = W = W = W = W = W = W = W). (W = W = W = W = W = W = W = W. (W = W = W = W = W = W). (W = W = W = W = W = W = W = W = W = W). (W = W = W = W = W = W = W = W = W = W = W = W = W = W = W. (W = W = W = W = W = W = W = W = W = W). (W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W). (W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W). (W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W). (W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W (W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W = W ="}, {"heading": "A Proofs for Section 3 (Decomposing A)", "text": "The proof of term 1. If we explicitly write each term into the decomposition (8) and sum it up, we find a correspondence between all entries to those of A. Let us consider as a representative example the last entry, n = P (n | n). The first term results in (C\u03b2A-B) [n, n] = (1 \u2212 \u03b2) P (n \u00b2 | n).The second term results in (C\u03b2\u03b4 outcT\u03b2) [n, n] = \u2212 \u03b2 (1 \u2212 \u03b2) inspoutn \u2212 1 = \u2212 \u03b2 (1 \u2212 \u03b2) (P (n \u00b2 | 1) \u2212 P (n \u00b2 | n)).The third term results in (b \u00b2 outcT\u03b2) [n, n] = \u2212 \u03b4inn \u2212 1 = \u2212 \u03b2 (n \u2212 1 \u2212 \u03b2) P (1 \u2212 \u03b2 (1 \u2212 \u03b2) and finally the fourth term \u2212 n \u2212 P (n \u00b2)."}, {"heading": "B Proofs for Section 4.1 (Minimality)", "text": "First of all, let us show that a minimum distribution is necessary when a minimum distribution is not possible. Let H = (A, II, II) is a minimum distribution. Let H = (A, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II,"}, {"heading": "C Proofs for Section 4.2 (Identifiability)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Proof of Theorem 2", "text": "Before characterizing \"H\" let us first have some intuitions about the role of \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" = \"H\" H \"=\" H \"=\" H \"=\" H \"=\" H \"=\" H \"H\" = \"H\" H \"=\" H \"H\" = \"H\" H \"=\" H \"H\" = \"H\" and \"H\" (H \"): H,\" H \"and\" H \"(H\"): \"H\" (H \")."}, {"heading": "C.2 Conditions for |\u0393H | = 1", "text": "First of all, let us write (\u03c4n \u2212 1, \u03c4n) = (1, 0) + (\u2206 \u03c4n \u2212 1, \u0445 \u03c4n).We characterize the conditions for the geometric constraints on the input of the transitional matrix A on the basis of the actual neighbourhood, in order to ensure that the exact determination of the AH (1 + \u0445n \u2212 1, vastness control control control over the input of the transitional matrix A, in order to ensure the exact determination of the AH (1 + \u0445n \u2212 1, vastness control control over the input of the AH), vastness control over the input of the AH (1 + \u0445n \u2212 1, vastness control over the input of the AH)."}, {"heading": "C.3 Examples.", "text": "We demonstrate our algorithm 1 for determining the identifiability of 2A-HMM on the 2A-HMM shown in section 6 (left). If we go through the steps of algorithm 1, we get the following diagrams for the effectively realizable region: from Table 1: A1.3 = 0 A1.4 6 = 0 from Table 1: A2.4 = 0 A2.3 6 = 0 from Table 2: from Table 2: 2 = 0 =. Since their intersection leads to a point like the diagram, this 2A-HMM is identifiable. More generally, with minimal stationary 2A-HMM assumptions (A1A2) with aliased states n and n \u2212 1, a sufficient condition for uniformity is the following constraints on the permitted transitions between the hidden states: in \u2212 n \u2212 1, jn \u2212 n \u2212 jn, in jn, jn \u2192 in jn \u2192 in jn \u2192 n \u2192 in the states mentioned above."}, {"heading": "D Proofs for Section 5 (Learning)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Proof of Lemma 3", "text": "The assertion in (21) that M (1) = BAC\u03b2 = A \u00b2 follows directly from (14) and (16) results from the fact that the final equality is that (In \u2212 C\u03b2B) = bcT\u03b2 (BAC\u03b2 = BA (In \u2212 C\u03b2B) AC\u03b2 = BAbcT\u03b2AC\u03b2 (In \u2212 C\u03b2B) = bcT\u03b2. Since BAb = \u03b4 out and cT\u03b2AC\u03b2 = (\u03b4 in) T, we have claimed R (2) = \u03b4out (\u03b4in) T as in (22). For (23) we have claimed R (3) = BAAAC\u03b2 \u2212 BAAC\u03b2 \u2212 BAC\u03b2 + BAC\u03b2 \u00b7 BAC\u03b2 = BA (In \u2212 C\u03b2B) A (In \u2212 C\u03b2B) AC\u03b2 (In \u2212 C\u03b2) (In \u2212 B) (In \u2212 B\u03b2) (In \u2212 B)."}, {"heading": "D.2 Proof of Lemma 4", "text": "The assumption (A2) in combination with the fact that the HMM has a finite number of states implies that the HMM is geometrically ergodic: there are parameters G < \u221e and vice versa [0, 1), so that we can apply the concentration limit given in Kontorovich & Wei\u00df [2014]: Theorem 4. Let Y = Y0,..., YT \u2212 1, YT \u2212 YT be the output of an HMM with transition matrixA and output parameters. Suppose A is geometrically ergodic with constants G. Let F: (Y0,., YT \u2212 1) 7 \u2192 R be the output of a HMM with transition matrixA and output parameters. Suppose A is geometrically ergodic with constants G."}, {"heading": "D.3 Proof of Lemma 6", "text": "If you combine Weyl's theorem [Stewart & Sun, 1990] with Lemma 4, there is a high probability of getting 1 < R (2) - R (2) - F = OP (T \u2212 12), remember that under the null hypothesis we have H0 1 = 0. So we have a high probability of 1 < T \u2212 1 2, for some 1 < < < 12, with hT = chT \u2212 12 +, in CaseH0: 1 < hT \u2212 1 < 2, i.e. for each ch > 0 and 0 < < 12, with hT = chT \u2212 12 +, in CaseH0: 1 < hT in CaseH1: 3 > hT, with a high probability."}, {"heading": "D.4 Proof of Lemma 7", "text": "We define the following scoring function for all i-points (n \u2212 1), score (i) = 1, score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n \u2212 1), score (n), (n), (n), (n), (n), (n), (n), (1, 1, 1, 1, (n), 1, (n), 1, (n), 1, (n), 1, (n), 1, (n), 1, (n), (n), 1, (n), (n), 1, (n), 1, (n), (n), 1, (n), (n), 1, 1, (n), 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,"}, {"heading": "D.5 Estimating \u03b3 and \u03b2", "text": "As discussed in Section 5.4, this is done by searching for a solution that ensures the non-negativity of (33), namely A \"H,\" (63), (0), where A \"H,\" (0 \"), (0\"), (0 \"), (0\"), (0 \"), (0\"), (0 \"), (0\"), (0 \"), (0\"), (0 \"), (0\"), (0 \"), (0\"), (0 \"), (0\"), \"(0\"), \"(0\"), \"(0\"), \"(0\"), \"(\"), \"(\"), \"(\"), \"(\"), \"(\"), \"(\"), \"(\"), \"(\"), \"(\"), \"(\"), \"(\", \",\", \",\" (\"), (\", \",\"), (\",\", \",\" (\"), (\", \",\"), (\",\", \",\", \"(\"), (\",\"), (\",\", \",\", (\"), (\", \",\", \",\", \",\", \", (\"), (\"), (), (), (), (), (), (), (), (), (), (), (), (\", (), (), (), (\", (), (),\", (), (), (), (), (\", (),\", (), \", (),\", (), ((), (), (), (, (, (), (), (), (, (, (),), (, (), (, (),), (, (,), (, (, (), (), (,), (), (,), (),), (, (), (, (, (),), (,), (), (, (, (, (),),), (), (, ("}, {"heading": "D.6 Proof of Theorem 3", "text": "Recall the definitions of A \"H\" (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (\"H\") (H \") (H) (\" H \") (H) (\" H \") (H\") (H) (\") (\" H \") (H) (\") (\"H\") (H) (\") (\" H) (H) (\") (\" H (\") (H) (H) (\" (H) (H) (\") (H) (H) (\" (H) (H) (\") (H) (H) (\" (H) (\"(H)\" (H) \"(\" (H) \"(H)\" (\"(H)\" (H) \"(\" (H) \"(H)\" (H \"(\" (H) \"(\" (H) \"(H)\" (\"(H)\" (H) \"(H\" (H \"(H)\" (\"(H)\" (\"(H)\" (H) \"(H\" (\"(H)\" (\"(H)\" (H) (H) (H) (H \"(\" (H) (H) (H) (H) (\"(\" (H) (\"(H) (H) (\" (H) (H) (H) (H) (\"(\" (H) (\"(H\" (H) (H) (\"(H) (H) (H\" (\"(H) (H) (H) (H\" (\"(\" (H) (\"(H) (H) (\" (H) (H) (\"(H\" (H) (H \"(H) (H"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>In various applications involving hidden Markov models (HMMs), some of<lb>the hidden states are aliased, having identical output distributions. The minimal-<lb>ity, identifiability and learnability of such aliased HMMs have been long standing<lb>problems, with only partial solutions provided thus far. In this paper we focus<lb>on parametric-output HMMs, whose output distributions come from a parametric<lb>family, and that have exactly two aliased states. For this class, we present a com-<lb>plete characterization of their minimality and identifiability. Furthermore, for a<lb>large family of parametric output distributions, we derive computationally efficient<lb>and statistically consistent algorithms to detect the presence of aliasing and learn<lb>the aliased HMM transition and emission parameters. We illustrate our theoretical<lb>analysis by several simulations.", "creator": "TeX"}}}