{"id": "1502.02206", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2015", "title": "Learning to Search Better than Your Teacher", "abstract": "Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor?", "histories": [["v1", "Sun, 8 Feb 2015 03:18:50 GMT  (975kb,D)", "https://arxiv.org/abs/1502.02206v1", null], ["v2", "Wed, 20 May 2015 05:48:10 GMT  (366kb,D)", "http://arxiv.org/abs/1502.02206v2", "In ICML 2015"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["kai-wei chang", "akshay krishnamurthy", "alekh agarwal", "hal daum\u00e9 iii", "john langford"], "accepted": true, "id": "1502.02206"}, "pdf": {"name": "1502.02206.pdf", "metadata": {"source": "META", "title": "Learning to Search Better than Your Teacher", "authors": ["Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal"], "emails": ["KCHANG10@ILLINOIS.EDU", "AKSHAYKR@CS.CMU.EDU", "ALEKHA@MICROSOFT.COM", "HAL@UMIACS.UMD.EDU", "JCL@MICROSOFT.COM"], "sections": [{"heading": null, "text": "We provide a new search algorithm, LOLS, which performs well compared to the reference policy, but also guarantees a low remorse compared to deviations from the learned policy: a local optimism guarantee. Consequently, LOLS can improve the reference policy in contrast to previous algorithms, allowing us to develop structured contextual bandits, a partially information-structured prediction setting with many potential applications."}, {"heading": "1. Introduction", "text": "In fact, it is as if most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is as if they are able to understand the rules of the market. (...) It is not as if they are able to abide by the rules of the market. (...) It is not as if they are able to abide by the rules of the market. (...) It is as if they abide by the rules of the market. (...) It is as if they are able to abide by the rules of the market. (...) It is as if they do not abide by the rules of the market. (...) It is as if they abide by the rules of the market. (...) It is as if they abide by the rules of the market and the rules of the market. (...)"}, {"heading": "2. Learning to Search", "text": "A structured prediction problem consists of an input space X, an output space Y, a fixed but unknown distribution D over X-Y, and a non-negative loss function (y-p, y-p) \u2192 R-0, which measures the distance between the true (y-p) and predicted (y-p) output. However, the goal of structured learning is to establish an initial state b (which we will consider the key difference between (1) contextual bandits is that the action space is exponentially large (in the length of the trajectories in the search space); and of (2) amplification of learning before we learn."}, {"heading": "3. Theoretical Analysis", "text": "In this section, we analyze LOLS and answer the questions raised in Section 1 (Ross Bagell, 2010). In this section, we use \u03c0 to describe the average policy we have received by first choosing n [1, N] and then acting according to the policy we have learned. We begin by discussing the decisions of the roll-in and roll-out policy. Table 1 summarizes the results of using different strategies for roll-out and roll-out measures, which are much more difficult than making other bad decisions and then dealing with the policy we have learned because the learner is blind to the reference policy. It reduces the structured learning problem to an affirmation problem, which is much more difficult to build intuition, we point out two other bad cases. Roll-in with a reference policy causes the government distribution to be unrealistically good. As a result, the learned policy never learns to correct previous mistakes that work badly when a corresponding discussion can be found in Theorem 2.1 in (Ross Bagell)."}, {"heading": "3.2. Regret Guarantees", "text": "It is the number of decisions that are required to reach an end state. Finally, we use the method to denote the distribution across states in time t (st, p) = if we act according to the policy. The expected loss of a policy is: J (p) = It is expected that a policy will be in time t (st, p) = It is expected that a policy will be in time t (st, p) = It is expected that a policy will be in time t (st) = It will not be in time t (st) = It is expected that a policy will be in time t (t, p) = It will not be in time t (p) = It will be in time t (s, p) = It will be in time t (t) = It will be in time t (t), in which we will take the expected cost of rolling in time t (st) = It is expected that a policy will be in time t (t, p) = It will not be in time t (t, p) = It will be in time t (s, p) = It will be in time t (t) = It will be in time t (t), in which we will take the expected cost of rolling in the time in which we will take the measures and then complete the time. Our main regret guarantee for algorithm 1 shows that LOLS is a combination of regret in the reference policy and regret in time t."}, {"heading": "3.3. Hardness of local optimality", "text": "In this section, we show that the process of achieving a local optimum (with one-level deviations) can be exponentially slow if the initial start policy is arbitrary, reflecting the hardship of learning to look for problems when it is endowed with a bad reference policy, even if local rather than global optimality is used as a yardstick. We establish this lower limit for a class of algorithms that is much more powerful than LOLS. We start by defining a search space and a political class. Our search space consists of trajectories of length T, with 2 actions available at each step of the orbit. We use 0 and 1 to index the two actions. We consider measures whose only characteristic in a state is the depth of the state in the orbit, meaning that the measures taken by a state depend only on t. Consequently, any policy can be indexed by a bit string of length T."}, {"heading": "4. Structured Contextual Bandit", "text": "We now show that a variant of LOLS can be carried out in a \"structured contextual constellation,\" in which only the loss of a single structured label can be observed. (As already mentioned, this setting has applications to the website layout, personalised search and several other areas.) In each round, the learner is given an input example x, which predicts and suffers structured losses. (1) We assume that the structured losses are interval [0, 1] that the search space has depth T and that there are at most K measures in each state. As before, the algorithm has access to a political class, and also to a reference policy. It is important to stress that the reference policy has no access to true labelling and the aim is to improve the reference policy.Our approach is based on the -greedy method, which is a common strategy in partial feedback problems."}, {"heading": "5. Experiments", "text": "(This section shows that LOLS is able to improve a sub-optimal reference policy and provides empirical evidence to support the analysis in Section 3. We have conducted experiments on the following three application areas. Cost-sensitive multiclass classification. For each cost-sensitive multiclass sample, each choice of label has an associated cost reference. The search space for this task is a binary search tree. The root of the tree corresponds to the entire set of labels. We divide the set of labels recursively into two halves until each subgroup contains only one label. A path through the search space is a path of root-to-leaf in that tree. The loss of the end state is defined by the cost. An optimal reference policy can lead the agent to the minimum cost. We also show results of the use of a poor reference policy that arbitrarily chooses an action on each state. The experiments are carried out on KD99 Cup dat5."}, {"heading": "6. Proofs of Main Results", "text": "Lemma 1 (Ross & Bagnell Lemma 4,3): For all two policies, the following applies: \u03c01, \u03c02: J (\u03c01) \u2212 J (\u03c02) = TEt \u0445 U (1, T), s \u0445 dt\u03c01 [Q \u03c02 (s, \u03c01) \u2212 Q\u03c02 (s, \u03c02)] = TEt \u0445 U (1, T), s \u00b2 [Q \u03c01 (s, \u03c01) \u2212 Q\u03c01 (s, \u03c02)] Proof. So let us establish a policy which performs \u03c01 in the first t steps and then performs \u03c02 of the time steps t + 1 to T. We have J (\u03c01) = J (\u03c0t \u2212 1) and J (\u03c02) = J (\u03c00). Consequently, we can define the telescope sum: J (\u03c01) \u2212 J (\u03c01) \u2212 J (\u03c0t \u2212 1) = J (\u03c01), \u03c01 (\u03c01) = E \u00b2 (\u03c01), \u03c01 (2), 2, \u03c0\u03c0\u03c02 (2), \u03c0\u03c0\u03c0\u03c0\u03c02 (2) (2), \u03c0\u03c0\u03c0\u03c02)."}, {"heading": "6.1. Proof of Theorem 3", "text": "We start with an application of Lemma 1. In use of the Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Lemma-Definition we have: J (\u03c0) - \u03b2 i (\u03c0ref) = 1 N \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s s s s \u00b2 s s s s s \u00b2 s s s s s s s \u00b2 s s s \u00b2 s s s s s s s s s s s s"}, {"heading": "6.2. Proof of Corollary 1", "text": "The proof of this is relatively simple by definition. According to the definition of no repentance, it is immediate that the gapN number i = 1 T number t = 1 E [ci, t (\u03c0 i (st)) \u2212 ci, t (\u03c0 (st))] = o (NT), (8) for all policies \u03c0, remembering that ci, t is the cost vector of the measures in round i when we roll out from the key decision point. Let egg denote the conditional expectation in round i, caused by the previous rounds in the algorithm 1. Then it is easy to see that egg [ci, t (a) = egg [ei, t (a) \u2212 mina \u2032 (ei, a), where ei, t is the final state reached at the conclusion of the implementation of the policy."}, {"heading": "6.3. Proof sketch of Theorem 5", "text": "(Only sketch) We dissect the analysis about the exploration and exploitation rounds. For the exploration rounds, we bind the regret to its maximum possible value of 1. In order to control the regret about the exploitation rounds, we concentrate on the updates carried out during the exploration rounds. The cost vector c (a), which is used for an exploration round i satisfiesEi [c (a)] = Ei [K '(e (at)) 1 [a = Et \u0445 U (0: T \u2212 1), can be used to prove a result that is similar to the result of exploration rounds i [Q\u03c0 out i (s, a)], consequence 1. Since the cost vector is identical to that of algorithm 1, the proof of the theorem 3, which depends only on the expectations, can be used to prove a result that is similar to the theorem 3 times of exploration rounds i [Q\u03c0 out i (s, a)], consequence 1. Since the cost vector is identical to that of algorithm 1, the proof of the theorem 1-Z-Z-Z-U (0: T \u2212 1), the proof of the theorem 3, which depends only on the expectations, can be used T-Z-Z-Z-T to prove a result for the exploration rounds i."}, {"heading": "6.4. Proof of corollary 2", "text": "We start by replacing equation 5 in the remorse limit of theorem 5. This yields remorse \u2264 + T N \u03b4class + cKT N \u2211 i = 1 \u221a log | \u0432 | ni. We would like to continue replacing ni with its expectation, which i. However, this does not lead directly to a valid upper limit. Instead, we apply a chernoff bound to the set ni, which is a sum of i i.i.d. Bernoulli random variables with mean. Consequently, we have P (ni \u2264 (1 \u2212 \u03b3) i) \u2264 exp (\u2212 2 i2) \u2264 exp (\u2212 i / 8) \u2264 exp (\u2212 KIS), leaving i0 = 16 logN / + 1. Then we can add up the failure probabilities for all i \u2265 i0 and obtainN \u0432i = i0 P (ni \u2264 i / 2) \u2264 i / 2 exp (\u2212 KIS), allowing the exability (\u2212 i / 8) and the exability first (NIS / 2) and then NIS / 2 (Exability) (NIS / 2)."}, {"heading": "6.5. Proof of Theorem 4", "text": "\"I think it's going to take a lot of time to get to the bottom of it, but I think it's going to take a lot of time to get to the bottom of it,\" he said."}, {"heading": "Acknowledgements", "text": "Part of this work was carried out during the visit of Kai-Wei, Akshay and Hal to Microsoft Research."}, {"heading": "A. Details of cost-sensitive reduction", "text": "In this section we present the details of the reduction to a cost-sensitive multi-class classification, which is used in our experimental evaluation. In words: The algorithm uses a feature vector xt, i for class i at round t. It then trains a regressor to predict the corresponding cost ct, i for specifying the characteristics. In a fresh example, the predicted name is the one with the lowest predicted cost. This is a natural extension of the One Against All (OAA) approach to multi-class classification at cost-sensitive settings. Note that this also includes the alternative approach of a common feature vector, xt, i, zt for all i, and instead K trains different cost predictors, one for each class. If zt, i, RdK, one simply xt, i = zt in the ith block and zero elsewhere, a common predictor is used, then a common step for predicting K, one for each class."}, {"heading": "B. Details of Experiments", "text": "Our implementation is based on Vowpal Wabbit (VW) version 7.8 (http: / / hunch.net / \u02dc vw /). It is available at https: / / github.com / KaiWeiChang / vowpal _ wabbit / tree / icmlexp. For LOLS we use flags \"- search rollin,\" \"-search rollout,\" \"-search beta\" to specify the rollin policy, the rollout policy or \u03b2. We use \"-search interpolation policy -search passes per policy -5\" to enable SEARN. Detailed settings of various VW flags for the three experiments are shown below: \u2022 POS tagging: we use \"-search task sequence -search 45 -holdout off -affix -2w, + 2w -search neighbor features - 1: w, 1: w -b 28\" \u2022 Dependency parsing: we use \"-search task dep parser -search holdout search: -12 search w -1: neighbor search off -1: w -1"}], "references": [{"title": "On the snake in the box problem", "author": ["Abbott", "H.L", "M. Katchalski"], "venue": "Journal of Combinatorial Theory, Series B,", "citeRegEx": "Abbott et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Abbott et al\\.", "year": 1988}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["Daum\u00e9 III", "Hal", "Marcu", "Daniel"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "III et al\\.,? \\Q2005\\E", "shortCiteRegEx": "III et al\\.", "year": 2005}, {"title": "Search-based structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine Learning Journal,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Efficient programmable learning to search", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Ross", "St\u00e9phane"], "venue": null, "citeRegEx": "III et al\\.,? \\Q2014\\E", "shortCiteRegEx": "III et al\\.", "year": 2014}, {"title": "HC-Search: A learning framework for search-based structured prediction", "author": ["Doppa", "Janardhan Rao", "Fern", "Alan", "Tadepalli", "Prasad"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Doppa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Doppa et al\\.", "year": 2014}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Goldberg", "Yoav", "Nivre", "Joakim"], "venue": "Transactions of the ACL,", "citeRegEx": "Goldberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2013}, {"title": "A tabular method for dynamic oracles in transition-based parsing", "author": ["Goldberg", "Yoav", "Sartorio", "Francesco", "Satta", "Giorgio"], "venue": "Transactions of the ACL,", "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Imitation learning by coaching", "author": ["He", "Daum\u00e9 III", "Hal", "Eisner", "Jason"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Dynamic programming algorithms for transition-based dependency parsers", "author": ["Kuhlmann", "Marco", "G\u00f3mez-Rodr\u0131\u0301guez", "Carlos", "Satta", "Giorgio"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Kuhlmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kuhlmann et al\\.", "year": 2011}, {"title": "Sensitive error correcting output codes", "author": ["Langford", "John", "Beygelzimer", "Alina"], "venue": "In Learning Theory, pp", "citeRegEx": "Langford et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2005}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Marcus", "Mitch", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["McDonald", "Ryan", "Pereira", "Fernando", "Ribarov", "Kiril", "Hajic", "Jan"], "venue": "In Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP),", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Nivre", "Joakim"], "venue": "In International Workshop on Parsing Technologies (IWPT), pp", "citeRegEx": "Nivre and Joakim.,? \\Q2003\\E", "shortCiteRegEx": "Nivre and Joakim.", "year": 2003}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "J. Andrew"], "venue": "In Proceedings of the Workshop on Artificial Intelligence and Statistics (AI-Stats),", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "Reinforcement and imitation learning via interactive no-regret learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "J. Andrew"], "venue": null, "citeRegEx": "Ross et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2014}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoff J", "Bagnell", "J. Andrew"], "venue": "In Proceedings of the Workshop on Artificial Intelligence and Statistics (AI-Stats),", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 15, "context": "One approach to structured prediction is learning to search (L2S) (Collins & Roark, 2004; Daum\u00e9 III & Marcu, 2005; Daum\u00e9 III et al., 2009; Ross et al., 2011; Doppa et al., 2014; Ross & Bagnell, 2014), which solves the problem by: 1.", "startOffset": 66, "endOffset": 199}, {"referenceID": 4, "context": "One approach to structured prediction is learning to search (L2S) (Collins & Roark, 2004; Daum\u00e9 III & Marcu, 2005; Daum\u00e9 III et al., 2009; Ross et al., 2011; Doppa et al., 2014; Ross & Bagnell, 2014), which solves the problem by: 1.", "startOffset": 66, "endOffset": 199}, {"referenceID": 1, "context": "One approach to structured prediction is learning to search (L2S) (Collins & Roark, 2004; Daum\u00e9 III & Marcu, 2005; Daum\u00e9 III et al., 2009; Ross et al., 2011; Doppa et al., 2014; Ross & Bagnell, 2014), which solves the problem by: 1. converting structured prediction into a search problem with specified search space and actions; 2. defining structured features over each state to capture the interdependency between output variables; 3. constructing a reference policy based on training data; 4. learning a policy that imitates the reference policy. Empirically, L2S approaches have been shown to be competitive with other structured prediction approaches both in accuracy and running time (see e.g. Daum\u00e9 III et al. (2014)).", "startOffset": 121, "endOffset": 724}, {"referenceID": 15, "context": ", 2009), DAgger (Ross et al., 2011) and AggreVaTe (Ross & Bagnell, 2014).", "startOffset": 16, "endOffset": 35}, {"referenceID": 6, "context": ", (Goldberg & Nivre, 2013; Goldberg et al., 2014)).", "startOffset": 2, "endOffset": 49}, {"referenceID": 15, "context": ", (Ross et al., 2011; He et al., 2012)) defines the loss of a policy as an accumulation of the costs of states and actions in the trajectory generated by the policy.", "startOffset": 2, "endOffset": 38}, {"referenceID": 7, "context": ", (Ross et al., 2011; He et al., 2012)) defines the loss of a policy as an accumulation of the costs of states and actions in the trajectory generated by the policy.", "startOffset": 2, "endOffset": 38}, {"referenceID": 1, "context": ", the sequence tagging example in (Daum\u00e9 III et al., 2009)). In general, finding the optimal action at states not in the optimal trajectory can be tricky (e.g., (Goldberg & Nivre, 2013; Goldberg et al., 2014)). Finally, like most other L2S algorithms, LOLS assumes access to a cost-sensitive classification algorithm. A cost-sensitive classifier predicts a label \u0177 given an example x, and receives a loss cx(\u0177), where cx is a vector containing the cost for each possible label. In order to perform online updates, we assume access to a no-regret online cost-sensitive learner, which we formally define below. Definition 1. Given a hypothesis classH : X \u2192 [K], the regret of an online cost-sensitive classification algorithm which Doppa et al. (2014) discuss several approaches for defining a search space.", "startOffset": 41, "endOffset": 750}, {"referenceID": 15, "context": "Since the previous works in this area (Daum\u00e9 III et al., 2009; Ross et al., 2011; Ross & Bagnell, 2014) have only studied regret guarantees to the reference policy, the quantity we\u2019re studying is strictly more difficult.", "startOffset": 38, "endOffset": 103}, {"referenceID": 10, "context": "We train on 38k sentences and test on 11k from the Penn Treebank (Marcus et al., 1993).", "startOffset": 65, "endOffset": 86}, {"referenceID": 11, "context": "A dependency parser learns to generate a tree structure describing the syntactic dependencies between words in a sentence (McDonald et al., 2005; Nivre, 2003).", "startOffset": 122, "endOffset": 158}, {"referenceID": 8, "context": "We implemented a hybrid transition system (Kuhlmann et al., 2011) which parses a sentence from left to right with three actions: SHIFT, REDUCELEFT and REDUCERIGHT.", "startOffset": 42, "endOffset": 65}], "year": 2015, "abstractText": "Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.", "creator": "LaTeX with hyperref package"}}}