{"id": "1502.02322", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Rademacher Observations, Private Data, and Boosting", "abstract": "The minimization of the logistic loss is a popular approach to batch supervised learning. Our paper starts from the surprising observation that, when fitting linear (or kernelized) classifiers, the minimization of the logistic loss is \\textit{equivalent} to the minimization of an exponential \\textit{rado}-loss computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over the \\textit{same} classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be \\textit{directly} used to classify \\textit{observations}. We provide a learning algorithm over rados with boosting-compliant convergence rates on the \\textit{logistic loss} (computed over examples). Experiments on domains with up to millions of examples, backed up by theoretical arguments, display that learning over a small set of random rados can challenge the state of the art that learns over the \\textit{complete} set of examples. We show that rados comply with various privacy requirements that make them good candidates for machine learning in a privacy framework. We give several algebraic, geometric and computational hardness results on reconstructing examples from rados. We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy framework. Tests reveal that learning from differentially private rados can compete with learning from random rados, and hence with batch learning from examples, achieving non-trivial privacy vs accuracy tradeoffs.", "histories": [["v1", "Mon, 9 Feb 2015 01:12:11 GMT  (317kb,D)", "https://arxiv.org/abs/1502.02322v1", null], ["v2", "Thu, 2 Apr 2015 03:55:51 GMT  (317kb,D)", "http://arxiv.org/abs/1502.02322v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard nock", "giorgio patrini", "arik friedman"], "accepted": true, "id": "1502.02322"}, "pdf": {"name": "1502.02322.pdf", "metadata": {"source": "CRF", "title": "Rademacher Observations, Private Data, and Boosting", "authors": ["Richard Nock", "Giorgio Patrini"], "emails": ["richard.nock@nicta.com.au", "giorgio.patrini@anu.edu.au", "arik.friedman@nicta.com.au"], "sections": [{"heading": "1 Introduction", "text": "eDi eeisrteeGsrsrteeeeecnlhsrcnlhsrteee\u00fciiiiiiiiiiiiiueeteeteeteeteeteersrrrrrrrsrrteeeeterrrrrrrrrteerrrrteerrrrrteerrrrrrteerteerrrteerrrteerrteerrteerrteerrrteeteerrteerteerteerrteerteerteerrrrteeteerrrrrteeteeteerrrrteeteeteerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2 Rados and supervised learning", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,"}, {"heading": "3 Boosting using rados", "text": "The (unknown) Rademacher mappings are therefore essentially defined as follows: U. = {\u03c31, \u03c32,..., \u03c3n} (examples): These rados were calculated from some samples that RadoBoost does not know. [...] In the statement of the algorithm, a feature index (t) becomes a weak feature index oracle, wfi. [...] In its general form, there is a feature index oracle, wfi, wfi, which can have a feature index maximizing index maximizing index maximizing index maximizing index maximizing index maximizing maximizing index maximizing in (9). The weight update was preferred because RadaBoost may have large features and the weight update precision precision precision errors that might otherwise occur."}, {"heading": "4 Basic experiments with RadoBoost", "text": "We have compared RadoBoost with its main competitor, AdaBoost [27], using the same weak learner; in AdaBoost there is a feature that maximizes | rt | as in eq. (14). In these basic experiments, we deliberately did not optimize the set of Rados in which we select U for RadoBoost; therefore, we have presented the results in Table 1. Each algorithm has been executed for a total number of T = 1000 iterations; furthermore, the classifier stored for testing is the one that minimizes the empirical risk in the T iterations [2] of different sizes. For space reasons, Table 1 presents the results. Each algorithm has been executed for a total number of T = 1000 iterations; moreover, the classifier that minimizes the empirical risk during T iterations is."}, {"heading": "5 Rados and differential privacy", "text": "Intuitively, an algorithm is DP-compliant if it assigns a probability for two adjacent datasets similar to any possible output O. In other words, each individual dataset has only limited influence on the probability of a given output of the algorithm, and therefore the output reveals very little information about a particular dataset in the input. Formally, a randomized algorithm A (,) -differentiated-private [11] is for some, even if it is compatible > 0 iff: PA [O | S] \u2264 exp () \u00b7 PA [O | S \u2032] + GP (O \u00b2 S \u00b2 S \u00b2 S \u00b2 S \u00b2, O \u00b2), where the probability lies with the coin flip of A. This model is very strong, especially if it is = 0, and in the context of MDP / S \u00b2 maintaining high accuracy (S \u00b2)."}, {"heading": "5.1 A feature-wise DP mechanism for rados", "text": "In this subsection, we look at a loosening of differential privacy, namely a differentiated privacy in which differential privacy applies to the different data sets: we assume that two examples S, S, and S each contain a medical database that represents a physician's HIV status (1 line = a patient), and we do not wish that changing a single HIV status would significantly change the density of that function. This setting would also be very useful in genetic applications that influence one or a few genes."}, {"heading": "5.2 Boosting from DP-compliant examples via rados", "text": "We will now show how to build the difference between the two values in order to increase the convergence rates of RadoBoost. Specifically, since edge vectors are sufficient to learn (eq. 1), we assume that the edge vectors are oriented towards the respective edge vectors. (1) A common mechanism is the Gaussian mechanism [12, 16], which uses data with independent Gaussian variables N (0, 2I), the standard deviation of which depends on the DP requirement. (2) Strong DP regimes are difficult to deal with the learning algorithms. (2) The approximation factor of the singular vectors under DP noise behaves roughly as follows. (16) (Corollary 1.1), where the difference between the two values exists."}, {"heading": "6 Experiments on differential privacy", "text": "In a first series of experiments, we evaluated the impact on learning the functional DP mechanism: on each domain tested, we randomly selected a binary feature, and then we used DP-Feat algorithm to protect the feature for different values of the DP parameter, in an area that covers common DP experiments [18]. The main conclusion that can be drawn from the experiments is that learning DP-compliant rados can compete with learning random rados, and even learning examples (AdaBoost), even for the relatively small boots."}, {"heading": "7 From rados to examples: hardness results", "text": "The problem we are addressing here is how to recover examples from Rados, and when we cannot recover examples from Rados. This last setting is particularly useful from a privacy point of view, as it can save us costly obfuscation techniques that hamper ML tasks [4]."}, {"heading": "7.1 Algebraic and geometric hardness", "text": "& & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & & # 10; & & & & # 10; & & & & & # 10; & & & & & & # 10; & & & & # 10; & & & & & & # 10; & & & # 10; & & & # 10; & & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & &"}, {"heading": "7.2 Computational hardness", "text": "In this subsection we will examine two important problems in recovering examples. The first problem we cite relates to the hardness of solving underdetermined linear systems for sparse solutions [9]. The sparsity limitation can be embedded in the compressed query frame [8] to obtain finer hardness and approximations that exceed the scope of our essay. We define the problem \"sparse approximation\" as: (instance): set of rados Sr = {1, 2,..., m \u00b2 N, \",\" R +, which exceeds the scope of our essay. We define the problem \"sparse approximation\" as: (instance) set of rados Sr = {1, 2,..., m \u00b2, m \u00b2, m \u00b2, \"ending.\""}, {"heading": "8 Conclusion", "text": "We have introduced novel quantities that are sufficient for efficient learning, according to Rademacher. The fact that a subset of these quantities can replace traditional examples of efficient learning raises interesting problems as to how these subsets can be designed to cope with additional constraints. In the latter case, the results are based on NP hardness and therefore go beyond the \"hardness\" of factoring integrators, on which some popular cryptographic techniques are based [4]. Finally, Rados are cryptographic compliant: Homomorphic encryption schemes can be used to calculate encrypted rados from encrypted edge vectors or examples - Rado calculations can be easily distributed in secure multi-party computing applications."}, {"heading": "9 Acknowledgments", "text": "The authors would like to thank Tiberio Cae \ufffd tano for early discussions that brought the idea of the Rademacher observations and their use in data protection related applications. Thanks are also due to Stephen Hardy and Hugh Durrant-Whyte for many stimulating discussions and feedback on this topic. NICTA is funded by the Australian government through the Ministry of Communications and the Australian Research Council through the ICT Center of Excellence Program."}, {"heading": "10 Appendix \u2014 Proofs", "text": "To simplify the proofs, we define the following quantity: \u03c0-\u03c3. = \u2211 i-\u03c3ixi, \u0435\u043c-\u03c3. (24), so that each Rado can be defined as follows: \u03c0\u03c3 = (1 / 2) \u00b7 (\u03c0-\u03c3 + \u03c3-y). We remember that y is the label vector."}, {"heading": "10.1 Proof of Lemma 2", "text": "We dispose of Flog (S, \u03b8). = 1 m \u00b2 i log (1 + exp (\u2212 yi\u03b8 > xi)) = 1 m \u00b2 i log (1 2 \u00b7 y\u03b8 > xi) \u2212 1 m \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (26) = log (2) = 1 log \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (2) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (2))."}, {"heading": "10.2 Proof of Theorem 3", "text": "Suppose that our set of rados U (2) is fulfilled for any sample S (2): U (2), (27), where we have a fixed reference set of 3), (2), (2), (3), (3), (3), (3), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, 5, 5, (5), (5), (5), (5, 5, 5, 5, 5, 5, 5 (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5, (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5, ("}, {"heading": "10.3 Proof of Lemma 4", "text": "Theorem 1 in ([22]) immediately yields 1 n exp (\u2212 \u03b8 > T \u03c0j) \u2264 T \u0435t = 1 \u221a 1 \u2212 r2t \u00b7 w (T + 1) j, \u0394j [n]. (48) Since 1 > wT + 1 = 1 the sum above j [n]: F rexp (S, \u03b8T, U) \u2264 T doesn t = 1 \u221a 1 \u2212 r2t \u2264 exp (\u2212 12 \u2211 t r2t).With the help of the (WLA) results in ineq. (12)."}, {"heading": "10.4 Proof of Lemma 5", "text": "We write rt (wt) in dependence of the examples around: rt (wt) = 1\u03c0 * k \u00b2 n \u00b2 j = 1 wtj\u03c0jk = 1\u03c0 * k \u00b2 j = 1 \u2211 i: \u03c3ji = yi wtjyixik = 1\u0445 k \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p p p \u00b2 p p p p \u00b2 p p p p p p \u00b2 p p p p \u00b2 p p p p \u00b2 p p p p \u00b2 p p p p p \u00b2 p p p p = p \u00b2 p p p \u00b2 p p p \u00b2 p p p p p p \u00b2 p p p p p p p p p = p \u00b2 p p p \u00b2 p p p = p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p p p = p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p p p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p p p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p p p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p p \u00b2 p \u00b2 p p p \u00b2 p p p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p p p \u00b2 p"}, {"heading": "10.5 Proof of Theorem 6", "text": "To simplify the notation further, we consider the following: \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (S), (56), \u2212 (S), \u2212 m (S), \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (S), (56), \u2212 m (S), \u2212 \u2212 \u2212 \u2212 (S), \u2212 (S), \u2212 (S), \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (S), \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (S), \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (S), \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (S), \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (56), (56), and they differ by the value of a (Boolean) property. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (S), (S), \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (S), \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (S), \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, (56, (56, (56, (56, (56, (56), (56, (56, (M), (M (m m), (m (m (m), m (m), m (m (m (m), m (m, m, m, m, m, m, m, m, m"}, {"heading": "10.6 Proof of Theorem 7", "text": "We retain the same notations as in the proof of theorem 6. The wheel maker rejection of \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 (2) has a probability of rejecting a single Rado which is limited by (a fraction of) the tail of binomial \u2212 \u2212 \u2212 \u2212 \u2212 BE (2) \u00b7 BE (2) \u00b7 BE (6) \u00b7 S] = 12m \u00b7 p < \u2212 (m \u2212 mk (+) + \u03b2 (m \u2212 mk (+) \u2212 r > mk (+) \u2212 m (+) \u2212 m (2) \u2212 r (2) < (m \u2212 mk (+) \u2212 m (+) \u2212 m (+) \u2212 mk (+) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 r (m \u00b7 \u00b7 r (m +)) \u2212 mk (+) \u2212 m (m) \u2212 m (m) \u2212 m (+) \u2212 m (+) \u2212 m (+) \u2212 m (+) \u2212 m (+) \u2212 m) \u2212 m (+) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m (m) \u2212 m) \u2212 m (m) \u2212 m (m) \u2212 m (m) \u2212 m (m) \u2212 m (m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) (\u2212 m) (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m) \u2212 m (\u2212 m (\u2212 m) (\u2212 m) (\u2212 m) (\u2212 m) (\u2212 m) (\u2212 m) (\u2212 m) (\u2212 m"}, {"heading": "10.7 Proof of Theorem 8", "text": "Let us first note that the DP protection of the vector edges by calculating a particular example (1 = > P = 1 = > P = 1 (x + i, yi). = (xi + x, yi), (94), where xri \u00b2 N (0, x 2I), is equivalent to noisy edges, because (1 / 2), and the pdf of the Gaussian mechanism is unchangeable by multiplying y.The key quantity to prove the theorem is that the noisy edges + j \u00b2 (1 / 2), (1 / 2), and the pdf of the Gaussian mechanism is unchangeable by multiplying y.The key quantity to test the theorem is, for all noisy rapes + j \u00b2). = (1 / 2), the noisy rapes + j \u00b2 (1 / 2), the noisy rapes (1 / 2)."}, {"heading": "10.8 Proof of Lemma 10", "text": "First, let us consider that m \u2265 2d. A simple proof of the term is the largest d-dimming square with edge length '= 2R / \u221a d, represented by a thick dashed line in Figure 5. We then pack this square with m + 1 spheres as shown. Since the edge length of dlog (m) / log (d) e diameter of these spheres is covered, we get that the radius r of each of these spheres meets: r = 2R \u221a d protocol (m + 1) protocol d \u00b2 d protocol (m + 1), (119) because m \u00b2 d > d. Due to the construction, at least one of these spheres does not contain an edge vector of C (E) and is therefore empty. Let us consider such an empty sphere whose center e \u00b2 is closest to 0, as shown in Figure 5, and let us consider an adjacent sphere that is not further away."}, {"heading": "10.9 Proof of Lemma 11", "text": "We make a reduction from the X3C3 ([25]) problem, whose instance is a series of S = 1, s2,..., sn and a series of 3 subsets of S, C, c2,..., cd, and a whole series of examples of S belongs to exactly three subsets of C. The question is whether there is a coverage of S, most of the m elements of C. The reduction is the following: \u2022 to each attribute belongs an element of C; \u2022 to each element sj of S we associate a Boolean rado that is 1 in coordinate k iff sj, and zero otherwise: [1] {k: sj, ck, ck}. (1I is \"1\" in coordinate ik for k, and zero everywhere else) \u2022 The number of examples is m; \u2022 the parameters r and \"are fixed as follows: - if p 6 = 0, the value of r is 21 / p."}, {"heading": "10.10 Proof of Lemma 12", "text": "We make the same reduction as with the sparse approximation. The example sentence S consists of all canonical base vectors associated with positive class."}, {"heading": "11 Appendix \u2014 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11.1 Supplementary experiments to Table 1", "text": "The calculation of rt inStep 2.2 of RadoBoost (see (9)) is supplemented by the following step: rt \u2190 Characters (rt) \u00b7 max {0,1, | rt |} (131) The same change is made in AdaBoost ([27]) (episode 1) to prevent domains with unusual characteristic values from causing AdaBoost to select the wrong character for \u03b1t for a large number of iterations due to very small dirt values (but wrong sign). Experiments show that this corrects the bad results of AdaBoost on Twitter, but leads to worse results for AdaBoost and / or AdaBoost (n) on other domains such as Fertility, Haberman, Sonar, Abalone."}, {"heading": "11.2 Supplementary experiments to Section 5 \u2014 I / III", "text": "Tables 4, 5, 6, 7 show the results compared to AdaBoost, RadoBoost with random rados and RadoBoost with fixed rados (m \u0445). Unless otherwise stated in the tables, the following experimental setup applies: \u2022 RadoBoost is trained with n = min {1000, size of tensile pleat / 2} rados; \u2022 AdaBoost is trained with the entire training pleat; \u2022 for each standard deviation \u03c3 we generate 10 noisy domains; each is then processed after 10 folds of layered cross validations. Thus, each point on the coloured curves corresponds to the average of ten experiments; \u2022 RadoBoost is trained with two types of rados: random rados as in section 4 - this results in the grey dashed curves - or rados with fixed rados as in section 5.2 - this results in the coloured curves -;"}, {"heading": "11.3 Supplementary experiments to Section 5 \u2014 II / III", "text": "Tables 8 and 9 compare RadoBoost, which was trained with Rado's fixed support and uses a \"prudential\" weak learner (which selects the median attribute according to | rt |), with RadoBoost, which was trained with simple random rados and uses the \"strongest\" weak learner, which selects the best attribute according to | rt |."}, {"heading": "11.4 Supplementary experiments to Section 5 \u2014 III / III", "text": "Tables 10 and 11 compare two different RadoBoost generation mechanisms: random generation of arbitrary rados (Section 4) and random generation of fixed-support rados (Section 5.2). In both tables, the weak learner is always the same (as opposed to Tables 8 and 9), i.e. the \"strong\" weak learner who selects the best attribute according to | rt | at each repetition."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>The minimization of the logistic loss is a popular approach to batch supervised learning. Our<lb>paper starts from the surprising observation that, when fitting linear (or kernelized) classifiers,<lb>the minimization of the logistic loss is equivalent to the minimization of an exponential rado-loss<lb>computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over<lb>the same classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be<lb>directly used to classify observations. We provide a learning algorithm over rados with boosting-<lb>compliant convergence rates on the logistic loss (computed over examples). Experiments on<lb>domains with up to millions of examples, backed up by theoretical arguments, display that<lb>learning over a small set of random rados can challenge the state of the art that learns over<lb>the complete set of examples. We show that rados comply with various privacy requirements<lb>that make them good candidates for machine learning in a privacy framework. We give several<lb>algebraic, geometric and computational hardness results on reconstructing examples from rados.<lb>We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy<lb>framework. Tests reveal that learning from differentially private rados can compete with learning<lb>from random rados, and hence with batch learning from examples, achieving non-trivial privacy<lb>vs accuracy tradeoffs.", "creator": "LaTeX with hyperref package"}}}