{"id": "1502.02362", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback", "abstract": "We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method -- called Policy Optimizer for Exponential Models (POEM) -- for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art.", "histories": [["v1", "Mon, 9 Feb 2015 05:09:25 GMT  (52kb)", "http://arxiv.org/abs/1502.02362v1", "9 pages"], ["v2", "Wed, 20 May 2015 23:29:49 GMT  (54kb)", "http://arxiv.org/abs/1502.02362v2", "10 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["adith swaminathan", "thorsten joachims"], "accepted": true, "id": "1502.02362"}, "pdf": {"name": "1502.02362.pdf", "metadata": {"source": "META", "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback", "authors": ["Adith Swaminathan"], "emails": ["ADITH@CS.CORNELL.EDU", "TJ@CS.CORNELL.EDU"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.02 362v 1 [cs.L G] 9F eb2 01"}, {"heading": "1. Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "2. Related Work", "text": "The first approach is to reduce the problem to supervised learning. In principle, since the protocols give us an incomplete overview of the feedback for various predictions, one could first use regression to estimate a feedback oracle for invisible predictions, and then use any supervised learning algorithm that uses this feedback oracle. Such a two-step approach is known to not generalize well (Beygelzimer & Langford, 2009); more sophisticated techniques that allow a cost-weighted classification (Zadrozny et al., 2003) or the offset tree algorithm (Beygelzimer & Langford, 2009) to perform batch learning when the space for possible predictions is small; in contrast, our approach generalizes structured predictions with exponentially large prediction spaces."}, {"heading": "3. Learning Setting: Batch Learning with Logged Bandit Feedback", "text": "Consider a structured output prediction problem that is used as input x-X and outputs a prediction y-Y. For example, in the classification of multi-label documents, x could be a news article and y a bit vector indicating the terms associated with this article. Inputs are drawn from a fixed but unknown distribution Pr (X), x i.d. \u00b2 Pr (X). Consider the hypotheses space H stochastic politics. A hypothesis h (Y | x) H defines a probability distribution over the output space Y, and the hypothesis makes predictions by scanning y h (Y | x). Note that this definition of a hypotheses space h also includes deterministic hypotheses where the distributions assign probability 1 to a single y. For notational convenience, denote h (Y | x) prediction values are assigned by h (x) and probability by h (x)."}, {"heading": "4. Learning Principle: Counterfactual Risk Minimization", "text": "The distribution mismata between h0 and H (h).S (h) S (h) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S) S (H) S (H) S) S (H) S (H) S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S) S (S) S (S) S (S) S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S (S) S (S) S (S (S) S (S) S (S (S) S (S) S (S (S) S (S (S) S (S (S) S (S) S (S (S) S (S (S) S (S (S) S (S (S) S (S (S) S (S (S (S) H) H) H) H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S (H) S) S (H) S (H) S) S (H) S (H) S (H) S) S (H) S (H) S (H) S) S (H) S) S (H) S (H (H (H) S) S (H) S) S (H (H) S) S) S (H (H (H (H) S) S)"}, {"heading": "4.1. Optimal Loss Scaling", "text": "When performing supervised learning with real labels y \u00b2 \u00b2 and a loss function \u0445 (y \u00b2, \u00b7), the empirical risk minimization using the standard estimator is invariant compared to additive translation and multiplicative scaling of \u0445. However, the risk estimators R \u00b2 (h) and R \u00b2 M (h) in bandit learning require a degenerative effect! A hypothesis h \u00b2 H that trivially circumvents the sample D (i.e. i = 1,.., n, h (yi | xi) = 0) achieves the best possible R \u00b2 M (h) and the equation (10) (CRM) (= 0) with 0 variance. This degenerativity occurs because the optimization goals represent a lower probability."}, {"heading": "4.2. Selecting hyper-parameters", "text": "We suggest to select the hyperparameters M > 0 and \u03bb \u2265 0 by means of validation, but we must be careful not to be too large, the estimated risk R-M (h) [\u2212 M, 0], while the penalty of variance \u221a V arh (u) n [0, M 2 \u221a n], if \u03bb-0, a hypothesis h-H that D completely avoids, achieves a training target of 0, as a rule of thumb we can calibrate \u03bb-m so that the target is negative for some h-H. If h0-H, {R-M (h0) + \u043c-V arh0 (u) n} < 0 is a natural choice, minimizing relative to H is guaranteed to prevent the return of degenerated h."}, {"heading": "5. Learning Algorithm: POEM", "text": "We now derive an efficient algorithm for structured output predictions using linear rules from the CRM principle (> X principle), whereas classical linear models in supervised learning are Usinghsupw (x) = argmax y-y- (x, y), (11) where w is a d \u2212 dimensional weight vector, and \u03c6 (x, y) is a d \u2212 dimensional linear function board. For example, in multi-label document classification, for a news article x and a possible mapping of labels y that are represented as a bit vector (x, y) could simply be a concatenation of the document's concepts (x), a copy for each of the assigned labels in y, x-y. Several efficient inference algorithms have been developed to solve equations (11). Consider the following stochastic Hlin family, parameterized by w."}, {"heading": "5.1. Iterated Variance Majorization", "text": "The POEM training target in Equation (14), in particular the concept of variance \u221a V arw (u), resists stochastic gradient optimization in the presented form. To remove this obstacle, we are now developing a majorization-minimization scheme that can be shown to converge to a local optimum of the POEM training target. In particular, we will show how V arw (u) can be decomposed as the sum of differentiable functions (e.g. \"i uw i\" or \"i {uwi\" 2), so that we can optimize the general training target on a scale by stochastic gradient reduction."}, {"heading": "6. Experiments", "text": "We are considering a multi-stage classification with input x-Rp and prediction. (1, 2, 2, 3, 4, 4, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "6.1. Does variance regularization improve generalization?", "text": "The results are in Table 3. We statistically test the performance of POEM against IPS using a one-tailed pair difference t test at the significance level of 0.05 over 10 runs of the experiment and find that POEM is significantly better than IPS on each dataset. Furthermore, POEM learns a hypothesis on all datasets that significantly improves on the predictive performance of h0. IPS actually fails on the media dataset, where it returns a hypothesis that is worse than h0. This suggests that the CRM principle is handy for designing learning algorithms, and that the variance regulator actually provides a practical benefit."}, {"heading": "6.2. How computationally efficient is POEM?", "text": "To evaluate the efficiency of POEM training, we have implemented two versions of the algorithm: For POEM (S), we train the target using the iterative-majoritative stochastic gradient method described in Section 5.1. For POEM (B), we optimize the POEM target using L-BFGS, a batch gradient method. L-BFGS was also used to train the IPS (B) method. Table 4 shows the time needed (in CPU seconds) to execute each method on each dataset averaged over different validation runs when performing hyperparameter grid searches. Some of the timing results are distorted by outliers, e.g. with very weak l2 regulation, CRFs tend to converge for much longer. Overall, however, it is clear that POEM (S) is able to restore good parameter settings in a fraction of the sample time (if the FGS number is still predomed and the BGS number is)."}, {"heading": "6.3. Can MAP predictions derived from stochastic policies perform well?", "text": "In practice, this method of generating predictions can be much faster than sampling, since the calculation of Argmax does not require a calculation of the partition function Z (x), which can be expensive in structured output prediction. Table 5 shows that the loss of the deterministic prediction factor is typically not far from the loss of stochastic politics, but often slightly better. This suggests that POEM tends to find parameters w that behave almost deterministically (i.e., w is large), which has been proven to be true. In addition, we can also claim that for every stochastic policy hw there is a corresponding deterministic function that the risk is not greater than R (w) (and similarly for empirical risks), and that these techniques such as variance in regulation and regulation 2 become crucial."}, {"heading": "6.4. How does generalization improve with size of D?", "text": "Since we collect more data below h0, our generalization error limit indicates that the prediction performance should ultimately approach the optimal hypothesis in the hypotheses space. We can simulate n \u2192 \u221e by repeating the training data several times and collecting samples y-h0 (x). At the limit, we would observe any possible y in the bandit feedback data set, since h0 (x) has a nonzero chance of exploring any prediction y. However, the learning rate can be slow, as the exponential model family has very thin tails and thus may not be an ideal logging distribution to learn from. To ensure that we do not have confusing effects from stochastic optimization and premature termination, we are investigating the L-BFGS variant of POEM (B) here. Since we keep all other details of the experimental setup fixed, we vary the number of repetitions of the training set (collect code replay from 1, to a sample)."}, {"heading": "6.5. How does quality of h0 affect learning?", "text": "In this experiment, we change the portion of the training set f \u00b7 ntrain that was used to train logging policy; as f increases, the quality of h0 improves. Intuitively, there is a trade-off: Better h0 probably tries correct predictions more often, producing a higher-quality D from which to learn, but it should also be more difficult to beat h0. We vary f from 10% to 100% while maintaining all other conditions with the original setup, and focus again on the L-BFGS variant of POEM (B). We report on the performance of h0 and POEM (B) for a single trial run in Figure 2, and find that POEM (B) is able to consistently find a hypothesis that is at least as good as h0. In addition, the performance of POEM (B) plateau increases with increasing f, as opposed to its behavior in Figure 1, where it is steadily improving with increasing h0."}, {"heading": "6.6. How does stochasticity of h0 affect learning?", "text": "Finally, the theory suggests that counterfactual learning is only possible if h0 is sufficiently stochastical (the generalization limits are likely to apply in the samples taken from h0).Does CRM decompose gracefully if this assumption is violated? We test this by introducing the temperature multiplier w 7 \u2192 \u03b1w, \u03b1 > 0 (as discussed in Section 5) into the logging policy.For h0 = hw0, we scale w0 7 \u2192 \u03b1w0 to derive a more \"deterministic\" variant of h0 and generate D \u0445 h\u03b1w0. If we deduce all other test conditions, we report the performance of a single run of POEM (B) in Figure 3, if instead we cause a change [0.5,.,.,., 32] compared with h0, and the deterministic predictor - h0 map - from h0. As long as a minimum of stochastity is still present in POM, there is a slight improvement in POM (POB)."}, {"heading": "6.7. Can warm-starting the parameter search help?", "text": "In all the experiments reported so far, we naively started with our parameter search of w = 0. However, since our logging policy h0 (w0) Hlin enables us to achieve better local minima by warming up the search with w = w0, the generalization performance of POEM (B) with and without a warm start is shown in Table 6. Warm start consistently helps to find slightly better optima, suggesting that there are further opportunities for improvement by using clever optimization techniques to solve these non-convex targets."}, {"heading": "7. Conclusion", "text": "The most important finding for CRM is the extension of the classical notion of a hypotheses class to include stochastic guidelines, reasoning about variance in the risk estimator, and deriving a generalization error within that hypotheses space. The practical take-away is a simple, data-independent regulator that guarantees robust learning. We developed POEM, which uses CRM for structured output predictions. POEM can optimize across rich policy families (exponential models that correspond to linear rules in supervised learning) and handle massive output spaces just as efficiently as classical supervised methods. POEM efficiently dissects the CRM training target by repeated variance linearization and optimizes on a scale by stochastic gradient decentralization. CRM can be applied more generally to supervised learning with non-differentiable losses, as the goal does not require the gradient of loss from repeated variance linearization, and optimizes on a scale by using gradient decentricity to decrease the gradient of CRM."}, {"heading": "Acknowledgement", "text": "This research was partially funded by NSF awards IIS1247637 and IIS-1217686, as well as the JTCII Cornell-Technion Research Fund and a gift from Bloomberg."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Agarwal", "Alekh", "Hsu", "Daniel", "Kale", "Satyen", "Langford", "John", "Li", "Lihong", "Schapire", "Robert"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "The offset tree for learning with partial labels", "author": ["Beygelzimer", "Alina", "Langford", "John"], "venue": "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Reusing historical interaction data for faster online learning to rank for IR", "author": ["Hofmann", "Katja", "Schuth", "Anne", "Whiteson", "Shimon", "de Rijke", "Maarten"], "venue": "In Sixth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Hofmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2013}, {"title": "Truncated importance sampling", "author": ["Ionides", "Edward L"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Ionides and L.,? \\Q2008\\E", "shortCiteRegEx": "Ionides and L.", "year": 2008}, {"title": "Exploration scavenging", "author": ["Langford", "John", "Strehl", "Alexander", "Wortman", "Jennifer"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Langford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2008}, {"title": "Doubly robust policy evaluation and learning", "author": ["Langford", "John", "Li", "Lihong", "Dudk", "Miroslav"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Langford et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2011}, {"title": "Counterfactual estimation and optimization of click metrics for search", "author": ["Li", "Lihong", "Chen", "Shunbao", "Kleban", "Jim", "Gupta", "Ankur"], "venue": "engines. CoRR,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "On minimax optimal offline policy evaluation", "author": ["Li", "Lihong", "Munos", "R\u00e9mi", "Szepesv\u00e1ri", "Csaba"], "venue": "CoRR, abs/1409.3653,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques", "author": ["Mary", "J\u00e9r\u00e9mie", "Preux", "Philippe", "Nicol", "Olivier"], "venue": "In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,", "citeRegEx": "Mary et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mary et al\\.", "year": 2014}, {"title": "Empirical bernstein bounds and sample-variance penalization", "author": ["Maurer", "Andreas", "Pontil", "Massimiliano"], "venue": "In COLT 2009 - The 22nd Conference on Learning Theory,", "citeRegEx": "Maurer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maurer et al\\.", "year": 2009}, {"title": "The central role of the propensity score in observational studies for causal effects", "author": ["Rosenbaum", "Paul R", "Rubin", "Donald B"], "venue": null, "citeRegEx": "Rosenbaum et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Rosenbaum et al\\.", "year": 1983}, {"title": "Multi-armed bandit problems with history", "author": ["Shivaswamy", "Pannagadatta K", "Joachims", "Thorsten"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Shivaswamy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shivaswamy et al\\.", "year": 2012}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1998\\E", "shortCiteRegEx": "Vapnik", "year": 1998}, {"title": "Costsensitive learning by cost-proportionate example weighting", "author": ["Zadrozny", "Bianca", "Langford", "John", "Abe", "Naoki"], "venue": "In Proceedings of the Third IEEE International Conference on Data Mining,", "citeRegEx": "Zadrozny et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zadrozny et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 5, "context": "Such estimators have been developed recently for the off-policy evaluation problem (Langford et al., 2011), (Li et al.", "startOffset": 83, "endOffset": 106}, {"referenceID": 12, "context": "We first prove generalization error bounds analogous to structural risk minimization (Vapnik, 1998) for a stochastic hypothesis family.", "startOffset": 85, "endOffset": 99}, {"referenceID": 13, "context": "More sophisticated techniques using a cost weighted classification (Zadrozny et al., 2003) or the Offset Tree algorithm (Beygelzimer & Langford, 2009) allow us to perform batch learning when the space of possible predictions is small.", "startOffset": 67, "endOffset": 90}, {"referenceID": 5, "context": ", 2014b), and doubly robust estimators are even more efficient when we additionally have a good model of the feedback (Langford et al., 2011).", "startOffset": 118, "endOffset": 141}, {"referenceID": 4, "context": "Techniques like exploration scavenging (Langford et al., 2008) and bootstrapping (Mary et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 8, "context": ", 2008) and bootstrapping (Mary et al., 2014) allow us to perform counterfactual evaluation even when the historical algorithm was deterministic or adaptive.", "startOffset": 26, "endOffset": 45}, {"referenceID": 2, "context": "Beyond the problem of batch learning from bandit feedback, our approach can have implications for several applications that require learning from logged bandit feedback data: warm-starting multi-armed bandits (Shivaswamy & Joachims, 2012), pre-selecting retrieval functions for search engines (Hofmann et al., 2013), and policy evaluation for contextual bandits (Li et al.", "startOffset": 293, "endOffset": 315}, {"referenceID": 12, "context": "In analogy to Structural Risk Minimization (Vapnik, 1998), we call this principle Counterfactual Risk Minimization, since both pick the hypothesis with the tightest upper bound on the true risk R(h).", "startOffset": 43, "endOffset": 57}, {"referenceID": 0, "context": "We employ the Supervised 7\u2192 Bandit conversion (Agarwal et al., 2014) method.", "startOffset": 46, "endOffset": 68}], "year": 2017, "abstractText": "We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method \u2013 called Policy Optimizer for Exponential Models (POEM) \u2013 for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}