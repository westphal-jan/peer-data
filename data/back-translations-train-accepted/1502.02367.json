{"id": "1502.02367", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Gated Feedback Recurrent Neural Networks", "abstract": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden state and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.", "histories": [["v1", "Mon, 9 Feb 2015 05:25:54 GMT  (1592kb,D)", "https://arxiv.org/abs/1502.02367v1", null], ["v2", "Thu, 12 Feb 2015 19:18:07 GMT  (1592kb,D)", "http://arxiv.org/abs/1502.02367v2", "11 pages, corrected typos"], ["v3", "Wed, 18 Feb 2015 11:34:38 GMT  (1592kb,D)", "http://arxiv.org/abs/1502.02367v3", "11 pages, corrected typos"], ["v4", "Wed, 17 Jun 2015 06:26:21 GMT  (2181kb,D)", "http://arxiv.org/abs/1502.02367v4", "9 pages, removed appendix"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG stat.ML", "authors": ["junyoung chung", "\u00e7aglar g\u00fcl\u00e7ehre", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1502.02367"}, "pdf": {"name": "1502.02367.pdf", "metadata": {"source": "META", "title": "Gated Feedback Recurrent Neural Networks", "authors": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "emails": ["JUNYOUNG.CHUNG@UMONTREAL.CA", "CAGLAR.GULCEHRE@UMONTREAL.CA", "KYUNGHYUN.CHO@UMONTREAL.CA", "FIND-ME@THE.WEB"], "sections": [{"heading": "1. Introduction", "text": "Recent studies have shown that RNNs using gating units can achieve promising results in both classification and generational tasks (see e.g. Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014). Although RNNNs can theoretically capture any long-term dependence in an input sequence, it is known that it is difficult to train an RNN to actually do so (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998). One of the most successful and promising approaches to solving this problem is to modify the RNN architecture by using a gated activation function rather than the usual state-to-state transition function, which is affine transformation and punctual nonlinearity."}, {"heading": "2. Recurrent Neural Network", "text": "An RNN is able to process a sequence of arbitrary lengths by recursively applying a transition function to its internal hidden states for each symbol of the input sequence. In Timestep t, the activation of the hidden states in the timestep is calculated as a function of the current input symbol xt and the previous hidden states ht \u2212 1: ht = f (xt, ht \u2212 1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "2.1. Gated Recurrent Neural Network", "text": "The difficulty of training an RNN to detect long-term dependencies has long been known (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998). An early successful approach to this fundamental challenge was to modify the function of transition from state to state to encourage some hidden units to maintain their long-term memory adaptively, thereby creating paths in the time-unfolded RNN so that gradients can flow over many periods of time. LSTM maintains a separate memory cell in it that only updates and discloses its contents when deemed necessary. More recently, Cho et al. (2014) proposed a gated recurrent unit (GRU) that remembers its state and forgets it based on the input signal to the unit."}, {"heading": "2.1.1. LONG SHORT-TERM MEMORY", "text": "Since the original proposal of 1997, several variants of the LSTM have been introduced (Gers et al., 2000; Zaremba et al., 2014).Here we follow the implementation of Zaremba et al. (2014).Such an LSTM unit consists of a memory cell ct, an entrance gate it, a forgetting gate ft and an exit gate ot. The memory cell carries the memory content of an LSTM unit, while the gates control the amount of changes and the exposure of the memory content.The content of the memory cell cjt of the j-th LSTM unit at timestep t is updated similarly to the form of a gated leaky neuron, i.e., like the weighted sum of the new contents c-jt and the previous memory contents cjt \u2212 1 modulated by the input and forgetting gates, i j t and f-j t should be the input input gates, i j t and f-jxt should be the input and input units, and input units respectively:"}, {"heading": "2.1.2. GATED RECURRENT UNIT", "text": "The GRU was recently proposed by Cho et al. (2014). Like the GRU, it was designed to adaptively reset or update its memory content. Thus, each GRU has a reset gate rjt and an update gate zjt, which is reminiscent of the forget- and input-gate of the LSTM. However, unlike the LSTM, the GRU fully discloses its memory content at each time step and strictly balances between the previous memory content and the new memory content using the leaky integration, albeit with its adaptive time constant controlled by the update gate zjt. At timestep t, the state hjt of the j-th GRU is fully calculated and the new memory content is determined by byhjt = (1 \u2212 z j t) h \u2212 j t h \u2212 t h h h, if the adaptation time constant is controlled by the update gate zjt."}, {"heading": "3. Gated Feedback Recurrent Neural Network", "text": "Although we map the long-term dependencies in a sequence, each module is an important and difficult target for RNNs, to be precise. It is worth noting that a sequence often consists of slow and fast-moving components, only the former corresponding to the long-term dependencies, but ideally an RNN must capture both long-term and short-term dependencies. El Hihi & Bengio (1995) first shows that an RNN can capture these dependencies of different time levels more easily and efficiently if the hidden units of the RNN are explicitly divided into groups corresponding to different time levels. Clockwork RNN (CW-RNN) is another that can capture these dependencies from different time levels. In 2014, it implements the i-th module to operate the rate of 2i \u2212 1, where i is a positive integrator, meaning that the module will only be updated if it is 2i = 1 \u2212 1."}, {"heading": "3.1. Practical Implementation of GF-RNN", "text": "tanh unit. For a stacked tanh-RNN, the signal from the previous timestamp is calculated, where L is the number of hidden layers, W j \u2212 1 \u2192 j and U i \u2192 j is the weight matrices of the current input and the previous hidden states of the i-th module. Compared to eq. (1), the only difference is that the previous hidden states consist of several layers and are controlled by the global reset gate.Long short-term memory and gated recurrent unit. In the cases of LSTM and GRU, we do not use the global reset gates when calculating the unit-wise gates. In other words: Eqs. (4) - (6) for LSTM, and Eq.s (GR. 8 and GR.). (GR.). (GR.), the global reset gates are not used when the global gates (1) and global gates (U) are recalculated."}, {"heading": "4. Experiment Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Tasks", "text": "We evaluated the proposed GF-RNN at character level for speech modeling and for the evaluation of Python programs. Both tasks are representative examples of discrete sequence modeling in which a model is trained to minimize the negative log probability of training sequences: min \u03b81N N \u0445 n = 1 Tn \u2211 t = 1 \u2212 log p (xnt | xn1,..., xnt \u2212 1; \u03b8), where \u03b8 is a set of model parameters."}, {"heading": "4.1.1. LANGUAGE MODELING", "text": "We used the data set provided as part of the Compression of Human Knowledge Competition (Hutter, 2012). We refer to this data set as the Hutter data set. The data set, taken from Wikipedia, includes 100 MBytes of characters, including Latin alphabets, non-Latin alphabets, XML markups, and special characters. Following the protocols in (Mikolov et al., 2012; Graves, 2013), we used the first 90 MBytes of characters to train a model, the next 5 MBytes as a validation set, and the rest as a test set with the vocabulary of 205 characters, including a token for an unknown character. We used the average number of bits per character (BPC, E [\u2212 log2 P (xt + 1 | ht)])) to measure the performance of each model on the Hutter data set."}, {"heading": "4.1.2. PYTHON PROGRAM EVALUATION", "text": "Zaremba & Sutskever (2014) recently demonstrated that an RNN, or more precisely a stacked LSTM, is able to execute a short Python script. Here, we compared the proposed architecture with the traditional batch approach model for this task, known as Python program evaluation. The scripts used in this task include addition, multiplication, subtraction, for-loop, variable assignment, logical comparison, and if-else statement. The goal is to generate or predict the correct return value of a given Python script. Input is a program, while output is the result of a print instruction: each input script ends with a print instruction. Both the input script and output are strings in which the input and output vocabularies each consist of 41 and 13 symbols.The advantage of the sample evaluation with each task is that we can control artificially."}, {"heading": "4.2. Models", "text": "We compared three different RNN architectures: a single-layer RNN, a stacked RNN, and the proposed GF-RNN. For each architecture, we examined three different transition functions: tanh + affine, long-term short-term memory (LSTM), and gated recurrent unit (GRU). To make a fair comparison, we limited the number of parameters of each model to be roughly similar to each other. In addition to these capacity-controlled experiments, we performed a few additional experiments for each task to further test and better understand the properties of the GF-RNN."}, {"heading": "4.2.1. LANGUAGE MODELING", "text": "For the task of character-level speech modeling, we limited the number of parameters of each model to that of a single-layer RNN with 1000 tanh units (see Table 1 for more details), each model is trained for a maximum of 100 epochs. We used RMSProp (Hinton, 2012) and Impulse to tune the model parameters (Graves, 2013). According to the preliminary experiments and their results on the validation theorem, we used a learning rate of 0.001 and an impulse coefficient of 0.9 when the models were trained with either GRU or LSTM units. It was necessary to choose a much smaller learning rate of 5 x 10 \u2212 5 in the case of tanh units to ensure the stability of learning. Whenever the norm of gradient explodes, we halve the learning rate. Each update is performed with a minibatch of 100 sub-sequences of length 100 each to avoid memory overflow problems when full backward propagation occurs."}, {"heading": "4.2.2. PYTHON PROGRAM EVALUATION", "text": "For the task of evaluating the Python program, we used an RNN encoder decoder-based approach to learn how to assign Python scripts to the corresponding outputs, as from Cho et al. (2014); Sutskever et al. (2014) for machine translation. When the models are built, Python scripts are fed into the RNN encoder, and the hidden state of the RNN encoder is unfolded for 50 times. Prediction is made by the RNN decoder, whose initial hidden state is initialized with the last hidden state of the RNN encoder. The first hidden state of the RNN h0 encoder is always set to a zero vector. For this task, we used GRU and LSTM units with or without gated feedback connections. Each encoder or RNN decoder has three hidden layers."}, {"heading": "5. Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Language Modeling", "text": "It is clear that the proposed gated feedback architecture outperforms the other base architectures we have tried in conjunction with widely used gated units such as LSTM and GRU. However, the proposed architecture has failed to improve the performance of a vanilla RNN with tanh units. In addition to the final modeling of the performance in which we plotted the learning curves of some models against the wall clock time (measured in seconds), RNNs trained with the proposed gatedfeedback architecture tend to make much faster progress over time. This behavior is observed both when the number of parameters is limited and when the number of hid-den units is limited, suggesting that the proposed GF-RNN significantly facilitates the optimization / learning process."}, {"heading": "5.2. Python Program Evaluation", "text": "Fig. 3 shows the test results of each model displayed in heat maps. Accuracy tends to decrease with the length of the target sequences or the number of nesting levels, with the difficulty or complexity of the Python program increasing. We observed that GF-RNNs exceed stacked RNNNNs in most test sets, regardless of the type of units. Fig. 3 (c) represents the gaps between the test accuracy of stacked RNNNs and GF-RNNNNs calculated by subtracting (a) from (b). In Fig. 3 (c), the red and yellow colors indicating large gains focus on upper or right regions (either the number of nesting levels or the length of the target sequences increases), making it easier to see that the GF-RNN exceeds the stacked RNN, especially as the number of nested layers or the length of the target sequence increases."}, {"heading": "6. Conclusion", "text": "Our experiments focused on demanding sequence modeling tasks of character language modeling and evaluation of Python programs, and the results were consistent across data sets, clearly showing that gated feedback architecture is helpful when the models are trained on complicated sequences that involve long-term dependencies. We also demonstrated that gated feedback architecture was faster in the time between wall clock and training and performed better when compared to standard stacked RNNs at the same capacity. Great GFLSTM was able to outperform the previously reported best results in modeling of character language, suggesting that GF RNNNs are also scalable. GF RNNNs were able to outperform standard stacked RNNNs and the best previous records of Python program evaluation tasks with varying degrees of difficulty."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Bastien et al., 2012) and Pylearn2 (Goodfellow et al., 2013) as well as Yann N. Dauphin and Laurent Dinh for their insightful comments and discussions. We would like to thank the following research funding and computer support agencies: NSERC, Samsung, Calcul Que \u00b2 bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "Technical report, arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal"], "venue": "In Adv. Neural Inf. Proc. Sys", "citeRegEx": "Bengio et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["El Hihi", "Salah", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hihi et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hihi et al\\.", "year": 1995}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred A"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "Michiel", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "Neural networks for machine learning", "author": ["Hinton", "Geoffrey"], "venue": "Coursera, video lectures,", "citeRegEx": "Hinton and Geoffrey.,? \\Q2012\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["Hochreiter", "Sepp"], "venue": "Diploma thesis, Institut fu\u0308r Informatik, Lehrstuhl Prof. Brauer, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Hochreiter", "Sepp"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1998}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The human knowledge compression contest", "author": ["Hutter", "Marcus"], "venue": null, "citeRegEx": "Hutter and Marcus.,? \\Q2012\\E", "shortCiteRegEx": "Hutter and Marcus.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A clockwork rnn", "author": ["Koutn\u0131\u0301k", "Jan", "Greff", "Klaus", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Koutn\u0131\u0301k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutn\u0131\u0301k et al\\.", "year": 2014}, {"title": "Statistical Language Models based on Neural Networks", "author": ["Mikolov", "Tomas"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov and Tomas.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tomas.", "year": 2012}, {"title": "Subword language modeling with neural networks", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Deoras", "Anoop", "Le", "HaiSon", "Kombrink", "Stefan", "J. Cernocky"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["Stollenga", "Marijn F", "Masci", "Jonathan", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Stollenga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stollenga et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recent studies have revealed that RNNs using gating units can achieve promising results in both classification and generation tasks (see, e.g., Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 132, "endOffset": 204}, {"referenceID": 22, "context": "Recent studies have revealed that RNNs using gating units can achieve promising results in both classification and generation tasks (see, e.g., Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 132, "endOffset": 204}, {"referenceID": 2, "context": "Although RNNs can theoretically capture any long-term dependency in an input sequence, it is well-known to be difficult to train an RNN to actually do so (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 154, "endOffset": 211}, {"referenceID": 16, "context": "More recently, Koutn\u0131\u0301k et al. (2014) proposed a more explicit approach to partition the hidden units in an RNN into groups such that each group receives the signal from the input and the other groups at a separate, predefined rate, which allows feedback information between these partitions to be propagated at multiple timescales.", "startOffset": 15, "endOffset": 38}, {"referenceID": 16, "context": "More recently, Koutn\u0131\u0301k et al. (2014) proposed a more explicit approach to partition the hidden units in an RNN into groups such that each group receives the signal from the input and the other groups at a separate, predefined rate, which allows feedback information between these partitions to be propagated at multiple timescales. Stollenga et al. (2014) recently showed the importance of feedback information across multiple levels of feature hierarchy, however, with feedforward neural networks.", "startOffset": 15, "endOffset": 357}, {"referenceID": 2, "context": "The difficulty of training an RNN to capture long-term dependencies has been known for long (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 92, "endOffset": 149}, {"referenceID": 2, "context": "The difficulty of training an RNN to capture long-term dependencies has been known for long (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998). A previously successful approaches to this fundamental challenge has been to modify the state-to-state transition function to encourage some hidden units to adaptively maintain long-term memory, creating paths in the time-unfolded RNN, such that gradients can flow over many timesteps. Long short-term memory (LSTM) was proposed by Hochreiter & Schmidhuber (1997) to specifically address this issue of learning long-term dependencies.", "startOffset": 111, "endOffset": 515}, {"referenceID": 2, "context": "The difficulty of training an RNN to capture long-term dependencies has been known for long (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998). A previously successful approaches to this fundamental challenge has been to modify the state-to-state transition function to encourage some hidden units to adaptively maintain long-term memory, creating paths in the time-unfolded RNN, such that gradients can flow over many timesteps. Long short-term memory (LSTM) was proposed by Hochreiter & Schmidhuber (1997) to specifically address this issue of learning long-term dependencies. The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary. More recently, Cho et al. (2014) proposed a gated recurrent unit (GRU) which adaptively remembers and forgets its state based on the input signal to the unit.", "startOffset": 111, "endOffset": 736}, {"referenceID": 6, "context": "Since the initial 1997 proposal, several variants of the LSTM have been introduced (Gers et al., 2000; Zaremba et al., 2014).", "startOffset": 83, "endOffset": 124}, {"referenceID": 23, "context": "Since the initial 1997 proposal, several variants of the LSTM have been introduced (Gers et al., 2000; Zaremba et al., 2014).", "startOffset": 83, "endOffset": 124}, {"referenceID": 6, "context": "Since the initial 1997 proposal, several variants of the LSTM have been introduced (Gers et al., 2000; Zaremba et al., 2014). Here we follow the implementation provided by Zaremba et al. (2014).", "startOffset": 84, "endOffset": 194}, {"referenceID": 4, "context": "The GRU was recently proposed by Cho et al. (2014). Like the LSTM, it was designed to adaptively reset or update its memory content.", "startOffset": 33, "endOffset": 51}, {"referenceID": 16, "context": "The clockwork RNN (CW-RNN) (Koutn\u0131\u0301k et al., 2014) implemented this by allowing the i-th module to operate at the rate of 2i\u22121, where i is a positive integer, meaning that the module is updated only when t mod 2i\u22121 = 0.", "startOffset": 27, "endOffset": 50}, {"referenceID": 18, "context": "Closely following the protocols in (Mikolov et al., 2012; Graves, 2013), we used the first 90 MBytes of characters to train a model, the next 5 MBytes as a validation set, and the remaining as a test set, with the vocabulary of 205 characters including a token for an unknown character.", "startOffset": 35, "endOffset": 71}, {"referenceID": 4, "context": "For the task of Python program evaluation, we used an RNN encoder-decoder based approach to learn the mapping from Python scripts to the corresponding outputs as done by Cho et al. (2014); Sutskever et al.", "startOffset": 170, "endOffset": 188}, {"referenceID": 4, "context": "For the task of Python program evaluation, we used an RNN encoder-decoder based approach to learn the mapping from Python scripts to the corresponding outputs as done by Cho et al. (2014); Sutskever et al. (2014) for machine translation.", "startOffset": 170, "endOffset": 213}, {"referenceID": 21, "context": "Test set BPC of neural language models trained on the Hutter dataset, MRNN = multiplicative RNN results from Sutskever et al. (2011) and Stacked LSTM results from Graves (2013).", "startOffset": 109, "endOffset": 133}, {"referenceID": 21, "context": "Test set BPC of neural language models trained on the Hutter dataset, MRNN = multiplicative RNN results from Sutskever et al. (2011) and Stacked LSTM results from Graves (2013).", "startOffset": 109, "endOffset": 177}, {"referenceID": 21, "context": "In Table 4, we present the test set BPC by a multiplicative RNN (Sutskever et al., 2011), a stacked LSTM (Graves, 2013) and the GF-RNN with LSTM units.", "startOffset": 64, "endOffset": 88}, {"referenceID": 21, "context": "In Table 4, we present the test set BPC by a multiplicative RNN (Sutskever et al., 2011), a stacked LSTM (Graves, 2013) and the GF-RNN with LSTM units. The performance of the proposed GF-RNN is comparable to, or better than, the previously reported best results. Note that Sutskever et al. (2011) used the vocabulary of 86 characters (removed XML tags and the Wikipedia markups), and their result is not directly comparable with ours.", "startOffset": 65, "endOffset": 297}, {"referenceID": 1, "context": "The authors would like to thank the developers of Theano (Bastien et al., 2012) and Pylearn2 (Goodfellow et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 7, "context": ", 2012) and Pylearn2 (Goodfellow et al., 2013).", "startOffset": 21, "endOffset": 46}], "year": 2015, "abstractText": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GFRNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.", "creator": "LaTeX with hyperref package"}}}