{"id": "1502.02551", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Deep Learning with Limited Numerical Precision", "abstract": "Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.", "histories": [["v1", "Mon, 9 Feb 2015 16:37:29 GMT  (400kb,D)", "http://arxiv.org/abs/1502.02551v1", "10 pages, 6 figures, 1 table"]], "COMMENTS": "10 pages, 6 figures, 1 table", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["suyog gupta", "ankur agrawal", "kailash gopalakrishnan", "pritish narayanan"], "accepted": true, "id": "1502.02551"}, "pdf": {"name": "1502.02551.pdf", "metadata": {"source": "META", "title": "Deep Learning with Limited Numerical Precision", "authors": ["Suyog Gupta", "Kailash Gopalakrishnan"], "emails": ["suyog@us.ibm.com", "ankuragr@us.ibm.com", "kailash@us.ibm.com", "pnaraya@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2. Related Work", "text": "Determining the accuracy of data representation and processing units is an important design decision in the hardware (analog or digital) of artificial neural networks. It is not surprising that a rich body of literature aims to quantify the impact of this choice on the performance of the network. Some recent studies, which include this approach, focus primarily on the implementation of the feeder stage (inference), assuming that the network is equipped with high-precision calculations. Some recent studies, which rely on the vector instructions for performing multiple 8-bit operations (Vanhoucke et al., 2011), or rely on the use of reconfigurable hardware (FPGAs) for energy-efficient inferences (Farabet et al., 2011; Gokal."}, {"heading": "3. Limited Precision Arithmetic", "text": "Standard implementations of deep neural network training using the back-propagation algorithm typically use 32-bit floating-point representation of real numbers for data storage and manipulation. Instead, consider the generalized representation of fixed-point numbers: [QI.QF], where QI and QF correspond to the integer or fraction of the number, respectively; the number of integer bits (IL) plus the number of fractionated bits (FL) equals the total number of bits used to represent the number. Thesum IL + FL is also referred to as word length WL. In this essay, we use the notation < IL, FL > to specify a fixed-point representation where IL (FL) equals the length of the integer (fraction) portion of the number. We also use the designation of the smallest positive number that can be represented in the predefined fixed-point format. Therefore, < IL, Fixed-maFL, Fixed-FL-FL-FL-FL-2 format limits the precision to \u2212 FL and FL-IL-1."}, {"heading": "3.1. Rounding Modes", "text": "How to follow in the sections, the rounding mode assumed during the conversion of a number & < IL = > FLFL is probably represented with the float or a higher precision1 fixed point format into a lower precision fixed point representation < IL, FL >, we define bxc as the largest integer multiple of (= 2 \u2212 FL) less than or equal to x and consider the following rounding schemes: \u2022 IL to next round (x, < IL, FL >) = bxc if bxc \u2264 x \u2264 x \u2264 bxc + 2 < x \u2264 bxc + \u2022 Stochastic rounding: The probability of rounding x to bxc is proportional to the proximity of x to bxc: round (x, < IL, FL, FL >)."}, {"heading": "4. Training Deep Networks", "text": "In this section, we present the results of our study on the effect of the use of limited precision data representation during deep neural network training. We look at both fully connected deep neural networks (DNN) and conventional neural networks (CNN) and present the results for the MNIST (Lecun & Cortes) and CIFAR10 (Krizhevsky & Hinton, 2009) datasets. As a starting point for the comparison, we first evaluate the network performance (in terms of the reduction rate of both the training error and the error on the test set) using conventional 32-bit floating point arithmetic. We then narrow down the neural network parameters (weights W \u2212 l, error quotas, distorted Bl), as well as the other intermediate variables generated during the back propagation algorithm (layer outputs Y l, back propagated error rates, error quotas, weighting etc."}, {"heading": "4.1. MNIST", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1. Fully connected DNN", "text": "In the first group of experiments, we construct a fully connected neural network with 2 hidden layers, each containing 1000 units with ReLU activation function, and train this network to recognize the handwritten digits of the MNIST dataset. This dataset includes 60,000 training images and 10,000 test images - each image is 28 x 28 pixels with a digit from 0 to 9. Pixel values are normalized to be in the range [0, 1], but no other form of data preprocessing or augmentation is performed. Weights in each layer are initialized by scanning random values from N (0, 0.01), while the biasvectors are initialized to 0. The network is initialized using minibatch stochastic gradient descent (SGD) with a minibatch size of 100 to minimize entropy function."}, {"heading": "4.1.2. CNN", "text": "Using the MNIST dataset, we also evaluate a CNN with an architecture similar to LeNet-5 (LeCun et al., 1998), which consists of 2 revolutionary layers with 5x5 filters and ReLU activation function. The first layer has 8 feature maps, while the second revolutionary layer produces 16 feature maps. Each revolutionary layer is followed by a pooling / subsampling layer. The pooling layers implement the maximum pooling function via non-overlapping pooling windows of size 2x2. The output of the second pooling layer feeds into a fully connected layer consisting of 128 ReLU neurons, which is then converted into a 10-way softmax output layer. For the training of this network, we adopt an exponentially decreasing learning rate - scaling by a factor of 0.95 after each epoch of training, the learning rate is set to 0.9 for the first epoch of 0.1."}, {"heading": "4.2. CIFAR10", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "5. Hardware Prototyping", "text": "The execution time of the mini-batch stochastic gradient descent algorithm is dominated by a number of GEMM operations in the areas of feed-forward, error-back propagation and weight update calculation 6. As a result, an improvement in the processing throughput of GEMM operations was seen in an improvement in training time. GPUs, which offer a large number of parallel vector processors and a high memory bandwidth, were therefore very effective in accelerating these workloads.In this section we describe an FPGA-based hardware accelerator for matrix matrix multiplication. Our choice of using FPGAs as a hardware substrate is motivated by two factors. Firstly, FPGAs allow fast hardware development times and significantly lower costs compared to ASICs7. Secondly, while preparing this paper, we were aware that a very recent work is being done (Couraribaux, which shares a motivation)."}, {"heading": "5.1. System Description", "text": "Figure 4 shows a block diagram of our Fixedpoint Matrix Multiplier. The DSP units within the FPGA are organized as a massively parallel 2-dimensional systolic array (SA) (Kung, 1982) of size n such that n2 < 840 forms the core of the multiplier and is described in more detail in the next subsection. Most of the block RAM on the FPGA is called the L2 cache, in which a fraction of the input matrices are stored. READ logic sends data requests to the DDR3 memory and organizes the incoming data into the L2 cache. WRITE logic sends computed results back to external memory. The L2-to-SA circuit shifts relevant rows and columns from the L2 cache into the array. The TOPcontroller coordinates the entire process."}, {"heading": "5.2. Systolic Array Architecture", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "5.3. Results", "text": "For a 28x28 systolic array implemented on the KintexK325T FPGA, Xilinx \"Vivado synthesis and place-and-route tool estimated a maximum switching frequency of 166 MHz and a power consumption of 7 W. This corresponds to a throughput of 260 Gops / s with a power efficiency of 37 G-ops / s / W. Compared to the Intel i7-3720QM CPU, the NVIDIA GT650m and the GTX780 GPUs, which all achieve power efficiency in the range of 1-5 G-ops / s / W (Gokhale et al., 2014), Table 1 provides a summary of the utilization of various resources in the FPGA. Throughput rates can benefit from the migration to newer Xilinx FPGAs, such as the Ultrascale series, which have a much higher number of DSP units and can potentially operate at higher frequencies."}, {"heading": "6. Conclusion", "text": "In this paper, we take a top-down approach that exploits the noise tolerance of deep neural networks and their training algorithms to influence the design of low-level computing units. Specifically, replacing floating-point units with fixed-point computing brings significant improvements in energy efficiency and computing throughput while potentially jeopardizing the performance of the neural network. For high-precision fixed-point computing where traditional rounding schemes fail, the introduction of stochastic rounding during deep neural network training delivers results that are almost identical to 32-bit floating-point computing. In addition, we implement an energy-efficient architecture for high-throughput matrix multiplication that incorporates stochastic rounding with very low overhead."}], "references": [{"title": "Deep learning with cots hpc systems", "author": ["Coates", "Adam", "Huval", "Brody", "Wang", "Tao", "Wu", "David", "Catanzaro", "Bryan", "Andrew", "Ng"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "Low precision arithmetic for deep learning", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Neuflow: A runtime reconfigurable dataflow processor for vision", "author": ["Farabet", "Cl\u00e9ment", "Martini", "Berin", "Corda", "Benoit", "Akselrod", "Polina", "Culurciello", "Eugenio", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "A 240 gops/s mobile coprocessor for deep neural networks", "author": ["Gokhale", "Vinayak", "Jin", "Jonghoon", "Dundar", "Aysegul", "Martini", "Berin", "Culurciello", "Eugenio"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "Gokhale et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gokhale et al\\.", "year": 2014}, {"title": "A vlsi architecture for highperformance, low-cost, on-chip learning", "author": ["Hammerstrom", "Dan"], "venue": "In Neural Networks,", "citeRegEx": "Hammerstrom and Dan.,? \\Q1990\\E", "shortCiteRegEx": "Hammerstrom and Dan.", "year": 1990}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Probabilistic rounding in neural network learning with limited", "author": ["H\u00f6hfeld", "Markus", "Fahlman", "Scott E"], "venue": "precision. Neurocomputing,", "citeRegEx": "H\u00f6hfeld et al\\.,? \\Q1992\\E", "shortCiteRegEx": "H\u00f6hfeld et al\\.", "year": 1992}, {"title": "Finite precision error analysis of neural network hardware implementations", "author": ["JL Holt", "Hwang", "Jenq-Neng"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "Holt et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Holt et al\\.", "year": 1993}, {"title": "An artificial neural network accelerator using general purpose 24 bit floating point digital signal processors", "author": ["Iwata", "Akira", "Yoshida", "Yukio", "Matsuda", "Satoshi", "Sato", "Yukimasa", "Suzumura", "Nobuo"], "venue": "In Neural Networks,", "citeRegEx": "Iwata et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Iwata et al\\.", "year": 1989}, {"title": "X1000 real-time phoneme recognition vlsi using feed-forward deep neural networks", "author": ["Kim", "Jonghong", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Why systolic architectures", "author": ["H.T. Kung"], "venue": "Jan 1982. doi: 10.1109/MC.1982", "citeRegEx": "Kung,? \\Q1982\\E", "shortCiteRegEx": "Kung", "year": 1982}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Enhanced mlp performance and fault tolerance resulting from synaptic weight noise during training", "author": ["Murray", "Alan F", "Edwards", "Peter J"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Murray et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Murray et al\\.", "year": 1994}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Recht", "Benjamin", "Re", "Christopher", "Wright", "Stephen", "Niu", "Feng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Deep image: Scaling up image recognition", "author": ["Wu", "Ren", "Yan", "Shengen", "Shan", "Yi", "Dang", "Qingqing", "Sun", "Gang"], "venue": "arXiv preprint arXiv:1501.02876,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": ", 2014), or high-end graphics processors (GPUs) (Krizhevsky & Hinton, 2009), or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et al., 2013; Wu et al., 2015).", "startOffset": 139, "endOffset": 177}, {"referenceID": 16, "context": ", 2014), or high-end graphics processors (GPUs) (Krizhevsky & Hinton, 2009), or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et al., 2013; Wu et al., 2015).", "startOffset": 139, "endOffset": 177}, {"referenceID": 14, "context": "With the exception of employing the asynchronous version of the stochastic gradient descent algorithm (Recht et al., 2011) to reduce network traffic, the state-of-the-art large-scale deep learning systems fail to adequately capitalize on the error-resiliency of their workloads.", "startOffset": 102, "endOffset": 122}, {"referenceID": 15, "context": "Some recent studies that embrace this approach have relied on the processor\u2019s vector instructions to perform multiple 8 bit operations in parallel (Vanhoucke et al., 2011), or employ reconfigurable hardware (FPGAs) for highthroughput, energy-efficient inference (Farabet et al.", "startOffset": 147, "endOffset": 171}, {"referenceID": 2, "context": ", 2011), or employ reconfigurable hardware (FPGAs) for highthroughput, energy-efficient inference (Farabet et al., 2011; Gokhale et al., 2014), or take the route of custom hardware implementations (Kim et al.", "startOffset": 98, "endOffset": 142}, {"referenceID": 3, "context": ", 2011), or employ reconfigurable hardware (FPGAs) for highthroughput, energy-efficient inference (Farabet et al., 2011; Gokhale et al., 2014), or take the route of custom hardware implementations (Kim et al.", "startOffset": 98, "endOffset": 142}, {"referenceID": 9, "context": ", 2014), or take the route of custom hardware implementations (Kim et al., 2014; Merolla et al., 2014).", "startOffset": 62, "endOffset": 102}, {"referenceID": 8, "context": "(Iwata et al., 1989) implements the backpropagation algorithm using 24-bit floating-point processing units.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "with an architecture similar to LeNet-5 (LeCun et al., 1998).", "startOffset": 40, "endOffset": 60}, {"referenceID": 5, "context": "This architecture is similar to the one introduced in (Hinton et al., 2012) with the exception that it does not implement local response normalization or dropout layers.", "startOffset": 54, "endOffset": 75}, {"referenceID": 1, "context": "While preparing this paper, we became aware of a very recent work (Courbariaux et al., 2014) that shares our motivations but adopts an orthogonal approach.", "startOffset": 66, "endOffset": 92}, {"referenceID": 11, "context": "The DSP units within the FPGA are organized as a massively parallel 2dimensional systolic array (SA) (Kung, 1982) of size n such that n < 840.", "startOffset": 101, "endOffset": 113}, {"referenceID": 3, "context": "This compares very favorably against the Intel i7-3720QM CPU, the NVIDIA GT650m and the GTX780 GPUs, all of which achieve power efficiency in the range of 1-5 G-ops/s/W (Gokhale et al., 2014).", "startOffset": 169, "endOffset": 191}], "year": 2015, "abstractText": "Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network\u2019s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.", "creator": "LaTeX with hyperref package"}}}