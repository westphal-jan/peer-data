{"id": "1502.02606", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "The Power of Randomization: Distributed Submodular Maximization on Massive Datasets", "abstract": "A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We develop a simple distributed algorithm that is embarrassingly parallel and it achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting.", "histories": [["v1", "Mon, 9 Feb 2015 19:04:43 GMT  (1448kb,D)", "https://arxiv.org/abs/1502.02606v1", null], ["v2", "Wed, 22 Apr 2015 17:49:22 GMT  (1448kb,D)", "http://arxiv.org/abs/1502.02606v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DC", "authors": ["rafael da ponte barbosa", "alina ene", "huy l nguyen", "justin ward"], "accepted": true, "id": "1502.02606"}, "pdf": {"name": "1502.02606.pdf", "metadata": {"source": "CRF", "title": "The Power of Randomization Distributed Submodular Maximization on Massive Datasets\u2217", "authors": ["Rafael da Ponte Barbosa", "Alina Ene", "Huy L. Nguy\u1ec5n", "Justin Ward"], "emails": ["J.D.Ward}@dcs.warwick.ac.uk", "hlnguyen@cs.princeton.edu"], "sections": [{"heading": "1 Introduction", "text": "A specified function f: 2V \u2192 R \u2265 0 on a specified area V is submodular when f (A) + f (B) \u2265 f (B) + f (B) for any two areas A, B V. Multiple interest problems can be modeled as maximizing a submodular objective function that is subject to certain limitations: max f (A) is subject to A \u00b2 C, with C \u00b2 2V being the family of feasible solutions. In fact, the general metaphor of optimizing a limited submodular function can capture a variety of problems in machine learning applications, including exemplator clustering, document summary, sensor placement, image segmentation, maximum entropy sampling, and functional selection problems. The authors are listed alphabetically. \u00b2 The work supported by EPSRC is EP / J021814 / 1.ar Xiv: 150 2.02 606v 2 [csAt the same time, in this large amount of physical data is widely collected."}, {"heading": "1.1 Background and Related Work", "text": "In our setting, we assume that the data is much larger than the memory of a single machine and must therefore be distributed across all machines. At a high level, a MapReduce calculation is performed in several rounds. In a given round, the data is shifted back and forth between machines. After the data is distributed, each machine performs some calculations on the data it has at its disposal. Output of these calculations is either returned as an end result or becomes input in the next MapReduce round. We emphasize that the machines can only communicate and exchange data during the shuffle phase. To put our contributions into context, we briefly discuss two distributed greedy algorithms, the complementary trade-offs in terms of approximation guarantees and head communication.Mirzasoleiman gives a distributed algorithm called GreDi Di Di Di Di Di Di Di to achieve a complementary solution."}, {"heading": "1.2 Our Contribution", "text": "In this paper, we show that we can achieve both the communication efficiency of the GreeDi algorithm and a verifiable, constant factor approximation guarantee. In fact, our algorithm is the GreeDi algorithm with a very simple and decisive modification: instead of randomly partitioning the data on the machines, we randomly partition the data set. Our analysis could perhaps provide a theoretical justification for the very good empirical performance of the GreeDi algorithm, which was previously established in the extensive experiments of [10]. In contrast, our analysis suggests that the approach can perform well in much broader environments than originally envisaged. Originally, the GreeDi algorithm was studied in the specific case of monotonous submodular maximization under a cardinality compulsion. In contrast, our analysis applies to any hereditary limitation. Specifically, we show that our randomized variant of the GreeDi algorithm yields an approximate factor approximation for each approximation."}, {"heading": "1.3 Preliminaries", "text": "MapReduce (A).A (A).A (A).A (A).A (A).A (A).A).A (A).A (A).A (A).A (A).A (A).A).A (A).A (A).A (A).A).A (A).A).A (A).A).A (A).A (A).A (A).A).A (A).A (A).A).A (A).A (A).A).A (A).A).A (A).A (A).A).A (A) (A).A) (A) (A).A) (A) (A) (A).A) (A) (A) (A).A) (A) (A).A) (A).A (A) (A).A).A (A) (A).A) (A) (A).A).A (A) (A).A) (A).A).A (A) (A).A) (A).A).A (A) (A).A) (A).A) (A).A) (A) (A).A) (A).A) (A) (A) (A).A) (A) (A).A) (A).A) (A) (.A) (A) (A) (A).A) (A) (A) (A).A) (A) (.A) (A) (A) (A) (A) (A).A) (A) (A) (A).A) (A) (A).A) (A) (A).A) (A) (A) (A).A) (A) (A) (A).A) (A) (A) (A) (A).A) (A) (A) (A) (A) (A).A) (A) (A) (A).A) (A) (A)."}, {"heading": "2 The Standard Greedy Algorithm", "text": "Before describing our general algorithm, let us remember the standard greedy algorithm, greedy, represented in algorithm 1. The algorithm takes as input < V, I, f >, where V is a group of elements, I 2V is a hereditary constraint, represented as a membership oracle for I, and f: 2V \u2192 R \u2265 0 is a nonnegative submodular function, represented as an oracle of value. Given < V, I, f > Greedy constructs an iterative solution S I by selecting at each step the element that maximizes the marginal increase of f. For some A V, we leave Greedy (A) the sentence S I produced by the greedy algorithm, which only satisfies elements from A. The greedy algorithm satisfies the following property: Algorithm 2 The distributed algorithm edge gredi for e V do assign e to a machine i, selected at random."}, {"heading": "3 A Randomized, Distributed Greedy Algorithm for Monotone Submodular Maximization", "text": "In the second round, we put all these selected subsets on a single machine, and run some algorithms on this machine to choose a final solution that is better: the final solution of the elements on this machine. In the second round, we place all these selected subsets on a single machine, and run some algorithms on this machine to choose a final solution."}, {"heading": "4 Non-Monotone Submodular Functions", "text": "Algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorithms-algorith"}, {"heading": "5 Experiments", "text": "In fact, it is the case that we will be able to go in search of a solution that is capable, that we are able, that we are in, in order to find it."}, {"heading": "B A tight example for Deterministic GreeDI", "text": "Here we give a series of examples showing that the Greek algorithm of Mirzasoleiman et al. can achieve no approximation better than 1 / \u221a k. Consider the following instance of maximum k coverage. We have \"2 + 1 machines and k = '+' 2. Let N be a basic set of '2 +' 3 elements, N = {1, 2,...,\" 2 + '3}. We define a coverage function on a covering S of subsets of N as follows. In the following we define how the sets of S are distributed on the machines. \"On machine 1 we have the following sets of OPT:\" O1 = {1, 2,. \u2212 Oool., \"O2 = {' + 1,.., 2 '}., O' = {2 - elements of machines,."}], "references": [{"title": "A tight linear time (1/2)approximation for unconstrained submodular maximization", "author": ["Niv Buchbinder", "Moran Feldman", "Joseph Naor", "Roy Schwartz"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Mapreduce: Simplified data processing on large clusters", "author": ["Jeffrey Dean", "Sanjay Ghemawat"], "venue": "Commun. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014II", "author": ["M L Fisher", "G L Nemhauser", "L A Wolsey"], "venue": "Mathematical Programming Studies,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1978}, {"title": "Constrained nonmonotone submodular maximization: Offline and secretary algorithms", "author": ["Anupam Gupta", "Aaron Roth", "Grant Schoenebeck", "Kunal Talwar"], "venue": "In Internet and Network Economics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Composable core-sets for diversity and coverage maximization", "author": ["Piotr Indyk", "Sepideh Mahabadi", "Mohammad Mahdian", "Vahab S Mirrokni"], "venue": "In Proceedings of the 33rd ACM SIGMOD- SIGACT-SIGART symposium on Principles of database systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "The efficacy of the \"greedy\" algorithm", "author": ["T A Jenkyns"], "venue": "In Proceedings of the 7th Southeastern Conference on Combinatorics, Graph Theory, and Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1976}, {"title": "Finding groups in data: an introduction to cluster analysis, volume 344", "author": ["Leonard Kaufman", "Peter J Rousseeuw"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Fast greedy algorithms in mapreduce and streaming", "author": ["Ravi Kumar", "Benjamin Moseley", "Sergei Vassilvitskii", "Andrea Vattani"], "venue": "In Proceedings of the twenty-fifth annual ACM symposium on Parallelism in algorithms and architectures,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "How to select a good training-data subset for transcription: Submodular active selection for sequences", "author": ["Hui Lin", "Jeff A. Bilmes"], "venue": "In Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Distributed submodular maximization: Identifying representative elements in massive data", "author": ["Baharan Mirzasoleiman", "Amin Karbasi", "Rik Sarkar", "Andreas Krause"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014I", "author": ["George L Nemhauser", "Laurence A Wolsey", "Marshall L Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1978}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Antonio Torralba", "Robert Fergus", "William T Freeman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1958}], "referenceMentions": [{"referenceID": 1, "context": "Our algorithm can be easily implemented in a parallel model of computation such as MapReduce [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "[10] give a distributed algorithm, called GreeDi, for maximizing a monotone submodular function subject to a cardinality constraint.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] give a family of instances where the approximation achieved is only 1/min {k,m} if the solution picked on each of the machines is the optimal solution for the set of items on the machine.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] give distributed algorithms for maximizing a monotone submodular function subject to a cardinality or more generally, a matroid constraint.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Their algorithm combines the Threshold Greedy algorithm of [4] with a sample and prune strategy.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "[5] studied coreset approaches to develop distributed algorithms for finding representative and yet diverse subsets in large collections.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Our analysis may perhaps provide some theoretical justification for the very good empirical performance of the GreeDi algorithm that was established previously in the extensive experiments of [10].", "startOffset": 192, "endOffset": 196}, {"referenceID": 5, "context": "This is the case not only for cardinality constraints, but also for matroid constraints, knapsack constraints, and p-system constraints [6], which generalize the intersection of p matroid constraints.", "startOffset": 136, "endOffset": 139}, {"referenceID": 0, "context": "The Lov\u00e1sz extension f\u2212 : [0, 1]V \u2192 R\u22650 of a submodular function f is given by: f\u2212(x) = E \u03b8\u2208U(0,1) [f({i : xi \u2265 \u03b8})].", "startOffset": 26, "endOffset": 32}, {"referenceID": 0, "context": "For any submodular function f , the Lov\u00e1sz extension f\u2212 satisfies the following properties: (1) f(1S) = f(S) for all S \u2286 V , (2) f\u2212 is convex, and (3) f\u2212(c \u00b7 x) \u2265 c \u00b7 f\u2212(x) for any c \u2208 [0, 1].", "startOffset": 185, "endOffset": 191}, {"referenceID": 0, "context": "Let S be a random set, and suppose that E[1S ] = c \u00b7p (for c \u2208 [0, 1]).", "startOffset": 63, "endOffset": 69}, {"referenceID": 10, "context": "3The best-known values of \u03b1 are taken from [11] (cardinality), [3] (matroid and p-system), and [13] (knapsack).", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "3The best-known values of \u03b1 are taken from [11] (cardinality), [3] (matroid and p-system), and [13] (knapsack).", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "Let p \u2208 [0, 1]n be the following vector.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "In the second inequality, we have used the fact that f\u2212 is convex and f\u2212(c\u00b7x) \u2265 cf\u2212(x) for any constant c \u2208 [0, 1].", "startOffset": 108, "endOffset": 114}, {"referenceID": 3, "context": "Our approach is a slight modification of the randomized, distributed greedy algorithm described in Section 3, and it builds on the work of [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "In the second inequality, we have used the fact that f\u2212 is convex and f\u2212(c\u00b7x) \u2265 cf\u2212(x) for any constant c \u2208 [0, 1].", "startOffset": 108, "endOffset": 114}, {"referenceID": 3, "context": "We remark that one can use the following approach on the last machine [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "Additionally, we select a subset T3 \u2286 T1 using an unconstrained submodular maximization algorithm on T1, such as the Double Greedy algorithm of [1], which is a 2 -approximation.", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "If Greedy satisfies property GP, then it follows from the analysis of [4] that the resulting solution T satisfies f(T ) \u2265 \u03b1 2(1+\u03b1) \u00b7 f(OPT).", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "We experimentally evaluate and compare the following distributed algorithms for maximizing a monotone submodular function subject to a cardinality constraint: the RandGreeDi algorithm described in Section 3, the deterministic GreeDi algorithm of [10], and the Sample&Prune algorithm of [8].", "startOffset": 246, "endOffset": 250}, {"referenceID": 7, "context": "We experimentally evaluate and compare the following distributed algorithms for maximizing a monotone submodular function subject to a cardinality constraint: the RandGreeDi algorithm described in Section 3, the deterministic GreeDi algorithm of [10], and the Sample&Prune algorithm of [8].", "startOffset": 286, "endOffset": 289}, {"referenceID": 9, "context": "Our experimental setup is similar to that of [10].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "Our goal is to find a representative set of objects from a dataset by solving a k-medoid problem [7] that aims to minimize the sum of pairwise dissimilarities between the chosen objects and the entire dataset.", "startOffset": 97, "endOffset": 100}, {"referenceID": 11, "context": "Tiny Images experiments: In our experiments, we used a subset of the Tiny Images dataset consisting of 32\u00d732 RGB images [12], each represented as 3, 072 dimensional vector.", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "We also considered a variant similar to that described by [10], in which 10,000 additional random images from the original dataset were added to the final machine.", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We used the objective function of Lin and Bilmes [9]: f(A) = \u2211 i\u2208V \u2211 j\u2208A sij \u2212 \u03bb \u2211 i,j\u2208A sij , where \u03bb is a redundancy parameter and {sij}ij is a similarity matrix.", "startOffset": 49, "endOffset": 52}], "year": 2015, "abstractText": "A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We develop a simple distributed algorithm that is embarrassingly parallel and it achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting.", "creator": "LaTeX with hyperref package"}}}