{"id": "1502.02643", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions", "abstract": "Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of ``simple\" functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.", "histories": [["v1", "Mon, 9 Feb 2015 20:31:18 GMT  (390kb,D)", "http://arxiv.org/abs/1502.02643v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["alina ene", "huy l nguyen"], "accepted": true, "id": "1502.02643"}, "pdf": {"name": "1502.02643.pdf", "metadata": {"source": "CRF", "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions", "authors": ["Alina Ene", "Huy L. Nguy\u1ec5n"], "emails": ["A.Ene@dcs.warwick.ac.uk", "hlnguyen@cs.princeton.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only take a few days to reach an agreement."}, {"heading": "1.1 Preliminaries and Background", "text": "It is not the way in which we solve the problem of reduction of F: 2V \u2192 R is easy if there is a fast subroutine for reduction of F: 2V \u2192 R. (D) F: 2V \u2192 R is easy if there is a fast subroutine for reduction. (D) F: 2V \u2192 R is easy if there is a reduction for reduction. (D) F: 2V \u2192 R is easy if there is a fast subroutine for reduction. (D) F: 2V (D) S: 1) S: 1 (D) S: 1 (D) S: 1 (D) S: 1 (D) S: 1 (D) S: 1 (D) S: 1 (D) S: 1 (D) S: 1 (D) S: 1 (D) S: 1 (D: S: S) (S 1: S 1: D: S) (S 1 D: S: S) 1 (D: S 1: S) (D: S 1: S: S) 1 (D: S: S: S) 1 (D: S: S: S) 1 (D: S: S: S) 1 (D: S: S: D) 1 (D: S: S: S: S: D) 1 (D: S: S: D) 1 (S: D (S: S: S: D) 1 (S: D: S: D) 1 (S: D: S: S: D) 1 (S: D: S: S: D) 1 (S: D: S: D (S: S: D) 1 (S: D) 1 (S: D: S: D) 1 (S: D: D: S: D) 1 (S: D: S: D) 1 (S: S: S: D) 1 (S: D: S: D) 1 (S: D: S: D) 1 (S: D: S: D) 1 (S: S: S: D) 1 (S: S: D) 1 (S: D: D: S: D) 1 (S: D: S: S: D) 1 (S: S: D) 1 (S: S: D) 1 (S: D: D) (S: D) 1 (S:"}, {"heading": "2 Random Coordinate Descent Algorithm", "text": "In this section, we will give an algorithm for the problem (Prox-DSM) based on the Random Coordinate Analysis (RCDM) of [10]. The algorithm is very easy to implement and oracles are used for problems of form that are very efficient in this section. In the rest of this section, we will analyze the convergence rate of the RCDM algorithms. We emphasize that the objective function of (Prox-DSM) is not strongly convex and therefore cannot be used as a black box Nesterov's analysis of the RCDM method to minimize strongly convex functions. Instead, we will use the specific structure of the problem to achieve convergence guarantees that match strong convectives."}, {"heading": "3 Accelerated Coordinate Descent Algorithm", "text": "In this section, we enter an accelerated random coordinate system (ACDM) with a starting point of 0 > Y (Prox-DSM). The algorithms use the APPROX algorithm problem (Prox-DSM). < r (Prox-DSM) problem. < r (Prox-DSM) algorithm runs in a sequence of epochs (see Figure 3). In each epoch, the algorithm begins with the solution of the previous epoch and the APPROX algorithms for the epoch (nr3 / 2) iterate. The solution constructed by the APPROX algorithms will be the starting point of the next epoch. Note that for each i, the graded ACDM algorithms for (Prox-DSM) < < we can be the starting point of the next epoch."}, {"heading": "4 Experiments", "text": "We are empirically able to evaluate and compare the following algorithms: the RCDM described in Section 2, the ACDM described in Section 3, and the alternate projections (AP).The AP algorithms solve the following problem of best approximation, which is (Prox-DSM) equivalent to (Prox-DSM): min a-DSM, y-DSM, a-DSM, 2 (Best-Approx), where the AP algorithms begin with a point a (1), a (2),., a (r), Rnr: p = 1 a (i) and Y = 1 B (Fi).The AP algorithms begin with a point a0 (ak, yk)."}, {"heading": "A Proof of Lemma 1", "text": "According to the definition of the Lov\u00e1sz extension, we can for each i-line (r) havefi (x) = max y (i) \u0445B (Fi) < y (i), x >. Thereafter x-Rn r-i = 1 (fi (x) + 1 2r-x-2) = min x-Rn r-i = 1 (max y (i) x-B (fi) < y (i), x > + 12r-x-2) = min x-Rn max y (1) x-B (F1),..., B (Fr) min x-Rn r r-i = 1 (< y (i), x > + 12r-x-2) = max y (1), B (F1), c-i (F1),..., y (r), B (Fr) min x-Rn r r r-i r-i = 1 (< y (i), x-R-R-2) = max y (1), b-R min-R (c-i), c-i (x-i) = 1 (< y), x-R-R-R-R-R (R) = max (1), b-R-R-R (min-R), c-c (c (c-i), c-i (x-i), x-i (x-i) = 1 (< y (i), x-R-R-R-R-R) = max (1), x-R-R-R-R, x-R-R-R-R-R-R (1) = max (1), x-R-R-R-R-R-R-R, x-R-R-R-R, x-R-R-R-R-R-R, x-R-R-R-R, x-R-R-R, x-R-R-R-R, x-R-R-R-R-R-R, x-R-R-R-R-R-R-R-R-R-R-R-R, x-R-R-R"}, {"heading": "B Proofs omitted from Section 2", "text": "If X-Rnr and X are a subset of Rnr, we leave X-X (x) the orthogonal supplement of X-Proposition 9. For any point x-Rnr, Q-Rnr (x) = STSx and thus Q-Q (x) = x-STSx.Proof: Since Q is the zero space of S, Q-Proposition is the line space of S. Since the lines of S are orthonormal, they form a basis for Q-Proposition 10. So if we call v1,.. vn the lines of S, we have Q-Proof-Q-Proof-Proof-Proof-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Proof-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Proof-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Proof-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Proof-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Proof-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Proof-Prop-Prop-Prop-Prop-Prop-Prop-Prop-Prop-"}], "references": [{"title": "Learning with submodular functions: A convex optimization perspective", "author": ["Francis Bach"], "venue": "ArXiv preprint arXiv:1111.6453,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Accelerated, parallel and proximal coordinate descent", "author": ["Olivier Fercoq", "Peter Richt\u00e1rik"], "venue": "ArXiv preprint arXiv:1312.5799,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "A push-relabel framework for submodular function minimization and applications to parametric optimization", "author": ["Lisa Fleischer", "Satoru Iwata"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "The ellipsoid method and its consequences in combinatorial optimization", "author": ["Martin Gr\u00f6tschel", "L\u00e1szl\u00f3 Lov\u00e1sz", "Alexander Schrijver"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1981}, {"title": "A faster scaling algorithm for minimizing submodular functions", "author": ["Satoru Iwata"], "venue": "SIAM Journal on Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Reflection methods for user-friendly submodular optimization", "author": ["Stefanie Jegelka", "Francis Bach", "Suvrit Sra"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["Stefanie Jegelka", "Jeff Bilmes"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Minimizing a sum of submodular functions", "author": ["Vladimir Kolmogorov"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Submodular functions and convexity", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz"], "venue": "In Mathematical Programming The State of the Art,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yu. Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "On the convergence rate of decomposable submodular function minimization", "author": ["Robert Nishihara", "Stefanie Jegelka", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "On the convergence rate of decomposable submodular function minimization", "author": ["Robert Nishihara", "Stefanie Jegelka", "Michael I Jordan"], "venue": "ArXiv preprint arXiv:1406.6474,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["James B Orlin"], "venue": "Mathematical Programming,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Convex analysis", "author": ["R Tyrrell Rockafellar"], "venue": "Number 28 in Princeton Mathematical Series. Princeton university press,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1970}, {"title": "Grabcut: Interactive foreground extraction using iterated graph cuts", "author": ["Carsten Rother", "Vladimir Kolmogorov", "Andrew Blake"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "A combinatorial algorithm minimizing submodular functions in strongly polynomial time", "author": ["Alexander Schrijver"], "venue": "Journal of Combinatorial Theory, Series B,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Combinatorial optimization: polyhedra and efficiency, volume 24", "author": ["Alexander Schrijver"], "venue": "Springer Science & Business Media,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 16, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 4, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 2, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 13, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 7, "context": "Following [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions.", "startOffset": 10, "endOffset": 24}, {"referenceID": 5, "context": "Following [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions.", "startOffset": 10, "endOffset": 24}, {"referenceID": 11, "context": "Following [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions.", "startOffset": 10, "endOffset": 24}, {"referenceID": 5, "context": "The recent work of [6, 8, 19] has developed several algorithms with very good empirical performance that exploit the special structure of decomposable functions.", "startOffset": 19, "endOffset": 29}, {"referenceID": 7, "context": "The recent work of [6, 8, 19] has developed several algorithms with very good empirical performance that exploit the special structure of decomposable functions.", "startOffset": 19, "endOffset": 29}, {"referenceID": 5, "context": "In particular, [6] have shown that the problem of minimizing decomposable submodular functions can be formulated as a distance minimization problem between two polytopes.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "This formulation, when coupled with powerful convex optimization techniques such as gradient descent or projection methods, it yields algorithms that are very fast in practice and very simple to implement [6].", "startOffset": 205, "endOffset": 208}, {"referenceID": 11, "context": "[12] have made a significant progress in this direction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] using the ellipsoid method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "There are several combinatorial algorithms for the problem [17, 5, 3, 14].", "startOffset": 59, "endOffset": 73}, {"referenceID": 4, "context": "There are several combinatorial algorithms for the problem [17, 5, 3, 14].", "startOffset": 59, "endOffset": 73}, {"referenceID": 2, "context": "There are several combinatorial algorithms for the problem [17, 5, 3, 14].", "startOffset": 59, "endOffset": 73}, {"referenceID": 13, "context": "There are several combinatorial algorithms for the problem [17, 5, 3, 14].", "startOffset": 59, "endOffset": 73}, {"referenceID": 13, "context": "Among the combinatorial methods, Orlin\u2019s algorithm [14] achieves the best time complexity of O(n5T +n6), where n is the size of the ground set and T is the maximum amount of time it takes to evaluate the function.", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12].", "startOffset": 87, "endOffset": 101}, {"referenceID": 5, "context": "Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12].", "startOffset": 87, "endOffset": 101}, {"referenceID": 11, "context": "Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12].", "startOffset": 87, "endOffset": 101}, {"referenceID": 11, "context": "[12] give an algorithm based on alternating projections that achieves a linear convergence rate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "The Lov\u00e1sz extension f of F can be written as the support function of the base polytope B(F ): f(x) = max w\u2208B(F ) \u3008w, x\u3009 \u2200x \u2208 R Even though the base polytope B(F ) has exponentially many vertices, the Lov\u00e1sz extension f can be evaluated efficiently using the greedy algorithm of Edmonds (see for example [18]).", "startOffset": 304, "endOffset": 308}, {"referenceID": 8, "context": "Lov\u00e1sz showed that a set function F is submodular if and only if its Lov\u00e1sz extension f is convex [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "min x\u2208[0,1]n f(x) \u2261 min x\u2208[0,1]n r \u2211", "startOffset": 6, "endOffset": 11}, {"referenceID": 0, "context": "min x\u2208[0,1]n f(x) \u2261 min x\u2208[0,1]n r \u2211", "startOffset": 26, "endOffset": 31}, {"referenceID": 5, "context": "Following previous work [6, 12], we consider a proximal version of the problem (\u2016\u00b7 \u2016 denotes the `2-norm):", "startOffset": 24, "endOffset": 31}, {"referenceID": 11, "context": "Following previous work [6, 12], we consider a proximal version of the problem (\u2016\u00b7 \u2016 denotes the `2-norm):", "startOffset": 24, "endOffset": 31}, {"referenceID": 0, "context": "6 in [1]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Lemma 1 ([6]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "Lemma 1 was proved in [6]; we include a proof in Section A for completeness.", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "In this section, we give an algorithm for the problem (Prox-DSM) that is based on the random coordinate gradient descent method (RCDM) of [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "Our analysis shows that the RCDM algorithm is faster by a factor of r than the alternating projections algorithm from [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "First, we build on the work of [12] in order to prove a key theorem (Theorem 2).", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "We start by introducing some notation; for the most part, we follow the notation of [10] and [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "We start by introducing some notation; for the most part, we follow the notation of [10] and [12].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Our first step is to prove the following key theorem that builds on the work of [12].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "The proof of Theorem 2 uses the following key result from [13].", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "We will need the following definitions from [13].", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "By combining Corollary 5 and Proposition 11 from [13], we obtain the following theorem.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "Theorem 3 ([12]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "In the remainder of this section, we use Nesterov\u2019s analysis [10] in conjunction with Theorem 2 in order to show that the RCDM algorithm converges at a linear rate.", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "5 in [11]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "Figure 2: The APPROX algorithm of [2] applied to (Prox-DSM).", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "The algorithm uses the APPROX algorithm of Fercoq and Richt\u00e1rik [2] as a subroutine.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "The APPROX algorithm (Algorithm 2 in [2]), when applied to the (Prox-DSM) problem, yields the algorithm in Figure 2.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "In the remainder of this section, we use the analysis of [2] together with Theorem 2 in order to show that the ACDM algorithm converges at a linear rate.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "After ` epochs of the ACDM algorithm (equivalently, (4nr3/2 + 1)` iterations), we have E[g(y`+1)\u2212 g(y\u2217)] \u2264 1 2`+1 (g(y0)\u2212 g(y \u2217)) In the following lemma, we show that the objective function of (Prox-DSM) satisfies Assumption 1 in [2] and thus the convergence analysis given in [2] can be applied to our setting.", "startOffset": 230, "endOffset": 233}, {"referenceID": 1, "context": "After ` epochs of the ACDM algorithm (equivalently, (4nr3/2 + 1)` iterations), we have E[g(y`+1)\u2212 g(y\u2217)] \u2264 1 2`+1 (g(y0)\u2212 g(y \u2217)) In the following lemma, we show that the objective function of (Prox-DSM) satisfies Assumption 1 in [2] and thus the convergence analysis given in [2] can be applied to our setting.", "startOffset": 277, "endOffset": 280}, {"referenceID": 1, "context": "Lemma 7 together with Theorem 3 in [2] give us the following theorem.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Theorem 8 (Theorem 3 of [2]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "Proof: It follows from Lemma 7 that the objective function g of (Prox-DSM) and the random blocks Rk used by the APPROX algorithm satisfy Assumption 1 in [2] with \u03c4 = 1 and \u03bdi = 4 for each i \u2208 {1, 2, .", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Thus we can apply Theorem 3 in [2].", "startOffset": 31, "endOffset": 34}, {"referenceID": 11, "context": "We empirically evaluate and compare the following algorithms: the RCDM described in Section 2, the ACDM described in Section 3, and the alternating projections (AP) algorithm of [12].", "startOffset": 178, "endOffset": 182}, {"referenceID": 5, "context": "Our experimental setup is similar to that of [6].", "startOffset": 45, "endOffset": 48}, {"referenceID": 15, "context": "We set up the image segmentation problems on a 8-neighbor grid graph with unary potentials derived from Gaussian Mixture Models of color features [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "Additionally, we compute a discrete duality gap for the discrete problem (DSM) and the dual of its Lov\u00e1sz relaxation; the latter is the problem maxz\u2208B(F )(z)\u2212(V ), where (z)\u2212 = min {z, 0} applied elementwise [6].", "startOffset": 208, "endOffset": 211}, {"referenceID": 6, "context": "We evaluated the algorithms on four image segmentation instances2 [7, 16].", "startOffset": 66, "endOffset": 73}, {"referenceID": 15, "context": "We evaluated the algorithms on four image segmentation instances2 [7, 16].", "startOffset": 66, "endOffset": 73}], "year": 2015, "abstractText": "Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of \u201csimple\" functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.", "creator": "LaTeX with hyperref package"}}}