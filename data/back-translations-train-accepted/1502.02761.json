{"id": "1502.02761", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2015", "title": "Generative Moment Matching Networks", "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.", "histories": [["v1", "Tue, 10 Feb 2015 02:54:58 GMT  (855kb,D)", "http://arxiv.org/abs/1502.02761v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["yujia li", "kevin swersky", "richard s zemel"], "accepted": true, "id": "1502.02761"}, "pdf": {"name": "1502.02761.pdf", "metadata": {"source": "META", "title": "Generative Moment Matching Networks", "authors": ["Yujia Li", "Kevin Swersky", "Richard Zemel"], "emails": ["YUJIALI@CS.TORONTO.EDU", "KSWERSKY@CS.TORONTO.EDU", "ZEMEL@CS.TORONTO.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that the greater people who are able to put themselves in the world, to understand and understand what they are doing in order to change the world, to change the world, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to think, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change,"}, {"heading": "2. Maximum Mean Discrepancy", "text": "Suppose we get two sets of samples X = {xi} Ni = 1 and Y = {yj} Mj = 1 and are asked if the generating distributions are PX = PY. The maximum mean discrepancy is a frequency estimator to answer this question, also known as the two sample tests (Gretton et al., 2007; 2012a). The idea is simple: compare statistics between the two sets of data and if they are similar, then the samples probably come from the same distribution.Formally, the following MMD measurement calculates the mean squared difference in the statistics of the two samples. LMMD2 = a trick where 1N N N = 1 distributions (xi) -1 M = 1 M = 1 M = 1 (yj) a certain number of terms."}, {"heading": "3. Related Work", "text": "It is true that most of them are able to play by the rules that they have imposed on themselves, and that they are able to play by the rules that they have imposed on themselves. (...) In fact, it is true that they are able to break the rules. (...) It is not that they play by the rules, that they play by the rules. (...) It is not that they play by the rules. (...) It is that they play by the rules. (...) It is as if they play by the rules. (...)"}, {"heading": "4. Generative Moment Matching Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data Space Networks", "text": "The high-level idea of the GMMN is to use a neural network to learn a deterministic mapping of samples of a simple, easy-to-sample distribution to samples from the data distribution. The architecture of the generative network is exactly the same as a generative adversarial network (Goodfellow et al., 2014). However, we suggest to train the network by simply minimizing the MMD criterion by avoiding the hard minimax objective function used in generative adversarial networks. (4) Here U (h) = 12I [\u2212 1) is a stochastic hidden layer h-RH with hidden units at the top with a previous uniform distribution to each unit x-x."}, {"heading": "4.2. Auto-Encoder Code Space Networks", "text": "Real-world data can be complicated and high-dimensional, which is one reason why generative modeling is such a difficult task. Auto-encoders, on the other hand, are designed to solve a arguably simpler task of reconstruction. If properly trained, auto-encoder models can represent data very well in a code space that captures enough statistical information to reliably reconstruct the data. An auto-encoder's code space has several advantages for creating a generative model. The first is that dimensionality can be explicitly controlled. Visual data that is represented in a high dimension often exist on a low-dimensional variety. This is advantageous for a statistical estimator like MMD, because the amount of data needed to generate a reliable estimator grows with the dimensionality of the data (Ramdas et al., 2015). The second advantage is that each dimension of code space at the end represents variations in the original data space."}, {"heading": "4.3. Practical Considerations", "text": "Bandwidth parameters in the kernel play a crucial role in determining the statistical efficiency of MMD, and the optimum setting is an open problem. A good heuristics is to perform a line search to obtain the bandwidth that generates the maximum distance (Sriperumbudur et al., 2009), other more advanced heuristics are also available (Gretton et al., 2012b). As a simpler approximation, we use a mix of K-cores covering multiple ranges for most of our experiments, meaning we choose the kernel to be: k (x, x)."}, {"heading": "5. Experiments", "text": "We trained GMMNs on two benchmark datasets MNIST (LeCun et al., 1998) and the Toronto Face Dataset (TFD) (Susskind et al., 2010). For MNIST, we then used the standard test set of 10,000 images and split 5000 of the standard 60,000 training images for validation, with the remaining 55,000 used for training. For both datasets, we used the same training and testing sets and folding splits as they did from (Goodfellow et al., 2014), but split a small set of training data and used it as validation. For both datasets, rescaling the images to have pixel intensities between 0 and 1, we used the only preprocessing step. We trained the GMMN network both in the input data room and in the code room of an auto encoder. For all networks we used in this section, a uniform distribution was used."}, {"heading": "6. Conclusion and Future Work", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "Acknowledgements", "text": "We thank David Warde-Farley for providing helpful clarifications on (Goodfellow et al., 2014) and Charlie Tang for providing relevant references. We thank CIFAR, NSERC and Google for funding the research."}], "references": [{"title": "A learning algorithm for boltzmann machines", "author": ["D.H. Ackley", "G.E. Hinton", "T.J. Sejnowski"], "venue": "Cognitive science,", "citeRegEx": "Ackley et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Ackley et al\\.", "year": 1985}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H Larochelle"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Better mixing via deep representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Thibodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "arXiv preprint arXiv:1411.4952,", "citeRegEx": "Fang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves and Jaitly,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly", "year": 2014}, {"title": "A kernel method for the two-sampleproblem", "author": ["A. Gretton", "K.M. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gretton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2007}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "The \u201cwake-sleep\u201d algorithm for unsupervised neural networks", "author": ["G.E. Hinton", "P. Dayan", "B.J. Frey", "R.M. Neal"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "In roceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Larochelle and Murray,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers", "author": ["D.J. MacKay"], "venue": "Detectors and Associated Equipment,", "citeRegEx": "MacKay,? \\Q1995\\E", "shortCiteRegEx": "MacKay", "year": 1995}, {"title": "Neural networks for density estimation", "author": ["M. Magdon-Ismail", "A. Atiya"], "venue": "In NIPS, pp", "citeRegEx": "Magdon.Ismail and Atiya,? \\Q1998\\E", "shortCiteRegEx": "Magdon.Ismail and Atiya", "year": 1998}, {"title": "A winner-take-all method for training sparse convolutional autoencoders", "author": ["A. Makhzani", "B. Frey"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "Makhzani and Frey,? \\Q2014\\E", "shortCiteRegEx": "Makhzani and Frey", "year": 2014}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Connectionist learning of belief networks", "author": ["R.M. Neal"], "venue": "Artificial intelligence,", "citeRegEx": "Neal,? \\Q1992\\E", "shortCiteRegEx": "Neal", "year": 1992}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi and Recht,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions", "author": ["A. Ramdas", "S.J. Reddi", "B. Poczos", "A. Singh", "L. Wasserman"], "venue": "In The Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15),", "citeRegEx": "Ramdas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ramdas et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Input warping for bayesian optimization of non-stationary functions", "author": ["J. Snoek", "K. Swersky", "R.S. Zemel", "R.P. Adams"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Snoek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2014}, {"title": "Kernel choice and classifiability for rkhs embeddings of probability distributions", "author": ["B.K. Sriperumbudur", "K. Fukumizu", "A. Gretton", "G.R. Lanckriet", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2009}, {"title": "The toronto face dataset", "author": ["J. Susskind", "A. Anderson", "G.E. Hinton"], "venue": "Technical report,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning fair representations", "author": ["R. Zemel", "Y. Wu", "K. Swersky", "T. Pitassi", "C. Dwork"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Zemel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zemel et al\\.", "year": 2013}, {"title": "Fastmmd: Ensemble of circular discrepancy for efficient two-sample test", "author": ["J. Zhao", "D. Meng"], "venue": "arXiv preprint arXiv:1405.2664,", "citeRegEx": "Zhao and Meng,? \\Q2014\\E", "shortCiteRegEx": "Zhao and Meng", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014).", "startOffset": 182, "endOffset": 207}, {"referenceID": 16, "context": "Models such as convolutional neural networks (CNNs), and long short term memory (LSTM) networks are now achieving impressive results on a number of tasks such as object recognition (Krizhevsky et al., 2012; Sermanet et al., 2014; Szegedy et al., 2014), speech recognition (Graves & Jaitly, 2014; Hinton et al.", "startOffset": 181, "endOffset": 251}, {"referenceID": 32, "context": "Models such as convolutional neural networks (CNNs), and long short term memory (LSTM) networks are now achieving impressive results on a number of tasks such as object recognition (Krizhevsky et al., 2012; Sermanet et al., 2014; Szegedy et al., 2014), speech recognition (Graves & Jaitly, 2014; Hinton et al.", "startOffset": 181, "endOffset": 251}, {"referenceID": 5, "context": ", 2014), machine translation (Cho et al., 2014; Sutskever et al., 2014), and more.", "startOffset": 29, "endOffset": 71}, {"referenceID": 37, "context": ", 2014), machine translation (Cho et al., 2014; Sutskever et al., 2014), and more.", "startOffset": 29, "endOffset": 71}, {"referenceID": 0, "context": "Thus, with GMMNs it is easy to quickly draw independent random samples, as opposed to expensive MCMC procedures that are necessary in other models such as Boltzmann machines (Ackley et al., 1985; Hinton, 2002; Salakhutdinov & Hinton, 2009).", "startOffset": 174, "endOffset": 239}, {"referenceID": 11, "context": "Thus, with GMMNs it is easy to quickly draw independent random samples, as opposed to expensive MCMC procedures that are necessary in other models such as Boltzmann machines (Ackley et al., 1985; Hinton, 2002; Salakhutdinov & Hinton, 2009).", "startOffset": 174, "endOffset": 239}, {"referenceID": 7, "context": "The structure of a GMMN is most analogous to the recently proposed generative adversarial networks (GANs) (Goodfellow et al., 2014), however unlike GANs, whose training involves a difficult minimax optimization problem, GMMNs are comparatively simple; they are trained to minimize a straightforward loss function using backpropagation.", "startOffset": 106, "endOffset": 131}, {"referenceID": 9, "context": "The key idea behind GMMNs is the use of a statistical hypothesis testing framework called maximum mean discrepancy (Gretton et al., 2007).", "startOffset": 115, "endOffset": 137}, {"referenceID": 9, "context": "Maximum mean discrepancy is a frequentist estimator for answering this question, also known as the two sample test (Gretton et al., 2007; 2012a).", "startOffset": 115, "endOffset": 144}, {"referenceID": 9, "context": "When this feature space corresponds to a universal reproducing kernel Hilbert space, it is shown that asymptotically, MMD is 0 if and only if PX = PY (Gretton et al., 2007; 2012a).", "startOffset": 150, "endOffset": 179}, {"referenceID": 0, "context": "One popular class of generative models used in deep learning are undirected graphical models, such as Boltzmann machines (Ackley et al., 1985), restricted Boltzmann machines (Hinton, 2002), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009).", "startOffset": 121, "endOffset": 142}, {"referenceID": 11, "context": ", 1985), restricted Boltzmann machines (Hinton, 2002), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009).", "startOffset": 39, "endOffset": 53}, {"referenceID": 25, "context": "Next there is the class of fully visible directed models such as fully visible sigmoid belief networks (Neal, 1992) and the neural autoregressive distribution estimator (Larochelle & Murray, 2011).", "startOffset": 103, "endOffset": 115}, {"referenceID": 28, "context": "Also related to our own work, there is the class of deep, variational networks (Rezende et al., 2014; Kingma & Welling, 2014; Mnih & Gregor, 2014).", "startOffset": 79, "endOffset": 146}, {"referenceID": 19, "context": "MacKay (1995) proposed a model that is closely related to ours, which also used a feed-forward network to map the prior samples to the data space.", "startOffset": 0, "endOffset": 14}, {"referenceID": 19, "context": "MacKay (1995) proposed a model that is closely related to ours, which also used a feed-forward network to map the prior samples to the data space. However, instead of directly outputing samples, an extra distribution is associated with the output. Sampling was used extensively for learning and inference in this model. Magdon-Ismail & Atiya (1998) proposed to use a neural network to learn a transformation from the data space to another space where the transformed data points are uniformly distributed.", "startOffset": 0, "endOffset": 349}, {"referenceID": 7, "context": "The architecture of the generative network is exactly the same as a generative adversarial network (Goodfellow et al., 2014).", "startOffset": 99, "endOffset": 124}, {"referenceID": 27, "context": "This is beneficial for a statistical estimator like MMD because the amount of data required to produce a reliable estimator grows with the dimensionality of the data (Ramdas et al., 2015).", "startOffset": 166, "endOffset": 187}, {"referenceID": 1, "context": "Greedy layer-wise pretraining of the auto-encoder (Bengio et al., 2007).", "startOffset": 50, "endOffset": 71}, {"referenceID": 29, "context": "This is analogous to the motivation behind contractive and denoising auto-encoders (Rifai et al., 2011; Vincent et al., 2008).", "startOffset": 83, "endOffset": 125}, {"referenceID": 38, "context": "This is analogous to the motivation behind contractive and denoising auto-encoders (Rifai et al., 2011; Vincent et al., 2008).", "startOffset": 83, "endOffset": 125}, {"referenceID": 35, "context": "A good heuristic is to perform a line search to obtain the bandwidth that produces the maximal distance (Sriperumbudur et al., 2009), other more advanced heuristics are also available (Gretton et al.", "startOffset": 104, "endOffset": 132}, {"referenceID": 18, "context": "We trained GMMNs on two benchmark datasets MNIST (LeCun et al., 1998) and the Toronto Face Dataset (TFD) (Susskind et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 36, "context": ", 1998) and the Toronto Face Dataset (TFD) (Susskind et al., 2010).", "startOffset": 43, "endOffset": 66}, {"referenceID": 7, "context": "For TFD, we used the same training and test sets and fold splits as used by (Goodfellow et al., 2014), but split out a small set of the training data and used it as the validation set.", "startOffset": 76, "endOffset": 101}, {"referenceID": 4, "context": ", 2013a), Deep Generative Stochastic Network (Deep GSN) from (Bengio et al., 2014) and Adversarial nets (GANs) from (Goodfellow et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 7, "context": ", 2014) and Adversarial nets (GANs) from (Goodfellow et al., 2014).", "startOffset": 41, "endOffset": 66}, {"referenceID": 4, "context": ", 2013a), (Bengio et al., 2014), and (Goodfellow et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": ", 2014), and (Goodfellow et al., 2014).", "startOffset": 13, "endOffset": 38}, {"referenceID": 33, "context": "The hyperparameters of the networks, including the learning rate and momentum for both auto-encoder and GMMN training, dropout rate for the auto-encoder, and number of hidden units on each layer of both auto-encoder and GMMN, were tuned using Bayesian optimization (Snoek et al., 2012; 2014)1 to optimize the validation set likelihood under the Gaussian Parzen window density estimation.", "startOffset": 265, "endOffset": 291}, {"referenceID": 7, "context": "To determine whether the models learned to merely copy the data, we follow the example of (Goodfellow et al., 2014) and visualize the nearest neighbour of several samples in terms of Euclidean pixel-wise distance in Figure 2(e-h).", "startOffset": 90, "endOffset": 115}, {"referenceID": 12, "context": "This is reminiscent of the recognition models used in the wake-sleep algorithm (Hinton et al., 1995), or variational auto-encoders (Kingma & Welling, 2014).", "startOffset": 79, "endOffset": 100}, {"referenceID": 40, "context": "An interesting application of MMD that is not directly related to generative modelling comes from recent work on learning fair representations (Zemel et al., 2013).", "startOffset": 143, "endOffset": 163}, {"referenceID": 22, "context": "For example, it may be possible to use a GMMN+AE with convolutional auto-encoders (Zeiler et al., 2010; Masci et al., 2011; Makhzani & Frey, 2014) in order to create generative models of high resolution color images.", "startOffset": 82, "endOffset": 146}, {"referenceID": 7, "context": "We thank David Warde-Farley for helpful clarifications regarding (Goodfellow et al., 2014), and Charlie Tang for providing relevant references.", "startOffset": 65, "endOffset": 90}], "year": 2015, "abstractText": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.", "creator": "LaTeX with hyperref package"}}}