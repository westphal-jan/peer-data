{"id": "1502.02763", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2015", "title": "Cascading Bandits: Learning to Rank in the Cascade Model", "abstract": "The cascade model is a well-established model of user interaction with content. In this work, we propose cascading bandits, a learning variant of the model where the objective is to learn $K$ most attractive items out of $L$ ground items. We cast the problem as a stochastic combinatorial bandit with a non-linear reward function and partially observed weights of items. Both of these are challenging in the context of combinatorial bandits. We propose two computationally-efficient algorithms for our problem, CascadeUCB1 and CascadeKL-UCB, and prove gap-dependent upper bounds on their regret. We also derive a lower bound for cascading bandits and show that it matches the upper bound of CascadeKL-UCB up to a logarithmic factor. Finally, we evaluate our algorithms on synthetic problems. Our experiments demonstrate that the algorithms perform well and robustly even when our modeling assumptions are violated.", "histories": [["v1", "Tue, 10 Feb 2015 02:56:04 GMT  (85kb,D)", "http://arxiv.org/abs/1502.02763v1", null], ["v2", "Mon, 18 May 2015 19:03:38 GMT  (78kb,D)", "http://arxiv.org/abs/1502.02763v2", "Proceedings of the 32nd International Conference on Machine Learning"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["branislav kveton", "csaba szepesv\u00e1ri", "zheng wen", "azin ashkan"], "accepted": true, "id": "1502.02763"}, "pdf": {"name": "1502.02763.pdf", "metadata": {"source": "META", "title": "Cascading Bandits", "authors": ["Branislav Kveton", "Csaba Szepesv\u00e1ri", "Zheng Wen", "Azin Ashkan"], "emails": ["KVETON@ADOBE.COM", "SZEPESVA@CS.UALBERTA.CA", "ZHENGWEN@YAHOO-INC.COM", "AZIN.ASHKAN@TECHNICOLOR.COM"], "sections": [{"heading": "1. Introduction", "text": "The user interacts with the model as follows: The user is recommended a list of K-elements, such as websites or movies. Each element attracts the user with some likelihood, regardless of the other elements. The user examines the list from the first elements to the last, and stops at the first attractive element. In the context of web search, this is interpreted as a click point. The elements before the first attractive element are not attractive, because the user examines these elements, but does not stop looking at them. It is unknown which elements are attractive after the first attractive element, because the user never examines these elements. The list of K-elements, which maximizes the likelihood that the user will find an attractive element, are K-elements with the highest likelihood of attractiveness. Although the model is effective, the so-called position bias was found in the statistics of search engine clicks (we find an attractive element), we are an attractive element, we are an attractive element, we are an attractive element, we are an element, we are, we are an attractive element, we are an element, we are an attractive element."}, {"heading": "2. Background", "text": "In all models, the user is recommended an ordered list of K-sites A = (a1,.., aK), which we refer to as Articles. These articles belong to some dregs E = {1,.., L}, like all possible websites. Models differ in how they explain clicks and many models have been proposed (Becker et al., 2007; Craswell et al., 2008; Richardson et al., 2007). Due to spatial limitations, we only review the Cascade model (Craswell et al., 2008) which is one of the most popular models of user interaction with content (Chapelle & Zhang, 2009)."}, {"heading": "3. Cascading Bandits", "text": "In this section, we propose a learning variant of the cascade model (Section 2) and two learning algorithms to solve it (Section 3.2). For simplicity, all random variables are shown in bold."}, {"heading": "3.1. Setting", "text": "A more general approach to the use of e-learning systems, which are created on the basis of the user's observations, is the way in which the e-learning agents select the e-learning preferences of the user. (Wt) n, in which the e-learning agents select the e-learning preferences of the user. (Wt) n, in which the e-learning agents select the e-learning agents. (at1,.) n, in which the k-learning agents select the e-learning agents. (Wt) n, in which the e-learning agents select a list of e-learning agents. (Wt) n, in which the e-learning agents select a list of e-learning agents. (at1,.) n, in which the k-learning agents select a list of e-learning agents. (Wt) n, in which the e-learning agents select a list of e-learning agents."}, {"heading": "3.2. Algorithms", "text": "We propose two algorithms for cascading bandits: the first algorithm is motivated by q q (Auer et al., 2002) and we call it CascadeUCB1. \u2212 The second algorithm is motivated by KL-UCB (Garivier & Cappe, 2011) and therefore we call it CascadeKL-UCB. Both algorithms have the same shape (algorithm 1) and differ only in how they calculate the upper confidence limits (UCBs) for the expected items at the time t. After the UCBs are calculated, the algorithms choose K-items with the largest UCBs: At = arg maxA (E) f (Ut) f (UCBs) and recommend them to the user. Note that the solution At is determined only up to a permutation of the items therein. While the payout is not affected by this order, the observations are in our algorithms."}, {"heading": "3.3. Initialization", "text": "We assume that both CascadeUCB1 and CascadeKL-UCB are initialized by a weight sample w0 \u0445 P. This assumption is relatively mild if L is small. In particular, such a sample can be generated in most L steps by repeatedly recommending a list of objects that have not yet been observed, the corresponding regret being O (L)."}, {"heading": "4. Analysis", "text": "Our analysis is broken down as follows: In Section 4.1, we break down regret in due course t so that we can identify the gaps between the attraction probabilities of individual items and the indicators of events that observe the recommended items. This is the key step of our analysis. In Section 4.2, we tie the regret of CascadeUCB1 and CascadeKL-UCB. In Section 4.3, we derive a lower limit for cascading bandits. Finally, we discuss our results in Section 4.4. The combinatory structure of our problem is a matroid, finding K most attractive items from L. However, we cannot simply analyze our problem as a matroid bandit (Kveton et al., 2014a), since our reward function is non-linear and the weights of recommended items are partially observed. Our analysis is also very different from that of Radlinski et al. (2008). To obtain close repentance bounds, we must modify the analysis of our base algorithms, such as UCL and black boxes."}, {"heading": "4.1. Regret Decomposition", "text": "Before we begin, we will introduce some notation and conventions. Without loss of generality, we assume that the items in E are ordered in such a way that the items in E (1) and K (1) are ordered in such a way that the items in K (1) and K (1) are optimal, that the items in E (1) and K (1) are optimal when the items in K (1) and K (1) are ordered. We say that the items in E (1) and K (1) are suboptimal when the items in K < e). The hardness of the distinction of a suboptimal item e is measured by a gap between the probability of attraction of the items: \"e, e\" and the items in E (e). (3) Whenever we view an ordered list of items in the list of items in the listing. Our most important technical problem is below. The item states that the value of the expected form of the differences in a particular beneficial product can be written."}, {"heading": "4.2. Upper bounds", "text": "In this section, we deduce two upper limits of n-step regret of CascadeUCB1 and CascadeKL-UCB.Theorem 2. The full proof is in Appendix A.1. The proof has four main steps. First, we have bound the regret of the event that w (e) is outside the most likely confidence interval by w (Tt \u2212 1 (e) (e) (e) for any point e. Second, we decompose the regret in due time t and tie it from above using Theorem 1. Third, we have the number of times each suboptimal element is selected in n steps. Fourth, we apply the peeling argument of Kveton et al. (2014a) and eliminate an additional factor of K in our upper limit."}, {"heading": "4.3. Lower Bound", "text": "Our lower limit is derived from the following problem: The base set is L itemsE = {1,.., L}. The distribution P is a product of the L Bernoulli distributions Pe, each of which is parameterized by the following parameters: w-K = (E) = {p, e-K; p-\u0430, otherwise (6), where \"K\" is the gap between an optimal and a suboptimal item. We refer to the resulting bandit as Btop-K = (E, P (K, p). Our lower limit is derived for the class of consistent algorithms, which is defined as follows. We say that the algorithm is consistent if for a cascading bandit each suboptimal item A and each suboptimal item 0, E [Tn (A)] = o (n\u03b1), where Tn (A) is the number of cases in which the solution A is chosen in steps. The limitation to the algorithm is consistent without loss of the liquidity."}, {"heading": "4.4. Discussion", "text": "The boundaries are O (log n), scale linearly with the number of items L and improve as the number of recommended items K. The boundaries do not depend on the order of the recommended items. To this end, we consider Btop-K = (E, P (K, p, p)) Bandit from Section 4.3, where p = 1 / K. In this problem, Theorem 4 implies an asymptotic lower boundary of: (L \u2212 K) p (p \u2212 valent) p. \u2212 Ktop-K = (K, p = UC3, where p = 1 / K. In Theorem 4, Casymptotic implies lower boundary of: (L \u2212 K) p."}, {"heading": "5. Experiments", "text": "We conduct three experiments: In the first experiment, we show that the regret of our algorithms grows, as our upper limits suggest (Section 4.2); in the second experiment, we modify our algorithms to recommend K items in reverse order, from the smallest UCB to the largest; and in the third experiment, we show that CascadeKL-UCB performs well on various synthetic problems; and we also compare it to RankedKL-UCB."}, {"heading": "5.1. Regret Bounds", "text": "In this section, we confirm the qualitative behaviour of our limits (Section 4.2). We experiment with the problem class Btop-K = (E, P (K, p, \u2206) of Section 4.3 with p = 0.2. We vary L, K and \u0445; and perform CascadeUCB1 and CascadeKL-UCB in n = 105 steps. The maximum attraction probability p = 0.2 is chosen to be close to 1 / K for the maximum number of recommended points K in our experiment. In this environment, our upper limits are disproportionately narrow (Section 4.4), and the regret of our algorithms should be scaled similarly to our upper limits. The recommended points in Algorithm 1 are ordered in decreasing order of their UCBs. This order is motivated by practical applications, higher ranking points in the web search are considered more attractive. Our results are shown in Table 1."}, {"heading": "5.2. \u201cWorst-of-Best First\u201d Item Ordering", "text": "In the second experiment, we evaluated a variant of our algorithms in which the recommended items are sorted in increasing order of their UCBs. This choice seems strange, perhaps even dangerous. In practice, the user might be annoyed if high-ranking items were not very attractive. On the other hand, the user would give a lot of feedback about low-quality items and accelerate learning. Note that the expected return in our model is invariant in the order of the recommended items (Section 3.2). Therefore, the payout does not change when the items are reordered. In any case, we find it important to investigate the effects of this counterintuitive order in the context of the present model, at least to demonstrate an undesirable effect of our modeling assumptions. The experimental setup is the same as in Section 5.1. Our results are reported in Table 2. Compared to Table 1, the regret of UCcadeBUCK is both the most current case and the most interesting one of all of the BUCK phenomena as well as for a large BUCL."}, {"heading": "5.3. Imperfect Model", "text": "In the third experiment, we do not examine CascadeKL-UCB if the user is not satisfied with the reward to test its potential beyond the cascade model. Specifically, we generate data from a dynamic Bayesian network (DBN) model (Chapelle & Zhang, 2009), a popular extension of the cascade model. The DBN model is based on the user attraction probabilities (a1,., aK) and examines them from the first element a1 to the last aK. Item ak attracts the user with the probability (ak). If the user is clicked on the element and the element satisfies the user with the probability (ak)."}, {"heading": "6. Conclusions", "text": "In this paper, we propose a learning variant of the cascade model (Craswell et al., 2008), a popular model of user interaction with content. We propose two algorithms to solve it, CascadeUCB1 and CascadeKL-UCB, and incorporate their regret. Our analysis deals with two challenging problems in stochastic combinatorial bandits. Firstly, the reward function is not linear. Secondly, we only partially observe the weights of the selected items. We also demonstrate a lower limit for cascading bandits and show that it corresponds to the upper limit of CascadeKL-UCB up to a logarithmic factor. Finally, we evaluate the proposed algorithms on several synthetic sample lems and show that they provide robust performance even if our modeling assumptions are violated. We leave many questions of interest open. Firstly, our algorithms perform very well, even our modeling assumptions are violated (section 5.3 of DN), suggesting that the implementation of the model may not be implemented in practice."}, {"heading": "A. Proofs of Main Theorems", "text": "The proof of the theorem 2Let Rt = 1 (At, wt) be the regret of CascadeUCB1 at time t, where At is the solution chosen by CascadeUCB1 at time t, and wt are the weights at time t. Let Et = {e) is not in the high-probability confidence interval around w Tt \u2212 1 (e) (e) (e) for some e at time t; and let Et be the complement of Et, w s (e) is in the high-probability interval around w Tt \u2212 1 (e) (e) (e) (e) for some e at time t; and let Et be the complement of Et, w (e) is in the high-probability interval around w Tt \u2212 1 (e) (e) for all e at time t. Then the expected regret of CascadeUCB1 can be written as: R (n) = 1 {Et."}, {"heading": "B. Technical Lemmas", "text": "Lemma 2: K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K) K (K) K (K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K (K) K (K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K) K (K) K (K) K) K (K) K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K (K) K (K) K) (K) K (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K"}], "references": [{"title": "Improving Web search ranking by incorporating user behavior information", "author": ["Agichtein", "Eugene", "Brill", "Eric", "Dumais", "Susan"], "venue": "In Proceedings of the 29 annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Agichtein et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agichtein et al\\.", "year": 2006}, {"title": "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space", "author": ["R. Agrawal", "D. Teneketzis", "V. Anantharam"], "venue": "IEEE Transaction on Automatic Control,", "citeRegEx": "Agrawal et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1989}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Fischer", "Paul"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "An adaptive algorithm for finite stochastic partial monitoring", "author": ["G. Bart\u00f3k", "N. Zolghadr", "Szepesv\u00e1ri", "Cs"], "venue": "In ICML,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2012}, {"title": "Modeling contextual factors of click rates", "author": ["H. Becker", "C. Meek", "D.M. Chickering"], "venue": "In Proceedings of the 22 AAAI Conference on Artificial Intelligence,", "citeRegEx": "Becker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2007}, {"title": "Concentration inequalities: A nonasymptotic theory of independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2012}, {"title": "A dynamic Bayesian network click model for Web search ranking", "author": ["O. Chapelle", "Y. Zhang"], "venue": "In Proceedings of the 18 World Wide Web Conference,", "citeRegEx": "Chapelle and Zhang,? \\Q2009\\E", "shortCiteRegEx": "Chapelle and Zhang", "year": 2009}, {"title": "Combinatorial multi-armed bandit and its extension to probabilistically triggered arms", "author": ["Chen", "Wei", "Wang", "Yajun", "Yuan", "Yang"], "venue": "CoRR, abs/1407.8339,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "An experimental comparison of click position-bias models", "author": ["N. Craswell", "O. Zoeter", "M. Taylor", "B. Ramsey"], "venue": "In Proceedings of the First International Conference on Web search and Web Data Mining,", "citeRegEx": "Craswell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Craswell et al\\.", "year": 2008}, {"title": "Parametric bandits: The generalized linear case", "author": ["S. Filippi", "O. Capp\u00e9", "A. Garivier", "Szepesv\u00e1ri", "Cs"], "venue": "In NIPS,", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations", "author": ["Gai", "Yi", "Krishnamachari", "Bhaskar", "Jain", "Rahul"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "Gai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gai et al\\.", "year": 2012}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Garivier", "Aurelien", "Cappe", "Olivier"], "venue": "In Proceeding of the 24th Annual Conference on Learning Theory, pp", "citeRegEx": "Garivier et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Garivier et al\\.", "year": 2011}, {"title": "Click chain model in Web search", "author": ["F. Guo", "C. Liu", "A. Kannan", "T. Minka", "M. Taylor", "Y.M. Wang", "C. Faloutsos"], "venue": "In Proceedings of 18 World Wide Web Conference,", "citeRegEx": "Guo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "Efficient multiple-click models in Web search", "author": ["F. Guo", "C. Liu", "Y.M. Wang"], "venue": "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Guo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "Matroid bandits: Fast combinatorial optimization with learning", "author": ["Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Eydgahi", "Hoda", "Eriksson", "Brian"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Kveton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2014}, {"title": "Learning to act greedily: Polymatroid semibandits", "author": ["Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Valko", "Michal"], "venue": "CoRR, abs/1405.7752,", "citeRegEx": "Kveton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2014}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Szepesvari", "Csaba"], "venue": "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kveton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2015}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "Sequential learning for multi-channel wireless network monitoring with channel switching costs", "author": ["T. Le", "R. Zheng", "Szepesv\u00e1ri", "Cs"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Combinatorial partial monitoring game with linear feedback and its applications", "author": ["Lin", "Tian", "Abrahao", "Bruno", "Kleinberg", "Robert", "Lui", "John", "Chen", "Wei"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "From bandits to experts: On the value of side-observations", "author": ["Mannor", "Shie", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mannor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2011}, {"title": "Query chains: learning to rank from implicit feedback", "author": ["Radlinski", "Filip", "Joachims", "Thorsten"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "Radlinski et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2005}, {"title": "Learning diverse rankings with multi-armed bandits", "author": ["Radlinski", "Filip", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "In Proceedings of the Twenty-Fifth International Conference on Machine Learning,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Predicting clicks: estimating the clickthrough rate for new ads", "author": ["M. Richardson", "E. Dominowska", "R. Ragno"], "venue": "In Proceedings of the 16 International World Wide Web Conference,", "citeRegEx": "Richardson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2007}, {"title": "Ranked bandits in metric spaces: learning diverse rankings over large document collections", "author": ["Slivkins", "Aleksandrs", "Radlinski", "Filip", "Gollapudi", "Sreenivas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Slivkins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Slivkins et al\\.", "year": 2013}, {"title": "An online algorithm for maximizing submodular functions", "author": ["Streeter", "Matthew", "Golovin", "Daniel"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Streeter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Streeter et al\\.", "year": 2009}, {"title": "2014a), the above term is bounded by 12", "author": ["Kveton"], "venue": null, "citeRegEx": "Kveton,? \\Q2014\\E", "shortCiteRegEx": "Kveton", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Introduction The cascade model (Craswell et al., 2008) was originally proposed in the context of web search and is one of the most popular models of user interaction with content.", "startOffset": 31, "endOffset": 54}, {"referenceID": 8, "context": "Although simple, the model was found effective in explaining the so-called position bias in the statistics of search engine click logs (Craswell et al., 2008).", "startOffset": 135, "endOffset": 158}, {"referenceID": 22, "context": "Ranked bandits (Radlinski et al., 2008; Slivkins et al., 2013) are a popular approach in learning to rank and they are closely related to our paper.", "startOffset": 15, "endOffset": 62}, {"referenceID": 24, "context": "Ranked bandits (Radlinski et al., 2008; Slivkins et al., 2013) are a popular approach in learning to rank and they are closely related to our paper.", "startOffset": 15, "endOffset": 62}, {"referenceID": 16, "context": "This class of problems can be solved both computationally and sample efficiently (Kveton et al., 2015).", "startOffset": 81, "endOffset": 102}, {"referenceID": 7, "context": "Several other papers (Mannor & Shamir, 2011; Chen et al., 2014) considered an opposite problem to ours, where the learning agent observes the weights of items that are similar to the chosen items.", "startOffset": 21, "endOffset": 63}, {"referenceID": 10, "context": "The former is motivated by CombUCB1 (Gai et al., 2012; Kveton et al., 2015), a computationally and sample efficient algorithm for stochastic combinatorial semi-bandits.", "startOffset": 36, "endOffset": 75}, {"referenceID": 16, "context": "The former is motivated by CombUCB1 (Gai et al., 2012; Kveton et al., 2015), a computationally and sample efficient algorithm for stochastic combinatorial semi-bandits.", "startOffset": 36, "endOffset": 75}, {"referenceID": 6, "context": "Filippi et al. (2010) study a generalized linear bandit with bandit feedback.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "Chen et al. (2013) study a stochastic combinatorial semi-bandit where the reward function is a known monotone function of a linear function in unknown parameters.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Chen et al. (2013) study a stochastic combinatorial semi-bandit where the reward function is a known monotone function of a linear function in unknown parameters. Le et al. (2014) consider a network-optimization problem, where the payoff is a nonlinear function of the observations.", "startOffset": 0, "endOffset": 180}, {"referenceID": 2, "context": "Bart\u00f3k et al. (2012) considers finite partial monitoring problems.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite.", "startOffset": 0, "endOffset": 446}, {"referenceID": 1, "context": "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite. As in the case of Bart\u00f3k et al. (2012), this algorithm would suffer from computational inefficiencies in our setting.", "startOffset": 0, "endOffset": 582}, {"referenceID": 1, "context": "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite. As in the case of Bart\u00f3k et al. (2012), this algorithm would suffer from computational inefficiencies in our setting. Lin et al. (2014) recently studied partial monitoring in combinatorial bandits.", "startOffset": 0, "endOffset": 679}, {"referenceID": 1, "context": "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite. As in the case of Bart\u00f3k et al. (2012), this algorithm would suffer from computational inefficiencies in our setting. Lin et al. (2014) recently studied partial monitoring in combinatorial bandits. The setting of this paper is different from ours. In particular, Lin et al. (2014) assume that the observation is a linear transformation of the weights of the items.", "startOffset": 0, "endOffset": 824}, {"referenceID": 1, "context": "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite. As in the case of Bart\u00f3k et al. (2012), this algorithm would suffer from computational inefficiencies in our setting. Lin et al. (2014) recently studied partial monitoring in combinatorial bandits. The setting of this paper is different from ours. In particular, Lin et al. (2014) assume that the observation is a linear transformation of the weights of the items. This transformation is fixed, known, and indexed only by the action. In our work, the transformation depends on the weights of the items. Several other papers (Mannor & Shamir, 2011; Chen et al., 2014) considered an opposite problem to ours, where the learning agent observes the weights of items that are similar to the chosen items. Chen et al. (2014) studied this problem in the context of stochastic combinatorial semi-bandits.", "startOffset": 0, "endOffset": 1262}, {"referenceID": 0, "context": "Background Ranking functions in web search are typically learned by training a model of user interaction with content from click data (Agichtein et al., 2006; Radlinski & Joachims, 2005).", "startOffset": 134, "endOffset": 186}, {"referenceID": 4, "context": "they explain the clicks and many models have been proposed (Becker et al., 2007; Craswell et al., 2008; Richardson et al., 2007).", "startOffset": 59, "endOffset": 128}, {"referenceID": 8, "context": "they explain the clicks and many models have been proposed (Becker et al., 2007; Craswell et al., 2008; Richardson et al., 2007).", "startOffset": 59, "endOffset": 128}, {"referenceID": 23, "context": "they explain the clicks and many models have been proposed (Becker et al., 2007; Craswell et al., 2008; Richardson et al., 2007).", "startOffset": 59, "endOffset": 128}, {"referenceID": 8, "context": "The cascade model (Craswell et al., 2008) is one of the most popular models of user interaction with content (Chapelle & Zhang, 2009).", "startOffset": 18, "endOffset": 41}, {"referenceID": 2, "context": "The first algorithm is motivated by UCB1 (Auer et al., 2002) and we call it CascadeUCB1.", "startOffset": 41, "endOffset": 60}, {"referenceID": 14, "context": "However, we cannot analyze our problem straightforwardly as a matroid bandit (Kveton et al., 2014a), because our reward function is nonlinear and the weights of recommended items are partially observed. Our analysis is also very different from that of Radlinski et al. (2008). To obtain tight regret bounds, we need modify the analysis of our base algorithms, UCB1 and KL-UCB, instead of treating them just like black boxes.", "startOffset": 78, "endOffset": 276}, {"referenceID": 14, "context": "Fourth, we apply the peeling argument of Kveton et al. (2014a) and eliminate an extra factor of K in our upper bound.", "startOffset": 41, "endOffset": 63}, {"referenceID": 8, "context": "Conclusions In this work, we propose a learning variant of the cascade model (Craswell et al., 2008), a popular model of user interaction with content.", "startOffset": 77, "endOffset": 100}], "year": 2017, "abstractText": "The cascade model is a well-established model of user interaction with content. In this work, we propose cascading bandits, a learning variant of the model where the objective is to learn K most attractive items out of L ground items. We cast the problem as a stochastic combinatorial bandit with a non-linear reward function and partially observed weights of items. Both of these are challenging in the context of combinatorial bandits. We propose two computationally-efficient algorithms for our problem, CascadeUCB1 and CascadeKL-UCB, and prove gap-dependent upper bounds on their regret. We also derive a lower bound for cascading bandits and show that it matches the upper bound of CascadeKL-UCB up to a logarithmic factor. Finally, we evaluate our algorithms on synthetic problems. Our experiments demonstrate that the algorithms perform well and robustly even when our modeling assumptions are violated.", "creator": "TeX"}}}