{"id": "1502.02791", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2015", "title": "Learning Transferable Features with Deep Adaptation Networks", "abstract": "Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is critical to formally reduce the domain bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded to a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn invariant features with enhanced transferability, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence demonstrates the proposed architecture significantly outperforms state-of-the-art results on standard domain adaptation benchmarks.", "histories": [["v1", "Tue, 10 Feb 2015 06:01:30 GMT  (143kb)", "https://arxiv.org/abs/1502.02791v1", null], ["v2", "Wed, 27 May 2015 05:28:35 GMT  (147kb)", "http://arxiv.org/abs/1502.02791v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mingsheng long", "yue cao", "jianmin wang 0001", "michael i jordan"], "accepted": true, "id": "1502.02791"}, "pdf": {"name": "1502.02791.pdf", "metadata": {"source": "META", "title": "Learning Transferable Features with Deep Adaptation Networks", "authors": ["Mingsheng Long", "Yue Cao", "Jianmin Wang", "Michael I. Jordan"], "emails": ["MINGSHENG@TSINGHUA.EDU.CN", "YUE-CAO14@MAILS.TSINGHUA.EDU.CN", "JIMWANG@TSINGHUA.EDU.CN", "JORDAN@BERKELEY.EDU"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.02 791v 2 [cs.L G] May 27 2"}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Related Work", "text": "In fact, most of them are able to survive themselves if they are not able to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves, most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "3. Deep Adaptation Networks", "text": "In the case of unattended domain adjustments, we obtain a source domain Ds = {(xsi, y s i)} ns i = 1 with ns-marked examples and a target domain Dt = {xtj} nt j = 1 with nt-marked examples. The source domain and target domain are characterized by probability distributions p or q, respectively. Our goal is to construct a deep neural network capable of learning transferable characteristics that bridge the cross-domain discrepancy and establish a classification y = \u03b8 (x) that, using source monitoring, can minimize the target risk of t (\u03b8) = Pr (x, y) \u0445 q [\u03b8 (x) 6 = y]. In the case of semi-monitored adjustments where the target has a small number of marked examples, we designate the na-commented examples of source and target domains with Da = {(xai, y) i}."}, {"heading": "3.1. Model", "text": "In this paper, we focus on the multiple variant of MMD (MMD) proposed by Gretton al. (2012b), which leads to the two sample tests. (Sejdinovic et al.) In this paper, we focus on the multiple variant of MMD (MMD), which leads to the two sample tests. (Sejdinovic et al.) In this paper, we focus on the dual variant of MMMMD. (MMD), which is formalized to the two sample tests and to minimize type II error."}, {"heading": "3.2. Algorithm", "text": "It is a matter of time in which we have to deal with the question of whether we will be able to stay in the world, and whether we will be able to stay in the world in which we live. (SGD) It is difficult to stay in the world. (SGD) It is important to stay in the world in which we live. (SGD) It is important to stay in the world in which we live. (SGD) It is difficult to stay in the world. (SGD) It is crucial to learn in the world. (SGD) It is important to stay in the world. (SGD) It is important to stay in the world in the world in which we live. (SGD) It is crucial for the effectiveness of the formation of deep networks. (Pan et al., 2011; SGD) It is rarer that we learn in the world. (SGD) It is indispensable to live in the world, in the world, in the world of the world. (SGD) It is essential to live in the world, in the world in the world. (SGD) It is indispensable to live in the world, in the world in the world in the world. (SGD) It is indispensable to live in the world in the world. (SGD) It is essential to live in the world in the world in the world, in the world in the world in the world. (SGD) It is indispensable. (SGD) It is, in the world in the world in the world, in the world in the world in the world in the world in the world. (SGD)."}, {"heading": "3.3. Analysis", "text": "We provide an analysis of the expected target-domain risk of our approach, using the theory of q q domain adaptation (Ben-David et al., 2007; Mansour et al., 2010; Mansour et al., 2009) and the theory of kernel embedding of probability distributions (Sriperumbudur et al., 2009; Gretton et al., 2012a; b).Theorem 1 Let it be a hypothesis where C is a constant for the complexity of the hypotheses space and the risk of an ideal hypothesis for both domains.Proof sketch: A result from Ben-David et al. (2007) shows that the p-hypotheses of H hypotheses can be a constant for the complexity of the hypotheses space and the risk of an ideal hypothesis."}, {"heading": "4. Experiments", "text": "We compare the DAN model with state-of-the-art transfer learning and deep learning methods on both unattended and semi-monitored adaptation problems, focusing on the effectiveness of multi-core multi-layer adaptation to FMD."}, {"heading": "4.1. Setup", "text": "It consists of 4,652 images within 31 categories, collected from three different domains: Amazon (A), which contains images downloaded from amazon.com, Webcam (W), and DSLR (D), taken from web cameras and digital SLR cameras in an office with different environmental variations. We evaluate our method through the other 3 transfer tasks, A \u2192 W and W \u2192 D, which are commonly adopted in deep learning methods (Donahue et al., 2014; Tzeng et al., 2014). For completeness, we include the evaluation of the other 3 transfer tasks. D, D \u2192 W, and W \u2192 D, which are generally adopted in deep learning methods. (Gong et al., 2012) This dataset consists of the 10 common categories of Office31 and Caltech-256 (C)."}, {"heading": "4.2. Results and Discussion", "text": "The undisputed results for the first six months of this year are shown in Table 1, and the results for the other six months of the year are shown in Table 2."}, {"heading": "4.3. Empirical Analysis", "text": "To demonstrate the transferability of learned DAN features, we follow Donahue et al. (2014) and Tzeng et al. (2014) and plot in Figures 2 (a) - 2 (b) and 2 (c) -2 (d) the t-SNE embeddings of the Imagesin task C \u2192 W with DDC features and DAN features, respectively. We make the following observations: (1) with DDC features, the target points are not discriminated very well, while with DAN features, the points are discriminated much better. (2) With DDC features, the categories between the source and the target is not aligned well, while DAN features, the categories are aligned much better between domains. Both observations can explain the superior performance of DAN over DDC: (1) implies that the targets are easier to distinguish with DAN features, and (2) implies that the targets are better classifiable with the source."}, {"heading": "5. Conclusion", "text": "In this paper, we have proposed a novel Deep Adaptation Network (DAN) architecture to improve the transferability of characteristics from task-specific levels of the neural network. We confirm that general characteristics can be generalized well to a new task, but specific characteristics that are tailored to an original task cannot effectively bridge the domain discrepancy. We show that the transferability of characteristics can be improved by the middle embedding of multi-layered representations across domains into a reproducing Hilbert space. An optimal multi-kernel selection strategy further improves the embedding of matching effectiveness, while an unbiased estimation of the mean embedding naturally leads to a linear time algorithm that is very desirable for deep learning of large data sets."}, {"heading": "Acknowledgments", "text": "This work was supported by the National Science Funds for Distinguished Young Scholars (No. 613250154), the National Science and Technology Supporting Program Project (No. 2015BAH14F02) and the Tsinghua TNList Fund for Big Data Science and Technology."}], "references": [{"title": "Domain-adversarial neural networks", "author": ["H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand"], "venue": "In NIPS 2014 Workshop on Transfer and Multi-task learning: Theory Meets Practice,", "citeRegEx": "Ajakan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ajakan et al\\.", "year": 2014}, {"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["M. Baktashmotlagh", "M.T. Harandi", "B.C. Lovell", "M. Salzmann"], "venue": "In ICCV,", "citeRegEx": "Baktashmotlagh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baktashmotlagh et al\\.", "year": 2013}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": null, "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine Learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "F. Sha"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "In ICML,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Multi-source deep learning for information trustworthiness estimation", "author": ["L. Ge", "J. Gao", "X. Li", "A. Zhang"], "venue": "In KDD,", "citeRegEx": "Ge et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2013}, {"title": "Domain adaptive neural networks for object recognition", "author": ["M. Ghifary", "W.B. Kleijn", "M. Zhang"], "venue": "Technical report,", "citeRegEx": "Ghifary et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ghifary et al\\.", "year": 2014}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "In ICML,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "B. Sriperumbudur", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu"], "venue": "In NIPS,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": "Technical report, California Institute of Technology,", "citeRegEx": "Griffin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Griffin et al\\.", "year": 2007}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "LSDA: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Guadarrama", "E. Tzeng", "R. Hu", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko"], "venue": "In NIPS,", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Transfer feature learning with joint distribution adaptation", "author": ["M. Long", "J. Wang", "G. Ding", "J. Sun", "P.S. Yu"], "venue": "In ICCV,", "citeRegEx": "Long et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Long et al\\.", "year": 2013}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "COLT,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In ECCV,", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Equivalence of distance-based and rkhs-based statistics in hypothesis testing", "author": ["D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu"], "venue": "The Annals of Statistics,", "citeRegEx": "Sejdinovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sejdinovic et al\\.", "year": 2013}, {"title": "Kernel choice and classifiability for rkhs embeddings of probability distributions", "author": ["B.K. Sriperumbudur", "K. Fukumizu", "A. Gretton", "G. Lanckriet", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2009}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A.A. Efros"], "venue": "In CVPR,", "citeRegEx": "Torralba and Efros,? \\Q2011\\E", "shortCiteRegEx": "Torralba and Efros", "year": 2011}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": "Technical report,", "citeRegEx": "Tzeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}, {"title": "Flexible transfer learning under support and model shift", "author": ["X. Wang", "J. Schneider"], "venue": "In NIPS,", "citeRegEx": "Wang and Schneider,? \\Q2014\\E", "shortCiteRegEx": "Wang and Schneider", "year": 2014}, {"title": "Deep learning via semisupervised embedding", "author": ["J. Weston", "F. Rattle", "R. Collobert"], "venue": "In ICML,", "citeRegEx": "Weston et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2008}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In NIPS,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Domain adaptation under target and conditional shift", "author": ["K. Zhang", "B. Sch\u00f6lkopf", "K. Muandet", "Z. Wang"], "venue": "In ICML,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": "In this direction, a fruitful line of prior work has focused on learning shallow features by jointly minimizing a distance metric of domain discrepancy (Pan et al., 2011; Long et al., 2013; Baktashmotlagh et al., 2013; Gong et al., 2013; Zhang et al., 2013; Ghifary et al., 2014; Wang & Schneider, 2014).", "startOffset": 152, "endOffset": 303}, {"referenceID": 19, "context": "In this direction, a fruitful line of prior work has focused on learning shallow features by jointly minimizing a distance metric of domain discrepancy (Pan et al., 2011; Long et al., 2013; Baktashmotlagh et al., 2013; Gong et al., 2013; Zhang et al., 2013; Ghifary et al., 2014; Wang & Schneider, 2014).", "startOffset": 152, "endOffset": 303}, {"referenceID": 1, "context": "In this direction, a fruitful line of prior work has focused on learning shallow features by jointly minimizing a distance metric of domain discrepancy (Pan et al., 2011; Long et al., 2013; Baktashmotlagh et al., 2013; Gong et al., 2013; Zhang et al., 2013; Ghifary et al., 2014; Wang & Schneider, 2014).", "startOffset": 152, "endOffset": 303}, {"referenceID": 11, "context": "In this direction, a fruitful line of prior work has focused on learning shallow features by jointly minimizing a distance metric of domain discrepancy (Pan et al., 2011; Long et al., 2013; Baktashmotlagh et al., 2013; Gong et al., 2013; Zhang et al., 2013; Ghifary et al., 2014; Wang & Schneider, 2014).", "startOffset": 152, "endOffset": 303}, {"referenceID": 32, "context": "In this direction, a fruitful line of prior work has focused on learning shallow features by jointly minimizing a distance metric of domain discrepancy (Pan et al., 2011; Long et al., 2013; Baktashmotlagh et al., 2013; Gong et al., 2013; Zhang et al., 2013; Ghifary et al., 2014; Wang & Schneider, 2014).", "startOffset": 152, "endOffset": 303}, {"referenceID": 8, "context": "In this direction, a fruitful line of prior work has focused on learning shallow features by jointly minimizing a distance metric of domain discrepancy (Pan et al., 2011; Long et al., 2013; Baktashmotlagh et al., 2013; Gong et al., 2013; Zhang et al., 2013; Ghifary et al., 2014; Wang & Schneider, 2014).", "startOffset": 152, "endOffset": 303}, {"referenceID": 9, "context": "However, recent studies have shown that deep neural networks can learn more transferable features for domain adaptation (Glorot et al., 2011; Donahue et al., 2014; Yosinski et al., 2014), which produce breakthrough results on some domain adaptation datasets.", "startOffset": 120, "endOffset": 186}, {"referenceID": 6, "context": "However, recent studies have shown that deep neural networks can learn more transferable features for domain adaptation (Glorot et al., 2011; Donahue et al., 2014; Yosinski et al., 2014), which produce breakthrough results on some domain adaptation datasets.", "startOffset": 120, "endOffset": 186}, {"referenceID": 31, "context": "However, recent studies have shown that deep neural networks can learn more transferable features for domain adaptation (Glorot et al., 2011; Donahue et al., 2014; Yosinski et al., 2014), which produce breakthrough results on some domain adaptation datasets.", "startOffset": 120, "endOffset": 186}, {"referenceID": 31, "context": "In other words, the features computed in higher layers of the network must depend greatly on the specific dataset and task (Yosinski et al., 2014), which are task-specific features and are not safely transferable to", "startOffset": 123, "endOffset": 146}, {"referenceID": 9, "context": "Another curious phenomenon is that disentangling the variational factors in higher layers of the network may enlarge the domain discrepancy, as different domains with the new deep representations become more \u201ccompact\u201d and are more mutually distinguishable (Glorot et al., 2011).", "startOffset": 256, "endOffset": 277}, {"referenceID": 20, "context": "Although deep features are salient for discrimination, enlarged dataset bias may deteriorate domain adaptation performance, resulting in statistically unbounded risk for the target tasks (Mansour et al., 2009; Ben-David et al., 2010).", "startOffset": 187, "endOffset": 233}, {"referenceID": 3, "context": "Although deep features are salient for discrimination, enlarged dataset bias may deteriorate domain adaptation performance, resulting in statistically unbounded risk for the target tasks (Mansour et al., 2009; Ben-David et al., 2010).", "startOffset": 187, "endOffset": 233}, {"referenceID": 31, "context": ", 2014) are representative for generalpurpose tasks (Yosinski et al., 2014; Hoffman et al., 2014), the proposed DAN model is trained by fine-tuning from the AlexNet model (Krizhevsky et al.", "startOffset": 52, "endOffset": 97}, {"referenceID": 16, "context": ", 2014) are representative for generalpurpose tasks (Yosinski et al., 2014; Hoffman et al., 2014), the proposed DAN model is trained by fine-tuning from the AlexNet model (Krizhevsky et al.", "startOffset": 52, "endOffset": 97}, {"referenceID": 18, "context": ", 2014), the proposed DAN model is trained by fine-tuning from the AlexNet model (Krizhevsky et al., 2012) pre-trained on ImageNet, which is implemented in Caffe (Jia et al.", "startOffset": 81, "endOffset": 106}, {"referenceID": 17, "context": ", 2012) pre-trained on ImageNet, which is implemented in Caffe (Jia et al., 2014).", "startOffset": 63, "endOffset": 81}, {"referenceID": 23, "context": "Transfer learning aims to mitigate the effort of manual labeling for machine learning (Pan et al., 2011; Gong et al., 2013; Zhang et al., 2013; Wang & Schneider, 2014) and computer vision (Saenko et al.", "startOffset": 86, "endOffset": 167}, {"referenceID": 11, "context": "Transfer learning aims to mitigate the effort of manual labeling for machine learning (Pan et al., 2011; Gong et al., 2013; Zhang et al., 2013; Wang & Schneider, 2014) and computer vision (Saenko et al.", "startOffset": 86, "endOffset": 167}, {"referenceID": 32, "context": "Transfer learning aims to mitigate the effort of manual labeling for machine learning (Pan et al., 2011; Gong et al., 2013; Zhang et al., 2013; Wang & Schneider, 2014) and computer vision (Saenko et al.", "startOffset": 86, "endOffset": 167}, {"referenceID": 24, "context": ", 2013; Wang & Schneider, 2014) and computer vision (Saenko et al., 2010; Gong et al., 2012; Baktashmotlagh et al., 2013; Long et al., 2013), etc.", "startOffset": 52, "endOffset": 140}, {"referenceID": 10, "context": ", 2013; Wang & Schneider, 2014) and computer vision (Saenko et al., 2010; Gong et al., 2012; Baktashmotlagh et al., 2013; Long et al., 2013), etc.", "startOffset": 52, "endOffset": 140}, {"referenceID": 1, "context": ", 2013; Wang & Schneider, 2014) and computer vision (Saenko et al., 2010; Gong et al., 2012; Baktashmotlagh et al., 2013; Long et al., 2013), etc.", "startOffset": 52, "endOffset": 140}, {"referenceID": 19, "context": ", 2013; Wang & Schneider, 2014) and computer vision (Saenko et al., 2010; Gong et al., 2012; Baktashmotlagh et al., 2013; Long et al., 2013), etc.", "startOffset": 52, "endOffset": 140}, {"referenceID": 4, "context": "Deep neural networks learn nonlinear representations that disentangle and hide different explanatory factors of variation behind data samples (Bengio et al., 2013).", "startOffset": 142, "endOffset": 163}, {"referenceID": 31, "context": "The learned deep representations manifest invariant factors underlying different populations and are transferable from the original tasks to similar novel tasks (Yosinski et al., 2014).", "startOffset": 161, "endOffset": 184}, {"referenceID": 9, "context": "Hence, deep neural networks have been explored for domain adaptation (Glorot et al., 2011; Chen et al., 2012), multimodal and multi-source learning problems (Ngiam et al.", "startOffset": 69, "endOffset": 109}, {"referenceID": 5, "context": "Hence, deep neural networks have been explored for domain adaptation (Glorot et al., 2011; Chen et al., 2012), multimodal and multi-source learning problems (Ngiam et al.", "startOffset": 69, "endOffset": 109}, {"referenceID": 21, "context": ", 2012), multimodal and multi-source learning problems (Ngiam et al., 2011; Ge et al., 2013), where significant performance gains have been obtained.", "startOffset": 55, "endOffset": 92}, {"referenceID": 7, "context": ", 2012), multimodal and multi-source learning problems (Ngiam et al., 2011; Ge et al., 2013), where significant performance gains have been obtained.", "startOffset": 55, "endOffset": 92}, {"referenceID": 9, "context": "In reality, the domain discrepancy can be alleviated, but not removed, by deep neural networks (Glorot et al., 2011).", "startOffset": 95, "endOffset": 116}, {"referenceID": 20, "context": "Dataset shift has posed a bottleneck to the transferability of deep networks, resulting in statistically unbounded risk for target tasks (Mansour et al., 2009; Ben-David et al., 2010).", "startOffset": 137, "endOffset": 183}, {"referenceID": 3, "context": "Dataset shift has posed a bottleneck to the transferability of deep networks, resulting in statistically unbounded risk for target tasks (Mansour et al., 2009; Ben-David et al., 2010).", "startOffset": 137, "endOffset": 183}, {"referenceID": 0, "context": "There are several very recent efforts in learning domain-invariant features in the context of shallow neural networks (Ajakan et al., 2014; Ghifary et al., 2014).", "startOffset": 118, "endOffset": 161}, {"referenceID": 8, "context": "There are several very recent efforts in learning domain-invariant features in the context of shallow neural networks (Ajakan et al., 2014; Ghifary et al., 2014).", "startOffset": 118, "endOffset": 161}, {"referenceID": 18, "context": "Due to the limited capacity of shallow architectures, the performance of these proposals does not surpass deep CNN (Krizhevsky et al., 2012).", "startOffset": 115, "endOffset": 140}, {"referenceID": 31, "context": "While performance was improved, DDC only adapts a single layer of the network, which may be restrictive in that there are multiple layers where the hidden features are not transferable (Yosinski et al., 2014).", "startOffset": 185, "endOffset": 208}, {"referenceID": 25, "context": "Our work is primarily motivated by Yosinski et al. (2014), which comprehensively explores feature transferability of deep convolutional neural networks.", "startOffset": 35, "endOffset": 58}, {"referenceID": 0, "context": "There are several very recent efforts in learning domain-invariant features in the context of shallow neural networks (Ajakan et al., 2014; Ghifary et al., 2014). Due to the limited capacity of shallow architectures, the performance of these proposals does not surpass deep CNN (Krizhevsky et al., 2012). Tzeng et al. (2014) proposed a DDC model that adds an adaptation layer and a dataset shift loss to the deep CNN for learning a domain-invariant representation.", "startOffset": 119, "endOffset": 325}, {"referenceID": 3, "context": "To approach this problem, many existing methods aim to bound the target error by the source error plus a discrepancy metric between the source and the target (Ben-David et al., 2010).", "startOffset": 158, "endOffset": 182}, {"referenceID": 25, "context": "Two classes of statistics have been explored for the two-sample testing, where acceptance or rejection decisions are made for a null hypothesis p = q, given samples generated respectively from p and q: energy distances and maximum mean discrepancies (MMD) (Sejdinovic et al., 2013).", "startOffset": 256, "endOffset": 281}, {"referenceID": 2, "context": "To approach this problem, many existing methods aim to bound the target error by the source error plus a discrepancy metric between the source and the target (Ben-David et al., 2010). Two classes of statistics have been explored for the two-sample testing, where acceptance or rejection decisions are made for a null hypothesis p = q, given samples generated respectively from p and q: energy distances and maximum mean discrepancies (MMD) (Sejdinovic et al., 2013). In this paper, we focus on the multiple kernel variant of MMD (MK-MMD) proposed by Gretton et al. (2012b), which is formalized to jointly maximize the two-sample test power and minimize the Type II error, i.", "startOffset": 159, "endOffset": 573}, {"referenceID": 12, "context": "As studied theoretically in Gretton et al. (2012b), the kernel MK-", "startOffset": 28, "endOffset": 51}, {"referenceID": 3, "context": "One of the feasible strategies for controlling the domain discrepancy is to find an abstract feature representation through which the source and target domains are similar (Ben-David et al., 2010).", "startOffset": 172, "endOffset": 196}, {"referenceID": 23, "context": "Although this idea has been explored in several papers (Pan et al., 2011; Zhang et al., 2013; Wang & Schneider, 2014), to date there has been no attempt to enhance the transferability of feature representation via MK-MMD in deep neural networks.", "startOffset": 55, "endOffset": 117}, {"referenceID": 32, "context": "Although this idea has been explored in several papers (Pan et al., 2011; Zhang et al., 2013; Wang & Schneider, 2014), to date there has been no attempt to enhance the transferability of feature representation via MK-MMD in deep neural networks.", "startOffset": 55, "endOffset": 117}, {"referenceID": 18, "context": "We start with deep convolutional neural networks (CNN) (Krizhevsky et al., 2012), a strong model when it is adapted to novel tasks (Donahue et al.", "startOffset": 55, "endOffset": 80}, {"referenceID": 6, "context": ", 2012), a strong model when it is adapted to novel tasks (Donahue et al., 2014; Hoffman et al., 2014).", "startOffset": 58, "endOffset": 102}, {"referenceID": 16, "context": ", 2012), a strong model when it is adapted to novel tasks (Donahue et al., 2014; Hoffman et al., 2014).", "startOffset": 58, "endOffset": 102}, {"referenceID": 18, "context": "We extend the AlexNet architecture (Krizhevsky et al., 2012), which is comprised of five convolutional layers (conv1\u2013conv5) and three fully connected layers (fc6\u2013 fc8).", "startOffset": 35, "endOffset": 60}, {"referenceID": 31, "context": "We will not discuss how to compute the convolutional layers as we will not impose distribution-adaptation regularization in those layers, given that the convolutional layers can learn generic features that tend to be transferable in layers conv1\u2013conv3 and are slightly domain-biased in conv4\u2013conv5 (Yosinski et al., 2014).", "startOffset": 298, "endOffset": 321}, {"referenceID": 15, "context": "Hence, when adapting the pre-trained AlexNet to the target, we opt to freeze conv1\u2013conv3 and fine-tune conv4\u2013conv5 to preserve the efficacy of fragile co-adaptation (Hinton et al., 2012).", "startOffset": 165, "endOffset": 186}, {"referenceID": 31, "context": "In standard CNNs, deep features must eventually transition from general to specific by the last layer of the network, and the transferability gap grows with the domain discrepancy and becomes particularly large when transferring the higher layers fc6\u2013fc8 (Yosinski et al., 2014).", "startOffset": 255, "endOffset": 278}, {"referenceID": 31, "context": "As revealed by (Yosinski et al., 2014), feature transferability gets worse on conv4\u2013conv5 and significantly drops on fc6\u2013fc8, hence it is critical to adapt multiple layers instead of only one layer.", "startOffset": 15, "endOffset": 38}, {"referenceID": 32, "context": "Another benefit of multi-layer adaptation is that by jointly adapting the representation layers and the classifier layer, we could essentially bridge the domain discrepancy underlying both the marginal distribution and the conditional distribution, which is crucial for domain adaptation (Zhang et al., 2013).", "startOffset": 288, "endOffset": 308}, {"referenceID": 29, "context": "Training a deep CNN requires a large amount of labeled data, which is prohibitive for many domain adaptation problems, hence we start with an AlexNet model pretrained on ImageNet 2012 and fine-tune it as in Yosinski et al. (2014). With the proposed DAN optimization framework (4), we are able to learn transferable features from a source domain to a related target domain.", "startOffset": 207, "endOffset": 230}, {"referenceID": 12, "context": "As pointed out by Gretton et al. (2012b), kernel choice is critical to the testing power of MMD since different kernels may embed probability distributions in different RKHSs where different orders of sufficient statistics can be emphasized.", "startOffset": 18, "endOffset": 41}, {"referenceID": 23, "context": "While prior work based on MMD (Pan et al., 2011; Tzeng et al., 2014) rarely addresses this issue, we believe it is critical in the context of deep learning.", "startOffset": 30, "endOffset": 68}, {"referenceID": 28, "context": "While prior work based on MMD (Pan et al., 2011; Tzeng et al., 2014) rarely addresses this issue, we believe it is critical in the context of deep learning.", "startOffset": 30, "endOffset": 68}, {"referenceID": 17, "context": "Such a mini-batch SGD can be easily implemented within the Caffe framework for CNNs (Jia et al., 2014).", "startOffset": 84, "endOffset": 102}, {"referenceID": 2, "context": "We provide an analysis of the expected target-domain risk of our approach, making use of the theory of domain adaptation (Ben-David et al., 2007; 2010; Mansour et al., 2009) and the theory of kernel embedding of probability distributions (Sriperumbudur et al.", "startOffset": 121, "endOffset": 173}, {"referenceID": 20, "context": "We provide an analysis of the expected target-domain risk of our approach, making use of the theory of domain adaptation (Ben-David et al., 2007; 2010; Mansour et al., 2009) and the theory of kernel embedding of probability distributions (Sriperumbudur et al.", "startOffset": 121, "endOffset": 173}, {"referenceID": 2, "context": "Proof sketch: A result from Ben-David et al. (2007) shows that \u01ebt(\u03b8) 6 \u01ebs(\u03b8) + dH(p, q) + C0, where dH(p, q) is the H-divergence between p and q, which is defined as", "startOffset": 28, "endOffset": 52}, {"referenceID": 26, "context": "By choosing \u03b7 as a (kernel) Parzen window classifier (Sriperumbudur et al., 2009), dH(p, q) can be bounded by the empirical estimate", "startOffset": 53, "endOffset": 81}, {"referenceID": 24, "context": "Office-31 (Saenko et al., 2010) This dataset is a standard benchmark for domain adaptation.", "startOffset": 10, "endOffset": 31}, {"referenceID": 6, "context": "We evaluate our method across the 3 transfer tasks, A \u2192 W, D \u2192 W and W \u2192 D, which are commonly adopted in deep learning methods (Donahue et al., 2014; Tzeng et al., 2014).", "startOffset": 128, "endOffset": 170}, {"referenceID": 28, "context": "We evaluate our method across the 3 transfer tasks, A \u2192 W, D \u2192 W and W \u2192 D, which are commonly adopted in deep learning methods (Donahue et al., 2014; Tzeng et al., 2014).", "startOffset": 128, "endOffset": 170}, {"referenceID": 10, "context": "Office-10 + Caltech-10 (Gong et al., 2012).", "startOffset": 23, "endOffset": 42}, {"referenceID": 14, "context": "This dataset consists of the 10 common categories shared by the Office31 and Caltech-256 (C) (Griffin et al., 2007) datasets and is widely adopted in transfer learning methods (Long et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 19, "context": ", 2007) datasets and is widely adopted in transfer learning methods (Long et al., 2013; Baktashmotlagh et al., 2013).", "startOffset": 68, "endOffset": 116}, {"referenceID": 1, "context": ", 2007) datasets and is widely adopted in transfer learning methods (Long et al., 2013; Baktashmotlagh et al., 2013).", "startOffset": 68, "endOffset": 116}, {"referenceID": 23, "context": "We compare to a variety of methods: TCA (Pan et al., 2011), GFK (Gong et al.", "startOffset": 40, "endOffset": 58}, {"referenceID": 10, "context": ", 2011), GFK (Gong et al., 2012), CNN (Krizhevsky et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 18, "context": ", 2012), CNN (Krizhevsky et al., 2012), LapCNN (Weston et al.", "startOffset": 13, "endOffset": 38}, {"referenceID": 30, "context": ", 2012), LapCNN (Weston et al., 2008), and DDC (Tzeng et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 28, "context": ", 2008), and DDC (Tzeng et al., 2014).", "startOffset": 17, "endOffset": 37}, {"referenceID": 31, "context": "CNN was the leading method in the ImageNet 2012 competition, and it turns out to be a strong model for learning transferable features (Yosinski et al., 2014).", "startOffset": 134, "endOffset": 157}, {"referenceID": 17, "context": ", CNN, LapCNN, DDC, and DAN based on the Caffe (Jia et al., 2014) implementation of AlexNet (Krizhevsky et al.", "startOffset": 47, "endOffset": 65}, {"referenceID": 18, "context": ", 2014) implementation of AlexNet (Krizhevsky et al., 2012) trained on the ImageNet dataset.", "startOffset": 34, "endOffset": 59}, {"referenceID": 11, "context": "We mainly follow standard evaluation protocol for unsupervised adaptation and use all source examples with labels and all target examples without labels (Gong et al., 2013).", "startOffset": 153, "endOffset": 172}, {"referenceID": 24, "context": "To make our results directly comparable to most published results, we report a classical protocol (Saenko et al., 2010) in that we randomly down-sample the source examples, and further require 3 labeled target examples per category for semi-supervised adaptation.", "startOffset": 98, "endOffset": 119}, {"referenceID": 26, "context": "As minimizing MMD is equivalent to maximizing the error of classifying the source from the target (two-sample classifier) (Sriperumbudur et al., 2009), we can automatically select the MMD penalty parameter \u03bb on a validation set (comprised of source-labeled instances and target-unlabeled instances) by jointly assessing the test errors of the source classifier and the two-sample classifier.", "startOffset": 122, "endOffset": 150}, {"referenceID": 31, "context": "We use the finetuning architecture (Yosinski et al., 2014), however, due to limited training examples in our datasets, we fix convolutional layers conv1\u2013conv3 that were copied from pretrained model, fine-tune conv4\u2013conv5 and fully connected layers fc6\u2013fc7, and train classifier layer fc8, both via back propagation.", "startOffset": 35, "endOffset": 58}, {"referenceID": 24, "context": "We can observe that DAN significantly outperforms the comparison methods on most transfer tasks, and achieves comparable performance on the easy transfer tasks, D \u2192 W and W \u2192 D, where source and target are similar (Saenko et al., 2010).", "startOffset": 214, "endOffset": 235}, {"referenceID": 11, "context": "Accuracy on Office-31 dataset with standard unsupervised adaptation protocol (Gong et al., 2013).", "startOffset": 77, "endOffset": 96}, {"referenceID": 11, "context": "Accuracy on Office-10 + Caltech-10 dataset with standard unsupervised adaptation protocol (Gong et al., 2013).", "startOffset": 90, "endOffset": 109}, {"referenceID": 24, "context": "Accuracy on Office-31 dataset with classic unsupervised and semi-supervised adaptation protocols (Saenko et al., 2010).", "startOffset": 97, "endOffset": 118}, {"referenceID": 6, "context": "Note that while DDC based on Caffe AlexNet was shown to significantly outperform DeCAF (Donahue et al., 2014) in which fine-tuning was not carried out, it does not yield a large gain over Caffe AlexNet using fine-tuning.", "startOffset": 87, "endOffset": 109}, {"referenceID": 12, "context": "The reason is that multiple kernels with different bandwidths can match both the low-order moments and high-order moments to minimize the Type II error (Gretton et al., 2012b). (2) DANSK also attains higher accuracy than DDC, which confirms the capability of deep architecture for distribution adaptation. The rationale is similar to that of deep networks: each layer of deep network is intended to extract features at a different abstraction level, and hence we need to match the distributions at each task-specific layer to consolidate the adaptation quality at all levels. The multi-layer architecture is one of the most critical contributors to the efficacy of deep learning, and we believe it is also important for MMD-based adaptation. The evidence of comparable performance between the multilayer variant DANSK and multi-kernel variants DAN7 and DAN8 shows their equal importance for domain adaptation. As expected, DAN obtains the best performance by jointly exploring multi-layer adaptation with multi-kernel MMD. Another benefit of DAN is that it uses a linear-time unbiased estimate of the kernel embedding, which makes it an order more efficient than existing methods TCA and DDC. Though Tzeng et al. (2014) speed up DDC by computing the MMD within each mini-batch of the SGD, this leads to a biased estimate of MMD and lower adaptation accuracy.", "startOffset": 153, "endOffset": 1220}, {"referenceID": 6, "context": "Feature Visualization To demonstrate the transferability of the DAN learned features, we follow Donahue et al. (2014) and Tzeng et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 6, "context": "Feature Visualization To demonstrate the transferability of the DAN learned features, we follow Donahue et al. (2014) and Tzeng et al. (2014) and plot in Figures 2(a)\u2013 2(b) and 2(c)\u20132(d) the t-SNE embeddings of the images", "startOffset": 96, "endOffset": 142}, {"referenceID": 3, "context": "However, domain adaptation may be deteriorated by the enlarged domain discrepancy (Ben-David et al., 2010).", "startOffset": 82, "endOffset": 106}, {"referenceID": 2, "context": "A-Distance A theoretical result in Ben-David et al. (2010) suggests A-distance as a measure of domain discrepancy.", "startOffset": 35, "endOffset": 59}, {"referenceID": 2, "context": "A-Distance A theoretical result in Ben-David et al. (2010) suggests A-distance as a measure of domain discrepancy. As computing the exact A-distance is intractable, an approximate distance is defined as d\u0302A = 2 (1\u2212 2\u01eb), where \u01eb is the generalization error of a two-sample classifier (kernel SVM in our case) trained on the binary problem to distinguish input samples between the source and target domains. Figure 3(a) displays d\u0302A on transfer tasks A \u2192 W and C \u2192 W using Raw features, CNN features, and DAN features, respectively. It reveals a surprising observation that the d\u0302A on both CNN and DAN features are larger than the d\u0302A on Raw features. This implies that abstract deep features can be salient both for discriminating different categories and different domains, which is consistent with Glorot et al. (2011). However, domain adaptation may be deteriorated by the enlarged domain discrepancy (Ben-David et al.", "startOffset": 35, "endOffset": 820}], "year": 2015, "abstractText": "Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.", "creator": "LaTeX with hyperref package"}}}