{"id": "1502.03044", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2015", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.", "histories": [["v1", "Tue, 10 Feb 2015 19:18:29 GMT  (8243kb,D)", "http://arxiv.org/abs/1502.03044v1", null], ["v2", "Wed, 11 Feb 2015 02:58:54 GMT  (8243kb,D)", "http://arxiv.org/abs/1502.03044v2", null], ["v3", "Tue, 19 Apr 2016 16:43:09 GMT  (8243kb,D)", "http://arxiv.org/abs/1502.03044v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["kelvin xu", "jimmy ba", "ryan kiros", "kyunghyun cho", "aaron c courville", "ruslan salakhutdinov", "richard s zemel", "yoshua bengio"], "accepted": true, "id": "1502.03044"}, "pdf": {"name": "1502.03044.pdf", "metadata": {"source": "META", "title": "Show, Attend and Tell: Neural Image CaptionGeneration with Visual Attention", "authors": ["Kelvin Xu", "Jimmy Lei Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "emails": ["KELVIN.XU@UMONTREAL.CA", "JIMMY@PSI.UTORONTO.CA", "RKIROS@CS.TORONTO.EDU", "KYUNGHYUN.CHO@UMONTREAL.CA", "AARON.COURVILLE@UMONTREAL.CA", "RSALAKHU@CS.TORONTO.EDU", "ZEMEL@CS.TORONTO.EDU", "FIND-ME@THE.WEB"], "sections": [{"heading": "1. Introduction", "text": "The automatic generation of captions on an image is a task that comes very close to the heart of scene understanding - one of the primary objectives of computer vision. Not only must captions generation models be powerful enough to solve the challenges of computer vision, which are objects in an image, but they must also be able to capture and express their relationships in a natural language. Therefore, captions generation has long been considered a difficult problem. It is a very important challenge for machine learning algorithms, since it amounts to mimicking the remarkable human ability to compress their visual informatibility into a descriptive language. Despite the challenging nature of this task, there has recently been a surge in research interest in captions generation. Supported by advances in the formation of neural networks (Krizhevsky et al., 2012) and large classification datasets (Ruskosavsky et al., 2014), the quality of image generation has significantly improved."}, {"heading": "2. Related Work", "text": "Many of these methods are based on recurring neural networks and are inspired by the successful use of sequence training with neural networks for machine translation (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014). A key reason for captioning images is well suited to the encoder decoder framework (Cho et al., 2014) of machine translation is that it is analogous to \"translating\" an image into a phrase. The first approach to using neural networks for captioning was Kiros et al. (2014a), which proposed a multimodal log-bilinear model distorted by features from the image. This work was later developed by Kiros et al al. (2014b), whose method explicitly enables a way to make both ranking and generation."}, {"heading": "3. Image Caption Generation with Attention Mechanism", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Model Details", "text": "In this section, we describe the two variants of our attention-based model by first describing their common framework. The main difference lies in the definition of the \u03c6 function, which we describe in detail in Section 4. We refer to vectors with bold letters and matrices with uppercase letters. In our description below, we suppress distorted terms for readability."}, {"heading": "3.1.1. ENCODER: CONVOLUTIONAL FEATURES", "text": "Our model takes a single raw image and generates a caption y, encoded as a sequence of 1-of-K encoded words.y = {y1,.., yC}, yi \u0440RK, where K is the size of the vocabulary and C is the length of the caption. We use a revolutionary neural network to extract a series of feature vectors, which we call annotation vectors.The extractor generates L vectors, each of which represents a D-dimensional representation corresponding to a part of the picture.a = {a1,..., aL}, ai \u0109RDIn order to obtain a match between the feature vectors and parts of the 2-D image, we extract features from a lower folding layer as opposed to previous work, which instead uses a fully connected layer. This allows the decoder to focus selectively on specific parts of an image by selecting a set of all the features."}, {"heading": "3.1.2. DECODER: LONG SHORT-TERM MEMORY NETWORK", "text": "We use a long-term short-term memory (LSTM) network (Hochreiter & Schmidhuber, 1997) that creates a caption by referring one word in each step to a context vector, the previous hidden state, and the previously generated words. (2014) (see Figure 4) (see Figure 4) (see Figure 4). (3) The implementation of LSTM follows the method used in Zaremba and al (see Figure). (1) The use of Ts, t: Rs \u2192 Rt to denote a simple affine transformation with parameters that are learned, it is ft ot = n, n Eyt \u2212 1ht \u2212 1 z (1) ct \u2212 1) ct \u2212 t (2) ht = ot tanh (ct). (3) Here it is, ft, ct, ot, ht are the input, forgotten, output, and hidden state of the LSTM, or the context vector is the context vector associated with a particular place."}, {"heading": "4. Learning Stochastic \u201cHard\u201d vs Deterministic \u201cSoft\u201d Attention", "text": "In this section we discuss two alternative mechanisms for the attention model bold: stochastic attention and deterministic attention."}, {"heading": "4.1. Stochastic \u201cHard\u201d Attention", "text": "We represent the position variable st as the place where the model decides to draw attention when generating the Tth Word. st, i is an indicator of a hot variable that is set to 1 if the i-th place (from L) is the one used to extract visual characteristics. By treating the attention locations as intermediate latent variables, we can assign a multinoulli distribution parameter used by [n] n random variable: p (st, i = 1 | sj < t, a) = inverse latent variables, i (8) z [t] s = [t] st, iai. (9) We define a new objective function Ls that has a variational lower limit on the marginal log probability p (y | a) of observing the sequence of words y given image characteristics. The learning algorithms for the W can be derived directly from the optimization of the models."}, {"heading": "4.2. Deterministic \u201cSoft\u201d Attention", "text": "The hidden activation of LSTM ht is a linear projection of the stochastic context vector z, followed by the imprecision of attention."}, {"heading": "4.2.1. DOUBLY STOCHASTIC ATTENTION", "text": "In the formation of the deterministic version of our model, we introduce a form of double stochastic regularization in which we also encourage the improvement of the overall BLEU result, which can be interpreted as encouraging the model to pay the same attention to each part of the image over the course of the generation. In our experiments, we observed that this penalty was quantitatively important in order to improve the overall BLEU result and that this leads to richer and more descriptive captions in terms of quality. Furthermore, the soft attention model predicts a goal scale \u03b2 from the previous hidden state \u2212 1 at each step of time, so that at each step t ({ai}, {\u03b1i}) = \u03b2 \u00b2 L i \u03b1iai, where \u03b2t = \u03c3 (f\u00df (ht -1)) is generated."}, {"heading": "4.3. Training Procedure", "text": "For the Flickr8k dataset, we found that RMSProp (Tieleman & Hinton, 2012) worked best, while Flickr30k / MS COCO datasets we used the recently proposed Adam algorithm (Kingma & Ba, 2014). To create the annotations used by our decoder, we used Oxford VGnet (Simonyan & Zisserman, 2014), which is prepared for ImageNet without finetuning, but in principle any encoding function could be used. Moreover, we were able to train the encoder from the ground up (Simonyan & Zisserman, 2014) to explore the rest of the model."}, {"heading": "5. Experiments", "text": "We describe our experimental methodology and quantitative results, which confirm the effectiveness of our caption generation model."}, {"heading": "5.1. Data", "text": "We report on the results of the popular Flickr8k and Flickr30k datasets, with 8,000 and 30,000 images respectively, and the more sophisticated Microsoft COCO dataset, with 82,783 images. Flickr8k / Flickr30k datasets each contain 5 reference sets per image, but the MS COCO dataset contains some images with references of more than 5, which we discard for consistency in our datasets. We report on the results using the commonly used BLEU Metric2, which is the standard in captioning literature. We report BLEU from 1 to 4 without a penalty, but there has been criticism of our attention-based architecture according to Table 4.2.1."}, {"heading": "5.2. Evaluation Procedures", "text": "For identical decoder architectures that use newer architectures such as GoogLeNet or Oxford VGG Szegedy et al. (2014), Simonyan & Zisserman (2014) can provide a performance boost over AlexNet (Krizhevsky et al., 2012). In our review, we directly compare only results that use the comparable features of GoogLeNet / Oxford VGG, but for METEOR comparison, we note some results that use AlexNet. The second challenge is a comparison between individual models and ensembles. While other methods see performance gains through the use of ensemble, our results report only one model performance.Finally, there is a challenge due to differences between dataset splits."}, {"heading": "5.3. Quantitative Analysis", "text": "In Table 4.2.1 we provide a summary of the experiment to validate the quantitative effectiveness of attention. We get the current performance on Flickr8k, Flickr30k and MS COCO. Furthermore, we find that in our experiments we are able to significantly improve the current performance METEOR on MS COCO, which we suspect is related to some of the regulation techniques we use 4.2.1 and our lower representation. Finally, we find that we are able to achieve this performance with a single model without ensemble."}, {"heading": "5.4. Qualitative Analysis: Learning to attend", "text": "By visualizing the attention component learned from the model, we are able to add an additional layer of interpretability to the output of the model (see Figure 1).Other systems that have done this rely on object detection systems to create targets for candidate alignment (Karpathy & Li, 2014).Our approach is much more flexible because the model can take care of \"non-object-related\" prominent regions.The 19-layer OxfordNet uses stacks of 3x3 filters, meaning that the only time the feature maps get smaller is due to the maximum pooling layers. The input image is shrunk so that the shortest side is 256-dimensional, with retained aspect ratio. Input into the rotary network is the central section 224x224 image. As a result, 4-max pooling layers give us an output layer of the pivot layer 14x14."}, {"heading": "6. Conclusion", "text": "We propose an attention-based approach that reflects the state of the art on three benchmark data sets using BLEU and METEOR metrics. We also show how the learned attention can be used to bring more interpretability into the process of modeling, and show that the alignments learned correspond very well with human intuition. We hope that the results of this paper will encourage future work in dealing with visual attention. We also expect that the modularity of the encoder decoder approach combined with attention will provide useful applications in other areas."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for their support of the following research funding and computer support organisations: NSERC, Samsung, Calcul Que'bec, Compute Canada, the Canada Research Chairs and CIFAR. The authors would also like to thank Nitish Srivastava for supporting his ConvNet package as well as the preparation of the Oxford Convolutional Network and Relu Patrascu for helping with numerous infrastructure related issues."}, {"heading": "A. Appendix", "text": "Visualizations of our \"hard\" (a) and \"soft\" (b) attention model. (b) A stop sign stands on a road with a mountain in the background. (b) A man in a suit holding a remote control. (a) A man in a suit and hat with a remote control. (b) A man in a hat and hat on a skateboard.Figure 11.A little girl sits on a couch with a teddy bear. (a) A little girl sits on a bed with a teddy bear. (b) A man stands on a beach with a hat and a hat on a skateboard.Figure 11.A little girl sits on a sofa with a teddy bear. (a) A man stands on a beach with a surfboard."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Inspired by recent work in machine translation<lb>and object detection, we introduce an attention<lb>based model that automatically learns to describe<lb>the content of images. We describe how we<lb>can train this model in a deterministic manner<lb>using standard backpropagation techniques and<lb>stochastically by maximizing a variational lower<lb>bound. We also show through visualization how<lb>the model is able to automatically learn to fix its<lb>gaze on salient objects while generating the cor-<lb>responding words in the output sequence. We<lb>validate the use of attention with state-of-the-<lb>art performance on three benchmark datasets:<lb>Flickr8k, Flickr30k and MS COCO.", "creator": "LaTeX with hyperref package"}}}