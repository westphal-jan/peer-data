{"id": "1502.03167", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2015", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.", "histories": [["v1", "Wed, 11 Feb 2015 01:44:18 GMT  (30kb)", "http://arxiv.org/abs/1502.03167v1", null], ["v2", "Fri, 13 Feb 2015 17:31:36 GMT  (30kb)", "http://arxiv.org/abs/1502.03167v2", null], ["v3", "Mon, 2 Mar 2015 20:44:12 GMT  (30kb)", "http://arxiv.org/abs/1502.03167v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sergey ioffe", "christian szegedy"], "accepted": true, "id": "1502.03167"}, "pdf": {"name": "1502.03167.pdf", "metadata": {"source": "CRF", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "authors": ["Sergey Ioffe"], "emails": ["sioffe@google.com", "szegedy@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.03 167v 1 [cs.L G] February 11, 2015Training Deep Neural Networks is complicated by the fact that the distribution of inputs for each layer changes during training as the parameters of the previous layers change. This slows down training because lower learning rates and careful parameter initialization are required, and makes it notoriously difficult to train models with saturating nonlinearity. We call this phenomenon an internal covariate shift and address the problem by normalizing layer entries. Our method draws its strength from making normalization part of the model architecture and performing normalization for each minibatch. Batch normalization allows us to use much higher learning rates and be less careful with initialization. It also acts as a regulator, eliminating in some cases the need for dropout-out. Applied to a state-of-the-art image classification model, batch normalization achieves the same accuracy 14 times less than the signed and 4.5% accuracy of the original training result."}, {"heading": "1 Introduction", "text": "This is not the first time that the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects are so small that the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects are so large that the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution are so large that"}, {"heading": "2 Towards Reducing Internal Covariate Shift", "text": "To improve training, we are trying to reduce the internal covariate shift by adjusting the distribution of input factors in each layer more quickly - i.e., we expect an improvement in training speed. (1998b) It has long been known that LeCun et al. (1998b); Wiesler & Ney (2011) that network training becomes faster when its input factors are whitened - i.e., linear transformed to have zero mean and unit variances, and decorative. Since each layer observes the input factors produced by the layers below, it would be advantageous to achieve the input factors of each layer. Inputs to each layer would take a step toward achieving the fixed distributions of input factors that have the sick effects of the internal covariate shift. We could look at the whitening of inputs in each layer, either by modifying the network or directly changing the network activities."}, {"heading": "3 Normalization via Mini-Batch Statistics", "text": "Since full normalization of the inputs of each level is costly and cannot be differentiated everywhere, we make two necessary simplifications. The first is that instead of reducing the characteristics of the inputs and outputs of each level, we will normalize both types of activation independently by taking the mean of zero and the variance of 1. For a level with d-dimensional input x = (1). x (d)), we will normalize each dimension (k) = x by taking the mean of zero and the variance of 1. (k) Var [x), where the expectation and variance are calculated using the training data. As shown in LeCun et al, such normalization accelerates convergence, even if the characteristics are not decorrelated.Note that simply the normalization of each input of a layer can be changed, which would represent the normalization of the inputs of a sigmoid."}, {"heading": "3.1 Training and Inference with BatchNormalized Networks", "text": "To normalize a network, we specify a subset of activations and insert the BN transformation for each of them, according to Alg. 1. Each layer that previously received x as input now receives BN (x). A model that uses batch normalization can be trained using batch gradient descent or stochastic gradient descent with a mini-batch size m > 1 or with one of its variants, such as AdagradDuchi et al. (2011). Normalization of activations that depend on the mini-batch allows efficient training, but is neither necessary nor desirable during inference; we want the output to depend only on the input, deterrent. To this end, we use the normalization x = x \u2212 E [x]."}, {"heading": "3.2 Batch-Normalized Convolutional Networks", "text": "Here we focus on transformations consisting of an affine transformation, followed by an elementary nonlinearity: z = g (Wu + b), where W and b are learned parameters of the model, and g (\u00b7) is nonlinearity such as sigmoid or ReLU. This formulation covers both fully connected and revolutionary layers. We add the BN transformation immediately before nonlinearity by normalizing x = Wu + b. We could also normalize the layer inputs u, but since u is probably the output of another nonlinearity, the form of their distribution will probably change during training, and the limitation of their first and second moments would not eliminate the covariate shift. In contrast, Wu + b is likely to have a symmetrical, not sparse distribution, which is \"more Gaussian\" hyva \ufffd rinen & Oja (2000); the normalization will probably be replaced by a stable activation."}, {"heading": "3.3 Batch Normalization enables higher", "text": "In traditional deep networks, too high a learning rate can lead to gradients exploding or disappearing, as well as settling in poor local minima. Batch normalization helps solve these problems. By normalizing activations throughout the network, it prevents small changes in parameters that turn into larger and sub-optimal changes in gradients. Normally, large learning rates can increase the scale of layer parameters, which then amplify the gradients during the baking propagation and lead to the model explosion. Batch normalization also makes the training more resistant to the parameter scale. Large learning rates can increase the scale of layer parameters, which then amplify the gradients during the baking propagation and lead to the model explosion. But with batch normalization, the baking propagation is unaffected by a layer that is not affected by the scale of its parameters."}, {"heading": "3.4 Batch Normalization regularizes the model", "text": "When training with batch normalization, a training example is seen in conjunction with other examples in the mini-batch, and the training network no longer produces deterministic values for a particular training example. In our experiments, we found that this effect is beneficial for generalizing the network. While Dropout Srivastava et al. (2014) is typically used to reduce overfits, we found in a batch normalized network that it can either be removed or lost in force."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Activations over time", "text": "To verify the effects of the internal covariate shift on the formation and ability of batch normalization to combat it, we considered the problem of predicting the digit class on the MNIST dataset LeCun et al. (1998a). We used a very simple network with a 28x28 binary image as input, and3 fully connected hidden layers, each with 100 activations. Each hidden layer computes y = g (Wu + b) with sigmoid nonlinearity, and the weights W initialized to small random Gauss values. The last hidden layer is followed by a fully connected layer with 10 activations (one per class) and cross-entropy loss. We trained the network for 50,000 steps with 60 examples per mini-batch. We added batch normalization to each hidden layer of the network, as in Sec. 3.1. We were interested in comparing the baseline state of the batch to reach the IST network, rather than the normalized state of the IST."}, {"heading": "4.2 ImageNet classification", "text": "We applied batch normalization to a new variant of the Inception network Szegedy et al. (2014), which was trained on the ImageNet classification task Russakovsky et al. (2014). The network has a large number of convolutions and pooling layers with a Softmax layer to predict the image class of 1000 possibilities. Convolutions layers use ReLU as non-linearity. The main difference to the network described in Szegedy et al. (2014) is that the 5 x 5 convolutions layers are replaced by two successive layers of 3 x 3 convolutions with up to 128 filters. The network contains 13.6 x 106 parameters and, unlike the top Softmax layer, has no fully connected layers. Further details are given in the appendix. We refer to this model as an inception in the rest of the text. The model was based on a prediction of Stochastic Descent with dynamics."}, {"heading": "4.2.1 Accelerating BN Networks", "text": "To do this, we modified the network and its training parameters as follows: Increase the learning rate. In a batch-normalized model, we succeeded in achieving a training acceleration through higher learning rates without negative side effects (Sec. 3.3). Removal of dropout. As described in Sec. 3.4, batch normalization fulfills some of the same goals as dropout. Removing dropout from modified BN perceptions speeds up the training without increasing the overfitting. Reduce the L2 weight regulation. While in Inzeption an L2 loss in the model parameters controls the adjustment, in Modified BN perception the weight of this loss is reduced by a factor of 5 within a modified BN perception. We find that this improves the accuracy of the validation data presented. Accelerate the learning rate."}, {"heading": "4.2.2 Single-Network Classification", "text": "We evaluated the following networks, all of which were trained on the LSVRC2012 training data and tested on the validation data: Inception: the network described at the beginning of Section 4.2, trained at the initial learning rate of 0.0015.BN-Baseline: Same as Inception with Batch Normalization before each nonlinearity.BN-x5: Inception with Batch Normalization and the modifications in Sec. 4.2.1. The initial learning rate was increased by a factor of 5, to 0.0075. The same learning rate increased with the original inception caused the model parameters to reach machine infinity. BN-x30: Same as BN-x5, but with the initial learning rate 0.045 (30 times that of the inception).BN-x5-Sigmoid: Like BN-x5-Sigmoid, but with sigmoid nonlinearity (t) = 11 + (exp \u2212 LU)."}, {"heading": "4.2.3 Ensemble Classification", "text": "The current best result on the ImageNet Large Scale Visual Recognition Competition is achieved by the Deep Image Ensemble of traditional models Wu et al. (2015). At this point, we report a validation error of the top 5 of 4.9% (and 4.82% over the test set), which improves over the previous best result, although 15x fewer parameters and a lower resolution receiver field were used. Our system exceeds the estimated accuracy of human raters according to Russakovsky et al. (2014).For our ensemble, we used 6 networks. Each was based on BN-x30, modified by some of the following: increased initial weights in the winding layers; use of dropout (with the dropout probability of 5% or 10%, compared to 40% for the original conception); and use of non-revolutionary, per-activation modified batch normalization with the last hidden layers of the model. Each network achieved its maximum accuracy after approximately 6 steps of training."}, {"heading": "5 Conclusion", "text": "We have presented a new mechanism for dramatically accelerating the formation of deep networks, based on the premise that covariate shift, which is known to complicate the formation of machine learning systems, also applies to subnets and layers, and that it may be supported by external activations of the network in training. Our proposed method refers to the normalization of activations and the incorporation of this normalization into the network architecture itself, ensuring that normalization is handled appropriately by any optimization method used to enable stochastic optimization methods commonly used in deep network training, and we perform normalization for each mini-batch batch batch and the regression of gradients through the normalization savings. Batch normalization adds only two additional parameters per activation, thus preserving the network's representativeness."}], "references": [{"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Bengio", "Yoshua", "Glorot", "Xavier"], "venue": "In Proceedings of AISTATS 2010,", "citeRegEx": "Bengio et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg S", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Le", "Quoc V", "Mao", "Mark Z", "Ranzato", "Marc\u2019Aurelio", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Independent component analysis: Algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural Netw.,", "citeRegEx": "Hyv\u00e4rinen and Oja,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Oja", "year": 2000}, {"title": "A literature survey on domain adaptation of statistical classifiers", "author": ["Jiang", "Jing"], "venue": null, "citeRegEx": "Jiang and Jing.,? \\Q2008\\E", "shortCiteRegEx": "Jiang and Jing.", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Nonlinear image representation using divisive normalization", "author": ["S Lyu", "Simoncelli", "E P"], "venue": "In Proc. Computer Vision and Pattern Recognition,", "citeRegEx": "Lyu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2008}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In ICML,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Critical points of the singular value decomposition", "author": ["O\u2019Neil", "Kevin A"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "O.Neil and A.,? \\Q2005\\E", "shortCiteRegEx": "O.Neil and A.", "year": 2005}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["Povey", "Daniel", "Zhang", "Xiaohui", "Khudanpur", "Sanjeev"], "venue": "CoRR, abs/1410.7455,", "citeRegEx": "Povey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2014}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["Raiko", "Tapani", "Valpola", "Harri", "LeCun", "Yann"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Raiko et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "CoRR, abs/1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Shimodaira", "Hidetoshi"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodaira and Hidetoshi.,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira and Hidetoshi.", "year": 2000}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George E", "Hinton", "Geoffrey E"], "venue": "In ICML (3),", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Mean-normalized stochastic gradient for large-scale deep learning", "author": ["Wiesler", "Simon", "Richard", "Alexander", "Schl\u00fcter", "Ralf", "Ney", "Hermann"], "venue": "Neural Information Processing Systems", "citeRegEx": "Wiesler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wiesler et al\\.", "year": 2011}, {"title": "The notable architecture changes compared to the GoogLeNet model", "author": ["table", "please consult Szegedy"], "venue": null, "citeRegEx": "table and Szegedy,? \\Q2014\\E", "shortCiteRegEx": "table and Szegedy", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Stochastic gradient descent (SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum Sutskever et al. (2013) and Adagrad Duchi et al.", "startOffset": 129, "endOffset": 153}, {"referenceID": 2, "context": "(2013) and Adagrad Duchi et al. (2011) have been used to achieve state of the art performance.", "startOffset": 19, "endOffset": 39}, {"referenceID": 13, "context": "In practice, the saturation problem and the resulting vanishing gradients are usually addressed by using Rectified Linear Units Nair & Hinton (2010) ReLU(x) = max(x, 0), careful initialization Bengio & Glorot (2010); Saxe et al. (2013), and small learning rates.", "startOffset": 217, "endOffset": 236}, {"referenceID": 15, "context": "Furthermore, batch normalization regularizes the model and reduces the need for Dropout Srivastava et al. (2014). Finally, Batch Normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.", "startOffset": 88, "endOffset": 113}, {"referenceID": 5, "context": "It has been long known LeCun et al. (1998b); Wiesler & Ney (2011) that the network training converges faster if its inputs are whitened \u2013 i.", "startOffset": 23, "endOffset": 44}, {"referenceID": 5, "context": "It has been long known LeCun et al. (1998b); Wiesler & Ney (2011) that the network training converges faster if its inputs are whitened \u2013 i.", "startOffset": 23, "endOffset": 66}, {"referenceID": 15, "context": "We could consider whitening activations at every training step or at some interval, either by modifying the network directly or by changing the parameters of the optimization algorithm to depend on the network activation values Wiesler et al. (2014); Raiko et al.", "startOffset": 228, "endOffset": 250}, {"referenceID": 11, "context": "(2014); Raiko et al. (2012); Povey et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 11, "context": "(2012); Povey et al. (2014); Desjardins & Kavukcuoglu.", "startOffset": 8, "endOffset": 28}, {"referenceID": 5, "context": "As shown in LeCun et al. (1998b), such normalization speeds up convergence, even when the features are not decorrelated.", "startOffset": 12, "endOffset": 33}, {"referenceID": 2, "context": "A model employing Batch Normalization can be trained using batch gradient descent, or Stochastic Gradient Descent with a mini-batch size m > 1, or with any of its variants such as Adagrad Duchi et al. (2011). The normalization of activations that depends on the mini-batch allows efficient training, but is neither necessary nor desirable during inference; we want the output to depend only on the input, deterministically.", "startOffset": 188, "endOffset": 208}, {"referenceID": 13, "context": "We further conjecture that Batch Normalization may lead the layer Jacobians to have singular values close to 1, which is known to be beneficial for training Saxe et al. (2013). Consider two consecutive layers with normalized inputs, and the transformation between these normalized vectors: \u1e91 = F (x\u0302).", "startOffset": 157, "endOffset": 176}, {"referenceID": 15, "context": "Whereas Dropout Srivastava et al. (2014) is typically used to reduce overfitting, in a batch-normalized network we found that it can be either removed or reduced in strength.", "startOffset": 16, "endOffset": 41}, {"referenceID": 5, "context": "To verify the effects of internal covariate shift on training, and the ability of Batch Normalization to combat it, we considered the problem of predicting the digit class on the MNIST dataset LeCun et al. (1998a). We used a very simple network, with a 28x28 binary image as input, and", "startOffset": 193, "endOffset": 214}, {"referenceID": 15, "context": "The model was trained using a version of Stochastic Gradient Descent with momentum Sutskever et al. (2013), using the mini-batch size of 32.", "startOffset": 83, "endOffset": 107}, {"referenceID": 1, "context": "The training was performed using a large-scale, distributed architecture (similar to Dean et al. (2012)).", "startOffset": 85, "endOffset": 104}, {"referenceID": 15, "context": "Remove Local Response Normalization While Inception and other networks Srivastava et al. (2014) benefit from it, we found that with Batch Normalization it is not necessary.", "startOffset": 71, "endOffset": 96}, {"referenceID": 10, "context": "Our future work includes applications of our method to Recurrent Neural Networks Pascanu et al. (2013), where the internal covariate shift and the vanishing or exploding gradients may be especially severe, and which would allow us to more thoroughly test the hypothesis that normalization improves gradient propagation (Sec.", "startOffset": 81, "endOffset": 103}], "year": 2015, "abstractText": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.", "creator": "LaTeX with hyperref package"}}}