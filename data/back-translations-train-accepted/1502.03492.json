{"id": "1502.03492", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2015", "title": "Gradient-based Hyperparameter Optimization through Reversible Learning", "abstract": "Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.", "histories": [["v1", "Wed, 11 Feb 2015 23:52:36 GMT  (235kb,D)", "https://arxiv.org/abs/1502.03492v1", "10 figures. Submitted to ICML"], ["v2", "Fri, 13 Feb 2015 19:26:39 GMT  (235kb,D)", "http://arxiv.org/abs/1502.03492v2", "10 figures. Submitted to ICML"], ["v3", "Thu, 2 Apr 2015 17:40:44 GMT  (235kb,D)", "http://arxiv.org/abs/1502.03492v3", "10 figures. Submitted to ICML"]], "COMMENTS": "10 figures. Submitted to ICML", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dougal maclaurin", "david k duvenaud", "ryan p adams"], "accepted": true, "id": "1502.03492"}, "pdf": {"name": "1502.03492.pdf", "metadata": {"source": "META", "title": "Gradient-based Hyperparameter Optimization through Reversible Learning", "authors": ["Dougal Maclaurin", "David Duvenaud", "Ryan P. Adams"], "emails": ["MACLAURIN@PHYSICS.HARVARD.EDU", "DDUVENAUD@SEAS.HARVARD.EDU", "RPA@SEAS.HARVARD.EDU"], "sections": [{"heading": "1. Introduction", "text": "These can be parameters that control the complexity of the model, such as penalties L1 and L2, or parameters that determine the learning process itself - step variables, dynamics decay parameters, and initialization conditions. Selecting the best hyperparameters is both critical and frustratingly difficult; the current gold standard for hyperparameter selection is gradient-free model-based optimization (Snoek et al., 2012; Bergstra et al., 2011; Hutter et al., 2011). Hyperparameters are chosen to optimize the validation loss after the model parameters have been fully formed. These approaches have shown that the automatic matching of hyperparameters can provide state-of-the-art performance, but in general they are unable to effectively optimize more than 10 to 20 hyperparameters."}, {"heading": "1.1. Contributions", "text": "\u2022 We provide an algorithm for calculating Gradi-ar Xiv: 150 2.03 492v 3 [stat. ML] 2 Apr 201 5ents with respect to all continuous training parameters. \u2022 We show how to efficiently store only the information needed to accurately reverse learning dynamics. For example, if the impulse pattern is 0.9, this method reduces the memory required to differentiate hyperparameters in reverse mode by a factor of 200. \u2022 We show that these gradients enable optimization of validation losses for thousands of hyperparameters. \u2022 We provide insights into learning processes by analyzing optimized learning rate plans, initialization distributions of neural network parameters per level, regulation schemes per input and data preprocessing per pixel, and comparing them with standard suggestions in the literature."}, {"heading": "2. Hypergradients", "text": "Reverse mode differentiation (RMD) has been an asset in the field of machine learning (LeCun et al., 1989) (see the 7 for a refresher).The RMD method, known in the deep learning community as \"back propagation,\" allows the gradient of a scalar loss in terms of its parameters to be calculated in a single reverse gear, increasing the computing load by only a factor of two over the evaluation of the loss itself, regardless of the number of parameters.Obtaining the same type of information by differentiating the forward mode or raw force finite differences would require a separate pass for each parameter and would make deep learning completely unfeasible. Applying RMD to hyperparameter optimization was proposed by Bengio (2000) and Baydin & Pearlmutter (2014), and applying it to minor problems by Domke (2012)."}, {"heading": "2.1. Reversible learning with exact arithmetic", "text": "Stochastic Gradient Descent (SGD) with impulse (algorithm 1) can be regarded as a physical simulation of a system moving through a series of fixed force fields indexed by time. \u2212 With exact arithmetic, this procedure is reversible. Thus, we can write algorithm 2, which reverses the steps in algorithm 1, and all other hyperparameters that affect the training gradients. \u2212 It returns the gradient of a function of the trained weights f (w) (such as the validation loss) with respect to the initial weights w1, the learning rate and impulse plans, as well as all other hyperparameters that affect the training gradients. \u2212 Algorithm 1 Stochastic Gradient Descent with impulse 1: Input: Initial gradient w1, decay rates, loss functions L (w), loss functions L (w) 2: Initialization v1: f = 0 3: do for T = 4 for T: wt = T (T = 4)."}, {"heading": "2.2. Reversible learning with finite precision arithmetic", "text": "Every time we apply step 8 to slow down the speed, we lose information. Suppose we use a fixed point representation, 2 any multiplication by \u03b3 < 1 shifts bits to the right, destroying the least significant bits. That's more than a pedantic worry. Trying to perform the reverse training requires repeated multiplication by 1 / \u03b3. Errors pile up exponentially, and the reverse learning process ends far from the starting point (and usually it overflows). Do we need \u03b3 < 1? Unfortunately, we lead to unstable dynamics \u03b3 > 1, and vice versa = 1, restores the erratic integrator (Hut et al., 1995), a completely reversible learning process ends far from the starting point (and usually overflows)."}, {"heading": "2.3. Optimal storage of discarded entropy", "text": "This chapter gives the technical details on how to efficiently store the information discarded each time the impulse decay operation is applied (step 8). If \u03b3 = 0.5, we can simply store the single bit that drops off at each iteration, and if \u03b3 = 0.25 we could store two bits. But, for fine-grained control over \u03b3, we need a way to store the information we lost when we multiply, say, \u03b3 = 0.9, which will be less than one bit on average. Here, we are giving a procedure that achieves just that. We represent the velocity v and the parameters w vectors with2We assume that the fix point representation simplifies the discussion (and the implementation). Courbariaux et al. (2014) show that fixed point tarithmetics are sufficient to train deep networks. Floatingpoint representation does not fix the problem, it only separates the loss of information from the division."}, {"heading": "3. Experiments", "text": "In typical machine learning applications, only a few hyperparameters (less than 20) are optimized. As each experiment yields only a single number (the validation loss), the search quickly becomes more difficult as the dimension of the hyperparameter vector increases. In contrast, with hypergradients available, the amount of information gained from each training run increases with the number of hyperparameters, allowing us to optimize thousands of hyperparameters. How can we take advantage of this new capability? This section shows several proof-of-concept experiments where we can parameterize training and regulation systems more abundantly in ways that would not previously have been feasible to optimize them."}, {"heading": "3.1. Gradient-based optimization of gradient-based optimization", "text": "Modern neural training methods often employ various hyporistics to define learning layers, or set their form using one or two hyperparameters defined by cross-validation (Dahl et al., 2014; Sutskever et al., 2013) These scheduling decisions are supported by a mixture of intuition, arguments about the form of objective function and empirical alignment. In order to shine a more direct light on good learning layers, we have collectively separated learning rates for each individual learning layer of a deep neural network and separately optimized for weights and distortions in each layer. Each meta-iteration has trained a network of 100 SGD iterations, which means that the learning layer plans were specified by 800 hyperparameters (100 iterations x 4 layers x 2 types of parameters), in order to prevent an optimization scheme for the layers of learning that are dependent on the quotas of a particular learning layer initiation of the learning layer, the learning layer location of the learning layer, the location of the learning layer we are in."}, {"heading": "3.2. Optimizing regularization parameters", "text": "Typically, a single parameter controls a single L2 standard or splitting penalty for the entire parameter vector of a neural network. As different types of parameters play different roles in different layers, it is reasonable to assume that separate regularization hyperparameters for each parameter type would improve performance. In fact, Snoek et al. (2012) optimized separate regularization parameters for each layer of a neural network and found that they improved performance. We chose this model because the EVA weight corresponds to an input pixel and output label pair, meaning that these 7,840 hyperparameters could be relatively interpretable. Figure 6 shows a number of regularization hyperparameters learned for a particular regularization scheme."}, {"heading": "3.3. Optimizing training data", "text": "By concatenating gradients by transforming the data, we can calculate gradients of the validation target in terms of data pre-processing, weighting or augmentation procedures. We demonstrate a simple proof-of-concept, where a complete training set is learned by gradient descent, starting with blank images. Figure 7 shows a training set whose pixels have been optimized to improve performance on a validation set of 10,000 examples from the MNIST. We optimized 10 training examples, each with a different fixed label, also from 0 to 9. Learning the labels of a larger training set could shed light on which classes are hard to distinguish and therefore require more examples."}, {"heading": "3.4. Optimizing initial parameters", "text": "The last remaining parameter of SGD is the initial parameter vector. Treating this vector as a hyperparameter blurs the distinction between learning and meta-learning. In extreme cases, when all elementary learning rates are set to zero, the training set no longer matters and the meta-learning procedure is reduced to elementary learning on the validation set. Due to philosophical hoax, we decided not to optimize the initial parameter vector."}, {"heading": "3.5. Learning continuously parameterized architetures", "text": "In fact, most of them will be able to feel as if they are able to play by the rules, and that they will be able to play by the rules."}, {"heading": "3.6. Implementation Details", "text": "Software packages for automatic differentiation (AD) such as Theano (Bastien et al., 2012; Bergstra et al., 2010) are mainstays of deep learning and significantly speed up development time by automatically providing gradients. Because we needed access to the internal logic of RMD to implement Algorithm 2, we implemented our own automatic differentiation package for Python, available at github.com / HIPS / autograd. This package differentiates the standard numpy code (Oliphant, 2007) and can differentiate code that includes loops, branches and even gradient valuations.Code for all experiments in this essay is available at github.com / HIPS / hypergrad."}, {"heading": "4. Limitations", "text": "The use of hypergradients also has several obvious difficulties that need to be addressed before they become practicable. In this section, several problems with this technique are explored, which became apparent in our experiments.When do gradients make sense? Bengio et al. (1994) noted that \"learning long-term dependencies with gradient descent is difficult.\" Our situation is even worse: we use gradients to optimize functions that depend on their hyperparameters through hundreds of iterations of the SGD. To make matters worse, the gradient of each elementary iteration itself depends on forward and backward propagation through a neural network. Therefore, the same problems that sometimes make elementary learning difficult are convenient. For example, Pearlmutter (1996, Chapter 4) shows that high learning rates cause chaotic behavior in learning dynamics, making the gradient uninformative."}, {"heading": "5. Related work", "text": "The most closely related work is Domke (2012), which derived algorithms to calculate reverse mode derivatives of gradient descent with dynamics and L-BFGS, updating the hyperparameters of CRF image models, but based on naive caching of all parameter vectors w1, w2,.., wT, making it impractical for large models with many training processes.Larsen et al. (1998), Eigenmann & Nossek (1999), Chen & Hagan (1999), Bengio (2000), Abdel-Gawad & Ratner (2007) and Foo et al. (2008) showed that gradients of regulation parameters are available in closed form when the training has been converted exactly to a local minimum. In contrast, our method can calculate exact gradients of hyperparameters of any kind of hyperparameters, whether or not."}, {"heading": "6. Extensions and future work", "text": "Bayesque gradient optimization methods could be used with parallel, model-based optimizations of hypergradients (Solak et al., 2003), which could rely on parallel evaluations of hypergradients whose sequential evaluation may be too slow (Martens & Sutskever, 2012).Reversible elementary calculation Recurring neural network models may require so much memory to distinguish that checkpointing is required to easily calculate their elementary gradients (Martens & Sutskever, 2012).Reversible calculations may provide memory savings for some architectures, such as evaluations of Long ShortTerm Memory (Hochreiter & Schmidhuber, 1997) or a Neural Turing Machines (Graves et al., 2014) based on long chains of mostly small parameter updates."}, {"heading": "7. Conclusion", "text": "In this work, we derived a computationally efficient method for calculating gradients by stochastic gradient pedigree with Momentum. We demonstrated how the approximate reversibility of learning dynamics can be used to drastically reduce the memory requirement for exact backpropagation of gradients through hundreds of training processes. We demonstrated how these gradients allow optimization of validation losses in relation to thousands of hyperparameters, which was not possible before. This new ability enables automatic matching of most details of neural network training. We demonstrated the coordination of detailed training plans, regulatory plans and neural network architectures."}, {"heading": "Acknowledgments", "text": "We thank Christian Steinruecken, Oren Rippel and Matthew James Johnson for helpful conversations. We also thank Brenden Lake for kindly providing the Omniglot dataset. Thanks to Jason Rolfe for helpful feedback. We thank Analog Devices International and Samsung Advanced Institute of Technology for their support."}, {"heading": "Appendix: Forward vs. reverse-mode differentiation", "text": "By the chain rule, the gradient of a number of nested functions is given by the product of the individual derivatives of each function: \u2202 f4 (f2 (f1 (x)) \u2202 x = \u2202 f4 \u2202 f3 \u00b7 \u2202 f3 \u2202 f2 \u00b7 \u2202 f2 \u2202 f1 \u2202 xIf each function has multivariate inputs and outputs, the gradients are Jacobic matrices. Differentiation in forward and backward mode differs only in the order in which they evaluate this product. Differentiation in forward mode works by multiplying gradients in the same order as the functions are evaluated."}], "references": [{"title": "Adaptive optimization of hyperparameters in L2-regularised logistic regression", "author": ["Abdel-Gawad", "Ahmed", "Ratner", "Simon"], "venue": "Technical report,", "citeRegEx": "Abdel.Gawad et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Abdel.Gawad et al\\.", "year": 2007}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Automatic differentiation of algorithms for machine learning", "author": ["A.G. Baydin", "B.A. Pearlmutter"], "venue": "In Proceedings of the AutoML Workshop at the International Conference on Machine Learning (ICML),", "citeRegEx": "Baydin and Pearlmutter,? \\Q2014\\E", "shortCiteRegEx": "Baydin and Pearlmutter", "year": 2014}, {"title": "Gradient-based optimization of hyperparameters", "author": ["Bengio", "Yoshua"], "venue": "Neural computation,", "citeRegEx": "Bengio and Yoshua.,? \\Q2000\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2000}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Algorithms for hyper-parameter optimization", "author": ["Bergstra", "James", "Bardenet", "R\u00e9mi", "Bengio", "Yoshua", "K\u00e9gl", "Bal\u00e1zs"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures", "author": ["Bergstra", "James", "Yamins", "Daniel", "Cox", "David"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Bergstra et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2013}, {"title": "Choosing multiple parameters for support vector machines", "author": ["Chapelle", "Olivier", "Vapnik", "Vladimir", "Bousquet", "Mukherjee", "Sayan"], "venue": "Machine learning,", "citeRegEx": "Chapelle et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2002}, {"title": "Optimal use of regularization and cross-validation in neural network modeling", "author": ["Chen", "Dingding", "Hagan", "Martin T"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Chen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1999}, {"title": "JeanPierre. Low precision arithmetic for deep learning", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Multi-task neural networks for QSAR predictions", "author": ["Dahl", "George E", "Jaitly", "Navdeep", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1406.1231,", "citeRegEx": "Dahl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2014}, {"title": "Generic methods for optimization-based modeling", "author": ["Domke", "Justin"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Domke and Justin.,? \\Q2012\\E", "shortCiteRegEx": "Domke and Justin.", "year": 2012}, {"title": "Gradient based adaptive regularization", "author": ["Eigenmann", "Robert", "Nossek", "Josef A"], "venue": "In Proceedings of the 1999 IEEE Signal Processing Society Workshop on Neural Networks,", "citeRegEx": "Eigenmann et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Eigenmann et al\\.", "year": 1999}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Efficient multiple hyperparameter learning for log-linear models", "author": ["Foo", "Chuan-sheng", "Do", "Chuong B", "Ng", "Andrew Y"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Foo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Foo et al\\.", "year": 2008}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Nested variational compression in deep Gaussian processes", "author": ["Hensman", "James", "Lawrence", "Neil D"], "venue": "arXiv preprint arXiv:1412.1370,", "citeRegEx": "Hensman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2014}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Building a better leapfrog", "author": ["P. Hut", "J. Makino", "S. McMillan"], "venue": "Astrophysical Journal, Part 2 - Letters,", "citeRegEx": "Hut et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hut et al\\.", "year": 1995}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["Hutter", "Frank", "Hoos", "Holger H", "Leyton-Brown", "Kevin"], "venue": "In Proceedings of LION-5,", "citeRegEx": "Hutter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Towards more human-like concept learning in machines: Compositionality, causality, and learning-to-learn", "author": ["Lake", "Brenden M"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Lake and M.,? \\Q2014\\E", "shortCiteRegEx": "Lake and M.", "year": 2014}, {"title": "Adaptive regularization in neural network modeling", "author": ["Larsen", "Jan", "Svarer", "Claus", "Andersen", "Lars Nonboe", "Hansen", "Lars Kai"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Larsen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 1998}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Information theory, inference, and learning algorithms", "author": ["MacKay", "David J.C"], "venue": null, "citeRegEx": "MacKay and J.C.,? \\Q2003\\E", "shortCiteRegEx": "MacKay and J.C.", "year": 2003}, {"title": "Automatic relevance determination for neural networks", "author": ["MacKay", "David J.C", "Neal", "Radford M"], "venue": "Technical Report. Cambridge University,", "citeRegEx": "MacKay et al\\.,? \\Q1994\\E", "shortCiteRegEx": "MacKay et al\\.", "year": 1994}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["Martens", "James", "Sutskever", "Ilya"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Martens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2012}, {"title": "Python for scientific computing", "author": ["Oliphant", "Travis E"], "venue": "Computing in Science & Engineering,", "citeRegEx": "Oliphant and E.,? \\Q2007\\E", "shortCiteRegEx": "Oliphant and E.", "year": 2007}, {"title": "Understanding the exploding gradient problem", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "An investigation of the gradient descent process in neural networks", "author": ["Pearlmutter", "Barak"], "venue": "PhD thesis,", "citeRegEx": "Pearlmutter and Barak.,? \\Q1996\\E", "shortCiteRegEx": "Pearlmutter and Barak.", "year": 1996}, {"title": "Fast exact multiplication by the Hessian", "author": ["Pearlmutter", "Barak A"], "venue": "Neural computation,", "citeRegEx": "Pearlmutter and A.,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter and A.", "year": 1994}, {"title": "Reversemode AD in a functional framework: Lambda the ultimate backpropagator", "author": ["Pearlmutter", "Barak A", "Siskind", "Jeffrey Mark"], "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),", "citeRegEx": "Pearlmutter et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pearlmutter et al\\.", "year": 2008}, {"title": "Gaussian Processes for Machine Learning", "author": ["Rasmussen", "Carl E", "Williams", "Christopher K.I"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Markov chain Monte Carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1410.6460,", "citeRegEx": "Salimans et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2014}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Derivative observations in Gaussian process models of dynamic systems", "author": ["E. Solak", "R. Murray Smith", "W.E. Leithead", "D. Leith", "Rasmussen", "Carl E"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Solak et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Solak et al\\.", "year": 2003}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V. V"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "Coursera: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}], "referenceMentions": [{"referenceID": 34, "context": "The current gold standard for hyperparameter selection is gradient-free model-based optimization (Snoek et al., 2012; Bergstra et al., 2011; 2013; Hutter et al., 2011).", "startOffset": 97, "endOffset": 167}, {"referenceID": 5, "context": "The current gold standard for hyperparameter selection is gradient-free model-based optimization (Snoek et al., 2012; Bergstra et al., 2011; 2013; Hutter et al., 2011).", "startOffset": 97, "endOffset": 167}, {"referenceID": 19, "context": "The current gold standard for hyperparameter selection is gradient-free model-based optimization (Snoek et al., 2012; Bergstra et al., 2011; 2013; Hutter et al., 2011).", "startOffset": 97, "endOffset": 167}, {"referenceID": 23, "context": "Reverse-mode differentiation (RMD) has been an asset to the field of machine learning (LeCun et al., 1989) (see the 7 for a refresher).", "startOffset": 86, "endOffset": 106}, {"referenceID": 37, "context": "In large neural networks, the amount of memory required to store the millions of parameters being trained is typically close to the amount of physical RAM available (Sutskever et al., 2014).", "startOffset": 165, "endOffset": 189}, {"referenceID": 18, "context": "\u03b3 > 1 results in unstable dynamics, and \u03b3 = 1, recovers the leapfrog integrator (Hut et al., 1995), a perfectly reversible set of dynamics, but one that does not converge.", "startOffset": 80, "endOffset": 98}, {"referenceID": 9, "context": "Courbariaux et al. (2014) show that fixed-point arithmetic is sufficient to train deep networks.", "startOffset": 0, "endOffset": 26}, {"referenceID": 10, "context": "Modern neural net training procedures often employ various heuristics to set learning rate schedules, or set their shape using one or two hyperparameters set by crossvalidation (Dahl et al., 2014; Sutskever et al., 2013).", "startOffset": 177, "endOffset": 220}, {"referenceID": 36, "context": "Modern neural net training procedures often employ various heuristics to set learning rate schedules, or set their shape using one or two hyperparameters set by crossvalidation (Dahl et al., 2014; Sutskever et al., 2013).", "startOffset": 177, "endOffset": 220}, {"referenceID": 13, "context": "Because learning schedules can implicitly regularize networks (Erhan et al., 2010), for example by enforcing early stopping, for this experiment we optimized the learning rate schedules on the training error rather than on the validation set error.", "startOffset": 62, "endOffset": 82}, {"referenceID": 34, "context": "Indeed, Snoek et al. (2012) optimized separate regularization parameters for each layer in a neural network, and found that it improved performance.", "startOffset": 8, "endOffset": 28}, {"referenceID": 1, "context": "Automatic differentiation (AD) software packages such as Theano (Bastien et al., 2012; Bergstra et al., 2010) are mainstays of deep learning, significantly speeding up development time by providing gradients automatically.", "startOffset": 64, "endOffset": 109}, {"referenceID": 4, "context": "When are gradients meaningful? Bengio et al. (1994) noted that \u201clearning long-term dependencies with gradient descent is difficult.", "startOffset": 31, "endOffset": 52}, {"referenceID": 28, "context": "This phenomenon is related to the exploding-gradient problem (Pascanu et al., 2012).", "startOffset": 61, "endOffset": 83}, {"referenceID": 14, "context": "(1998), Eigenmann & Nossek (1999), Chen & Hagan (1999), Bengio (2000), Abdel-Gawad & Ratner (2007), and Foo et al. (2008) showed that gradients of regularization parameters are available in closed form when training has converged exactly to a local minimum.", "startOffset": 104, "endOffset": 122}, {"referenceID": 7, "context": "Support vector machines Chapelle et al. (2002) introduced a differentiable bound on the SVM loss in order to be able to compute derivatives with respect to hundreds of hyperparameters, including weighting parameters for each input dimension in the kernel.", "startOffset": 24, "endOffset": 47}, {"referenceID": 33, "context": "Gradients with respect to Markov chain parameters Salimans et al. (2014) tune the step-size and mass-matrix parameters of Hamiltonian Monte Carlo by chaining gradients from a lower bound on the marginal likelihood through several iterations of leapfrog dynamics.", "startOffset": 50, "endOffset": 73}, {"referenceID": 35, "context": "For example, Gaussian-process-based optimization methods could incorporate gradient information (Solak et al., 2003).", "startOffset": 96, "endOffset": 116}, {"referenceID": 15, "context": "For example, evaluations of Long ShortTerm Memory (Hochreiter & Schmidhuber, 1997) or a Neural Turing Machines (Graves et al., 2014) rely on long chains of mostly-small updates of parameters.", "startOffset": 111, "endOffset": 132}], "year": 2015, "abstractText": "Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.", "creator": "LaTeX with hyperref package"}}}