{"id": "1502.03508", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Adding vs. Averaging in Distributed Primal-Dual Optimization", "abstract": "Distributed optimization algorithms for large-scale machine learning suffer from a communication bottleneck. Reducing communication makes the efficient aggregation of partial work from different machines more challenging. In this paper we present a novel generalization of the recent communication efficient primal-dual coordinate ascent framework (CoCoA). Our framework, CoCoA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allowed conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both CoCoA as well as our new variants, and generalize the theory for both methods to also cover non-smooth convex loss functions. We provide an extensive experimental comparison on several real-world distributed datasets, showing markedly improved performance, especially when scaling up the number of machines.", "histories": [["v1", "Thu, 12 Feb 2015 01:51:08 GMT  (767kb,D)", "http://arxiv.org/abs/1502.03508v1", null], ["v2", "Fri, 3 Jul 2015 19:35:13 GMT  (769kb,D)", "http://arxiv.org/abs/1502.03508v2", "ICML 2015: JMLR W&amp;CP volume37, Proceedings of The 32nd International Conference on Machine Learning, pp. 1973-1982"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chenxin ma", "virginia smith", "martin jaggi", "michael i jordan", "peter richt\u00e1rik", "martin tak\u00e1c"], "accepted": true, "id": "1502.03508"}, "pdf": {"name": "1502.03508.pdf", "metadata": {"source": "META", "title": "Adding vs. Averaging in Distributed Primal-Dual Optimization ", "authors": ["Chenxin Ma", "Martin Jaggi"], "emails": ["CHM514@LEHIGH.EDU", "VSMITH@BERKELEY.EDU", "JAGGI@INF.ETHZ.CH", "JORDAN@CS.BERKELEY.EDU", "PETER.RICHTARIK@ED.AC.UK", "TAKAC.MT@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "With the wide availability of large data sets that exceed the storage capacity of individual machines, distributed optimization methods have become increasingly important for machine learning. Existing methods require significant communication between workers, which often offsets the amount of local calculations (or reading local data). As a result, we focus on optimization problems with the minimization structure, which means that the goal is a sum of the loss functions of each data set, with the most commonly used variants of linear regression and classification methods being slower. In this work, we focus on optimization problems with the empirical loss structure, which means that the goal is a sum of the loss functions of each data set. This includes the most commonly used variants of linear regression and classification methods."}, {"heading": "1.1. Contributions", "text": "Strong Scaling. To our knowledge, our distributed optimization framework is the first to provide a favorable strong scaling for the class of problems considered when the number of machines K increases while the data size is fixed. Specifically, while the convergence rate of the COCOA decreases with increasing number of workers K, the stronger theoretical convergence rate here is - at worst - independent of K. Our practical experiments in Section 7 confirm the improved speed of convergence. Since the number of communicated vectors is only one per round per worker, this favorable scaling could be surprising. In fact, splitting the data among more machines generally increases the requirements for communication (Shamir & Srebro, 2014), which often has serious effects on total runtime. Theoretical analysis for non-frictionless functions While the existing analysis for COCOA in (Jaggi et al., 2014) only covers smooth loss functions, we are expanding the general class of functions that apply to machines (e.g., the Vectors)."}, {"heading": "1.2. History and Related Work", "text": "While optimal algorithms for the serial (single machine) case are already very well researched and understood, the literature in the distributed environment is relatively sparse: in particular, the details of optimal trade-offs between calculation and communication as well as optimisation or statistical accuracy are still largely unclear. We refer the reader to Balcan et al., 2012; Richta'rik et al., 2013b; Duchi et al., 2013; Yang et al., 2013;?; Liu & Wright, 2014; Fercoq & Richta'rik, 2013; Fercoq et al., 2014; Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al., 2014a; Zhang & Xiao, 2015; Qu & Richta'rik, 2014) and the references contained therein."}, {"heading": "2. Setup", "text": "We consider regulated empirical loss minimization problem of the following well-established form: min w \u00b2 Rd {P (w): = 1n \u2211 i = 1 \"i\" (x T i) + 0 \"2\" 2. \"(1) Here the vectors {xi} ni = 1\" Rd \"represent the educational data examples, and the\" i \"(.) are any convex real loss functions (e.g. hinge loss), possibly dependent on the identification information for the i-th datapoints. The constant \u03bb > 0 is the regulation parameter, and the\" i \"class of problems includes many standard problems of great interest in the fields of machine learning, statistics and signal processing, including support vector machines, as well as regulated linear and logistic regression, ordinal regression and others. Dual problem, and primary dual certificates. The conjugated dual optimization problem of (1) assumes the following form max:"}, {"heading": "3. The COCOA+ Algorithm Framework", "text": "We write the size of each part by nk = | Pk |. For all k-points [K] and \u03b1-points [n], we use the notation \u03b1 [k] and Rn for the vector (\u03b1 [k]). We can define a datalogical sub-problem of the original dual optimization problem (2), which can be solved on the machine k and requires only access to data that is already available locally on the machine, i.e. data points with i-Pk. Formally, we assign the following sub-problem to each machine k, which is valid only depending on the modification of local dual variables. \u2212 k data that is available locally on the machine, i.e. data points with i-Pk. Formally, we will assign to each machine k the following sub-problem, which is valid only depending on the modification of local dual variables."}, {"heading": "Notion of Approximation Quality of the Local Solver.", "text": "We assume that there is such a problem that the local solution of any iteration generates a (possibly) randomized approximate solution. (13) We are now ready to describe the COCOA + algorithm system presented in algorithm 1. The crucial difference from the existing COCOA algorithm 2. (Jaggi et al., 2014) is the more general sub-problem as defined in (9). (The crucial difference compared to the existing COCOA algorithm 1. (Jaggi et al., 2014)."}, {"heading": "4. Convergence Guarantees", "text": "Before we can set out our main convergence results, let me introduce some useful quantities and the following main dilemma, which characterizes the effects of the iterations of the algorithm 1 for each local solver selected. Lemma 5. Let me say that it applies to all iterations of t of the algorithm 1 assuming 1, and to all s [0, 1] that E [D (t + 1)) \u2212 D (t) (t) (15) (1 \u2212) (sG (t) (n) (n) (n) (n) (n) (n) (t) (t) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n)."}, {"heading": "4.1. Primal-Dual Convergence for General Convex Losses", "text": "The following theorem shows the convergence of non-smooth loss functions with respect to objective values and primary-dual gap. Analysis in Jaggi et al., 2014 covers only the case of smooth loss functions. Theorem 8. Let us consider algorithm 1 starting from \u03b1 (0) = 0 \"Rn\" and let us hold assumption 1 and expect that \"i () be L-Lipschitz continuous and G > 0 be the desired duality gap (and thus an upper-bound on primal sub-optimality).\" Then after T-iterations, whereas T0 + max \"1\" (1) the accuracy of T0 + max. \"(1), 4L2st-bound on primal sub-optimality.\" Then, after T-iterations, T0 + 1) the accuracy of T0 + max. (1)"}, {"heading": "4.2. Primal-Dual Convergence for Smooth Losses", "text": "The following theorem shows the convergence for smooth loss functions, both with respect to objective values and with regard to primary-dual deviation.Theorem 10. Let us assume that the loss functions are \"i\" (1 / \u00b5) -smooth, for i [n]. We define \"max\" = maxk \"[K] \u03c3k. Then, after T-iterations, the algorithm 1 applies, with T-iterations of max. 1\u03b3 (1 \u2212 \u0432) \u03bb\u00b5n + max.n log 1 D, that E [D (\u03b1) \u2212 D (T))] \u2264 D. In addition, after T-iterations with T-iterations without T-iterations, the max iterations without T-iteration results. \u2212 p-iterations without T-iteration. \u2212 n log."}, {"heading": "4.3. Comparison with Original COCOA", "text": "Note 12. If we opt for the averaging option \u03b3: = 1K to aggregate the updates together with \u03c3 \u2032: = 1, then the resulting algorithm 1 is identical to the COCOA analyzed (Jaggi et al., 2014). However, their results only provide convergence for smooth loss functions'i and guarantees for dual sub-optimality and not for the duality gap. Formally, the sub-problems (9) differ only in an additive constant that does not affect the local optimization algorithms used in cocoa."}, {"heading": "5. SDCA as a Local Solver", "text": "The previous section 4 has shown the convergence rates of algorithm 1, provided that one uses a local solver that provides an approximate quality, as described in assumption 1. The obvious question to be answered is which solver can provide us with a solution for this specified quality. In this section, we will show that the coordination aspect (SDCA) applied to the local sub-problem will actually provide such a solution. The LOCALSDCA solver is summarized in algorithm 2. as input, the method receives the local alpha variables as well as a common vector w (3) = w (\u03b1), which is compatible with the last state of all local alpha variables. The LOCALSDCA algorithm then produces a random sequence of iterates (h). H h = 1, on each part k: i."}, {"heading": "6. Discussion and Related Work", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "Dataset n d Sparsity", "text": "However, the known convergence rates for ADMM are weaker than the more problem-oriented methods mentioned above, and the choice of the penalty parameter is often unclear."}, {"heading": "7. Numerical Experiments", "text": "We present experiments with several large real-world datasets distributed across multiple machines running on Apache Spark. We show that COCOA + converges faster to the optimal solution in terms of total rounds and elapsed time than COCOA in all cases, despite different values: dataset, regularization values, batch size and cluster size (Section 7.2). In Section 7.3, we show that this performance translates into orders of convergence in scaling the number of machines K compared to COCOA and several other state-of-the-art methods. Finally, in Section 7.4, we examine the effects of the \u03c3 \u00b2 sub-problem parameter on the COCOA + framework."}, {"heading": "7.1. Implementation Details", "text": "We implement COCOA + and all other algorithms for comparison in Apache Spark (Zaharia et al., 2012) and execute them in the Amazon cloud using m3 large EC2 instances. We apply all methods to the binary hinge loss support vector engine. Analysis of this non-smooth loss was not captured (Jaggi et al., 2014), but was captured here and is therefore both theoretically and practically justified. A summary of the data sets used is presented in Table 2."}, {"heading": "7.2. Comparison of COCOA+ and COCOA", "text": "In Figure 1, we compare the COCOA + and COCOA frameworks directly with two sets of data (cover type and RCV1) on different values of \u03bb, the regulator. For each value of \u03bb, we consider both methods with different values of H, the number of local iterations that are performed before communication with the master. For all runs of COCOA +, we use the safe upper limit of \u03b3K for \u03c3 \u2032. Both in terms of the total number of communications performed and the elapsed time, COCOA + (shown in blue) approaches the optimal solution faster than COCOA (red). The discrepancy is greater with larger values of \u03bb, where the strongly convex regulator has more influence and the difficulty of the problem is reduced. We also see a greater performance gap with smaller values of H, where there is frequent communication between machine and master, and changes between algorithms therefore play a greater role."}, {"heading": "7.3. Scaling the Number of Machines K", "text": "In Figure 2 we show the ability of COCOA + to scale with an increasing number of machines K. The experiments confirm the ability of a strong scaling of the new method as predicted by our theory in Section 4, as opposed to the competing methods. In contrast to COCOA, which slows down linearly as the number of machines increases, the performance of COCOA + improves with additional machines and only begins to degrade slightly at K = 16 for the RCV1 dataset."}, {"heading": "7.4. Impact of the Subproblem Parameter \u03c3\u2032", "text": "Finally, in Figure 3, we consider the effects of the selection of the partial problem parameter \u03c3 on convergence. We record both the number of communications and the time on a log log log scale for the RCV1 dataset with K = 8 and H = 1e4. For \u03b3 = 1 (the most aggressive variant of COCOA + in which updates are added), we take into account several different values of \u03c3 \"ranging from \u03c3\" = 1 to \u03c3 \"= 8. The value \u03c3\" = 8 represents the safe upper limit of \u03b3K. We see that the optimal convergence occurs by \u03c3 \"= 4 and deviates for \u03c3\" \u2264 2. Remarkably, we see that the easily calculated upper limit of \u03c3 \": = \u03b3K (as given by Lemma 3) performs only slightly worse than the best possible partial problem parameter in our environment."}, {"heading": "8. Conclusion", "text": "Finally, we present a novel COCOA + framework that enables fast and communication-efficient additive aggregation into distributed algorithms for primary-dual optimization. We analyze the theoretical performance of this method and indicate strong primordial-dual convergence rates with external iterations that scale regardless of the number of machines. We expanded our theory to allow for non-smooth losses. Our experimental results show significant accelerations over previous methods, including the original COCOAframework and other modern methods."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Technical Lemmas", "text": "Lemma 16 (Lemma 21 in (Shalev-Shwartz & Zhang, 2013c)) Let \"i: R \u2192 R be a L-Lipschitz. Then, for each real value a with | a | > L, we have the\" i (a) = \u221e. Lemma 17. Assuming the loss functions \"i is limited by\" i (0) \u2264 1 for all i [n] (as we assumed in (5) above), then for the zero vector \u03b1 (0): = 0 \"Rn\" we have D (\u03b1) \u2212 D (\u03b1 (0) = D (\u03b1) - D (\u03b1) \u2212 D (0) \u2264 1. (26) Proof. For \u03b1: = 0 \"Rn\" we have w (\u03b1) = 1 \"nA\u03b1 = 0\" R d. Therefore, after definition of the dual target D (2), 0 \"D (\u03b1) \u2212 D (\u03b1) \u2264 P (\u03b1) \u2212 D (\u03b1) = 1.\""}, {"heading": "B. Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1. Proof of Lemma 3", "text": "See (Judgment \"rik & Taka\" c, \"2013b)."}, {"heading": "B.2. Proof of Lemma 4", "text": "In fact, we have to connect the terms A and B separately. We have the terms A = \u2212 1 n K [k] k = 1 (p) k = 1 (p) k (p) k (p) k = 1 (p) k (p) k (p) k (p) k (p) k (p) k (p) k (p) k (p) k (p) k (p) k (p) k (p) k) k (p) k (p) k) k (k) k (p) k (p) k (k) k (p) k (k) k (p) k (p) k) k (p) k (p) k (k) k (p) k) k (k) k k k k k k k k k (k) k k k k k k (k) k k (k) k (p) k (k) k (k) k (p) k (k) k (k) k (p) k i i p) p (p) p) k (p) k) k (p) k (k) k) k (k) k (k) k) k (p) k (k) k i p) k (k i p) k (p) k) k (k) k (k) k k) k k k k k k k k k (k) k k k k k k k) k k k k k k k k (k) k (p) k i i (k) k (k) k i p) k (p) k (k) k i p) k (p) k (k) k i p) k (k (k) k (k i p) k) k (k) k (k k k k k k k k k k k k k k k (k k k k k k k k k k k k k k k k k k) k (k k k) k k (k k k) k) k (k) k (k (k k) k k) k (k (k) k) k) k (k) k) k (k (k) k (k) k (k) k (k) k) k) k (k) k (k (k) k) k) k (k (k"}, {"heading": "B.3. Proof of Lemma 5", "text": "To get to the heart of the matter, let us write \u03b1 instead of \u03b1 (t), w instead of w (t) and u instead of u (t). Let us now estimate the expected change in the dual target using the definition of the dual target (t + 1): = \u03b1 (t) + \u03b1 (k) + \u03b1 (n) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) k (k) k k (k) k k) k k (k) k k k k (k) k) k (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) k k) (k) (k) k k (k) (k) (k) k) (k) (k) k (k) k (k) (k) k) (k) (k) k (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k (k) (k) (k) (k) (k (k) (k) (k (k) (k) (k) (k (k) (k) (k) (k) (k) (k) (k (k) (k) (k) (k) (k) (k) (k (k) (k) (k (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k (k) (k) (k (k (k (k) k) (k) (k) (k) k (k) (k (k) k) (k) k (k) k (k (k) (k (k) k) k (k) k (k)"}, {"heading": "B.4. Proof of Lemma 6", "text": "For general convex functions, the strong convexity parameter \u00b5 = 0, and therefore R (t) R (t) (16) = K \u2211 k = 1 \u0445 A (u (t) \u2212 \u03b1 (t)) [k] 2 (19) \u2264 K \u2211 k = 1 \u03c3k \u0432 (u (t) \u2212 \u03b1 (t)) [k] 2 Lemma 16 \u2264 K \u0445 k = 1 \u03c3k | Pk | 4L2."}, {"heading": "B.5. Proof of Theorem 8", "text": "(1) (1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 2) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 2) (+ 2) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 2) (+ 2) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 2) (+ 1) (+ 1) (+ 1) (+ 1) (+ 2) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (+ 1) (1) (+ 1) (1) (+ 1) (1) (+ 1) (1) (+ 1) (1) (+ 1) (1) (+ 1) (1) (1) (+ 1) (1) (1) (+ 1) (1) (1) (+ 1) (1) (1) (1) (1) (1) (1) (1"}, {"heading": "B.6. Proof of Theorem 10", "text": "If the function \"i\" (.) is (1 / \u00b5) -smooth then \"i\" (.) is \"i\" (.) is \"i\" (.) -strongly convex in relation to the \"i\" -Norm. From (16) we have \"i\" (16) -smooth then \"i\" (1 \u2212 s) \"s\" s \"u\" (t) -strongly convex \"(t) - (t) - (h) - (h) - (h) - (h) - (h) - (h) - (h) - (h) - (h) - (h) - (h) - (h) - (h) - (h) - (h) - (h - (h - h - (h - h - (h) (h) (h) (h) - (h) (h) - (h) (h) - (h) (h) (h) - (h - (h) (h) - (h) - (h - (h) (h) (h) - (h) - (h - (h) (h) - (h - (h) - (h - (h) - (h - (h) - (h - (h) - (h) (h - (h) (h - (h) - (h - (h) (h - (h) - (h) (h - (h) (h - (h) - (h - (h - (h) (h - (h) - (h) (h - (h) (h - (h) - (h - (h) - (h) - (h - (h) - (h) - (h - (h) - (h) - (h) (h - (h) - (h - (h) (h) - (h) (h) - (h - (h) (h) - (h) (h - (h) - (h) - (h) - (h - (h) (h) - (h) (h - (h) - (h) - (h - (h) - (h - (h) (h) - (h) (h) - (h - (h) - (h - (h) - (h)"}, {"heading": "B.7. Proof of Theorem 13", "text": "The proof is based on techniques developed in recent work on coordinate parentage, including (Richta \"rik & Taka\" c, \"2014; Qu\" Richta, \"2014; Tappenden et al., 2015; Marec\" max \"ek et al., 2014; Fercoq\" and Richta \"rik,\" 2013; Fercoq \"rik & Taka,\" 2014; Qu \"Richta\" rik, 2014; Qu \"and al., 2014) (Efficient accelerated variants were considered in (Fercoq\" and Richta \"rik,\" 2013; Shalev-Shwartz \"& Zhang, 2013a). First, let us define the function F (\" Richta \"rik,\" 2014; Qu \"and al., 2014) (Efficient accelerated variants were considered in (Fercoq\" and Richta \"rik,\" 2013; Shalev \"Zhang,\" 2013a)."}, {"heading": "B.8. Proof of Theorem 14", "text": "Similar to the proof of theorem 13, we define a composite function F (\u0430) = f (\u0445) + \u03a6 (\u0445). However, in this case the functions \"\u0445 i\" are not guaranteed to be strongly convex. However, the first part still has a coordinatingly continuous Lipschitz gradient with constant \u03c3. \"Therefore, we have from theorem 3 in (Tappenden et al., 2015) that we have the following: E [G\u03c3 \u2032 k (\u0432 [k], w) \u2212 G \u0445\" k \"k (\u0445 \u03b1 (h) [k], w)] \u2264 nk + h (G\u0445\" k, \"w) \u2212 G\" k \"k (0, w) + 12 \u03c3\" rmax. \"(47) Now, the choice of h = H is sufficient (25) to have the right side of (47): D\" k \"(G), W\" k, \"W\" k, \"W\" k, \"W.\""}], "references": [{"title": "Distributed Learning, Communication Complexity and Privacy", "author": ["Balcan", "Maria-Florina", "Blum", "Avrim", "Fine", "Shai", "Mansour", "Yishay"], "venue": "In COLT,", "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "Accelerated, parallel and proximal coordinate descent", "author": ["Fercoq", "Olivier", "Richt\u00e1rik", "Peter"], "venue": null, "citeRegEx": "Fercoq et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fercoq et al\\.", "year": 2013}, {"title": "Fast distributed coordinate descent for nonstrongly convex losses", "author": ["Fercoq", "Olivier", "Qu", "Zheng", "Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "IEEE Workshop on Machine Learning for Signal Processing,", "citeRegEx": "Fercoq et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fercoq et al\\.", "year": 2014}, {"title": "Consensus-Based Distributed Support", "author": ["Forero", "Pedro A", "Cano", "Alfonso", "Giannakis", "Georgios B"], "venue": "Vector Machines. JMLR,", "citeRegEx": "Forero et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Forero et al\\.", "year": 2010}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Jaggi", "Martin", "Smith", "Virginia", "Tak\u00e1\u010d", "Terhorst", "Jonathan", "Krishnan", "Sanjay", "Hofmann", "Thomas", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Distributed optimization with arbitrary local solvers", "author": ["Kone\u010dn\u00fd", "Jakub", "Ma", "Chenxin", "Richt\u00e1rik", "Peter", "Jaggi", "Martin", "Tak\u00e1\u010d"], "venue": "Technical report,", "citeRegEx": "Kone\u010dn\u00fd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kone\u010dn\u00fd et al\\.", "year": 2014}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["Liu", "Ji", "Wright", "Stephen J"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "An Asynchronous Parallel Stochastic Coordinate Descent Algorithm", "author": ["Liu", "Ji", "Wright", "Stephen J", "R\u00e9", "Christopher", "Bittorf", "Victor", "Sridhar", "Srikrishna"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "On the complexity analysis of randomized block-coordinate descent methods", "author": ["Lu", "Zhaosong", "Xiao", "Lin"], "venue": "arXiv preprint arXiv:1305.4723,", "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Efficient LargeScale Distributed Training of Conditional Maximum Entropy Models", "author": ["Mann", "Gideon", "McDonald", "Ryan", "Mohri", "Mehryar", "Silberman", "Nathan", "Walker", "Daniel D"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2009}, {"title": "Distributed block coordinate descent for minimizing partially separable functions", "author": ["Mare\u010dek", "Jakub", "Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": null, "citeRegEx": "Mare\u010dek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mare\u010dek et al\\.", "year": 2014}, {"title": "LOCO: Distributing Ridge Regression with Random Projections", "author": ["McWilliams", "Brian", "Heinze", "Christina", "Meinshausen", "Nicolai", "Krummenacher", "Gabriel", "Vanchinathan", "Hastagiri P"], "venue": "In NIPS 2014 Workshop on Distributed Machine Learning and Matrix Computations,", "citeRegEx": "McWilliams et al\\.,? \\Q2014\\E", "shortCiteRegEx": "McWilliams et al\\.", "year": 2014}, {"title": "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["Niu", "Feng", "Recht", "Benjamin", "R\u00e9", "Christopher", "Wright", "Stephen J"], "venue": "In NIPS,", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "Coordinate descent with arbitrary sampling I: Algorithms and complexity", "author": ["Qu", "Zheng", "Richt\u00e1rik", "Peter"], "venue": null, "citeRegEx": "Qu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2014}, {"title": "Randomized dual coordinate ascent with arbitrary sampling", "author": ["Qu", "Zheng", "Richt\u00e1rik", "Peter", "Zhang", "Tong"], "venue": null, "citeRegEx": "Qu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2014}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": null, "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2012}, {"title": "On optimal probabilities in stochastic coordinate descent methods", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": null, "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2013}, {"title": "Distributed coordinate descent method for learning with big data", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "arXiv preprint arXiv:1310.2059,", "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2013}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2014}, {"title": "Accelerated minibatch stochastic dual coordinate ascent", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "In NIPS, pp", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": null, "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "JMLR, 14:567\u2013599,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Distributed Stochastic Optimization and Learning", "author": ["Shamir", "Ohad", "Srebro", "Nathan"], "venue": "In Allerton,", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "Communication efficient distributed optimization using an approximate newton-type method", "author": ["Shamir", "Ohad", "Srebro", "Nathan", "Zhang", "Tong"], "venue": "In ICML,", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "Communication-Efficient Distributed Optimization using an Approximate Newton-type Method", "author": ["Shamir", "Ohad", "Srebro", "Nathan", "Zhang", "Tong"], "venue": null, "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "On the complexity of parallel coordinate descent", "author": ["Tappenden", "Rachael", "Tak\u00e1\u010d", "Martin", "Richt\u00e1rik", "Peter"], "venue": "Technical report,", "citeRegEx": "Tappenden et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tappenden et al\\.", "year": 2015}, {"title": "Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent", "author": ["Yang", "Tianbao"], "venue": "In NIPS,", "citeRegEx": "Yang and Tianbao.,? \\Q2013\\E", "shortCiteRegEx": "Yang and Tianbao.", "year": 2013}, {"title": "On Theoretical Analysis of Distributed Stochastic Dual Coordinate Ascent", "author": ["Yang", "Tianbao", "Zhu", "Shenghuo", "Jin", "Rong", "Lin", "Yuanqing"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Large Linear Classification When Data Cannot Fit in Memory", "author": ["Yu", "Hsiang-Fu", "Hsieh", "Cho-Jui", "Chang", "Kai-Wei", "Lin", "Chih-Jen"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing", "author": ["Zaharia", "Matei", "Chowdhury", "Mosharaf", "Das", "Tathagata", "Dave", "Ankur", "McCauley", "Murphy", "Franklin", "Michael J", "Shenker", "Scott", "Stoica", "Ion"], "venue": null, "citeRegEx": "Zaharia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2012}, {"title": "Communication-efficient distributed optimization of self-concordant empirical loss", "author": ["Zhang", "Yuchen", "Xiao", "Lin"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Communication-Efficient Algorithms for Statistical Optimization", "author": ["Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Parallelized Stochastic Gradient Descent", "author": ["Zinkevich", "Martin A", "Weimer", "Markus", "Smola", "Alex J", "Li", "Lihong"], "venue": "NIPS 2010: Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}, {"title": "Proof of Lemma 3 See (Richt\u00e1rik & Tak\u00e1\u010d, 2013b). B.2", "author": ["B. Proofs B"], "venue": "Proof of Lemma", "citeRegEx": "B.1.,? \\Q2013\\E", "shortCiteRegEx": "B.1.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "For this class of problems, the recently proposed COCOA approach (Yang, 2013; Jaggi et al., 2014; Kone\u010dn\u00fd et al., 2014) has formulated a communication efficient primaldual coordinate ascent scheme which targets the communication bottleneck, by allowing more computation to be performed on data local subproblems on each machine before a single vector is communicated.", "startOffset": 65, "endOffset": 119}, {"referenceID": 5, "context": "For this class of problems, the recently proposed COCOA approach (Yang, 2013; Jaggi et al., 2014; Kone\u010dn\u00fd et al., 2014) has formulated a communication efficient primaldual coordinate ascent scheme which targets the communication bottleneck, by allowing more computation to be performed on data local subproblems on each machine before a single vector is communicated.", "startOffset": 65, "endOffset": 119}, {"referenceID": 4, "context": "While the existing analysis for COCOA in (Jaggi et al., 2014) only covered smooth loss functions, here we extend the class of functions where the rates apply (to include e.", "startOffset": 41, "endOffset": 61}, {"referenceID": 0, "context": "For an overview over the currently active research field, we refer the reader to (Balcan et al., 2012; Richt\u00e1rik & Tak\u00e1\u010d, 2013b; Duchi et al., 2013; Yang, 2013; ?; Liu & Wright, 2014; Fercoq & Richt\u00e1rik, 2013; Fercoq et al., 2014; Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al., 2014a; Zhang & Xiao, 2015; Qu & Richt\u00e1rik, 2014) and the references therein.", "startOffset": 81, "endOffset": 337}, {"referenceID": 2, "context": "For an overview over the currently active research field, we refer the reader to (Balcan et al., 2012; Richt\u00e1rik & Tak\u00e1\u010d, 2013b; Duchi et al., 2013; Yang, 2013; ?; Liu & Wright, 2014; Fercoq & Richt\u00e1rik, 2013; Fercoq et al., 2014; Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al., 2014a; Zhang & Xiao, 2015; Qu & Richt\u00e1rik, 2014) and the references therein.", "startOffset": 81, "endOffset": 337}, {"referenceID": 4, "context": "For an overview over the currently active research field, we refer the reader to (Balcan et al., 2012; Richt\u00e1rik & Tak\u00e1\u010d, 2013b; Duchi et al., 2013; Yang, 2013; ?; Liu & Wright, 2014; Fercoq & Richt\u00e1rik, 2013; Fercoq et al., 2014; Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al., 2014a; Zhang & Xiao, 2015; Qu & Richt\u00e1rik, 2014) and the references therein.", "startOffset": 81, "endOffset": 337}, {"referenceID": 4, "context": "The crucial difference compared to the existing COCOA algorithm (Jaggi et al., 2014) is the more general subproblem, as defined in (9), as well as the aggregation parameter \u03b3.", "startOffset": 64, "endOffset": 84}, {"referenceID": 4, "context": "The analysis in (Jaggi et al., 2014) only covered the case of smooth loss functions.", "startOffset": 16, "endOffset": 36}, {"referenceID": 4, "context": "If we choose the averaging option \u03b3 := 1 K for aggregating the updates, together with \u03c3\u2032 := 1, then the resulting Algorithm 1 is identical with COCOA analyzed in (Jaggi et al., 2014).", "startOffset": 162, "endOffset": 182}, {"referenceID": 12, "context": "Several distributed variants of SGD have been proposed (Niu et al., 2011; Liu et al., 2014; Duchi et al., 2013).", "startOffset": 55, "endOffset": 111}, {"referenceID": 6, "context": "Several distributed variants of SGD have been proposed (Niu et al., 2011; Liu et al., 2014; Duchi et al., 2013).", "startOffset": 55, "endOffset": 111}, {"referenceID": 31, "context": "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).", "startOffset": 130, "endOffset": 238}, {"referenceID": 32, "context": "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).", "startOffset": 130, "endOffset": 238}, {"referenceID": 9, "context": "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).", "startOffset": 130, "endOffset": 238}, {"referenceID": 31, "context": "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).", "startOffset": 130, "endOffset": 238}, {"referenceID": 11, "context": "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).", "startOffset": 130, "endOffset": 238}, {"referenceID": 0, "context": "(Balcan et al., 2012) shows additional relevant lower bounds on the minimum number of communication rounds necessary for a given approximation quality for similar machine learning problems.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "The COCOA framework (Jaggi et al., 2014) allows using local solvers of weak local approximation quality in each round, while still giving a convergence rate for smooth losses.", "startOffset": 20, "endOffset": 40}, {"referenceID": 28, "context": "By making use of the primal-dual structure in the line of work of (Yu et al., 2012; Shalev-Shwartz & Zhang, 2013c; Yang, 2013; Yang et al., 2013; Jaggi et al., 2014), the COCOA framework as well as the extension here allow to have more control over the meaningful aggregation of updates between different machines.", "startOffset": 66, "endOffset": 165}, {"referenceID": 27, "context": "By making use of the primal-dual structure in the line of work of (Yu et al., 2012; Shalev-Shwartz & Zhang, 2013c; Yang, 2013; Yang et al., 2013; Jaggi et al., 2014), the COCOA framework as well as the extension here allow to have more control over the meaningful aggregation of updates between different machines.", "startOffset": 66, "endOffset": 165}, {"referenceID": 4, "context": "By making use of the primal-dual structure in the line of work of (Yu et al., 2012; Shalev-Shwartz & Zhang, 2013c; Yang, 2013; Yang et al., 2013; Jaggi et al., 2014), the COCOA framework as well as the extension here allow to have more control over the meaningful aggregation of updates between different machines.", "startOffset": 66, "endOffset": 165}, {"referenceID": 13, "context": "Mini-batch versions of both SGD and coordinate descent (CD) (Richt\u00e1rik & Tak\u00e1\u010d, 2013b;a; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu & Richt\u00e1rik, 2014; Qu et al., 2014) suffer from their convergence rate degrading towards the rate of batch gradient descent, as the size of the mini-batch is increased.", "startOffset": 60, "endOffset": 170}, {"referenceID": 4, "context": "This follows because the updates from all datapoints in the minibatch are based on the previous parameter vector w, contrasting methods that allow local updates such as COCOA or local-SGD (Jaggi et al., 2014).", "startOffset": 188, "endOffset": 208}, {"referenceID": 3, "context": "in (Forero et al., 2010).", "startOffset": 3, "endOffset": 24}, {"referenceID": 29, "context": "Implementation Details We implement COCOA+ and all other algorithms for comparison in Apache Spark (Zaharia et al., 2012), and run them in the Amazon cloud, using m3.", "startOffset": 99, "endOffset": 121}, {"referenceID": 4, "context": "The analysis for this non-smooth loss was not covered in (Jaggi et al., 2014) but has been captured here, and thus is both theoretically and practically justified.", "startOffset": 57, "endOffset": 77}], "year": 2015, "abstractText": "Distributed optimization algorithms for largescale machine learning suffer from a communication bottleneck. Reducing communication makes the efficient aggregation of partial work from different machines more challenging. In this paper we present a novel generalization of the recent communication efficient primal-dual coordinate ascent framework (COCOA). Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allowed conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to also cover non-smooth convex loss functions. We provide an extensive experimental comparison on several real-world distributed datasets, showing markedly improved performance, especially when scaling up the number of machines.", "creator": "LaTeX with hyperref package"}}}