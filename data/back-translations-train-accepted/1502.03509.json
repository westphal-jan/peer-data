{"id": "1502.03509", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "MADE: Masked Autoencoder for Distribution Estimation", "abstract": "There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.", "histories": [["v1", "Thu, 12 Feb 2015 02:06:07 GMT  (104kb,D)", "https://arxiv.org/abs/1502.03509v1", "9 pages and 1 page of supplementary material"], ["v2", "Fri, 5 Jun 2015 14:37:32 GMT  (112kb,D)", "http://arxiv.org/abs/1502.03509v2", "9 pages and 1 page of supplementary material. Updated to match published version"]], "COMMENTS": "9 pages and 1 page of supplementary material", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["mathieu germain", "karol gregor", "iain murray", "hugo larochelle"], "accepted": true, "id": "1502.03509"}, "pdf": {"name": "1502.03509.pdf", "metadata": {"source": "META", "title": "MADE: Masked Autoencoder for Distribution Estimation", "authors": ["Mathieu Germain", "Karol Gregor", "Iain Murray"], "emails": ["MATHIEU.GERMAIN2@USHERBROOKE.CA", "KAROL.GREGOR@GMAIL.COM", "I.MURRAY@ED.AC.UK", "HUGO.LAROCHELLE@USHERBROOKE.CA"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they had in the past, and they will be able to play by the rules they have set in the past, \"he said in an interview with the New York Times."}, {"heading": "2. Autoencoders", "text": "In this paper, we assume that we will get a number of examples relating to the statistical structure of the distributions they have generated."}, {"heading": "3. Distribution Estimation as Autoregression", "text": "An interesting question is what property we could impose on the autoencoder so that its output can be used to obtain valid probabilities. Specifically, we want to write p (x) so that it can be calculated based on the output of a correctly corrected autoencoder. First, we can take advantage of the fact that the probability product rule for each distribution implies that we can always break it down into the product of its nested condition sp (x) = D value d = 1p (xd | x < d), (4) where x < d = [x1,., xd \u2212 1] >. By defining p (xd = 1 | x < d) = x value, and thus p (xd = 0 | x < d) = 1 \u2212 x value (xltd), the loss of Equation 3 becomes a valid negative log probability: \u2212 x & lt."}, {"heading": "4. Masked Autoencoders", "text": "The question is how to modify the masks for W and V accordingly to satisfy the autoregressive property of the MW."}, {"heading": "4.1. Deep MADE", "text": "One advantage of the masked autoencoder framework described in the previous section is that it generalizes the hidden layer in a natural way. In fact, by assigning a maximum number of connected inputs to all units in the deep network, we are using superscripts to index the layers; the first hidden layer matrix (previously W) is called W1, the second hidden layer matrix is called W2, and so on. The number of hidden units (previously K) in each hidden layer is indexed similarly to Kl, where l is the hidden layer index. We will also generalize the notation for the maximum number of connected inputs of the kest unit in the oldest layer to ml (k). We have already discussed how to define the mask matrix of the first layer to ensure that the kest layer is connected to m (k) (now m1)."}, {"heading": "4.2. Order-agnostic training", "text": "So far, we have assumed that the conditions modelled by MADE are consistent with the natural arrangement of the dimensions of x. However, we may be interested in modelling the conditions associated with an arbitrary arrangement of the input dimensions. In particular, Uria et al. (2014) have shown that training an auto-regressive model can be beneficial for all arrangements. We call this approach order agnostic training. It can be achieved by ordering before each stochastic / minibatch gradient update of the model. There are two advantages of this approach. Firstly, missing values can be efficiently implied in partially observed input vectors: We invoke an order where the observed dimensions all lie in front of the unobserved ones, making conclusions straightforward. Secondly, an interplay of authoregressive models can be constructed by arranging x by taking advantage of the fact that the conditions for two different arrangements are not exactly consistent."}, {"heading": "4.3. Connectivity-agnostic training", "text": "One advantage of command diagnostics is that it effectively allows us to train as many models as there are commands that use a common set of parameters. We can take advantage of this by creating ensembles of models at test points. To achieve this, we also need to select the values of ml (k) for all units and layers once and for all before training, which we actually resample for each training example or minibatch. This is still handy, as the operation of creating the masks is simple. Denoting ml = [ml (1),., ml (Kl)], and adopting an elementary and parallel implementation of operation 1a b for vectors."}, {"heading": "5. Related Work", "text": "This year, it is more than ever before in the history of the country."}, {"heading": "6. Experiments", "text": "To test the performance of our model, we considered two different benchmarks: a set of UCI binary datasets and the binary MNIST dataset. The code for reproducing the experiments of this paper is available at https: / / github.com / mgermain / MADE / releases / tag / ICML2015. The results given here are the average negative log probability for the test set of each dataset. All experiments were conducted with stochastic gradient drop (SGD) with size 100 mini-batches and an early stop outlook of 30."}, {"heading": "6.1. UCI evaluation suite", "text": "This is a collection of 7 relatively small data sets from the University of California, Irvine machine learning repository and the OCR-letters dataset from the Stanford AI Lab. Table 2 gives an overview of the size of these data sets and the way in which they were distributed. Experiments were conducted with networks of 500 units per hidden layer, using the adadelta learning update (Zeiler, 2012) with a decay of 0.95. Other hyperparameters were varied as stated in Table 3. We note as # of the masks the number of different masks through which MADE cycles were performed during the training. In no case are masks sampled on the fly and never explicitly re-used unless they are re-sampled by chance. In this situation, 300 and 1000 sampled masks are used during validation and testing time to mean the probabilities.The results are listed in Table 4, in which half of the MADS models are not rated best."}, {"heading": "6.2. Binarized MNIST evaluation", "text": "The version of MNIST we used to be generated by Salakhutdinov & Murray (2008). MNIST is a set of 70,000 handwritten digits of 28 x 28 pixels. We use the same split as in Larochelle & Murray (2011), consisting of much more efficient than the validation and 10,000 for the test. The experiments were updated with the adagradar learning plane (Duchi et al., 2010), with an epsilon of 10 \u2212 6, we are considering the hidden layers from 500 to 8000 units. Seeing that the increase in the number of units tends to help, we used 8000. Also, our GPU implementation was fairly efficient."}, {"heading": "7. Conclusion", "text": "We proposed MADE, a simple modification of autoencoders that allows them to be used as distribution estimators. MADE shows that it is possible to obtain direct, cost-effective estimates of high-dimensional joint probabilities from a single pass through an autoencoder. Like standard autoencoders, our enhancement is easily vectorizable and can be implemented on GPUs. MADE can evaluate high-dimensional probable distributions with better scaling than before, while maintaining the most advanced statistical performance."}, {"heading": "Acknowledgments", "text": "We thank Marc-Alexandre Co-te \u0301 for his help in implementing NADE in Theano and the entire Theano team (Bastien et al., 2012; Bergstra et al., 2010), as well as NSERC, Calcul Que \u0301 bec and Compute Canada."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Modeling highdimensional discrete data with multi-layer neural networks", "author": ["Bengio", "Yoshua", "Samy"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1999}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Laufer", "Eric", "Alain", "Guillaume", "Yosinski", "Jason"], "venue": "In Proceedings of the 31th Annual International Conference on Machine Learning (ICML", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "NICE: non-linear independent components estimation", "author": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Technical report,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning representations by maximizing compression", "author": ["Gregor", "Karol", "LeCun", "Yann"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2011}, {"title": "Deep AutoRegressive Networks", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Mnih", "Andriy", "Blundell", "Charles", "Wierstra", "Daan"], "venue": "In Proceedings of the 31th Annual International Conference on Machine Learning (ICML", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the 2nd International Conference on Learning Representations (ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "The neural autoregressive distribution estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS 2011),", "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Sum-product networks: A new deep architecture", "author": ["Poon", "Hoifung", "Domingos", "Pedro"], "venue": "In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "Poon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2011}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In Proceedings of the 25th Annual International Conference on Machine Learning (ICML", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Generative versus discriminative training of RBMs for classification of fMRI images", "author": ["Schmah", "Tanya", "Hinton", "Geoffrey E", "Zemel", "Richard S", "Small", "Steven L", "Strother", "Stephen C"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Schmah et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Schmah et al\\.", "year": 2008}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "RNADE: The real-valued neural autoregressive densityestimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "A deep and tractable density estimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew D", "Zhang", "Sixin", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "ADADELTA: an adaptive learning rate", "author": ["Zeiler", "Matthew D"], "venue": null, "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": ", 2009), denoising or missing input imputation (Poon & Domingos, 2011; Dinh et al., 2014), data (e.", "startOffset": 47, "endOffset": 89}, {"referenceID": 15, "context": "Specifically, learning algorithms for a variety of neural network models have been proposed (Bengio & Bengio, 2000; Larochelle & Murray, 2011; Gregor & LeCun, 2011; Uria et al., 2013; 2014; Kingma & Welling, 2014; Rezende et al., 2014; Bengio et al., 2014; Gregor et al., 2014; Goodfellow et al., 2014; Dinh et al., 2014).", "startOffset": 92, "endOffset": 321}, {"referenceID": 11, "context": "Specifically, learning algorithms for a variety of neural network models have been proposed (Bengio & Bengio, 2000; Larochelle & Murray, 2011; Gregor & LeCun, 2011; Uria et al., 2013; 2014; Kingma & Welling, 2014; Rezende et al., 2014; Bengio et al., 2014; Gregor et al., 2014; Goodfellow et al., 2014; Dinh et al., 2014).", "startOffset": 92, "endOffset": 321}, {"referenceID": 2, "context": "Specifically, learning algorithms for a variety of neural network models have been proposed (Bengio & Bengio, 2000; Larochelle & Murray, 2011; Gregor & LeCun, 2011; Uria et al., 2013; 2014; Kingma & Welling, 2014; Rezende et al., 2014; Bengio et al., 2014; Gregor et al., 2014; Goodfellow et al., 2014; Dinh et al., 2014).", "startOffset": 92, "endOffset": 321}, {"referenceID": 7, "context": "Specifically, learning algorithms for a variety of neural network models have been proposed (Bengio & Bengio, 2000; Larochelle & Murray, 2011; Gregor & LeCun, 2011; Uria et al., 2013; 2014; Kingma & Welling, 2014; Rezende et al., 2014; Bengio et al., 2014; Gregor et al., 2014; Goodfellow et al., 2014; Dinh et al., 2014).", "startOffset": 92, "endOffset": 321}, {"referenceID": 5, "context": "Specifically, learning algorithms for a variety of neural network models have been proposed (Bengio & Bengio, 2000; Larochelle & Murray, 2011; Gregor & LeCun, 2011; Uria et al., 2013; 2014; Kingma & Welling, 2014; Rezende et al., 2014; Bengio et al., 2014; Gregor et al., 2014; Goodfellow et al., 2014; Dinh et al., 2014).", "startOffset": 92, "endOffset": 321}, {"referenceID": 3, "context": "Specifically, learning algorithms for a variety of neural network models have been proposed (Bengio & Bengio, 2000; Larochelle & Murray, 2011; Gregor & LeCun, 2011; Uria et al., 2013; 2014; Kingma & Welling, 2014; Rezende et al., 2014; Bengio et al., 2014; Gregor et al., 2014; Goodfellow et al., 2014; Dinh et al., 2014).", "startOffset": 92, "endOffset": 321}, {"referenceID": 15, "context": "Specifically, Uria et al. (2014) have shown that training an autoregressive model on all orderings can be beneficial.", "startOffset": 14, "endOffset": 33}, {"referenceID": 15, "context": "In a similar situation, Uria et al. (2014) informed each hidden unit which units were providing input with binary indicator variables, connected with additional learnable weights.", "startOffset": 24, "endOffset": 43}, {"referenceID": 16, "context": "Recently, a deep extension of NADE was proposed, improving even further the state-of-the-art in distribution estimation (Uria et al., 2014).", "startOffset": 120, "endOffset": 139}, {"referenceID": 14, "context": "An interesting interpretation of the autoregressive mask sampling is as a structured form of dropout regularization (Srivastava et al., 2014).", "startOffset": 116, "endOffset": 141}, {"referenceID": 17, "context": "Specifically, it bears similarity with the masking in dropconnect networks (Wan et al., 2013).", "startOffset": 75, "endOffset": 93}, {"referenceID": 14, "context": "An interesting interpretation of the autoregressive mask sampling is as a structured form of dropout regularization (Srivastava et al., 2014). Specifically, it bears similarity with the masking in dropconnect networks (Wan et al., 2013). The exception is that the masks generated here must guaranty the autoregressive property of the autoencoder, while in Wan et al. (2013), each element in the mask is generated independently.", "startOffset": 117, "endOffset": 374}, {"referenceID": 4, "context": "Experiments were run using the adagrad learning update (Duchi et al., 2010), with an epsilon of 10\u22126.", "startOffset": 55, "endOffset": 75}, {"referenceID": 0, "context": "We thank Marc-Alexandre C\u00f4t\u00e9 for helping to implement NADE in Theano and the whole Theano (Bastien et al., 2012; Bergstra et al., 2010) team of contributors.", "startOffset": 90, "endOffset": 135}], "year": 2015, "abstractText": "There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder\u2019s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with stateof-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.", "creator": "LaTeX with hyperref package"}}}