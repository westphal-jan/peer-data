{"id": "1502.03529", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Scalable Stochastic Alternating Direction Method of Multipliers", "abstract": "Stochastic alternating direction method of multipliers (ADMM), which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM. However, most stochastic methods can only achieve a convergence rate $O(1/\\sqrt T)$ on general convex problems,where T is the number of iterations. Hence, these methods are not scalable with respect to convergence rate (computation cost). There exists only one stochastic method, called SA-ADMM, which can achieve convergence rate $O(1/T )$ on general convex problems. However, an extra memory is needed for SA-ADMM to store the historic gradients on all samples, and thus it is not scalable with respect to storage cost. In this paper, we propose a novel method, called scalable stochastic ADMM(SCAS-ADMM), for large-scale optimization and learning problems. Without the need to store the historic gradients, SCAS-ADMM can achieve the same convergence rate $O(1/T )$ as the best stochastic method SA-ADMM and batch ADMM on general convex problems. Experiments on graph-guided fused lasso show that SCAS-ADMM can achieve state-of-the-art performance in real applications", "histories": [["v1", "Thu, 12 Feb 2015 04:01:46 GMT  (823kb,D)", "https://arxiv.org/abs/1502.03529v1", null], ["v2", "Sun, 1 Mar 2015 13:15:14 GMT  (825kb,D)", "http://arxiv.org/abs/1502.03529v2", null], ["v3", "Mon, 20 Jul 2015 10:01:27 GMT  (829kb,D)", "http://arxiv.org/abs/1502.03529v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shen-yi zhao", "wu-jun li", "zhi-hua zhou"], "accepted": true, "id": "1502.03529"}, "pdf": {"name": "1502.03529.pdf", "metadata": {"source": "CRF", "title": "Scalable Stochastic Alternating Direction Method of Multipliers", "authors": ["Shen-Yi Zhao", "Wu-Jun Li", "Zhi-Hua Zhou"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The method of multipliers (ADMM) [1] is proposed to solve the problems that can be formulated as follows: \"There are only two ways.\" (\"There is only one way.\") \"There are only two ways.\" (\"There is only one way.\") \"There are two ways.\" (\"There is only one way.\") \"(\" There is only one way. \")\" There is only one way. \"(\" There is only one way. \")\" There are two ways. \"(\" There is only one way. \")\" (\"There is only one way.\" (\"There is only one way.\") \"(\" There is \")\" (\"There is\" There is \").\" (\"There is\" There is. \"\" (\"There is.\") \"(\" There is. \")\" (\"There is.\" (\"There is.\") \"(\" There is. \"(\" There is. \")\" (\"There is.\") \"(\" There is. \"(\" There is. \")\" (\"There is.\") \"(\" There is. \"(\" There is. \")\" (\"There is.\") \"(\" There is. \"(\" There is. \")\" (\"There is.\")"}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Convex and Smooth Functions", "text": "A function h (\u00b7) is referred to as \"hLipschitz continuous,\" when: \"h\" > 0, \"\" a, \"\" h \"(b) \u2212 h\" (a), \"h\" (b), \"h\" (b), \"h\" (b), \"b\" (b), \"a.\" Suppose h \"(\u00b7) is differentiable, and let\" h \"(a) be the gradient of h (\u00b7) at a. A function h\" (\u00b7) is referred to as a convex when: \"a,\" b, \"h\" (b), h \"(b), h\" (a), h \"(b \u2212 a). Let's say h\" is convex \"and differentiable.\" h \"(\u00b7) is referred to as\" h \"smooth,\" when: \"a,\" b, \"h\" (b), \"b\" b \"b\", \"b\" b \"(b)."}, {"heading": "2.2 ADMM", "text": "ADMM solves (1) on the basis of the extended Lagrangian function: L (x, y, \u03b2) = f (x) + g (y) + \u03b2T (Ax + By \u2212 c) + \u03c1 2 * Ax + By \u2212 c (2), where \u03b2 is a vector of Lagrangian multipliers, and \u03c1 > 0 is a penalty parameter. Just like the Gauss-Seidel method, ADMM iteratively updates the variables in an alternating manner as follows [1]: xt + 1 = arg min x L (x, yt, \u03b2t), (3) yt + 1 = arg min y L (xt + 1, y, \u03b2t), (4) \u03b2 + 1 + (ax + 1 + byt + 1 \u2212 c), (5), where xt, yt and \u00df denote the values of x, y and \u03b2 at the tth iteration."}, {"heading": "3 Scalable Stochastic ADMM", "text": "In this section, we present the details of our SCAS-ADMM, which is scalable in terms of both convergence rate and storage costs. Similar to most existing stochastic ADMM methods, which adjust the stochastic gradient decrease (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic Variance Reduced Gradient (SVRG) [15]. However, unlike SVRG, our SCAS-ADMM can be used to model more complex problems with equality constraints. In this paper, we assume that f (\u00b7) and all {fi (\u00b7)} vf -smooth. For g (\u00b7), we only assume that it is convex, but not necessarily smooth or slippery continuous. This is a reasonable assumption for many machine learning problems, such as the loss of a square or laser."}, {"heading": "3.1 General Convex Problems", "text": "In general convex problems, f (\u00b7) vf -smooth and generally convex, but not necessarily strongly convex."}, {"heading": "3.1.1 Algorithm", "text": "As in existing stochastic ADMM methods (6, 4), the update rules for y and \u03b2 are still the same as in (4) and (5). We just need to design a new strategy for updating x. The algorithm for our SCAS ADMM is briefly presented in Algorithm 1. It changes (7) to be: xt + 1 = 1 m = 0 wm Mt, (8) where Mt is a parameter indicating the number of iterations in the inner loop, andw0 = xt, wm + 1 = 2 x X (wm).T (wm) \u2212 fim (w0) \u2212 fim (w0) + AT\u03b2t + byt \u2212 c)])), (9) with in being an index randomly sampled by {1, \u00b7 2, \u00b7 f (xt)."}, {"heading": "3.1.2 Convergence Analysis", "text": "Suppose we have (xt, yt, \u03b2t), and we define: L (x) = L (x, yt, \u03b2t). (10) We can get the following convergence theory: Theorem 1. Suppose the optimal solution of (2) is (x), X is limited by D and contains x (x), and all functions {fi (x)} are generally convex and f -smooth, and the function g (y) is convex. We have the following convergence result for the algorithm 1: E [f (x) T) + g (y) T) \u2212 f (f) is converted, and all functions {fi (x) are convervex and f -smooth, and the function g (y) is converted."}, {"heading": "3.2 Strongly Convex Problems", "text": "In algorithm 1, with the increase of t, the iteration number of the inner loop Mt must be increased and the increment \u03b7t reduced. This can lead to large calculations when T becomes large. We can get a better algorithm if f (x) in (1) is strongly convex."}, {"heading": "3.2.1 Algorithm", "text": "If f (x) is strongly convex, our SCAS ADMM is briefly represented in algorithm 2. we can find that algorithm 2 is similar to algorithm 1, but with constant values for Mt and \u03b7t.Algorithm 2 SCAS ADMM for strongly convex problematicInitializes: (x0, y0, \u03b20), r = 2\u03b7 1 \u2212 \u03bdL\u03b72, s = \u03bdL\u03b72, a convex quantity X; for t = 0 to T \u2212 1 doCompute zt = f (xt) = 1n \u2211 n i = 1 x fi (xt); w0 = xt; s = 0; for m = p 0 to M \u2212 1 doRandomly select an in from {1, 2, \u00b7 \u00b7 \u00b7 \u00b7, n}; wm + 1 = \u03c0X (wm \u2212 \u03b7) \u2212 fim (w0) \u2212 t + t (Awm + p + byt \u2212 c); wm + ym + p (wm) + p (p = 0 to M \u2212 1 doomly select an in from p 1, 2, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, n; wm + 1 t = 1, x = 1, x (wm) x (p, p x, p = 1, p, p = 1, p, x, p = 1, p, p, x (1, p) x (p, x, x, p x, p x, p x, p x, p x, m m) x x, p x, p x, p x x, p x, p x, p x, p x x, x x, p m (p, p x x x x x x x x, p, p, p m), p, x x x x x x x x, p, p, x x x x x x x x x, p, p, p, p, x x x x x x, p, x x x x, p, x x x x x, p, p, x x x x, p, x, x x x x, x x x, p, x, x x x, x x, x, x x, x, x, x x, x, x, x x, x, x, x, x, x, x"}, {"heading": "3.2.2 Convergence Analysis", "text": "Theorem 2. Suppose the optimal solution of (2) is (x *, y *, \u03b2 *), all functions {fi (x)} are generally convex and receive -smooth, f (x) is strongly convex and receive -smooth and g (y) is convex. We have the following result: E [f (x * T) + g (y * T) \u2212 f (x *) \u2212 g (y *) + \u03b3 * Ax * T + Through T \u2212 c *] \u2264 \u00b5f 4T * x0 \u2212 x * 2 + \u03c1 2T * y0 \u2212 y * 2H + 1 \u03c1T (\u03b2\u03b2\u03b20 * 2 + \u03b32), (12) where H = BTB and \u03b3 > 0 is a constant. In this case we can set M and p as constants. Please note that in the proof of Theorem 2 \u2212 M and \u03b7 the following conditions must be fulfilled:"}, {"heading": "3.3 Comparison to Related Methods", "text": "We compare our SCAS-ADMM methods with other stochastic ADMM methods for three key factors: linearization of the penalty term, convergence rate for general convex problems, and storage costs. Matrix inversion (1\u03b7t I + \u03c1A TA) -1 can be avoided by linearizing the penalty term \u03c12 and Ax + By \u2212 c \u00b2 2 [4]. The comparison results are summarized in Table 1, where SA-IU-ADMM is a variant of SAADMM with penalty term linearization. Please note that A-Rl \u00b7 p, B-Rl \u00b7 q, x-Rp, y-Rq, c-Rl, p are the number of parameters to learn and n the number of samples to train.It is easy to see that only SCAS-ADMM can achieve the best performance in both convergence rate and storage costs."}, {"heading": "4 Experiments", "text": "As in [6, 7, 4], we evaluate our method on the basis of the generalized lasso model [16], which can be formulated as follows: min x1n \u2211 i = 1 fi (x) + \u03bb \u0445 Ax \u0442 1, (13), where fi (x) is the logistical loss, A is a matrix for specifying the desired structured thrift pattern for x, and \u03bb is the regularization hyperparameter. We can obtain different models such as melting glaze and shaft smoothing by specifying different A. In this paper, we focus on the graph-guided melting glaze [9], which is also used in [4]. As in [6, 4], we use the method of inverse coefficient selection [17] to obtain a graph matrix (thrift pattern) G, on which we can obtain A = [G; I]. In general, we can formulate both G \u2212 and A with the ADMM frame [1]: x (yg) (x), y (x) (1)."}, {"heading": "4.1 Baselines and Datasets", "text": "They are: \u2022 Batch ADMM [1]: The deterministic (batch) variant of ADMM that (7) uses to update x directly by visiting all training samples in each iteration. \u2022 STOC-ADMM [6]: The stochastic ADMM variant without using historical gradients for optimization, which has a convergence rate of O (1 / \u221a T) for general convex problems and O (log T / T) for strongly convex problems. \u2022 SA-ADMM [4]: The stochastic ADMM variant without using historical gradients to approximate the full gradient, which has a convergence rate of O (1 / T) for general convex problems. Please note that other methods, such as OPG-ADMM, RDA-ADMM and OSADMM, are not accepted for comparison."}, {"heading": "4.2 Convergence Results", "text": "As in [4], we examine the variation in the objective value on the school bench and the test loss versus the number of effective passes over the data. For all methods, an effective pass over the data means that n samples are visited. Specifically, an effective pass refers to an iteration in batch ADMM. For stochastic ADMM methods that visit a sample in each iteration, an effective pass will refer to n iterations. For SCAS-ADMM, we set Mt = n and each iteration of the outer loop must visit 2n samples. Therefore, each iteration of the outer loop contributes two effective passes. Although different methods visit in each iteration, we can see that the number of effective passes over the data is a good metric for a fair comparison."}, {"heading": "5 Conclusion", "text": "In this thesis, we have proposed a new stochastic ADMM method called SCASADMM, which can achieve the same convergence rate as the best existing stochastic ADMM method SA-ADMM for common convex problems. Furthermore, it costs much less memory than SA-ADMM. Therefore, SCAS-ADMM is scalable in terms of both convergence rate and storage costs."}, {"heading": "A Notations for Proof", "text": "We letvm, t = fim (wm) \u2212 fim (w0) + zt, (15) bm, t = \u03b2t + \u03c1 (Awm + Byt \u2212 c), (16) pm, t = vm, t + A Tbm, t. (17) Then the update rule for wm + 1 can be rewritten in the inner loop of algorithm 1 aswm + 1 = \u03c0X (wm \u2212 \u03b7t (vm, t + ATbm, t)) = \u03c0X (wm \u2212 \u03b7tpm, t). (18) Suppose we have (xt, yt, \u03b2t), and we define: L (x) = L (x, yt, \u03b2t), (19) Li (x) = fi (x) + g (yt) + \u03b2Tt (Ax + Byt \u2212 c) + \u03c12% Ax \u2212 c (20)."}, {"heading": "B Lemmas for the Proof of Theorem 1", "text": "Lemma 1: If f (x) is, we can state that L (x), that we only f (a), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (b), f (f), f (b), f (f), f (b), f (b), f (b), f (b), f (f), f (f), f (b), f (b), f (f), f (f), f (f), f (b), f (f), f (f), f (f), f (b), f (f (b), f (f), f (f), f (f), f (b), f (f), f (b), f (f (b), f (f), f (b), f (f (b), f (f), f (b), f (b), f (f (f), f (b), f (f (f), f (b), f (b), f (f (b), f (f (f), f (b), f (f (f), f (b), f (f (f), f (b), f (f (b), f (f (f), f (b), f (f (b), f (f (b), f (f (f), f (f), f (b), f (f, f (f), f (b), f (f (b), f (f (f), f (f), f, f (b), f (f, f), f (f, f (b), f, f (b), f (f), f (b), f (f, f, f), f (f, f, f"}, {"heading": "C Proof of Theorem 1", "text": "The proof: Let us take u = xy \u03b1, ut = xtyt\u03b1t, u \u00b2 T = 1 T = 1 T = 1 T = 1 T, and F (u) = AT\u03b1BT\u03b1 \u2212 (Ax + By \u2212 c). If we add up the equations in (22), (26) and (27), we have: E [P (xt + 1, yt + 1) \u2212 P (x, y) + F (ut + 1 \u2212 u)] (28) \u2264 D 22Mt\u03b7t (\u03bd2 LD 2 + G2t) + 2 E [\u0432 yt \u2212 y + 2 H \u2212 yt \u2212 yt \u2212 yt \u2212 yt \u2212 yt \u2212 2 H] (28) \u2264 D 22Mt\u03b7t (\u03bd2 LD 2 + G2t) + T (\u03b1 2 + \u03b1 T) + 2 E [\u0441\u04422 E [\u0441yt \u2212 T]."}, {"heading": "D Lemmas for the Proof of Theorem 2", "text": "Lemma (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm), wm (wm, wm (wm), wm (wm), wm (wm), wm (wm, wm (wm), wm (wm), wm (wm), wm (wm (wm), wm (wm), wm (wm), wm (wm (wm), wm (wm), wm (wm (wm), wm (wm), wm (wm (wm), wm (wm), wm (wm), wm (wm), wm (wm (wm), wm (wm), wm (wm (wm), wm (wm), wm (wm), wm (wm (wm), wm (wm), wm (wm (wm), wm (wm), wm (wm), wm (wm, wm (wm), wm (wm), wm (wm (wm), wm (wm), wm (wm), wm ("}, {"heading": "E Proof of Theorem 2", "text": "The proof: Let us add u = xy \u03b1, ut = xtyt\u03b1t, u \u0445 T = 1 T = 1 ut, and F (u) = AT\u03b1BT\u03b1 \u2212 (Ax + By \u2212 c). If we sum up the equations in (35), (26) and (27), we have: E [P (xt + 1, yt + 1) \u2212 P (x, y) + F (ut + 1) T (ut + 1 \u2212 u)] \u2264 \u00b5f4 (xt \u2212 x \u00b2 2 \u2212 E (xt + 1 \u2212 x \u00b2 2) + \u03c1 2 E [\u0442 yt \u2212 y 2 H \u2212 yt \u2212 yt \u2212 yt \u2212 yt \u2212 yt \u2212 yt \u2212 yt \u2212 yt \u2212 1 H] + 1 2\u0432t \u2212 2 \u0432t \u2212 x \u00b2 T [\u0445t \u2212 2 \u2212 x \u00b2 T + 1 \u2212 x \u00b2 T)."}], "references": [{"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen P. Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Dual averaging and proximal gradient descent for online alternating direction multiplier method", "author": ["Taiji Suzuki"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Fast stochastic alternating direction method of multipliers", "author": ["Wenliang Zhong", "James Tin-Yau Kwok"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Online alternating direction method", "author": ["Huahua Wang", "Arindam Banerjee"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Stochastic alternating direction method of multipliers", "author": ["Hua Ouyang", "Niao He", "Long Tran", "Alexander G. Gray"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Towards an optimal stochastic alternating direction method of multipliers", "author": ["Samaneh Azadi", "Suvrit Sra"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "On the o(1/n) convergence rate of the douglas-rachford alternating direction method", "author": ["Bingsheng He", "Xiaoming Yuan"], "venue": "SIAM J. Numerical Analysis,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "A multivariate regression approach to association analysis of a quantitative trait", "author": ["Seyoung Kim", "Kyung-Ah Sohn", "Eric P. Xing"], "venue": "network. Bioinformatics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["Lin Xiao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John C. Duchi", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Nicolas Le Roux", "Mark W. Schmidt", "Francis Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Optimization with first-order surrogate functions", "author": ["Julien Mairal"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "The solution path of the generalized lasso", "author": ["Ryan J. Tibshirani", "Jonathan Taylor"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The alternating direction method of multipliers (ADMM) [1] is proposed to solve the problems which can be formulated as follows:", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "be the L1-norm and the constraint to be x\u2212 y = 0, we can get the well-known lasso formulation [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Similarly, we can take more complex constraints than that in lasso to get more complex regularization problems such as the structured sparse regularization problems [3, 4].", "startOffset": 165, "endOffset": 171}, {"referenceID": 3, "context": "Similarly, we can take more complex constraints than that in lasso to get more complex regularization problems such as the structured sparse regularization problems [3, 4].", "startOffset": 165, "endOffset": 171}, {"referenceID": 2, "context": "Compared with other optimization methods such as gradient decent, ADMM has demonstrated better performance in many complex regularization problems [3, 4].", "startOffset": 147, "endOffset": 153}, {"referenceID": 3, "context": "Compared with other optimization methods such as gradient decent, ADMM has demonstrated better performance in many complex regularization problems [3, 4].", "startOffset": 147, "endOffset": 153}, {"referenceID": 0, "context": "Furthermore, ADMM can be easily adapted to solve large-scale distributed problems [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "Hence, ADMM has been widely used in a large variety of areas [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Existing works have shown that batch ADMM is not efficient enough for big data applications with a large amount of training samples [5, 6].", "startOffset": 132, "endOffset": 138}, {"referenceID": 5, "context": "Existing works have shown that batch ADMM is not efficient enough for big data applications with a large amount of training samples [5, 6].", "startOffset": 132, "endOffset": 138}, {"referenceID": 4, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 5, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 2, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 6, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 3, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 6, "context": "Hence, stochastic ADMM has become a hot research topic and attracted much attention [7, 4].", "startOffset": 84, "endOffset": 90}, {"referenceID": 3, "context": "Hence, stochastic ADMM has become a hot research topic and attracted much attention [7, 4].", "startOffset": 84, "endOffset": 90}, {"referenceID": 4, "context": "Online alternating direction method (OADM) [5] is the first online ADMM method.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 212, "endOffset": 215}, {"referenceID": 6, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 251, "endOffset": 254}, {"referenceID": 3, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 294, "endOffset": 297}, {"referenceID": 7, "context": "STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM achieve a convergence rate of O(1/ \u221a T ) for general convex problems, worse than batch ADMM that has a convergence rate of O(1/T ) [8].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "Different from STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM, SA-ADMM [4] can achieve a convergence rate of O(1/T ) for general convex problems by using historic gradients to approximate the full gradients in each iteration.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "\u2022 Experimental results on graph-guided fused lasso [9] show that SCASADMM can achieve state-of-the-art performance in real applications.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "Just like the Gauss-Seidel method, ADMM iteratively updates the variables in an alternating manner as follows [1]:", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "constraint y = x, we can get the lasso formulation [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Some works [5, 8] have proved that the above batch ADMM has a convergence rate O(1/T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex, where T is the number of iterations.", "startOffset": 11, "endOffset": 17}, {"referenceID": 7, "context": "Some works [5, 8] have proved that the above batch ADMM has a convergence rate O(1/T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex, where T is the number of iterations.", "startOffset": 11, "endOffset": 17}, {"referenceID": 4, "context": "Recent works have shown that stochastic ADMM can achieve better performance than batch ADMM to handle large-scale datasets in terms of computation complexity and accuracy [5, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 5, "context": "Recent works have shown that stochastic ADMM can achieve better performance than batch ADMM to handle large-scale datasets in terms of computation complexity and accuracy [5, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 9, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 10, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 11, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 12, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 13, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 14, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 265, "endOffset": 269}, {"referenceID": 5, "context": "1 Algorithm As in existing stochastic ADMM methods [6, 4], the update rules for y and \u03b2 are still the same as those in (4) and (5).", "startOffset": 51, "endOffset": 57}, {"referenceID": 3, "context": "1 Algorithm As in existing stochastic ADMM methods [6, 4], the update rules for y and \u03b2 are still the same as those in (4) and (5).", "startOffset": 51, "endOffset": 57}, {"referenceID": 14, "context": "Compared with SVRG [15], the update rule in (9) has an extra vector A\u03b2t+ \u03c1A (Awm + Byt \u2212 c) = \u03c1AAwm + A (\u03b2t + \u03c1(Byt \u2212 c)).", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "The matrix inversion ( 1 \u03b7t I+\u03c1A TA)\u22121 can be avoided by linearizing the penalty term \u03c12 \u2016Ax + By \u2212 c\u2016 2 [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 219, "endOffset": 222}, {"referenceID": 3, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 256, "endOffset": 259}, {"referenceID": 3, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 296, "endOffset": 299}, {"referenceID": 5, "context": "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:", "startOffset": 6, "endOffset": 15}, {"referenceID": 6, "context": "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:", "startOffset": 6, "endOffset": 15}, {"referenceID": 3, "context": "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:", "startOffset": 6, "endOffset": 15}, {"referenceID": 15, "context": "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "In this paper, we focus on the graph-guided fused lasso [9] which is also used in [4].", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "In this paper, we focus on the graph-guided fused lasso [9] which is also used in [4].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "As in [6, 4], we use sparse inverse covariance selection method [17] to get a graph matrix (sparsity pattern) G, based on which we can get A = [G; I].", "startOffset": 6, "endOffset": 12}, {"referenceID": 3, "context": "As in [6, 4], we use sparse inverse covariance selection method [17] to get a graph matrix (sparsity pattern) G, based on which we can get A = [G; I].", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "They are: \u2022 Batch-ADMM [1]: The deterministic (batch) variant of ADMM which uses (7) to directly update x by visiting all training samples in each iteration.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "\u2022 STOC-ADMM [6]: The stochastic ADMM variant without using historic gradient for optimization, which has a convergence rate of O(1/ \u221a T ) for general convex problems and O(log T/T ) for strongly convex problems.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "\u2022 SA-ADMM [4]: The stochastic ADMM variant by using historic gradient to approximate the full gradient, which has a convergence rate of O(1/T ) for general convex problems.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Furthermore, both theoretical and empirical results have shown that SA-ADMM can outperform other methods like RDA-ADMM and OPG-ADMM [4].", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "The variant of SA-ADMM, SA-IU-ADMM, is also not adopted for comparison because it has similar performance as SA-ADMM [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "As in [4], four widely used datasets are adopted to evaluate our method and other baselines.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "As in [4], for each dataset we randomly choose half of the samples for training and use the rest for testing.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "The hyper-parameter \u03bb in (14) is set by using the same values in [4], which are also listed in Table 2.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "We adopt the same strategy as that in [4] to set the hyper-parameters \u03c1 in (2) and the", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "As in [4], we use y(x\u0304T ) = Ax\u0304T to replace \u0233T since the methods cannot necessarily guarantee that Ax\u0304T = \u0233T .", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "2 Convergence Results As in [4], we study the variation of the objective value on training set and the testing loss versus the number of effective passes over the data.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "[1] Stephen P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Robert Tibshirani.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Taiji Suzuki.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Wenliang Zhong and James Tin-Yau Kwok.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Huahua Wang and Arindam Banerjee.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Hua Ouyang, Niao He, Long Tran, and Alexander G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Samaneh Azadi and Suvrit Sra.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Bingsheng He and Xiaoming Yuan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Seyoung Kim, Kyung-Ah Sohn, and Eric P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Lin Xiao.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] John C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Nicolas Le Roux, Mark W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Julien Mairal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Rie Johnson and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Ryan J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "According to the results in [4], we have the following Lemma 4 and Lemma 5 about the estimation of yt+1 and \u03b1t+1.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "The proof of Lemma 4 and Lemma 5 can be directly derived from the results in [4], which is omitted here for space saving.", "startOffset": 77, "endOffset": 80}], "year": 2015, "abstractText": "Most stochastic ADMM (alternating direction method of multipliers) methods can only achieve a convergence rate which is slower than O(1/T ) on general convex problems, where T is the number of iterations. Hence, these methods are not scalable in terms of convergence rate (computation cost). There exists only one stochastic method, called SA-ADMM, which can achieve a convergence rate of O(1/T ) on general convex problems. However, an extra memory is needed for SA-ADMM to store the historic gradients on all samples, and thus it is not scalable in terms of storage cost. In this paper, we propose a novel method, called scalable stochastic ADMM (SCAS-ADMM), for large-scale optimization and learning problems. Without the need to store the historic gradients on all samples, SCAS-ADMM can achieve the same convergence rate of O(1/T ) as the best stochastic method SA-ADMM and batch ADMM on general convex problems. Experiments on graph-guided fused lasso show that SCASADMM can achieve state-of-the-art performance in real applications.", "creator": "LaTeX with hyperref package"}}}