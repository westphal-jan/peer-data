{"id": "1502.03630", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Ordering-Sensitive and Semantic-Aware Topic Modeling", "abstract": "Topic modeling of textual corpora is an important and challenging problem. In most previous work, the \"bag-of-words\" assumption is usually made which ignores the ordering of words. This assumption simplifies the computation, but it unrealistically loses the ordering information and the semantic of words in the context. In this paper, we present a Gaussian Mixture Neural Topic Model (GMNTM) which incorporates both the ordering of words and the semantic meaning of sentences into topic modeling. Specifically, we represent each topic as a cluster of multi-dimensional vectors and embed the corpus into a collection of vectors generated by the Gaussian mixture model. Each word is affected not only by its topic, but also by the embedding vector of its surrounding words and the context. The Gaussian mixture components and the topic of documents, sentences and words can be learnt jointly. Extensive experiments show that our model can learn better topics and more accurate word distributions for each topic. Quantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM obtains significantly better performance in terms of perplexity, retrieval accuracy and classification accuracy.", "histories": [["v1", "Thu, 12 Feb 2015 12:32:39 GMT  (44kb,D)", "http://arxiv.org/abs/1502.03630v1", "To appear in proceedings of AAAI 2015"]], "COMMENTS": "To appear in proceedings of AAAI 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["min yang", "tianyi cui", "wenting tu"], "accepted": true, "id": "1502.03630"}, "pdf": {"name": "1502.03630.pdf", "metadata": {"source": "META", "title": "Ordering-sensitive and Semantic-aware Topic Modeling", "authors": ["Min Yang", "Tianyi Cui", "Wenting Tu"], "emails": ["myang@cs.hku.hk", "tianyicui@gmail.com", "wttu@cs.hku.hk"], "sections": [{"heading": null, "text": "In fact, it is the case that you are able to play by the rules and that you are able to change the rules."}, {"heading": "Related works", "text": "Over the past decade, a wide variety of topic pairs have been proposed that can filter interesting topics out of texts in the form of multinomial distributions (Lead, Ng, and Jordan 2003) (Lead, White, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009), including that LDA (Lead, Ng, and Jordan 2003) and its variants are the most popular models for theme modeling. (The mix of topics per document in the LDA model is generated by a dirichlet that applies to all documents in the corpus.) Various extensions of the LDA model have been proposed. (Teh et al) assumes that the number of mixing components is unknown. (Mcauliffe and Lead) develops a supervisioned latent model."}, {"heading": "The GMNTM Model", "text": "In this section we first describe the GMNTM model as a probabilistic generative model. Then we illustrate the input algorithm for estimating the model parameters."}, {"heading": "Generative model", "text": "We assume that there are different words in the vocabulary, and there are D-words in the vocabulary. (...) We assume that there are different words in the vocabulary. (...) We assume that there are different words in the vocabulary. (...) We assume that there are different words in the vocabulary. (...) We assume that there are different words in the vocabulary. (...) We assume that there are different words in the vocabulary. (...) We assume that there are different words in the vocabulary. (...) We assume that there are different words in the vocabulary. (...) We assume that there are different words in the vocabulary. (...) We assume that there are different words in the vocabulary. (...) We assume that there are different words in the vocabulary."}, {"heading": "Estimating model parameters", "text": "The parameter estimation consists of two stages. In stage I we maximize the probability of the model in relation to the probability of distribution in relation to U and E. This procedure can be implemented by a stochastic mixing model. We alternatively execute stage I and stage II until the parameters are converted. The algorithms in this section are summarized in algorithm 1.Stage I. At this stage, the latent vectors of words, sentences and documents are specified. We estimate the parameters of the Gaussian mixing model. We estimate the parameters of the Gaussian mixing model."}, {"heading": "Experiments", "text": "In this section, we evaluate our model based on the 20 newsgroups and Reuters Corpus Volume 1 (RCV1-v2) datasets. Following the analysis in (Srivastava, Salakhutdinov, and Hinton 2013), we compare our GMNTM model with current topic models in the areas of helplessness, quality of retrieval, and classification accuracy."}, {"heading": "Datasets description", "text": "We use two widely used data sets in our evaluations, the 20 newsgroups data and the RCV1-v2 data. Data pre-processing is performed on both data sets. First, we remove nonal phabetic characters, numbers, pronouns, punctuation and stopwords from the text. Then, stammering is applied to reduce vocabulary size and clarify the question of data economy. Detailed characteristics of the data sets are described as follows: 20 newsgroups record: This data set is a collection of 18,845 newsgroup documents1. The corpus is divided into 20 different newsgroups, each corresponding to a separate topic. After pre-processing in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the data set is chronologically divided into 11,314 training documents and 7,531 test documents."}, {"heading": "Baseline methods", "text": "In the experiments, the proposed thematic modeling approach is compared with several basic methods, which we describe below: Latent Dirichlet Allocation (LDA): In the LDA model (Lead, Ng, and Jordan 2003), we used the online implementation of the gensim toolkit 3. We used the recommended parameter setting \u03b1 = 1 / T. Hidden Topic Markov Models (HMM): This model was proposed by (Gruber, Wei\u00df, and Rosen-Zvi 2007), which models the themes of the words in the document as a Markov chain. The HMM model is executed using the publicly available Code4. We use defaults for all hyperparameters. Over-Replicated Softmax (ORS): This model is proposed by (Srivastava, Salakhutdinov, and Hinton 2013)."}, {"heading": "Implementation details", "text": "In our GMNTM model, the learning rate \u03b1 is set to 0.025 and gradually reduced to 0.0001. For each word, a maximum of m = 6 previous words in the same sentence are used as context. For easy comparison with other models, the word vector size is set to the same number of topics V = T = 128. Increasing the word vector size could further improve the quality of the topics generated by the GMNTM model. Documents are divided into sentences and words using the NLTK toolkit (Bird 2006). 5. To conduct comparable experiments with limited vocabulary, words outside1Available are derived from http: / / qwone.com / ~ jason / 20Newsgroups 2Available in kitlearn toolkit (Pedregosa et al. 2011)."}, {"heading": "Generative model evaluation", "text": "We first evaluate the performance of our model as a generative model for documents. We perform our evaluation using the 20 newsgroups dataset and the RCV1 v2 dataset. For each of the datasets, we extract the words from raw data and preserve the order of the words. We follow the same evaluation as in (Srivastava, Salakhutdinov and Hinton 2013) and compare our model with the other models in terms of perplexity. We estimate the log probability for 1000 held documents that are randomly taken from the test records. After running the algorithm to derive the vector representations of words, sentences and documents in held test documents, the average test perplexity per word is estimated as Exp (\u2212 1N-w log p (w)), with N representing the total number of words in the proposed test documents, sentences and documents in held test documents as Explexity (\u2212 N-log w)."}, {"heading": "Document retrieval evaluation", "text": "To evaluate the quality of the document representations that are learned through our model, we perform an information query. Once entered in (Srivastava, Salakhutdinov, and Hinton 2013), the documents in the training set are used as a database, while the test set is used as a query. For each query, the documents in the database are classified as a benchmark based on the cosine spacing. The retrieval task is performed separately for each label and the results are averaged. Figure 1 compares the accuracy retrieval curves with 128 topics. The curves for LDA and Over-Replicated originate from (Srivastava, Salakhutdinov and Hinton 2013). We see that our model performs equally or slightly better than the other models for the 20 newsgroups dataset. While the RCV1 v2 dataset achieves a significant improvement, as RCV1 v2 contains a larger set of texts, the Minor Words model is included in the GMTM."}, {"heading": "Document classification evaluation", "text": "Following the evaluation of the (Srivastava, Salakhutdinov, and Hinton 2013), we also perform the document classification with the learned topics of our model. The same as in (Srivastava, Salakhutdinov, and Hinton 2013), multinomial logistic regression with a cross-entropy function is used for the 20 newsgroups, and the evaluation metric is the classification accuracy. For the RCV1 v2 dataset, we use independent logistic regression for each label. The evaluation metric is Mean Average Precision.We summarize the experimental results with 128 topics in Table 3. The results of the document classification for LDA and Over-Replicated Softmax are used for each label."}, {"heading": "Conclusion and Future Work", "text": "Instead of ignoring the semantics of words and assuming that the distribution of themes within a document is conditionally independent, in this work we introduce an orderly and semantically conscious theme modeling approach. Together, the GMNTM model learns the topic of documents and the vectorized representation of words, sentences and documents; the model learns better themes and unique words that belong to different themes. Compared to modern theme modeling approaches, the GMNTM performs better in terms of perplexity, retrievability and classification accuracy. In future work, we will explore the use of non-parametric models to cluster word vectors. For example, we look forward to applying infinite Dirichelet methods to automatically detect the number of themes. We can also use archaic models to further capture the subtle semantics of the text."}], "references": [{"title": "Nltk: the natural language toolkit", "author": ["S. Bird"], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions, 69\u201372. Association for Computational Linguistics.", "citeRegEx": "Bird,? 2006", "shortCiteRegEx": "Bird", "year": 2006}, {"title": "Pattern recognition and machine learning, volume 1", "author": ["C.M. Bishop"], "venue": "springer New York.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM 55(4):77\u201384.", "citeRegEx": "Blei,? 2012", "shortCiteRegEx": "Blei", "year": 2012}, {"title": "Deep learning of semantic word representations to implement a content-based recommender for the recsys", "author": ["O.U. Florez", "L. Nachman"], "venue": null, "citeRegEx": "Florez and Nachman,? \\Q2014\\E", "shortCiteRegEx": "Florez and Nachman", "year": 2014}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D. Griffiths", "M. Tenenbaum"], "venue": "Advances in neural information processing systems 16:17.", "citeRegEx": "Griffiths and Tenenbaum,? 2004", "shortCiteRegEx": "Griffiths and Tenenbaum", "year": 2004}, {"title": "Hidden topic markov models", "author": ["A. Gruber", "Y. Weiss", "M. Rosen-Zvi"], "venue": "International Conference on Artificial Intelligence and Statistics, 163\u2013170.", "citeRegEx": "Gruber et al\\.,? 2007", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 1607\u20131614.", "citeRegEx": "Hinton and Salakhutdinov,? 2009", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2009}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, 289\u2013296. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Hofmann,? 1999", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Lauly"], "venue": "Advances in Neural Information Processing Systems, 2708\u20132716.", "citeRegEx": "Larochelle and Lauly,? 2012", "shortCiteRegEx": "Larochelle and Lauly", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "arXiv preprint arXiv:1405.4053.", "citeRegEx": "Le and Mikolov,? 2014", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "The Journal of Machine Learning Research 5:361\u2013 397.", "citeRegEx": "Lewis et al\\.,? 2004", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Weakly supervised joint sentiment-topic detection from text", "author": ["C. Lin", "Y. He", "R. Everson", "S. Ruger"], "venue": "CIKM 24(6):1134\u20131145.", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Modeling user rating profiles for collaborative filtering", "author": ["B.M. Marlin"], "venue": "Advances in neural information processing systems.", "citeRegEx": "Marlin,? 2003", "shortCiteRegEx": "Marlin", "year": 2003}, {"title": "Supervised topic models", "author": ["J.D. Mcauliffe", "D.M. Blei"], "venue": "Advances in neural information processing systems, 121\u2013128.", "citeRegEx": "Mcauliffe and Blei,? 2008", "shortCiteRegEx": "Mcauliffe and Blei", "year": 2008}, {"title": "A probabilistic approach to spatiotemporal theme pattern mining on weblogs", "author": ["Q. Mei", "C. Liu", "H. Su", "C. Zhai"], "venue": "Proceedings of the 15th international conference on World Wide Web, 533\u2013542. ACM.", "citeRegEx": "Mei et al\\.,? 2006", "shortCiteRegEx": "Mei et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 1081\u20131088.", "citeRegEx": "Mnih and Hinton,? 2009", "shortCiteRegEx": "Mnih and Hinton", "year": 2009}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems, 2265\u2013 2273.", "citeRegEx": "Mnih and Kavukcuoglu,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "arXiv preprint arXiv:1206.6426.", "citeRegEx": "Mnih and Teh,? 2012", "shortCiteRegEx": "Mnih and Teh", "year": 2012}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "The Journal of Machine Learning Research 12:2825\u20132830.", "citeRegEx": "Pedregosa et al\\.,? 2011", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "Proceedings of the 20th conference on Uncertainty in artificial intelligence, 487\u2013494. AUAI Press.", "citeRegEx": "Rosen.Zvi et al\\.,? 2004", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "Modeling documents with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1309.6865.", "citeRegEx": "Srivastava et al\\.,? 2013", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "Probabilistic topic models", "author": ["M. Steyvers", "T. Griffiths"], "venue": "Handbook of latent semantic analysis 427(7):424\u2013 440.", "citeRegEx": "Steyvers and Griffiths,? 2007", "shortCiteRegEx": "Steyvers and Griffiths", "year": 2007}, {"title": "Probabilistic author-topic models for information discovery", "author": ["M. Steyvers", "P. Smyth", "M. Rosen-Zvi", "T. Griffiths"], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, 306\u2013315. ACM.", "citeRegEx": "Steyvers et al\\.,? 2004", "shortCiteRegEx": "Steyvers et al\\.", "year": 2004}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the american statistical association 101(476).", "citeRegEx": "Teh et al\\.,? 2006", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Topic modeling: beyond bag-ofwords", "author": ["H.M. Wallach"], "venue": "Proceedings of the 23rd international conference on Machine learning, 977\u2013984. ACM.", "citeRegEx": "Wallach,? 2006", "shortCiteRegEx": "Wallach", "year": 2006}, {"title": "A hybrid neural network-latent topic model", "author": ["L. Wan", "L. Zhu", "R. Fergus"], "venue": "International Conference on Artificial Intelligence and Statistics, 1287\u20131294.", "citeRegEx": "Wan et al\\.,? 2012", "shortCiteRegEx": "Wan et al\\.", "year": 2012}, {"title": "Topics over time: a non-markov continuous-time model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, 424\u2013433. ACM.", "citeRegEx": "Wang and McCallum,? 2006", "shortCiteRegEx": "Wang and McCallum", "year": 2006}, {"title": "Lda-based document models for ad-hoc retrieval", "author": ["X. Wei", "W.B. Croft"], "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, 178\u2013185. ACM.", "citeRegEx": "Wei and Croft,? 2006", "shortCiteRegEx": "Wei and Croft", "year": 2006}, {"title": "A topic model for building fine-grained domainspecific emotion lexicon", "author": ["M. Yang", "B. Peng", "Z. Chen", "D. Zhu", "K.-P. Chow"], "venue": "ACL 2014 421\u2013426.", "citeRegEx": "Yang et al\\.,? 2014a", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Learning domain-specific sentiment lexicon with supervised sentiment-aware lda", "author": ["M. Yang", "D. Zhu", "R. Mustafa", "K.-P. Chow"], "venue": "ECAI 2014 927\u2013932.", "citeRegEx": "Yang et al\\.,? 2014b", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Topic models have been applied to a variety of applications, including information retrieval (Wei and Croft 2006), collaborative filtering (Marlin 2003), authorship identification (Rosen-Zvi et al.", "startOffset": 93, "endOffset": 113}, {"referenceID": 13, "context": "Topic models have been applied to a variety of applications, including information retrieval (Wei and Croft 2006), collaborative filtering (Marlin 2003), authorship identification (Rosen-Zvi et al.", "startOffset": 139, "endOffset": 152}, {"referenceID": 21, "context": "Topic models have been applied to a variety of applications, including information retrieval (Wei and Croft 2006), collaborative filtering (Marlin 2003), authorship identification (Rosen-Zvi et al. 2004) and opinion extraction (Lin et al.", "startOffset": 180, "endOffset": 203}, {"referenceID": 12, "context": "2004) and opinion extraction (Lin et al. 2012), etc.", "startOffset": 29, "endOffset": 46}, {"referenceID": 5, "context": "Existing topic models (Griffiths and Tenenbaum 2004; Mcauliffe and Blei 2008; Blei 2012) are built based on the assumption that each document is represented by a mixture of topics, where each topic defines a probability distribution over words.", "startOffset": 22, "endOffset": 88}, {"referenceID": 14, "context": "Existing topic models (Griffiths and Tenenbaum 2004; Mcauliffe and Blei 2008; Blei 2012) are built based on the assumption that each document is represented by a mixture of topics, where each topic defines a probability distribution over words.", "startOffset": 22, "endOffset": 88}, {"referenceID": 3, "context": "Existing topic models (Griffiths and Tenenbaum 2004; Mcauliffe and Blei 2008; Blei 2012) are built based on the assumption that each document is represented by a mixture of topics, where each topic defines a probability distribution over words.", "startOffset": 22, "endOffset": 88}, {"referenceID": 8, "context": "These models, including the probabilistic latent semantic analysis (PLSA) (Hofmann 1999) model and latent Dirichlet allocation (LDA) (Blei, Ng, and Jordan 2003) model, can be viewed as graphical models with latent variables.", "startOffset": 74, "endOffset": 88}, {"referenceID": 25, "context": "Some nonparametric extensions to these models have also been quite successful (Teh et al. 2006; Steyvers and Griffiths 2007).", "startOffset": 78, "endOffset": 124}, {"referenceID": 23, "context": "Some nonparametric extensions to these models have also been quite successful (Teh et al. 2006; Steyvers and Griffiths 2007).", "startOffset": 78, "endOffset": 124}, {"referenceID": 7, "context": "New undirected graphical model approaches, including the Replicated softmax model (Hinton and Salakhutdinov 2009), are also successfully applied to exploring the topics of the text, and in particular cases they outperform LDA (Srivastava, Salakhutdinov, and Hinton 2013).", "startOffset": 82, "endOffset": 113}, {"referenceID": 26, "context": "When deciding which topic generated the word \u201cchair\u201d in the first sentence, knowing that it was immediately preceded by the word \u201cdepartment\u201d makes it much more likely to have been generated by a topic that assigns high probability to words related to university administration (Wallach 2006).", "startOffset": 278, "endOffset": 292}, {"referenceID": 17, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 19, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 18, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 16, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 10, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 21, "context": "Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables.", "startOffset": 0, "endOffset": 15}, {"referenceID": 17, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 19, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 18, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 16, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 10, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 5, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009).", "startOffset": 175, "endOffset": 309}, {"referenceID": 3, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009).", "startOffset": 175, "endOffset": 309}, {"referenceID": 7, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009).", "startOffset": 175, "endOffset": 309}, {"referenceID": 28, "context": "Recent work incorporates context information into the topic modeling, such as time (Wang and McCallum 2006), geographic location (Mei et al.", "startOffset": 83, "endOffset": 107}, {"referenceID": 15, "context": "Recent work incorporates context information into the topic modeling, such as time (Wang and McCallum 2006), geographic location (Mei et al. 2006), authorship (Steyvers et al.", "startOffset": 129, "endOffset": 146}, {"referenceID": 24, "context": "2006), authorship (Steyvers et al. 2004), and sentiment (Yang et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 31, "context": "2004), and sentiment (Yang et al. 2014b; 2014a), to make topic models fit expectations better.", "startOffset": 21, "endOffset": 47}, {"referenceID": 3, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009). Among these approaches, LDA (Blei, Ng, and Jordan 2003) and its variants are the most popular models for topic modeling. The mixture of topics per document in the LDA model is generated from a Dirichlet prior mutual to all documents in the corpus. Different extensions of the LDA model have been proposed. For example, Teh et al. (2006) assumes that the number of mixture components is unknown a prior and is to be inferred from the data.", "startOffset": 176, "endOffset": 648}, {"referenceID": 3, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009). Among these approaches, LDA (Blei, Ng, and Jordan 2003) and its variants are the most popular models for topic modeling. The mixture of topics per document in the LDA model is generated from a Dirichlet prior mutual to all documents in the corpus. Different extensions of the LDA model have been proposed. For example, Teh et al. (2006) assumes that the number of mixture components is unknown a prior and is to be inferred from the data. Mcauliffe and Blei (2008) develops a supervised latent Dirichlet allocation model (sLDA) for document-response pairs.", "startOffset": 176, "endOffset": 776}, {"referenceID": 9, "context": "Neural network based approaches, such as Neural Autoregressive Density Estimators (DocNADE) (Larochelle and Lauly 2012) and Hybrid Neural Network-Latent Topic Model (Wan, Zhu, and Fergus 2012), are also shown outperforming the LDA model.", "startOffset": 92, "endOffset": 119}, {"referenceID": 17, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 19, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 18, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 16, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 10, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document.", "startOffset": 14, "endOffset": 26}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document. Hinton and Salakhutdinov (2009) extend \u201cReplicated Softmax\u201d by adding another layer of hidden units on top of the first with bipartite undirected connections.", "startOffset": 14, "endOffset": 285}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document. Hinton and Salakhutdinov (2009) extend \u201cReplicated Softmax\u201d by adding another layer of hidden units on top of the first with bipartite undirected connections. Neural network based approaches, such as Neural Autoregressive Density Estimators (DocNADE) (Larochelle and Lauly 2012) and Hybrid Neural Network-Latent Topic Model (Wan, Zhu, and Fergus 2012), are also shown outperforming the LDA model. However, all of these these topic models employ the bagof-words assumption, which is rarely true in practice. The bag-of-word assumption loses the ordering of the words and ignore the semantics of the context. There are several previous literature taking the order of words into account. Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables.", "startOffset": 14, "endOffset": 953}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document. Hinton and Salakhutdinov (2009) extend \u201cReplicated Softmax\u201d by adding another layer of hidden units on top of the first with bipartite undirected connections. Neural network based approaches, such as Neural Autoregressive Density Estimators (DocNADE) (Larochelle and Lauly 2012) and Hybrid Neural Network-Latent Topic Model (Wan, Zhu, and Fergus 2012), are also shown outperforming the LDA model. However, all of these these topic models employ the bagof-words assumption, which is rarely true in practice. The bag-of-word assumption loses the ordering of the words and ignore the semantics of the context. There are several previous literature taking the order of words into account. Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables. They extend a unigram topic model so that it can reflect properties of a hierarchical Dirichlet bigram model. Gruber, Weiss, and Rosen-Zvi (2007) propose modeling the topic of words a Markov chain.", "startOffset": 14, "endOffset": 1223}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document. Hinton and Salakhutdinov (2009) extend \u201cReplicated Softmax\u201d by adding another layer of hidden units on top of the first with bipartite undirected connections. Neural network based approaches, such as Neural Autoregressive Density Estimators (DocNADE) (Larochelle and Lauly 2012) and Hybrid Neural Network-Latent Topic Model (Wan, Zhu, and Fergus 2012), are also shown outperforming the LDA model. However, all of these these topic models employ the bagof-words assumption, which is rarely true in practice. The bag-of-word assumption loses the ordering of the words and ignore the semantics of the context. There are several previous literature taking the order of words into account. Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables. They extend a unigram topic model so that it can reflect properties of a hierarchical Dirichlet bigram model. Gruber, Weiss, and Rosen-Zvi (2007) propose modeling the topic of words a Markov chain. Florez and Nachman (2014) exploits the semantics regularities captured by a Recurrent Neural Network (RNN) in text documents to build a recommender system.", "startOffset": 14, "endOffset": 1301}, {"referenceID": 1, "context": "The reader can refer to the book (Bishop 2006) for the implementation details.", "startOffset": 33, "endOffset": 46}, {"referenceID": 7, "context": "Following the preprocessing in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the dataset is partitioned chronologically into 11,314 training documents and 7,531 testing documents.", "startOffset": 31, "endOffset": 62}, {"referenceID": 9, "context": "Following the preprocessing in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the dataset is partitioned chronologically into 11,314 training documents and 7,531 testing documents.", "startOffset": 67, "endOffset": 94}, {"referenceID": 11, "context": "Reuters Corpus Volume 1 (RCV1-v2): This dataset is an archive of 804,414 newswire stories produced by Reuters journalists between August 20, 1996, and August 19, 1997 (Lewis et al. 2004)2.", "startOffset": 167, "endOffset": 186}, {"referenceID": 7, "context": "As in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the data was randomly split into 794,414 training documents and 10,000 testing documents.", "startOffset": 6, "endOffset": 37}, {"referenceID": 9, "context": "As in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the data was randomly split into 794,414 training documents and 10,000 testing documents.", "startOffset": 42, "endOffset": 69}, {"referenceID": 7, "context": "It is a two hidden layer DBM model, which has been shown to obtain a state-of-the-art performance in terms of classification and retrieval tasks compared with Replicated Softmax model (Hinton and Salakhutdinov 2009) and Cannonade model (Larochelle and Lauly 2012).", "startOffset": 184, "endOffset": 215}, {"referenceID": 9, "context": "It is a two hidden layer DBM model, which has been shown to obtain a state-of-the-art performance in terms of classification and retrieval tasks compared with Replicated Softmax model (Hinton and Salakhutdinov 2009) and Cannonade model (Larochelle and Lauly 2012).", "startOffset": 236, "endOffset": 263}, {"referenceID": 0, "context": "Documents are split into sentences and words using the NLTK toolkit (Bird 2006)5.", "startOffset": 68, "endOffset": 79}, {"referenceID": 20, "context": "The Gaussian mixture model is inferred using the variational inference algorithm in scikitlearn toolkit (Pedregosa et al. 2011)6.", "startOffset": 104, "endOffset": 127}], "year": 2015, "abstractText": "Topic modeling of textual corpora is an important and challenging problem. In most previous work, the \u201cbag-of-words\u201d assumption is usually made which ignores the ordering of words. This assumption simplifies the computation, but it unrealistically loses the ordering information and the semantic of words in the context. In this paper, we present a Gaussian Mixture Neural Topic Model (GMNTM) which incorporates both the ordering of words and the semantic meaning of sentences into topic modeling. Specifically, we represent each topic as a cluster of multi-dimensional vectors and embed the corpus into a collection of vectors generated by the Gaussian mixture model. Each word is affected not only by its topic, but also by the embedding vector of its surrounding words and the context. The Gaussian mixture components and the topic of documents, sentences and words can be learnt jointly. Extensive experiments show that our model can learn better topics and more accurate word distributions for each topic. Quantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM obtains significantly better performance in terms of perplexity, retrieval accuracy and classification accuracy.", "creator": "TeX"}}}