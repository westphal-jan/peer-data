{"id": "1502.03671", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Phrase-based Image Captioning", "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.", "histories": [["v1", "Thu, 12 Feb 2015 14:17:15 GMT  (730kb,D)", "http://arxiv.org/abs/1502.03671v1", null], ["v2", "Thu, 9 Apr 2015 09:48:52 GMT  (572kb,D)", "http://arxiv.org/abs/1502.03671v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\u00e9mi lebret", "pedro h o pinheiro", "ronan collobert"], "accepted": true, "id": "1502.03671"}, "pdf": {"name": "1502.03671.pdf", "metadata": {"source": "CRF", "title": "Phrase-based Image Captioning", "authors": ["R\u00e9mi Lebret", "Pedro O. Pinheiro", "Ronan Collobert"], "emails": ["REMI@LEBRET.CH,", "PEDRO@OPINHEIRO.COM", "RONAN@COLLOBERT.COM"], "sections": [{"heading": "1. Introduction", "text": "The problem is that it is necessary to recognize and interact with different objects in the images. Another challenge is that an image description generator must express these interactions in a natural language (e.g. English). Most of the experiments are based on recursive neural networks to generate sentences, which use the power of neural networks to transform images and sentences into a common space."}, {"heading": "2. Related Works", "text": "The classical approach to sentence generation is to present the problem as a retrieval problem: A particular test image is described with the highest annotations in the training set (Hodosh et al., 2013; Socher et al., 2014; Srivastava & Salakhutdinov, 2014) These matching methods may not generate adequate descriptions for a new combination of objects. Due to this limitation, several generative approaches have been proposed, many of which use syntactic and semantic constraints in the generation process (Yao et al., 2010; Mitchell et al., 2012; Kulkarni et al., 2011; Kuznetsova et al., 2012). These approaches benefit from visual recognition systems to induce words or phrases, but unlike our work, they do not use multimodal metrics between images and phrases."}, {"heading": "3. Syntax Analysis of Image Descriptions", "text": "The art of writing sentences can vary greatly depending on the domain. When reporting on news or reviewing an article, not only the choice of words can vary, but also the general structure of the sentence. In this section, we would like to analyze the syntax of image descriptions to determine whether images have their own structures. Therefore, we continue with an exploratory analysis of two current data sets that contain a large number of images with descriptions: Flickr30k (Hodosh et al., 2013) and COCO (Lin et al., 2014)."}, {"heading": "3.1. Datasets", "text": "The Flickr30k dataset contains 31,014 images, of which 1,014 images will be used for validation, 1,000 for testing and the rest for training (i.e. 29,000 images).The COCO dataset contains 123,287 images, 82,783 training images and 40,504 validation images.The test images have not yet been published, so we are using two sets of 5,000 images from the validation images for validation and testing, as in Karpaie & Fei file (2014) 2.Both datasets contain images with five (or six) sentence descriptions commented on with Amazon Mechanical Turk, resulting in 559,113 sets when the two training datasets are combined.2Available at http: / / cs.stanford.edu / people / karpathy / deepimagesent / 0 1 2 2 3 3 4 4 5 6 + NP VP PPA ppar eanc efr ec eque eque eque cie per CO1. (CO20% of the number of the training data, plus the number of CO40 + the number of CO5050P)."}, {"heading": "3.2. Chunking-based Approach", "text": "This interaction between objects is described as the action or relative position between different objects. It may be short or long, but it generally respects this process. To confirm this assertion and better understand the descriptive structures, we used a chunking approach (also referred to as shallow parsing) that identifies the components of a sentence. These components are usually noun phrases (NP), verbal phrases (VP), and prepositional phrases (PP). We extract them from training sets using the SENNA software.3. Pre-verbal and post-verbal adverb phrases are fused with verb phrases to limit the number of phrase types. Statistics reported in Figure 1 and Figure 2 confirm that image descriptions have a simple and unique structure. These phrases do not exhibit much variability. All key elements in a given image are usually described with a phrase or phrase between these phrases."}, {"heading": "4. Phrase-based Model for Image Descriptions", "text": "Using previous work on word and image representation, we propose a simple model that can predict the phrases that best describe a particular image. To this end, a metric between images and phrases is trained, as illustrated in Figure 3."}, {"heading": "4.1. Image Representations", "text": "We use a revolutionary neural network to display images. CNN has been used in various areas of vision and is currently state-of-the-art in many object recognition tasks. We are looking at a CNN that has been pre-trained in the task of object classification (Chatfield et al., 2014). We are using a CNN exclusively for feature extraction, i.e. no learning is done in the CNN layers."}, {"heading": "4.2. Learning a Common Space for Image and Phrase Representations", "text": "Let me be the set of training images, C the set of all the phrases used to describe I, and \u03b8 the trainable parameters of the model. By representing each image i-I thanks to the pre-trained CNN with a vector zi-Rn, we define a metric between the image i and a phrase c as a bilinear operation: f\u03b8 (c, i) = u T c V zi, (1) withU = (uc1,..., uc | C |)."}, {"heading": "4.3. Phrase Representations Initialization", "text": "Good word vector representations can be achieved very efficiently with many different newer approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014). Mikolov et al. (2013a) also showed that simple vector additions can often produce significant results, such as King - Man + Woman - Queen. Using the ability of these word vector representations to compose by simple summation, phrases can easily be represented with an elemental addition. Therefore, any phrase c consisting of K-words wk is represented by a vector xwk-Rm, thanks to a word representation model that is pre-trained on large unlabeled text corpora. A vector representation uc for a phrase c = {w1,.., wK} is then performed by an average value of its word vector representations: Vector = 1k for all Phrase."}, {"heading": "4.4. Training with Negative Sampling", "text": "Each image i is described by a variety of possible phrases Ci C. We look at | C | classifiers that assign a score to each phrase. We train our model to distinguish a target phrase cj from a series of negative phrases ck-C-C, with ck-6 = cj. With \u03b8 = {U, V} we minimize the following logistic loss function in relation to \u03b8: \u03b8-7-I-I-cj-Ci (log (1 + e + uTcj-V-zi) + \u2211 ck-C-log (1 + e-u-ck-V-zi)). (3) The model is trained using stochastic gradient lineage."}, {"heading": "5. From Phrases to Sentence", "text": "Once we have identified the most likely L-components cj in Figure i, we propose to generate sentences from it. From this sentence, l-phrases are used to create a syntactically correct description."}, {"heading": "5.1. Sentence Generation", "text": "Using a statistical linguistic frame, the probability of a particular sentence is given as follows: P (c1, c2,.., cl) = l = 1P (cj | c1,.., cj \u2212 1) (4) To keep this system as simple as possible and to use the second-order Markov property, we approach equation 4 with a trigram language model: P (c1, c2,..., cl) \u2248 l = 1P (cj | cj \u2212 2, cj \u2212 1). (5) The best candidate corresponds to the sentence P (c1, c2,., cl), which maximizes the probability of equation 5 over all possible sentence sizes. Since we want to restrict the decryption algorithm to include previous knowledge of the chunking tags t (NP, V P, PP), we write equation 5 as: l = \u2212 cj \u2212 tP (cj, tj = cj, cj, cj \u2212 2, cj, cj, cj, \u2212 j, cj, \u2212 j, 1, \u2212 j, \u2212 j, \u2212 j, cj \u2212 j, cj \u2212 j, cj, cj \u2212 j, c1 \u2212 j, cj \u2212 j, c1 \u2212 j, cj, cj \u2212 1, cj, cj, cj \u2212 1, cj, cj, cj \u2212 1, cj, cj, cj \u2212 1, cl, cl, cl, c1, c1, c1, c1, c1, c1, c1, cj, cj \u2212 1, c1, c1, c1, cj \u2212 1, c1, cj, cj, cj, cj \u2212 1, etc."}, {"heading": "5.2. Sentence Decoding", "text": "In decoding the time, we truncate the graph of all possible sentences from the uppermost L phrases with a bar search for three heuristics: (i) we only look at the transitions that are likely to occur (we discard any sentence that would have a trigram transition probability less than 0.01), which helps to discard sentences that are semantically wrong; (ii) any predicted phrase cj may occur only once; (iii) we add syntactical constraints that are in Figure 4. The last heuristics is based on the analysis of the syntax in Section 3. In Figure 2, we see that a noun phrase is generally followed by a verb phrase or a prepositive phrase, and then follows both by another noun phrase. A large majority of sentences contain three noun phrases that are interspersed with verbs or prepositive phrases. According to the statistics reported in Figure 1, sentences with this phrase or two phrases are more frequent than sentences with three (so we repeat four)."}, {"heading": "5.3. Sentence Re-ranking", "text": "For each test image i, the proposed model generates a series of M-phrases. Sentence generation is not determined by the image, except for phrases selected beforehand. Some phrases may be syntactically good, but only slightly correspond to the image. Consider an4This, for example, is easy to do with a bar search, but with a full search it is difficult. Both phrases \"a cat sitting on a mat and a dog eating a bone\" and \"a cat sitting on a mat\" are correct, but the second part of the image is missing. A ranking of the generated phrases is therefore necessary to select the sentence that best matches the image. As a generated set consists of l-phrases predicted by our system, we simply cut the phrase values given by Equation 1."}, {"heading": "6. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1.1. FEATURE SELECTION", "text": "According to Karpathy & Fei-Fei (2014), the image characteristics are extracted using VGG CNN (Chatfield et al., 2014).This model generates image representations of the dimension 4096 from RGB input images. For each training set, only phrases that occur at least ten times are taken into account; this threshold is chosen to meet two objectives: (i) limit the number of phrases C and thus the size of the matrix U, and (ii) exclude rare phrases to better generalize the descriptions.Statistics on the number of phrases are then presented in Table 1. For Flickr30k, this threshold covers about 81% of NP, 83% of VP, and 99% of PP. For COCO, it covers about 73% of NP, 75% of VP, and 99% of PP. Phrase representations are then represented by aver-Patchr30k COCONoun Phrase (NP) 4818 8982Verb Phrase (total of Pre102words)."}, {"heading": "6.1.2. LEARNING THE MULTIMODAL METRIC", "text": "The parameters \u03b8 are V-R400 x 4096 (randomly initialized) and U-R400 x | C | (initialized with phrase representations), which are matched to the validation data sets. They are trained with 15 randomly selected negative samples and a learning rate of 0.00025."}, {"heading": "6.1.3. GENERATING SENTENCES FROM THE PREDICTED PHRASES", "text": "The transition probabilities for our restricted language model (see Figure 4) are calculated independently for each training set. No smoothing was used in the experiments. In terms of the number of best placed phrases for a given test image, we select only the five best predicted verb phrases and the five best predicted prepositional phrases. As the average number of noun phrases is higher than for the other two phrases (see Figure 1), more noun phrases are needed."}, {"heading": "6.2. Experimental Results", "text": "As an initial assessment, we consider the task of retrieving the down-to-earth phrases from the test images. Results in the table show that our system has received a callback of approximately 50% on this task with respect to both sets of data."}, {"heading": "6.3. Diversity of Image Descriptions", "text": "Unlike RNN-based models, our model is not designed to match a particular image i with its descriptions of truth, i.e. giving P (s | i). Since our model prints a set of phrases instead, it is not really surprising that only 1% of our generated descriptions are in the training set for Flickr30k and 9.7% for COCO. While an RNN-based model is generative, it could easily match small training data. Vinyals et al. (2014), for example, report that the generated set is present 80% of the time in the training set. Our model therefore offers a good alternative with the ability to create invisible descriptions using a combination of phrases from the training set."}, {"heading": "6.4. Phrase Representation Fine-Tuning", "text": "Prior to the training of the model, the matrix U is initialized with phrases taken from the entire English Wikipedia. This corpus of unlabeled text is well structured and large enough to provide good word vector representations, which can then produce good phrases representations. However, the content of Wikipedia differs significantly from the content of the image descriptions. Some words used to describe images could be used in different contexts in Wikipedia, which can lead to exotic expressions for certain phrases before and after the training. Therefore, this becomes crucial in order to fine-tune these phrases by fine-tuning the Matrix U during the training. Some examples of noun phrases are reported in Table 4 with their nearest neighbors before and after the training, which confirm the importance of fine-tuning to integrate visual characteristics. In Wikipedia, cat appears to appear in the same context as dog or other animals. When looking at the nearest neighbors of a phrase such as a gray metaphor, the other phrase is most likely to violate the gray one, the phrase is most obvious after the training."}, {"heading": "7. Conclusion", "text": "In this thesis, we propose a simple model that is able to derive various phrases from examples of images. By using a statistical language model, our model can automatically generate sentences from the predicted phrases. We show that the problem of sentence generation can be achieved effectively without the use of complex, recurring networks. Although our algorithm is simpler than modern models, it achieves similar results in this task. In addition, our model generates new sentences that are not normally present in training. Future research will focus on the use of unattended data and more complex language models to improve sentence generation, as well as assessing the impact of visually generated phrase representations on existing natural language processing systems."}], "references": [{"title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Learning a Recurrent Visual Representation for Image Caption Generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CoRR, abs/1411.5654,", "citeRegEx": "Chen and Zitnick,? \\Q2014\\E", "shortCiteRegEx": "Chen and Zitnick", "year": 2014}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Framing image description as a ranking task: data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CoRR, abs/1412.2306,", "citeRegEx": "Karpathy and Fei.Fei,? \\Q2014\\E", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2014}, {"title": "Unifying Visual-Semantic Embeddings with Multimodal", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Neural Language Models. volume abs/1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "In ACL,", "citeRegEx": "Kuznetsova et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2012}, {"title": "Rehabilitation of Countbased Models for Word Vector Representations", "author": ["R. Lebret", "R. Collobert"], "venue": "CoRR, abs/1412.4930,", "citeRegEx": "Lebret and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshp,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Midge: Generating Image Descriptions from Computer Vision Detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "III Daum\u00e9"], "venue": "In EACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "Kavukcuoglu", "Koray"], "venue": "In NIPS", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "TACL,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Multimodal Learning with Deep Boltzmann Machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava and Salakhutdinov,? \\Q2014\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov", "year": 2014}, {"title": "Translating Videos to Natural Language", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "Using Deep Recurrent Neural Networks. CoRR,", "citeRegEx": "Venugopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "I2T: Image Parsing to Text Description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "These models leverage the power of neural networks to transform image and sentence representations into a common space (Mao et al., 2014; Karpathy & Fei-Fei, 2014; Vinyals et al., 2014; Donahue et al., 2014).", "startOffset": 119, "endOffset": 207}, {"referenceID": 3, "context": "The quality of our sentence generation is evaluated on two very popular datasets for the task: Flickr30k (Hodosh et al., 2013) and the recently published COCO (Lin et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 13, "context": "Using the popular BLEU score (Papineni et al., 2002), our results are competitive with other recent works.", "startOffset": 29, "endOffset": 52}, {"referenceID": 3, "context": "The classical approach to sentence generation is to pose the problem as a retrieval problem: a given test image will be described with the highest ranked annotation in the training set (Hodosh et al., 2013; Socher et al., 2014; Srivastava & Salakhutdinov, 2014).", "startOffset": 185, "endOffset": 261}, {"referenceID": 15, "context": "The classical approach to sentence generation is to pose the problem as a retrieval problem: a given test image will be described with the highest ranked annotation in the training set (Hodosh et al., 2013; Socher et al., 2014; Srivastava & Salakhutdinov, 2014).", "startOffset": 185, "endOffset": 261}, {"referenceID": 18, "context": "Many of them use syntactic and semantic constraints in the generation process (Yao et al., 2010; Mitchell et al., 2012; Kulkarni et al., 2011; Kuznetsova et al., 2012).", "startOffset": 78, "endOffset": 167}, {"referenceID": 11, "context": "Many of them use syntactic and semantic constraints in the generation process (Yao et al., 2010; Mitchell et al., 2012; Kulkarni et al., 2011; Kuznetsova et al., 2012).", "startOffset": 78, "endOffset": 167}, {"referenceID": 6, "context": "Many of them use syntactic and semantic constraints in the generation process (Yao et al., 2010; Mitchell et al., 2012; Kulkarni et al., 2011; Kuznetsova et al., 2012).", "startOffset": 78, "endOffset": 167}, {"referenceID": 8, "context": "As starting point, these solutions use the rich representation of images generated by Convolutional Neural Networks (LeCun et al., 1998) (CNN) that were previously trained for object recognition tasks.", "startOffset": 116, "endOffset": 136}, {"referenceID": 2, "context": "These CNN are generally followed by recurrent neural networks (RNN) in order to generate full sentence descriptions (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014).", "startOffset": 116, "endOffset": 272}, {"referenceID": 17, "context": "These CNN are generally followed by recurrent neural networks (RNN) in order to generate full sentence descriptions (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014).", "startOffset": 116, "endOffset": 272}, {"referenceID": 5, "context": "These CNN are generally followed by recurrent neural networks (RNN) in order to generate full sentence descriptions (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014).", "startOffset": 116, "endOffset": 272}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem.", "startOffset": 34, "endOffset": 393}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose an encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional RNN over sentences and a structured objective responsible for a multimodal embedding.", "startOffset": 34, "endOffset": 659}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose an encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional RNN over sentences and a structured objective responsible for a multimodal embedding. They then propose a second RNN architecture to generate new sentences. Similarly, Mao et al. (2014) and Donahue et al.", "startOffset": 34, "endOffset": 909}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose an encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional RNN over sentences and a structured objective responsible for a multimodal embedding. They then propose a second RNN architecture to generate new sentences. Similarly, Mao et al. (2014) and Donahue et al. (2014) propose a system that uses a CNN to extract image features and a RNN for sentences.", "startOffset": 34, "endOffset": 935}, {"referenceID": 2, "context": ", 2014; Karpathy & Fei-Fei, 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Mao et al., 2014; Venugopalan et al., 2014; Kiros et al., 2014). Among these recent works, long short-term memory (LSTM) is often chosen as RNN. In such approaches, the key point is to learn a common space between images and words or between images and sentences, i.e. a multimodal embedding. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose an encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional RNN over sentences and a structured objective responsible for a multimodal embedding. They then propose a second RNN architecture to generate new sentences. Similarly, Mao et al. (2014) and Donahue et al. (2014) propose a system that uses a CNN to extract image features and a RNN for sentences. The two networks interact with each other in a multimodal common layer. Our model shares some similarities with these recent proposed approaches. We also use a pre-trained CNN to extract image features. However, thanks to the phrase-based approach, our model does not rely on complex recurrent networks for sentence generation, and we do not fine-tune the image features. As our approach, Fang et al. (2014) proposes to not use recurrent networks for generating the sentences.", "startOffset": 34, "endOffset": 1427}, {"referenceID": 3, "context": "We therefore proceed to a exploratory analysis of two recent datasets containing a large amount of images with descriptions: Flickr30k (Hodosh et al., 2013) and COCO (Lin et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 0, "context": "We consider a CNN that has been pre-trained for the task of object classification (Chatfield et al., 2014).", "startOffset": 82, "endOffset": 106}, {"referenceID": 14, "context": "Good word vector representations can be obtained very efficiently with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014).", "startOffset": 104, "endOffset": 204}, {"referenceID": 9, "context": "Good word vector representations can be obtained very efficiently with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014). Mikolov et al. (2013a) also showed that simple vector addition can often produce meaningful results, such as king man + woman \u2248 queen.", "startOffset": 105, "endOffset": 229}, {"referenceID": 0, "context": "Following Karpathy & Fei-Fei (2014), the image features are extracted using VGG CNN (Chatfield et al., 2014).", "startOffset": 84, "endOffset": 108}, {"referenceID": 13, "context": "We measure the quality of the generated sentences using the popular, yet controversial, BLEU score (Papineni et al., 2002).", "startOffset": 99, "endOffset": 122}, {"referenceID": 2, "context": "67 - - Donahue et al. (2014) 0.", "startOffset": 7, "endOffset": 29}, {"referenceID": 2, "context": "67 - - Donahue et al. (2014) 0.59 0.39 0.25 0.16 0.63 0.44 0.30 0.21 Fang et al. (2014) - - - - - - 0.", "startOffset": 7, "endOffset": 88}], "year": 2017, "abstractText": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.", "creator": "LaTeX with hyperref package"}}}