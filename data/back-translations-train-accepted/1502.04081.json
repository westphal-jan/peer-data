{"id": "1502.04081", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2015", "title": "A Linear Dynamical System Model for Text", "abstract": "Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words' local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words' representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple cooccurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.", "histories": [["v1", "Fri, 13 Feb 2015 18:39:29 GMT  (62kb,D)", "http://arxiv.org/abs/1502.04081v1", null], ["v2", "Sun, 31 May 2015 20:04:53 GMT  (68kb,D)", "http://arxiv.org/abs/1502.04081v2", "Accepted at International Conference of Machine Learning 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["david belanger", "sham m kakade"], "accepted": true, "id": "1502.04081"}, "pdf": {"name": "1502.04081.pdf", "metadata": {"source": "CRF", "title": "A Linear Dynamical System Model for Text", "authors": ["David Belanger", "Sham Kakade"], "emails": ["BELANGER@CS.UMASS.EDU", "SKAKADE@MICROSOFT.COM"], "sections": [{"heading": "1. Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a region and in which it is a country."}, {"heading": "2. Related Work", "text": "We offer a continuous analogy of popular discrete state generative models used in NLP to generate class membership for tokens, including class-based language models (Brown et al., 1992; Chelba & Jelinek, 2000) and induction of POS tags (Christodoulopoulos et al., 2010). In practice, however, Brown clusters (Brown et al., 1992) are often used by practitioners with many unlabeled in-domain data. Our learning algorithm is highly scalable because it operates on aggregated number matrices rather than on individual tokens. Similar algorithms have been proposed to obtain type-level embedding via matrix factorization (Pennington et al., 2014; Levy & Goldberg, 2014), which are context-independent and ignore the transition dynamics that incorporate tokens."}, {"heading": "3. Background: Gaussian Linear Dynamical Systems", "text": "A Gaussian LDS follows the following generative model (Kalman, 1960; Roweis & Ghahramani, 1999): xt = axe \u2212 1 + \u03b7 (1) wt = Cxt +, (2) where h < V is the dimensionality of the hidden states xt and \u0445 N (0, D). For simplicity's sake, we assume that x0 is constant. The latent space for x is completely unobserved, and we could choose any coordinate system for it, maintaining the same data probability. Therefore, without losing generality, we can fix either A = I or Q = I and Q. Furthermore, we note that the magnitude of the maximum eigenvalue of A must not be greater than 1 if the system is stable. We assume that the data to which we fit are specified as riririsky."}, {"heading": "3.1. Inference", "text": "The xt are distributed as multivariate Gaussian under the LDS before > J (depending on observations w =), so they can be fully characterized by a mean and a variance. Therefore, we will use x \u2212 t and St for the mean and the covariance below the back for the text w1: (t \u2212 1), calculated with Kalman filtering, and x \u2212 t and ST for xt taking into account the back part, taking into account all the data w1: T, calculated with Kalman smoothing. In Appendix B.1, we provide the complete filtering and smoothing of updates, which calculate different means and variances for each timeframe. Note that the updates must reverse a V \u2212 V \u2212 V-V matrix at each step. We deal with the widespread \"stationary\" approximation, which results in much more efficient filtering and smoothing of updates."}, {"heading": "3.2. Learning: Expectation-Maximization", "text": "See Ghahramani & Hinton (1996) for a complete description of learning the parameters of an LDS using EM. Assuming stationary, the M-step requires: E [x-tx-t], E [x-tx-t + 1], E [x-tw-t], (8) taking the expectation in terms of time and back side for the latent variables, which can be calculated using Kalman smoothing and then averaged over time. M-step can then be performed in closed form, since it solves backsteps for xt + 1 against xt and wt against xt to obtain A and C. Finally, D can be restored with: D = 0 \u2212 CE [x-tw > t] \u2212 E [wtx-x-t] C > + CE [x-x-x-t] C > (9)."}, {"heading": "3.3. Learning: EM with ASOS (Martens, 2010)", "text": "While these can be calculated using the Kalman smoothing of the entire training set, we are interested in data sets with billions of points in time. Fortunately, we can avoid smoothing by using Martens \"ASOS method (Approximate Second Order Statistics) (2010), which allows us to directly draw conclusions about time averaged second order statistics. Under the constant state assumption, this is feasible because we can recursively define relationships between second order statistics with delay k and delay k + 1 using the recursive evaluations of the underlying dynamic system."}, {"heading": "3.4. Learning: Subspace Identification", "text": "We initialize a method in which there is no statistical consistency, due to the specification of the data we apply. We initialize a method that is not statistically specified. \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"\" \"\" \"\" \"We.\" \"\" \"\" \"\" We. \"\" \"\" \"\" \"\". \"\" \"\" \"We.\" \"\" \"\". \"\" \"\" \"We.\". \"\" \".\" \"\" \"\" We. \"\" \"\". \"\" \"\" \"\".. \"\" \"..\" \"\".. \"\".. \"\" \"..\" \"..\" \"..\".. \"\".. \"\".. \"\" \"..\" \"\".. \"\".. \"\" \"..\" \"..\" \"\" \"..\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"..\" \"\".. \"\" \"..\" \"\" \"..\" \"\" \"..\" \"\".. \"\" \"..\" \"\" \"..\" \"\" \"..\" \"\" \"..\" \"\".. \"\" \"..\" \"..\" \""}, {"heading": "4. Linear Dynamical Systems for Text", "text": "In this section we first describe certain characteristics of the data distribution. Then we describe novel techniques for exploiting these characteristics to exploit scalable learning algorithms. Define w: t as an indicator vector, which is 1 in the index of word 1 and defines the frequency of the word."}, {"heading": "4.1. Scalable Spectral Decomposition", "text": "SSID requires a rank h SVD of the very large Block-Hankel matrix Hr (11). We use the randomized approximate SVD algorithm from Halko et al. (2011). To factorize a matrix X, this requires repeated multiplication with X and X >. All submatrices in Hr are sparse-minus-low-ranking, so we treat the sparse and low-ranking terms within the multiplication subroutines individually."}, {"heading": "4.2. Modeling Full-Rank Noise Covariance", "text": "The noise covariance matrix D is V \u00b7 V, which is unmanageably large for our application, and therefore it is reasonable to use a spherical D = dI or diagonal D = diag (d1,.., dV) approximation, but for our problem we found that these approximations performed poorly. Due to property 1 > wt = 0, extra-diagonal elements of D are crucial for modelling the anticorrelations between coordinates, which would have been captured if we had gone through a logistic multinomic link function, but this prevents simple conclusions using Kalman filters. To maintain conjugation, practitioners sometimes use the square upper limit for a logistic multinomic probability introduced in Boehning (1992), which encrypts the coordinate link function using the Kalman filter."}, {"heading": "4.3. Whitening", "text": "Before applying our learning algorithms, we first grind the \"matrices\" with the diagonal transformation.W = \"12 0 = diag\" (\"12 1,\"., 12 V. \"). (17) Adaptation to\" W \"kW >, instead of\" k, \"preserves the sparse\" minus \"-low\" and \"diagonally minus\" -low \"structures of the data. In addition,\" EM \"is unaffected, i.e. the application of\" EM \"to linearly transformed data is equivalent to learning from the original data and the subsequent transformation after the hoc. On the other hand, the SSID output is affected by\" whitening, \"since the square reconstruction loss that SVD implicitly minimizes depends on the coordinate system of the data. We have found such a brightening to be crucial for achieving high-quality starting parameters.\" Whitening \"for SSID,\" recommended by Van Overschee & De Moillor (1996), DXis applied as a very similar factor in the DX."}, {"heading": "5. Embedding Tokens using the LDS", "text": "The only data-dependent term in the stationary filter and smoothing equations (4) and (5) is Kwt. Since wt can only assume V possible values, we calculate these word-typical vectors in advance. The calculation costs for filtering / smoothing a length T sequence are O (Th2), which is identical to the cost of inferring to a discrete sequence model of the first order. (6) is not directly usable to obtain K due to the lack of data, and we offer an efficient alternative in Appendix A.2. This also requires the matrix inversion problem in order to avoid the instantiation of S \u2212 1 in (6). In our experiments, we use the latent to define characteristics for tokens. However, the distances in this space are not well defined, as the probability for any linear transformation of the latent is invariant. To place xt in reasonable coordinates, we calculate the empirical OS values with the help of the X."}, {"heading": "6. Relation to Recurrent Neural Networks", "text": "We will now emphasize the similarity between the parameterization of an RNN architecture commonly used for speech modeling and our Kalman filter, which allows us to use our LDS as a novel method of initializing the parameters of a nonlinear RNN, which we will examine in Section 7.4 below (Mikolov, 2012), looking at the network structure: ht = \u03c3 (Aht \u2212 1 + Bwt \u2212 1) (18) wt \u0445 SoftMax (Cht), (19) Here we will use the SoftMax transformation of a vector v as vi \u2192 exp (vi) / \u2211 k exp (vk). Coordinate nonlinearity is a sigmoid, for example, and the network will use a fixed vector h0.Consider the use of the stationary Kalman filter (4) as an online prediction instance, where the mean prediction for wt is given by Cx-t. Then, if we replace the identity with the max regime, we will replace the identity with the CA and the caldynamics regime."}, {"heading": "7. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. LDS Transition Dynamics", "text": "Many popular word embedding methods learn word-to-vector mapping, but do not learn the dynamics of text evolution in latent space. Using the specific LDS model we describe in the next section, we use the transition matrix A to explore the properties of this dynamics. Since the state evolution is linear, it can easily be studied by means of a spectral decomposition. A converts its left singular vectors into (scaled) right singular vectors. For each vector, we find the words most likely to be generated from this state. In Table 1, we present these singular vector pairs. We find that they reflect an interpretable transition dynamics."}, {"heading": "7.2. POS Tagging", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country and in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country, in which it is a country and in which it is a country."}, {"heading": "7.3. Named Entity Recognition", "text": "We are using the same LDS and Word2Vec models as in the previous section and are also comparing them with the Brown clusters used for NER in Ratinov & Roth (2009).As before, we note that Word2Vec and Word2Vec show significant improvements in accuracy over the baseline, and we expect that the reason LDS does not exceed Word2Vec is that NER mainly relies on performing local pattern comparisons, rather than capturing far-reaching discourse structures."}, {"heading": "7.4. RNN initialization", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "8. Conclusion and Future Work", "text": "These representations and the transition dynamics of the LDS parameters capture a useful syntactic and semantic structure. Our algorithm requires a single pass over the training data and no painful adjustment of learning rates. Next, we apply our LDS to demanding data such as Twitter and try to extend ASOS to new model structures. In addition, we will investigate improving the initialization of alternative RNN architectures, including those with hierarchical softmaxes, by applying not only the LDS parameters but also the LDS posterior to the training data."}, {"heading": "A. Scaling UP LDS Learning to Text", "text": "As discussed in Section 4.3, we need to adjust our data accordingly. In this particular case, we will seek the space in which we find ourselves. < p > p > p > p > p > p (\u00b5 \u2212 12 V). (20) In addition to improving the empirical performance of SSID, we can simplify various details used in Section 4 in scaling LDS learning processes for text. (20) In addition to improving the empirical performance of SSID \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p (12), this simplifies various steps because our estimators (12) and (9) are of the form I \u2212 [low rank matrix] instead of diag (\u00b5) \u2212 p (low rank matrix). In the whitened coordinates, the data is orthogonal to \u00b5 12, rather than 1.A.1. The processing of PSD in SSIDWhile SSID is consistent, for finite data the method is not guaranteed to provide a positive (SD), because it is required to provide a semidefined (SD) matrix."}, {"heading": "B. Background", "text": "\"We have to play by the rules,\" he said. \"We have to play by the rules,\" he said. \"We have to play by the rules,\" he said. \"We have to play by the rules,\" he said. \"We have to play by the rules.\""}, {"heading": "C. SSID Initialization v.s. Random Initialization", "text": "In Figure 2, we compare the progress of the EM in terms of the log probability of the training data when initializing with SSID v.s. by randomly initializing (random). Note that the initial values of SSID and random are nearly identical, due to the error specification of the model and the fact that we posthumously selected the length scales of the random parameters by looking at the length scales of the SSID parameters. Over the course of 100 EM iterations, the model initialized with SSID climbs quickly and begins to even out while it takes a long time for the random model to even begin climbing. We cap it to 100 EM iterations, because we actually use the SSID-initialized model after the 50th iteration. Afterwards, we find that the accuracy of the local POS marker decreases."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Low dimensional representations of words allow<lb>accurate NLP models to be trained on limited<lb>annotated data. While most representations ig-<lb>nore words\u2019 local context, a natural way to in-<lb>duce context-dependent representations is to per-<lb>form inference in a probabilistic latent-variable<lb>sequence model. Given the recent success of<lb>continuous vector space word representations,<lb>we provide such an inference procedure for con-<lb>tinuous states, where words\u2019 representations are<lb>given by the posterior mean of a linear dynam-<lb>ical system. Here, efficient inference can be<lb>performed using Kalman filtering. Our learn-<lb>ing algorithm is extremely scalable, operating<lb>on simple cooccurrence counts for both param-<lb>eter initialization using the method of moments<lb>and subsequent iterations of EM. In our exper-<lb>iments, we employ our inferred word embed-<lb>dings as features in standard tagging tasks, ob-<lb>taining significant accuracy improvements. Fi-<lb>nally, the Kalman filter updates can be seen as a<lb>linear recurrent neural network. We demonstrate<lb>that using the parameters of our model to ini-<lb>tialize a non-linear recurrent neural network lan-<lb>guage model reduces its training time by a day<lb>and yields lower perplexity.", "creator": "LaTeX with hyperref package"}}}