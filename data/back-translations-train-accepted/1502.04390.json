{"id": "1502.04390", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2015", "title": "Equilibrated adaptive learning rates for non-convex optimization", "abstract": "Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes, i.e., diagonal preconditioners. We show that the optimal preconditioner is based on taking the absolute value of the Hessian's eigenvalues, which is not what Newton and classical preconditioners like Jacobi's do. In this paper, we propose a novel adaptive learning rate scheme based on the equilibration preconditioner and show that RMSProp approximates it, which may explain some of its success in the presence of saddle points. Whereas RMSProp is a biased estimator of the equilibration preconditioner, the proposed stochastic estimator, ESGD, is unbiased and only adds a small percentage to computing time. We find that both schemes yield very similar step directions but that ESGD sometimes surpasses RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.", "histories": [["v1", "Sun, 15 Feb 2015 23:41:33 GMT  (433kb,D)", "http://arxiv.org/abs/1502.04390v1", null], ["v2", "Sat, 29 Aug 2015 23:04:39 GMT  (456kb,D)", "http://arxiv.org/abs/1502.04390v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yann dauphin", "harm de vries", "yoshua bengio"], "accepted": true, "id": "1502.04390"}, "pdf": {"name": "1502.04390.pdf", "metadata": {"source": "META", "title": "RMSProp and equilibrated adaptive learning rates for non-convex optimization", "authors": ["Yann N. Dauphin", "Harm de Vries", "Junyoung Chung", "Yoshua Bengio"], "emails": ["YANN@DAUPHIN.IO", "MAIL@HARMDEVRIES.COM", "ELECEGG@GMAIL.COM", "BENGIOY@IRO.MONTREAL.CA"], "sections": [{"heading": "1. Introduction", "text": "It is time for the EU Presidency to be able to find a solution that paves the way to the European Union."}, {"heading": "2. Preconditioning", "text": "The loss functions of neural networks exhibit notoriously pathological curvature (Martens, 2010).The loss surface may be strongly curved in some directions, while it is flat in others. This is problematic for gradient lineage, since a single learning rate is not suitable for all search directions. However, a high learning rate will be good for directions of low curvature, but will differ for those with high curvature. Some methods have recently been proposed to adjust a separate learning rate for each parameter, but they assume convexity of function (Zeiler, 2012; Duchi et al., 2011).It is not clear that applying these methods to neural networks will produce good outcomes.Preconditioning is a geometric solution to the problem of pathological curvature, which does not require convexity. Preconditioning aims to transform the optimization landscape locally so that its curvature is equal in all directions, as shown in Figure 1."}, {"heading": "3. Equilibration", "text": "Equilibrium calculation is a type of preconditioning that works well for indefinite matrices (Bradley & Murray, 2011).A matrix H is balanced by a series if all its rows have the same norm, which can be achieved by the diagonal preconditioning matrix that shows its relationship to the absolute value of the Hessian: Changing the order of the square root and diagonal gives us the diagonal of the absolute Hessian diag (\u221a diag (H2).As we discuss in Section 5, this link can help explain the success of the equilibrium matrices and clarify the relationship between the square root and the diagonal."}, {"heading": "4. RMSProp approximates equilibration", "text": "In this section we uncover an unexpected link between RMSProp and equilibrium factors. RMSProp is an adaptive learning rate method that has found much success in practice (Tieleman & Hinton, 2012; Korjus et al.; Carlson et al., 2015).Tieleman & Hinton (2012) propose to normalize the gradients by an exponential moving average of the order of magnitude of the gradients for each parameter (1 \u2212 \u03b1). (2), where 0 < 1) indicates the gradient rate.The update stage is given by indicating the number of gradients (1 + 2), where the learning rate and speed is a damping factor. Although it is observed in practice that the standardizing gradients support the optimization process, until this paper there was no published theoretical justification for RMSProp."}, {"heading": "5. The absolute Hessian is better for non-convex problems", "text": "First, we show that the absolute value of the Hessian is the only symmetrical positive definitive ideal prerequisite. Second, we show theoretically that the absolute value of the Hessian represents a better second order step than that of the Hessian. Third, we show that if you approach the Hessian to get curvature information, you consistently underestimate the curvature in a certain direction. This can lead to a completely wrong step. This problem is completely avoided by the absolute value of the Hessian."}, {"heading": "5.1. The symmetric positive-definite preconditioner", "text": "A preconditioning matrix D is considered ideal if it decreases the conditional number \u0432 (HD) = 1. Another useful property for a conditioner is to be positive \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 If the conditioner \u2212 \u2212 is not positive, then it can actually multiply the sign of the gradient by \u2212 1. \u2212 This can cause the optimizer to take steps that increase the target rather than decrease it. \u2212 \u2212 The absolute value of the Hessian is unique because it is the only symmetric positive definite predition.Proof. Theorem 1. Let H be a symmetric non-singular matrix in RN \u00b7 N. Then he notes that the matrix | H | \u2212 1 is the only symmetric positive ideal predition.Proof. According to the definition, the conditional number of the conditional number is given (AM) = 1 (AM) dibenetric prediality = 1 x = 1 M \u00b2 (M = 2 M = 1 M \u2212 x) where M = 1 m \u00b2 is a \u2212 x."}, {"heading": "5.2. Comparison to the Newton step", "text": "The fact that the absolute value of the Hessian national currency is positive and the Hessian national currency may be indeterminate has a direct effect on the optimization. Theoretically, the non-convex optimization is still a very open question. It is not clear how to show a faster convergence rate for a non-convex problem. However, we can show that in each step the absolute value of the Hessian national currency decreases equal to or more than the Newton step. This difference arises because an indeterminate Hessian step will lead to a wrong step towards negative curvature. The result is given by Theorem 2. Lemma 1. Let us be a non-singular square matrix in RN \u00b7 N, and x RN will be some vector."}, {"heading": "5.3. Better approximation of the curvature", "text": "As explained below, Krylov partial space approximations based on indeterminate Hessians tend not to underestimate curvature. Therefore, even for approximate methods, it is better to work with a quantity close to the absolute Hessian. Let's take the diagonal Jacobi approximationDJacobi = \u221a diag (H) 2.The elements DJacobii estimate the curvature of the Euclidean axis V = I. More precisely, they are equivalent to the Raleigh quotient in the direction of each axis DJacobii = | R (H, Vi).The elements DJacobii estimate the curvature of the Euclidean axis V = I. More precisely, they are equivalent to the eigenvalues and eigenvectors of H. We can see that the terms associated with negative eigenvalues cancel out the positive terms. This can make the curvature and the poor conditioning values in the proximity of the saturation points."}, {"heading": "6. Implementation", "text": "We propose to build a scalable algorithm for pre-conditioning neural networks using equilibrium factors. This method will estimate the same curvature information \u221a diag (H2) with the unbiased estimator described in Equation 1. It is prohibitive to calculate the full expectation with each learning step. Instead, we simply update our running average with each learning step similar to RMSProp. The pseudo-code is converted into Algorithm 1. The cost of this is a product with the Hessian, which roughly equals the cost of two additional gradient calculations and the cost of sampling a vector from a random Gaussian. In practice, we amortize the cost considerably by updating only all 20 iterations, bringing the cost of equilibrium very close to that of the regular SGD, which roughly equals the cost of two additional gradient calculations and the cost of sampling a vector from a random Gausage, all of which we derive significantly from one random Gausal practice."}, {"heading": "7. Experimental setup", "text": "We aim to experimentally confirm the theoretical results proposed in the previous sections. First, we measure how well RMSProp estimates the equilibrium matrix \u221a diag (H2); second, we evaluate the equilibrium SGD proposed in Algorithm 1. Finally, we want to confirm that there is a significant performance difference between the Jacobi preconditioner and equilibrium methods for high-dimensional non-convex problems. (Martens, 2010; Sutskever et al., 2013; Vinyals & Povey, 2011), we train deep auto-encoders that need to reconstruct their input under the constraint that a layer is very low-dimensional. This makes the reconstruction task difficult because it requires the optimizer to adjust the parameters accurately."}, {"heading": "8. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1. Measuring the bias of RMSProp", "text": "In Section 4, we argued that under certain conditions, RMSProp should not exceed the absolute Hessian diagonal. In this section, we verify to what extent these conditions are valid and experimentally determine the bias of RMSProp in estimating the equilibrium matrix. We train deep autoencoders with RMSProp and measure the equilibrium matrix Dequilibration = Diag (H2) andJacobi matrix DJacobi = Diag (H) 2 every 10 epochs using 100 samples of the unbiased estimators described in Equations 1. We then measure the paired differences between these quantities in relation to the cosmic distance Cosine (u, u \u00b7 v), which measures the angle between two vectors and ignores their norms."}, {"heading": "8.2. Comparison of preconditioned SGD methods", "text": "We present the optimization curves of the SGD and its preconditioned variants in Figure 5. We observe that the preconditioned methods outperform the SGD on both issues, and our results for the MNIST show that the proposed balanced SGD significantly outperforms both RMSProp and Jacobi SGD, and the performance difference becomes particularly noticeable after 250 epochs. The gap with the classical SGD is even greater, and we observe a convergence rate that is about three times faster. ESGD also performs best with CURVES, although the difference from RMSProp and Jacobi SGD is not as significant as with MNIST. These results correlate very well with the observation in the previous section. Figure 4 shows that the Jacobi is closer to the equilibrium in CURVES than the MNIST, which explains its narrower performance during optimization, and may be because the Hessian has fewer negative curves in the CURVES method."}, {"heading": "8.3. Measuring diagonal dominance", "text": "We have also measured the degree of diagonal dominance (H) for the preconditioned SGD methods. Remarkably, there is a significant degree of diagonal dominance, exceeding 15% for all optimization methods, which is greater than we would expect from a random symmetric matrix. For example, if all elements of a matrix were drawn independently of a distribution with zero mean and unit variance, then the expected degree of diagonal dominance E [d (H)] = 1 \u221a N \u00b7 100 where N is the dimension of the parameters. For the millions of parameters of our deep autoencoder, the expected degree of diagonal dominance is only 0.01%. Hessian is thus clearly diagonally dominant, which is good news for diagonally preconditioned SGD methods, since most disease-related conditions can then be removed by a portion of the diagonal dominance."}, {"heading": "9. Conclusion", "text": "We have proposed a new preconditioning method for neural networks called Equilibrated SGD (ESGD), which is closely related to RMS Prop. We have shown that this method approaches the absolute value of Hessian and performs better in non-convex problems."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "An estimator for the diagonal of a matrix", "author": ["Bekas", "Costas", "Kokiopoulou", "Effrosyni", "Saad", "Yousef"], "venue": "Applied numerical mathematics,", "citeRegEx": "Bekas et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bekas et al\\.", "year": 2007}, {"title": "Matrixfree approximate equilibration", "author": ["Bradley", "Andrew M", "Murray", "Walter"], "venue": "arXiv preprint arXiv:1110.2805,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Stochastic spectral descent for restricted boltzmann machines", "author": ["Carlson", "David", "Cevher", "Volkan", "Carin", "Lawrence"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2015}, {"title": "The loss surface of multilayer", "author": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "Grard Ben", "LeCun", "Yann"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in highdimensional non-convex optimization", "author": ["Dauphin", "Yann", "Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Ganguli", "Surya", "Bengio", "Yoshua"], "venue": "In NIPS\u20192014,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "In ICML\u20192010, pp", "citeRegEx": "Martens,? \\Q2010\\E", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "Unit tests for stochastic optimization", "author": ["Schaul", "Tom", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1312.6055,", "citeRegEx": "Schaul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2013}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Schraudolph", "Nicol N"], "venue": "Neural Computation,", "citeRegEx": "Schraudolph and N.,? \\Q2002\\E", "shortCiteRegEx": "Schraudolph and N.", "year": 2002}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Krylov subspace descent for deep learning", "author": ["Vinyals", "Oriol", "Povey", "Daniel"], "venue": "arXiv preprint arXiv:1111.4259,", "citeRegEx": "Vinyals et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2011}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "One of the questions regarding SGD is how to set the learning rate adaptively, both over time and for different parameters, and several methods have been proposed (Schaul et al., 2013), including the RMSProp method studied in this paper.", "startOffset": 163, "endOffset": 184}, {"referenceID": 5, "context": "On the other hand, recent work (Dauphin et al., 2014; Choromanska et al., 2014) has brought theoretical and empirical evidence suggesting that local minima are with high probability not the main obstacle to optimizing large and deep neural networks, contrary to what was previously believed: instead, saddle points are the most prevalent critical points on the optimization path (except when we approach the value of the global minimum).", "startOffset": 31, "endOffset": 79}, {"referenceID": 4, "context": "On the other hand, recent work (Dauphin et al., 2014; Choromanska et al., 2014) has brought theoretical and empirical evidence suggesting that local minima are with high probability not the main obstacle to optimizing large and deep neural networks, contrary to what was previously believed: instead, saddle points are the most prevalent critical points on the optimization path (except when we approach the value of the global minimum).", "startOffset": 31, "endOffset": 79}, {"referenceID": 5, "context": "This last result also serves as a justification for the method presented by Dauphin et al. (2014), which however involved ar X iv :1 50 2.", "startOffset": 76, "endOffset": 98}, {"referenceID": 8, "context": "The loss functions of neural networks notoriously exhibit pathological curvature (Martens, 2010).", "startOffset": 81, "endOffset": 96}, {"referenceID": 6, "context": "Some methods have recently been proposed to tune a separate learning rate for each parameter but they assume convexity of the function (Zeiler, 2012; Duchi et al., 2011).", "startOffset": 135, "endOffset": 169}, {"referenceID": 7, "context": "There has been some success in applying this preconditioner to neural networks using a Gauss-Newton approximation of the Hessian (LeCun et al., 2012).", "startOffset": 129, "endOffset": 149}, {"referenceID": 5, "context": ", for saddle points) on the training trajectory of deep neural networks is both theoretical and empirical (Dauphin et al., 2014; Choromanska et al., 2014).", "startOffset": 106, "endOffset": 154}, {"referenceID": 4, "context": ", for saddle points) on the training trajectory of deep neural networks is both theoretical and empirical (Dauphin et al., 2014; Choromanska et al., 2014).", "startOffset": 106, "endOffset": 154}, {"referenceID": 3, "context": "RMSProp is an adaptative learning rate method that has found much success in practice (Tieleman & Hinton, 2012; Korjus et al.; Carlson et al., 2015).", "startOffset": 86, "endOffset": 148}, {"referenceID": 3, "context": "; Carlson et al., 2015). Tieleman & Hinton (2012) propose to normalize the gradients by an exponential moving average of the magnitude of the gradient for each parameter:", "startOffset": 2, "endOffset": 50}, {"referenceID": 5, "context": "Thus RMSProp can be thought as a biased estimator of equilibration and this may explain some of its success in optimizing in the presence of negative eigenvalues of the Hessian, as is the case when training neural networks (Dauphin et al., 2014; Choromanska et al., 2014).", "startOffset": 223, "endOffset": 271}, {"referenceID": 4, "context": "Thus RMSProp can be thought as a biased estimator of equilibration and this may explain some of its success in optimizing in the presence of negative eigenvalues of the Hessian, as is the case when training neural networks (Dauphin et al., 2014; Choromanska et al., 2014).", "startOffset": 223, "endOffset": 271}, {"referenceID": 5, "context": "This formalizes the intuitions of (Dauphin et al., 2014).", "startOffset": 34, "endOffset": 56}, {"referenceID": 5, "context": "This is problematic because of the proliferation of saddle points in neural networks (Dauphin et al., 2014).", "startOffset": 85, "endOffset": 107}, {"referenceID": 7, "context": "LeCun et al. (2012) proposed a Gauss-Newton approximation to the Jacobi preconditioner for neural networks.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Bekas et al. (2007) show that the diagonal of a matrix can be recovered by the expression", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Following (Martens, 2010; Sutskever et al., 2013; Vinyals & Povey, 2011), we train deep auto-encoders which have to reconstruct their input under the constraint that one layer is very low-dimensional.", "startOffset": 10, "endOffset": 72}, {"referenceID": 11, "context": "Following (Martens, 2010; Sutskever et al., 2013; Vinyals & Povey, 2011), we train deep auto-encoders which have to reconstruct their input under the constraint that one layer is very low-dimensional.", "startOffset": 10, "endOffset": 72}, {"referenceID": 8, "context": "We use the standard network architectures described in (Martens, 2010) for the MNIST and CURVES dataset.", "startOffset": 55, "endOffset": 70}, {"referenceID": 8, "context": "The networks are initialized using the sparse initialization described in (Martens, 2010).", "startOffset": 74, "endOffset": 89}, {"referenceID": 0, "context": "The networks and algorithms were implemented using Theano (Bastien et al., 2012), simplifying the use of the Roperator in Jacobi and equilibrated SGD.", "startOffset": 58, "endOffset": 80}], "year": 2015, "abstractText": "Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes, i.e., diagonal preconditioners. We show that the optimal preconditioner is based on taking the absolute value of the Hessian\u2019s eigenvalues, which is not what Newton and classical preconditioners like Jacobi\u2019s do. In this paper, we propose a novel adaptive learning rate scheme based on the equilibration preconditioner and show that RMSProp approximates it, which may explain some of its success in the presence of saddle points. Whereas RMSProp is a biased estimator of the equilibration preconditioner, the proposed stochastic estimator, ESGD, is unbiased and only adds a small percentage to computing time. We find that both schemes yield very similar step directions but that ESGD sometimes surpasses RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.", "creator": "LaTeX with hyperref package"}}}