{"id": "1502.04585", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2015", "title": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions", "abstract": "The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission.", "histories": [["v1", "Mon, 16 Feb 2015 15:53:03 GMT  (90kb,D)", "http://arxiv.org/abs/1502.04585v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["avrim blum", "moritz hardt"], "accepted": true, "id": "1502.04585"}, "pdf": {"name": "1502.04585.pdf", "metadata": {"source": "CRF", "title": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions", "authors": ["Avrim Blum", "Moritz Hardt"], "emails": [], "sections": [{"heading": null, "text": "In this thesis, we introduce a concept of ranking accuracy tailored to the format of a competition. We introduce a natural algorithm called \"ladder\" and show that it simultaneously supports strong theoretical guarantees in a fully adaptive estimation model, withstands practical hostile attacks, and has a high benefit for real submission files of one of Kaggle.It is noteworthy that we are able to circumvent a strong current hardness result for adaptive risk assessment that excludes algorithms like ours under a seemingly very similar concept of accuracy. In practice, we provide a completely parameter-free variant of our algorithm that can be used in a real contest without any necessary coordination."}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves, and they will be able to play by the rules they have imposed on themselves."}, {"heading": "1.1 Our Contributions", "text": "We introduce an idea of accuracy, called ranking accuracy, which is tailored to the format of a competition. Intuitively, a high ranking accuracy results in each result represented on the ranking being close to the true score of the corresponding classifier on the unknown distribution from which the data was drawn. Our primary theoretical contributions are the following. We show that there is a simple and natural algorithm we call ladder that achieves a high ranking accuracy in a fully adaptive estimation model, in which we do not place any limitations on the data analyst. In fact, we do not even limit the number of submissions an analyst can make. Formally, our worst-case limit shows that when we normalize the results to be in [0.1] the maximum error of our algorithm on each estimate is never worse than O (log) / n), where k is the number of submissions of an analyst and n the size of the data used to calculate the ranking."}, {"heading": "1.2 Related Work", "text": "There is a great deal of literature on preventing overfitting in the context of model evaluation and selection. See, for example, Chapter 7 of [HTF] for background. Two particularly popular practical approaches are different forms of cross-validation and bootstrapping. However, it is important to note that none of these techniques is applicable when evaluating a template for the leaderboard. One problem is that participants only submit a list of labels and not the corresponding learning algorithms. In particular, the organizer of the competition has no means to retrain the model on a different distribution of data. Likewise, the natural bootstrap estimate of the expected loss of a classifier in the face of a finite sample simply requires the empirical average of the loss of the finite sample, which are already existing solutions. The other significant obstacle is that even if these methods are used, their theoretical guarantees in the adaptive adjustment of the estimate are largely not undervalued.A highly relevant recent work [DFH] inspires us, a more general question:"}, {"heading": "Acknowledgments", "text": "We thank Ben Hamner of Kaggle Inc., who provided us with the photo quality contest submission files, as well as many helpful discussions. We are grateful to Aaron Roth for pointing out an argument similar to that used in Proof of Theorem 3.1 in a different context. We thank John Duchi for many stimulating discussions."}, {"heading": "1.3 Preliminaries", "text": "A loss function is a figure of the form ': Y \u00b7 Y \u2192 [0,1] and a classifier is a figure f: X \u2192 Y. A standard loss function is the 0 / 1 loss function, which is defined as' 01 (y, y \u2032) = 1 if y, y \u2032 and 0 are different. We assume that we are given a sample S = {(x1, y1),..., (xn, yn)} drawn from an unknown distribution D over X \u00d7 Y. We define the empirical loss of a classifier f on the sample S asRS (f) def = 1 n \u00b2 (f (xi), yi).1The parameter corresponds to the depth of the tree loss, assuming that the value of the tree loss f (f) (f), the value of the tree loss f (f), (f), (f), (f), (n), yi).1The parameter corresponds to the depth of the tree loss (f)."}, {"heading": "2 Sequential and Adaptive Loss Estimation", "text": "In this section, we formally define the adaptive model of estimation in which we work and present our definition of the accuracy of the ranking. In view of a sequence of classifiers f1,... fk and a finite sample S of size n, a fundamental estimation problem is to calculate estimates R1,... Rk in such a way that Pr {t [k]: | Rt \u2212 RD (ft) | > \u03b5} 6 \u03b4. (1) The standard method for estimating the true loss is empirical loss. If we assume that all functions f1,.. fk are fixed independently of the sample S, then Hoeffdings bound and the union-bound implyPr \u2212 t [k]: | RS (ft) \u2212 RD (ft) - 6 2k exp (\u2212 2k exp). (\u2212 2) In the adaptive setting, however, we assume that the classifier ft will be selected as a function of the previous estimates and that the previously selected formation A = 1 exists."}, {"heading": "2.1 Leaderboard Accuracy", "text": "The aim of an accurate ranking is to ensure that at each step t 6 k accurately reflects the best classifier among these classifiers f1,.., fk that has been submitted so far. In other words, while we do not need an accurate estimate for each foot, we would like to maintain that the t-th estimate Rt accurately reflects the minimal loss achieved by each classifier to date. This leads to the following definition. Definition 2.1. In view of an adaptively chosen order of classifiers f1,..., fk, we define the ranking error of the estimates R1,..., Rk aslberr (R1,.., Rk) def = max16t6k,. min16i6tRD (fi) \u2212 Rt,. (3) Given an algorithm that achieves high ranking accuracy, there are two simple ways to extend it to provide a complete ranking."}, {"heading": "3 The Ladder Mechanism", "text": "We present an algorithm called the conductor mechanism, which achieves a small guiding accuracy (1 / 3). The algorithm is very simple. For each given function, it compares the empirical loss estimate of the function with the previously lowest loss. If the estimate is below the previous best value, the estimate and update of the best estimate (1 / 2) is published, if the estimate is not lower, the algorithm releases the previous best loss (instead of the new estimate). A formal description follows in Figure 1.Theorem 3.1. For each sequence of adaptively selected classifiers f1,. fk, the charging mechanism meets 6 k for all t and the height > 0."}, {"heading": "3.1 A lower bound on leaderboard accuracy", "text": "This is true even if the functions are not adaptively selected but fixed ahead of time. (D) There are indeed classifiers f1,.. fk and a limited loss function for which we have the minimum lower limit. (D) Sup D [lberr] (R (x1,.) In fact, there are classifiers f1,. fk and a limited loss function for which we have the minimum lower limit. (D) Sup D [lberr] (R (x1,.,.)]] In fact, the infimum is taken across all estimators. (R: Xn \u2192 [0,1] k, which takes the samples from a distribution. D and produces k estimates R1,.,.,. (xn) The expectation is reduced by n samples from D.Proof. We will reduce the problem of average estimation in a particular high-dimensional distribution family to this small deviation."}, {"heading": "4 A parameter-free Ladder mechanism", "text": "When applying the conductor mechanism in practice, it can be difficult to select a fixed step variable \u03b7 in advance that will work throughout the competition. Therefore, we are now giving a completely parameter-free version of our algorithm that we will use in our experiments.The algorithm will adaptively find a suitable step variable based on previous inputs to the algorithm. The idea is to perform a statistical significance test to assess whether the given input improves over the previous one. The test is such that the best classifier becomes more and more accurate, the step size shrinks accordingly. The empirical loss of a classifier is the average of n limited numbers and follows a very accurate normal approximation for sufficiently large n \u2212 as long as the loss does not tend too much towards 0. In our setting, the typical loss when delimited from the form 0 is so that the normal approximation to a particular algorithm is reasonable. To test whether the empirical loss of a classifier is significantly lower than the empirical one."}, {"heading": "4.1 Remark on the interpretation of the significance test", "text": "The test performed in our algorithm at each step corresponds to the refutation of the null hypothesis approximately at the significance level of 0.15. However, it is important to note that our use of this significance test is primarily heuristic, since for t > 1, due to the adaptive decisions of the analyst, the functional ft cannot generally be independent of the sample S. In such a case, the student approximation is no longer valid. Moreover, we apply the test many times, but do not control it for multiple comparisons. Nevertheless, the significance test is an intuitive guide to decide which improvements are statistically significant."}, {"heading": "5 The boosting attack", "text": "In this section, we describe a new canonical attack that an opposing analyst could carry out to improve his ranking on the public rankings. In addition to being practical in some cases, it also serves as an analytical tool to assess the accuracy of concrete mechanisms. For the sake of simplicity, we describe the attack only for the 0 / 1 loss, although it generalizes to other reasonable functions such as clipped logarithmic loss frequently used by Kaggle. We assume that the hidden solution is a vector y. The analyst can submit a vector u {1} n and (up to a small error) the loss of '01 (y, u) def = 1' 01 (ui, yi) The attack proceeds as follows: 1. Pick u1,."}, {"heading": "5.1 Experiments with the boosting attack", "text": "Figure 3 compares the performance of the conductor mechanism with that of the standard Kaggle mechanism under the boosting attack. We chose N = 12000 as the total number of labels, of which n = 4000 labels will be used to determine the public ranking under both mechanisms. Other parameter settings result in a similar picture, but these settings roughly correspond to the properties of the real data set that we will analyze later. The Kaggle mechanism provides answers up to a rounding error of 10 \u2212 5. Note that 1 / 4000 \u2248 0.0158, so the rounding error is well below the critical level of 1 / \u221a n. The vector y in the description of our attack corresponds to the 4000 labels used for the public ranking. Since the answers given by Kaggle depend only on these labels, the remaining labels play no role in the attack. It is important that the attack on the vectors is not known as the vector index of the public ranking list for the entire vector that is used within the vector."}, {"heading": "6 Experiments on real Kaggle data", "text": "To demonstrate the usefulness of the ladder mechanism, we turn to real submission data from Kaggle's \"Photo Quality Forecast.\" 2 Here is some basic information about the Competition.Number of test samples 12,000 - used for the private Leaderboard 8400 - used for the public Leaderboard 3600 Number of submissions 1830 - successfully processed 1785 Number of teams 200Our first experiment was to apply the parameter-free ladder mechanism instead of the Kaggle mechanism across all 1785 submissions. 2https: / www.kaggle.com / c / PhotoQualityPredictionFigure 4 turns out that the resulting rankings are very close to those calculated by Kaggle. Table 1 shows the only glitches in ranking among the top 10 submissions. 2https: / www.kaggle.com / c / PhotoQualityPredictionFigure 4 shows the public versus the private results of the leading 50 submissions in the ranking line (see the public)."}, {"heading": "6.1 Statistical significance analysis", "text": "To get a better sense of the statistical significance of the difference between the results of competing submissions, we performed a sequence of significance tests. Specifically, we looked at the top 10 submissions taken from the Kaggle public ranking and tested on the basis of private data, when the true value of the top submission differs significantly from the rank r submission for r = 2.3,... A suitable significance test is the paired t test. The value of a submission is the mean of a large number of samples at intervals [0.2] and follows a sufficiently accurate normal approximation. We chose a paired t test instead of an unpaired t test because it has much greater statistical power in our environment. This is primarily due to the strong correlation between competing submissions. See Equation 6 for a definition of test statistics. Note that the data used to determine the selection of the top 10 classifiers are independent of the ones used to perform the signatures."}, {"heading": "7 Conclusion", "text": "Beyond the scope of machine learning competitions, it is conceivable that the conductor mechanism could be useful in other areas where overfitting is currently a concern. For example, in the context of false discoveries in the empirical sciences [Ioa, GL], one could imagine using the conductor mechanism as a method to monitor scientific progress in important public datasets. Our algorithm can also be seen as an intuitive explanation of why overfitting the holdout is sometimes not a major problem even in an adaptive environment. Indeed, if every analyst only uses the holdout system to test whether their latest template is significantly above previous records, then they effectively simulate our algorithm. A nice theoretical problem is the resolution of the gap between our upper and lower limits. On the practical side, it would be interesting to use the conductor mechanism in a real competition."}, {"heading": "A Kaggle reference mechanism", "text": "As with the conductor mechanism, we describe the algorithm as if the analyst were submitting classifiers f: X \u2192 Y. In reality, the analyst only provides a list of labels. It is easy to see that such a list of labels is sufficient to calculate the empirical loss, which is all the algorithm needs to do. Input quantity S in the description of our algorithm corresponds to the amount of data points (and corresponding labels) that Kaggle uses for the public ranking."}], "references": [{"title": "Preserving statistical validity in adaptive data analysis", "author": ["DFH+] Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth"], "venue": "In Proc. 47th Symposium on Theory of Computing (STOC). ACM,", "citeRegEx": "Dwork et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2014}, {"title": "The garden of forking paths: Why multiple comparisons can be a problem, even when there is no \u201cfishing expedition", "author": ["Andrew Gelman", "Eric Loken"], "venue": "or \u201cp-hacking\u201d and the research hypothesis was posited ahead of time,", "citeRegEx": "Gelman and Loken.,? \\Q2013\\E", "shortCiteRegEx": "Gelman and Loken.", "year": 2013}, {"title": "Has\u2019minskii. A lower bound on the risks of nonparametric estimates of densities in the uniform metric", "author": ["Z. R"], "venue": "Theory of Probability and Applications,", "citeRegEx": "R.,? \\Q1978\\E", "shortCiteRegEx": "R.", "year": 1978}, {"title": "The Elements of Statistical Learning", "author": ["HTF] Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Preventing false discovery in interactive data analysis is hard", "author": ["HU] Moritz Hardt", "Jonathan Ullman"], "venue": "In Proc. 55th Foundations of Computer Science (FOCS),", "citeRegEx": "Hardt and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Ullman.", "year": 2014}, {"title": "Why Most Published Research Findings Are False", "author": ["Ioa] John P.A. Ioannidis"], "venue": "PLoS Medicine,", "citeRegEx": "Ioannidis.,? \\Q2005\\E", "shortCiteRegEx": "Ioannidis.", "year": 2005}, {"title": "Interactive fingerprinting codes and the hardness of preventing false discovery", "author": ["SU] Thomas Steinke", "Jonathan Ullman"], "venue": "CoRR, abs/1410.1228,", "citeRegEx": "Steinke and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Steinke and Ullman.", "year": 2014}, {"title": "Introduction to Nonparametric Estimation", "author": ["Tsy] Alexandre B. Tsybakov"], "venue": "Springer, 1st edition,", "citeRegEx": "Tsybakov.,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov.", "year": 2008}], "referenceMentions": [], "year": 2015, "abstractText": "The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from an actual competition hosted by Kaggle. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.", "creator": "LaTeX with hyperref package"}}}