{"id": "1502.04681", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2015", "title": "Unsupervised Learning of Video Representations using LSTMs", "abstract": "We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (\"percepts\") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.", "histories": [["v1", "Mon, 16 Feb 2015 20:00:07 GMT  (2381kb,D)", "http://arxiv.org/abs/1502.04681v1", null], ["v2", "Tue, 31 Mar 2015 23:45:59 GMT  (2373kb,D)", "http://arxiv.org/abs/1502.04681v2", "Fixed incorrect figure in Figure 6"], ["v3", "Mon, 4 Jan 2016 00:42:07 GMT  (2373kb,D)", "http://arxiv.org/abs/1502.04681v3", "Added link to code on github"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["nitish srivastava", "elman mansimov", "ruslan salakhutdinov"], "accepted": true, "id": "1502.04681"}, "pdf": {"name": "1502.04681.pdf", "metadata": {"source": "META", "title": "Unsupervised Learning of Video Representations using LSTMs", "authors": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "emails": ["NITISH@CS.TORONTO.EDU", "EMANSIM@CS.TORONTO.EDU", "RSALAKHU@CS.TORONTO.EDU"], "sections": [{"heading": "1. Introduction", "text": "Lately, recurring neural networks using the Long Short Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been successfully used to perform various monitored sequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machine translation (Sutskever et al., 2014; Choet al., 2014) and caption generation (Vinyals et al., 2014); they have also been applied to videos to detect actions and generate natural language descriptions (Donahue et al., 2014); a general sequence learning has been described by Sutskever et al. (2014) in which a recurring network is used to encode a sequence into a fixed length representation, and then another recursive network is used to decode a sequence from that representation. In this work, we apply and expand this framework to learn images representation."}, {"heading": "1.1. Why Unsupervised Learning?", "text": "Therefore, it is only natural to extend the same approach to video representation and explore different ways to render visual information in revolutionary networks (Simonyan & Zisserman, 2014). However, videos are much more dimensional units compared to frames. Therefore, it becomes increasingly difficult to lend and explore far-reaching structures unless we collect much more labeled data or do a lot of feature engineering (for example, calculating the right types of flow features) to obtain the ar Xiv: 150 2.04 681v 1 [cs.L G] 16 Feb 2015 as parent videos or perform a lot of feature engineering to get the right types of flow features."}, {"heading": "1.2. Our Approach", "text": "In this paper, we use the LSTM encoder decoder framework to learn video representation, but the same inductive bias must be applied at each step to disseminate information about the next step, forcing the fact that the physics of the world remains the same regardless of input, and the same physics that affects any state must produce the next state. Our model works as follows: The LSTM encoder runs through a sequence of frames to produce a representation, which is then decrypted by another LSTM to produce a target sequence."}, {"heading": "1.3. Related Work", "text": "The first approaches to unattended video representation were based on ICA (van Hateren & Ruderman, 1998; Hurri & Hyva \ufffd rinen, 2003).Le et al. (2011) approached this problem with several levels of modules of independent sub-space analysis. Generative models for understanding transformations between pairs of consecutive images are also well studied (Memisevic, 2013; Memisevic & Hinton, 2010; Susskind et al., 2011).This work was recently expanded by Michalski et al. (2014) to model longer sequences. Recently, Ranzato et al. (2014) proposed a generative model for video, which uses a recurring neural network to predict the next frame or interpolate between frames. In this paper, the authors emphasize the importance of choosing the correct loss function. It is argued that square loss in the input space is not the right target because it does not respond well to small distortions in the input space."}, {"heading": "2. Model Description", "text": "In this section we describe several variants of our LSTM encoder decoder model. The base unit of our network is the LSTM cell block. Our implementation of LSTMs closely follows that discussed by Graves (2013)."}, {"heading": "2.1. Long Short Term Memory", "text": "In this section we briefly describe the LSTM unit, which is the basic building block of our model. (1) The unit is shown in Fig. (1) (1). Each LSTM unit has a cell that has a state that can be thought of as a storage unit at the time of activation. (1) Access to this storage unit for reading or modifying memory cells is controlled by sigmoid gates, is better at finding and exploiting long-distance dependencies in the data. (2) The version of the LSTM memory cell is implemented in this paper. (7) H is implemented by the following composite function: it is the cell (Wxixt + Whiht \u2212 1)."}, {"heading": "2.2. LSTM Autoencoder Model", "text": "rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rf"}, {"heading": "2.3. LSTM Future Predictor Model", "text": "The design of the Future Predictor model is similar to that of the autoencoder model, except that in this case the LSTM decoder predicts frames of the video that come after the input sequence (Fig. 3). Ranzato et al. (2014) use a similar model, but predict only the next frame at each time step. On the other hand, this model predicts a long sequence into the future. Here, too, we can look at two variants of the decoder - conditional and unconditional. Why should this learn good properties? In order to be able to correctly predict the next frames, the model needs information about what objects and backgrounds are present and how they move, so that the motion can be extrapolated."}, {"heading": "2.4. Conditional Decoder", "text": "For each of these two models, we can consider two possibilities - one in which the LSTM decoder is bound to the last frame generated, and the other in which it is not. In the experimental section, we examine these choices quantitatively. Here, we briefly discuss arguments for and against a conditional decoder. A strong argument for using a conditional decoder is that it allows the decoder to model several modes in the target sequence distribution. Without this method, we would end up calculating the different modes in the low input space. However, this is only a problem if we expect several modes in the target sequence distribution. For the LSTM autoencoder, there is only one correct target target and thus an unimodale target distribution, but for the LSTM Future Predictor, there is the possibility that multiple targets will receive input because we assume that a deterministic universe, everything necessary to predict the future, is not necessarily observed in the input."}, {"heading": "2.5. A Composite Model", "text": "The two tasks - reconstructing the input and predicting the future - can be combined to create a composite model, as shown in Fig. 4. Here, the LSTM encoder is asked to find a state from which we can both predict the next images and reconstruct the inputs. This composite model attempts to overcome the shortcomings that each model suffers on its own. A powerful autoencoder would suffer from the tendency to learn trivial representations that only remember the inputs. However, this storage is not at all useful for predicting the future. Therefore, the composite model cannot simply be memo-unattended learning with LSTMsrize information. On the other hand, the future forecaster suffers from the tendency to store information only about the last few frames, as these are most important for predicting the future, i.e., to forget the frames {vt \u2212 1,.., vt \u2212 k} are much more important than we can forget the value at the end of the last frame."}, {"heading": "3. Experiments", "text": "We design experiments to achieve the following objectives: \u2022 Obtain a qualitative understanding of what LSTM learns. \u2022 Measure the benefits of initializing networks for supervised learning tasks with the weights found through unattended learning, especially with very few training examples. \u2022 Compare the various models proposed - Autoencoder, Future Predictor and Composite models and their conditional variants. \u2022 Compare the current standards for action detection."}, {"heading": "3.1. Datasets", "text": "We use the UCF-101 and HMDB-51 datasets for verified tasks instead of collecting them. UCF-101 dataset (Soomro et al., 2012) contains 13,320 videos with an average length of 6.2 seconds, which belong to 101 different action categories. However, the dataset has 3 standard moves / test splitters, each with around 9,500 videos in each split (the rest is a test). HMDB-51 dataset (Kuehne et al., 2011) contains 5100 videos belonging to 51 different action categories. The length of the videos is 3.2 seconds. This also has 3 pull / test splitters with 3570 videos in the training set and rest in the tests.To train the unverified models, we used a subset of Sport-1M datasets."}, {"heading": "3.2. Visualization and Qualitative Analysis", "text": "This year it is more than ever before."}, {"heading": "3.3. Action Recognition on UCF-101/HMDB-51", "text": "The aim of this series of experiments is to see if the features learned through unattended learning can help improve the performance of the monitored tasks. We have trained a two-layer composite model with 2048 hidden units without conditioning on both decoders. The model was trained on perceptions extracted from 300 hours of YouTube data. The model was trained to auto-encode 16 frames and predict the next 13 frames. We initialize an LSTM classifier with the weights learned from this model by the LSTM encoder. The classifier is shown in Fig. 11. The output from each LSTM in the second layer goes into a softmax classifier, which makes a prediction of the action to be performed at each time step. Since only one action is performed in each video in the datasets we are looking at, the goal is the same at each time step."}, {"heading": "W (2) W (2) W (2)", "text": "For HMDB-51, the improvement is from 42.8% to 44.0% for the entire dataset (70 videos per class) and 14.4% to 19.1% for one video per class. Although the improvement in classification through the use of unattended learning was not as great as we expected, we still managed to achieve additional improvement through a strong baseline. We discuss some possible improvements later. We conducted similar experiments on the optical flow data extracted from the UCF-101 dataset. A revolutionary time stream network similar to that of Simonyan & Zisserman (2014b) was trained on individual optical flows and stacks of 10 optical flows, giving an accuracy of 72.2% and 77.5%, respectively. LSTMs with 128 hidden units trained on sequences of 4 frames improved accuracy by 2.1% to 74.3%."}, {"heading": "3.4. Comparison of Different Model Variants", "text": "The aim of this series of experiments is to compare the different variants of the model proposed in this paper. Since it is always possible to obtain lower reconstruction errors by copying the input, we cannot use the reconstruction error as a measure of how well a model performs. However, Model Cross Entropyon MNIST Squared loss on image patches Future Predictor 350.2 225.2 Composite Model 344.9 210.7 Conditional Future Predictor 343.5 22221.3 Composite Model with Conditional Future Predictor 341.2 208.1Table 2. Future prediction results on MNIST and image patches. All models use 2 layers of LSTMs.we can use the error in predicting the future as a reasonable measure of how well the model performs. In addition, we can use performance on monitored tasks as a proxy for how well the unattended model performs."}, {"heading": "3.5. Comparison with Other Action Recognition Benchmarks", "text": "The performance is summarized in Table 4. The table is divided into three sets: the first set compares models that only use RGB data (single or multiple images); the second set compares models that only use explicitly calculated flow functions. Models in the third set use both. In terms of RGB data, our model performs equally well with the best deep models; it performs 3% better than the LRCN model, which also uses LSTMs based on conventional flows.1 Our model performs better than C3D functions that use a 3D conventional mesh. However, when the C3D functions are associated with fc6 perceptions, they perform slightly better than our model.The improvement in flow functions through the use of a randomly initialized LSTM network is quite small. We believe that this is at least in part due to the fact that the flow functions already capture a lot of the motion information that the STM would otherwise uncover on the basis of 8GB models and the 8GB improvement of both."}, {"heading": "4. Conclusions", "text": "We suggested models based on LSTMs that can learn good video representation, compared them and analyzed their properties through visualizations, and we also managed to achieve an improvement in monitored tasks, the most powerful of which was the composite model, which combined an autoencoder and a future predictor. Conditioning generated results did not have a significant impact on unattended learning with LSTM performance for monitored tasks, but did make future predictions look a little better. It was able to persistently generate motion well beyond the time scale for which it was trained, but it lost accurate object characteristics quickly after the training time scale, and the features at the input and output level showed some interesting characteristics. In order to achieve further improvements for monitored tasks, we believe that the model can be expanded by applying it in a revolutionary manner to spots on the video and stacking multiple layers of such models, and that applying this model to the lower layers of a winding network would contribute to the loss of information in our motion layers."}, {"heading": "Acknowledgments", "text": "We thank Samsung, Raytheon BBN Technologies and NVIDIA Corporation for donating a GPU used in this research, and thank Geoffrey Hinton and Ilya Sutskever for their helpful discussions and comments."}], "references": [{"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue", "Jeff", "Hendricks", "Lisa Anne", "Guadarrama", "Sergio", "Rohrbach", "Marcus", "Venugopalan", "Subhashini", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "CoRR, abs/1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Simple-cell-like receptive fields maximize temporal coherence in natural video", "author": ["Hurri", "Jarmo", "Hyv\u00e4rinen", "Aapo"], "venue": "Neural Computation,", "citeRegEx": "Hurri et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hurri et al\\.", "year": 2003}, {"title": "3d convolutional neural networks for human action recognition", "author": ["Ji", "Shuiwang", "Xu", "Wei", "Yang", "Ming", "Yu", "Kai"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Ji et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2013}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Karpathy", "Andrej", "Toderici", "George", "Shetty", "Sanketh", "Leung", "Thomas", "Sukthankar", "Rahul", "Fei-Fei", "Li"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "HMDB: a large video database for human motion recognition", "author": ["H. Kuehne", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre"], "venue": "In Proceedings of the International Conference on Computer Vision (ICCV),", "citeRegEx": "Kuehne et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kuehne et al\\.", "year": 2011}, {"title": "Beyond gaussian pyramid: Multi-skip feature stacking for action recognition", "author": ["Lan", "Zhen-Zhong", "Lin", "Ming", "Li", "Xuanchong", "Hauptmann", "Alexander G", "Raj", "Bhiksha"], "venue": "CoRR, abs/1411.6660,", "citeRegEx": "Lan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2014}, {"title": "Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "In CVPR,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Learning to relate images", "author": ["Memisevic", "Roland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Memisevic and Roland.,? \\Q2013\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2013}, {"title": "Learning to represent spatial transformations with factored higher-order boltzmann machines", "author": ["Memisevic", "Roland", "Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Memisevic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2010}, {"title": "Modeling deep temporal dependencies with recurrent grammar cells", "author": ["Michalski", "Vincent", "Memisevic", "Roland", "Konda", "Kishore"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Michalski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Michalski et al\\.", "year": 2014}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["Ranzato", "Marc\u2019Aurelio", "Szlam", "Arthur", "Bruna", "Joan", "Mathieu", "Micha\u00ebl", "Collobert", "Ronan", "Chopra", "Sumit"], "venue": "CoRR, abs/1412.6604,", "citeRegEx": "Ranzato et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2014}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "author": ["Soomro", "A. Roshan Zamir", "M. Shah"], "venue": "In CRCV-TR-12-01,", "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Modeling the joint density of two images under a variety of transformations", "author": ["J. Susskind", "R. Memisevic", "G. Hinton", "M. Pollefeys"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Susskind et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V. V"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "C3D: generic features for video analysis", "author": ["Tran", "Du", "Bourdev", "Lubomir D", "Fergus", "Rob", "Torresani", "Lorenzo", "Paluri", "Manohar"], "venue": "CoRR, abs/1412.0767,", "citeRegEx": "Tran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2014}, {"title": "Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex. Proceedings", "author": ["J.H. van Hateren", "D.L. Ruderman"], "venue": "Biological sciences / The Royal Society,", "citeRegEx": "Hateren and Ruderman,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Ruderman", "year": 1998}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "CoRR, abs/1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "Recently, recurrent neural networks using the Long Short Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been used successfully to perform various supervised sequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machine translation (Sutskever et al., 2014; Cho et al., 2014), and caption generation for images (Vinyals et al.", "startOffset": 277, "endOffset": 319}, {"referenceID": 0, "context": "Recently, recurrent neural networks using the Long Short Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been used successfully to perform various supervised sequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machine translation (Sutskever et al., 2014; Cho et al., 2014), and caption generation for images (Vinyals et al.", "startOffset": 277, "endOffset": 319}, {"referenceID": 22, "context": ", 2014), and caption generation for images (Vinyals et al., 2014).", "startOffset": 43, "endOffset": 65}, {"referenceID": 1, "context": "They have also been applied on videos for recognizing actions and generating natural language descriptions (Donahue et al., 2014).", "startOffset": 107, "endOffset": 129}, {"referenceID": 0, "context": ", 2014; Cho et al., 2014), and caption generation for images (Vinyals et al., 2014). They have also been applied on videos for recognizing actions and generating natural language descriptions (Donahue et al., 2014). A general sequence to sequence learning framework was described by Sutskever et al. (2014) in which a recurrent network is used to encode a sequence into a fixed length representation, and then another recurrent network is used to decode a sequence out of that representation.", "startOffset": 8, "endOffset": 307}, {"referenceID": 6, "context": "This has led to research in 3D convolutional nets (Ji et al., 2013; Tran et al., 2014), different temporal fusion strategies (Karpathy et al.", "startOffset": 50, "endOffset": 86}, {"referenceID": 20, "context": "This has led to research in 3D convolutional nets (Ji et al., 2013; Tran et al., 2014), different temporal fusion strategies (Karpathy et al.", "startOffset": 50, "endOffset": 86}, {"referenceID": 7, "context": ", 2014), different temporal fusion strategies (Karpathy et al., 2014) and exploring different ways of presenting visual information to convolutional nets (Simonyan & Zisserman, 2014a).", "startOffset": 46, "endOffset": 69}, {"referenceID": 18, "context": "Generative models for understanding transformations between pairs of consecutive images are also well studied (Memisevic, 2013; Memisevic & Hinton, 2010; Susskind et al., 2011).", "startOffset": 110, "endOffset": 176}, {"referenceID": 10, "context": "Le et al. (2011) approached this problem using multiple layers of Independent Subspace Analysis modules.", "startOffset": 0, "endOffset": 17}, {"referenceID": 10, "context": "Le et al. (2011) approached this problem using multiple layers of Independent Subspace Analysis modules. Generative models for understanding transformations between pairs of consecutive images are also well studied (Memisevic, 2013; Memisevic & Hinton, 2010; Susskind et al., 2011). This work was extended recently by Michalski et al. (2014) to model longer sequences.", "startOffset": 0, "endOffset": 342}, {"referenceID": 14, "context": "Recently, Ranzato et al. (2014) proposed a generative model for videos.", "startOffset": 10, "endOffset": 32}, {"referenceID": 14, "context": "Ranzato et al. (2014) use a similar model but predict only the next frame at each time step.", "startOffset": 0, "endOffset": 22}, {"referenceID": 17, "context": "The UCF-101 dataset (Soomro et al., 2012) contains 13,320 videos with an average length of 6.", "startOffset": 20, "endOffset": 41}, {"referenceID": 8, "context": "The HMDB-51 dataset (Kuehne et al., 2011) contains 5100 videos belonging to 51 different action categories.", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": "To train the unsupervised models, we used a subset of the Sports-1M dataset (Karpathy et al., 2014), that contains 1 million YouTube clips.", "startOffset": 76, "endOffset": 99}, {"referenceID": 23, "context": "All classifiers used dropout regularization, where we dropped activations as they were communicated across layers but not through time within the same LSTM as proposed in Zaremba et al. (2014). We emphasize that this is a very strong baseline and does significantly better than just using single frames.", "startOffset": 171, "endOffset": 193}, {"referenceID": 20, "context": "C3D (Tran et al., 2014) 72.", "startOffset": 4, "endOffset": 23}, {"referenceID": 20, "context": "3 C3D + fc6 (Tran et al., 2014) 76.", "startOffset": 12, "endOffset": 31}, {"referenceID": 1, "context": "4 LRCN (Donahue et al., 2014) 71.", "startOffset": 7, "endOffset": 29}, {"referenceID": 1, "context": "LRCN (Donahue et al., 2014) 77.", "startOffset": 5, "endOffset": 27}, {"referenceID": 1, "context": "LRCN (Donahue et al., 2014) 82.", "startOffset": 5, "endOffset": 27}, {"referenceID": 9, "context": "Multi-skip feature stacking (Lan et al., 2014) 89.", "startOffset": 28, "endOffset": 46}], "year": 2015, "abstractText": "We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences \u2013 patches of image pixels and high-level representations (\u201cpercepts\u201d) of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem \u2013 human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.", "creator": "LaTeX with hyperref package"}}}