{"id": "1502.05477", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2015", "title": "Trust Region Policy Optimization", "abstract": "We propose a family of trust region policy optimization (TRPO) algorithms for learning control policies. We first develop a policy update scheme with guaranteed monotonic improvement, and then we describe a finite-sample approximation to this scheme that is practical for large-scale problems. In our experiments, we evaluate the method on two different and very challenging sets of tasks: learning simulated robotic swimming, hopping, and walking gaits, and playing Atari games using images of the screen as input. For these tasks, the policies are neural networks with tens of thousands of parameters, mapping from observations to actions.", "histories": [["v1", "Thu, 19 Feb 2015 06:44:25 GMT  (547kb,D)", "http://arxiv.org/abs/1502.05477v1", "16 pages"], ["v2", "Mon, 18 May 2015 14:56:50 GMT  (540kb,D)", "http://arxiv.org/abs/1502.05477v2", "16 pages, ICML 2015"], ["v3", "Mon, 8 Jun 2015 10:47:03 GMT  (540kb,D)", "http://arxiv.org/abs/1502.05477v3", "16 pages, ICML 2015"], ["v4", "Mon, 6 Jun 2016 01:00:57 GMT  (541kb,D)", "http://arxiv.org/abs/1502.05477v4", "16 pages, ICML 2015"], ["v5", "Thu, 20 Apr 2017 18:04:12 GMT  (541kb,D)", "http://arxiv.org/abs/1502.05477v5", "16 pages, ICML 2015"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john schulman", "sergey levine", "pieter abbeel", "michael i jordan", "philipp moritz"], "accepted": true, "id": "1502.05477"}, "pdf": {"name": "1502.05477.pdf", "metadata": {"source": "META", "title": "Trust Region Policy Optimization", "authors": ["John Schulman"], "emails": ["joschu@eecs.berkeley.edu", "slevine@eecs.berkeley.edu", "pcmoritz@eecs.berkeley.edu", "jordan@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "Most policy optimization algorithms fall into three broad categories: policy iteration methods, which alternate between estimating the value function of current policies and improving policies (Bertsekas, 2005); policy gradient methods, which use an estimate of the expected costs from the sample paths (Peters & Schaal, 2008a) (and which, as we will discuss later, are closely linked to policy iteration); and derivative-free optimization methods, such as the Cross-Entropy Method (CEM) and Covariance Adjustment (CMA), which treat costs as a black box function optimized in terms of policy parameters (Fu et al. 2005; Szita & Lo-Rincz, 2006). General Derivative-free Shock Adjustment Methods (CMA)."}, {"heading": "2. Preliminaries", "text": "Consider an infinite horizon discounted Markov decision-making process (MDP), defined by the tuple (S, A, P, c, \u03c10, \u043c), where S \u00b2 s is a finite series of states, A is a finite series of actions, P: S \u00b7 A \u00b7 S \u2192 R is the transition probability distribution, c: S \u2192 R is the cost function, 0: S \u2192 R is the distribution of the initial state s0, and vice versa (0, 1) is the discount factor.ar Xiv: 150 2.05 477v 1 [cs.L G] 19 Feb 20Let us follow a stochastic policy, p \u00b2 s a stochastic policy, p \u00b7 A \u2192 [0, 1] and let us allow the expected discount costs: Es0, a0... [permanence] t = 0, [permanence] we use the quantitc function, where s0 (s0), where we use the quantitative function."}, {"heading": "3. Monotonic Improvement Guarantee for General Stochastic Policies", "text": "Equation (6) applied to conservative policies implies that updating policies that improve the right-hand side guarantees the improvement of the actual expected cost target (6). Our principal theoretical result is that the policy improvement tied to Equation (6) can also be extended to general stochastic measures, rather than replacing mixture strategies by replacing them with a distance measurement between \u03c0 and \u03c0. Since mixture strategies are rarely applied in practice, this result is crucial for extending the guarantee of improvement to practical problems. The particular distance we use is the total variation divergence defined by DTV (p) = 12 x x x x measurement method i = pi-qi | for discrete probability distributions p, q.1 Define DmaxTV (sp.) asDmaxTV (sp.) = maximum s divergence defined by DTV."}, {"heading": "4. Optimization of Parameterized Policies", "text": "In the previous section, we considered the problem of policy optimization independently of the parameterization of \u03c0 and assuming that policy can be evaluated in all states. We will now describe how we can derive a practical algorithm from these theoretical foundations, using finite samples and arbitrary parameterizations. As we look at parameterized strategies, we will overload our previous notation to use functions of \u03b8 and not \u03c0, e.g.: \u03b7 (\u03b8), LTB (\u03b8): = LTB (\u03b8): = LTB (\u0432), and DKL (\u03b8) to overload our previous notation to use functions of \u03b8 and not \u03c0. We will apply the previous policy parameters that we want to improve. The previous section shows that the policies of \u03b7 (\u03b8) + CDmaxKL, with equality in inequality in inequality in inequality and the old problem."}, {"heading": "5. Sample-Based Estimation of the Objective and Constraint", "text": "In the previous section, a limited optimization problem was proposed with respect to the political parameters (Equation (13)), which optimizes an estimate of the expected costs (Bartq), which is subject to a restriction of the change of policy at each update. This section describes how the goal and limitation functions can be approximately achieved by means of the Monte Carlo simulation. We try to solve the following optimization problem, which is achieved by the extension of L\u03b8old in Equation (13): Minimizing the effects of the effects of the effects of the effects of the effects of the effects of the effects (a, a) Effects of the effects of the effects of the effects of the effects of the effects (s, a) Effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects (s, a) Effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects (a), (a) Effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects (a), (a) Effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects (a), (a), (a) Effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects of the effects"}, {"heading": "5.1. Single Path", "text": "In this estimation method, we capture a sequence of states by selecting s0 \u0445 \u03c10 randomly and then simulating the \u03c0\u03b8old policy for a certain number of time periods to generate a trajectory s0, a0, s1, a1,... This results in q (a | s) = perspective (a | s). Q\u03b8old (s, a) is calculated for each pair of states (st, at) by taking the discounted sum of future costs along the trajectory."}, {"heading": "5.2. Vine", "text": "In this estimation method, we estimate Q = Q to generate a number of trajectories. We then select a subset of N states along these trajectories, which are referred to as s1, s2..., sN, which we call the \"rollout method,\" will produce a consistent estimate. In practice, we have found that q (sn) = conceptual measures for ongoing problems, such as robotic locomotion, while the uniform distribution works well on discrete tasks such as the Atari games, where they can sometimes achieve better exploration.For each action on, k sampled on each state."}, {"heading": "6. Practical Algorithm", "text": "Here we present two practical policy optimization algorithms based on the ideas described above, which use either the individual path or the sample scheme of the vines from the previous section. Algorithms repeatedly perform the following steps: 1. Use the procedures of the individual paths or vines to acquire estimates of their Q-values together with Monte Carlo. 2. By averaging through random samples, construct the estimated target and the constraint in Equation (15). 3. Solve this limited optimization problem roughly to update the parameter vector of the policy. We use the conjugated gradient algorithm followed by a line search that is only slightly more expensive overall than the calculation of the gradient itself. See Appendix C for details. With regard to (3), we construct the Fisher Information Matrix (FIM) by analytically calculating the Hessian divergence of the L divergence and not by using the covariance of the gradient."}, {"heading": "7. Connections with Prior Work", "text": "As mentioned in section 4, our derivation leads to an update of the policy, which refers to several previous methods and provides a unified perspective on a number of policies for updating the policy. Natural policy gradient (Kakade, 2002) can be maintained as a special case for updating the equation (13) by applying a linear approximation to L and a square approximation to the DKL constraint, which leads to the following problem: Minimize the approaches to updating the equation (13) by applying a linear approximation to L and a square approximation to the DKL constraint (18) by applying the optimization of the TA constraint (previous), where A (previous) ij = unlimited."}, {"heading": "8. Experiments", "text": "We have designed our experiments to examine the following questions: 1. What are the performance characteristics of the single path method and vine sampling? 2. TRPO is related to earlier methods (e.g. the natural policy gradient), but makes several changes, mainly by using a fixed KL divergence instead of a fixed penalty coefficient. How does this affect the performance of the algorithm? 3. Can TRPO be used to solve demanding major problems? How does TRPO compare with other methods when applied to major problems in terms of final performance, computing time and sample complexity? To answer (1) and (2), we compare the performance of the single path method and vine variants of TRPO, several remote variants and a number of previous policy optimization algorithms. With regard to (3) we show that both the single path and the vine algorithm can maintain high-quality ground-to-soil motion control, which can be considered a hard problem."}, {"heading": "8.1. Simulated Robotic Locomotion", "text": "The three simulated robots are able to identify themselves with their different views and viewpoints. The following models are taken into account in our evaluation: 1. There is only one way to make progress, and there is only one way to reduce costs. 2. There is only one way to reduce costs. 3. There is only one way to reduce costs. 4. There is only one way to reduce costs. 5. There is only one way to reduce costs. 5. There is only one way to reduce costs. 6. There is only one way to reduce costs. 6. There is only one way to reduce costs. 6. There is only one way to reduce costs. 6. There is only one way to reduce costs. 6. There is only one way to retaliate. 6. There is only one way to avoid costs."}, {"heading": "8.2. Playing Games from Images", "text": "To evaluate TRPO using a partially observed task with complex observations, we trained strategies for playing Atari games, using raw images as input; the games require learning a variety of behaviors, such as dodging balls and hitting balls with paddles; in addition to the high dimensionality, the challenging elements of these games include delayed rewards (there is no immediate penalty if a life is lost in Breakout or Space Invaders); complex behaviors (Q * bert requires a character to hop on 21 different platforms); and non-stationary visual statistics (Enduro involves a changing and flickering background); we tested our algorithms on the same seven games reported in (Mnih et al., 2013) and (Guo et al., 2014); the images were pre-processed according to the protocol in Mnih et al (2013), and the politics were represented by the network of evolutionary, followed by the network of evolutionary, each with three, followed by the network of evolutionary, with three."}, {"heading": "9. Discussion", "text": "We have shown that an approach to this method, which incorporates KL divergence constraint, yields good empirical results in a number of challenging policy learning tasks that exceed previous methods. Our analysis also provides a perspective that combines political gradients and policy iteration methods, and shows that they are special limiting cases of an algorithm that optimizes a particular goal subject to a crisis of confidence. In the field of robotic locomotion, we have successfully learned controllers for swimming, walking, and jumping in a physics simulator using neural networks and minimal informational costs. Ours is not a previous work that controllers have learned from the ground up for all these tasks, using a generic political search method and undeveloped, universal policy representations."}, {"heading": "Acknowledgements", "text": "We would like to thank Emo Todorov and Yuval Tassa for providing the MuJoCo simulator and Bruno Scherrer for their insightful comments. This research was partially funded by the Office of Naval Research through a Young Investigator Award and grant number N00014-11-1-0688, DARPA through a Young Faculty Award, and the Army Research Office through the MAST program."}, {"heading": "A. Proof of Policy Improvement Bound", "text": "We will adapt Kakade and Langford's evidence to the more general situation considered in this work. First, we will review the Kakade and Langford evidence (using our own notation)."}, {"heading": "B. Perturbation Theory Proof of Policy Improvement Bound", "text": "We also offer another proof for 1 using the error theory. This method makes it possible to provide a slightly stronger limit (theory 1a). Let \u03b1 be the maximum total deviation between stochastic policies \u03c0 and \u03c0, as defined in Equation (9), and let L be defined as in Equation (3). Then\u03b7 (numerical value) \u2264 L (numerical value) + 2 \u00b2 (numerical value) \u2212 2 (32), where = max s {numerical value a (numerical value) Q\u03c0 (s) (a) \u2212 numerical value (a) Q\u03c0 (s) Q\u03c0 (a) \u00b2 numerical value (a | s) \u2212 numerical value (33) Note that the definition in Equation (33) is less than or equal to the definition in 1, i.e., equation (7)."}, {"heading": "C. Efficiently Solving the Trust-Region Constrained Optimization Problem", "text": "This section describes how to efficiently solve the following constrained optimization problem that we must solve with each iteration of TRPO. (However, this means that we use a linear approach to the target and a square approach to the condition; and (2) perform a line search in this direction to ensure that we improve the nonlinear target while meeting the nonlinear condition; the search direction is calculated by roughly solving the equation Ax = \u2212 g, where A is the Fisher information matrix, i.e., the quadratic approach to the KL divergence condition: DKL (that old quadratic condition) the search direction is calculated, where A is the Fisher information matrix, i.e., the Fisher information matrix condition. (The quadratic approach to the KL divergence condition: DL)."}, {"heading": "D. Approximating Factored Policies with Neural Networks", "text": "The easiest way to do this is to have the map of the neural network (deterministic) from the state vector s to a vector \u00b5, which determines a distribution over the action space. Then, we can calculate the probability p (a | \u00b5) and scan a \u00b2 p (a | \u00b5). For our experiments with continuous states and action spaces, we used a Gaussian distribution where the covariance matrix was diagonal and independent of the state. A neural network with several fully connected (dense) layers maps from the input characteristics to the mean of a Gaussian distribution. A separate set of parameters specifies the standard deviation of each element. More specifically, the parameters are a set of weights and distortions for the neural network."}, {"heading": "F. Learning Curves for the Atari Domain", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A. Barto", "R. Sutton", "C. Anderson"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Dynamic programming and optimal control, volume", "author": ["D. Bertsekas"], "venue": null, "citeRegEx": "Bertsekas,? \\Q2005\\E", "shortCiteRegEx": "Bertsekas", "year": 2005}, {"title": "A survey on policy search for robotics", "author": ["M. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "Deisenroth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Simulation optimization: a review, new developments, and applications", "author": ["Fu", "Michael C", "Glover", "Fred W", "April", "Jay"], "venue": "In Proceedings of the 37th conference on Winter simulation,", "citeRegEx": "Fu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2005}, {"title": "Approximate dynamic programming finally performs well in the game of Tetris", "author": ["Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Scherrer", "Bruno"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2013}, {"title": "Fast biped walking with a reflexive controller and realtime policy searching", "author": ["T. Geng", "B. Porr", "F. W\u00f6rg\u00f6tter"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Geng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geng et al\\.", "year": 2006}, {"title": "Deep learning for real-time atari game play using offline Monte-Carlo tree search planning", "author": ["X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation", "author": ["Hansen", "Nikolaus", "Ostermeier", "Andreas"], "venue": "In Evolutionary Computation,", "citeRegEx": "Hansen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1996}, {"title": "A natural policy gradient", "author": ["Kakade", "Sham"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade and Sham.,? \\Q2002\\E", "shortCiteRegEx": "Kakade and Sham.", "year": 2002}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In ICML,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Reinforcement learning as classification: Leveraging modern classifiers", "author": ["Lagoudakis", "Michail G", "Parr", "Ronald"], "venue": "In ICML,", "citeRegEx": "Lagoudakis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis et al\\.", "year": 2003}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Martens and Sutskever,? \\Q2012\\E", "shortCiteRegEx": "Martens and Sutskever", "year": 2012}, {"title": "Playing Atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Efficient methods in convex programming", "author": ["Nemirovski", "Arkadi"], "venue": null, "citeRegEx": "Nemirovski and Arkadi.,? \\Q2005\\E", "shortCiteRegEx": "Nemirovski and Arkadi.", "year": 2005}, {"title": "PEGASUS: A policy search method for large mdps and pomdps", "author": ["A.Y. Ng", "M. Jordan"], "venue": "In Uncertainty in artificial intelligence (UAI),", "citeRegEx": "Ng and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Ng and Jordan", "year": 2000}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "Neural Networks,", "citeRegEx": "Peters and Schaal,? \\Q2008\\E", "shortCiteRegEx": "Peters and Schaal", "year": 2008}, {"title": "Relative entropy policy search", "author": ["J. Peters", "K. M\u00fclling", "Y. Alt\u00fcn"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Reinforcement learning by reward-weighted regression for operational space control", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Peters et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2007}, {"title": "Asymptopia: an exposition of statistical asymptotic theory", "author": ["Pollard", "David"], "venue": "Neural computation,", "citeRegEx": "Pollard and David.,? \\Q2000\\E", "shortCiteRegEx": "Pollard and David.", "year": 2000}, {"title": "Stochastic policy gradient reinforcement learning on a simple 3d biped", "author": ["R. Tedrake", "T. Zhang", "H. Seung"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Tedrake et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tedrake et al\\.", "year": 2004}, {"title": "MuJoCo: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Optimal gait and form for animal locomotion", "author": ["Wampler", "Kevin", "Popovi\u0107", "Zoran"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "Wampler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wampler et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Most algorithms for policy optimization can be classified into three broad categories: policy iteration methods, which alternate between estimating the value function under the current policy and improving the policy (Bertsekas, 2005); policy gradient methods, which use an estimator of the gradient of the expected cost obtained from sample trajectories (Peters & Schaal, 2008a) (and which, as we later discuss, have a close connection to policy iteration); and derivative-free optimization methods, such as the cross-entropy method (CEM) and covariance matrix adaptation (CMA), which treat the cost as a black box function to be optimized in terms of the policy parameters (Fu et al.", "startOffset": 217, "endOffset": 234}, {"referenceID": 3, "context": "Most algorithms for policy optimization can be classified into three broad categories: policy iteration methods, which alternate between estimating the value function under the current policy and improving the policy (Bertsekas, 2005); policy gradient methods, which use an estimator of the gradient of the expected cost obtained from sample trajectories (Peters & Schaal, 2008a) (and which, as we later discuss, have a close connection to policy iteration); and derivative-free optimization methods, such as the cross-entropy method (CEM) and covariance matrix adaptation (CMA), which treat the cost as a black box function to be optimized in terms of the policy parameters (Fu et al., 2005; Szita & L\u00f6rincz, 2006).", "startOffset": 675, "endOffset": 715}, {"referenceID": 4, "context": "For example, while Tetris is a classic benchmark problem for approximate dynamic programming (ADP) methods, stochastic optimization methods are difficult to beat on this task (Gabillon et al., 2013).", "startOffset": 175, "endOffset": 198}, {"referenceID": 2, "context": "A practical approximation to this policy improvement procedure can efficiently optimize nonlinear policies with tens of thousands of parameters, which have previously posed a major challenge for model-free policy search (Deisenroth et al., 2013).", "startOffset": 220, "endOffset": 245}, {"referenceID": 17, "context": "Similar policy updates have been proposed in prior work (Bagnell & Schneider, 2003; Peters & Schaal, 2008b; Peters et al., 2010), and we compare our approach to prior methods in Section 7 and in the experiments in Section 8.", "startOffset": 56, "endOffset": 128}, {"referenceID": 4, "context": "This method has mostly been explored in the context of policy iteration methods (Lagoudakis & Parr, 2003; Gabillon et al., 2013).", "startOffset": 80, "endOffset": 128}, {"referenceID": 1, "context": "See (Bertsekas, 2005) for additional discussion on Monte Carlo estimation of Q-values and (Ng & Jordan, 2000) for a discussion of common random numbers in reinforcement learning.", "startOffset": 4, "endOffset": 21}, {"referenceID": 17, "context": "Relative entropy policy search (REPS) (Peters et al., 2010) constrains the stateaction marginals p(s, a), while TRPO constraints the conditionals p(a|s).", "startOffset": 38, "endOffset": 59}, {"referenceID": 17, "context": "Relative entropy policy search (REPS) (Peters et al., 2010) constrains the stateaction marginals p(s, a), while TRPO constraints the conditionals p(a|s). Unlike REPS, our approach does not require a costly nonlinear optimization in the inner loop. Levine and Abbeel (2014) also use a KL divergence constraint, but its purpose is to encourage the policy not to stray from regions where the estimated dynamics model is valid, while we do not attempt to estimate the system dynamics explicitly.", "startOffset": 39, "endOffset": 273}, {"referenceID": 21, "context": "We conducted the robotic locomotion experiments using the MuJoCo simulator (Todorov et al., 2012).", "startOffset": 75, "endOffset": 97}, {"referenceID": 0, "context": "To establish a standard baseline, we also included the classic cart-pole balancing problem, based on the formulation from Barto et al. (1983), using a linear policy with six parameters that is easy to optimize with derivativefree black-box optimization methods.", "startOffset": 122, "endOffset": 142}, {"referenceID": 20, "context": "This is in contrast with most prior methods for learning locomotion, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping (Tedrake et al., 2004; Geng et al., 2006; Wampler & Popovi\u0107, 2009).", "startOffset": 180, "endOffset": 246}, {"referenceID": 5, "context": "This is in contrast with most prior methods for learning locomotion, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping (Tedrake et al., 2004; Geng et al., 2006; Wampler & Popovi\u0107, 2009).", "startOffset": 180, "endOffset": 246}, {"referenceID": 12, "context": "We tested our algorithms on the same seven games reported on in (Mnih et al., 2013) and (Guo et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 6, "context": ", 2013) and (Guo et al., 2014).", "startOffset": 12, "endOffset": 30}, {"referenceID": 6, "context": ", 2013) and (Guo et al., 2014). The images were preprocessed following the protocol in Mnih et al (2013), and the policy was represented by the convolutional neural network shown in Figure 3, with two convolutional layers with 16 channels and stride 2, followed by one fully-connected layer with 20 units, yielding 33,500 parameters.", "startOffset": 13, "endOffset": 105}, {"referenceID": 12, "context": "The results of the vine and single path algorithms are summarized in Table 1, which also includes an expert human performance and two recent methods: deepQ-learning (Mnih et al., 2013), and a combination of Monte-Carlo Tree Search with supervised training (Guo et al.", "startOffset": 165, "endOffset": 184}, {"referenceID": 6, "context": ", 2013), and a combination of Monte-Carlo Tree Search with supervised training (Guo et al., 2014), called UCC-I.", "startOffset": 79, "endOffset": 97}], "year": 2017, "abstractText": "We propose a family of trust region policy optimization (TRPO) algorithms for learning control policies. We first develop a policy update scheme with guaranteed monotonic improvement, and then we describe a finitesample approximation to this scheme that is practical for large-scale problems. In our experiments, we evaluate the method on two different and very challenging sets of tasks: learning simulated robotic swimming, hopping, and walking gaits, and playing Atari games using images of the screen as input. For these tasks, the policies are neural networks with tens of thousands of parameters, mapping from observations to actions.", "creator": "LaTeX with hyperref package"}}}