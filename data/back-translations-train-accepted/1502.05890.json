{"id": "1502.05890", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Contextual semibandits via supervised learning oracles", "abstract": "We study a variant of the contextual bandit problem, where on each round, the learner plays a sequence of actions, receives a feature for each individual action, and reward that is linearly related to these features. This setting has applications to network routing, crowd-sourcing, personalized search, and many other domains. If the linear transformation is known, we analyze an algorithm that is structurally similar to the algorithm of Agarwal et a. [2014] and show that it enjoys a regret bound between $\\tilde{O}(\\sqrt{KLT \\ln N})$ and $\\tilde{O}(L\\sqrt{KT \\ln N})$, where $K$ is the number of actions, $L$ is the length of each action sequence, $T$ is the number of rounds, and $N$ is the number of policies. If the linear transformation is unknown, we show that an algorithm that first explores to learn the unknown weights via linear regression and thereafter uses the estimated weights can achieve $\\tilde{O}(\\|w\\|_1(KT)^{3/4} \\sqrt{\\ln N})$ regret, where $w$ is the true (unknown) weight vector. Both algorithms use an optimization oracle to avoid explicit enumeration of the policies and consequently are computationally efficient whenever an efficient algorithm for the fully supervised setting is available.", "histories": [["v1", "Fri, 20 Feb 2015 14:55:41 GMT  (39kb,D)", "http://arxiv.org/abs/1502.05890v1", null], ["v2", "Thu, 5 Mar 2015 01:38:23 GMT  (39kb,D)", "http://arxiv.org/abs/1502.05890v2", null], ["v3", "Tue, 14 Jun 2016 00:43:13 GMT  (234kb,D)", "http://arxiv.org/abs/1502.05890v3", null], ["v4", "Fri, 4 Nov 2016 19:28:07 GMT  (236kb,D)", "http://arxiv.org/abs/1502.05890v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay krishnamurthy", "alekh agarwal", "miroslav dud\u00edk"], "accepted": true, "id": "1502.05890"}, "pdf": {"name": "1502.05890.pdf", "metadata": {"source": "CRF", "title": "Efficient Contextual Semi-Bandit Learning", "authors": ["Akshay Krishnamurthy", "Alekh Agarwal"], "emails": ["akshaykr@cs.cmu.edu", "alekha@microsoft.com", "mdudik@microsoft.com"], "sections": [{"heading": null, "text": "If the linear transformation is unknown, we show that an algorithm that first explores the unknown weights by means of linear regression and then uses the estimated weights can obtain O values, where w is the true (unknown) weight vector. Both algorithms use an optimization oracle to avoid explicit enumeration of policies, and are therefore computationally efficient whenever an efficient algorithm is available for the fully monitored setting."}, {"heading": "1 Introduction", "text": "The goal of the learner is to receive a reward for the chosen action, and finally we observe a reward for the chosen action. This learning paradigm does not capture the partial feedback aspect of the above examples as a reward for the unselected action. The goal of the learner is to receive a reward for the chosen action, and finally we observe a reward for the chosen action. This learning paradigm does not capture the partial feedback of the above examples as learners."}, {"heading": "2 SEMIBANDIT-VCEE for Contextual Semi-Bandits with Known Weights", "text": "When the weights in the linear transformation are known, we propose an algorithm that has a structure similar to that of a current algorithm for the classic contextual bandit problem [Agarwal et al., 2014]. The algorithm maintains a distribution over politics and uses the smoothed version to play actions in each turn. The core of the algorithm is to find this distribution by solving a convex optimization problem that we call OP, where the previous balance of interaction H is a feasibility problem that is viewed with both little regret (measured by empirical regret) and little deviation. The constraint in Equation 1 forces the distribution to exhibit little empirical regret, making the mass perform well."}, {"heading": "2.1 Regret Guarantee for SEMIBANDIT-VCEE", "text": "Our analysis of this algorithm leads to the following Reue-Garantie: Theorem 1 = IleyQ = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "2.2 Computational Guarantee for SEMIBANDIT-VCEE", "text": "There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplies all weights. Otherwise, if there is a policy found to violate the variance constraint in Equation 2, the algorithm adds weight to this policy so that the algorithm multiplies all weights to shrink. Otherwise, if there is a policy that violates the variance constraint in Equation 2, the algorithm adds weight to this policy so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descend algorithm: Theorem 2. For each story H and parameters, algorithm 2, we use for Asian weight constraints so that the constraints are no longer violated."}, {"heading": "2.3 Proof Sketch of Theorem 1", "text": "The proof of the borderline of regret is quite technical, and we outline the arguments here by moving all the details to Appendix A. First, we establish two uniform borderlines of deviation, one based on the variance estimates used in Eq.2 and the other based on the reward estimates used in Appendix A. For the first example, we show that if \u00b5t and t are large enough, then for all P, \u03c0, with a high probability V (P, \u03c0, \u00b5t) \u2264 6.4V; t (P, \u03c0, \u00b5t) + 81.3KL / pmin, where V (P, \u03c0, \u00b5t) = Ex \u00b2 D \u00b2 L = 1 P\u00b5 (x) l | x) and V \u00b2 t is the empirical version. The other main deviation is limited to the reward estimates. We use Freedman's inequality to show that with a high probability, for each time t and each political boundary K \u2212 R: The other main deviation is that we have the boundary between the reward estimates."}, {"heading": "2.4 Proof Sketch of Theorem 2", "text": "Firstly, if the algorithm is stopped, then both conditions must be met."}, {"heading": "3 Contextual Semi-bandits with Unknown Weights", "text": "The aim of this algorithm is to explore two things: We assume that the time horizon for our algorithms, SEMIBANDIT-EELS, is provided with a uniform pattern of action for each round. Structurally, the algorithm considers the first rounds for a uniform research, which leads to reliable estimates for the weight vectors as well as the expected characteristics for each policy."}, {"heading": "4 Conclusion and Discussion", "text": "In this paper, we investigated the contextual semi-bandit problem, in which the learner plays a composite action and observes characteristics for each simple action in that tuple in addition to the overall reward. We assumed that the reward was linearly linked to the characteristics of the simple actions. If this linear relationship is known, we showed that an adjustment of the algorithm by Agarwal et al. [2014] elicits regret between O (KLT) and O (L) and can be efficiently implemented with access to an optimization oracle. If the weights are unknown, we provided a simple algorithm that achieves O (KLT) 3 / 4 lnN (regret). These algorithms show how to use additional feedback to avoid scales with KL, the size of the composite action space, being limited."}, {"heading": "A Full Proof of Theorem 1", "text": "The first limit of deviation shows that the deviation estimates used in Eq.2 are suitable for the actual deviation of the distribution. To specify this deviation, we need some definitions: V (P, \u03c0, \u00b5, \u00b5): V (P, l, x): The deviation is contained in the following theorem: Theorem 4 (P, \u2264 P, \u00b5) = E (E, x x) Ht [L, l, x) (5): The deviation is contained in the following theorem: Theorem 4 (0, 1), if: \u00b5P (P) = E, x, x) Ht [L = 11P\u00b5 (x) l, x)] (5): The deviation is not contained in the following theorem: Theorem 4 (P, x)."}, {"heading": "B Full Proof for Theorem 2", "text": "In this section we will prove theorem 2, which describes the coordinate optimization."}, {"heading": "C Full Proof of Theorem 3", "text": "The proof of the theorem is based on the trading off the regret associated with Exploration and Exploitation Round. The first indication of the evidence is a decomposition of the regret for exploitation. \u2212 n The first indication of the evidence is a decomposition of the regret for exploitation. \u2212 n The first indication of the evidence is a decomposition of the regret for exploitation. \u2212 n The first indication of the evidence is a decomposition of the regret for exploitation. \u2212 n The first indication of the evidence is a deviation from the second indication. \u2212 n The second indication is a deviation from the second indication. \u2212 n The second indication is a deviation from the second indication. \u2212 n The second indication is a deviation from the second indication. \u2212 n The first indication is a deviation from the second indication. \u2212 The first indication is a deviation from the second indication. \u2212 The first indication is a deviation from the second indication."}, {"heading": "D Deviation Bounds", "text": "Here we collect several limits of deviation that we use in our evidence, all of which are known and we point to references rather than proofs. The first inequality that is a deviation from the amber type that is limited to Martingales is Freedman's inequality, that of Beygelzimer et al. [2011] Lemma 18 (Freedman's inequality). Let X1, X2,. \u2212 XT be a sequence of real-rated random variables. Let's assume that Xt \u2264 R and E [Xt | X1,. \u2212 XT] will be a sequence of real-rated random variables. [Xt \u2212 1] a sequence of real-rated random variables. Let's suppose that Xt | X1,."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E Schapire"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Minimax policies for combinatorial prediction", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "games. arXiv:1105.4871,", "citeRegEx": "Audibert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2011}, {"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2014}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E Schapire"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2011}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L\u00e9on Bottou", "Jonas Peters", "Joaquin Qui\u00f1onero-Candela", "Denis Xavier Charles", "D. Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Simard", "Ed Snelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Combinatorial pure exploration of multi-armed bandits", "author": ["Shouyuan Chen", "Tian Lin", "Irwin King", "Michael R Lyu", "Wei Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Combinatorial multi-armed bandit: General framework and applications", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Contextual bandits with linear payoff functions", "author": ["Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E Schapire"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Chu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2011}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dud\u0131\u0301k", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "In Uncertainty and Artificial Intelligence,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Parametric bandits: The generalized linear case", "author": ["Sarah Filippi", "Olivier Cappe", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "The on-line shortest path problem under partial monitoring", "author": ["Andr\u00e1s Gy\u00f6rgy", "Tam\u00e1s Linder", "G\u00e1bor Lugosi", "Gy\u00f6rgy Ottucs\u00e1k"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gy\u00f6rgy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gy\u00f6rgy et al\\.", "year": 2007}, {"title": "Non-stochastic bandit slate problems", "author": ["Satyen Kale", "Lev Reyzin", "Robert E Schapire"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kale et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kale et al\\.", "year": 2010}, {"title": "Matroid bandits: Fast combinatorial optimization with learning", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Hoda Eydgahi", "Brian Eriksson"], "venue": "In Uncertainty and Artificial Intelligence,", "citeRegEx": "Kveton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2014}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesv\u00e1ri"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Kveton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2015}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Langford and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E. Schapire"], "venue": "In International Conference on World Wide Web,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Contextual combinatorial bandit and its application on diversified online recommendation", "author": ["Lijing Qin", "Shouyuan Chen", "Xiaoyan Zhu"], "venue": "In SIAM International Conference on Data Mining,", "citeRegEx": "Qin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qin et al\\.", "year": 2014}, {"title": "The analysis of randomized and nonrandomized AIDS treatment trials using a new approach to causal inference in longitudinal studies", "author": ["J.M. Robins"], "venue": "In Health Service Research Methodology: A Focus on AIDS,", "citeRegEx": "Robins.,? \\Q1989\\E", "shortCiteRegEx": "Robins.", "year": 1989}, {"title": "Hanson-wright inequality and sub-gaussian concentration", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": null, "citeRegEx": "Rudelson and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2013}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong and Tsitsiklis.", "year": 2010}, {"title": "User-Friendly Tail Bounds for Sums of Random Matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Tropp.", "year": 2011}, {"title": "Algorithms for adversarial bandit problems with multiple plays", "author": ["Taishi Uchiya", "Atsuyoshi Nakamura", "Mineichi Kudo"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Uchiya et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Uchiya et al\\.", "year": 2010}, {"title": "The proof of this theorem is similar in spirit to a related theorem", "author": ["Agarwal"], "venue": null, "citeRegEx": "Agarwal,? \\Q2014\\E", "shortCiteRegEx": "Agarwal", "year": 2014}, {"title": "The first three are fairly straightforward and the proof of the later two are based on the arguments", "author": ["Agarwal"], "venue": null, "citeRegEx": "Agarwal,? \\Q2014\\E", "shortCiteRegEx": "Agarwal", "year": 2014}, {"title": "Freedman\u2019s Inequality)", "author": ["Beygelzimer"], "venue": "Let X1,", "citeRegEx": "Beygelzimer,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "If the linear transformation is known, we analyze an algorithm that is structurally similar to the algorithm of Agarwal et al. [2014] and show that it enjoys a regret bound between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), where K is the number of actions, L is the length of each action sequence, T is the number of rounds, and N is the number of policies.", "startOffset": 112, "endOffset": 134}, {"referenceID": 6, "context": "Learning from partial feedback (\u201cbandit\u201d feedback) is of great practical importance and has seen a recent surge of research interest [Bubeck and Cesa-Bianchi, 2012].", "startOffset": 133, "endOffset": 164}, {"referenceID": 19, "context": "Motivating examples include healthcare [Robins, 1989] \u2013 where we only observe the result of the treatment prescribed to the patient, but obtain no information about how other treatments would have worked \u2013 or Internet applications [Li et al.", "startOffset": 39, "endOffset": 53}, {"referenceID": 2, "context": "When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 13, "context": ", 2014] or slate bandits [Kale et al., 2010] in the literature.", "startOffset": 25, "endOffset": 44}, {"referenceID": 0, "context": "When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al., 2010] in the literature. Our goal is to design learning algorithms whose running time and statistical performance (measured by regret) scale with the number of simple actions rather than the number of composite actions. In the first part of the paper, we assume that the linear relationship between the reward and the feedback on the simple actions is known, and we derive a new algorithm for contextual semi-bandits that meets our goal. Our approach builds on the recent contextual bandit algorithms of Dud\u0131\u0301k et al. [2011] and Agarwal et al.", "startOffset": 88, "endOffset": 667}, {"referenceID": 0, "context": "[2011] and Agarwal et al. [2014] and enjoys a regret guarantee between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2.", "startOffset": 11, "endOffset": 33}, {"referenceID": 0, "context": "[2011] and Agarwal et al. [2014] and enjoys a regret guarantee between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2. The policy class \u03a0 is a set of functions mapping contexts into composite actions (e.g., linear learners, decision trees, or neural nets), which we access via an optimization oracle. We show that the algorithm makes \u00d5(T ) calls to the optimization oracle3, meaning that, given an efficient supervised learning algorithm, the algorithm has running time that is only logarithmic in |\u03a0|. This contrasts with the work of Kale et al. [2010] on contextual semi-bandits, which explicitly enumerates the policy class, and therefore has running time that is linear in |\u03a0|.", "startOffset": 11, "endOffset": 787}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al.", "startOffset": 8, "endOffset": 353}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting.", "startOffset": 8, "endOffset": 375}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al.", "startOffset": 8, "endOffset": 427}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a \u00d5( \u221a KLT ) regret but require explicit enumeration of the policy class.", "startOffset": 8, "endOffset": 481}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a \u00d5( \u221a KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al.", "startOffset": 8, "endOffset": 617}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a \u00d5( \u221a KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al. [2011] to semi-bandits, imposing the assumption that the feedback on the simple actions is linearly related 2Extension to VC classes is straightforward using standard arguments.", "startOffset": 8, "endOffset": 678}, {"referenceID": 0, "context": "3The dependence can be improved to \u00d5(T 1/2) using warm-start and epoching ideas identical to Agarwal et al. [2014].", "startOffset": 93, "endOffset": 115}, {"referenceID": 16, "context": "Except for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions.", "startOffset": 23, "endOffset": 41}, {"referenceID": 16, "context": "Except for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions. Qin et al. [2014] generalize this slightly by assuming that the reward is a known function of the context and features.", "startOffset": 23, "endOffset": 189}, {"referenceID": 0, "context": "When the weights in the linear transformation are known, we propose an algorithm that has a similar structure to a recent algorithm for the classical contextual bandit problem [Agarwal et al., 2014].", "startOffset": 176, "endOffset": 198}, {"referenceID": 0, "context": "The main differences between SEMIBANDIT-VCEE and the algorithm of Agarwal et al. [2014] are in the OP and the definitions.", "startOffset": 66, "endOffset": 88}, {"referenceID": 13, "context": "In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting.", "startOffset": 65, "endOffset": 84}, {"referenceID": 13, "context": "In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting. Specifically, they assume that weights w = 1 and that uniform exploration is possible, and they obtain an \u00d5( \u221a KLT ln |\u03a0|) regret bound. Theorem 1 matches this bound, as \u2016w\u20162 = \u2016w\u20161 = L and pmin = L in this case. Our result improves on theirs in two directions: statistically we show how a non-uniform weight vector and restricted exploration distribution affects the regret and, computationally, our algorithm can be efficiently implemented with an optimization oracle while theirs cannot. When uniform exploration is not allowed, as considered by Kveton et al. [2015] in the non-contextual setting, we can set pmin = 1 and our bound is worse than theirs by a factor of \u221a L.", "startOffset": 65, "endOffset": 720}, {"referenceID": 0, "context": "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2).", "startOffset": 43, "endOffset": 65}, {"referenceID": 0, "context": "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter \u03bc, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u03bc)) \u03bcpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.", "startOffset": 43, "endOffset": 1062}, {"referenceID": 0, "context": "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter \u03bc, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u03bc)) \u03bcpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as \u00d5 ( T 3/2 \u221a K pmin log(|\u03a0|/\u03b4) ) by the setting of \u03bct. Moreover, due to the nature of the coordinate descent algorithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |\u03a0|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over \u2206|\u03a0|.", "startOffset": 43, "endOffset": 1591}, {"referenceID": 0, "context": "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter \u03bc, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u03bc)) \u03bcpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as \u00d5 ( T 3/2 \u221a K pmin log(|\u03a0|/\u03b4) ) by the setting of \u03bct. Moreover, due to the nature of the coordinate descent algorithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |\u03a0|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over \u2206|\u03a0|. We mention in passing that Agarwal et al. [2014] also develop two improvements that lead to a more efficient algorithm.", "startOffset": 43, "endOffset": 1689}, {"referenceID": 0, "context": "These are the analogs of Lemmas 6 and 7 in Agarwal et al. [2014]. The proof of the first is based on showing that the derivative of the function g(c) = \u03a6(cQ) is positive so that by convexity of \u03a6, shrinking the weights Q can only decrease the potential.", "startOffset": 43, "endOffset": 65}, {"referenceID": 16, "context": "The algorithm requires knowledge of the time horizon T , which can be relaxed by variants of the Epoch-Greedy [Langford and Zhang, 2008] or -greedy approaches, although the analysis here is significantly simpler.", "startOffset": 110, "endOffset": 136}, {"referenceID": 15, "context": "When the weights are known, can we obtain \u00d5( \u221a KLT ) regret even when the set of feasible actions are constrained, rather than \u00d5(L \u221a KT ) regret as in Theorem 1? The work on non-contextual combinatorial bandits suggests that the answer is yes [Kveton et al., 2015], but all algorithms for contextual bandit learning involve some degree of uniform exploration, which would prohibit such a regret bound.", "startOffset": 243, "endOffset": 264}, {"referenceID": 0, "context": "If this linear relationship is known, we showed that an adaptation of the algorithm of Agarwal et al. [2014] achieves regret between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), and can be implemented efficiently with access to an optimization oracle.", "startOffset": 87, "endOffset": 109}, {"referenceID": 0, "context": "The proof of this theorem is similar in spirit to a related theorem in Agarwal et al. [2014]. We first use Freedman\u2019s inequality (Lemma 18) to argue that for a fixed P, \u03c0, \u03bc, and t, the empirical version of the variance is close to the true variance.", "startOffset": 71, "endOffset": 93}, {"referenceID": 10, "context": "To prove the variance deviation bound, we next use a discretization lemma from Dud\u0131\u0301k et al. [2011], which immediately implies that for any P , there exists a distribution P \u2032 supported on at most Nt policies such that for ct > 0, if Nt \u2265 6 \u03b32 t \u03bctpmin :", "startOffset": 79, "endOffset": 100}, {"referenceID": 0, "context": "Our definition of \u03bct differs from Agarwal et al. [2014] only in the introduction of pmin, so by a straightforward adaptation we have: Lemma 10.", "startOffset": 34, "endOffset": 56}, {"referenceID": 0, "context": "The first three are fairly straightforward and the proof of the later two are based on the arguments of Agarwal et al. [2014]. For the first claim we have:", "startOffset": 104, "endOffset": 126}, {"referenceID": 20, "context": "Specifically, Lemma 20, due to Rudelson and Vershynin [2013], reveals that with probablity \u2265 1\u2212 \u03b4: \u2016U \u03be\u20162 \u2264 \u221a L+ \u221a c ln(2/\u03b4)", "startOffset": 31, "endOffset": 61}, {"referenceID": 4, "context": "The first inequality, which is a Bernstein-type deviation bound for martingales, is Freedman\u2019s inequality, which is from Beygelzimer et al. [2011] Lemma 18 (Freedman\u2019s Inequality).", "startOffset": 121, "endOffset": 147}, {"referenceID": 20, "context": "We also make use of a vector-valued version of Hoeffding\u2019s inequality, due to Rudelson and Vershynin [2013]. Lemma 20 (Vector-valued subgaussian concentration).", "startOffset": 78, "endOffset": 108}, {"referenceID": 22, "context": "Finally, we use a well known matrix-valued version of Hoeffding\u2019s inequality, for example from Tropp [2011]. Lemma 21 (Matrix-Hoeffding).", "startOffset": 95, "endOffset": 108}], "year": 2017, "abstractText": "We study a variant of the contextual bandit problem, where on each round, the learner plays a sequence of actions, receives a feature for each individual action, and reward that is linearly related to these features. This setting has applications to network routing, crowd-sourcing, personalized search, and many other domains. If the linear transformation is known, we analyze an algorithm that is structurally similar to the algorithm of Agarwal et al. [2014] and show that it enjoys a regret bound between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), where K is the number of actions, L is the length of each action sequence, T is the number of rounds, and N is the number of policies. If the linear transformation is unknown, we show that an algorithm that first explores to learn the unknown weights via linear regression and thereafter uses the estimated weights can achieve \u00d5(\u2016w\u20161(KT ) \u221a lnN) regret, where w is the true (unknown) weight vector. Both algorithms use an optimization oracle to avoid explicit enumeration of the policies and consequently are computationally efficient whenever an efficient algorithm for the fully supervised setting is available.", "creator": "LaTeX with hyperref package"}}}