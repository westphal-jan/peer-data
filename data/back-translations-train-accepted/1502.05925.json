{"id": "1502.05925", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Feature-Budgeted Random Forest", "abstract": "We seek decision rules for prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-specified {\\it average} feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which amplifies costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost-weighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate superior accuracy-cost curves against state-of-the-art prediction-time algorithms.", "histories": [["v1", "Fri, 20 Feb 2015 16:42:40 GMT  (530kb,D)", "http://arxiv.org/abs/1502.05925v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["feng nan", "joseph wang", "venkatesh saligrama"], "accepted": true, "id": "1502.05925"}, "pdf": {"name": "1502.05925.pdf", "metadata": {"source": "META", "title": "Feature-Budgeted Random Forest", "authors": ["Feng Nan", "Joseph Wang", "Venkatesh Saligrama"], "emails": ["FNAN@BU.EDU", "JOEWANG@BU.EDU", "SRV@BU.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, the time has come for us to put ourselves in a position to lead the way into the future."}, {"heading": "2. Feature-Budgeted Random Forest", "text": "First, we present the general problem of learning under predictive time budgets similar to the formulation in (Trapeznikov & Saligrama, 2013; Wang et al., 2014b). Suppose sample / label pairs (x, y) are randomly distributed as (x, y) d trees. The goal is to learn a classifier f from a family of functions F that minimizes the expected loss under budget constraints: min f-FExy [L (y, f (x))], s.t. Ex [C (f, x), (1), where L (y, y) is a loss function, C (f, x) are the costs of evaluating the function of f in the example of x andB is a user-specified budget constraint. In this paper, we assume that the function acquisition cost C (f, x) is a modular function of supporting the functions f in the example of x, that is that the acquisition of each feature has fixed cost."}, {"heading": "2.1. Our Algorithm", "text": "In fact, it is as if it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about which it is about a way in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is not, which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is not, which it is about which is about which it is about which it is not, which is not, which is about which is about which is not, which is, which is not, which is, which is not, which is, which is not, which is, which is not, which is, which is, which is not, which is, which is not, as it is, as it is not, which is not, which is, which is not, which is, which is not, which is not, which is, but which is, which is not, which is, which is not, which is not, which is, is,"}, {"heading": "2.2. Bounding the Cost of Each Tree", "text": "(D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D).). (D). (D). (D).). (D). (D). (D). (D). (D).). (D).). (D).). (D). (D). (D).). (D). (D).).). (D). (D).). (D).). (D).). (D).). (D).). (D). (D).). (D). (D). (D).).). (D). (D).). (D).). (D). (D).). (D).). (D). (D). (D).).). (D). (D). (D).). (D).). (D)."}, {"heading": "2.3. Admissible Impurity Functions", "text": "A wide range of functions fall into the class of permissible impurity functions. In our work, we use a specific function called threshold pairs, where niG denotes the number of objects in G belonging to class i, [x] + = max (x, 0) and \u03b1 is a threshold parameter. We include proof of the following problem in the appendix. Lemma 2.3 F\u03b1 (G) is permissible. Neither entropy nor Gini index fulfill the notion of admissibility, because they are not monotonous set functions, i.e. a subset of examples does not necessarily have a smaller entropy or Gini index compared to the whole set. Therefore, traditional decision tree learning algorithms do not take into account the cost of features and do not guarantee the maximum cost as stated in our paper. We have examined more impurity functions that are permissible because they are not significant on the smaller plasmas."}, {"heading": "2.4. Discussions of the Algorithm", "text": "Before we complete the BUDGETRF algorithm and its analysis, we will discuss further various design issues and their implications. The selection of thresholds \u03b1. In the GREEDYTREE undervaluation, each tree is therefore greedily built until a minimum leaf impurity is achieved, then the random examples are added to the random forest. Threshold \u03b1 can be used to find a balance between average tree depth and number of trees. However, the lower results in deeper trees with higher classification performance and acquisition costs are many added to the random forest before the budget constraint is met. Conversely, a higher yield of flatter trees with poorer classification performance, but due to the low cost of each tree, many are added to the random forest before the budget constraint is met. As such, we can consider bias as a trade variance."}, {"heading": "3. Experiments", "text": "For the establishment of base comparisons, we use the same configuration / validation / validation / validation / validation as comparison data sets in 4 real world benchmarked datasets. The first has different feature acquisition costs in terms of computation time and the purpose is to show our algorithms that can achieve high accuracy during prediction while saving enormous amount of feature acquisition time. The other 3 datasets have no explicit feature cost; instead, we assign each feature cost that is uniformed for each feature. The purpose is to show our algorithm using a small fraction of the features, with our algorithm being adaptive only a small fraction of the features, meaning it acquires different features for different features during testing. Thus, the feature cost in the plots is understood as an average of the cost of all test examples. We use CSTC et al., 2013) and ASTC (state performance, meaning it acquires different features for different features during testing. Thus, the feature cost in the plots is understood as an average of the cost of all test examples."}, {"heading": "4. Conclusion and Future Work", "text": "We propose a novel algorithm to solve the budgeted learning problem. Our approach is to build a random forest of low-cost trees with theoretical guarantees. We show that our algorithmic performance far outperforms the most advanced algorithms on 4 real-world benchmarked datasets. While we have explored the greedy algorithm based on minimax splits, similar algorithms can be proposed based on expectation splits. An interesting future work is to investigate the theoretical and empirical properties of such algorithms."}], "references": [{"title": "Group-based active query selection for rapid diagnosis in time-critical situations", "author": ["G Bellala", "S.K. Bhavnani", "C. Scott"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Bellala et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellala et al\\.", "year": 2012}, {"title": "Fast classification using sparse decision dags", "author": ["R. Busa-Fekete", "D. Benbouzid", "B. K\u00e9gl"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2012}, {"title": "Classifier cascade: Tradeoff between accuracy and feature evaluation cost", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "O. Chapelle", "D. Kedem"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Diagnosis determination: decision trees optimizing simultaneously worst and expected testing cost", "author": ["F Cicalese", "Laber", "E. S", "A.M. Saettler"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Cicalese et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cicalese et al\\.", "year": 2014}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.G. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates and Ng,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng", "year": 2011}, {"title": "Datum-wise classification: a sequential approach to sparsity", "author": ["G. Dulac-Arnold", "L. Denoyer", "P. Preux", "P. Gallinari"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2011}, {"title": "Active classification based on value of classifier", "author": ["T. Gao", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gao and Koller,? \\Q2011\\E", "shortCiteRegEx": "Gao and Koller", "year": 2011}, {"title": "Imitation learning by coaching", "author": ["H He", "H Daume III", "J. Eisner"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Cost-sensitive feature acquisition and classification", "author": ["S Ji", "L. Carin"], "venue": "Pattern Recognition,", "citeRegEx": "Ji and Carin,? \\Q2007\\E", "shortCiteRegEx": "Ji and Carin", "year": 2007}, {"title": "Prediction-time Active FeatureValue Acquisition for Cost-Effective Customer Targeting", "author": ["P. Kanani", "P. Melville"], "venue": "In Advances In Neural Information Processing Systems (NIPS),", "citeRegEx": "Kanani and Melville,? \\Q2008\\E", "shortCiteRegEx": "Kanani and Melville", "year": 2008}, {"title": "Breaking boundaries: Active information acquisition across learning and diagnosis", "author": ["A Kapoor", "E. Horvitz"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Kapoor and Horvitz,? \\Q2009\\E", "shortCiteRegEx": "Kapoor and Horvitz", "year": 2009}, {"title": "Dynamic feature selection for classification on a budget", "author": ["S Karayev", "M Fritz", "T. Darrell"], "venue": "In International Conference on Machine Learning (ICML): Workshop on Prediction with Sequential Models,", "citeRegEx": "Karayev et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karayev et al\\.", "year": 2013}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Krizhevsky", "Alex"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Feature-cost sensitive learning with submodular trees of classifiers", "author": ["M Kusner", "W Chen", "Q Zhou", "E Zhixiang", "K Weinberger", "Y. Chen"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Kusner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2014}, {"title": "Information-based objective functions for active data selection", "author": ["D.J.C. MacKay"], "venue": "Neural computation,", "citeRegEx": "MacKay,? \\Q1992\\E", "shortCiteRegEx": "MacKay", "year": 1992}, {"title": "Greedy algorithm with weights for decision tree construction", "author": ["M.J. Moshkov"], "venue": "Fundam. Inf.,", "citeRegEx": "Moshkov,? \\Q2010\\E", "shortCiteRegEx": "Moshkov", "year": 2010}, {"title": "Generalized binary search", "author": ["Nowak", "Robert"], "venue": "Proceedings of the 46th Allerton Conference on Communications, Control, and Computing,", "citeRegEx": "Nowak and Robert.,? \\Q2008\\E", "shortCiteRegEx": "Nowak and Robert.", "year": 2008}, {"title": "Supervised sequential classification under budget constraints", "author": ["K Trapeznikov", "V. Saligrama"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Trapeznikov and Saligrama,? \\Q2013\\E", "shortCiteRegEx": "Trapeznikov and Saligrama", "year": 2013}, {"title": "Robust Real-time Object Detection", "author": ["P Viola", "M. Jones"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola and Jones,? \\Q2001\\E", "shortCiteRegEx": "Viola and Jones", "year": 2001}, {"title": "Model selection by linear programming", "author": ["J. Wang", "T. Bolukbasi", "K Trapeznikov", "V. Saligrama"], "venue": "In European Conference on Computer Vision, pp", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "An lp for sequential learning under budgets", "author": ["J Wang", "K Trapeznikov", "V. Saligrama"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Costsensitive tree of classifiers", "author": ["Z Xu", "M Kusner", "M Chen", "K.Q. Weinberger"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "A Survey of Recent Advances in Face Detection", "author": ["C. Zhang", "Zhang"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "Related Work: The problem of learning from full training data for prediction-time cost reduction (MacKay, 1992; Kanani & Melville, 2008) has been extensively studied.", "startOffset": 97, "endOffset": 136}, {"referenceID": 2, "context": "One simple structure for incorporating costs into learning is through detection cascades (Viola & Jones, 2001; Zhang & Zhang, 2010; Chen et al., 2012), where cheap features are used to discard examples belonging to the negative class.", "startOffset": 89, "endOffset": 150}, {"referenceID": 11, "context": "To overcome the need to estimate distributions, reinforcement learning (Karayev et al., 2013; BusaFekete et al., 2012; Dulac-Arnold et al., 2011) and imitation learning (He et al.", "startOffset": 71, "endOffset": 145}, {"referenceID": 5, "context": "To overcome the need to estimate distributions, reinforcement learning (Karayev et al., 2013; BusaFekete et al., 2012; Dulac-Arnold et al., 2011) and imitation learning (He et al.", "startOffset": 71, "endOffset": 145}, {"referenceID": 7, "context": ", 2011) and imitation learning (He et al., 2012) approaches have also been studied, where the reward or oracle action is predicted, however these generally require classifiers capable of operating on a wide range of missing feature patterns.", "startOffset": 31, "endOffset": 48}, {"referenceID": 3, "context": "Construction of simple decision trees with low costs has also been studied for discrete function evaluation problems (Cicalese et al., 2014; Moshkov, 2010; Bellala et al., 2012).", "startOffset": 117, "endOffset": 177}, {"referenceID": 15, "context": "Construction of simple decision trees with low costs has also been studied for discrete function evaluation problems (Cicalese et al., 2014; Moshkov, 2010; Bellala et al., 2012).", "startOffset": 117, "endOffset": 177}, {"referenceID": 0, "context": "Construction of simple decision trees with low costs has also been studied for discrete function evaluation problems (Cicalese et al., 2014; Moshkov, 2010; Bellala et al., 2012).", "startOffset": 117, "endOffset": 177}, {"referenceID": 21, "context": "Without the cost constraint, the problem is equivalent to a supervised learning problem, however, adding the cost constraint makes this a combinatorial problem (Xu et al., 2013).", "startOffset": 160, "endOffset": 177}, {"referenceID": 13, "context": "Comparison of BUDGETRF against ASTC (Kusner et al., 2014) and CSTC (Xu et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 21, "context": ", 2014) and CSTC (Xu et al., 2013) on 4 real world datasets.", "startOffset": 17, "endOffset": 34}, {"referenceID": 21, "context": "We use CSTC (Xu et al., 2013) and ASTC (Kusner et al.", "startOffset": 12, "endOffset": 29}, {"referenceID": 13, "context": ", 2013) and ASTC (Kusner et al., 2014) for comparison because they have been shown to have state-of-the-art cost-error performance.", "startOffset": 17, "endOffset": 38}, {"referenceID": 13, "context": "The algorithm parameters for ASTC are set using the same configuration as in (Kusner et al., 2014).", "startOffset": 77, "endOffset": 98}, {"referenceID": 13, "context": "We report values for CSTC from (Kusner et al., 2014).", "startOffset": 31, "endOffset": 52}, {"referenceID": 13, "context": "We use the Average Precision@5 as performance metric, same as that used in (Kusner et al., 2014).", "startOffset": 75, "endOffset": 96}], "year": 2015, "abstractText": "We seek decision rules for prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-specified average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which amplifies costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost-weighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate superior accuracy-cost curves against state-of-the-art prediction-time algorithms.", "creator": "LaTeX with hyperref package"}}}