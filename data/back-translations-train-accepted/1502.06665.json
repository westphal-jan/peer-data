{"id": "1502.06665", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2015", "title": "Reified Context Models", "abstract": "A classic tension exists between exact inference in a simple model and approximate inference in a complex model. The latter offers expressivity and thus accuracy, but the former provides coverage of the space, an important property for confidence estimation and learning with indirect supervision. In this work, we introduce a new approach, reified context models, to reconcile this tension. Specifically, we let the amount of context (the arity of the factors in a graphical model) be chosen \"at run-time\" by reifying it---that is, letting this choice itself be a random variable inside the model. Empirically, we show that our approach obtains expressivity and coverage on three natural language tasks.", "histories": [["v1", "Tue, 24 Feb 2015 01:26:43 GMT  (6804kb,D)", "http://arxiv.org/abs/1502.06665v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jacob steinhardt", "percy liang"], "accepted": true, "id": "1502.06665"}, "pdf": {"name": "1502.06665.pdf", "metadata": {"source": "META", "title": "Reified Context Models", "authors": ["Jacob Steinhardt", "Percy Liang"], "emails": ["JSTEINHARDT@CS.STANFORD.EDU", "PLIANG@CS.STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "Many structured prediction tasks relating to natural language processing, computer vision and computer-aided biology can only be formulated in such a way that we exclude a distribution of results (1: 1,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,....,....,...,....,......,.....,...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2. Description of Tasks", "text": "It is indeed the case that we will be able to find the path we have chosen to follow. \""}, {"heading": "3. Reified Context Models", "text": "We now formally introduce a contextual model, this collection of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of sets of"}, {"heading": "4. Adaptive Context Sets", "text": "The previous section shows how to set a tractable model for each collection of canonical contexts Ci. We now show how to select such sentences adaptively, at runtime. We use a heuristic model motivated by beam search, which greedily selects the configurations with the highest score of y1: i based on an estimate of their mass. We work at a higher level of abstraction by selecting contexts instead of configurations; this allows us to maintain coverage while we are still adaptable. Our idea has already been illustrated to an extent in Figures 1 and 2: If some of our contexts are very rough (such as?? in Figure 2) and others are much more limited (such as abc in Figure 2), then we can achieve coverage of space while still modeling complex dependencies. We will do this by modelling each context c..."}, {"heading": "4.1. Relationship to beam search", "text": "The idea of greedily selecting contexts based on qi\u03b8 is similar in the spirit of bar search, an approximate inference algorithm that greedily selects individual values of y1: i based on qi\u03b8. Formally, the bar search maintains a bar Bi Y1: i of size B constructed as follows: \u2022 Let B i = Bi \u2212 1 \u00b7 Yi. \u2022 Calculates the mass of each element of B i under qi\u03b8. \u2022 Let Bi be the B elements of B i with the highest mass. The similarity can be specified: The bar search is a degenerated instance of our procedure. To return to bar search (which has no range), we add an additional feature to our model: I [ci = Y1]. This feature simply helps us increase the weight of each element of Bi: i to ensure coverage."}, {"heading": "4.2. Featurizations", "text": "We end this section with a recipe for selecting characteristics \u03c6i (ci \u2212 1, yi). We focus on n-grams and orientation characteristics that we use in our experiments.n-gram characteristics. We look at Markov chains via text nth-order, typically characterized by (n + 1) -grams: \u03c6i (y1: i \u2212 1, yi) = (I [yi \u2212 n: i = y) y-yi-n: i. (5) To extend this to our setting, we define Yi = Yi- {?} and Y1: i = i-j = 1 Yi. We can identify each pair (ci \u2212 1, yi) with a sequence s-Yi-n (ci \u2212 1, yi) in the same way as before: in each position j \u2264 i-y where yj-y is determined: (ci \u2212 1, yi), sj = yj."}, {"heading": "5. Generating High Precision Predictions", "text": "Let us remember that a symptom resulting from a lack of coverage is a poor estimate of uncertainty and the inability to generate highly accurate predictions. In this section, we show that the coverage offered by RCMS mitigates this problem compared to beam search. Specifically, we are interested in whether an algorithm can find a large subset of test examples that it can classify with high accuracy (\u2248 99%). Formally, we assume that a method gives a prediction y with reliability p [0, 1] for each example. We sort the examples by confidence and see what fraction R of examples we can answer before our accuracy falls below a certain threshold P. In this case, P is precision and R is repeatability. Good recall at a high precision level (e.g. P = 0.99) is useful in applications where we can pass predictions below the accuracy threshold for a human being, but we still need to automatically verify these examples as many as possible."}, {"heading": "6. Learning with Indirect Supervision", "text": "The second symptom of the lack of coverage is the inability to learn from indirect surveillance. In this context, we have an exponential family model based on the following principles: \"We have an exact model of (marginal) logarithmetic.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" We. \"\" \"We.\". \"\" \"We.\". \"\" \"\" We. \".\" \"\" We. \".\". \"\" \"We.\". \".\" \"\" \"We.\". \"\" \"\" We. \"\" \"\" We. \"\" \"We..\" \"\""}, {"heading": "7. Refinement of Contexts During Training", "text": "When learning with indirect supervision and approximate conclusions, one intuition is that we can learn \"bootstrap\" by first learning from simple examples and then using the information gleaned from these examples to draw better conclusions about the remaining examples (Liang et al., 2011). However, this can fail if there are insufficiently many simple examples (as in the language task), if the examples are difficult to identify, or if they are statistically different from the remaining examples. We consider the above to be \"vertical bootstrapping\": using the full model on an increasing number of examples. RCMS instead performs \"horizontal bootstrapping\": for each example, it selects a model (via the context sets) based on the information available. As the training progresses, we expect these contexts to become increasingly fine as our parameters improve. To measure this quantitatively, we define the length of a context from 1 to the number \u2212 yci."}, {"heading": "8. Related work", "text": "This year, we have reached the point where we are able to create a new system in which we have to play by the rules in order to reform it, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "9. Discussion", "text": "We have presented a new framework, reified context models that mature context as a random variable, defining a family of expressive but comprehensible probability distributions. By adaptive selection of context sets at runtime, our RCMS method uses short contexts in regions of high uncertainty and long contexts in regions of low uncertainty, thus reproducing the behavior of coarse to fine-grained training methods in an organic and fine-grained manner. Furthermore, because RCMS maintains full coverage of space, it is able to break the precision upper limit faced by beam search. Coverage also helps with indirect supervision training, since we can better identify settings of latent variables that assign a high probability to the data. At a high level, our method provides a framework for structuring conclusions with respect to the contexts it takes into account; because the context lies more likely in each of the contexts we can support, as well as in the context we can."}, {"heading": "B. Further Details of Experimental Setup", "text": "During the training with AdaGrad, we performed several stochastic gradient updates in parallel, similar to the approach described in Recht et al. (2011) (although we paralleled even more aggressively at the expense of theoretical guarantees), and we used a randomized truncation scheme to round most small gradient coordinates to zero, which significantly reduces memory usage and simultaneous overhead. To decrypt, we used absolute discounting at a discount of 0.25 and smoothing 0.01, and laplace smoothing with parameters 0.01. In the 1st order model, bar searching works better when we use laplace smoothing instead of absolute discounting (though even worse than RCMS). To maintain a uniform experimental setup, we excluded this result from the main text. In the hybrid selection algorithm in the language experiments, we merge the bars at each step (as opposed to calculating two individual bars at the end)."}, {"heading": "C. Additional Files", "text": "In the supplementary material we also insert the source code and the data sets for the decryption task. A README is attached to explain how these experiments are to be carried out."}], "references": [{"title": "An overview of existing methods and recent advances in sequential Monte Carlo", "author": ["Capp\u00e9", "Olivier", "Godsill", "Simon J", "Moulines", "Eric"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2007}, {"title": "Search-based structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "A tutorial on particle filtering and smoothing: Fifteen years later", "author": ["Doucet", "Arnaud", "Johansen", "Adam M"], "venue": "In Oxford Handbook of Nonlinear Filtering. Citeseer,", "citeRegEx": "Doucet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Training structural svms when exact inference is intractable", "author": ["Finley", "Thomas", "Joachims", "Thorsten"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Finley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finley et al\\.", "year": 2008}, {"title": "Prosodylab-aligner: A tool for forced alignment of laboratory speech", "author": ["Gorman", "Kyle", "Howell", "Jonathan", "Wagner", "Michael"], "venue": "Canadian Acoustics,", "citeRegEx": "Gorman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gorman et al\\.", "year": 2011}, {"title": "Insights into spoken language gleaned from phonetic transcription of the switchboard corpus", "author": ["Greenberg", "Steven", "Hollenback", "Joy", "Ellis", "Dan"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "Greenberg et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Greenberg et al\\.", "year": 1996}, {"title": "Structured perceptron with inexact search", "author": ["Huang", "Liang", "Fayong", "Suphan", "Guo", "Yang"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A comparison of approaches to on-line handwritten character recognition", "author": ["Kassel", "Robert H"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Kassel and H.,? \\Q1995\\E", "shortCiteRegEx": "Kassel and H.", "year": 1995}, {"title": "Improved backingoff for m-gram language modeling", "author": ["Kneser", "Reinhard", "Ney", "Hermann"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Statistical phrase-based translation", "author": ["Koehn", "Philipp", "Och", "Franz Josef", "Marcu", "Daniel"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Structured learning with approximate inference", "author": ["Kulesza", "Alex", "Pereira", "Fernando"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kulesza et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2007}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Association for Computational Linguistics (ACL), pp", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Approximate inference for infinite contingent bayesian networks", "author": ["Milch", "Brian", "Marthi", "Bhaskara", "Sontag", "David", "Russell", "Stuart", "Ong", "Daniel L", "Kolobov", "Andrey"], "venue": "In Proc. 10th AISTATS,", "citeRegEx": "Milch et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Milch et al\\.", "year": 2005}, {"title": "On structuring probabilistic dependences in stochastic language modelling", "author": ["Ney", "Hermann", "Essen", "Ute", "Kneser", "Reinhard"], "venue": "Computer Speech & Language,", "citeRegEx": "Ney et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ney et al\\.", "year": 1994}, {"title": "Exchangeable variable models", "author": ["Niepert", "Mathias", "Domingos", "Pedro"], "venue": "arXiv preprint arXiv:1405.0501,", "citeRegEx": "Niepert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Niepert et al\\.", "year": 2014}, {"title": "Improved decipherment of homophonic ciphers", "author": ["Nuhn", "Malte", "Ney", "Hermann"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Nuhn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nuhn et al\\.", "year": 2014}, {"title": "Beam search for solving substitution ciphers", "author": ["Nuhn", "Malte", "Schamper", "Julian", "Ney", "Hermann"], "venue": "In Annual Meeting of the Assoc. for Computational Linguistics,", "citeRegEx": "Nuhn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nuhn et al\\.", "year": 2013}, {"title": "Sparse forward-backward using minimum divergence beams for fast training of conditional random fields", "author": ["Pal", "Chris", "Sutton", "Charles", "McCallum", "Andrew"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Pal et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pal et al\\.", "year": 2006}, {"title": "Coarse-to-fine natural language processing", "author": ["Petrov", "Slav", "Charniak", "Eugene"], "venue": "Springer Science & Business Media,", "citeRegEx": "Petrov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2011}, {"title": "Sum-product networks: A new deep architecture", "author": ["Poon", "Hoifung", "Domingos", "Pedro"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "Poon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2011}, {"title": "Attacking letter substitution ciphers with integer programming. Cryptologia", "author": ["Ravi", "Sujith", "Knight", "Kevin"], "venue": null, "citeRegEx": "Ravi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2009}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Recht", "Benjamin", "Re", "Christopher", "Wright", "Stephen", "Niu", "Feng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Learning where to sample in structured prediction", "author": ["Shi", "Tianlin", "Steinhardt", "Jacob", "Liang", "Percy"], "venue": null, "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Approximate inference in graphical models using LP relaxations", "author": ["Sontag", "David Alexander"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Sontag and Alexander.,? \\Q2010\\E", "shortCiteRegEx": "Sontag and Alexander.", "year": 2010}, {"title": "Filtering with abstract particles", "author": ["Steinhardt", "Jacob", "Liang", "Percy"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Steinhardt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Steinhardt et al\\.", "year": 2014}, {"title": "A new class of upper bounds on the log partition function", "author": ["Wainwright", "Martin J", "Jaakkola", "Tommi S", "Willsky", "Alan S"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "Sidestepping intractable inference with structured ensemble cascades", "author": ["Weiss", "David", "Sapp", "Benjamin", "Taskar", "Ben"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Weiss et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2010}, {"title": "Structured prediction cascades", "author": ["Weiss", "David", "Sapp", "Benjamin", "Taskar", "Ben"], "venue": "arXiv preprint arXiv:1208.3279,", "citeRegEx": "Weiss et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2012}, {"title": "A stochastic memoizer for sequence data", "author": ["Wood", "Frank", "Archambeau", "C\u00e9dric", "Gasthaus", "Jan", "James", "Lancelot", "Teh", "Yee Whye"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Wood et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2009}, {"title": "A generalized mean field algorithm for variational inference in exponential families", "author": ["Xing", "Eric P", "Jordan", "Michael I", "Russell", "Stuart"], "venue": "In Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Maxviolation perceptron and forced decoding for scalable mt training", "author": ["Yu", "Heng", "Huang", "Liang", "Mi", "Haitao", "Zhao", "Kai"], "venue": "In EMNLP,", "citeRegEx": "Yu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2013}, {"title": "Online learning for inexact hypergraph search", "author": ["Zhang", "Hao", "Huang", "Liang", "Zhao", "Kai", "McDonald", "Ryan"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Context-assisted face clustering framework with human-in-the-loop", "author": ["Zhang", "Liyan", "Kalashnikov", "Dmitri V", "Mehrotra", "Sharad"], "venue": "International Journal of Multimedia Information Retrieval,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "although we parallelized even more aggressively at the expense of theoretical guarantees). We also used a randomized truncation scheme to round most small coordinates of the gradients", "author": ["Recht"], "venue": null, "citeRegEx": "Recht,? \\Q2011\\E", "shortCiteRegEx": "Recht", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": ", 2011), sequential Monte Carlo (Capp\u00e9 et al., 2007; Doucet & Johansen, 2011), or beam search (Koehn et al.", "startOffset": 32, "endOffset": 77}, {"referenceID": 10, "context": ", 2007; Doucet & Johansen, 2011), or beam search (Koehn et al., 2003).", "startOffset": 49, "endOffset": 69}, {"referenceID": 33, "context": "\u2022 precision: In user-facing applications, it is important to only predict on inputs where the system is confident, leaving hard decisions to the user (Zhang et al., 2014).", "startOffset": 150, "endOffset": 170}, {"referenceID": 31, "context": "An approximate inference algorithm might not even consider the true y (whereas one always has the true y in a fully-supervised setting), which leads to invalid parameter updates (Yu et al., 2013).", "startOffset": 178, "endOffset": 195}, {"referenceID": 27, "context": "Word recognition The first task is the word recognition task from Kassel (1995); we use the \u201cclean\u201d version of the dataset as in Weiss et al. (2012). This contains 6, 876 examples, split into 10 folds (numbered 0 to 9); we used fold 1 for testing and the rest for training.", "startOffset": 129, "endOffset": 149}, {"referenceID": 6, "context": "Speech recognition Our second task is from the Switchboard speech transcription project (Greenberg et al., 1996).", "startOffset": 88, "endOffset": 112}, {"referenceID": 3, "context": "To train the models, we maximized the approximate log-likelihood using AdaGrad (Duchi et al., 2010) with a step size \u03b7 of 0.", "startOffset": 79, "endOffset": 99}, {"referenceID": 5, "context": "\u2022 Forced decoding (Gorman et al., 2011): first train a simple model for which exact inference is tractable to infer the most likely z, conditioned on x and y.", "startOffset": 18, "endOffset": 39}, {"referenceID": 14, "context": "We used the given plain text to learn the transition probabilities, using absolute discounting (Ney et al., 1994) for smoothing.", "startOffset": 95, "endOffset": 113}, {"referenceID": 17, "context": "We measured performance by mapping accuracy: the fraction of unique symbols that are correctly mapped (Nuhn et al., 2013).", "startOffset": 102, "endOffset": 121}, {"referenceID": 12, "context": "When learning with indirect supervision and approximate inference, one intuition is that we can \u201cbootstrap\u201d by first learning from easy examples, and then using the information gained from these examples to make better inferences about the remaining ones (Liang et al., 2011).", "startOffset": 255, "endOffset": 275}, {"referenceID": 7, "context": "The first modifies the learning updates to account for the inference procedure, as in the max-violation perceptron and related algorithms (Huang et al., 2012; Zhang et al., 2013; Yu et al., 2013); reinforcement learning approaches to inference (Daum\u00e9 III et al.", "startOffset": 138, "endOffset": 195}, {"referenceID": 32, "context": "The first modifies the learning updates to account for the inference procedure, as in the max-violation perceptron and related algorithms (Huang et al., 2012; Zhang et al., 2013; Yu et al., 2013); reinforcement learning approaches to inference (Daum\u00e9 III et al.", "startOffset": 138, "endOffset": 195}, {"referenceID": 31, "context": "The first modifies the learning updates to account for the inference procedure, as in the max-violation perceptron and related algorithms (Huang et al., 2012; Zhang et al., 2013; Yu et al., 2013); reinforcement learning approaches to inference (Daum\u00e9 III et al.", "startOffset": 138, "endOffset": 195}, {"referenceID": 23, "context": ", 2013); reinforcement learning approaches to inference (Daum\u00e9 III et al., 2009; Shi et al., 2015) also fit into this category.", "startOffset": 56, "endOffset": 98}, {"referenceID": 27, "context": "Another approach modifies the inference algorithm to obtain better coverage, as in coarse-to-fine inference (Petrov et al., 2006; Weiss et al., 2010), where simple models are used to direct the focus of more complex models.", "startOffset": 108, "endOffset": 149}, {"referenceID": 1, "context": ", 2013); reinforcement learning approaches to inference (Daum\u00e9 III et al., 2009; Shi et al., 2015) also fit into this category. Another approach modifies the inference algorithm to obtain better coverage, as in coarse-to-fine inference (Petrov et al., 2006; Weiss et al., 2010), where simple models are used to direct the focus of more complex models. Pal et al. (2006) encourage coverage for beam search by adaptively increasing the beam size.", "startOffset": 63, "endOffset": 370}, {"referenceID": 30, "context": "duality gaps from convex programs (Sontag, 2010) or variational bounds (Xing et al., 2002; Wainwright et al., 2005).", "startOffset": 71, "endOffset": 115}, {"referenceID": 26, "context": "duality gaps from convex programs (Sontag, 2010) or variational bounds (Xing et al., 2002; Wainwright et al., 2005).", "startOffset": 71, "endOffset": 115}, {"referenceID": 14, "context": "Certain smoothing techniques in natural language processing also interpolate between contexts of different order, such as absolute discounting (Ney et al., 1994) and KneserNey smoothing (Kneser & Ney, 1995).", "startOffset": 143, "endOffset": 161}, {"referenceID": 13, "context": "Some Bayesian nonparametric approaches such as infinite contingent Bayesian networks (Milch et al., 2005) and hierarchical Pitman-Yor processes (Teh, 2006; Wood et al.", "startOffset": 85, "endOffset": 105}, {"referenceID": 29, "context": ", 2005) and hierarchical Pitman-Yor processes (Teh, 2006; Wood et al., 2009) also reason about contexts; again, such models do not lead to tractable inference.", "startOffset": 46, "endOffset": 76}], "year": 2015, "abstractText": "A classic tension exists between exact inference in a simple model and approximate inference in a complex model. The latter offers expressivity and thus accuracy, but the former provides coverage of the space, an important property for confidence estimation and learning with indirect supervision. In this work, we introduce a new approach, reified context models, to reconcile this tension. Specifically, we let the amount of context (the arity of the factors in a graphical model) be chosen \u201cat run-time\u201d by reifying it\u2014that is, letting this choice itself be a random variable inside the model. Empirically, we show that our approach obtains expressivity and coverage on three natural language tasks.", "creator": "LaTeX with hyperref package"}}}