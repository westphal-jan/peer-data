{"id": "1502.06668", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2015", "title": "Learning Fast-Mixing Models for Structured Prediction", "abstract": "Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate inference inside learning, but their slow mixing can be difficult to diagnose and the approximations can seriously degrade learning. To alleviate these issues, we define a new model family using strong Doeblin Markov chains, whose mixing times can be precisely controlled by a parameter. We also develop an algorithm to learn such models, which involves maximizing the data likelihood under the induced stationary distribution of these chains. We show empirical improvements on two challenging inference tasks.", "histories": [["v1", "Tue, 24 Feb 2015 01:42:09 GMT  (75kb,D)", "http://arxiv.org/abs/1502.06668v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jacob steinhardt", "percy liang"], "accepted": true, "id": "1502.06668"}, "pdf": {"name": "1502.06668.pdf", "metadata": {"source": "META", "title": "Learning Fast-Mixing Models for Structured Prediction", "authors": ["Jacob Steinhardt", "Percy Liang"], "emails": ["JSTEINHARDT@CS.STANFORD.EDU", "PLIANG@CS.STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "We have two notable disadvantages: (i) the diagnosis by Markov and Pereira, 2007). In this paper, we use MCMC to define the model family itself: For a given T, we construct a family of Markov chains that uses arbitrary rich characteristics, but whose mixing with most O (T) is guaranteed."}, {"heading": "2. A Fast-Mixing Family of Markov Chains", "text": "Considering a Markov chain with transition matrix A (yt | yt \u2212 1) and a distribution chain u (yt), we define a new Markov chain with transitions given by A (yt | yt \u2212 1) def = (1 \u2212) A (yt | yt \u2212 1) + u (yt). (We suppress dependence on \u03b8 and x for the time being.) In the matrix notation, we can write A (yt | yt \u2212 1) A (def = (1 \u2212) A + u1 >. (2) In other words, with the probability we start from u; otherwise, according to A. Intuitively, A (n) should mingle quickly, because a restart of u makes the past independent of the future (we formalize this in Section 3). We consider u as a simple traceable model that provides coverage, and A as a complex model that provides precision. Simple example: To gain a certain degree of intuition, we are presented with an intuition in Figure 1."}, {"heading": "3. Theoretical Properties", "text": "In this section we characterize the stationary distribution and mixing time of A-2 (A-2), with the stationary distribution of A-2 (A-2) being the second largest eigenvalue (in complex norm). A standard result for Markov chains is that under mild assumptions the mixing time of A-2 (A-2) is defined as the mixing time of A-2 (A-2) the mixing time of A-2 (A-2) is defined as the mixing time of A-2 (A-2). We assume that the entire sec-tion thatA is ergodic but not necessarily reversible. See section 12.4 of Levin et al. (2009) for more details.Our first result refers to the mixing time."}, {"heading": "3.1. Staged strong Doeblin chains", "text": "The intuition is that the sampling from the raw distribution allows a global exploration of the state space, while the refined transition A hones takes place in one mode. However, in complex problems, there could be a considerable gap between what is possible with precise inference (u) and what is required for accurate modeling (A). This motivates us to use several stages of the MCMC to bridge the gap. To do this, we introduce an auxiliary variable z, which is in the phase of the MCMC we are currently in. For the eight days z, we have a Markov chain az (yt \u2212 1) over the original state space. We also define a Markov chain C (zt \u2212 1) over the stages we are in. To transition from (yt \u2212 1, zt \u2212 1) to (yt, zt \u2212 1), we will first define a sample from C (zt \u2212 1) and then a second sample from the original state space."}, {"heading": "4. Learning strong Doeblin chains", "text": "We will focus on the discriminatory learning setting in which we have a Dataset (i), (4), in which now the stationary distribution of A (1), (1), (1), (1), (1), (2), (1), (2), (1), (2), (1), (2), (2), (2), (2), (2), (2), (2), (3), (4), (3), (4), (4), (4), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, 5, (5), (5), (5, 5, 5, (5), (5), (5, 5, 5, 5, (5), (5, 5, 5, 5, 5, 5, 5, (5), (5, 5, 5, 5, 5, 5, 5, 5, 5, (5), (5), (5, 5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5, (5, (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5),"}, {"heading": "4.2. Implementation", "text": "With the theory described above, we now describe some important implementation details of our learning algorithm with algorithm 1 > Algorithm 1 | Algorithm for calculating an estimate of the sequence of the partitioning function (y | x). This estimate is unbiased within the limit of an infinite number of samples k, but is prejudiced for a finite number of samples that are due to the variance in the estimation of the partitioning function.SampleGradient (x, y, \u03b8, k) k is the number of samples, the Z \u2190 0; g \u2190 0 Z is the total mass of all samples, gZ is the gradient for i = 1 to k doSample T \u0445 Geometric () Example y0 from u\u03b8 (\u00b7 x) k is the number of samples, the Z \u2212 1: Example yt \u2264 T \u2212 1: Sample yt from A\u00da (\u00b7 yt \u2212 1, x) is the total mass of all samples, gZ is the gradient for i = 1 dok T. Sample"}, {"heading": "5. Experiments", "text": "These tasks are difficult due to the importance of highly arranged factors; local information is insufficient to identify even highly probable regions of the space (see above). For each word, we have used a time series model to synthetically generate finger gestures for the word. A typical instance of this process is given in Figure 3. The learning task is to discriminatively infer from the intended word that the intended word is the key x that the fingers x that the banana is generated from the word \"Gibjj.\" The learning task is to discriminatively capture the intended word. Given an input x that has the alignment z, the alignment z is either \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"has the alignment z."}, {"heading": "6. Discussion", "text": "We have proposed a family of models based on strong Doeblin-Markov chains that guarantee fast mixing, and our design allows us to simultaneously use a simple, traceable model (u\u03b8) that provides coverage along with a complex, precise model (A\u03b8) that provides precision. As such, we avoid a typical dilemma - whether we use a simple model with an exact conclusion, or deal with the consequences of an approximate conclusion in a more complex model. While our approach works well in practice, there are still some outstanding issues. One is the non-conformity of the learning goal, which makes the process dependent on initialization. Another problem is that the graditions returned by Algorithm 1 can be large, heterogeneous, and highly variant. The adaptive nature of ADAGRAD mitigates this somewhat, but it would still be ideal to have a sampling method that has lower variance than the algorithm 1.gibs method, although there are many practitioners."}, {"heading": "A. Proofs", "text": "In fact, the proof of Proposition 3.1 (A) shows that for all k > 1, \u03bbk (A) = 1 (A) = 1 (A) = 1 (A) (A), with the same eigenvector for both matrices. In other words, while the stationary distribution of A) is different from A (A), all other eigenvectors are invariable. Let's see the eigenvector of A (9) as the eigenvector of A (9) that 1 > wk = 0. This implies that 1 > wk = 1 > wk (8), since A (8) is stochastical, and1 > Awk = \u03bbk1 > wk (9), since wk (9) is an eigenvector of A (6 = 1), that this implies that 1 > wk = 0.Now we have a property of A (1) wk = (1 > wk) that we all have the same."}, {"heading": "B. Correctness of Importance Sampling Algorithm", "text": "In section 4.1 of the main text, we had a distribution u over Y and a Markov chain A (yt | yt-yt -1) in the same space. We then built a distribution over Y-yp = p-yp. \u2212 \u2212 \u2212 YT by using T-yp geometric (), y0-yt-y and yt-1 \u00b2 A for y = 1,. \u2212 T (we use pT (y0: T) to represent the distribution over y0: T. We assume that we are interested in calculating the expectation of a function g: Y-yp (R) and showing that the given weights of importance for (T, y0: T), yt-yt: T [g]."}], "references": [{"title": "A new approach to the limit theory of recurrent Markov chains", "author": ["KB Athreya", "P Ney"], "venue": "Transactions of the AMS,", "citeRegEx": "Athreya and Ney.,? \\Q1978\\E", "shortCiteRegEx": "Athreya and Ney.", "year": 1978}, {"title": "Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing", "author": ["Xavier Carreras", "Michael Collins", "Terry Koo"], "venue": "In CoNLL,", "citeRegEx": "Carreras et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Multilevel coarse-to-fine PCFG parsing", "author": ["I Haxton", "C Hill", "R Shrivaths", "J Moore", "M Pozar"], "venue": "In NAACL,", "citeRegEx": "Haxton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Haxton et al\\.", "year": 2006}, {"title": "Discriminative reranking for natural language parsing", "author": ["Michael Collins", "Terry Koo"], "venue": "Computational Linguistics,", "citeRegEx": "Collins and Koo.,? \\Q2005\\E", "shortCiteRegEx": "Collins and Koo.", "year": 2005}, {"title": "Perfect sampling of Harris recurrent Markov chains", "author": ["JN Corcoran", "RL Tweedie"], "venue": null, "citeRegEx": "Corcoran and Tweedie.,? \\Q1998\\E", "shortCiteRegEx": "Corcoran and Tweedie.", "year": 1998}, {"title": "Markov chain Monte Carlo convergence diagnostics: a comparative review", "author": ["MK Cowles", "BP Carlin"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Cowles and Carlin.,? \\Q1996\\E", "shortCiteRegEx": "Cowles and Carlin.", "year": 1996}, {"title": "Search-based structured prediction", "author": ["H Daum\u00e9 III", "J Langford", "D Marcu"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Elements d\u2019une theorie generale des chaines simples constantes de markoff", "author": ["W Doeblin"], "venue": "In Annales scientifiques de l\u2019E\u0301cole Normale Supe\u0301rieure,", "citeRegEx": "Doeblin.,? \\Q1940\\E", "shortCiteRegEx": "Doeblin.", "year": 1940}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J Duchi", "E Hazan", "Y Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Parallel tempering: Theory, applications, and new perspectives", "author": ["David J Earl", "Michael W Deem"], "venue": "Physical Chemistry Chemical Physics,", "citeRegEx": "Earl and Deem.,? \\Q2005\\E", "shortCiteRegEx": "Earl and Deem.", "year": 2005}, {"title": "Simulating normalizing constants: From importance sampling to bridge sampling to path sampling", "author": ["A Gelman", "XL Meng"], "venue": "Statistical science,", "citeRegEx": "Gelman and Meng.,? \\Q1998\\E", "shortCiteRegEx": "Gelman and Meng.", "year": 1998}, {"title": "A single series from the Gibbs sampler provides a false sense of security", "author": ["A Gelman", "DB Rubin"], "venue": "Bayesian statistics,", "citeRegEx": "Gelman and Rubin.,? \\Q1992\\E", "shortCiteRegEx": "Gelman and Rubin.", "year": 1992}, {"title": "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination", "author": ["PJ Green"], "venue": null, "citeRegEx": "Green.,? \\Q1995\\E", "shortCiteRegEx": "Green.", "year": 1995}, {"title": "Recognition using regions", "author": ["Chunhui Gu", "Joseph J Lim", "Pablo Arbel\u00e1ez", "Jitendra Malik"], "venue": "In CVPR,", "citeRegEx": "Gu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2009}, {"title": "Program verification as probabilistic inference", "author": ["S Gulwani", "N Jojic"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Gulwani and Jojic.,? \\Q2007\\E", "shortCiteRegEx": "Gulwani and Jojic.", "year": 2007}, {"title": "Structured perceptron with inexact search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Structured learning with approximate inference", "author": ["Alex Kulesza", "Fernando Pereira"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kulesza and Pereira.,? \\Q2007\\E", "shortCiteRegEx": "Kulesza and Pereira.", "year": 2007}, {"title": "Markov chains and mixing times", "author": ["DA Levin", "Y Peres", "EL Wilmer"], "venue": null, "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}, {"title": "Computable bounds for geometric convergence rates of Markov chains", "author": ["SP Meyn", "RL Tweedie"], "venue": "The Annals of Applied Probability,", "citeRegEx": "Meyn and Tweedie.,? \\Q1994\\E", "shortCiteRegEx": "Meyn and Tweedie.", "year": 1994}, {"title": "Exact sampling from a continuous state space", "author": ["DJ Murdoch", "PJ Green"], "venue": "Scandinavian Journal of Stat.,", "citeRegEx": "Murdoch and Green.,? \\Q1998\\E", "shortCiteRegEx": "Murdoch and Green.", "year": 1998}, {"title": "Notes on the KLdivergence between a Markov chain and its equilibrium distribution", "author": ["I Murray", "R Salakhutdinov"], "venue": null, "citeRegEx": "Murray and Salakhutdinov.,? \\Q2008\\E", "shortCiteRegEx": "Murray and Salakhutdinov.", "year": 2008}, {"title": "Coarse-to-fine natural language processing", "author": ["S Petrov"], "venue": null, "citeRegEx": "Petrov.,? \\Q2011\\E", "shortCiteRegEx": "Petrov.", "year": 2011}, {"title": "Exact sampling with coupled Markov chains and applications to statistical mechanics", "author": ["JG Propp", "DB Wilson"], "venue": "Random structures and Algorithms,", "citeRegEx": "Propp and Wilson.,? \\Q1996\\E", "shortCiteRegEx": "Propp and Wilson.", "year": 1996}, {"title": "Bounds on regeneration times and convergence rates for Markov chains", "author": ["GO Roberts", "RL Tweedie"], "venue": "Stochastic Processes and their applications,", "citeRegEx": "Roberts and Tweedie.,? \\Q1999\\E", "shortCiteRegEx": "Roberts and Tweedie.", "year": 1999}, {"title": "Minorization conditions and convergence rates for markov chain monte carlo", "author": ["JS Rosenthal"], "venue": "JASA,", "citeRegEx": "Rosenthal.,? \\Q1995\\E", "shortCiteRegEx": "Rosenthal.", "year": 1995}, {"title": "The new york times annotated corpus", "author": ["Evan Sandhaus"], "venue": "Linguistic Data Consortium, Philadelphia,", "citeRegEx": "Sandhaus.,? \\Q2008\\E", "shortCiteRegEx": "Sandhaus.", "year": 2008}, {"title": "Cascaded models for articulated pose estimation", "author": ["B Sapp", "A Toshev", "B Taskar"], "venue": "In ECCV,", "citeRegEx": "Sapp et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sapp et al\\.", "year": 2010}, {"title": "From invariant checking to invariant inference using randomized search", "author": ["R Sharma", "A Aiken"], "venue": "In CAV,", "citeRegEx": "Sharma and Aiken.,? \\Q2014\\E", "shortCiteRegEx": "Sharma and Aiken.", "year": 2014}, {"title": "Discriminative reranking for machine translation", "author": ["Libin Shen", "Anoop Sarkar", "Franz Josef Och"], "venue": "In NAACL,", "citeRegEx": "Shen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2004}, {"title": "Learning where to sample in structured prediction", "author": ["Tianlin Shi", "Jacob Steinhardt", "Percy Liang"], "venue": null, "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["RS Sutton", "D Mcallester", "S Singh", "Y Mansour"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Robust real-time face detection", "author": ["Paul Viola", "Michael J Jones"], "venue": "International journal of computer vision,", "citeRegEx": "Viola and Jones.,? \\Q2004\\E", "shortCiteRegEx": "Viola and Jones.", "year": 2004}, {"title": "Estimating the wrong graphical model: Benefits in the computation-limited setting", "author": ["Martin J Wainwright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Wainwright.,? \\Q2006\\E", "shortCiteRegEx": "Wainwright.", "year": 2006}, {"title": "Sidestepping intractable inference with structured ensemble cascades", "author": ["David Weiss", "Benjamin Sapp", "Ben Taskar"], "venue": "In NIPS,", "citeRegEx": "Weiss et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2010}, {"title": "Discriminative re-ranking of diverse segmentations", "author": ["Payman Yadollahpour", "Dhruv Batra", "Gregory Shakhnarovich"], "venue": "In CVPR,", "citeRegEx": "Yadollahpour et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yadollahpour et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "While this recipe can generate good results in practice, it has two notable drawbacks: (i) diagnosing convergence of Markov chains is extremely difficult (Gelman and Rubin, 1992; Cowles and Carlin, 1996); and (ii) approximate inference can be highly suboptimal in the context of learning (Wainwright, 2006; Kulesza and Pereira, 2007).", "startOffset": 154, "endOffset": 203}, {"referenceID": 5, "context": "While this recipe can generate good results in practice, it has two notable drawbacks: (i) diagnosing convergence of Markov chains is extremely difficult (Gelman and Rubin, 1992; Cowles and Carlin, 1996); and (ii) approximate inference can be highly suboptimal in the context of learning (Wainwright, 2006; Kulesza and Pereira, 2007).", "startOffset": 154, "endOffset": 203}, {"referenceID": 32, "context": "While this recipe can generate good results in practice, it has two notable drawbacks: (i) diagnosing convergence of Markov chains is extremely difficult (Gelman and Rubin, 1992; Cowles and Carlin, 1996); and (ii) approximate inference can be highly suboptimal in the context of learning (Wainwright, 2006; Kulesza and Pereira, 2007).", "startOffset": 288, "endOffset": 333}, {"referenceID": 16, "context": "While this recipe can generate good results in practice, it has two notable drawbacks: (i) diagnosing convergence of Markov chains is extremely difficult (Gelman and Rubin, 1992; Cowles and Carlin, 1996); and (ii) approximate inference can be highly suboptimal in the context of learning (Wainwright, 2006; Kulesza and Pereira, 2007).", "startOffset": 288, "endOffset": 333}, {"referenceID": 7, "context": "We construct Markov chains of the following form, called strong Doeblin chains (Doeblin, 1940):", "startOffset": 79, "endOffset": 94}, {"referenceID": 7, "context": "A classic result is that a given strong Doeblin chain mixes in time at most 1 (Doeblin, 1940), and that we can draw an exact sample from the stationary distribution in expected timeO( 1 ) (Corcoran and Tweedie, 1998).", "startOffset": 78, "endOffset": 93}, {"referenceID": 4, "context": "A classic result is that a given strong Doeblin chain mixes in time at most 1 (Doeblin, 1940), and that we can draw an exact sample from the stationary distribution in expected timeO( 1 ) (Corcoran and Tweedie, 1998).", "startOffset": 188, "endOffset": 216}, {"referenceID": 7, "context": "Markov chains that can be expressed according to (2) are said to have strong Doeblin parameter (Doeblin, 1940).", "startOffset": 95, "endOffset": 110}, {"referenceID": 17, "context": "4 of Levin et al. (2009) for more details.", "startOffset": 5, "endOffset": 25}, {"referenceID": 20, "context": "To show this, we make use of the following lemma from Murray and Salakhutdinov (2008): Lemma 3.", "startOffset": 54, "endOffset": 86}, {"referenceID": 8, "context": "At a high level, we can just use Algorithm 1 to compute estimates of the gradient and then apply an online learning algorithm such as ADAGRAD (Duchi et al., 2011) to identify a good choice of \u03b8.", "startOffset": 142, "endOffset": 162}, {"referenceID": 25, "context": "We generated the data by sampling words from the New York Times corpus (Sandhaus, 2008).", "startOffset": 71, "endOffset": 87}, {"referenceID": 8, "context": "All algorithms are trained with AdaGrad (Duchi et al., 2011) with 16 independent chains run for each example.", "startOffset": 40, "endOffset": 60}, {"referenceID": 15, "context": "(2007) and Huang et al. (2012). In contrast, our method directly optimizes the loglikelihood of the data under the distribution \u03c0\u0303\u03b8, so that accuracy continues to increase with more passes through the training data.", "startOffset": 11, "endOffset": 31}, {"referenceID": 14, "context": "This is an important subroutine in loop invariant synthesis, where MCMC methods have recently shown great promise (Gulwani and Jojic, 2007; Sharma and Aiken, 2014).", "startOffset": 114, "endOffset": 163}, {"referenceID": 27, "context": "This is an important subroutine in loop invariant synthesis, where MCMC methods have recently shown great promise (Gulwani and Jojic, 2007; Sharma and Aiken, 2014).", "startOffset": 114, "endOffset": 163}, {"referenceID": 12, "context": "Though Gibbs sampling is the de facto method for many practitioners, there are also many more sophisticated approaches to MCMC (Green, 1995; Earl and Deem, 2005).", "startOffset": 127, "endOffset": 161}, {"referenceID": 9, "context": "Though Gibbs sampling is the de facto method for many practitioners, there are also many more sophisticated approaches to MCMC (Green, 1995; Earl and Deem, 2005).", "startOffset": 127, "endOffset": 161}, {"referenceID": 30, "context": "Our learning algorithm is reminiscent of policy gradient algorithms in reinforcement learning (Sutton et al., 2000), as well as Searn, which tries to learn an optimal search policy for structured prediction (Daum\u00e9 III et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 10, "context": "Our staged construction is also similar in spirit to path sampling (Gelman and Meng, 1998), as it uses a multi-stage approach to smoothly transition from a very simple to a very complex distribution.", "startOffset": 67, "endOffset": 90}, {"referenceID": 31, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 28, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 3, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 1, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 13, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 33, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 26, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 21, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 34, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 7, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978).", "startOffset": 165, "endOffset": 254}, {"referenceID": 23, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978).", "startOffset": 165, "endOffset": 254}, {"referenceID": 18, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978).", "startOffset": 165, "endOffset": 254}, {"referenceID": 0, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978).", "startOffset": 165, "endOffset": 254}, {"referenceID": 22, "context": "The strong Doeblin property is typically used to study convergence of continuousspace Markov chains, but Rosenthal (1995) has used it to analyze Gibbs sampling, and several authors have provided algorithms for sampling exactly from arbitrary strong Doeblin chains (Propp and Wilson, 1996; Corcoran and Tweedie, 1998; Murdoch and Green, 1998).", "startOffset": 264, "endOffset": 341}, {"referenceID": 4, "context": "The strong Doeblin property is typically used to study convergence of continuousspace Markov chains, but Rosenthal (1995) has used it to analyze Gibbs sampling, and several authors have provided algorithms for sampling exactly from arbitrary strong Doeblin chains (Propp and Wilson, 1996; Corcoran and Tweedie, 1998; Murdoch and Green, 1998).", "startOffset": 264, "endOffset": 341}, {"referenceID": 19, "context": "The strong Doeblin property is typically used to study convergence of continuousspace Markov chains, but Rosenthal (1995) has used it to analyze Gibbs sampling, and several authors have provided algorithms for sampling exactly from arbitrary strong Doeblin chains (Propp and Wilson, 1996; Corcoran and Tweedie, 1998; Murdoch and Green, 1998).", "startOffset": 264, "endOffset": 341}, {"referenceID": 2, "context": ", 2000), as well as Searn, which tries to learn an optimal search policy for structured prediction (Daum\u00e9 III et al., 2009); see also Shi et al. (2015), who apply reinforcement learning in the context of MCMC.", "startOffset": 106, "endOffset": 152}, {"referenceID": 0, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978). The strong Doeblin property is typically used to study convergence of continuousspace Markov chains, but Rosenthal (1995) has used it to analyze Gibbs sampling, and several authors have provided algorithms for sampling exactly from arbitrary strong Doeblin chains (Propp and Wilson, 1996; Corcoran and Tweedie, 1998; Murdoch and Green, 1998).", "startOffset": 232, "endOffset": 378}], "year": 2015, "abstractText": "Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate inference inside learning, but their slow mixing can be difficult to diagnose and the approximations can seriously degrade learning. To alleviate these issues, we define a new model family using strong Doeblin Markov chains, whose mixing times can be precisely controlled by a parameter. We also develop an algorithm to learn such models, which involves maximizing the data likelihood under the induced stationary distribution of these chains. We show empirical improvements on two challenging inference tasks.", "creator": "LaTeX with hyperref package"}}}