{"id": "1502.06895", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2015", "title": "On the consistency theory of high dimensional variable screening", "abstract": "Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension $p$ is substantially larger than the sample size $n$, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. \\par This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the weak diagonally dominant (WDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods $SIS$ and $HOLP$ are both strong screening consistent (subject to additional constraints) with large probability if $n &gt; O((\\rho s + \\sigma/\\tau)^2\\log p)$ under random designs. In addition, we relate the WDD condition to the irrepresentable condition, and highlight limitations of $SIS$.", "histories": [["v1", "Tue, 24 Feb 2015 17:52:20 GMT  (19kb)", "http://arxiv.org/abs/1502.06895v1", "Screening theory"], ["v2", "Tue, 31 Mar 2015 18:38:42 GMT  (19kb)", "http://arxiv.org/abs/1502.06895v2", "Correction on the rate of $\\kappa$ in Lemma 4 and Theorem 6"], ["v3", "Sat, 6 Jun 2015 07:33:02 GMT  (21kb)", "http://arxiv.org/abs/1502.06895v3", "adding comments on REC"]], "COMMENTS": "Screening theory", "reviews": [], "SUBJECTS": "math.ST cs.LG stat.ML stat.TH", "authors": ["xiangyu wang", "chenlei leng", "david b dunson"], "accepted": true, "id": "1502.06895"}, "pdf": {"name": "1502.06895.pdf", "metadata": {"source": "CRF", "title": "On the consistency theory of high dimensional variable screening", "authors": ["Xiangyu Wang", "Chenlei Leng", "David Dunson"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 2.06 895v 1 [mat h.ST] 2 4Fe bVariable screening is a fast method of dimension reduction to support high-dimensional feature selection. As a pre-selection method, it selects a moderate subset of candidate variables for further refinement via the feature selection to create the final model. The performance of variable screening depends both on computational efficiency and on the ability to dramatically reduce the number of variables without discarding the important ones. If the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) faster feature selection algorithms are required; 2) Conditions that may not maintain the consistency of the selection. This article examines a class of linear screening methods and establishes the consistency theory for this particular class. In particular, we prove that the weakly diagonally dominant (WDD) condition is a necessary and sufficient condition for both DIS and DIS (if we show a strong consistency of both)."}, {"heading": "1 Introduction", "text": "The rapid increase in data has led to new challenges that require statistical selection. \"It's about how people should behave,\" he says. \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" \"It's about how they should behave.\" It's about how they should behave. \"It's about\" It's about the question, \"it's about the question.\" \"It's about the question.\" It's about the question. \"It's about the question,\" It's about the question. \"It's about the question,\" It's about the question, \"it's about the question.\" It's about the question, \"It's about the question,\" it's about the question. \""}, {"heading": "2 Linear screening", "text": "Consider the usual linear regressionY = X\u03b2 +, where Y is the n > \u03b2-1 response vector, X is the n \u00b7 p design matrix and X is the noise. The regression task is to learn the coefficient vector \u03b2. The first is to regain the support of \u03b2, i.e., the position of non-zero coefficients is often imposed on \u03b2, so that only a small part of the coordinates is non-zero. The second is to estimate the value of these non-zero signals. This article focuses mainly on the first phase. As shown in the introduction, if the dimensionality is too high, regulation methods raise concerns both computationally and theoretically that the dimensionality, Fan and Lv (2008) suggest a variable screening model by finding asubdelMd."}, {"heading": "3 Deterministic guarantees", "text": "In this section, we derive the necessary and sufficient condition that guarantees that there will be a strong transfer. (...) Design matrix X and error matrix II are treated as fixed in this section, and we will examine random designs later. (...) We consider the number of sparse coefficient vectors defined by B (...) to be satisfactory. (...) Set B (...) contains vectors that have at most s-zero coordinates with the ratio of the largest and smallest coordinates. (...) Before proceeding to the main result of this section, we will introduce some terminology that helps establish the theories. (...) Definition 3.1. (...) Weak diagonally dominant matrix II is weak. (...)"}, {"heading": "4 Relationship with the irrepresentable condition", "text": "The weak diagonal dominant condition (WDD \u03b2) is closely related to the strong irrepresentative condition (IC) defined in Zhao and Yu (2006) as a necessary and sufficient condition for the consistency of characters in the world. < / p > Suppose each column of X is standardized to have a mean of 0 > 0, with CA, B representing the sub-matrix of C with row indices in A and column indices in B. The authors cite several scenarios of C, so IC is satisfied."}, {"heading": "5 Screening under random designs", "text": "In this section, we will consider linear screening under random designs if X and II are the Gaussians. The theory developed in this section can easily be extended to a broader family of distributions, for example, where the Gaussians follow a sub-Gaussian distribution (Vershynin, 2010) and X follows an elliptical distribution (Fan and Lv, 2008; Wang and Leng, 2013). We will check the screening consistency of SIS and HOLP by verifying the condition in Theorem 2. Recall the ancillary matrices for SIS and HOLP are each associated, AHOLP = X T (XXT) \u2212 1.For simplification, we assume that ii = 1, 2 \u00b7 \u00b7 p. To verify the WDD condition, it is essential to quantify the size of the Gaussians."}, {"heading": "6 Concluding remarks", "text": "This article examines the theoretical properties of a class of high-dimensional, variable screening methods. In particular, we establish a necessary and sufficient condition in the form of weakly diagonally dominant screening matrices for a strong screening consistency of linear screening. We verify the condition for both SIS and HOLP using random designs. In addition, we demonstrate a close relationship between WDD and the unrepresentable condition, highlighting the difficulty of using SIS in screening for arbitrarily correlated predictions. In future work, it will be interesting to see how linear screening can be adapted to compressed scanning (Xue and Zou, 2011) and how techniques such as preconditioning (Jia and Rohe, 2012) can improve the performance of marginal screening and variable selection."}, {"heading": "A Proofs for Section 3", "text": "In this section, we present the two theorems in section 3.Proof of theorems 1 and 2. If there is no clear dominance with respect to the number of characters, we have for each I, Q and I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each S, each S, each S, each S, each S, each I, each S, each S, each S, each S, each S, each S, each S, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, each I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I."}, {"heading": "B Proofs for Section 4", "text": "In this section, we prove the results of section 4. \u2212 \u2212 \u2212 \u2212 Then the sign becomes an accidental. \u2212 Then the sign becomes an accidental. \u2212 Then the sign becomes an accidental. \u2212 This completes the proof for the first case. Now, for the second case, we assume that the sum of a whole series (with the exception of the diagonal term) can be limited by an accidental."}, {"heading": "C Proofs for Section 5 (SIS)", "text": "The proofs in section 6 are divided into two parts. In this section, we put the proofs in relation to SIS = = k = and leave them to the next section. The proof requires the following statement: Proposition 1 \u2212 \u2212 \u2212 \u2212 k = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p \u00b2 p = p \u00b2 p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p \u00b2 p = p = p = p \u00b2 p = p = p \u00b2 p = p = p \u00b2 p = p = p = p = p \u00b2 p = p = p = p \u00b2 p = p = p = p = p = p \u00b2 and p = p = p \u00b2 = p = p \u00b2 = p = p \u00b2 = p = p \u00b2 = p = p = p = p = p \u00b2 = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p \u00b2 = p = p = p = p \u00b2 = p = p = p = p \u00b2 = p = p = p = p = p \u00b2 = p = p = p = p = p \u00b2 = p = p = p = p \u00b2 = p = p = p = p \u00b2 = p = p = p = p \u00b2 = p = p \u00b2 = p = p = p = p \u00b2 = p = p = p = p \u00b2 = p = p = p \u00b2 = p = p \u00b2 = p = p \u00b2 = p = p = p = p \u00b2 = p \u00b2 = p = p = p \u00b2 = p = p = p \u00b2 = p = p = p \u00b2 = p = p = p = p \u00b2 = p = p = p \u00b2 = p = p \u00b2 = p \u00b2 = p = p = p = p \u00b2 = p = p = p \u00b2 = p = p = p = p = p = p = p \u00b2 = p = p \u00b2 = p = p = p = p = p = p = p = p \u00b2 = p = p = p = p = p"}, {"heading": "D Proofs for Section 5 (HOLP)", "text": "In this section, we will prove the theory of theory, theory, theory, theory, theory, theory, theory, theory, theory, theory / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}], "references": [{"title": "Compressive sensing", "author": ["R. Baraniuk"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Baraniuk,? \\Q2007\\E", "shortCiteRegEx": "Baraniuk", "year": 2007}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": null, "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "The dantzig selector: statistical estimation when p is much larger than n", "author": ["E. Candes", "T. Tao"], "venue": null, "citeRegEx": "Candes and Tao,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao", "year": 2007}, {"title": "Statistics on special manifolds, volume 174", "author": ["Y. Chikuse"], "venue": null, "citeRegEx": "Chikuse,? \\Q2003\\E", "shortCiteRegEx": "Chikuse", "year": 2003}, {"title": "High dimensional variable selection via tilting", "author": ["H. Cho", "P. Fryzlewicz"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Cho and Fryzlewicz,? \\Q2012\\E", "shortCiteRegEx": "Cho and Fryzlewicz", "year": 2012}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information", "citeRegEx": "Donoho,? \\Q2006\\E", "shortCiteRegEx": "Donoho", "year": 2006}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Fan and Li,? \\Q2001\\E", "shortCiteRegEx": "Fan and Li", "year": 2001}, {"title": "Sure independence screening for ultrahigh dimensional feature space", "author": ["J. Fan", "J. Lv"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Fan and Lv,? \\Q2008\\E", "shortCiteRegEx": "Fan and Lv", "year": 2008}, {"title": "Preconditioning to comply with the irrepresentable condition. arXiv preprint arXiv:1208.5584", "author": ["J. Jia", "K. Rohe"], "venue": null, "citeRegEx": "Jia and Rohe,? \\Q2012\\E", "shortCiteRegEx": "Jia and Rohe", "year": 2012}, {"title": "On model selection consistency of m-estimators with geometrically decomposable penalties", "author": ["J.D. Lee", "Y. Sun", "J.E. Taylor"], "venue": "Advances in Neural Processing Information Systems", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Robust rank correlation based screening", "author": ["G. Li", "H. Peng", "J. Zhang", "L Zhu"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Forward regression for ultra-high dimensional variable screening", "author": ["H. Theory. Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2009\\E", "shortCiteRegEx": "Wang", "year": 2009}, {"title": "High dimensional ordinary least square projection for variable", "author": ["C. Leng"], "venue": "American Statistical Association,", "citeRegEx": "X. and Leng,? \\Q2013\\E", "shortCiteRegEx": "X. and Leng", "year": 2013}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["Zhang", "C.-H"], "venue": null, "citeRegEx": "Zhang and C..H.,? \\Q2010\\E", "shortCiteRegEx": "Zhang and C..H.", "year": 2010}, {"title": "The sparsity and bias of the lasso selection", "author": ["Zhang", "C.-H", "J. Huang"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "On model selection consistency of lasso", "author": ["P. Zhao", "B. Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}], "referenceMentions": [{"referenceID": 5, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al.", "startOffset": 123, "endOffset": 153}, {"referenceID": 0, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al.", "startOffset": 123, "endOffset": 153}, {"referenceID": 6, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010).", "startOffset": 196, "endOffset": 288}, {"referenceID": 2, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010).", "startOffset": 196, "endOffset": 288}, {"referenceID": 1, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010).", "startOffset": 196, "endOffset": 288}, {"referenceID": 7, "context": "However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice.", "startOffset": 141, "endOffset": 159}, {"referenceID": 15, "context": "We relate WDD to the irrepresentable condition (Zhao and Yu, 2006) that is necessary 2", "startOffset": 47, "endOffset": 66}, {"referenceID": 6, "context": "Bearing these concerns in mind, Fan and Lv (2008) propose the concept of \u201cvariable screening\u201d, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved.", "startOffset": 32, "endOffset": 50}, {"referenceID": 6, "context": "Bearing these concerns in mind, Fan and Lv (2008) propose the concept of \u201cvariable screening\u201d, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved. They propose a marginal correlation based fast screening technique \u201cSure Independence Screening\u201d (SIS) that can preserve signals with large probability. However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice. Li et al. (2012) extends the marginal correlation to the Spearman\u2019s rank correlation, which is shown to gain certain robustness but is still limited by the same strong assumption.", "startOffset": 32, "endOffset": 612}, {"referenceID": 6, "context": "Bearing these concerns in mind, Fan and Lv (2008) propose the concept of \u201cvariable screening\u201d, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved. They propose a marginal correlation based fast screening technique \u201cSure Independence Screening\u201d (SIS) that can preserve signals with large probability. However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice. Li et al. (2012) extends the marginal correlation to the Spearman\u2019s rank correlation, which is shown to gain certain robustness but is still limited by the same strong assumption. Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem.", "startOffset": 32, "endOffset": 787}, {"referenceID": 4, "context": "Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem.", "startOffset": 16, "endOffset": 42}, {"referenceID": 4, "context": "Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem. They both adopt variants of a forward selection type algorithm that includes one variable at a time for constructing a candidate variable set for further refining. These methods eliminate the strong marginal assumption in Fan and Lv (2008) and have been shown to achieve better empirical performance.", "startOffset": 16, "endOffset": 341}, {"referenceID": 4, "context": "Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem. They both adopt variants of a forward selection type algorithm that includes one variable at a time for constructing a candidate variable set for further refining. These methods eliminate the strong marginal assumption in Fan and Lv (2008) and have been shown to achieve better empirical performance. However, such improvement is limited by the extra computational burden caused by their iterative framework, which is reported to be high when p is large (Wang and Leng, 2013). To ameliorate concerns in both screening performance and computational efficiency, Wang and Leng (2013) develop a new type of screening method termed \u201cHigh-dimensional ordinary least-square projection\u201d (HOLP ).", "startOffset": 16, "endOffset": 682}, {"referenceID": 7, "context": "To reduce the dimensionality, Fan and Lv (2008) suggest a variable screening framework by finding a 3", "startOffset": 30, "endOffset": 48}, {"referenceID": 15, "context": "It is weaker than variable selection consistency in Zhao and Yu (2006). The requirement in (2) can be seen as a relaxation of the sign consistency defined in Zhao and Yu (2006), as no requirement for \u03b2\u0302i, i 6\u2208 S is needed.", "startOffset": 52, "endOffset": 71}, {"referenceID": 15, "context": "It is weaker than variable selection consistency in Zhao and Yu (2006). The requirement in (2) can be seen as a relaxation of the sign consistency defined in Zhao and Yu (2006), as no requirement for \u03b2\u0302i, i 6\u2208 S is needed.", "startOffset": 52, "endOffset": 177}, {"referenceID": 15, "context": "4 Relationship with the irrepresentable condition The weak diagonally dominant (WDD) condition is closely related to the strong irrepresentable condition (IC) proposed in Zhao and Yu (2006) as a necessary and sufficient condition for sign consistency of lasso.", "startOffset": 171, "endOffset": 190}, {"referenceID": 15, "context": "If \u03a6ii = 1, \u2200i and |\u03a6ij| < c/(2s), \u2200i 6= j for some 0 \u2264 c < 1 as defined in Corollary 1 and 2 in Zhao and Yu (2006), then \u03a6 is a weak diagonally dominant matrix with sparsity s and C0 \u2265 1/c.", "startOffset": 97, "endOffset": 116}, {"referenceID": 15, "context": "If \u03a6ii = 1, \u2200i and |\u03a6ij| < c/(2s), \u2200i 6= j for some 0 \u2264 c < 1 as defined in Corollary 1 and 2 in Zhao and Yu (2006), then \u03a6 is a weak diagonally dominant matrix with sparsity s and C0 \u2265 1/c. If |\u03a6ij| < r|i\u2212j|, \u2200i, j for some 0 < r < 1 as defined in Corollary 3 in Zhao and Yu (2006), then \u03a6 is a weak diagonally dominant matrix with sparsity s and C0 \u2265 (1\u2212r)2/(4r).", "startOffset": 97, "endOffset": 283}, {"referenceID": 7, "context": "The strong screening consistency defined in this article is stronger than conditions commonly used in justifying screening procedures as in Fan and Lv (2008).", "startOffset": 140, "endOffset": 158}, {"referenceID": 7, "context": "The theory developed in this section can be easily extended to a broader family of distributions, for example, where \u01eb follows a sub-Gaussian distribution (Vershynin, 2010) and X follows an elliptical distribution (Fan and Lv, 2008; Wang and Leng, 2013).", "startOffset": 214, "endOffset": 253}, {"referenceID": 8, "context": "For future work, it is of interest to see how linear screening can be adapted to compressed sensing (Xue and Zou, 2011) and how techniques such as preconditioning (Jia and Rohe, 2012) can improve the performance of marginal screening and variable selection.", "startOffset": 163, "endOffset": 183}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing.", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector.", "startOffset": 11, "endOffset": 146}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n.", "startOffset": 11, "endOffset": 273}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174.", "startOffset": 11, "endOffset": 411}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting.", "startOffset": 11, "endOffset": 526}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing.", "startOffset": 11, "endOffset": 690}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties.", "startOffset": 11, "endOffset": 795}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348\u20131360. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space.", "startOffset": 11, "endOffset": 972}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348\u20131360. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849\u2013911. Jia, J. and Rohe, K. (2012). Preconditioning to comply with the irrepresentable condition.", "startOffset": 11, "endOffset": 1163}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348\u20131360. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849\u2013911. Jia, J. and Rohe, K. (2012). Preconditioning to comply with the irrepresentable condition. arXiv preprint arXiv:1208.5584. Lee, J. D., Sun, Y., and Taylor, J. E. (2013). On model selection consistency of m-estimators with geometrically decomposable penalties.", "startOffset": 11, "endOffset": 1304}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening.", "startOffset": 0, "endOffset": 16}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening.", "startOffset": 0, "endOffset": 181}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6.", "startOffset": 0, "endOffset": 362}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing.", "startOffset": 0, "endOffset": 439}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing. Biometrika, 98(2):371\u2013380. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.", "startOffset": 0, "endOffset": 546}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing. Biometrika, 98(2):371\u2013380. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894\u2013942. Zhang, C.-H. and Huang, J. (2008). The sparsity and bias of the lasso selection in highdimensional linear regression.", "startOffset": 0, "endOffset": 688}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing. Biometrika, 98(2):371\u2013380. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894\u2013942. Zhang, C.-H. and Huang, J. (2008). The sparsity and bias of the lasso selection in highdimensional linear regression. The Annals of Statistics, 36(4):1567\u20131594. Zhao, P. and Yu, B. (2006). On model selection consistency of lasso.", "startOffset": 0, "endOffset": 842}, {"referenceID": 3, "context": "Then H is in the Stiefel manifold (Chikuse, 2003).", "startOffset": 34, "endOffset": 49}, {"referenceID": 7, "context": "[Lemma 4 in Fan and Lv (2008)]Let U be uniformly distributed on the Stiefel manifold Vn,p.", "startOffset": 12, "endOffset": 30}, {"referenceID": 11, "context": "Part II: Second, for off-diagonal terms, although the proof is almost identical to the proof of Lemma 5 in Wang and Leng (2013), we still provide a complete version here due to the importance of this result.", "startOffset": 107, "endOffset": 128}], "year": 2017, "abstractText": "Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the weak diagonally dominant (WDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS andHOLP are both strong screening consistent (subject to additional constraints) with large probability if n > O((\u03c1s + \u03c3/\u03c4)2 log p) under random designs. In addition, we relate the WDD condition to the irrepresentable condition, and highlight limitations of SIS.", "creator": "LaTeX with hyperref package"}}}