{"id": "1502.07073", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2015", "title": "Strongly Adaptive Online Learning", "abstract": "\\emph{Strongly adaptive algorithms} are algorithms whose performance on {\\em every time interval} is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems.", "histories": [["v1", "Wed, 25 Feb 2015 07:24:40 GMT  (18kb)", "http://arxiv.org/abs/1502.07073v1", null], ["v2", "Mon, 8 Jun 2015 15:10:11 GMT  (18kb)", "http://arxiv.org/abs/1502.07073v2", null], ["v3", "Fri, 19 Jun 2015 07:31:45 GMT  (19kb)", "http://arxiv.org/abs/1502.07073v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "alon gonen", "shai shalev-shwartz"], "accepted": true, "id": "1502.07073"}, "pdf": {"name": "1502.07073.pdf", "metadata": {"source": "CRF", "title": "Strongly Adaptive Online Learning", "authors": ["Amit Daniely", "Alon Gonen", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 2.07 073v 1 [cs.L G] 25 FeStark adaptive algorithms are algorithms whose performance is nearly optimal at any time interval. We present a reduction that can convert standard algorithms with little regret into strongly adaptive ones. As a result, we derive simple but efficient, strongly adaptive algorithms for a handful of problems."}, {"heading": "1 Introduction", "text": "Dealing with changing environments and quickly adapting to change is a key component in so many undertakings. A broker is highly rewarded for quickly adapting to new trends; a reliable routing algorithm needs to respond quickly to traffic jams; a web recruiter should adapt to new ads and changes in users \"tastes; but a politician can also benefit from quickly adapting to changes in public opinion; and the list goes on. Most current algorithms and theoretical analyses focus on relatively stationary environments; in statistical learning, an algorithm should be good at training distribution; even in online learning, an algorithm should normally compete with the best strategy (from a pool), which is fixed and does not change over time.Our main focus is to investigate to what extent such strategies can be modified to cope with changing environments.We look at a general online learning framework that includes various learning problems, including prediction with online expert advice, optimisation and more."}, {"heading": "1.1 Problem setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Framework for Online Learning", "text": "rE \"s for the leaders for the leaders for the leaders for the leaders for the leaders for the leaders for the leaders for the leaders for the leaders for the leaders for the leaders for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership for the leadership.\""}, {"heading": "1.2 Our Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A strongly adaptive meta-algorithm", "text": "\"We have a very adaptable system that can apply any kind of algorithm.\" The specific instance of SAOL, which uses B as a black box, is led by strategies and a number of strategies whose regrets w.r.t. W. Satisfaction RBpT q (1), where \"P p0, 1q, and C 2, is somewhat scalar."}, {"heading": "1.3 Related Work", "text": "Perhaps the most relevant previous work, from which we borrowed many of our techniques, was [BM07], which focused on the expert setting and suggested an enhanced notion of remorse using time selection functions, which are functions from the time interval rT s to r0, 1s. Learner A's regret with regard to a time selection function I is defined by RIA pT q \"maxiPrNs'\u0159T\" 1 Iptqs tpxtq \"\u0159T\" 1 Iptqs tpiq, \"with\" tpiq \"being the loss of an expert i in due course t. This setting can be seen as a generalization of the sleep expert setting [FSSW97]. For a fixed group consisting of M time selection functions, they proved to be remorse bound by Op a | I | logpNMqq'logpNMqq 'in relation to the respective time selection function I P I. We observe that if we all of the indicators (the number of indicators) that"}, {"heading": "2 Reducing Adaptive Regret to Standard Re-", "text": "In this passage, we ask ourselves the question of how we should behave. \"The question we ask ourselves is:\" Do we want this? \"The answer is:\" Do we want this? \"The answer:\" Do we want this? \"The answer:\" Do we want this? \"The answer:\" Do we want this? \"The answer:\" Do we want this? \"The answer:\" Do we want this? \"The answer:\" Do we want this? \"The answer:\" Do we want this? \"The answer:\" Do we want this? \"The answer:\" Do we want this? \"The answer:\" Do we want this? \"We?\" The answer: \"Do we want this?\" The answer: \"Do we?\" The answer: \"Do we want this?\" The answer: \"Do we want this?\" The answer: \"Do we want this?\""}, {"heading": "2.1 Proof Sketch of Theorem 1", "text": "In this section we outline the proof for theorem 1. A complete proof is detailed in Appendix A. The analysis of SAOL is divided into two parts. First, the challenge is to prove the theorem for the intervals in I (see Lemma 2). Then, the theorem should be extended to each interval (end of Appendix A.) Let's start with the first task. Our first observation is that for each interval I the regret of SAOL during Interval I is equal (SAOL's regret relative to Bi's \"the regret of Bi\" (4) (during Interval I). Since the regret of BI during Interval I is already guaranteed to be small (Equation (1), the problem of ensuring low regret in each interval in I is reduced to the problem of ensuring low regret in relation to each of the BI's (while I).Next, we prove that the regret of SAOL is small in relation to the B1L."}, {"heading": "3 Strongly Adaptive Regret Is Stronger Than", "text": "In this section, we refer the concept of strong adaptability to the tracking factor of regret and show that algorithms with small, strongly adaptive regrets also have small regrets. Let's briefly rethink the problem of tracking. To make it simple, let's focus on contextual learning problems and the case where the set of strategies coincides with the decision space (although the result can easily be generalized). Let's fix a decision space D and a family L of loss functions. A compound action is a sequence of the tracking factor \"p\u03c3T P DT. Since there is no hope that all sequences will compete with each other, a typical limitation of the problem is that the number of switches in each sequence is limited. For a positive integer m, the class of compound actions is defined by most m switches defined by Bm\" #."}, {"heading": "4 Strongly Adaptive Regret in The Bandit Set-", "text": "It is a question of the extent to which it is actually a purely problem that has taken place in the United States in recent years. (...) It is a question of the extent to which it is actually a problem. (...) It is a question of the extent to which it is actually a problem. (...) It is a question of the extent to which it is actually a problem. (...) It is a question of the extent to which it is actually a problem. (...) It is a question of the extent to which it is actually a problem. (...) It is a question of the extent to which it is actually a problem. (...) It is a question of the extent to which it is a problem. (...) It is a question of the extent to which it is really a problem. (...) It is a question of the extent to which it is \"how it is going.\""}, {"heading": "Acknowledgments", "text": "We thank Yishay Mansour and Sergiu Hart for valuable talks."}, {"heading": "A Proof of Theorem 1", "text": "\"We must adjust to the fact that the claim for each t1qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq"}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "From external to internal regret", "author": ["Avrim Blum", "Yishay Monsour"], "venue": "Journal of Machine Learning,", "citeRegEx": "Blum and Monsour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Monsour.", "year": 2007}, {"title": "Tracking a small set of experts by mixing past posteriors", "author": ["Olivier Bousquet", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Bousquet and Warmuth.", "year": 2003}, {"title": "How to use expert advice", "author": ["Nicolo Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P Helmbold", "Robert E Schapire", "Manfred K Warmuth"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "A new look at shifting", "author": ["Nicolo Cesa-Bianchi", "Pierre Gaillard", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "regret. CoRR,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E Schapire", "Yoram Singer", "Manfred K Warmuth"], "venue": "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Adaptive algorithms for online decision problems", "author": ["Elad Hazan", "C Seshadhri"], "venue": "In Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Hazan and Seshadhri.,? \\Q2007\\E", "shortCiteRegEx": "Hazan and Seshadhri.", "year": 2007}, {"title": "Tracking the best expert", "author": ["Mark Herbster", "Manfred K Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Herbster and Warmuth.,? \\Q1998\\E", "shortCiteRegEx": "Herbster and Warmuth.", "year": 1998}, {"title": "Online optimization in dynamic environments", "author": ["Eric C Hall", "Rebecca M Willett"], "venue": "arXiv preprint arXiv:1307.5944,", "citeRegEx": "Hall and Willett.,? \\Q2013\\E", "shortCiteRegEx": "Hall and Willett.", "year": 2013}, {"title": "Online optimization: Competing with dynamic comparators", "author": ["Ali Jadbabaie", "Alexander Rakhlin", "Shahin Shahrampour", "Karthik Sridharan"], "venue": "arXiv preprint arXiv:1501.06225,", "citeRegEx": "Jadbabaie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jadbabaie et al\\.", "year": 2015}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["Sasha Rakhlin", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": null, "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [], "year": 2017, "abstractText": "Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems.", "creator": "LaTeX with hyperref package"}}}