{"id": "1503.00075", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "abstract": "A Long Short-Term Memory (LSTM) network is a type of recurrent neural network architecture which has recently obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).", "histories": [["v1", "Sat, 28 Feb 2015 06:31:50 GMT  (251kb,D)", "http://arxiv.org/abs/1503.00075v1", "10 pages, 4 figures"], ["v2", "Thu, 5 Mar 2015 20:13:25 GMT  (252kb,D)", "http://arxiv.org/abs/1503.00075v2", "10 pages, 4 figures"], ["v3", "Sat, 30 May 2015 06:51:20 GMT  (254kb,D)", "http://arxiv.org/abs/1503.00075v3", "Accepted for publication at ACL 2015"]], "COMMENTS": "10 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["kai sheng tai", "richard socher", "christopher d manning"], "accepted": true, "id": "1503.00075"}, "pdf": {"name": "1503.00075.pdf", "metadata": {"source": "CRF", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "emails": ["kst@cs.stanford.edu", "richard@metamind.io", "manning@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Most models of distributed representations of phrases and sentences - that is, models in which reality-weighted vectors are used to represent meaning - fall into one of three classes: bag-of-words models, sequence models, and tree-structured models. In bag-of-words models, phrases and sentence representations are independent of word order; they can be generated, for example, by averaging constituent word representations (Landauer and Dumais, 1997; Foltz et al., 1998). In contrast, sequence models construct sentence representations as an order-sensitive function of the sequence of tokens (Elman, 1990; Mikolov, 2012). Finally, in tree-structured models, each phrase and sentence representation is composed of its constituent subphrases, which follow a given syntactical structure over the words and phrases in the sentence (Goller and Kuchler, 1996; Socher et al al al al al al al al al al al al al al al al, 2011).To produce sentence representations that are touched by the natural sequences and subquests."}, {"heading": "2 Long Short-Term Memory Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Overview", "text": "Recursive neural networks (RNNs) are able to process input sequences of any length via the recursive application of a transition function to a hidden state vector ht. At each time step t, the hidden state ht is a function of the input vector xt, which the network at the time t and its previous hidden state ht \u2212 1. For example, the input vector xt could be a vector representation of the t-th word in the text body (Elman, 1990; Mikolov, 2012). The hidden state ht = tange unit i (Wxt + Uht \u2212 1).Unfortunately, the RNN transition function is an affine transformation followed by a pointedly non-linearity such as the hyperbolic tangent function: ht = tange unit i (Wxt + Uht \u2212 1).Unfortunately, there is a problem with RNNs with transformation functions of this form during the problem of the architecture component."}, {"heading": "2.2 Variants", "text": "Two commonly used variants of the basic LSTM architecture are the bidirectional LSTM and the multilayer LSTM (also known as stacked or deep LSTM). Bidirectional LSTM (Graves et al., 2013) consists of two LSTMs executed in parallel: one on the input sequence and the other on the back of the input sequence. For multilayer LSTM architectures, the hidden state of the bidirectional LSTM unit is used as input to the LSTM unit in layer '+ 1 at the same time (Graves et al., 2013; Sutskever et al., 2014; Zaremba and Sutskever, 2014)."}, {"heading": "3 Tree-Structured LSTMs", "text": "One limitation of the LSTM architectures described in the previous section is that they only allow strictly sequential information propagation. In this context, we propose two natural extensions of the basic LSTM architecture: the Child-Sum-Tree-LSTM and the N-ary Tree-LSTM. Both variants allow richer network topologies in which each LSTM unit is able to integrate information from multiple child units. As in standard LSTM units, each tree-LSTM unit contains input and output gates ij and oj, a memory cell cj and a hidden state hj. The difference between the standard LSTM unit and TreeLSTM units is that vectors and memory cell updates depend on the states of possibly many child units. Additionally, the tree-LSTM unit contains a single forgotten gate, a forgotten gate cj, and a forgotten state j instead of a single forgotten gate."}, {"heading": "3.1 Child-Sum Tree-LSTMs", "text": "The Child-Sum Tree-LSTM transition equations are the following: (3) fjk = \u03c3 (W (f) xj + U (f) hk + b (f)), (4) oj = \u03c3 (W (i) xj + U (i) h, j + b (i))), (5) uj = tanh (W (u) xj + U (u) h, j + b (u)))), (6) cj = ij uj + c (o) h, j + b (o))), (5) uj = tanh (cj) xj + U (u) h, j + b (u))), (6) cj = ij uj + b (k) k, c (j) k, c (7) fjk ck, (7) hj = oj tanh (cj), (8) where in eq. 4, k, c (j)."}, {"heading": "3.2 N -ary Tree-LSTMs", "text": "The N -ary tree LSTM can be used on tree structures where the branching factor is at most N and where the children are ordered, i.e. they can be indexed from 1 to N. Write for each node j the hidden state and the memory cell of its kth child as hjk or cjk. The N -ary tree LSTM transition equations are the following: ij = \u03c3 (W (i) xj + N \u2211 k = 1 U (i) k hjk + b (i))), (9) fjk = \u03c3 (W (f) xj + N \u2211 '= 1 U (f) k'hj' + b (f)), (10) oj = \u03c3 k (W (o) k hjk + b (o))) fjjk + b (o)), (11) uj = tanh (W (u) xj + N \u2211 k = 1 U (u) k hjk + tanb (u)."}, {"heading": "4 Models", "text": "We will now describe two specific models that apply the tree LSTM architectures described in the previous section."}, {"heading": "4.1 Tree-LSTM Classification", "text": "In this context, we would like to predict labels y \u00b2 from a separate group of classes Y for a subset of nodes in a tree. For example, the label of a node in a parse tree could correspond to a property of the phrase spanned by that node. At each node j, the label y \u00b2 j is predicted based on the inputs observed on nodes in the subtree {x} j by a Softmax classifier, which takes the hidden state hj at the node as input: p = Softmax (W (s) hj + b (s), y = argmax y \u00b2 (y | x} j). The cost function is the negative log probability of true class labels y (k) on each labeled node: J (IS) = \u2212 1 m \u00b2 k = 1 Logmax y (k)."}, {"heading": "4.2 Semantic Relatedness of Sentence Pairs", "text": "Considering a pair of sentences, we would like to predict a real value of similarity in a range [1, K], where K > 1 is an integer; the order {1, 2,.., K} is an order scale of similarity, in which higher values indicate a greater degree of similarity, and we allow the real values to take into account the truth values that are an average over the evaluations of several human annotations.We first create sentence representations hL and hR for each sentence in the pair using a tree-LSTM model over the parse tree of each sentence. In view of these sentence representations, we forecast the similarity value y, taking into account both the distance and the angle between the pair (hL, hR): h \u00d7 = hL hR, (15) h + = | hL \u2212 hR |, hs = the equal value y (W (\u00d7) h \u00d7 + W (+) h \u2212 b (h)))), p = softmax (W)."}, {"heading": "5 Experiments", "text": "We evaluate our tree-LSTM architectures based on two tasks: (1) moods classification of sentences from movie reviews and (2) prediction of the semantic relationship of sentence pairs. When comparing our tree-LSTMs with sequential LSTMs, we control the number of LSTM parameters by varying the dimensionality of the hidden states.2 Details for each model variant are in Table 1.1 In subsequent experiments, we found that optimizing this \"classification object\" yielded better performance than an average square error object.2For our bidirectional LSTMs, the parameters of the forward and reverse transition functions are shared, thus achieving superior performance over bidirectional LSTMs with unbound weights and the same number of parameters (and thus smaller hidden vector dimensions)."}, {"heading": "5.1 Sentiment Classification", "text": "We use the Stanford Sentiment Treebank (SST) (Socher et al., 2013). There are two subtasks: the binary classification of sentences and the fine-grained classification across five classes: very negative, negative, neutral, positive and very positive. We use the standard train / developer / test split of 6920 / 872 / 1821 for the binary classification and 8544 / 1101 / 2210 for the fine-grained classification (there are fewer examples of the binary subtask since neutral sentences are excluded). Mood labeling on each node is predicted using a Softmax classifier, as described in Sec. 4.1. We use binary tree LSTMs (Sec. 3.2) structured according to the constituency trees that are provided with the dataset. For the sequential LSTM models, we predict the mood for each phrase using the final hidden state after the entire phrase has been processed in the STM formation."}, {"heading": "5.2 Semantic Relatedness", "text": "For a given sentence pair, the semantic kinship task is to predict a man-made assessment of the significance of the similarity of the two sentences. We use the Sentences Involving Compositional Knowledge (SICK) (Marelli et al., 2014) dataset, consisting of 9927 sentence pairs in a 4500 / 500 / 4927 Train / Dev / Test Split. The sentences are taken from existing picture and video description datasets. Each sentence pair is commented with a reference y [1, 5], with 1 indicating that the two sentences are completely independent of each other, and 5 indicating that the two sentences are very related. Each dataset is the average of 10 ratings assigned by various human commentators. Here, we use the similarity model described in Sec. 4.2. For the similarity prediction network (Eqs. 15), we use a hidden layer of the size bin.We compare two Tree-Laric-Sechitree-tree architectures (Child Schitree-tree composition 3.3)."}, {"heading": "5.3 Hyperparameters and Training Details", "text": "The hyperparameters for our models were matched to the development set for each task. We initialized our word representations with publicly available 300-dimensional glove vectors (Pennington et al., 2014). For the task of mood classification, word representations were refined during the training with a learning rate of 0.1; no fine adjustment was made for the semantic relationship task. Our models were adjusted with AdaGrad (Duchi et al., 2011) with a learning rate of 0.05 and a minibatch size of 25. Model parameters were regulated with a regulation strength per minibatch L2 of 10 \u2212 4. The mood classifier was additionally regulated with the help of suspensions (Hinton et al., 2012)."}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Sentiment Classification", "text": "Our results are summarized in Table 2. As in the case of the Convolutionary Neural Network Model described by Kim (2014), we found that fine-tuning word representations resulted in a significant increase in performance in the fine-grained classification subtask, in contrast to the slight improvements in the partial task of binary classification. This suggests that fine-tuning helps to distinguish positive / negative from neutral, strong positive from positive and strong negative, as opposed to positive from negative in the binary case. Bidirectional LSTM significantly exceeded the standard LSTM in the fine-grained subtask. Note that this result is achieved without the introduction of additional parameters in the LSTM transition function, as the forward and backward parameters are divided, suggesting that the sentence length becomes a limiting factor for the (unidirectional) LSTM in the fine-grained subtask. Slightly surprisingly, we do not see a corresponding improvement in the binary subtask (we actually achieve similar results in our STM word order)."}, {"heading": "6.2 Semantic Relatedness", "text": "Our results are summarized in Table 3. Following Marelli et al. (2014), we use the Pearson correlation coefficient, the Spearman correlation coefficient, and the mean square error as evaluation metrics. The mean vector baseline calculates sentence representations as the mean of the representations of the constituent words. In the models DT-RNN and SDTRNN (Socher et al., 2014), the vector representation for each node in a dependence tree is a sum of affin-transformed child vectors, followed by a nonlinearity (the SDT-RNN determines the affine transformation on the dependence relationship with the child node). For each of our baselines, including the LSTM models, we use the similarity model, which is also compared in Section 4.2.We compare four of the topperformative systems 3 assigned to the SemEval 2014 semantic sequence model: ECNal et."}, {"heading": "7 Discussion and Qualitative Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Modeling Semantic Relatedness", "text": "In Table 4, we list sentences taken from a 1000-sentence sample of the SICK test set. We compare the neighbors ranked according to the dependency tree LSTM model with a ranking by cosinal similarity of the mean word vectors for each sentence. We observe that the dependency tree LSTM model has several desirable properties. Note that in the dependency analysis of the second query set, the word \"ocean\" is the second highest result we could find for this task; in some cases, these results are stronger than the official performance of the team in the joint task. For example, the listed result of Zhao et al. (2014) is stronger than the correlation value of their submitted system of 0.8280. We do not list the StanfordNLP template because no description of the system is available in Marelli et al. (2014)."}, {"heading": "7.2 Effect of Sentence Length", "text": "We examine the influence of sentence length on the performance of our LSTM models. Ranking by mean word vector cosine similarity ScoreRanking by Dependency Tree LSTM model ScoreIn Fig. 4, we show the relationship between Pearson correlation with gold ratings and sentence length. The bidirectional LSTM behaves similarly to the standard LSTM for shorter inputs, but significantly outperforms longer sentence pairs. Across all windows, the Dependency Tree LSTM consistently outperforms the sequential LSTM variants. The observation that the Dependency Tree LSTM performs better even on short sentences suggests that the model benefits from the dependency information captured by the parse tree structure, rather than simply compressing the path lengths to the root. We observe a similar superiority of the Tree LSTM in classifying the mood of shorter sentences (Fig. 3)."}, {"heading": "8 Related Work", "text": "Distributed word representations (Rumelhart et al., 1988; Collobert et al., 2011; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014) have found broad applicability in a variety of NLP tasks. Following this success, there has been considerable interest in learning distributed word and sentence representations (Mitchell and Lapata, 2010; Yessenalina and Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013) and distributed representations of longer text bodies such as paragraphs and documents (Srivastava et al., 2013; Le and Mikolov, 2014). Our approach builds on recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011), which we subsequently abbreviate as TreeRNs to avoid confusion with recursive words."}, {"heading": "9 Conclusion", "text": "In this paper, we introduced a generalization of LSTMs to tree-structured network topologies; the tree-LSTM architecture can be applied to trees with any branching factor. We demonstrated the effectiveness of the tree-LSTM by applying the architecture to two tasks: semantic kinship and mood classification that exceed existing systems on both. In terms of the dimensionality of the model, we demonstrated that tree-LSTM models are capable of outperforming their sequential counterparts, and our results point to further steps in characterizing the differences between sequential and tree-structured models."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "A Long Short-Term Memory (LSTM) network is a type of recurrent neural network architecture which has recently obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).", "creator": "LaTeX with hyperref package"}}}