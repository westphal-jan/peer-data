{"id": "1503.00107", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "Non-linear Learning for Statistical Machine Translation", "abstract": "Modern statistical machine translation (SMT) systems usually use a linear combination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the current data. In this paper, we propose a non-linear modeling for the quality of translation hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear models. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model.", "histories": [["v1", "Sat, 28 Feb 2015 09:53:32 GMT  (25kb)", "http://arxiv.org/abs/1503.00107v1", "submitted to a conference"]], "COMMENTS": "submitted to a conference", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["shujian huang", "huadong chen", "xinyu dai", "jiajun chen"], "accepted": true, "id": "1503.00107"}, "pdf": {"name": "1503.00107.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["email@domain", "email@domain"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.00 107v 1 [cs.C L] 28 Feb 2015"}, {"heading": "1 Introduction", "text": "One of the core problems in statistical machine translation research is the modeling of translation hypotheses (2003). Each modeling method defines a score of a target sentence e = e1, e2,..., eI, compared to a source sentence f = f1, f2,..., fJ, where each egg defines the target word and a language model.Pr (e), the known modeling method may begin with the source channel model (Brown et al., 1993). The scoring of e decomposes itself to calculate a translation model and a language model.Pr (e), the known modeling method, effectively begins with the source channel model (f).The modeling method is extended to log-linear models of Och and Ney (2002), as shown in Equation 2, where hm (e) is the corresponding function."}, {"heading": "2 Related work", "text": "Many research has tried to bring non-linearity into the formation of SMTs. These efforts could be roughly divided into the following three categories. The first line of research attempted to reinterpret original features through function transformation or additional learning. For example, Maskey and Zhou (2012) used a deep belief network to learn representations of phrase translation and lexical translation probability traits. Clark et al. (2014) used discretization to transform real-valued density traits into a set of binary indicator traits. Lu et al. (2014) learned new traits using a semi-monitored deep auto-encoder. This work focuses on explicit representation of features and usually deals with additional learning processes. Our proposed method only incorporates the original trait as input without transformation. Function transformation or combination is performed implicitly during the formation of the network and integrates the optimization of the translation system."}, {"heading": "3 Non-linear Translation", "text": "Nonlinear modelling of translation hypotheses could be used in both phrase-based systems and syntax-based systems. In this paper, we take the hierarchical phrase-based machine translation system (Chiang, 2005) as an example and introduce how we insert nonlinearity into the system."}, {"heading": "3.1 Decoding", "text": "The basic deciphering algorithm could be maintained almost in the same way as conventional phrase or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). For example, in the experiments of this paper we use a CKY-style decoding algorithm according to Chiang (2005). Our nonlinear translation system differs from conventional systems in the way we calculate the score for each hypothesis. Instead of calculating the score as a linear combination, we use neural networks (Section 3.2) to perform a nonlinear combination of characteristic values. In practice, we also achieve the cube cropping algorithm (Chiang, 2005) to keep the deciphering efficiently."}, {"heading": "3.2 Two-layer Neural Networks", "text": "We use a two-layer neural network as a nonlinear model for scoring translation hypotheses. The structure of a typical two-layer feed network comprises an input layer, a hidden layer, and an output layer (as shown in Figure 1). We use the input layer to accept input characteristics, the hidden layer to combine different input characteristics, the output layer on which only one node outputs the model score for each translation hypothesis based on the value of hidden nodes. Specifically, the score of the hypothesis e, which is called sN, is defined as: sN (e) = \u03c3o (Mo \u00b7 \u03c3h (Mh \u00b7 h m 1 (e | f) + bh) + bo) (4), where M, b is the weight matrix, bias vector of the neural node or layer; \u03c3 is the activation function, the function is often set to the function of the sublinear or tanmoh output, respectively."}, {"heading": "3.3 Features", "text": "We use the standard features of a typical hierarchical, phrase-based translation system (Chiang, 2005). Adding new features to the frame is left as future direction: \u2022 p (\u03b1 | \u03b3) and p (\u03b3 | \u03b1): conditional probability of translating \u03b1 as \u03b3 and translating \u03b1 as \u03b3, where \u03b1 and \u03b3 are the left and right sides of an initial phrase (or hierarchical translation rule) respectively; \u2022 pw (\u03b1 | \u03b3) and pw (\u03b3 | \u03b1): lexical probability of translating words into \u03b1 as words in \u03b3 and translating words into \u03b3; \u2022 plm: language model probability; \u2022 wc: cumulative number of individual words generated during translation; \u2022 pc: cumulative number of initial phrases used; \u2022 rc: cumulative number of hierarchical rules; \u2022 gc: cumulative number of adhesive rules used in this hypothesis; \u2022 cumulative number of unused words;"}, {"heading": "4 Non-linear Learning Framework", "text": "Traditional machine translation systems use MERT to adjust the weight of different features. MERT performs an efficient search by enumerating the scoring function of all hypotheses and using intersections of these linear functions to form the \"upper shell\" of the scoring function of the model (Och, 2003). If the scoring function is non-linear, it is not possible to find the intersections of these functions. In this section we will discuss alternatives to train the parameters of nonlinear models."}, {"heading": "4.1 Training Criteria", "text": "In fact, the fact is that most of them will be able to be in a position to be in a position, and that they will be able to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position."}, {"heading": "4.2 Training Objective", "text": "For the binary classification task, we use a hinge loss according to Watanabe (2012). Since the network has many parameters compared to the linear model, we use an L1 standard instead of the L2 standard as the regularization term to prefer sparse solutions. We define our training target function in Equation 6.argmin \u03b81N \u2211 (e1, e2).T\u03b4 (f, e1, e2; \u03b8) + \u03bb \u00b7 | \u03b8 | 1with\u03b4 (\u00b7) = max {s (f, e1; \u03b8) \u2212 s (f, e2; \u03b8) + 1, 0} (6) D are the given training data; (e1, e2) is a training hypothesis pair, assuming that e1 is the best (\u00b7) result; N is the total number of hypotheses pairs in D; T is the total number of hypotheses pairs in eampence.The T is the PRT hypothesis."}, {"heading": "4.3 Training Procedure", "text": "In the standard training algorithm for classification, the training processes remain the same in each iteration. In machine translation, decoding algorithms usually result in a very different n-best series with different parameters, due to the exponentially large size of the search space. MERT and PRO extend the current n-best series by merging the n-best series of all previous iterations into a pool (Papineni et al., 2002; Hopkins and May, 2011). In this way, the extended n-best series can result in a better approximation of the true hypothesis C has set and can lead to better and more stable training outcomes. We argue that training should still focus on hypotheses reached in the current round, because in each iteration the search for the n-best group is independent of previous iterations. To jeopardize the above two goals, training hypotheses are first generated from the previous series-n series in our practice."}, {"heading": "5 Structure of the Network", "text": "Although neural networks give strong meaning to the modelling of translation hypotheses, the formation of a neural network tends to produce a local minimum that can influence training outcomes. We suspect that one reason for these local minimum values is the structure of a well-networked network with too many parameters. Let's take a neural network with k nodes in the input layer and m nodes in the hidden layer as an example. Each node in the hidden layer is connected to each of the k input nodes. This simple structure results in at least k \u00b7 m parameters. In Section 4.2, we use L1 standard in the objective function to obtain more economical solutions. In this section, we propose some limited network structures according to our knowledge of the characteristics. These structures have much fewer parameters or simpler structures that are compared to original neural networks, thus reducing the possibility of getting stuck in local minimum systems."}, {"heading": "5.1 Network with two-degree Hidden Layer", "text": "The first pitfall of the standard two-layer neural network is that each node in the hidden layer receives input from each input layer node. Features used in the SMT are usually designed manually, which has their concrete meaning. In a network of several hidden nodes, the combination of all features in each hidden node can be redundant and is not necessary to represent the quality of a hypothesis. As a result, we take a hard step and restrict the nodes in the hidden layer to a degree of two, meaning that each hidden node only accepts input from two input nodes. We do not use any other prior knowledge of features in this setting. So on a network with k nodes in the input layer C2k = k (k \u2212 1) / 2 nodes, in order to accept all combinations from the input layer. We call this network structure Two-Degree Network = a hidden layer layer in the input layer (C2k = TN), because we should see that a combination of two (DN) is large (D2k = DN)."}, {"heading": "5.2 Network with Grouped Features", "text": "To loosen this constraint, we need more prior knowledge of the characteristics. Our first observation is that there are different types of characteristics, which differ from each other in terms of ranges of values, sources, meaning, etc. Characteristics of the language model usually assume a very low probability value, and the characteristics of the word count assume an integer value, and usually carry much more weight than other counters in a linear case. The second observation is that characteristics of the same group are essentially of the same type and may not have complex interaction with each other. So, for example, it is reasonable to combine characteristics of the language model with characteristics of the word count in a hidden node. However, it may not be necessary to combine the number of initial phrases and the number of unknown words in a hidden node. Based on the above two intuitions, we will design a new structure of the network that has the following limitations: In view of a common partition of characteristics G1, we will divide each node into two groups, with a basic ability of 3."}, {"heading": "6 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 General Settings", "text": "The parallel data comes from the LDC, including LDC2002E18, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T10, LDC2007T09, which consists of 8.6 million pairs of sentences. Monolingual data includes Xinhua portions of the gigaword corpus. We use multilingual data MT03 as training data, MT02 as development data, and MT04, MT05 as test data. These data are largely in the same genre and avoid the additional consideration of domain adaptation. The Chinese side of the company is word segmented using ICTCLAS1. Our translation system is an in-house implementation of the hierarchical, phrase-based translation system (Chiang, 2005). We set the beam size to 20."}, {"heading": "6.2 Experiments of Training Criteria", "text": "This series of experiments evaluates various training criteria discussed in Section 4.1. We create hypotheses pairs according to the criteria of BW, BR, and PW, and conduct training with these pairs. In the PW criterion, we use PRO's scanning method (Hopkins and May, 2011) and obtain the 50 hypotheses pairs for each set. We use 20 hidden nodes for all three settings to make a fair comparison, the results are shown in Table 2. The first two lines compare training with and without the weighted combination of hypotheses pairs that we discussed in Section 4.3. As the result suggests, performance improves significantly with the weighted combination of hypotheses pairs from previous iterations on both sets of tests. Although system performance varies on the Dev set, the performances on the test sets are almost comparable, suggesting that although the three training criteria are based on different assumptions, their results are basically equivalent to the training systems translation translation."}, {"heading": "6.3 Experiments of Network Structures", "text": "We first compare the neural network to a different number of hidden nodes. TLayer20, TLayer30 and TLayer50 are standard, two-layer neural feed networks with 20, 30 and 50 hidden layer nodes. We see that training a larger network leads to an improvement in translation quality. However, training a larger network is often time consuming. We experimented with neural networks with 100 or more hidden nodes (TLayer100). But TLayer30 takes 10 times longer in the training time for each iter-3TLayer20 is the same system as BW in Table 2ation as TLayer20 and did not end before the submission deadline. We then compared the two network structures proposed in Section 5."}, {"heading": "7 Conclusion", "text": "In this paper, we discuss a non-linear framework for modeling translation hypotheses for statistical machine translation systems. Compared to previous efforts to bring non-linearity into machine translation, our method uses a single-layer two-layered neural network and conducts training independently of all previous linear training methods (e.g. MERT). Our method also trains its parameters without pre- or post-training procedures. Experiments show that our method could improve the basic system, even with the same characteristic as input, in a large-scale machine translation task on a Chinese-English scale. In the formation of neural networks with hidden nodes, we use heuristics to reduce the complexity of network structures and achieve additional advantages over traditional networks. It shows that heuristics and intuition of data and features are still important to learn an machine translation system.Since future work will require more specific features to integrate our learning framework into more complex ones."}], "references": [{"title": "Decoder integration and expected BLEU training for recurrent neural network language models", "author": ["Auli", "Gao2014] Michael Auli", "Jianfeng Gao"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Auli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2014}, {"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Neural Networks for Pattern Recognition", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "The mathematic of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F. Brown", "Stephen Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Chen", "Goodman1998] S.F. Chen", "J.T. Goodman"], "venue": "Technical report,", "citeRegEx": "Chen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1998}, {"title": "A hierarchical phrase-based model for statistical machine translation. In annual meeting of the Association for Computational Linguistics", "author": ["David Chiang"], "venue": null, "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Hope and fear for discriminative training of statistical translation models", "author": ["David Chiang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Chiang.,? \\Q2012\\E", "shortCiteRegEx": "Chiang.", "year": 2012}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Chris Dyer", "Alon Lavie", "Noah A. Smith"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Clark et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Locally non-linear learning for statistical machine translation via discretization and structured regularization. Transactions of the Association for Computational Linguistics, 2:393\u2013404", "author": ["Clark et al.2014] Jonathan Clark", "Chris Dyer", "Alon Lavie"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M. Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Beyond log-linear models: Boosted minimum error rate training for n-best re-ranking", "author": ["Duh", "Kirchhoff2008] Kevin Duh", "Katrin Kirchhoff"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human", "citeRegEx": "Duh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duh et al\\.", "year": 2008}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan et al.2009] Dumitru Erhan", "Pierre antoine Manzagol", "Yoshua Bengio", "Samy Bengio", "Pascal Vincent"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao et al.2014] Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Tuning as ranking", "author": ["Hopkins", "May2011] Mark Hopkins", "Jonathan May"], "venue": null, "citeRegEx": "Hopkins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2011}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In HLT-NAACL", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "On optimization methods for deep learning", "author": ["Le et al.2011] Quoc V. Le", "Jiquan Ngiam", "Adam Coates", "Ahbik Lahiri", "Bobby Prochnow", "Andrew Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Tree-to-string alignment template for statistical machine translation", "author": ["Liu et al.2006] Yang Liu", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of the 44th Annual Meeting of the Association of Computational Linguistics", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Additive neural networks for statistical machine translation", "author": ["Liu et al.2013] Lemao Liu", "Taro Watanabe", "Eiichiro Sumita", "Tiejun Zhao"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Learning new semi-supervised deep auto-encoder features for statistical machine translation", "author": ["Lu et al.2014] Shixiang Lu", "Zhenbiao Chen", "Bo Xu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol-", "citeRegEx": "Lu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Unsupervised deep belief features for speech translation", "author": ["Maskey", "Zhou2012] Sameer Maskey", "Bowen Zhou"], "venue": "In INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication", "citeRegEx": "Maskey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Maskey et al\\.", "year": 2012}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Och", "Ney2002] Franz Josef Och", "Hermann Ney"], "venue": null, "citeRegEx": "Och et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Och et al\\.", "year": 2002}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL \u201902: Proceedings of the 40th Annual", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A brief introduction to boosting", "author": ["Robert E. Schapire"], "venue": "In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2,", "citeRegEx": "Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Schapire.", "year": 1999}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Optimized online rank learning for machine translation", "author": ["Taro Watanabe"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Watanabe.,? \\Q2012\\E", "shortCiteRegEx": "Watanabe.", "year": 2012}, {"title": "A syntax-based statistical translation model", "author": ["Yamada", "Knight2001] Kenji Yamada", "Kevin Knight"], "venue": "In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Yamada et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 3, "context": "The well-known modeling method starts from the Source-Channel model (Brown et al., 1993)(Equation 1).", "startOffset": 68, "endOffset": 88}, {"referenceID": 21, "context": "Pr(e|f) = Pr(e)Pr(f |e)/Pr(f) (1) The modeling method is extended to log-linear models by Och and Ney (2002), as shown in Equation 2, where hm(e|f) is the mth feature function and \u03bbm is the corresponding weight.", "startOffset": 90, "endOffset": 109}, {"referenceID": 8, "context": "bad ones (Clark et al., 2014).", "startOffset": 9, "endOffset": 29}, {"referenceID": 14, "context": "Taking features in a typical phrase-based machine translation system (Koehn et al., 2003) as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses.", "startOffset": 69, "endOffset": 89}, {"referenceID": 21, "context": "Loglinear models rely on minimum error rate training (MERT) (Och, 2003) to achieve best performance.", "startOffset": 60, "endOffset": 71}, {"referenceID": 2, "context": "to linear models; two-layer networks with sufficient nodes are capable of learning any continuous function (Bishop, 1995).", "startOffset": 107, "endOffset": 121}, {"referenceID": 11, "context": "ishing gradient (Erhan et al., 2009).", "startOffset": 16, "endOffset": 36}, {"referenceID": 7, "context": "Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder.", "startOffset": 0, "endOffset": 137}, {"referenceID": 24, "context": "Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014),", "startOffset": 59, "endOffset": 101}, {"referenceID": 12, "context": "translation models (Gao et al., 2014) or joint language and translation models (Auli et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 1, "context": ", 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014).", "startOffset": 49, "endOffset": 89}, {"referenceID": 9, "context": ", 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014).", "startOffset": 49, "endOffset": 89}, {"referenceID": 0, "context": ", 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target side", "startOffset": 50, "endOffset": 109}, {"referenceID": 5, "context": "In this paper, we take the hierarchical phrase based machine translation system (Chiang, 2005) as an example and introduce how we fit the non-linearity into the system.", "startOffset": 80, "endOffset": 94}, {"referenceID": 14, "context": "The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006).", "startOffset": 123, "endOffset": 200}, {"referenceID": 5, "context": "The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006).", "startOffset": 123, "endOffset": 200}, {"referenceID": 16, "context": "The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006).", "startOffset": 123, "endOffset": 200}, {"referenceID": 5, "context": ", 2003; Chiang, 2005; Liu et al., 2006). For example, in the experiments of this paper, we use a CKY style decoding algorithm following Chiang (2005).", "startOffset": 8, "endOffset": 150}, {"referenceID": 5, "context": "We also use the cube-pruning algorithm (Chiang, 2005) to keep the decoding efficient.", "startOffset": 39, "endOffset": 53}, {"referenceID": 5, "context": "We use the standard features of a typical hierarchical phrase based translation system(Chiang, 2005).", "startOffset": 86, "endOffset": 100}, {"referenceID": 21, "context": "MERT performs efficient search by enumerating the score function of all the hypotheses and using intersections of these linear functions to form the \u201dupper-envelope\u201d of the model score function (Och, 2003).", "startOffset": 194, "endOffset": 205}, {"referenceID": 22, "context": "The correctness is often introduced by some extrinsic metrics, such as BLEU (Papineni et al., 2002).", "startOffset": 76, "endOffset": 99}, {"referenceID": 6, "context": "This objective is motivated by the practice of separating the \u201dhope\u201d and \u201dfear\u201d translation hypothesis (Chiang, 2012).", "startOffset": 103, "endOffset": 117}, {"referenceID": 25, "context": "For the binary classification task, we use a hinge loss following Watanabe (2012). Because the network has a lot of parameters compared with the linear model, we use a L1 norm instead of L2 norm as the regularization term, to favor sparse solutions.", "startOffset": 66, "endOffset": 82}, {"referenceID": 22, "context": "MERT and PRO extend the current nbest set by merging the n-best set of all previous iterations into a pool (Papineni et al., 2002; Hopkins and May, 2011).", "startOffset": 107, "endOffset": 153}, {"referenceID": 23, "context": "This is inspired by the AdaBoost algorithm (Schapire, 1999) in weighting instances.", "startOffset": 43, "endOffset": 59}, {"referenceID": 15, "context": "\u03b8i+1 is obtained by solving Equation 6 using the Conjugate Sub-Gradient method (Le et al., 2011) (line 10).", "startOffset": 79, "endOffset": 96}, {"referenceID": 5, "context": "is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005).", "startOffset": 81, "endOffset": 95}, {"referenceID": 22, "context": "The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002).", "startOffset": 70, "endOffset": 93}, {"referenceID": 5, "context": "is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing(Chen and Goodman, 1998). For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002). Significant test is performed using bootstrap re-sampling implemented by Clark et al. (2011). We employ a twolayer neural network with 11 input layer nodes,", "startOffset": 82, "endOffset": 529}], "year": 2015, "abstractText": "Modern statistical machine translation (SMT) systems usually use a linear combination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the current data. In this paper, we propose a nonlinear modeling for the quality of translation hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear models. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model.", "creator": "LaTeX with hyperref package"}}}