{"id": "1503.00185", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "When Are Tree Structures Necessary for Deep Learning of Representations?", "abstract": "Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up from parse children, are a popular new architecture, promising to capture structural properties like the scope of negation or long-distance semantic dependencies. But understanding exactly which tasks this parse-based method is appropriate for remains an open question. In this paper we benchmark recursive neural models against sequential recurrent neural models, which are structured solely on word sequences. We investigate 5 tasks: sentiment classification on (1) sentences and (2) syntactic phrases; (3) question answering; (4) discourse parsing; (5) semantic relations (e.g., component-whole between nouns); We find that recurrent models have equal or superior performance to recursive models on all tasks except one: semantic relations between nominals. Our analysis suggests that tasks relying on the scope of negation (like sentiment) are well-handled by sequential models. Recursive models help only with tasks that require representing long-distance relations between words. Our results offer insights on the design of neural architectures for representation learning.", "histories": [["v1", "Sat, 28 Feb 2015 21:39:31 GMT  (578kb,D)", "http://arxiv.org/abs/1503.00185v1", null], ["v2", "Fri, 6 Mar 2015 18:16:50 GMT  (584kb,D)", "http://arxiv.org/abs/1503.00185v2", null], ["v3", "Fri, 24 Apr 2015 17:14:49 GMT  (585kb,D)", "http://arxiv.org/abs/1503.00185v3", null], ["v4", "Thu, 18 Jun 2015 22:07:45 GMT  (679kb,D)", "http://arxiv.org/abs/1503.00185v4", null], ["v5", "Tue, 18 Aug 2015 05:59:18 GMT  (261kb,D)", "http://arxiv.org/abs/1503.00185v5", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["jiwei li", "thang luong", "dan jurafsky", "eduard h hovy"], "accepted": true, "id": "1503.00185"}, "pdf": {"name": "1503.00185.pdf", "metadata": {"source": "CRF", "title": "When Are Tree Structures Necessary for Deep Learning of Representations?", "authors": ["Jiwei Li", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu,", "jurafsky@stanford.edu,", "ehovy@andrew.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Recursive and Recurrent Models", "text": "In this section we present the recursive and recursive models we are considering. Note that our work applies existing models to existing tasks and does not offer new algorithms or tasks or new state-of-the-art results. Our goal is rather analytical. We examine different versions of recursive and recursive models. We therefore pair each recursive model with a suitable recursive model (standard recursive versus standard recursive, bidirectional recursive versus bidirectional recursive, etc.) 1"}, {"heading": "2.1 Notations", "text": "We assume that the text unit S, which could be a phrase, a sentence, or a document, consists of a sequence of tokens / words: S = {w1, w2,..., wNS}, where Ns denotes the number of tokens in S. Each word w is associated with a K-dimensional vector embedding ew = {e1w, e 2 w,..., e K w}. The1Note that we are not able to examine various complex but less general algorithm variants, such as the use of auto-encoder pre-courses in the recursive models of [32] or the use of MV-RNN to represent each word with both a matrix and a vector in word-word configurations [21], since these approaches are usually tailored to specific task scenarios. The goal of recurrent and recurrent models is to build the sequence based on its tokens and their respective embets on a K."}, {"heading": "2.2 Recurrent Models", "text": "(Standard Recurrent Models A recurrent network successively takes wordwi at step i, combines its vector representation ewt with previous built embedding ei \u2212 1 from time i \u2212 1, calculates the resulting current embedding et, and goes it to the next step. Thus, the embedding et for the current time t is: et = f (W \u00b7 et \u2212 1 + V \u00b7 ewt) (1) where W and V dimensional convolutional matrixes and f (\u00b7) denotes the activation function. Bias vectors are omitted. If Ns specifies the length of the sequence, eNs represents the entire sequence S.Bidirectional Recurrent Models [33] add bidirektionality to the recurrent framework where embedings for each time are richly and backwardly: e \u2192 t = f (W \u2192 e = e = f (W \u00b7 e \u2192 1 + V \u2192 ewt) e models."}, {"heading": "2.3 Recursive Models", "text": "The standard recursive models are based on the structure of the parse trees, with each leaf node corresponding to a word from the origin clause and calculating a representation for each parent node based on its immediate children recursively from bottom to top until it reaches the root of the tree. Specifically, the standard recursive mesh for a particular node in the tree and its left child \"left\" (with vector representation on the left) and \"right\" (with vector representation on the right) calculates the standard recursive models as follows: e\u03b7 = f (W \u00b7 ecleft + V \u00b7 ecright) (6), whereas W and V characterize the activation function of the revolutionary matrices K \u00b7 K and f (\u00b7). Bidirectional recursive models extend the standard recursive models and consider both recursive and information propagation along the tree (18). Each node p is associated with two vectors, an upward vector and a downward vector."}, {"heading": "3 Experiments", "text": "In this section, we will detail our experimental settings and results. We will consider 5 tasks, each of which covers a different class of NLP assignments.1. Binary sentiment classification on the [30] dataset. This addresses the problems where supervisions exist worldwide only at the top of long sequences.2. Sentiment Classification on the Stanford Sentiment Treebank [17]: Comprehensive labels are found on token and phrase levels, where local compositional nodes for recursive models (such as negation, mood, or other phrase-structured modifications) are to be learned.3. Question answering the UMD QA dataset [20]: Learn matches between target and phrase components in the source sets, which are tree nodes for recursive models, and different timesteps for recurrent modifications. 4. Semantic relationship classification on the courses of the 2010-31 learning task may be far away from each other [over two words] long distances."}, {"heading": "3.1 Binary Sentiment Classification (Pang)", "text": "The sentiment data set of [30] consists of sentences with a sentiment label for each sentence. We divide the original data set into Training (8101) / Developing (500) / Testing (2000). No pre-training method is used as described in [32]. Word embeddings are initialized with GloVe [40] and fixed in the learning process. Sentence level embeddings are fed into a sigmoid classifier: for bidirectional recursive and RNTN recursive models, the concatenation of left-sided and rightly calculated embeddings is used as set level vectors. The performances are shown in Figure 2; no great difference appears between standard recursive and recursive models or RNTN recursive models. LSTM / BiLSTM achieve the best performance by slightly exceeding the gate recursion."}, {"heading": "3.2 Stanford Sentiment TreeBank", "text": "This year it is more than ever before."}, {"heading": "3.3 UMD Question Answering Dataset", "text": "The QANTA6 question and answer data set consists of 1,713 historical questions (8,149 questions) and 2,246 literary questions (10,552 sentences), each question coupled with an answer [20]. We give an illustrative example here: Question: He left a novel unfinished, the title character of which forges the signature of his father to get out of school and avoids the design by pretending to connect. Let's name this German author of the Magic Mountain and Death in Venice. Answer: Thomas Mann.The model of [20] minimizes the distances between answer embedding and embedding along the Parsetree of the question. Specifically, we leave c the correct answer to question S, with embedding ~ c, and z naming each random wrong answer. The objective function of the dot product between representation for each node and embedding along the Parsetree of the question."}, {"heading": "3.4 Semantic Relationship Classification", "text": "SemEval-2010 task 8 [31] is to find semantic relationships between nominal pairs, e.g. in \"My [apartment] e1 has a fairly large [kitchen] e2,\" which classifies the relationship between [apartment] and [kitchen] as a whole. The data set contains 9 ordered relationships, so that the task is formalized as a 19-class classification problem, treating directed relationships as separate labels; see [31, 21] for details. For the recursive implementations, we follow the neural framework defined in [21]. The path in the parse tree between the two nominals is retrieved, and the embedding is calculated on the basis of recursive models and fed to a Softmax classifier. 7 Retrieved paths are transformed for the recursive models, as shown in Figure 9.7 [21], whereby the parse tree between the two nominals achieves a recursive performance, presenting a general decision with both a highly developed MeltRV and a highly recursive one, each word being represented as a MeltRV in turn."}, {"heading": "3.5 Discourse Parsing", "text": "Our final task, the discourse sparse based on the RST-DT corpus [44], is to create a discourse tree for a document based on the assignment of Rhetorical Structure Theory (RST) relations between elementary discourse units (EDUs). Since discourse relations express the coherence structure of the discourse, they probably express other aspects of compositional meaning than feelings or nominal relations. A discourse relativity classifier takes as input a vector that embeds the representation of each of the two EDUs, and calculates the most likely relationship [24]. We compare recursive with recursive methods for calculating EDU embeddings. See [45, 36] for more details on discourse sparse and the RST-DT corpus. Representations for neighboring EDUs are fed into binary classifications (whether two EDUs are related) and multiclore relational models are defined as they are in [24]."}, {"heading": "4 Discussions", "text": "We compared recursive and recursive neural models for learning representation using five separate NLP tasks, for which recursive neural models are known to perform well [21, 17, 24, 20]. Except for a semantic relationship, recursive models perform just as well or better than recursive models. Our results suggest that recursive models, especially LSTMs, are sufficient to capture the kind of compositional semantics needed to build feelings for phrases or sentences or discourse relationships between sentences or to link answers to questions. Recursive models shine only in tasks such as semantic relationship extratzion, where single keywords have to be connected over long distances. To be able to model such phenomena, sequential models require methods to model such dependencies over long distances."}], "references": [{"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Learning state space trajectories in recurrent neural networks", "author": ["Barak A Pearlmutter"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Neural networks for time series processing", "author": ["Georg Dorffner"], "venue": "In Neural Network World. Citeseer,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "The use of recurrent neural networks in continuous speech recognition", "author": ["Tony Robinson", "Mike Hochberg", "Steve Renals"], "venue": "In Automatic speech and speaker recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Review of neural networks for speech recognition", "author": ["Richard P Lippmann"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Alex Graves", "Juergen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Alex Graves"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J Williams", "David Zipser"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Global belief recursive neural networks", "author": ["Romain Paulus", "Richard Socher", "Christopher D Manning"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Can recursive neural tensor networks learn logical reasoning", "author": ["Samuel R Bowman"], "venue": "arXiv preprint arXiv:1312.6192,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Recursive neural networks for learning logical semantics", "author": ["Samuel R Bowman", "Christopher Potts", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1406.1827,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Bidirectional recursive neural networks for token-level labeling with structure", "author": ["Ozan Irsoy", "Claire Cardie"], "venue": "arXiv preprint arXiv:1312.0493,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Adaptive recursive neural network for target-dependent twitter sentiment classification", "author": ["Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Simple customization of recursive neural networks for semantic relation classification", "author": ["Kazuma Hashimoto", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama"], "venue": "In EMNLP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "A model of coherence based on distributed sentence representation", "author": ["Jiwei Li", "Eduard Hovy"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP, pages 1700\u20131709,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": "In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "Lstm can solve hard long time lag problems", "author": [], "venue": "In Advances in Neural Information Processing Systems 9: Proceedings of the 1996 Conference,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1997}, {"title": "Hilda: a discourse parser using support vector machine classification", "author": ["Hugo Hernault", "Helmut Prendinger", "Mitsuru Ishizuka"], "venue": "Dialogue & Discourse,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Neural Networks,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1996}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory", "author": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 2, 3]), successfully capturing capturing syntactic and semantic aspects of text.", "startOffset": 2, "endOffset": 11}, {"referenceID": 1, "context": ", [1, 2, 3]), successfully capturing capturing syntactic and semantic aspects of text.", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": ", [1, 2, 3]), successfully capturing capturing syntactic and semantic aspects of text.", "startOffset": 2, "endOffset": 11}, {"referenceID": 3, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 57, "endOffset": 63}, {"referenceID": 4, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 57, "endOffset": 63}, {"referenceID": 5, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 88, "endOffset": 97}, {"referenceID": 6, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 88, "endOffset": 97}, {"referenceID": 7, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 88, "endOffset": 97}, {"referenceID": 8, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 125, "endOffset": 132}, {"referenceID": 9, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 125, "endOffset": 132}, {"referenceID": 10, "context": "They were also applied early on to NLP [11], modeling a sentence", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "Recursive neural models [12], by contrast, are structured by syntactic parse trees.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 2, "endOffset": 10}, {"referenceID": 13, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 69, "endOffset": 77}, {"referenceID": 15, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 69, "endOffset": 77}, {"referenceID": 16, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 98, "endOffset": 110}, {"referenceID": 17, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 98, "endOffset": 110}, {"referenceID": 18, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 98, "endOffset": 110}, {"referenceID": 19, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 131, "endOffset": 135}, {"referenceID": 20, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 161, "endOffset": 169}, {"referenceID": 21, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 161, "endOffset": 169}, {"referenceID": 22, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 185, "endOffset": 193}, {"referenceID": 23, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 185, "endOffset": 193}, {"referenceID": 17, "context": "For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 24, "context": "Recurrent models without parse structures have shown good results in sequence-to-sequence generation [25] for machine translation (e.", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 2, "endOffset": 14}, {"referenceID": 26, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 2, "endOffset": 14}, {"referenceID": 27, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 2, "endOffset": 14}, {"referenceID": 28, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 98, "endOffset": 101}, {"referenceID": 16, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "Binary sentiment classification at the sentence level ([30]) and the phrase level (Sentiment Treebank [17]) can help us understand the role of recursive models in dealing with semantic compositionality.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "Binary sentiment classification at the sentence level ([30]) and the phrase level (Sentiment Treebank [17]) can help us understand the role of recursive models in dealing with semantic compositionality.", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "Question answering on the UMD QA dataset [20] can helps see whether parsing is useful for finding similarities between source sentences and target.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "and Semantic Relation Classification on the SemEval-2010 [31] data helps us understand whether parsing is helpful in dealing with long-term dependencies, such as relations between two words that are far apart in the sequence.", "startOffset": 57, "endOffset": 61}, {"referenceID": 31, "context": "The 1Note that we are unable to investigate various sophisticated but less general algorithm variants, such as the use of auto-encoder pre-training in the recursive models of [32], or the MV-RNN use of representing each word with both a matrix and a vector in word-word-convolutions [21] since these approaches are usually tailored to specific task scenarios.", "startOffset": 175, "endOffset": 179}, {"referenceID": 20, "context": "The 1Note that we are unable to investigate various sophisticated but less general algorithm variants, such as the use of auto-encoder pre-training in the recursive models of [32], or the MV-RNN use of representing each word with both a matrix and a vector in word-word-convolutions [21] since these approaches are usually tailored to specific task scenarios.", "startOffset": 283, "endOffset": 287}, {"referenceID": 32, "context": "Bidirectional Recurrent Models [33] add bidirectionality to the recurrent framework where embeddings for each time are calculated both forwardly and backwardly: et = f(W \u2192 \u00b7 et\u22121 + V\u2192 \u00b7 ewt) et = f(W \u2190 \u00b7 et\u22121 + V\u2190 \u00b7 ewt) (2)", "startOffset": 31, "endOffset": 35}, {"referenceID": 33, "context": "Long Short Term Memory (LSTM) LSTM [34, 35] constitutes a sophisticated version of recurrent models by associating each timestep with an input, memory and output gate, denoted by it, ft and ot, the values of which are given by it = \u03c3(Wi \u00b7 ext + Vi \u00b7 et\u22121) ft = \u03c3(Wf \u00b7 ext + Vf \u00b7 et\u22121) ot = \u03c3(Wo \u00b7 ext + Vo \u00b7 et\u22121) (3)", "startOffset": 35, "endOffset": 43}, {"referenceID": 34, "context": "Long Short Term Memory (LSTM) LSTM [34, 35] constitutes a sophisticated version of recurrent models by associating each timestep with an input, memory and output gate, denoted by it, ft and ot, the values of which are given by it = \u03c3(Wi \u00b7 ext + Vi \u00b7 et\u22121) ft = \u03c3(Wf \u00b7 ext + Vf \u00b7 et\u22121) ot = \u03c3(Wo \u00b7 ext + Vo \u00b7 et\u22121) (3)", "startOffset": 35, "endOffset": 43}, {"referenceID": 0, "context": "where \u03c3 denotes the sigmoid function, and it, ft and ot are scalars in [0,1].", "startOffset": 71, "endOffset": 76}, {"referenceID": 33, "context": "See [34, 27] for further details on LSTM.", "startOffset": 4, "endOffset": 12}, {"referenceID": 26, "context": "See [34, 27] for further details on LSTM.", "startOffset": 4, "endOffset": 12}, {"referenceID": 16, "context": "Recurrent Tensor Network We extended the RNTN [17] recursive model to the recurrent version which enables richer convolution between early time output et\u22121 and current word embedding ewt .", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Bidirectional Recursive Models extend standard recursive models and consider both downward and upward information propagations along the tree [18].", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": "Recursive Neural Tensor Network (RNTN) RNTN [17] enables richer composition between children embeddings.", "startOffset": 44, "endOffset": 48}, {"referenceID": 29, "context": "Binary sentiment classification on the [30] dataset.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Sentiment Classification on the Stanford Sentiment Treebank [17]: comprehensive labels are found at token and phrase levels where local compositionally (such as from negation, mood, or other phrase-structured modifications) are to be learned.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Question Answering on the UMD QA dataset [20]: Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different timesteps for recurrent models.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "Semantic Relation Classification on the SemEval-2010 task [31].", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "Discourse Parsing [24, 36]: Learns sentence-to-sentence relations based on calculated representations.", "startOffset": 18, "endOffset": 26}, {"referenceID": 35, "context": "Discourse Parsing [24, 36]: Learns sentence-to-sentence relations based on calculated representations.", "startOffset": 18, "endOffset": 26}, {"referenceID": 36, "context": "We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad [37] with minibatches [38].", "startOffset": 124, "endOffset": 128}, {"referenceID": 37, "context": "We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad [37] with minibatches [38].", "startOffset": 146, "endOffset": 150}, {"referenceID": 38, "context": "Derivatives are calculated from standard back-propagation [39].", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "The sentiment dataset of [30] consists of sentences with a sentiment label for each sentence.", "startOffset": 25, "endOffset": 29}, {"referenceID": 31, "context": "No pre-training procedure as described in [32] is employed.", "startOffset": 42, "endOffset": 46}, {"referenceID": 39, "context": "Word embeddings are initialized using GloVe [40] and kept fixed in the learning procedure3.", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "Since sentiment labels at the sentence level may be too coarse to show the benefits of parse structure, we turn to the Stanford Sentiment TreeBank [17].", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "For recursive models, we followed the protocols in [17] where node embeddings in the parse trees are obtained from recursive models and then fed to a softmax classifier.", "startOffset": 51, "endOffset": 55}, {"referenceID": 40, "context": "3Slightly better performances are observed by using GloVe than Word2Vect (either skip-grammar or CBOW) [41, 42] and SENNA [3].", "startOffset": 103, "endOffset": 111}, {"referenceID": 41, "context": "3Slightly better performances are observed by using GloVe than Word2Vect (either skip-grammar or CBOW) [41, 42] and SENNA [3].", "startOffset": 103, "endOffset": 111}, {"referenceID": 2, "context": "3Slightly better performances are observed by using GloVe than Word2Vect (either skip-grammar or CBOW) [41, 42] and SENNA [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 13, "context": "For binary classification, we did not train a separate model, just decoding the obtained probability vector into a binary decision as in [14]5.", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "The figure suggests that the kind of local compositions that recursive models handle, like negation flipping the sentiment of positive examples [17], can also be captured in sequential models.", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "5The performance of our implementations of recursive models is not exactly identical to that reported in [17], but the relative difference is less than 1%.", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "State-of-the-art models [2, 14] employ more sophisticated recursive structures.", "startOffset": 24, "endOffset": 31}, {"referenceID": 13, "context": "State-of-the-art models [2, 14] employ more sophisticated recursive structures.", "startOffset": 24, "endOffset": 31}, {"referenceID": 19, "context": "The question-answering dataset QANTA6 is comprised of 1,713 history questions (8,149 questions) and 2,246 literature questions (10,552 sentences), each question paired with an answer [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 19, "context": "The model of [20] minimizes the distances between answer embeddings and node embeddings along the parse tree of the question.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Because the publicly released dataset is smaller than the version used in [20] due to privacy issues, our numbers are not comparable to those in [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "Because the publicly released dataset is smaller than the version used in [20] due to privacy issues, our numbers are not comparable to those in [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "Here the parse trees are dependency parses following [20].", "startOffset": 53, "endOffset": 57}, {"referenceID": 42, "context": "[43]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "SemEval-2010 Task 8 [31] is to find semantic relationships between pairs of nominals, e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see [31, 21] for details.", "startOffset": 166, "endOffset": 174}, {"referenceID": 20, "context": "The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see [31, 21] for details.", "startOffset": 166, "endOffset": 174}, {"referenceID": 20, "context": "For the recursive implementations, we follow the neural framework defined in [21].", "startOffset": 77, "endOffset": 81}, {"referenceID": 20, "context": "7[21] achieve state-of-art performance by combining a sophisticated model, MV-RNN, in which each word is presented with both a matrix and a vector with human-feature engineering.", "startOffset": 1, "endOffset": 5}, {"referenceID": 43, "context": "Our final task, discourse parsing based on the RST-DT corpus [44], is to build a discourse tree for a document, based on assigning Rhetorical Structure Theory (RST) relations between elementary discourse units (EDUs).", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "A discourse relation classifier takes as input a vector embedding representation of each of the two EDUs, and computes the most likely relation [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 35, "context": "See [45, 36] for more details on discourse parsing and the RST-DT corpus.", "startOffset": 4, "endOffset": 12}, {"referenceID": 23, "context": "Representations for adjacent EDUs are fed into binary classification (whether two EDUs are related) and multi-class relation classification models, as defined in [24].", "startOffset": 162, "endOffset": 166}, {"referenceID": 23, "context": "Following [24], a CKY bottomup algorithm is used to compute the most likely discourse parse tree using dynamic programming.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "We compared recursive and recurrent neural models for representation learning on five separate NLP tasks for which recursive neural models are known to achieve good performance [21, 17, 24, 20].", "startOffset": 177, "endOffset": 193}, {"referenceID": 16, "context": "We compared recursive and recurrent neural models for representation learning on five separate NLP tasks for which recursive neural models are known to achieve good performance [21, 17, 24, 20].", "startOffset": 177, "endOffset": 193}, {"referenceID": 23, "context": "We compared recursive and recurrent neural models for representation learning on five separate NLP tasks for which recursive neural models are known to achieve good performance [21, 17, 24, 20].", "startOffset": 177, "endOffset": 193}, {"referenceID": 19, "context": "We compared recursive and recurrent neural models for representation learning on five separate NLP tasks for which recursive neural models are known to achieve good performance [21, 17, 24, 20].", "startOffset": 177, "endOffset": 193}], "year": 2015, "abstractText": "Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up from parse children, are a popular new architecture, promising to capture structural properties like the scope of negation or long-distance semantic dependencies. But understanding exactly which tasks this parse-based method is appropriate for remains an open question. In this paper we benchmark recursive neural models against sequential recurrent neural models, which are structured solely on word sequences. We investigate 5 tasks: sentiment classification on (1) sentences and (2) syntactic phrases; (3) question answering; (4) discourse parsing; (5) semantic relations (e.g., component-whole between nouns); We find that recurrent models have equal or superior performance to recursive models on all tasks except one: semantic relations between nominals. Our analysis suggests that tasks relying on the scope of negation (like sentiment) are well-handled by sequential models. Recursive models help only with tasks that require representing long-distance relations between words. Our results offer insights on the design of neural architectures for representation learning.", "creator": "LaTeX with hyperref package"}}}