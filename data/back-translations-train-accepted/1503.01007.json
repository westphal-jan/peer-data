{"id": "1503.01007", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "abstract": "While machine learning is currently very successful in several application domains, we are still very far from a real Artificial Intelligence. In this paper, we study basic sequence prediction problems that are beyond the scope of what is learnable with popular methods such as recurrent networks. We show that simple algorithms can be learnt from sequential data with a recurrent network associated with trainable stacks. We focus our study on algorithmically generated sequences such as $a^n b^{n}$, that can only be learnt by models which have the capacity to count. Our study highlights certain topics in machine learning that deserve more attention, such as addressing the shortcomings of purely gradient based training of non-convex models. We achieve progress in this direction by incorporating search based strategy. Once trained, we show that our method is able generalize to sequences up to an arbitrary size.", "histories": [["v1", "Tue, 3 Mar 2015 16:50:28 GMT  (662kb,D)", "http://arxiv.org/abs/1503.01007v1", null], ["v2", "Fri, 6 Mar 2015 22:41:39 GMT  (669kb,D)", "http://arxiv.org/abs/1503.01007v2", null], ["v3", "Wed, 20 May 2015 19:23:44 GMT  (552kb,D)", "http://arxiv.org/abs/1503.01007v3", null], ["v4", "Mon, 1 Jun 2015 20:37:55 GMT  (510kb,D)", "http://arxiv.org/abs/1503.01007v4", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["armand joulin", "tomas mikolov"], "accepted": true, "id": "1503.01007"}, "pdf": {"name": "1503.01007.pdf", "metadata": {"source": "META", "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "authors": ["Armand Joulin", "Tomas Mikolov"], "emails": ["AJOULIN@FB.COM", "TMIKOLOV@FB.COM"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Algorithmic Patterns", "text": "We are interested in sequences generated by simple short algorithms, and our goal is to learn these algorithms by executing sequence predictions. We are primarily interested in discrete patterns that appear to be related to those that occur in the real world, such as various forms of long-term memory. Specifically, we assume that we only have access to a long stream of data obtained by concatenating sequences generated by a given algorithm. In this essay, we focus on algorithmic patterns that contain some form of counting, addition, multiplication, and memorization. In Table 1, we show some examples of these patterns as well as the stream of data obtained by concatenating symbols as algorithmic patterns. In this essay, we focus on algorithmic patterns that include some form of counting, addition, multiplication, and memorization. In Table 1, we refer to the regularities in these sequences of symbols as algorithms as algorithmic patterns as algorithmic patterns."}, {"heading": "3. Related work", "text": "The algorithmic patterns we examine in this paper are closely related to context-free and context-sensitive grammars that have often been studied in the past. However, some work employs recursive networks with hard-wired symbolic structures (Gr\u00c3 \u00bc nwald, 1996; Crocker, 1996; Fanty, 1994), which are a continuous implementation of the symbolic system and can handle recursive symbolic systems in computational linguistics. While these approaches are interesting for understanding the connection between symbolic systems and neural networks, they are often designed manually for each specific grammar. Wiles & Elman (1995) has shown that it was possible to learn a standard recurrent network that is able to count on a limited bandwidth. They train a recurrent network on sequences of the form anbn for n = 1 to 12 and it is able to be generalized up to n = 18. While this is a promising result, it is difficult to generalize it over relatively small capacities."}, {"heading": "4. Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Simple recurrent network", "text": "We assume that these sequences are generated and concatenated by an algorithm to form a long stream of data. For example, given algorithmic patterns of the form {anbn | n > 0}, a possible data stream would be aabbaaabbbab. Our goal is to design a model that is able to predict the next symbol in these data streams. Our approach is based on a standard model called Recurrent Neural Networks (RNN) and popularized by Elman (1990).A RNN consists of an input layer, a hidden layer with a recursive time-delayed connection and an output layer. The recursive link allows propagation through time information about the state of the hidden layer. Given a sequence of tokens, one RNN takes as the input of the most uniform encoding of the current tokens and predicts the probability of the next one."}, {"heading": "4.2. Pushdown network", "text": "Our simple algorithmic patterns are very similar to non-context grammars (CFGs), which can be recognized by push-down automation, i.e., the automation that uses a stack. To design an RNN-based model that is able to learn our algorithmic patterns, it seems natural to design a stack-like structure that can be learned from the data. This type of model was used in the early 1990 \"s with some encouraging results (Das et al., 1992; Zeng et al., 1994; Mozer & Das, 1993). In this section, we propose a novel approach based on similar ideas. We propose two optimization schemes: stochastic gradient descent (Werbos, 1988) and a combination of stochastic gradient descent and search-based optimization elements. Stack is a type of persistent memory that can only be accessed by the top element."}, {"heading": "4.3. Search-based learning and combination with SGD", "text": "In fact, it seems that SGD cannot be adapted: Ultimately, we want to learn how to run stack-based long-term memory, and PUSH and POP operations are discreet in principle. Therefore, there is a significant risk that not only trivial problems will be solved, but that SGD-based training will always be in local minimums. An example of such undesirable behavior can be learning a bigram (understanding the frequency of common occurrence of symbols), which may not be an optimal solution. Any subsequent attempt to move outside of such a solution can be prevented by the type of SGD training."}, {"heading": "5. Experiments and results", "text": "We evaluate our model based on two problems: In the first, we look at various simple sequences generated by simple algorithms, and the goal is to learn their generating rule. We look at patterns similar to those previously studied (Das et al., 1992; Rodriguez et al., 1999; Bode \u0301 n & Wiles, 2000).In the second experiment, we look at the synthetic tasks introduced in Weston et al. (2014). In these tasks, we have a finite world of 4 characters, 3 objects, and 5 spaces. Each task is a story about this finite world, and the goal is to answer questions of increasing complexity about the state of the world. These tasks are designed to test the ability of a model to obtain discrete information over a long period of time.Implementation details. We implemented the model according to the diagram in Figure 1. The recurring matrix R defined in equation. (1) is set to 0 if we use GQ with 1988, and reverse spread with 50 microns (1)."}, {"heading": "5.1. Learning simple algorithmic patterns", "text": "This year it is more than ever before."}, {"heading": "5.2. Simulated world question answering", "text": "In this section, we look at the question of answering the tasks proposed in Weston et al. (2014), which propose a simulated world with 4 characters, 3 objects and 5 rooms; at each step, a figure performs an action - it moves, picks up an object or drops it; the actions are written into a text with simple automated grammar and every now and then a question is asked about the location of a person or an object; there are also two types of questions: either the question concerns the current location of a unit or it concerns a previously visited place, as in Figure 7. These tasks require that the models can store information for a potentially long period of time; in addition, they require the storage of information about several objects and persons; typically, this means that our model requires a large amount of stacks; these tasks require that we store information for a long period of time, so that we can use the NO-OP operation.In Table 2, we compare our model with RNN, the long-term memory model (LM Schmidhuter, & N), which we use to answer the questions in 1997."}, {"heading": "6. Discussion and future work", "text": "In this section we discuss the results obtained, limitations of our model, and potential approaches to solving problems: continuity versus discrete models and search. In our work, we show that certain simple algorithmic patterns can be learned efficiently by trying to solve problems of a similar type (Das et al., 1994; Wiles & Elman, 1995).Our model seems to be much better than many other models used for this type of task (Tabor, 2000; Bode; n & Wiles).It is noteworthy that our model is relatively robust and works for a wide range of hyperparameters (number of stacks, size of hidden layers, or learning rate)."}, {"heading": "7. Acknowledgment", "text": "We would like to thank Arthur Szlam, Keith Adams, Jason Weston, Yann LeCun and the rest of the Facebook research team for their useful comments. 1The pre-edited data set is available as part of the archive at http: / / www.fit.vutbr.cz / imikolov / rnnlm / simple-examples.tgz."}, {"heading": "8. Conclusion", "text": "We have shown that simple stack-based recurring networks can solve certain basic problems such as anbn, without clues (such as where the sequence begins), while using only SGD. The solution generalizes to much larger n than what the model is trained on, which is quite a positive result considering previous work on this difficult topic. Nevertheless, the SGD seems to be severely limited, and when we turn to more complex tasks, we had to combine it with search-based learning to tackle more complex mold tasks that require much more complex manipulation with stack memory. Although we have successfully solved these toy problems, it is clear that a fully scalable solution for learning algorithmic patterns in sequential data is still an open problem. We hope that our work will help other researchers who are currently working on deep learning to avoid constraints on popular architectures and training algorithms, and to implement case-based models more artificially, if we want to advance intelligence in the direction of the field."}], "references": [{"title": "Scaling learning algorithms towards ai", "author": ["Bengio", "Yoshua", "LeCun", "Yann"], "venue": "Large-scale kernel machines,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Pattern recognition and machine learning", "author": ["Bishop", "Christopher M"], "venue": "springer New York,", "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Context-free and contextsensitive dynamics in recurrent neural networks", "author": ["Bod\u00e9n", "Mikael", "Wiles", "Janet"], "venue": "Connection Science,", "citeRegEx": "Bod\u00e9n et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bod\u00e9n et al\\.", "year": 2000}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["Bottou", "Leon"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou and Leon.,? \\Q2010\\E", "shortCiteRegEx": "Bottou and Leon.", "year": 2010}, {"title": "Toward a connectionist model of recursion in human linguistic performance", "author": ["Christiansen", "Morten H", "Chater", "Nick"], "venue": "Cognitive Science,", "citeRegEx": "Christiansen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Christiansen et al\\.", "year": 1999}, {"title": "Highperformance neural networks for visual object classification", "author": ["Ciresan", "Dan C", "Meier", "Ueli", "Masci", "Jonathan", "Gambardella", "Luca M", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1102.0183,", "citeRegEx": "Ciresan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2011}, {"title": "Mechanisms for sentence processing", "author": ["Crocker", "Matthew W"], "venue": "Centre for Cognitive Science, University of Edinburgh,", "citeRegEx": "Crocker and W.,? \\Q1996\\E", "shortCiteRegEx": "Crocker and W.", "year": 1996}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["Dahl", "George E", "Yu", "Dong", "Deng", "Li", "Acero", "Alex"], "venue": "IEEE Transactions on,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Using prior knowledge in a nnpda to learn context-free languages", "author": ["Das", "Sreerupa", "Giles", "C Lee", "Sun", "Guo-Zheng"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Das et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Das et al\\.", "year": 1993}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Learning and development in neural networks: The importance of starting", "author": ["Elman", "Jeffrey L"], "venue": "small. Cognition,", "citeRegEx": "Elman and L.,? \\Q1993\\E", "shortCiteRegEx": "Elman and L.", "year": 1993}, {"title": "Context-free parsing in connectionist networks", "author": ["Fanty", "Mark"], "venue": "Parallel natural language processing,", "citeRegEx": "Fanty and Mark.,? \\Q1994\\E", "shortCiteRegEx": "Fanty and Mark.", "year": 1994}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "A recurrent network that performs a context-sensitive prediction task", "author": ["Gr\u00fcnwald", "Peter"], "venue": "In Proceedings of the Eighteenth Annual Conference of the Cognitive Science Society: July 12-15,", "citeRegEx": "Gr\u00fcnwald and Peter.,? \\Q1996\\E", "shortCiteRegEx": "Gr\u00fcnwald and Peter.", "year": 1996}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Designing a counter: Another case study of dynamics and activation landscapes in recurrent networks", "author": ["Holldobler", "Steffen", "Kalinke", "Yvonne", "Lehmann", "Helko"], "venue": "In KI-97: Advances in Artificial Intelligence,", "citeRegEx": "Holldobler et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Holldobler et al\\.", "year": 1997}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Statistical language models based on neural networks", "author": ["Mikolov", "Tomas"], "venue": "PhD thesis,", "citeRegEx": "Mikolov and Tomas.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tomas.", "year": 2012}, {"title": "Marc\u2019Aurelio. Learning longer memory in recurrent neural networks", "author": ["Mikolov", "Tomas", "Joulin", "Armand", "Chopra", "Sumit", "Mathieu", "Michael", "Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "A connectionist symbol manipulator that discovers the structure of contextfree languages", "author": ["Mozer", "Michael C", "Das", "Sreerupa"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Mozer et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Mozer et al\\.", "year": 1993}, {"title": "The induction of dynamical recognizers", "author": ["Pollack", "Jordan B"], "venue": "Machine Learning,", "citeRegEx": "Pollack and B.,? \\Q1991\\E", "shortCiteRegEx": "Pollack and B.", "year": 1991}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Recht", "Benjamin", "Re", "Christopher", "Wright", "Stephen", "Niu", "Feng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "A recurrent neural network that learns to count", "author": ["Rodriguez", "Paul", "Wiles", "Janet", "Elman", "Jeffrey L"], "venue": "Connection Science,", "citeRegEx": "Rodriguez et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 1999}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Fractal encoding of context-free grammars in connectionist networks", "author": ["Tabor", "Whitney"], "venue": "Expert Systems,", "citeRegEx": "Tabor and Whitney.,? \\Q2000\\E", "shortCiteRegEx": "Tabor and Whitney.", "year": 2000}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Werbos", "Paul J"], "venue": "Neural Networks,", "citeRegEx": "Werbos and J.,? \\Q1988\\E", "shortCiteRegEx": "Werbos and J.", "year": 1988}, {"title": "Learning to count without a counter: A case study of dynamics and activation landscapes in recurrent networks", "author": ["Wiles", "Janet", "Elman", "Jeff"], "venue": "In Proceedings of the Seventeenth Annual Conference of the Cognitive Science Society,", "citeRegEx": "Wiles et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Wiles et al\\.", "year": 1995}, {"title": "Gradient-based learning algorithms for recurrent networks and their computational complexity", "author": ["Williams", "Ronald J", "Zipser", "David"], "venue": "Back-propagation: Theory, architectures and applications,", "citeRegEx": "Williams et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1995}, {"title": "Discrete recurrent neural networks for grammatical inference", "author": ["Zeng", "Zheng", "Goodman", "Rodney M", "Smyth", "Padhraic"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Zeng et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 18, "context": "History shows that there are two major sources of breakthroughs: scaling up the existing approaches to larger datasets, and development of novel, more accurate approaches (LeCun et al., 1998; Elman, 1990; Breiman, 2001).", "startOffset": 171, "endOffset": 219}, {"referenceID": 6, "context": "has been made in scaling up algorithms, by either using alternative hardware such as GPUs (Ciresan et al., 2011) or by taking advantage of large clusters (Recht et al.", "startOffset": 90, "endOffset": 112}, {"referenceID": 23, "context": ", 2011) or by taking advantage of large clusters (Recht et al., 2011).", "startOffset": 49, "endOffset": 69}, {"referenceID": 17, "context": "Recently, deep neural networks have become very successful at various tasks, leading to a shift in the computer vision (Krizhevsky et al., 2012) and speech recognition communities (Dahl et al.", "startOffset": 119, "endOffset": 144}, {"referenceID": 8, "context": ", 2012) and speech recognition communities (Dahl et al., 2012).", "startOffset": 43, "endOffset": 62}, {"referenceID": 25, "context": "This lead to the use of non-linear hidden layers (Rumelhart et al., 1985) and kernels methods (Bishop, 2006).", "startOffset": 49, "endOffset": 73}, {"referenceID": 25, "context": "This lead to the use of non-linear hidden layers (Rumelhart et al., 1985) and kernels methods (Bishop, 2006). Another example is the parity problem introduced by Minsky & Papert (1969): it demonstrates that while a single non-linearity is sufficient to represent any function, it is not guaranteed to represent it efficiently, and in some cases can even require exponentially many more parameters (and thus, also training data).", "startOffset": 50, "endOffset": 185}, {"referenceID": 20, "context": "Our definition of a stack in a recurrent net is through constraining part of the recurrent matrix, similar to (Mikolov et al., 2014) where it was shown that diagonal recurrent matrix can help the recurrent net to store longer memory.", "startOffset": 110, "endOffset": 132}, {"referenceID": 31, "context": "Our work can be seen as a follow up of the research done in early nineties, when similar types of stack RNNs were studied (Pollack, 1991; Das et al., 1992; Mozer & Das, 1993; Zeng et al., 1994).", "startOffset": 122, "endOffset": 193}, {"referenceID": 13, "context": "Among recent papers with similar motivation, we are aware of the Neural Turing Machine (Graves et al., 2014) and Memory Networks (Weston et al.", "startOffset": 87, "endOffset": 108}, {"referenceID": 24, "context": "Rodriguez et al. (1999) further", "startOffset": 0, "endOffset": 24}, {"referenceID": 31, "context": "Closer to our approach, many works have used external memory modules with a recurrent network, such as stacks (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993).", "startOffset": 110, "endOffset": 197}, {"referenceID": 16, "context": "Closer to our approach, many works have used external memory modules with a recurrent network, such as stacks (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993).", "startOffset": 110, "endOffset": 197}, {"referenceID": 9, "context": "Also, as in (Das et al., 1993; Mozer & Das, 1993), their model is train on supervised data, while we aim at learning on the more challenging task of sequence prediction.", "startOffset": 12, "endOffset": 49}, {"referenceID": 9, "context": "Closer to our approach, many works have used external memory modules with a recurrent network, such as stacks (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993). Zeng et al. (1994) uses a discrete external stack which may be hard to learn on long sequences.", "startOffset": 111, "endOffset": 218}, {"referenceID": 9, "context": "Closer to our approach, many works have used external memory modules with a recurrent network, such as stacks (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993). Zeng et al. (1994) uses a discrete external stack which may be hard to learn on long sequences. Das et al. (1992) learns a continuous stack which has some similarity with ours.", "startOffset": 111, "endOffset": 313}, {"referenceID": 1, "context": "However, linear hidden units are not simple to learn in a recurrent network because of the exploding gradient problem (Bengio et al., 1994).", "startOffset": 118, "endOffset": 139}, {"referenceID": 0, "context": "However, linear hidden units are not simple to learn in a recurrent network because of the exploding gradient problem (Bengio et al., 1994). In the next section, we follow another direction introduced in Das et al. (1992): we use a continuous version of a pushdown stack as an external memory which has the theoretical capacity to learn simple algorithmic patterns.", "startOffset": 119, "endOffset": 222}, {"referenceID": 31, "context": "model has been widely used in the early nineties with some encouraging success (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993).", "startOffset": 79, "endOffset": 166}, {"referenceID": 16, "context": "model has been widely used in the early nineties with some encouraging success (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993).", "startOffset": 79, "endOffset": 166}, {"referenceID": 25, "context": "The model presented above is continuous and can thus be trained with stochastic gradient descent (SGD) method and back-propagation through time (Rumelhart et al., 1985; Williams & Zipser, 1995; Werbos, 1988).", "startOffset": 144, "endOffset": 207}, {"referenceID": 24, "context": "We consider similar patterns to the one studied previously (Das et al., 1992; Rodriguez et al., 1999; Bod\u00e9n & Wiles, 2000).", "startOffset": 59, "endOffset": 122}, {"referenceID": 1, "context": "Note that if we would use linear hidden units in RNN, it would be in theory possible to make them work on these sequences, but learning a linear RNN is quite challenging because of the exploding gradient problem (Bengio et al., 1994).", "startOffset": 212, "endOffset": 233}, {"referenceID": 31, "context": "Note that our model works much better than prior work based on RNN from the nineties that attempts to solve problems of similar type (Das et al., 1992; Zeng et al., 1994; Wiles & Elman, 1995).", "startOffset": 133, "endOffset": 191}, {"referenceID": 13, "context": "At the same time, our model seems simpler than many other models used for this type of tasks (Tabor, 2000; Bod\u00e9n & Wiles, 2000; Graves et al., 2014).", "startOffset": 93, "endOffset": 148}, {"referenceID": 13, "context": "(2014) and Graves et al. (2014) use more advanced forms of a long term memory.", "startOffset": 11, "endOffset": 32}], "year": 2015, "abstractText": "While machine learning is currently very successful in several application domains, we are still very far from a real Artificial Intelligence. In this paper, we study basic sequence prediction problems that are beyond the scope of what is learnable with popular methods such as recurrent networks. We show that simple algorithms can be learnt from sequential data with a recurrent network associated with trainable stacks. We focus our study on algorithmically generated sequences such as ab, that can only be learnt by models which have the capacity to count. Our study highlights certain topics in machine learning that deserve more attention, such as addressing the shortcomings of purely gradient based training of non-convex models. We achieve progress in this direction by incorporating search based strategy. Once trained, we show that our method is able generalize to sequences up to an arbitrary size.", "creator": "LaTeX with hyperref package"}}}