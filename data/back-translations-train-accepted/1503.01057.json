{"id": "1503.01057", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)", "abstract": "We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.", "histories": [["v1", "Tue, 3 Mar 2015 19:06:17 GMT  (1333kb,D)", "http://arxiv.org/abs/1503.01057v1", "19 pages, 4 figures"]], "COMMENTS": "19 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andrew gordon wilson", "hannes nickisch"], "accepted": true, "id": "1503.01057"}, "pdf": {"name": "1503.01057.pdf", "metadata": {"source": "CRF", "title": "Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)", "authors": ["Andrew Gordon Wilson", "Hannes Nickisch"], "emails": ["andrewgw@cs.cmu.edu", "hannes@nickisch.org"], "sections": [{"heading": null, "text": "We introduce a new framework for structured kernel interpolation (SKI), which generalizes and unifies inductive point methods for scalable Gaussian processes (GPs). SKI methods generate kernel approximations for fast calculations by kernel interpolation. SKI framework illustrates how the quality of an inductive point approach depends on the number of inductive (also called interpolation) points, interpolation strategy and GP covariant cores. SKI also provides a mechanism for creating new scalable kernel methods by selecting different kernel interpolation strategies. With SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than the induction of point alternatives, 2) of course allows Kronecker and Toeplitz algebra for substantial additional gains in scalability, without any grid data being required, and we can evaluate Kernel GP and ISS expressive GP."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to go to another world, to go to another world, to go to another world."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Gaussian Processes", "text": "We assume that we have a dataset of n input (predictor) vectors X = {x1,.., xn}, each finite number having a common Gaussian distribution. With the help of a GP we can define a distribution using the f (x), k) functions, which means that any collection of function values f = f (X) = [f (x1),., f (xn) is the common Gaussian distribution., f (xn) is the common Gaussian distribution: f (X) = f (X) = [f (x1),., f (xn), f (xn)] >."}, {"heading": "2.2 Inducing Point Based Sparse Approximations", "text": "Many popular approaches to scaling GP conclusions belong to a family of inducing point methods (Quin onero-Candela and Rasmussen, 2005), which can be regarded as a substitute for the exact kernel k (x, z) by approximating k (x, z) for fast calculations. For example, the prominent subset of regressors (SoR) (Silverman, 1985) and completely independent training conditional (FITC) (Snelson and Ghahramani, 2006) methods leads to approximating kernel SoR (x, z) = Kx, UK \u2212 1 U, UKU, z, (5) k FITC (x, z) = k SoR (x, z) \u2212 k SoR (x, z))), (6) for a series of m inducing points U = [ui].m Kx, U \u2212 1 U, U \u2212 and KU \u2212 n, KU \u00b7 n, KU \u00b7 K, KU \u00b7, KU \u00b7 U, SoR (x, Kn, Kn, Kn, Kn, Kn, \u00b7 U, Kz, Kz, Kx, Kx, Kx, Kz, Kz, K (x), K (x, Kz, K, K, Kz, Kz, Kz, Kx, Kz, K (x)."}, {"heading": "2.3 Fast Structure Exploiting Inference", "text": "Kronecker and Toeplitz methods use the existing structure of the GP covariance matrix K to expand conclusions and learning without approximations."}, {"heading": "2.3.1 Kronecker Methods", "text": "It follows that we can find the original composition of K = QV efficiently by decompiling the original composition of each of the K1,.., KP. One can decompile the Kronecker structure for fast products (Wilson et al., 2014).Fast and the original composition of each of the K1,., KP. One can similarly yield Kronecker structure for fast products (Wilson et al., 2014).Fast decompress and the original composition of K1,., KP."}, {"heading": "2.3.2 Toeplitz Methods", "text": "Toeplitz and Kronecker methods complement each other. K is a Toeplitz covariance matrix when it is generated by a stationary covariance core, k (x, x) = k (x \u2212 x \u2032), with input x on a regular one-dimensional grid. Toeplitz matrices are constant along their diagonals: Ki, j = Ki + 1, j + 1 = k (xi \u2212 xj). Toeplitz matrices can be embedded in circulating matrices to perform fast matrix vector products using fast Fourier transformations, e.g. Wilson (2014). Subsequently, linear conjugate gradients can be used to solve linear systems (K + \u03c32I) \u2212 1y in O (m logm) operations and O (m) storage, for m grid data. Turner (2010) and Cunningham et al. (2008) contain examples of toeplitz methods applied to GPs."}, {"heading": "3 Structured Kernel Interpolation", "text": "We want to efficiently solve the large O (n3) calculations and O (n2) storage systems. (U) We want to mitigate the large O (n3) storage systems with the large U (n2) storage systems. (U) We want to mitigate the large U (n3) storage systems with the large U (n2) storage systems (n2). We want to mitigate the large U (n2) storage systems with the large U (nA) storage systems (nA), nA (nA), NO (nA), NO (nA), NO (nA), NO (nA), NO (nA), NO (NA), NO (NO), NO (NO), NA, NO (NO), NO (NA), NO (NO), NO (NO), NO (NA), NO (NO), NO (NA, NO, NO (), NO (nA), NO (nA), NO (nA, NO (nA), NO (nA), NO (nA, NO, NO (nA), NO (nA), NO (nA, NO (nA), NO (nA, NO (nA), NO (nA), NO (nA, NO (nA), NO (nA, NO (nA), NO (nA, NO (nA), NO (nO (nO), NO (nA, NO (nO), NO (nA, NO (nA, nO), NO (), NO (nO (nO (nA, nO), NO (nA, NO (nA, nO), nO (nO (nO), nO (nO, nO, nO), nO (nO (nO), nO (nO (nO, nO, nO), nO (nO (nO), nO (nO, n"}, {"heading": "4 Experiments", "text": "We evaluate the SKI for the approximation of the kernel matrix (Section 4.1), kernel learning (Section 4.2) and natural sound modeling (Section 4.3). We compare in particular with FITC (Snelson and Ghahramani, 2006) because 1) FITC is the most popular approach for inducing points, 2) FITC has proven to be superior predictive power and similar efficiency over other inducing methods and is generally recommended (Naish-Guzman and Holden, 2007; Quinonero-Candela et al., 2007) and 3) FITC is well understood, and therefore FITC comparisons help clarify the basic properties of SKI, which is our primary goal. However, we also offer comparisons with SoR and SSGPR (La civizaro-Gredilla et al., 2010), a state of the art, scalable GP methods based on random calculations and O (m2n) dimensions."}, {"heading": "4.1 Covariance Matrix Reconstruction", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "4.2 Kernel Learning", "text": "We are now testing the ability to choose large m; in fact, we can achieve greater than significant efficiency gains while having the standard data. In fact, it is intended to scale the Gaussian processes to large datasets - and large datasets provide a distinct way to discover rich statistical representations by using kernel learning methods that are best suited to simple evaluation methods such as FITC. In other words, these scalable GP methods often miss the scalability of structures, one of the biggest motivations for looking at large datasets in the front row. This limitation stems from popular evaluation methods that require the number of inductive points m n, for computational tractability, which deprives us of the information necessary to learn complicated kernels, since we are free to choose large m; in fact, we can be greater than significant effectiveness while retaining effectiveness."}, {"heading": "4.3 Natural Sound Modelling", "text": "This year, it has come to the point that it is only a matter of time before it happens, until it happens, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "5 Discussion", "text": "We have introduced a new Structured Kernel Interpolation Framework (SKI) that generalizes and unifies, resulting in a scalable Gaussian process conference. In particular, we have shown how standard inducing point methods correspond to the kernel approximations formed by global Gaussian process kernel interpolation, so that the trivial solution of prediction by empirical means yields an SMAE of 1, and lower values correspond to better fits of the data.Inducing point method, which is of course combined with Kronecker and Toeplitz algebra for additional gains in scalability. In fact, we can consider KISS-GP as a relaxation of the strict network assumptions in Kronecker and Toeplitzer methods that lead to arbitrary interventions."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.", "creator": "LaTeX with hyperref package"}}}