{"id": "1503.01161", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification", "abstract": "We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the \"quintessential\" observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.", "histories": [["v1", "Tue, 3 Mar 2015 23:25:55 GMT  (1372kb,D)", "http://arxiv.org/abs/1503.01161v1", "Published in Neural Information Processing Systems (NIPS) 2014, Neural Information Processing Systems (NIPS) 2014"]], "COMMENTS": "Published in Neural Information Processing Systems (NIPS) 2014, Neural Information Processing Systems (NIPS) 2014", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["been kim", "cynthia rudin", "julie a shah"], "accepted": true, "id": "1503.01161"}, "pdf": {"name": "1503.01161.pdf", "metadata": {"source": "CRF", "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification", "authors": ["Been Kim", "Cynthia Rudin", "Julie Shah"], "emails": ["shah}@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "You can ignore the recommendations of Amazon.com and instead look at the listmania of an Amazon customer to find an example of a customer like us. We could ignore medical guidelines calculated by a large number of patients in favor of medical blogs in which we find examples of the experiences of individual patients. Numerous studies have shown that copy-based reasoning, which includes various forms of matching and prototyping, is fundamental to our most effective decision-making strategies (26, 9, 21)."}, {"heading": "2 Background and Related Work", "text": "People organize and interpret information through copy-based reasoning, especially when solving problems (26, 7, 9, 21).AI Cased-Based Reasoning approaches are motivated by this knowledge and provide case studies along with the machine-learned solution. Studies show that the underlying structure of the solutions, compared to providing the solution, is also a challenge. [29] CBR models require solutions (i.e. labels) and do not learn the underlying structure of the data in an unmonitored manner. Transparency in complex situations also remains a challenge."}, {"heading": "3 The Bayesian Case Model", "text": "The model could generate a film profile that represents the underlying structure of the observations. It reinforces the standard mixing models with prototypes and subspace characteristics that characterize the clusters. We show in Section 4.2 that prototypes and subspace characteristics improve the human interpretation capability compared to the standard mixing models. The graphical model for BCM is shown in Figure 1. We begin with observations represented by x = {x1, x2,. xN}, with each xi as a random mix over clusters. There are S clusters where S is assumed to be known in advance."}, {"heading": "3.1 Motivating example", "text": "This section provides an example of prototypes and subspace clusters testing a mix of smileys and smartwatches."}, {"heading": "3.2 Inference: collapsed Gibbs sampling", "text": "We use the collapsed Gibbs sampling to draw conclusions from this, as this has been observed to converge quickly, especially in mixture models [17]. We sample \u03c9sj, zij, and ps, where \u03c6 and \u03c0 are integrated out. Note that we can restore \u03c6 simply by counting the number of characteristic values associated with each subspace. Integration of \u03c6 and \u03c0 results in the following expression for sampling zij: p (zij = s | zi \u00ac j, x, p, \u03c9) + n (s, \u00b7, j, \u00b7, \u00b7) \u03b1 / S + n (s, i, \u00ac j, \u00b7, \u00b7) \u03b1 + n \u00b7 g (psj, \u03bb) + n (s, \u00b7, \u00b7, xij, x). In other words, if xij takes the characteristic value v for feature j and is assigned to the cluster, it means that n (s, j, j, v, v, v)."}, {"heading": "4 Results", "text": "In this section, we show that BCM achieves predictive accuracy that is comparable or better than LDA for standard datasets. We also test the interpretability of BCM through human experimentation with a task that requires an understanding of clusters within a dataset. We show statistically significant improvements in objective measures of task fulfillment based on prototypes produced by BCM compared to the output of LDA. Finally, we visually demonstrate that the learned prototypes and sub-spaces serve as meaningful feedback for characterizing important aspects of the dataset."}, {"heading": "4.1 BCM maintains prediction accuracy.", "text": "We show that the BCM output generates predictive accuracy that is comparable or better than that of LDA, which uses the same blending model (Section 3) to learn the underlying structure but does not learn explanations (i.e. prototypes and subspaces).We validate this by using two standard datasets: handwritten numerals [19] and 20 newsgroups [22].We use the implementation of LDA available from [27], using the same inference technique used for BCM Figure 2a to represent the ratio of correctly assigned cluster identifiers for BCM and LDA. To compare the predictive accuracy with LDA, the learned cluster identifiers are described as characteristics for a support vector machine (SVM) with linear kernel, as is often done in the LDA literature on cluster identification [6].The improved accuracy of BM over groups is explained by LDA capability, as described in part in the previous 00001 diagram."}, {"heading": "4.2 Verifying the interpretability of BCM", "text": "We tested the interpretative capability of BCM by including human subjects who required an understanding of clusters within a dataset. This task required each participant to assign 16 recipes described only by a set of required ingredients (recipe names and instructions were withheld) to obtain a cluster representation from a set of four to six. (This approach is similar to those used in the previous paper to measure intelligibility.) We chose a recipe for dataset 1 for this task because such a dataset needs to be well explained so that the subjects can perform a classification, but does not require special expertise or training. Our experiment included a design within subjects that enabled more powerful statistical testing and mitigated the effects of internal variability. To take into account possible learning effects, we blocked the BCM and LDA questions and balanced the assignment of the participants into the two groups."}, {"heading": "4.3 Learning subspaces", "text": "Figure 4a illustrates the learned prototypes and sub-spaces as a function of scanning iterations for the dataset of handwritten digits. For the later iterations shown to the right of the figure, the BCM output effectively characterizes the important aspects of the data. In particular, the sub-spaces learned by BCM are pixels that define the number for the prototype of the cluster. Interestingly, the sub-space highlights the absence of font in certain areas. This makes sense: for example, one can define a \"7\" by showing the absence of pixels on the left of the image where the loop of a \"9\" might otherwise appear. Pixels located where the variability between the digits of the same cluster exists are not part of the defining sub-space for the cluster. As we have randomly initialized, in early iterations, the sub-spaces show that the characteristics common to the observations are defined in the pixels."}, {"heading": "5 Conclusion", "text": "The Bayesian case model provides a generative framework for case-based thinking and prototype-based modeling. Its clusters contain natural explanations, namely a prototype (a prime example of the cluster) and a number of defining features for this cluster. We demonstrated the quantitative advantages in terms of predictive quality and interpretability resulting from the use of BCM. Example-based modeling (neighboring models, case-based thinking) has historical roots going back to the early days of artificial intelligence; this method offers a new perspective on this topic and a new way of thinking about the balance of accuracy and interpretability in predictive modeling."}], "references": [{"title": "A knowledge-intensive, integrated approach to problem solving and sustained learning", "author": ["A. Aamodt"], "venue": "Knowledge Engineering and Image Processing Group. University of Trondheim, pages 27\u201385,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1991}, {"title": "Case-based reasoning: Foundational issues, methodological variations, and system approaches", "author": ["A. Aamodt", "E. Plaza"], "venue": "AI communications,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "How to explain individual classification decisions", "author": ["D. Baehrens", "T. Schroeter", "S. Harmeling", "M. Kawanabe", "K. Hansen", "K.R. M\u00fcller"], "venue": "JMLR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Case-based reasoning in the health sciences: What\u2019s next", "author": ["I. Bichindaritz", "C. Marling"], "venue": "AI in medicine,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Prototype selection for interpretable classification", "author": ["J. Bien", "R. Tibshirani"], "venue": "AOAS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JMLR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Analyzing decision behavior: The magician\u2019s audience", "author": ["J.S. Carroll"], "venue": "Cognitive processes in choice and decision behavior,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1980}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J. Chang", "J.L. Boyd-Graber", "S. Gerrish", "C. Wang", "D.M. Blei"], "venue": "NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Metarecognition in time-stressed decision making: Recognizing, critiquing, and correcting", "author": ["M.S. Cohen", "J.T. Freeman", "S. Wolf"], "venue": "Human Factors,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "Information Theory,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1967}, {"title": "An evaluation of the usefulness of case-based explanation", "author": ["P. Cunningham", "D. Doyle", "J. Loughrey"], "venue": "CBRRD. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Classification and regression trees: a powerful yet simple technique for ecological data analysis", "author": ["G. De\u2019ath", "K.E. Fabricius"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Sparse additive generative models of text", "author": ["J. Eisenstein", "A. Ahmed", "E. Xing"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Comprehensible classification models: a position paper", "author": ["A. Freitas"], "venue": "ACM SIGKDD Explorations,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Box drawings for learning with imbalanced data", "author": ["S. Goh", "C. Rudin"], "venue": "KDD,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Prototype classification: Insights from machine learning", "author": ["A. Graf", "O. Bousquet", "G. R\u00e4tsch", "B. Sch\u00f6lkopf"], "venue": "Neural computation,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "PNAS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "ACM SIGIR,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "TPAMI,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models", "author": ["J. Huysmans", "K. Dejaeger", "C. Mues", "J. Vanthienen", "B. Baesens"], "venue": "DSS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Do decision biases explain too much", "author": ["G.A. Klein"], "venue": "HFES,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "ICML,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Interpretable classifiers using rules and Bayesian analysis", "author": ["B. Letham", "C. Rudin", "T. McCormick", "D. Madigan"], "venue": "Technical report, University of Washington,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Ranking-order case-based reasoning for financial distress prediction", "author": ["H. Li", "J. Sun"], "venue": "KBSI,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Assessing elaborated hypotheses: An interpretive case-based reasoning approach", "author": ["J.W. Murdock", "D.W. Aha", "L.A. Breslow"], "venue": "ICCBR. Springer,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Human problem solving", "author": ["A. Newell", "H.A. Simon"], "venue": "Prentice-Hall Englewood Cliffs,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1972}, {"title": "GibbsLDA++, AC/C++ implementation of latent dirichlet allocation using gibbs sampling for parameter estimation and inference", "author": ["X. Phan", "C. Nguyen"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Case-based reasoning: A research paradigm", "author": ["S. Slade"], "venue": "AI magazine,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1991}, {"title": "Explanation in case-based reasoning\u2013perspectives and goals", "author": ["F. S\u00f8rmo", "J. Cassens", "A. Aamodt"], "venue": "AI Review,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "JRSS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Methods and models for interpretable linear classification", "author": ["B. Ustun", "C. Rudin"], "venue": "ArXiv,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "The IBP compound dirichlet process and its application to focused topic modeling", "author": ["S. Williamson", "C. Wang", "K. Heller", "D. Blei"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "MedLDA: maximum margin supervised topic models", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "JMLR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "Numerous studies have demonstrated that exemplar-based reasoning, involving various forms of matching and prototyping, is fundamental to our most effective strategies for tactical decisionmaking ([26, 9, 21]).", "startOffset": 196, "endOffset": 207}, {"referenceID": 8, "context": "Numerous studies have demonstrated that exemplar-based reasoning, involving various forms of matching and prototyping, is fundamental to our most effective strategies for tactical decisionmaking ([26, 9, 21]).", "startOffset": 196, "endOffset": 207}, {"referenceID": 20, "context": "Numerous studies have demonstrated that exemplar-based reasoning, involving various forms of matching and prototyping, is fundamental to our most effective strategies for tactical decisionmaking ([26, 9, 21]).", "startOffset": 196, "endOffset": 207}, {"referenceID": 20, "context": "For example, naturalistic studies have shown that skilled decision makers in the fire service use recognition-primed decision making, in which new situations are matched to typical cases where certain actions are appropriate and usually successful [21].", "startOffset": 248, "endOffset": 252}, {"referenceID": 1, "context": "Studies of human decision-making and cognition provided the key inspiration for artificial intelligence Case-Based Reasoning (CBR) approaches [2, 28].", "startOffset": 142, "endOffset": 149}, {"referenceID": 27, "context": "Studies of human decision-making and cognition provided the key inspiration for artificial intelligence Case-Based Reasoning (CBR) approaches [2, 28].", "startOffset": 142, "endOffset": 149}, {"referenceID": 27, "context": "CBR relies on the idea that a new situation can be well-represented by the summarized experience of previously solved problems [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "CBR has been used in important real-world applications [24, 4], but is fundamentally limited, in that it does not learn the underlying complex structure of data in an unsupervised fashion and may not scale to datasets with high-dimensional feature spaces (as discussed in [29]).", "startOffset": 55, "endOffset": 62}, {"referenceID": 3, "context": "CBR has been used in important real-world applications [24, 4], but is fundamentally limited, in that it does not learn the underlying complex structure of data in an unsupervised fashion and may not scale to datasets with high-dimensional feature spaces (as discussed in [29]).", "startOffset": 55, "endOffset": 62}, {"referenceID": 28, "context": "CBR has been used in important real-world applications [24, 4], but is fundamentally limited, in that it does not learn the underlying complex structure of data in an unsupervised fashion and may not scale to datasets with high-dimensional feature spaces (as discussed in [29]).", "startOffset": 272, "endOffset": 276}, {"referenceID": 25, "context": "People organize and interpret information through exemplar-based reasoning, particularly when they are solving problems ([26, 7, 9, 21]).", "startOffset": 121, "endOffset": 135}, {"referenceID": 6, "context": "People organize and interpret information through exemplar-based reasoning, particularly when they are solving problems ([26, 7, 9, 21]).", "startOffset": 121, "endOffset": 135}, {"referenceID": 8, "context": "People organize and interpret information through exemplar-based reasoning, particularly when they are solving problems ([26, 7, 9, 21]).", "startOffset": 121, "endOffset": 135}, {"referenceID": 20, "context": "People organize and interpret information through exemplar-based reasoning, particularly when they are solving problems ([26, 7, 9, 21]).", "startOffset": 121, "endOffset": 135}, {"referenceID": 10, "context": "Studies show that example cases significantly improve user confidence in the resulting solutions, as compared to providing the solution alone or by also displaying a rule that was used to find the solution [11].", "startOffset": 206, "endOffset": 210}, {"referenceID": 28, "context": "Maintaining transparency in complex situations also remains a challenge [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "CBR models designed explicitly to produce explanations [1] rely on the backward chaining of the causal relation from a solution, which does not scale as complexity increases.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "The cognitive load of the user also increases with the complexity of the similarity measure used for comparing cases [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "Other CBR models for explanations require the model to be manually crafted in advance by experts [25].", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "However, this approach does not provide intuitive explanations for the learned clusters (as pointed out in [8]).", "startOffset": 107, "endOffset": 110}, {"referenceID": 31, "context": "Sparse topic models are designed to improve interpretability by reducing the number of words per topic [32, 13].", "startOffset": 103, "endOffset": 111}, {"referenceID": 12, "context": "Sparse topic models are designed to improve interpretability by reducing the number of words per topic [32, 13].", "startOffset": 103, "endOffset": 111}, {"referenceID": 13, "context": "However, using the number of features as a proxy for interpretability is problematic, as sparsity is often not a good or complete measure of interpretability [14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "Even users with technical expertise in machine learning may have a difficult time interpreting such output, especially when the cluster is distributed over a large number of features [14].", "startOffset": 183, "endOffset": 187}, {"referenceID": 29, "context": "In our view, there are at least three widely known types of interpretable models: sparse linear classifiers ([30, 8, 31]); discretization methods, such as decision trees and decision lists (e.", "startOffset": 109, "endOffset": 120}, {"referenceID": 7, "context": "In our view, there are at least three widely known types of interpretable models: sparse linear classifiers ([30, 8, 31]); discretization methods, such as decision trees and decision lists (e.", "startOffset": 109, "endOffset": 120}, {"referenceID": 30, "context": "In our view, there are at least three widely known types of interpretable models: sparse linear classifiers ([30, 8, 31]); discretization methods, such as decision trees and decision lists (e.", "startOffset": 109, "endOffset": 120}, {"referenceID": 11, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 31, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 12, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 22, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 14, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 9, "context": ", nearest neighbors [10] or a supervised optimization-based method [5]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": ", nearest neighbors [10] or a supervised optimization-based method [5]).", "startOffset": 67, "endOffset": 70}, {"referenceID": 13, "context": "(See [14] for a review of interpretable classification.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": ") BCM is intended as the third model type, but uses unsupervised generative mechanisms to explain clusters, rather than supervised approaches [16] or by focusing myopically on neighboring points [3].", "startOffset": 142, "endOffset": 146}, {"referenceID": 2, "context": ") BCM is intended as the third model type, but uses unsupervised generative mechanisms to explain clusters, rather than supervised approaches [16] or by focusing myopically on neighboring points [3].", "startOffset": 195, "endOffset": 198}, {"referenceID": 17, "context": "BCM begins with a standard discrete mixture model [18, 6] to represent the underlying structure of the observations.", "startOffset": 50, "endOffset": 57}, {"referenceID": 5, "context": "BCM begins with a standard discrete mixture model [18, 6] to represent the underlying structure of the observations.", "startOffset": 50, "endOffset": 57}, {"referenceID": 5, "context": "(Note that Latent Dirichlet Allocation (LDA) [6] also begins with a standard mixture model, though our feature values exist in a discrete set that is not necessarily binary.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "BCM works differently to Latent Dirichlet Allocation (LDA) [6], which presents its output in a very different form.", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "We use collapsed Gibbs sampling to perform inference, as this has been observed to converge quickly, particularly in mixture models [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "The derivation is similar to the standard collapsed Gibbs sampling for LDA mixture models [17].", "startOffset": 90, "endOffset": 94}, {"referenceID": 18, "context": "Figure 2: Prediction test accuracy reported for the Handwritten Digit [19] and 20 Newsgroups datasets [22].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "Figure 2: Prediction test accuracy reported for the Handwritten Digit [19] and 20 Newsgroups datasets [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "We validate this through use of two standard datasets: Handwritten Digit [19] and 20 Newsgroups [22].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "We validate this through use of two standard datasets: Handwritten Digit [19] and 20 Newsgroups [22].", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "We use the implementation of LDA available from [27], which incorporates Gibbs sampling, the same inference technique used for BCM.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "In order to compare the prediction accuracy with LDA, the learned cluster labels are provided as features to a support vector machine (SVM) with linear kernel, as is often done in the LDA literature on clustering [6].", "startOffset": 213, "endOffset": 216}, {"referenceID": 32, "context": "01) matches that reported previously for this dataset when using a combined LDA and SVM approach [33].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "(This approach is similar to those used in prior work to measure comprehensibility [20].", "startOffset": 83, "endOffset": 87}], "year": 2015, "abstractText": "We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the \u201cquintessential\u201d observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants\u2019 understanding when using explanations produced by BCM, compared to those given by prior art.", "creator": "LaTeX with hyperref package"}}}