{"id": "1503.01558", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2015", "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision", "abstract": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.", "histories": [["v1", "Thu, 5 Mar 2015 07:07:48 GMT  (1602kb,D)", "https://arxiv.org/abs/1503.01558v1", null], ["v2", "Sun, 8 Mar 2015 04:11:49 GMT  (1602kb,D)", "http://arxiv.org/abs/1503.01558v2", "To appear in NAACL 2015"], ["v3", "Fri, 13 Mar 2015 18:55:22 GMT  (1602kb,D)", "http://arxiv.org/abs/1503.01558v3", "To appear in NAACL 2015"]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.IR", "authors": ["jonathan malmaud", "jonathan huang", "vivek rathod", "nicholas johnston", "andrew rabinovich", "kevin murphy 0002"], "accepted": true, "id": "1503.01558"}, "pdf": {"name": "1503.01558.pdf", "metadata": {"source": "CRF", "title": "What\u2019s Cookin\u2019? Interpreting Cooking Videos using Text, Speech and Vision", "authors": ["Jonathan Malmaud", "Jonathan Huang", "Vivek Rathod", "Nick Johnston", "Andrew Rabinovich"], "emails": ["malmaud@mit.edu", "kpmurphy}@google.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, there have been many successful attempts to build large \"knowledge bases\" (KBs), such as NELL (Carlson et al., 2010), KnowItAll (Etzioni et al., 2011), YAGO (Suchanek et al., 2007), and Google's Knowledge Graph / Vault (Dong et al., 2014). These KBs mostly focus on declarative facts like \"Barack Obama was born in Hawaii.\" But human knowledge also includes procedural information that does not yet fall within the scope of such declarative KBs - instructions and demonstrations on how to dance the tango, for example, or how to change a tire on your car. A KB for organizing and restoring such procedural knowledge could be a valuable resource to help people (and potentially even robots - e.g., (Saxena et al., 2014; Yang et al.)."}, {"heading": "2 Data and pre-processing", "text": "First, we describe how we collected our recipe and video corpus and what pre-processing steps we perform before applying our alignment model. The recipe corpus and alignment model results are available for download at github.com / malmaud / whats _ cookin."}, {"heading": "2.1 Collecting a large corpus of cooking videos with recipes", "text": "We first searched Youtube for videos that were automatically tagged with the freebase mids / m / 01mtb (cooking) and / m / 0p57p (recipe) and that created (automatic) English language transcripts that resulted in a collection of 7.4M videos. Of those videos, we kept the videos that also contained descriptive text and left 6.2M videos.Sometimes the recipe for a video is included in this text description, but sometimes it is stored on an external page. For example, the text description of a video might be: \"Click here for the recipe.\" To find the recipe in such cases, we search for phrases in the video description using the following keywords: \"recipe,\" \"\" steps, \"\" cooking, \"\" method, \"\" method. \"If we find such a recipe, we will find all URLs mentioned in the same sentence and extract the corresponding document."}, {"heading": "2.2 Parsing the recipe text", "text": "For every recipe we use, there is a set of in-house NLP tools, similar to the Stanford Core NLP pipeline. Specifically, we use the parse tree structure to divide each sentence into \"micro-steps.\" Specifically, we split each token that is classified as a connection by the parsers only if they include their parentage in the verbs."}, {"heading": "2.3 Processing the speech transcript", "text": "The output of Youtube's ASR system is a sequence of timestamped tokens produced by a standard Viterbi decoding system. We concatenate these tokens into a single long document and then apply our NLP pipeline to it. Note that in addition to the errors introduced by the ASR system2, the NLP system may have additional errors because it does not work well on text that can be ungrammatic and that is completely free of punctuation and punctuation marks.2 According to (Liao et al., 2013), the Youtube ASR system we use, based on Gaussian mixing models for the acoustic model, has a word error rate of about 52% (averaged across all English-language videos; some genres, such as news, had lower error rates than the newer system, which uses deep neural networks for the acoustic model."}, {"heading": "3 Methods", "text": "In this section, we describe our system for aligning teaching text and video."}, {"heading": "3.1 HMM to align recipe with ASR transcript", "text": "We align each step of the recipe to a corresponding word sequence in the ASR transcript = 1, using the input-output HMM shown in Figure 1. Here, X (1: K) represents the textual recipe steps (obtained using the procedure described in Section 2.2); Y (1: T) represents the ASR tokens (spoken words); R (t): This background variable is required because sometimes sequences of spoken words are not related to the content of the recipe, especially at the beginning and end of a video.The conditional probability distribution (CPDs) for the Markov chain is as follows: p (t) = r \u2032) = r \u2032."}, {"heading": "3.2 Keyword spotting", "text": "A simpler approach to labeling video segments is simply to search for verbs in the ASR transcript and then extract a fixed-size window around the timestamp where the keyword occurred. We call this approach \"keyword spotting.\" A similar method (Yu et al., 2014) filters out ASR transcripts based on speech texts and finds tokens that correspond to a small vocabulary to create a corpus of video clips (extracted from instructional videos), each labeled with an action / object pairing. In detail, we manually define a whitelist of about 200 actions (all transitive verbs) of interest, such as \"add,\" \"\" hack, \"\" hack, \"etc. We then identify when those words are spoken (relying on the POS tags to filter out non-verbs), and extract an 8-second video clip around that timestamp."}, {"heading": "3.3 Hybrid HMM + keyword spotting", "text": "We cannot use keyword spotting if the goal is to align the guide text with videos. However, if our goal is only to create a labeled corpus of video clips, keyword spotting is a reasonable approach. Unfortunately, we found that the quality of the markups generated by keyword spotting (especially the object markups) was not very high due to errors in the ASR. On the other hand, we also noticed that the callback of the HMM approach was about 5 times lower than the use of keyword spotting, and beyond that the temporal localization accuracy was sometimes verbose. To get the best of both worlds, we use the following hybrid technique. We perform keyword spotting for the action in the ASR transcript as before, but use the HMM alignment to determine the corresponding object. To avoid false positives, we only use the output of the HMM transcript for this video, if we are aligned to at least half of the recipe steps in the language line;"}, {"heading": "3.4 Temporal refinement using vision", "text": "In recent years, it has become clear that most people are people who are not able to integrate themselves, and that they are also not able to integrate themselves, \"he told the Deutsche Presse-Agentur.\" I don't think they are able to integrate themselves, \"he told the Deutsche Presse-Agentur.\" But I don't think they are able to integrate themselves, \"he told the Deutsche Presse-Agentur.\" I don't think they are able to integrate themselves, \"he told the Deutsche Presse-Agentur.\" I don't think they are able to integrate themselves, \"he told the Deutsche Presse-Agentur."}, {"heading": "3.5 Quantifying confidence via vision and affordances", "text": "The output of the search terms spotting and / or HMM systems is a linear combination of two quantities (action, object) associated with specific video clips. To estimate how much confidence we have in this label (to balance precision and retrieval), we use a linear combination of two quantities: (1) the final match score generated by visual refinement, which measures the visibility of the object in the given video segment, and (2) an affordability probability, which measures the probability that o is the direct object of a.The affordability model, for example, allows us to prioritize a segment labeled as (peel, garlic) over a segment labeled as (peel, sugar).The probabilities P (object = o | action = a) are estimated by first forming an inverse document frequency matrix that treats action / object events (actions) as documents."}, {"heading": "4 Evaluation and applications", "text": "In this section we evaluate experimentally how well our methods work, and then briefly demonstrate some prototypes."}, {"heading": "4.1 Evaluating the clip database", "text": "This year it will be so far that it will be able to mention the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "4.2 Automatically illustrating a recipe", "text": "A useful by-product of our alignment method is that each recipe step is associated with a segment of the respective video.5 We use a standard keyframe selection algorithm to select the best frame from each segment, and we can then link this box to the corresponding recipe step to automatically illustrate the recipe steps. An illustration of this process is shown in Figure 7."}, {"heading": "4.3 Search within a video", "text": "Another application that our methods allow is to search within a video. For example, if a user wants to find a clip that illustrates how dough is kneaded, we can simply browse through our body of labeled clips.5 The HMM can assign several non-consecutive regions of the video to the same recipe step (since the background state can be turned on and off).In such cases, we simply take the \"convex shell\" of the regions as the interval that corresponds to this step. It is also possible that the HMM does not assign a specific step to any section of the video and returns a list of matches (sorted by confidence).Since each clip has a corresponding \"provenance,\" we can return the results to the user as a series of videos in which we have automatically \"quickly forwarded\" (see Figure 8 for an example), unlike the standard video search on Youtube that returns the entire video but (generally video) does not specify where the search takes place within the user's video."}, {"heading": "5 Related work", "text": "There are several interrelated works. (Yu et al., 2014) performs keyword spotting in the language transcript to highlight clips from instructional videos. However, our hybrid approach is better; the gain is particularly significant for automatically generated language transcripts, as shown in Figure 4.The idea of using an HMM to align instructional steps with a video has also been explored in (Naim et al., 2014). However, their conditional model needs to generate images while ours only generate ASR words, which is an easier task. In addition, they only view 6 videos collected in a controlled laboratory environment while we collect over 180k videos \"in the wild.\" Another paper that processes recipes is (Print and Pang, 2012). They use the HMM to age the steps of a recipe."}, {"heading": "6 Discussion and future work", "text": "In this paper, we have introduced a novel method of aligning text with video, using both speech recognition and visual object recognition. We have used this method to align 180k pairs of recipes and video from which we have extracted a corpus of 1.4 million labeled video clips - a small but critical step in building a multimodal procedural knowledge base. In the future, we hope to use this corpus to train visual action detectors that can then be combined with existing visual object detectors to interpret novel videos. We also believe that the combination of visual and linguistic cues can help overcome long-standing challenges in language understanding, such as the resolution of anaphora and the disambiguation of meaning. We would like to thank Alex Gorban and Anoop Korattikara for their help with some of the experiments and Nancy Chang for their feedback on the paper."}], "references": [{"title": "Food-101 \u2013 mining discriminative components with random forests", "author": ["Bossard et al", "L. 2014] Bossard", "M. Guillaumin", "L. Van Gool"], "venue": "In Proc. European Conf. on Computer Vision", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Toward an architecture for never-ending language learning", "author": ["Carlson et al", "A. 2010] Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.H. Jr.", "T. Mitchell"], "venue": "In Procs. AAAI", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["Chen", "Dolan", "D.L. 2011] Chen", "W.B. Dolan"], "venue": "In Proc. ACL,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Knowledge vault: A webscale approach to probabilistic knowledge fusion", "author": ["Dong et al", "X. 2014] Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "In Proc. of the Int\u2019l Conf. on Knowledge Discovery", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Spice it up?: Mining refinements to online instructions from user generated content", "author": ["Druck", "Pang", "G. 2012] Druck", "B. Pang"], "venue": "In Proc. ACL,", "citeRegEx": "Druck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Druck et al\\.", "year": 2012}, {"title": "Open Information Extraction: the Second Generation", "author": ["Etzioni et al", "O. 2011] Etzioni", "A. Fader", "J. Christensen", "S. Soderland", "Mausam"], "venue": "In Intl. Joint Conf. on AI", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "YouTube2Text: Recognizing and describing arbitrary activities using semantic hierarchies and Zero-Shot recognition", "author": ["Guadarrama et al", "S. 2013] Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia et al", "Y. 2014] Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "The story picturing Engine-A system for automatic text illustration", "author": ["Joshi et al", "D. 2006] Joshi", "J.Z. Wang", "J. Li"], "venue": "ACM Trans. Multimedia Comp., Comm. and Appl.,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription", "author": ["Liao et al", "H. 2013] Liao", "E. McDermott", "A. Senior"], "venue": "In ASRU (IEEE Automatic Speech Recognition and Understanding Work-", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. http://arxiv.org/abs/1301.3781", "author": ["Mikolov et al", "T. 2013] Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Unsupervised alignment of natural language instructions with video segments", "author": ["Naim et al", "I. 2014] Naim", "Y.C. Song", "Q. Liu", "H. Kautz", "J. Luo", "D. Gildea"], "venue": "In Procs. of AAAI", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Petrov et al", "S. 2006] Petrov", "L. Barrett", "R. Thibaux", "D. Klein"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associ-", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "A database for fine grained activity detection of cooking activities", "author": ["Rohrbach et al", "M. 2012a] Rohrbach", "S. Amin", "M. Andriluka", "B. Schiele"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Script data for Attribute-Based recognition of composite activities", "author": ["Rohrbach et al", "M. 2012b] Rohrbach", "M. Regneri", "M. Andriluka", "S. Amin", "M. Pinkal", "B. Schiele"], "venue": "In Proc. European Conf. on Computer Vision,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Russakovsky et al", "O. 2014] Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "RoboBrain: Large-Scale knowledge engine for robots. http://arxiv.org/pdf/1412.0691.pdf", "author": ["Saxena et al", "A. 2014] Saxena", "A. Jain", "O. Sener", "A. Jami", "D.K. Misra", "H.S. Koppula"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for Large-Scale image recognition. http://arxiv.org/abs/1409.1556", "author": ["Simonyan", "Zisserman", "K. 2014] Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "YAGO: A Large Ontology from Wikipedia and WordNet", "author": ["Suchanek et al", "F.M. 2007] Suchanek", "G. Kasneci", "G. Weikum"], "venue": "J. Web Semantics,", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["Thomason et al", "J. 2014] Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R. Mooney"], "venue": "In Intl. Conf. on Comp. Linguistics", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Robot learning manipulation action plans by watching unconstrained videos from the world wide web", "author": ["Yang et al", "Y. 2015] Yang", "Y. Li", "C. Ferm\u00fcller", "Y. Aloimonos"], "venue": "In The Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Grounded language learning from video described with sentences", "author": ["Yu", "Siskind", "H. 2013] Yu", "J. Siskind"], "venue": "In Proc. ACL", "citeRegEx": "Yu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2013}, {"title": "Instructional videos for unsupervised harvesting and learning of action examples", "author": ["Yu et al", "2014] Yu", "S.-I", "L. Jiang", "A. Hauptmann"], "venue": "In Intl. Conf. Multimedia,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.", "creator": "LaTeX with hyperref package"}}}