{"id": "1503.01673", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2015", "title": "High Dimensional Bayesian Optimisation and Bandits via Additive Models", "abstract": "Bayesian Optimisation (BO) is a technique used in optimising a $D$-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been a notoriously difficult problem. Existing literature on the subject are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. In our theoretical analysis we prove that for additive functions the regret has only linear (as opposed to exponential) dependence on $D$ even though the function depends on all $D$ dimensions. We also demonstrate several other statistical and computational benefits in our framework. Empirically via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples when the function is not additive.", "histories": [["v1", "Thu, 5 Mar 2015 15:56:08 GMT  (2541kb,D)", "https://arxiv.org/abs/1503.01673v1", null], ["v2", "Fri, 12 Jun 2015 21:59:15 GMT  (2853kb,D)", "http://arxiv.org/abs/1503.01673v2", "Proceedings of The 32nd International Conference on Machine Learning 2015"], ["v3", "Fri, 13 May 2016 15:31:03 GMT  (2854kb,D)", "http://arxiv.org/abs/1503.01673v3", "Proceedings of The 32nd International Conference on Machine Learning 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["kirthevasan kandasamy", "jeff g schneider", "barnab\u00e1s p\u00f3czos"], "accepted": true, "id": "1503.01673"}, "pdf": {"name": "1503.01673.pdf", "metadata": {"source": "META", "title": "High Dimensional Bayesian Optimisation and Bandits via Additive Models", "authors": ["Kirthevasan Kandasamy", "Jeff Schneider", "Barnab\u00e1s P\u00f3czos"], "emails": ["KANDASAMY@CS.CMU.EDU", "SCHNEIDE@CS.CMU.EDU", "BAPOCZOS@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own."}, {"heading": "2. Related Work", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they are able"}, {"heading": "3. Problem Statement & Set up", "text": "We want a function f: X \u2192 R in which X is a rectangular region in RD = 1 procedure. Let's assume that we (primarily) have a noisy observation y: x (x) +. Let's leave an optimal point x (x) = argmaxx x (x). Let's assume that we can only interact with f by getting a noisy observation y (x) at any x (x) x (x). In the bandit setting, we are interested in the cumulative regret RT = 1 rt = 1 rt (x). We (in the optimization setting) are interested in having the simple regret rt = f (x). In the bandit setting, we are interested in the cumulative regret RT = 1 rt = 1 rt (x).We are interested in the optimization setting that we are interested in the simple regret ST = f (x) \u2212 f (x)."}, {"heading": "4. Algorithm", "text": "Under an additive assumption, our algorithm consists of two components: First, we get the posterior GP for each f (j) of the query pairs up to time t. Then, we maximize a d-dimensional GP-UCB-like capture function for each GP to construct the next query point. Since the optimization of the GP does not depend exponentially on the dimension, this is cheaper than optimizing a capture of the combined GP."}, {"heading": "4.1. Inference on Additive GPs", "text": "In this case, however, we are primarily interested in the distribution of f (j) and Y, whereby the distribution of f (j) and Y can primarily be called (j). (f) The distribution of f (j) and Y is primarily defined as (f), (j), (j), (x), (j), (x), (x), (x), (x), (x), (x), (j), (j), (j), (j), (j), (j), (j), (j), (j), (j), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), c, c, (c), (c), (c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (c, c, c, c, c, c, c,"}, {"heading": "4.3. Main Theoretical Results", "text": "(2010)..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4.4. Practical Considerations", "text": "In our experiments, we select the correct dependence on D, d, and t depending on Theorems 5 and 6.Data: Our analysis assumes that we know the GP kernel of the previous version. In reality, this is rarely the case. In our experiments, we select the hyperparameters of the GP marginal probability (Rasmussen & Williams, 2006) of each Ncyc iteration.Initialization: Marginal probability is based on unreliable data. This is a problem in the first few items."}, {"heading": "5. Experiments", "text": "We compare Add-GP-UCB against GP-UCB, Random Queries (RAND) and DiRect3. On the real data sets we also compare it with the acquisition function (GP-EI) popular in BO applications and the method of Wang et al. (2013), which uses a random projection before applying BO (REMBO). We have several instances of Add-GPUCB for different values for (d, M). For optimization we perform comparisons based on the simple regret ST and the method of Wang et al. (2013), which uses a random projection before applying BO (REMBO). We have several instances of Add-GPUCB for different values for (d, M)."}, {"heading": "5.1. Simulations on Synthetic Data", "text": "First, we demonstrate our technique using a series of synthetic examples. To do this, we construct additive functions for different values for the maximum group size d \"and the number of groups M.\" We use the prime number to distinguish them from Add-GP UCB instances with different combinations of (d, M) values. The d \"dimensional function fd\" is, fd \"(x) = log (0.1 1hd\" d \"exp\"), where v1, v2, v3 are fixed d \"dimensional vectors and hd\" = 0.01d \"0.1. Then we create M\" groups of coordinates by randomly adding d \"coordinates into each group."}, {"heading": "5.2. SDSS Astrophysical Dataset", "text": "The task is to find the maximum probability estimates for a simulation-based astrophysical probability model. Data and software for calculating probability come from Tegmark et al. (2006). The software itself only takes into account 9 parameters, but we expand them to 20 dimensions to emulate the fact that in practical astrophysical problems we may not know the true parameters on which the problem depends, which also allows us to effectively demonstrate the superiority of our methods over alternatives. Each query of this probability function takes about 2-5 seconds. To compete with the wall clock time, we only use 500 ratings for GP-UCB, GP-EI and REMBO and 450 for Add-d / M to maximize the capture function. We have shown that the maximum value achieved over 400 iterations of each algorithm is not sufficient in Figure 5 (a)."}, {"heading": "5.3. Viola & Jones Face Detection", "text": "The Viola & Jones (VJ) Cascade Classifier (Viola & Jones, 2001) is a popular method of face recognition in computer vision based on the Adaboost algorithm. K-Cascade features weak K-classifiers that give a score for a particular image. If we want to classify an image, we pass these images through any classifier. If the image falls below a certain threshold at any point, it is classified as negative. Thresholds at each stage are normally preset based on prior knowledge. There is no reason to believe that these thresholds are optimal. In this experiment, we want to find an optimal score for these thresholds by optimizing classification accuracy through a training set. For this task, we use 1,000 images from the Viola & Jones Facial Data Set, which includes both face and non-face images."}, {"heading": "6. Conclusion", "text": "Recommendations: Based on our experience, we recommend the following: If f is known to be additive, decomposition is known and d is small enough to be efficiently optimized, then the operation of Add-GP-UCB with known decomposition is likely to produce the best results. If not, use a small value for d and run AddGP-UCB while partially optimizing decomposition at regular intervals (Section 4.4). In our experiments, we found that the use of d appeared to be reasonable between 3 and 12 choices. Note, however, that this depends on the calculation budget for optimizing the acquisition, the query budget for f, and up to a certain degree on the function itself. Summary: Our algorithm takes into account several practical considerations in real GPB / BO applications, such as calculation limitations in optimizing the acquisition and the fact that we need to work with relatively few data points because function evaluations are expensive."}, {"heading": "Acknowledgements", "text": "We would like to thank Akshay Krishnamurthy and Andrew Gordon Wilson for their insightful conversations and Andreas Krause, Sham Kakade and Matthias Seeger for their helpful e-mail conversations. This research is partly funded by the DOE grant DESC001114. Our current analysis, specifically Equation 14, has an error. We are working on solving this problem and will publish an update shortly. We would like to thank Felix Berkenkamp and Andreas Krause from ETH Zurich for pointing this out."}, {"heading": "A. Some Auxiliary Material", "text": "We are unable to discern the differences in the choice of. \"Theorem 2\" in (Srinivas et al., 2010).) We are unable to discern the differences in the choice of. \"Theorem 2\" in (Srinivas et al., 2010). (We are unable to discern the differences in the choice of. \"Theorem 2\" in (Srinivas et al., 2010). (We are unable to discern the assumptions of. \"D\" (Srinivas et al., 2010). (We are unable to discern the assumptions of. \"D\"). (Srinivas et al., 2010). (We are unable to discern the assumptions of. \"D\" (Srinivas et al.). (0 \"D.\"). (Srinivas et al., 2010). (Srinivas et al., 2010). \"D.\" (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D. (D). (D)."}, {"heading": "B. Proofs of Results in Section 4.3", "text": "B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B."}], "references": [{"title": "Using Confidence Bounds for Exploitationexploration Trade-offs", "author": ["Auer", "Peter"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Auer and Peter.,? \\Q2003\\E", "shortCiteRegEx": "Auer and Peter.", "year": 2003}, {"title": "Batch Bayesian Optimization via Simulation Matching", "author": ["Azimi", "Javad", "Fern", "Alan", "Xiaoli Z"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Azimi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Azimi et al\\.", "year": 2010}, {"title": "Algorithms for Hyper-Parameter Optimization", "author": ["Bergstra", "James S", "Bardenet", "R\u00e9mi", "Bengio", "Yoshua", "K\u00e9gl", "Bal\u00e1zs"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning", "author": ["Brochu", "Eric", "Cora", "Vlad M", "de Freitas", "Nando"], "venue": null, "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Convergence Rates of Efficient Global Optimization Algorithms", "author": ["Bull", "Adam D"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bull and D.,? \\Q2011\\E", "shortCiteRegEx": "Bull and D.", "year": 2011}, {"title": "Joint Optimization and Variable Selection of High-dimensional Gaussian Processes", "author": ["Chen", "Bo", "Castro", "Rui", "Krause", "Andreas"], "venue": "In Int\u2019l Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations", "author": ["de Freitas", "Nando", "Smola", "Alex J", "Zoghi", "Masrour"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Freitas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Freitas et al\\.", "year": 2012}, {"title": "Learning Where to Attend with Deep Architectures for Image Tracking", "author": ["Denil", "Misha", "Bazzani", "Loris", "Larochelle", "Hugo", "de Freitas", "Nando"], "venue": "Neural Comput.,", "citeRegEx": "Denil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "High-Dimensional Gaussian Process Bandits", "author": ["Djolonga", "Josip", "Krause", "Andreas", "Cevher", "Volkan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Djolonga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Djolonga et al\\.", "year": 2013}, {"title": "Additive gaussian processes", "author": ["Duvenaud", "David K", "Nickisch", "Hannes", "Rasmussen", "Carl Edward"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Duvenaud et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2011}, {"title": "Posterior consistency of Gaussian process prior for nonparametric binary regression", "author": ["Ghosal", "Subhashis", "Roy", "Anindya"], "venue": "Annals of Statistics,", "citeRegEx": "Ghosal et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ghosal et al\\.", "year": 2006}, {"title": "Bayesian Optimization for Synthetic Gene Design", "author": ["Gonzalez", "Javier", "Longworth", "Joseph", "James", "David", "Lawrence", "Neil"], "venue": "In NIPS Workshop on Bayesian Optimization in Academia and Industry,", "citeRegEx": "Gonzalez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gonzalez et al\\.", "year": 2014}, {"title": "A Distribution Free Theory of Nonparametric Regression", "author": ["Gy\u00f6rfi", "L\u00e1szl\u00f3", "Kohler", "Micael", "Krzyzak", "Adam", "Walk", "Harro"], "venue": "Springer Series in Statistics,", "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2002}, {"title": "Generalized Additive Models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Hastie and Tibshirani,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1990}, {"title": "Portfolio Allocation for Bayesian Optimization", "author": ["Hoffman", "Matthew D", "Brochu", "Eric", "de Freitas", "Nando"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Automated Antenna Design with Evolutionary Algorithms", "author": ["G.S. Hornby", "A. Globus", "D.S. Linden", "J.D. Lohn"], "venue": "American Institute of Aeronautics and Astronautics,", "citeRegEx": "Hornby et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hornby et al\\.", "year": 2006}, {"title": "Lipschitzian Optimization Without the Lipschitz Constant", "author": ["D.R. Jones", "C.D. Perttunen", "B.E. Stuckman"], "venue": "J. Optim. Theory Appl.,", "citeRegEx": "Jones et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1993}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["Jones", "Donald R", "Schonlau", "Matthias", "Welch", "William J"], "venue": "J. of Global Optimization,", "citeRegEx": "Jones et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1998}, {"title": "Bayesian Active Learning for Posterior Estimation", "author": ["Kandasamy", "Kirthevasan", "Schneider", "Jeff", "P\u00f3czos", "Barnab\u00e1s"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Kandasamy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kandasamy et al\\.", "year": 2015}, {"title": "Automatic gait optimization with gaussian process regression", "author": ["Lizotte", "Daniel", "Wang", "Tao", "Bowling", "Michael", "Schuurmans", "Dale"], "venue": "In in Proc. of IJCAI,", "citeRegEx": "Lizotte et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lizotte et al\\.", "year": 2007}, {"title": "Active Pointillistic Pattern Search", "author": ["Ma", "Yifei", "Sutherland", "Dougal J", "Garnett", "Roman", "Schneider", "Jeff G"], "venue": "In International Conference on Artificial Intelligence and Statistics, AISTATS,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Adaptive MCMC with Bayesian Optimization", "author": ["Mahendran", "Nimalan", "Wang", "Ziyu", "Hamze", "Firas", "de Freitas", "Nando"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Mahendran et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mahendran et al\\.", "year": 2012}, {"title": "Active Policy Learning for Robot Planning and Exploration under Uncertainty", "author": ["R. Martinez-Cantin", "N. de Freitas", "A. Doucet", "J. Castellanos"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "Martinez.Cantin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Martinez.Cantin et al\\.", "year": 2007}, {"title": "Bayesian approach to global optimization and application to multiobjective and constrained problems", "author": ["J.B. Mockus", "L.J. Mockus"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Mockus and Mockus,? \\Q1991\\E", "shortCiteRegEx": "Mockus and Mockus", "year": 1991}, {"title": "Application of Bayesian approach to numerical methods of global and stochastic optimization", "author": ["Mockus", "Jonas"], "venue": "Journal of Global Optimization,", "citeRegEx": "Mockus and Jonas.,? \\Q1994\\E", "shortCiteRegEx": "Mockus and Jonas.", "year": 1994}, {"title": "Active Learning of Model Evidence Using Bayesian Quadrature", "author": ["M. Osborne", "D. Duvenaud", "R. Garnett", "C. Rasmussen", "S. Roberts", "Z. Ghahramani"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Osborne et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2012}, {"title": "A Bayesian model selection analysis of WMAP3", "author": ["Parkinson", "David", "Mukherjee", "Pia", "Liddle", "Andrew R"], "venue": "Physical Review,", "citeRegEx": "Parkinson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Parkinson et al\\.", "year": 2006}, {"title": "Gaussian Processes for Machine Learning. Adaptative computation and machine learning series", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Sparse Additive Models", "author": ["Ravikumar", "Pradeep", "Lafferty", "John", "Liu", "Han", "Wasserman", "Larry"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Ravikumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2009}, {"title": "Information Consistency of Nonparametric Gaussian Process Methods", "author": ["Seeger", "MW", "Kakade", "SM", "Foster", "DP"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Seeger et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Seeger et al\\.", "year": 2008}, {"title": "Practical Bayesian Optimization of Machine Learning Algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design", "author": ["Srinivas", "Niranjan", "Krause", "Andreas", "Kakade", "Sham", "Seeger", "Matthias"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "Cosmological Constraints from the SDSS Luminous Red Galaxies", "author": ["M. Tegmark et al"], "venue": "Physical Review,", "citeRegEx": "al,? \\Q2006\\E", "shortCiteRegEx": "al", "year": 2006}, {"title": "On the Likelihood that one Unknown Probability Exceeds", "author": ["W.R. Thompson"], "venue": "Another in View of the Evidence of Two Samples. Biometrika,", "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Rapid Object Detection using a Boosted Cascade of Simple Features", "author": ["Viola", "Paul A", "Jones", "Michael J"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Viola et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Viola et al\\.", "year": 2001}, {"title": "Bayesian Optimization in High Dimensions via Random Embeddings", "author": ["Wang", "Ziyu", "Zoghi", "Masrour", "Hutter", "Frank", "Matheson", "David", "de Freitas", "Nando"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures", "author": ["Yamins", "Daniel", "Tax", "David", "Bergstra", "James S"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Yamins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yamins et al\\.", "year": 2013}, {"title": "\u03b7\u22122) and C2 is a constant depending on a, b, D, \u03b4, L and \u03b7. Proof. Srinivas et al. (2010) bound the regret for exact maximisation of the GP-UCB acquisition \u03c6t. By following an analysis similar to our proof of Theorem", "author": ["T + C"], "venue": null, "citeRegEx": "C2.,? \\Q2010\\E", "shortCiteRegEx": "C2.", "year": 2010}, {"title": "The eigenvalues and eigenfunctions for the kernel are defined with respect to a base distribution on X . In the development of Theorem 8, Srinivas et al. (2010) draw nT samples from the uniform distribution on", "author": ["Seeger"], "venue": "X . Hence,", "citeRegEx": "Seeger,? \\Q2008\\E", "shortCiteRegEx": "Seeger", "year": 2008}, {"title": "argue that the uniform distribution still satisfies the required tail constraints and therefore", "author": ["case", "Srinivas"], "venue": null, "citeRegEx": "case and Srinivas,? \\Q2010\\E", "shortCiteRegEx": "case and Srinivas", "year": 2010}], "referenceMentions": [{"referenceID": 30, "context": "Gaussian process bandits and Bayesian optimisation (GPB/ BO) have been successfully applied in many applications such as tuning hyperparameters in learning algorithms (Snoek et al., 2012; Bergstra et al., 2011; Mahendran et al., 2012), robotics (Lizotte et al.", "startOffset": 167, "endOffset": 234}, {"referenceID": 2, "context": "Gaussian process bandits and Bayesian optimisation (GPB/ BO) have been successfully applied in many applications such as tuning hyperparameters in learning algorithms (Snoek et al., 2012; Bergstra et al., 2011; Mahendran et al., 2012), robotics (Lizotte et al.", "startOffset": 167, "endOffset": 234}, {"referenceID": 21, "context": "Gaussian process bandits and Bayesian optimisation (GPB/ BO) have been successfully applied in many applications such as tuning hyperparameters in learning algorithms (Snoek et al., 2012; Bergstra et al., 2011; Mahendran et al., 2012), robotics (Lizotte et al.", "startOffset": 167, "endOffset": 234}, {"referenceID": 19, "context": ", 2012), robotics (Lizotte et al., 2007; Martinez-Cantin et al., 2007) and object tracking (Denil et al.", "startOffset": 18, "endOffset": 70}, {"referenceID": 22, "context": ", 2012), robotics (Lizotte et al., 2007; Martinez-Cantin et al., 2007) and object tracking (Denil et al.", "startOffset": 18, "endOffset": 70}, {"referenceID": 7, "context": ", 2007) and object tracking (Denil et al., 2012).", "startOffset": 28, "endOffset": 48}, {"referenceID": 35, "context": "However, all such successes have been in low (typically < 10) dimensions (Wang et al., 2013).", "startOffset": 73, "endOffset": 92}, {"referenceID": 36, "context": "Expensive high dimensional functions occur in several problems in fields such as computer vision (Yamins et al., 2013), antenna design (Hornby et al.", "startOffset": 97, "endOffset": 118}, {"referenceID": 15, "context": ", 2013), antenna design (Hornby et al., 2006), computational astrophysics (Parkinson et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 26, "context": ", 2006), computational astrophysics (Parkinson et al., 2006) and biology (Gonzalez et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 11, "context": ", 2006) and biology (Gonzalez et al., 2014).", "startOffset": 20, "endOffset": 43}, {"referenceID": 31, "context": "Even current theoretical results suggest that GPB/ BO is exponentially difficult in high dimensions without further assumptions (Srinivas et al., 2010; Bull, 2011).", "startOffset": 128, "endOffset": 163}, {"referenceID": 12, "context": "Nonparametric regression is inherently difficult in high dimensions with known lower bounds depending exponentially in dimension (Gy\u00f6rfi et al., 2002).", "startOffset": 129, "endOffset": 150}, {"referenceID": 25, "context": "GPB/ BO methods follow a family of GP based active learning methods which select the next experiment based on the posterior (Osborne et al., 2012; Ma et al., 2015; Kandasamy et al., 2015).", "startOffset": 124, "endOffset": 187}, {"referenceID": 20, "context": "GPB/ BO methods follow a family of GP based active learning methods which select the next experiment based on the posterior (Osborne et al., 2012; Ma et al., 2015; Kandasamy et al., 2015).", "startOffset": 124, "endOffset": 187}, {"referenceID": 18, "context": "GPB/ BO methods follow a family of GP based active learning methods which select the next experiment based on the posterior (Osborne et al., 2012; Ma et al., 2015; Kandasamy et al., 2015).", "startOffset": 124, "endOffset": 187}, {"referenceID": 17, "context": "In the GPB/ BO setting, common acquisition functions include Expected improvement (Mockus, 1994), probability of improvement (Jones et al., 1998), Thompson sampling (Thompson, 1933) and upper confidence bound (Auer, 2003).", "startOffset": 125, "endOffset": 145}, {"referenceID": 33, "context": ", 1998), Thompson sampling (Thompson, 1933) and upper confidence bound (Auer, 2003).", "startOffset": 27, "endOffset": 43}, {"referenceID": 14, "context": "Some literature studies variants, such as combining several acquisition functions (Hoffman et al., 2011) and querying in batches (Azimi et al.", "startOffset": 82, "endOffset": 104}, {"referenceID": 1, "context": ", 2011) and querying in batches (Azimi et al., 2010).", "startOffset": 32, "endOffset": 52}, {"referenceID": 13, "context": "In the GPB/ BO setting, common acquisition functions include Expected improvement (Mockus, 1994), probability of improvement (Jones et al., 1998), Thompson sampling (Thompson, 1933) and upper confidence bound (Auer, 2003). Of particular interest to us, is the Gaussian process upper confidence bound (GPUCB). It was first proposed and analysed in the noisy setting by Srinivas et al. (2010) and extended to the noiseless case by de Freitas et al.", "startOffset": 126, "endOffset": 391}, {"referenceID": 5, "context": "(2010) and extended to the noiseless case by de Freitas et al. (2012). Some literature studies variants, such as combining several acquisition functions (Hoffman et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 5, "context": "To our knowledge, most literature for GPB/ BO in high dimensions are in the setting where the function varies only along a very low dimensional subspace (Chen et al., 2012; Wang et al., 2013; Djolonga et al., 2013).", "startOffset": 153, "endOffset": 214}, {"referenceID": 35, "context": "To our knowledge, most literature for GPB/ BO in high dimensions are in the setting where the function varies only along a very low dimensional subspace (Chen et al., 2012; Wang et al., 2013; Djolonga et al., 2013).", "startOffset": 153, "endOffset": 214}, {"referenceID": 8, "context": "To our knowledge, most literature for GPB/ BO in high dimensions are in the setting where the function varies only along a very low dimensional subspace (Chen et al., 2012; Wang et al., 2013; Djolonga et al., 2013).", "startOffset": 153, "endOffset": 214}, {"referenceID": 5, "context": "general than the setting in Chen et al. (2012). Even though it does not contain the settings in Djolonga et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 5, "context": "general than the setting in Chen et al. (2012). Even though it does not contain the settings in Djolonga et al. (2013); Wang et al.", "startOffset": 28, "endOffset": 119}, {"referenceID": 5, "context": "general than the setting in Chen et al. (2012). Even though it does not contain the settings in Djolonga et al. (2013); Wang et al. (2013), unlike them, we still allow the function to vary along the entire domain.", "startOffset": 28, "endOffset": 139}, {"referenceID": 30, "context": "Using an additive structure is standard in high dimensional regression literature both in the GP framework and otherwise. Hastie & Tibshirani (1990); Ravikumar et al.", "startOffset": 57, "endOffset": 149}, {"referenceID": 27, "context": "Hastie & Tibshirani (1990); Ravikumar et al. (2009) treat the function as a sum of one dimensional components.", "startOffset": 28, "endOffset": 52}, {"referenceID": 9, "context": "Duvenaud et al. (2011) assume a sum of functions of all combinations of lower dimensional coordinates.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Common techniques to maximise \u03c6t include grid search, Monte Carlo and multistart methods (Brochu et al., 2010).", "startOffset": 89, "endOffset": 110}, {"referenceID": 3, "context": "Common techniques to maximise \u03c6t include grid search, Monte Carlo and multistart methods (Brochu et al., 2010). In our work we use the Dividing Rectangles (DiRect) algorithm of Jones et al. (1993). While these methods are efficient in low dimensions they require exponential computation in high dimensions.", "startOffset": 90, "endOffset": 197}, {"referenceID": 31, "context": "Following Srinivas et al. (2010), we first bound the statistical difficulty of the problem as determined by the kernel.", "startOffset": 10, "endOffset": 33}, {"referenceID": 31, "context": "Srinivas et al. (2010) showed that the statistical difficulty of GPB/ BO is determined by the Maximum Information Gain as defined below.", "startOffset": 0, "endOffset": 23}, {"referenceID": 31, "context": "In contrast, for a D order kernel this is exponential (Srinivas et al., 2010).", "startOffset": 54, "endOffset": 77}, {"referenceID": 29, "context": "We use bounds on the eigenvalues of the SE and Mat\u00e9rn kernels from Seeger et al. (2008) and a result from Srinivas et al.", "startOffset": 67, "endOffset": 88}, {"referenceID": 29, "context": "We use bounds on the eigenvalues of the SE and Mat\u00e9rn kernels from Seeger et al. (2008) and a result from Srinivas et al. (2010) which bounds the information gain via the eigendecay of the kernel.", "startOffset": 67, "endOffset": 129}, {"referenceID": 31, "context": "Part of our proof uses ideas from Srinivas et al. (2010). We show that \u2211 j \u03b2t\u03c3 (j) t\u22121(\u00b7) forms a credible interval for f(\u00b7) about the posterior mean \u03bct(\u00b7) for an additive kernel in Add-GP-UCB.", "startOffset": 34, "endOffset": 57}, {"referenceID": 9, "context": "One could consider alternative lower order kernels \u2013 one candidate is the sum of all possible d order kernels (Duvenaud et al., 2011).", "startOffset": 110, "endOffset": 133}, {"referenceID": 31, "context": "Choice of \u03b2t: \u03b2t as specified by Theorems 5, usually tends to be conservative in practice (Srinivas et al., 2010).", "startOffset": 90, "endOffset": 113}, {"referenceID": 32, "context": "Data dependent prior: Our analysis assumes that we know the GP kernel of the prior. In reality this is rarely the case. In our experiments, we choose the hyperparameters of the kernel by maximising the GP marginal likelihood (Rasmussen & Williams, 2006) every Ncyc iterations. Initialisation: Marginal likelihood based kernel tuning can be unreliable with few data points. This is a problem in the first few iterations. Following the recommendations in Bull (2011) we initialise Add-GP-UCB (and GP-UCB) using Ninit points selected uniformly at random.", "startOffset": 28, "endOffset": 465}, {"referenceID": 3, "context": "Following, Brochu et al. (2010) we use DiRect to maximise \u03c6t, \u03c6\u0303t.", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": "Following, Brochu et al. (2010) we use DiRect to maximise \u03c6t, \u03c6\u0303t. We compare Add-GP-UCB against GP-UCB, random querying (RAND) and DiRect3. On the real datasets we also compare it to the Expected Improvement (GP-EI) acquisition function which is popular in BO applications and the method of Wang et al. (2013) which uses a random projection before applying BO (REMBO).", "startOffset": 11, "endOffset": 311}, {"referenceID": 32, "context": "Here we used Galaxy data from the Sloan Digital Sky Survey (SDSS). The task is to find the maximum likelihood estimators for a simulation based astrophysical likelihood model. Data and software for computing the likelihood are taken from Tegmark et al (2006). The software itself takes in only 9 parameters but we augment this to 20 dimensions to emulate the fact that in practical astrophysical problems we may not know the true parameters on which the problem is dependent.", "startOffset": 14, "endOffset": 259}], "year": 2016, "abstractText": "Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.", "creator": "LaTeX with hyperref package"}}}