{"id": "1503.01838", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2015", "title": "Encoding Source Language with Convolutional Neural Network for Machine Translation", "abstract": "The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) arguments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.01 BLEU points on average", "histories": [["v1", "Fri, 6 Mar 2015 03:04:54 GMT  (343kb,D)", "http://arxiv.org/abs/1503.01838v1", "10 pages plus 2 pages of references"], ["v2", "Mon, 9 Mar 2015 08:28:32 GMT  (343kb,D)", "http://arxiv.org/abs/1503.01838v2", "10 pages plus 2 pages of references"], ["v3", "Tue, 10 Mar 2015 01:34:58 GMT  (343kb,D)", "http://arxiv.org/abs/1503.01838v3", "10 pages plus 2 pages of references"], ["v4", "Fri, 24 Apr 2015 10:07:40 GMT  (343kb,D)", "http://arxiv.org/abs/1503.01838v4", "10 pages plus 2 pages of references, accepted as a full paper at ACL 2015"], ["v5", "Mon, 8 Jun 2015 09:04:14 GMT  (343kb,D)", "http://arxiv.org/abs/1503.01838v5", "Accepted as a full paper at ACL 2015"]], "COMMENTS": "10 pages plus 2 pages of references", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["fandong meng", "zhengdong lu", "mingxuan wang", "hang li", "wenbin jiang", "qun liu"], "accepted": true, "id": "1503.01838"}, "pdf": {"name": "1503.01838.pdf", "metadata": {"source": "CRF", "title": "Encoding Source Language with Convolutional Neural Network for Machine Translation", "authors": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "emails": ["mengfandong@ict.ac.cn", "wangmingxuan@ict.ac.cn", "jiangwenbin@ict.ac.cn", "liuqun@ict.ac.cn", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "Learning a continuous spatial representation of the source language has attracted a lot of attention in both traditional statistical machine translation (SMT) and neural machine translation (NMT). Various models, largely based on neural networks, have been proposed for the representation of the source set, mainly as an encoder part in an encoder frame (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). There has been some relatively recent work on the encoding of only \"relevant\" parts of the source set during the decryption process, especially neural network community models (NNNNJM) in (Devlin et al., 2014) that expand the target language models by including a fixed length window of the source set in order to achieve the performance of the art in machine statistical translation."}, {"heading": "2 Joint Language Model", "text": "Our common model with CNN encoders can be illustrated in Figure 1 (a) & (b), which consists of 1) a CNN encoder, namely tagCNN or inCNN, to represent the information in the source sentences, and 2) an NN-based model to predict the next words, with representations of CNN encoders and the history words in the target sentence as input. In the common language model, the probability of the target word holds en, given previous k targets {en \u2212 k, \u00b7 \u00b7, en \u2212 1}, and the representations of CNN encoders for the source sentence S aretagCNN: p (en | insp1 (S, index of the highlighted word) in CNN: p (en | insp2 (S, h \u2212 n \u2212 1n \u2212 k), {e \u2212 n \u2212 k), and the representations of CNN encoders for the source sentence S aretagCNN: p (S, index of the highlighted word) inCNN (p, index of the word)."}, {"heading": "3 Convolutional Models", "text": "We start with the generic architecture for revolutionary encoders and then move on to dayCNN and inCNN as two extensions."}, {"heading": "3.1 Generic CNN Encoder", "text": "The basic architecture consists of a generic CNN encoder, illustrated in Figure 2 (a), which has a fixed architecture consisting of six layers: Layer-0: the input layer that embeds words in the form of vectors. In our thesis, we set the maximum length of sentences to 40 words. For sentences that are shorter, we put zero padding at the beginning of the sentence. Layer-1: a conversion layer by layer-0, with window size = 3. As discussed in Section 3.2 and 3.3, the guidance signal is injected into this layer for the \"guided version.\" Layer-2: a local gating layer after layer-1, which simply performs a weighted sum via feature maps in a non-adjacent window with size = 2. Layer-3: a conversion layer after layer-2, we perform another conversion with window size = 3. Layer-4: we perform a global gating via feature maps on this layer-3 of the final layer, fully connected to the output of this layer-5."}, {"heading": "3.1.1 Convolution", "text": "As shown in Figure 2 (a), the folding in level-1 works on sliding word windows (width k1), and the similar definition of windows extends to higher levels. Formally, the folding unit for the characteristic map of type-f (under F \"of them) is on level-isz (,\" f) i (x) = \u03c3 (w (, \"f) z (\" \u2212 1) i + b (, \"f)))),\" = 1, 3, f = 1, 2, \u00b7, F \"(1), where \u2022 z (,\" f) i (x) is the output of the characteristic map of type-f for position i in Layer- '; \u2022 w (\", f) is the parameters for f at level-i (,\" f), f (\u00b7) is the sigmoid activation function; 1For an aligned target word, we take its aligned word as the associated word for a > i-associated word, where it belongs to one of many words > to a word (i)."}, {"heading": "3.1.2 Gating", "text": "Previous CNNs, including those for NLP tasks (Hu et al., 2014; Kalchbrenner et al., 2014), have followed a simple strategy of convolution pooling, in which \"fusion\" decisions (e.g. selecting the largest window in max pooling) are based on the values of feature maps. Essentially, this is a soft template matching that works for tasks such as classification, but is detrimental to maintaining the composition functionality of Layer-2, which is critical for modeling sentences. In this paper, we propose to use separate gating unit to remove the score functionality requirement from convolution and let it focus on composition. We take two types of gating: 1) for Layer-2, we take a local gating with non-overlapping windows (size = 2) on the feature maps of Convolutional Layer-1 for the representation of segments, and 2) for global gating, we take one for both, we can use both of the GNs for global representation."}, {"heading": "3.1.3 Training of CNN encoders", "text": "The CNN encoders, including tagCNN and inCNN, discussed below on the right, are trained in a common language model described in Section 2, along with the following parameters \u2022 the embedding of the words in the source and the procedures of words on target; \u2022 the parameters for the DNN of the common language model include the parameters of soft-max for word probability. The training procedure is identical to that of the neural network language except that the parallel corpus is used instead of a monolingual corpus. We try to maximize the log likelihood of training samples, with an example of each target word in the target corpus. Optimization is performed using conventional back propagation, implemented as stochastic gradient descent (LeCun et al., 1998) with mini batches.3.2 tagCNNtagCNN inherits the conversion and gating of generic CNN (as described in Section 3.1), showing only the modification in the input."}, {"heading": "4 Decoding with the Joint Model", "text": "For a hierarchical SMT decoder, we use the integrative method proposed by Devlin et al. (2014). As inherited from the n-gram language model for performing hierarchical decoding, the most leftmost and rightmost n \u2212 1 words of each component should be stored in the state space. We extend the state space by indexing the associated source words for each of these marginal words. For an aligned target word, we take its aligned source words as associated source words. And for an unaligned word, we use the affiliation euristics applied by Devlin et al. (2014). In this essay, we integrate the common model into the state-of-the-art dependency model and then translate the T-string as a case study of the effectiveness of our proposed approaches."}, {"heading": "4.1 Dependency-to-String Translation", "text": "In this paper, we use a state-of-the-art Dep2Str decoder (Xie et al., 2011), which is also a hierarchical decoder. This Dep2Str model uses rules that represent the source page as a header-dependent relationship and the destination page as strings. A header-dependent relationship (HDR) consists of a header and all its dependencies in dependency trees. Figure 3 shows a dependency tree (a) with three HDRs (in the shadow), an example of the HDR rule (b) for the top level of (a) and an example of a header rule (c). HDR rules are constructed from header-dependent relationships. HDR rules can function as both translation rules and reordering rules. And header rules are used for the translation of source words. We adopt the Dep2Str decoder Dep2Stang translation elements proposed by Meng et al. (2013), which are easiest to implement in 2007 (the HDR)."}, {"heading": "4.2 MT Decoder", "text": "According to Och and Ney (2002), we use a general loglinear framework. Let d be a derivative that converts a source dependency tree into a target string e. The probability of d is defined as: P (d), p (t), p (s), p (s), p (s | t) of the HDR rules; \u2022 lexical translation probabilities PLEX (t | s) and PLEX (s | t) of the HDR rules; \u2022 standard penalty exp (\u2212 1); \u2022 pseudo-translation rules exp (\u2212 1); \u2022 target word error exp (| e |); \u2022 n-gram language model PLM (e) and PLEX (s | t) of the HDR rules; proposed features: \u2022 n-gram tagged CNN community language model PTLM (\u2212 e); \u2022 n-gram interval language exp (\u2212 e | t); \u2022 target word error exp (PLM); (PLM); all language model HM-gram (n) (s)."}, {"heading": "5 Experiments", "text": "The experiments in this section are designed to answer the following questions: 1. Can our common language models day CNN and CNN improve translation quality and complement each other? 2. Do CNN and tagCNN benefit from their guiding signal compared to a generic CNN? 3. Is it helpful for dayCNN to embed more dependency structures, such as the head of dependence on each connected word, than additional information?"}, {"heading": "5.1 Setup", "text": "The bilingual training data consists of 221K sentence pairs containing 5.0 million Chinese words and 6.8 million English words. The development set is NIST MT03 (795 sentences) and the test sets are MT04 (1499 sentences) and MT05 (917 sentences) after filtering with length restriction. Pre-processing: Word alignments are achieved with GIZA + + (Och and Ney, 2003) at the companies in both directions using the \"Grow-Diag-Final-and-Balance-Strategy\" (Koehn et al., 2003). We adopt the SRI Language Modeling Toolkit (Stolcke et al., 2002) to derive a 4 gram language model with modified Kneser-Ney vocabulary."}, {"heading": "5.2 Setting for Model Comparisons", "text": "We use the common language models tagCNN and inCNN as additional decoding functions for a dependency-to-string base system (Dep2Str) and compare them with the common model of neural networks with 11 source context words (Devlin et al., 2014). We use the implementation of an open source toolkit4 with standard configuration except for the global settings described in Section 5.1. Since our tagCNN and inCNN models are source-to-target and links-to-right (on the landing page), we compare the source-to-target and links-to-right type NNNJM in (Devlin et al., 2014). We call this type NNJM in the following BBN-JM. Although BBN-JM in (Devlin et al., 2014) was originally tested in the hierarchical phrase base (Chiang et al., 2007) SMT and strings-to-dependency (Sal), it is relatively versatile, SMT has been tested in 2008 and SMT."}, {"heading": "5.3 The Main Results", "text": "The most important results of the different models are given in Table 1, LDC2003E07, DC2003E14, D200307. Before proceeding to a more detailed comparison, we first note that \u2022 the baseline Dep2Str system BLEU is 0.5 + higher than the open source phrase system Moses (Koehn et al., 2007); \u2022 BBN-JM can give more than 0.92 BLEU points over Dep2Str, a result similar to that given in (Devlin et al., 2014). Significantly from Table 1, tagCNN and inCNN improve the Dep2Str baseline by + 1.28 and + 1.63 BLEU words that exceed BBN-JM in the same setting by + 0.36 and + 0.71 BLEU points respectively, indicating that tagCNN and inCNN can provide individually discriminatory information in the Dep2Str baseline. It is more noteworthy that inCNN-JM exceeds the same BLEU setting by more than + 0.71 BLEU points according to The corporative04, and IST + 0.71."}, {"heading": "6 Related Work", "text": "The groundbreaking work of the Neural Network Language Model (NNLM) can therefore be traced back to Bengio et al. (2003) to monolingual text. It was recently expanded by Devlin et al. (2014) to include an additional source context (11 source words) in the modeling of the target sentence that is clearly most closely related to our work, with two important differences: 1) instead of the ad hoc method of selecting a context window in (Devlin et al., 2014), our model covers the entire source set and automatically distills the context relevant to the target modeling; 2) our revolutionary architecture can effectively utilize guidance signals of vastly different shapes and textures of the target. Prior to our model, there is also work to represent source sets with neural networks, including RNN (Cho et al., 2014; Sutskever et al al., 2014) and CNN (Kalchbrenner and Blunsom, 2013)."}, {"heading": "7 Conclusion and Future Work", "text": "We proposed revolutionary architectures to obtain a guided representation of the entire source set, which can be used to complement the n-gram target language model. Using different guidance signals from the target side, we are designing tagCNN and inCNN, both of which are being tested to improve dependency-to-string SMT with + 1.93 BLEU points above the baseline and + 1.01 BLEU points above the state of the art in (Devlin et al., 2014). For future work, we will consider encoding more complex language structures to further improve the common model."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rjean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal OF Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "Chiang.,? \\Q2007\\E", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["Philipp Koehn", "Ivona Ku\u010derov\u00e1"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Collins et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "What\u2019s in a translation rule", "author": ["Galley et al.2004] Michel Galley", "Mark Hopkins", "Kevin Knight", "Daniel Marcu"], "venue": "In Proceedings of HLT/NAACL,", "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Forest rescoring: Faster decoding with integrated language models", "author": ["Huang", "Chiang2007] Liang Huang", "David Chiang"], "venue": "In Annual Meeting-Association For Computational Linguistics,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A convolutional neural network for modelling", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Translation with source constituency and dependency trees", "author": ["Meng et al.2013] Fandong Meng", "Jun Xie", "Linfeng Song", "Yajuan L\u00fc", "Qun Liu"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Meng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2013}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Och", "Ney2002] Franz Josef Och", "Hermann Ney"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Och et al\\.", "year": 2002}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "A new string-to-dependency machine translation algorithm with a target dependency language model", "author": ["Shen et al.2008] Libin Shen", "Jinxi Xu", "Ralph Weischedel"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Shen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2008}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings of the international conference on spoken language processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Sequence to sequence learning with neural networks. CoRR, abs/1409.3215", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A novel dependency-to-string model for statistical machine translation", "author": ["Xie et al.2011] Jun Xie", "Haitao Mi", "Qun Liu"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Xie et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) arguments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT.", "startOffset": 56, "endOffset": 77}, {"referenceID": 2, "context": "Various models, mostly neural network-based, have been proposed for representing the source sentence, mainly as the encoder part in an encoder-decoder framework (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 161, "endOffset": 275}, {"referenceID": 0, "context": "Various models, mostly neural network-based, have been proposed for representing the source sentence, mainly as the encoder part in an encoder-decoder framework (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 161, "endOffset": 275}, {"referenceID": 4, "context": "Various models, mostly neural network-based, have been proposed for representing the source sentence, mainly as the encoder part in an encoder-decoder framework (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 161, "endOffset": 275}, {"referenceID": 20, "context": "Various models, mostly neural network-based, have been proposed for representing the source sentence, mainly as the encoder part in an encoder-decoder framework (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 161, "endOffset": 275}, {"referenceID": 6, "context": "There has been some quite recent work on encoding only \u201crelevant\u201d part of source sentence during the decoding process, most notably neural network joint model (NNJM) in (Devlin et al., 2014), which extends the n-grams target language model by additionally taking a fixed-length window of source sentence, achieving state-ofthe-art performance in statistical machine translation.", "startOffset": 169, "endOffset": 190}, {"referenceID": 21, "context": "We integrate the proposed joint models into a state-of-the-art dependency-to-string translation system (Xie et al., 2011) to evaluate their effectiveness.", "startOffset": 103, "endOffset": 121}, {"referenceID": 6, "context": "Our model also outperforms Devlin et al. (2014)\u2019s NNJM by up to +1.", "startOffset": 27, "endOffset": 48}, {"referenceID": 6, "context": "And for an unaligned word, we inherit its affiliation from the closest aligned word, with preference given to the right (Devlin et al., 2014).", "startOffset": 120, "endOffset": 141}, {"referenceID": 8, "context": "2 Gating Previous CNNs, including those for NLP tasks (Hu et al., 2014; Kalchbrenner et al., 2014), take a straightforward convolution-pooling strategy, in which the \u201cfusion\u201d decisions (e.", "startOffset": 54, "endOffset": 98}, {"referenceID": 11, "context": "2 Gating Previous CNNs, including those for NLP tasks (Hu et al., 2014; Kalchbrenner et al., 2014), take a straightforward convolution-pooling strategy, in which the \u201cfusion\u201d decisions (e.", "startOffset": 54, "endOffset": 98}, {"referenceID": 1, "context": "This is essentially a particular case of attention model, analogous to the automatic alignment mechanism in (Bahdanau et al., 2014), where the attention signal is from the state of a generative recurrent neural network (RNN) as decoder.", "startOffset": 108, "endOffset": 131}, {"referenceID": 6, "context": "For a hierarchical SMT decoder, we adopt the integrating method proposed by Devlin et al. (2014). As inherited from the n-gram language model for performing hierarchical decoding, the leftmost and rightmost n\u2212 1 words from each constituent should be stored in the state space.", "startOffset": 76, "endOffset": 97}, {"referenceID": 6, "context": "For a hierarchical SMT decoder, we adopt the integrating method proposed by Devlin et al. (2014). As inherited from the n-gram language model for performing hierarchical decoding, the leftmost and rightmost n\u2212 1 words from each constituent should be stored in the state space. We extend the state space to also include the indexes of the affiliated source words for each of these edge words. For an aligned target word, we take its aligned source words as its affiliated source words. And for an unaligned word, we use the affiliation heuristic adopted by Devlin et al. (2014). In this paper, we integrate the joint model into the state-of-the-art dependency-to-string machine translation decoder as a case study to test the efficacy of our proposed approaches.", "startOffset": 76, "endOffset": 577}, {"referenceID": 21, "context": "1 Dependency-to-String Translation In this paper, we use a state-of-the-art dependency-to-string (Xie et al., 2011) decoder (Dep2Str), which is also a hierarchical decoder.", "startOffset": 97, "endOffset": 115}, {"referenceID": 14, "context": "We adopt the decoder proposed by Meng et al. (2013) as a variant of Dep2Str translation that", "startOffset": 33, "endOffset": 52}, {"referenceID": 7, "context": "Basically they extract the HDR rules with GHKM (Galley et al., 2004) algorithm.", "startOffset": 47, "endOffset": 68}, {"referenceID": 3, "context": "The bottom-up chart-based decoding algorithm with cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items for each node.", "startOffset": 63, "endOffset": 101}, {"referenceID": 17, "context": "2 MT Decoder Following Och and Ney (2002), we use a general loglinear framework.", "startOffset": 23, "endOffset": 42}, {"referenceID": 17, "context": "The weights of all these features are tuned via minimum error rate training (MERT) (Och, 2003).", "startOffset": 83, "endOffset": 94}, {"referenceID": 12, "context": "Preprocessing: The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy (Koehn et al., 2003).", "startOffset": 164, "endOffset": 184}, {"referenceID": 5, "context": "Metric: We use the case-insensitive 4-gram NIST BLEU3 as our evaluation metric, with statistical significance test with sign-test (Collins et al., 2005) between the proposed models and two baselines.", "startOffset": 130, "endOffset": 152}, {"referenceID": 6, "context": "2 Setting for Model Comparisons We use the tagCNN and inCNN joint language models as additional decoding features to a dependency-to-string baseline system (Dep2Str), and compare them to the neural network joint model with 11 source context words (Devlin et al., 2014).", "startOffset": 247, "endOffset": 268}, {"referenceID": 6, "context": "Since our tagCNN and inCNN models are source-to-target and left-to-right (on target side), we only take the source-to-target and left-to-right type NNJM in (Devlin et al., 2014) in comparison.", "startOffset": 156, "endOffset": 177}, {"referenceID": 6, "context": "Although the BBN-JM in (Devlin et al., 2014) is originally tested in the hierarchical phrase-based (Chiang, 2007) SMT and string-to-dependency (Shen et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 3, "context": ", 2014) is originally tested in the hierarchical phrase-based (Chiang, 2007) SMT and string-to-dependency (Shen et al.", "startOffset": 62, "endOffset": 76}, {"referenceID": 18, "context": ", 2014) is originally tested in the hierarchical phrase-based (Chiang, 2007) SMT and string-to-dependency (Shen et al., 2008) SMT, it is fairly versatile and can be readily integrated into Dep2Str.", "startOffset": 106, "endOffset": 125}, {"referenceID": 13, "context": "5+ higher than the open-source phrase-based system Moses (Koehn et al., 2007); \u2022 BBN-JM can give about +0.", "startOffset": 57, "endOffset": 77}, {"referenceID": 6, "context": "92 BLEU score over Dep2Str, a result similar as reported in (Devlin et al., 2014).", "startOffset": 60, "endOffset": 81}, {"referenceID": 6, "context": "57 + BBN-JM (Devlin et al., 2014) 36.", "startOffset": 12, "endOffset": 33}, {"referenceID": 6, "context": "(2014) to include additional source context (11 source words) in modeling the target sentence, which is clearly most related to our work, with however two important differences: 1) instead of the ad hoc way of selecting a context window in (Devlin et al., 2014), our model covers the entire source sentence and automatically distill the context relevant for target modeling; 2) our convolutional architecture can effectively leverage guiding signals of vastly different forms and nature from the target.", "startOffset": 240, "endOffset": 261}, {"referenceID": 4, "context": "Prior to our model there is also work on representing source sentences with neural networks, including RNN (Cho et al., 2014; Sutskever et al., 2014) and CNN (Kalchbrenner and Blunsom, 2013).", "startOffset": 107, "endOffset": 149}, {"referenceID": 20, "context": "Prior to our model there is also work on representing source sentences with neural networks, including RNN (Cho et al., 2014; Sutskever et al., 2014) and CNN (Kalchbrenner and Blunsom, 2013).", "startOffset": 107, "endOffset": 149}, {"referenceID": 1, "context": "Our model, especially inCNN, is inspired by is the automatic alignment model proposed in (Bahdanau et al., 2014).", "startOffset": 89, "endOffset": 112}, {"referenceID": 1, "context": "6 Related Work The seminal work of neural network language model (NNLM) can be traced to Bengio et al. (2003) on monolingual text.", "startOffset": 89, "endOffset": 110}, {"referenceID": 1, "context": "6 Related Work The seminal work of neural network language model (NNLM) can be traced to Bengio et al. (2003) on monolingual text. It is recently extended by Devlin et al. (2014) to include additional source context (11 source words) in modeling the target sentence, which is clearly most related to our work, with however two important differences: 1) instead of the ad hoc way of selecting a context window in (Devlin et al.", "startOffset": 89, "endOffset": 179}], "year": 2015, "abstractText": "The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) arguments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.01 BLEU points on average.", "creator": "LaTeX with hyperref package"}}}