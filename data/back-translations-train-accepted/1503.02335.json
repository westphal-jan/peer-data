{"id": "1503.02335", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2015", "title": "An Unsupervised Method for Uncovering Morphological Chains", "abstract": "Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.", "histories": [["v1", "Sun, 8 Mar 2015 22:18:30 GMT  (120kb,D)", "http://arxiv.org/abs/1503.02335v1", "11 pages, Appearing in the Transactions of the Association for Computational Linguistics (TACL), 2015"]], "COMMENTS": "11 pages, Appearing in the Transactions of the Association for Computational Linguistics (TACL), 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karthik narasimhan", "regina barzilay", "tommi jaakkola"], "accepted": true, "id": "1503.02335"}, "pdf": {"name": "1503.02335.pdf", "metadata": {"source": "CRF", "title": "An Unsupervised Method for Uncovering Morphological Chains", "authors": ["Karthik Narasimhan", "Regina Barzilay", "Tommi Jaakkola"], "emails": ["tommi}@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "2 Related Work", "text": "In recent years, it has been shown that people are able to survive themselves by setting out in search of new paths that they take in order to travel the world. (...) In recent years, it has been shown that people are able to survive themselves. (...) In recent years, it has been shown that people who are able to travel the world. (...) In the last ten years, the world has changed. (...) In the last ten years, the world has changed. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.). (...). (...). (.). (...). (.). (.). (...). (.). (.). (.). (.).). (.). (.).). (.).). (.).). (.).). (.).). The future. (. (. (. (.). (.). (.).). (.). (.). (.).).). (.). (.). (. (.).).). (.).). (.). (. (.).). The future. (.). (. (.). (.). (.). (. (.). (.).). (.). (.). (.).). The future. The future. (. (. (. (. (.). (.). (.). (. (.).). (.).). (.).). (. (.).). (.). (.). (.).). The future.). (. (. (.). (.). ("}, {"heading": "3 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Definitions and Framework", "text": "We use morphological chains as part of another chain that ends in language. These chains are treated separately from each other, but the common parts for statistical support. A morphological chain is a short sequence of words that start from the keyword and end in a morphological variant. Each node in the chain is, depending on assumption, a valid word. We refer to the word that is morphologically changed as a parent word and its morphological variant as a child word. A word that has no morphological parents is a keyword (e.g. words like game, chat, run).2Words in a chain (other than the keyword) are created by their parents by adding morphemes (prefixes, suffixes or other words). For example, a morphological chain that could be international in the word. The keyword for this chain is nation. The same word can belong to several chains."}, {"heading": "3.2 Features", "text": "In fact, it is the case that most of them are in a position to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are, in which they"}, {"heading": "3.3 Learning", "text": "We assume that we have an uncommented word list D for this purpose. (A typical approach to learning such a model would be to maximize the probability of all observed words in D beyond the space of all observed words. (D) We assume that we have an uncommented word list D for this purpose. (D) We can maximize the probability of all observed words in D beyond the space of all possible terms. (D) We can use the terms in D (W) and D (W) in other words to maximize maximizeL (D) to maximizeL (D) to maximizeL (W)."}, {"heading": "3.4 Prediction", "text": "In each step, we use the learned weights to predict the best parent for the current word (from the group of candidates), or we decide to stop the current word and declare it as a base word if the stop case has the highest number of points. Once we have the chain, we can derive a morphological segmentation by inserting a segment point (in the test word) corresponding to each edge in the chain. Algorithms 1 and 2 provide details of the prediction method STOP. In both algorithms, type STOP refers to the type of modification (or deficiency) that the parent goes through: prefix / suffix addition, types of transformation such as repetition, deletion, modification or the stop case.STOP 1 method for predicting a parent for a word1: method PREDICT (word) 2: Candidates \u2190 CANDIDATES (word) 3: Best grade: Top grade:, top grade (5): STAID::: STAID:::: STAID::::: STAID:::::: STOP:: STOP:: STOP: STOP:: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP:: STOP: STOP: STOP: STOP:: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP: STOP:: STOP: STOP: STOP: STOP: STOP: STOP::: STOP: STOP: STOP:: STOP: STOP: STOP: STOP: STOP:"}, {"heading": "4 Experimental Setup", "text": "We conduct experiments in three different languages: English, Turkish and Arabic. For each language, we use corpora for the training, testing and learning of word vectors. Training data consists of an uncommented word list with frequency information, while test data have a series of gold morphological segmentations. We test our model for morphological segmentation (Mikolov et al., 2013) on large text corpora and obtain 200-dimensional vectors for all three languages. The evaluation is done by assigning morphological segmentation tasks. We evaluate performance on individual segmentation sites related to segmentation."}, {"heading": "5 Results", "text": "We can see that our method uses both variants of Morfessor, 11The grammars have been trained using data we have provided them.12We report numbers in Arabic directly from their paper patterns. AGMorph is no better than Morfessor in English and Arabic, but does very well in Turkish (60.9% F1 compared to our model 61.2%).This could be due to the fact that compounding grammar is well suited for agglutinative morphology in Turkish and hence to make more profits than for English and Arabic. Lee Segmenter (M2) performs the best languages in Arabic (82% F1), but lags behind English and Turkish. This is consistent with the fact that the system has been optimized. We have more gains than for English and Arabic."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed a discriminatory model of unattended morphological segmentation that seamlessly integrates the orthographic and semantic properties of words. We use morphological chains to model the word-building process, and demonstrate how to use the flexibility of log-linear models to integrate both morphemous and word characteristics while managing parent word transformations. Our model consistently matches or exceeds five state-of-the-art systems in Arabic, English, and Turkish. Future directions include the use of better neighborhood functions for contrasting estimates, exploring other views of the data that could be incorporated, studying better prediction schemes, and using morphological chains in other applications in the NLP."}, {"heading": "Acknowledgements", "text": "We thank Kairit Sirts and Yoong Keok Lee for their help in conducting experiments with their unattended morphology analyzers and Yonatan Belinkov for their help in error analysis in Arabic. We also thank the anonymous TACL reviewers and members of MIT's NLP group for their insightful comments and suggestions. This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) through the U.S. Army Research Laboratory of the Department of Defense under contract number W911NF-12C-0013. The U.S. government is authorized to reproduce and distribute reprints for government purposes, regardless of the copyright notice contained therein. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements of the IARPA, DoD / ARL or the U.S. government."}], "references": [{"title": "CELEX2 LDC96L14", "author": ["R Baayen", "R Piepenbrock", "L Gulikers."], "venue": "Philadelphia: Linguistic Data Consortium.", "citeRegEx": "Baayen et al\\.,? 1995", "shortCiteRegEx": "Baayen et al\\.", "year": 1995}, {"title": "Unsupervised discovery of morphologically related words based on orthographic and semantic similarity", "author": ["Marco Baroni", "Johannes Matiasek", "Harald Trost."], "venue": "CoRR, cs.CL/0205006.", "citeRegEx": "Baroni et al\\.,? 2002", "shortCiteRegEx": "Baroni et al\\.", "year": 2002}, {"title": "A limited memory algorithm for bound constrained optimization", "author": ["Richard H Byrd", "Peihuang Lu", "Jorge Nocedal", "Ciyou Zhu."], "venue": "SIAM Journal on Scientific Computing, 16(5):1190\u20131208.", "citeRegEx": "Byrd et al\\.,? 1995", "shortCiteRegEx": "Byrd et al\\.", "year": 1995}, {"title": "Inducing the morphological lexicon of a natural language from unannotated text", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR), pages", "citeRegEx": "Creutz and Lagus.,? 2005", "shortCiteRegEx": "Creutz and Lagus.", "year": 2005}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "ACM Trans. Speech Lang. Process., 4(1):3:1\u20133:34, February.", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "Discovering morphological paradigms from plain text using a dirichlet process mixture model", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 616\u2013627. Association for", "citeRegEx": "Dreyer and Eisner.,? 2011", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2011}, {"title": "Priors in bayesian learning of phonological rules", "author": ["Sharon Goldwater", "Mark Johnson."], "venue": "Proceedings of the 7th Meeting of the ACL Special Interest Group in Computational Phonology: Current Themes in Computational Phonology and Morphology, SIG-", "citeRegEx": "Goldwater and Johnson.,? 2004", "shortCiteRegEx": "Goldwater and Johnson.", "year": 2004}, {"title": "Modeling syntactic context improves morphological segmentation", "author": ["Yoong Keok Lee", "Aria Haghighi", "Regina Barzilay."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL \u201911, pages 1\u20139, Stroudsburg,", "citeRegEx": "Lee et al\\.,? 2011", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "CoNLL, Sofia, Bulgaria.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Arabic Treebank: Part 1 v 2.0 LDC2003T06. Philadelphia: Linguistic Data Consortium", "author": ["Mohamed Maamouri", "Ann Bies", "Hubert Jin", "Tim Buckwalter"], "venue": null, "citeRegEx": "Maamouri et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Maamouri et al\\.", "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1310.4546.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Arabic Gigaword fifth edition LDC2011T11", "author": ["Robert Parker", "David Graff", "Ke Chen", "Junbo Kong", "Kazuaki Maeda."], "venue": "Philadelphia: Linguistic Data Consortium.", "citeRegEx": "Parker et al\\.,? 2011", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["Hoifung Poon", "Colin Cherry", "Kristina Toutanova."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for", "citeRegEx": "Poon et al\\.,? 2009", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Turkish language resources: Morphological parser, morphological disambiguator and web corpus", "author": ["Ha\u015fim Sak", "Tunga G\u00fcng\u00f6r", "Murat Sara\u00e7lar."], "venue": "Advances in natural language processing, pages 417\u2013 427. Springer.", "citeRegEx": "Sak et al\\.,? 2008", "shortCiteRegEx": "Sak et al\\.", "year": 2008}, {"title": "Knowledgefree induction of morphology using latent semantic analysis", "author": ["Patrick Schone", "Daniel Jurafsky."], "venue": "Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning - Vol-", "citeRegEx": "Schone and Jurafsky.,? 2000", "shortCiteRegEx": "Schone and Jurafsky.", "year": 2000}, {"title": "Minimallysupervised morphological segmentation using adaptor grammars", "author": ["Kairit Sirts", "Sharon Goldwater."], "venue": "TACL, 1:255\u2013266.", "citeRegEx": "Sirts and Goldwater.,? 2013", "shortCiteRegEx": "Sirts and Goldwater.", "year": 2013}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Noah A. Smith", "Jason Eisner."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL \u201905, pages 354\u2013362, Stroudsburg, PA, USA. Association", "citeRegEx": "Smith and Eisner.,? 2005", "shortCiteRegEx": "Smith and Eisner.", "year": 2005}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "The Annual Conference of the Association for Computational Linguistics.", "citeRegEx": "Snyder and Barzilay.,? 2008", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2008}, {"title": "Unsupervised morphology rivals supervised morphology for arabic mt", "author": ["David Stallard", "Jacob Devlin", "Michael Kayser", "Yoong Keok Lee", "Regina Barzilay."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational", "citeRegEx": "Stallard et al\\.,? 2012", "shortCiteRegEx": "Stallard et al\\.", "year": 2012}, {"title": "Empirical comparison of evaluation methods for unsupervised learning of morphology", "author": ["Sami Virpioja", "Ville T. Turunen", "Sebastian Spiegler", "Oskar Kohonen", "Mikko Kurimo."], "venue": "TAL, 52(2):45\u201390.", "citeRegEx": "Virpioja et al\\.,? 2011", "shortCiteRegEx": "Virpioja et al\\.", "year": 2011}, {"title": "Morfessor 2.0: Python implementation and extensions for Morfessor Baseline. Report in Aalto University publication series SCIENCE + TECHNOLOGY, Department of Signal Processing", "author": ["Sami Virpioja", "Peter Smit", "Stig-Arne Gr\u00f6nroos", "Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009).", "startOffset": 154, "endOffset": 253}, {"referenceID": 4, "context": "Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009).", "startOffset": 154, "endOffset": 253}, {"referenceID": 17, "context": "Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009).", "startOffset": 154, "endOffset": 253}, {"referenceID": 12, "context": "Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009).", "startOffset": 154, "endOffset": 253}, {"referenceID": 14, "context": "In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002).", "startOffset": 124, "endOffset": 172}, {"referenceID": 1, "context": "In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002).", "startOffset": 124, "endOffset": 172}, {"referenceID": 16, "context": "We use Contrastive Estimation (Smith and Eisner, 2005) to efficiently learn this model in an unsupervised manner.", "startOffset": 30, "endOffset": 54}, {"referenceID": 20, "context": "We compare our performance against five state-of-the-art unsupervised systems: Morfessor Baseline (Virpioja et al., 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al.", "startOffset": 98, "endOffset": 121}, {"referenceID": 3, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al.", "startOffset": 26, "endOffset": 50}, {"referenceID": 15, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al.", "startOffset": 60, "endOffset": 87}, {"referenceID": 7, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al., 2011; Stallard et al., 2012) and the system of Poon et al.", "startOffset": 107, "endOffset": 148}, {"referenceID": 18, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al., 2011; Stallard et al., 2012) and the system of Poon et al.", "startOffset": 107, "endOffset": 148}, {"referenceID": 3, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al., 2011; Stallard et al., 2012) and the system of Poon et al. (2009). Our model consistently equals or outperforms these sys-", "startOffset": 27, "endOffset": 186}, {"referenceID": 13, "context": "Schone and Jurafsky (2000) employ an LSAbased similarity measure to identify morphological variants from a list of orthographically close word pairs.", "startOffset": 0, "endOffset": 27}, {"referenceID": 1, "context": "Based on similar intuition, Baroni et al. (2002) design a method that integrates these sources of information, captured as two word pair lists, ranked based on edit distance and mutual information.", "startOffset": 28, "endOffset": 49}, {"referenceID": 12, "context": "Our work also relates to the log-linear model for morphological segmentation developed by Poon et al. (2009). They propose a joint model over all", "startOffset": 90, "endOffset": 109}, {"referenceID": 15, "context": "Most recently, work by Sirts and Goldwater (2013) uses Adaptor Grammars for minimally supervised segmentation.", "startOffset": 23, "endOffset": 50}, {"referenceID": 5, "context": "In other related work, Dreyer and Eisner (2011) tackle the problem of recovering morphological paradigms and inflectional principles.", "startOffset": 23, "endOffset": 48}, {"referenceID": 7, "context": "Lee et al. (2011) present a model that takes advantage of syntactic context to perform better morphological segmentation.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Lee et al. (2011) present a model that takes advantage of syntactic context to perform better morphological segmentation. Stallard et al. (2012) improve on this approach using the technique of Maximum Marginal decoding to reduce noise.", "startOffset": 0, "endOffset": 145}, {"referenceID": 0, "context": "To validate this measure, we computed the cosine similarity between words and their morphological parents from the CELEX2 database (Baayen et al., 1995).", "startOffset": 131, "endOffset": 152}, {"referenceID": 6, "context": "the literature (Goldwater and Johnson, 2004):", "startOffset": 15, "endOffset": 44}, {"referenceID": 16, "context": "We employ Contrastive Estimation (Smith and Eisner, 2005) and replace the normalization term by a sum over the neighbors of each word.", "startOffset": 33, "endOffset": 57}, {"referenceID": 2, "context": "We use LBFGS-B (Byrd et al., 1995) to optimize LCE(\u03b8;D) with gradients given above.", "startOffset": 15, "endOffset": 34}, {"referenceID": 13, "context": "MC-10 = MorphoChallenge 201010, MC-05:10 = MorphoChallenges 2005-10 (aggregated), BOUN = BOUN corpus (Sak et al., 2008), Gigaword = Arabic Gigaword", "startOffset": 101, "endOffset": 119}, {"referenceID": 11, "context": "corpus (Parker et al., 2011), ATB = Arabic Treebank (Maamouri et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 9, "context": ", 2011), ATB = Arabic Treebank (Maamouri et al., 2003)", "startOffset": 31, "endOffset": 54}, {"referenceID": 10, "context": "For the word vectors, we train the word2vec tool (Mikolov et al., 2013) on large text corpora and obtain 200-dimensional vectors for all three languages.", "startOffset": 49, "endOffset": 71}, {"referenceID": 19, "context": "We evaluate performance on individual segmentation points, which is standard for this task (Virpioja et al., 2011).", "startOffset": 91, "endOffset": 114}, {"referenceID": 12, "context": "As is common in unsupervised segmentation (Poon et al., 2009; Sirts and Goldwater, 2013), we included the test words (without their segmentations) with the training words during parameter learning.", "startOffset": 42, "endOffset": 88}, {"referenceID": 15, "context": "As is common in unsupervised segmentation (Poon et al., 2009; Sirts and Goldwater, 2013), we included the test words (without their segmentations) with the training words during parameter learning.", "startOffset": 42, "endOffset": 88}, {"referenceID": 8, "context": "Morfessor has achieved excellent performance on the MorphoChallenge dataset, and is widely used for performing unsupervised morphological analysis on various languages, even in fairly recent work (Luong et al., 2013).", "startOffset": 196, "endOffset": 216}, {"referenceID": 11, "context": "Baselines We compare our model with five other systems: Morfessor Baseline (Morf-Base), Morfessor CatMap (Morf-Cat), AGMorph, the Lee Segmenter and the system of Poon et al. (2009). Morfessor has achieved excellent performance on the MorphoChallenge dataset, and is widely used for performing unsupervised morphological analysis on various languages, even in fairly recent work (Luong et al.", "startOffset": 162, "endOffset": 181}, {"referenceID": 20, "context": "available implementations of these variants (Virpioja et al., 2013; Creutz and Lagus, 2005).", "startOffset": 44, "endOffset": 91}, {"referenceID": 3, "context": "available implementations of these variants (Virpioja et al., 2013; Creutz and Lagus, 2005).", "startOffset": 44, "endOffset": 91}, {"referenceID": 7, "context": "The Lee Segmenter (Lee et al., 2011), improved upon by using Maximum Marginal decoding in Stallard et al.", "startOffset": 18, "endOffset": 36}, {"referenceID": 7, "context": "The Lee Segmenter (Lee et al., 2011), improved upon by using Maximum Marginal decoding in Stallard et al. (2012), has achieved excellent performance on the Arabic (ATB) dataset.", "startOffset": 19, "endOffset": 113}], "year": 2015, "abstractText": "Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and wordlevel features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.1", "creator": "LaTeX with hyperref package"}}}