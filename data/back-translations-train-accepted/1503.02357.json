{"id": "1503.02357", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Context-Dependent Translation Selection Using Convolutional Neural Network", "abstract": "We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.", "histories": [["v1", "Mon, 9 Mar 2015 02:16:19 GMT  (548kb,D)", "https://arxiv.org/abs/1503.02357v1", "9 pages, 4 figures"], ["v2", "Wed, 24 Jun 2015 01:07:40 GMT  (553kb,D)", "http://arxiv.org/abs/1503.02357v2", "Short version is accepted by ACL 2015"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["baotian hu", "zhaopeng tu", "zhengdong lu", "hang li", "qingcai chen"], "accepted": true, "id": "1503.02357"}, "pdf": {"name": "1503.02357.pdf", "metadata": {"source": "CRF", "title": "Context-Dependent Translation Selection Using Convolutional Neural Network", "authors": ["Zhaopeng Tu", "Hong Kong", "Baotian Hu", "Zhengdong Lu", "Hang Li"], "emails": ["tuzhaopeng@gmail.com"], "sections": [{"heading": null, "text": "Context-Dependent Translation Selection Using Convolutional Neural NetworkZhaopeng Tu, Huawei Technologies Noah's Ark Lab, Hong KongBaotian Hu Harbin Institute of Technology, Shenzhen Graduate SchoolZhengdong Lu Huawei Technologies Noah's Ark Lab, Hong KongHang Li Huawei Technologies Noah's Ark Lab, Hong KongWe propose a new method of translation selection in statistical machine translation, using a revolutionary neural network to assess the similarity between a sentence pair in two languages. The specially designed Constitutional architecture encodes not only the semantic similarity of the translation pair, but also the context that contains the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs in two languages."}, {"heading": "1. Introduction", "text": "In a conventional statistical system of machine translation (SMT), the translation model is constructed in two steps (Koehn et al. 2003). Firstly, bilingual pairs of phrases respecting word orientations are extracted from a word-aligned parallel corpus. Secondly, the pairs of phrases with their relative frequencies in the same corpus are calculated. To alleviate the above problems, several researchers have proposed to find and use semantically similar pairs of translations in a continuous space (Gao et al. 2014; Zhang et al. 2014b) to capture the translation pairs that are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed to learn and use semantically similar pairs of translations in a continuous space (Gao et al. 2003)."}, {"heading": "2. Related Work", "text": "Our research builds on previous work in the field of contextual regulation and bilingual phrase harvesters."}, {"heading": "3. Context-Dependent Convolutional Matching Model", "text": "The model architecture shown in Figure 1 is a variant of the revolutionary architecture by Hu et al. (2014) It consists of two components: \u2022 Convolutionary sentence model, which summarizes the meaning of the source sentence and the target phrase; \u2022 Matching model, which compares the two representations with a multi-layered sentence model (Bengio 2009).Let e, be a target phrase and f be the source sentence, which includes the source phrase as an additional feature. We first project f and e in feature vectors x and y over the revolutionary sentence model, and then calculate the matching score s (x, y) using the matching model. Finally, the score is introduced into a conventional SMT system as an additional feature. As shown in Figure 1, the model takes as input the embedding of the words (trained elsewhere) in f and e."}, {"heading": "4. Training", "text": "Ideally, the trained CDCM model is expected to assign a higher matching score to a positive example (a source phrase in a specific context f and its correct translation e +) and a lower score to a negative example (the source phrase and a bad translation e \u2212 in the specific context). To this end, we adopt a discriminatory training strategy with maximum margin objectivity. Suppose we get the following triples (x, y +, y \u2212) from the oracle in which x, y +, y \u2212 are the feature vectors for f, e, e, e \u2212 each. We target the rank-based loss: L\u0443 (x, y +, y \u2212) = max (0, 1 + s (x, y \u2212) \u2212 s (6), in which s (x, y) the matching score function defined in Equation 5 consists of parameters for the conventional sentence model and MLP."}, {"heading": "4.1 Initialization by Context-Dependent Bilingual Word Embeddings", "text": "The initialization of the CDCM model is the embedding of words in both languages, a real value and a dense representation of words. Typical word embeddings are formed on monolingual data (Mikolov et al. 2013), so it is not possible to capture the useful semantic relationship between languages. It has been shown that bilingual word embeddings represent an essential step towards better capture of semantic equivalence at word level (Zou et al. 2013; Wu et al. 2014), so our model could be initialized with strong semantic information. Bilingual word embeddings refer to the semantic embeddings that are mapped into two languages, so that similar units in each language and in all languages have similar representations. Zou et al. (2013) we used MT word alignments to encourage pairs of frequent representation."}, {"heading": "4.2 Curriculum Training", "text": "It has been shown that the learning in the individual countries can benefit from an improved general education. (f, e) We have three types of negative examples with respect to the difficulty levellevauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveauniveurveurlevel levellevellevellevellevellevellevellevellevellevellevellevellevellevellevellevellevellevellevellevellevellevellevellevelseurseurseurseurseurseurseurlevel levellevellevellevellevellevellevellevellevellevellevellevellevellevellevelseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurseurse"}, {"heading": "5. Experiments", "text": "In this section, we try to answer two questions: 1 Does the proposed approach achieve higher translation quality than the base system? Does the approach outperform its context-independent counterpart? 2 Does model initialization through bilingual word embeddings outperform its monolingual counterpart in terms of translation quality? In Section 5.2, we evaluate our approach to a Chinese-English translation task. By using the CDCM model, our approach achieves a significant improvement in the BLEU score by up to 1.4 points. In addition, the CDCM model significantly outperforms its context-independent counterpart and confirms our hypothesis that local contexts are very useful for machine translation. In Section 5.3, we compare model embeddings through bilingual word embeddings and through conventional monolingual word embeddings. Experimental results show that bilingual word initialization consistently outperforms its monolingual counterpart, suggesting that better bilingual word embeddings provide a better CDM initialization."}, {"heading": "5.1 Setup", "text": "Our training data includes 1.5 million set pairs from the LDC dataset. The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansard's part of the LDC2004T07, LDC2004T08, and LDC2005T06. We train a 4 gram language model on the Xinhua part of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke 2002). We use the 2002 NIST MT evaluation test data as development data and the 2004, 2005 NIST MT evaluation test data as test data. We use minimal error rate training (Och 2003) to optimize feature weights."}, {"heading": "5.2 Evaluation of Translation Quality", "text": "We have two baseline systems: \u2022 Baseline: The baseline system is an open source model of phrase editing - Moses (Koehn et al. 2007) with a number of common features, including translation models, word and phrase delimitation, a linear distortion model, a lexical reorder model, and a language module that meets the selection criteria."}, {"heading": "5.3 Evaluation of Bilingual Word Embeddings", "text": "In this section, we will examine the influence of the bilingual word embeddings we use to initialize the CDCM model. We will use Word2Vec (Mikolov et al. 2013) to train the monolingual word embeddings. We will train the bilingual word embeddings using the approach described in Section 4.1. The dimensions of the bilingual and monolingual embeddings are 50.Table 2 shows the comparative results between bilingual and monolingual word embeddings. As we have seen, our bilingual word embeddings model outperforms its monolingual counterpart Zhaopeng Tu Context-Dependent Translation Selection Using Convolutional Neural Networkthroughout. Zou et al. (2013) and Wu et al. (2014) reported that semantic relationships at the word level between languages covered by bilingual word embeddings improve bilingual word embeddings."}, {"heading": "5.4 Discussion", "text": "Previous work on bilingual phrase representation usually uses Recurrent Neural Network (RNN) (Cho et al. 2014b) or Recursive AutoEncoder (RAE) (Zhang et al. 2014). In contrast, in (Kalchbrenner and Blunsom 2013; Sutskever et al. 2014; Cho et al. 2014a) it was observed that recursive approaches suffer from a significant decline in translation quality in the translation of long sentences. Kalchbrenner et al. (2014), on the other hand, show that the convolutional model could accurately represent the semantic content of a long sentence. Therefore, we choose the convolutional architecture to model the meaning of the sentence. Limitations. Unlike recursive models, the convolutional architecture has a fixed depth that limits the level of the composition. In this task, this limitation can be largely compensated by a network that can perform a \"global\" synthesis of the learned sentence."}, {"heading": "6. Conclusion", "text": "In this paper, we propose a context-dependent revolutionary matching model to capture semantic similarities between context-sensitive phrase pairs. Experimental results show that our approach significantly improves translation performance and achieves an improvement of 1.0 BLEU values across the entire test data. Integrating deep architecture into context-specific translation selection is a promising way to improve machine translation. This paper is the first step in what will hopefully be a long and fruitful journey. In the future, we will try to use context-related information on the target side (e.g. partial translations)."}], "references": [{"title": "On the properties of neural machine translation: encoder\u2013decoder approaches", "author": ["Cho et al.2014a]Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "SSST", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014b]Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008]Ronan Collobert", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Learning topic representation for smt with neural networks", "author": ["Cui et al.2014]Lei Cui", "Dongdong Zhang", "Shujie Liu", "Qiming Chen", "Mu Li", "Ming Zhou", "Muyun Yang"], "venue": null, "citeRegEx": "Cui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2014}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["Dahl et al.2013]George E Dahl", "Tara N Sainath", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014]Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao et al.2014]Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Improving statistical machine translation using lexicalized rule selection", "author": ["He et al.2008]Zhongjun He", "Qun Liu", "Shouxun Lin"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2008\\E", "shortCiteRegEx": "He et al\\.", "year": 2008}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014]Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003]Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Koehn et al.2007]Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": "ACL", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Maximum entropy based rule selection model for syntax-based statistical machine translation", "author": ["Liu et al.2008]Qun Liu", "Zhongjun He", "Yang Liu", "Shouxun Lin"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Soft syntactic constraints for hierarchical phrased-based translation", "author": ["Marton", "Resnik2008]Yuval Marton", "Philip Resnik"], "venue": null, "citeRegEx": "Marton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marton et al\\.", "year": 2008}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Meng et al.2015]Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "venue": null, "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov et al.2013]Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Josef Och"], "venue": "ACL", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improve statistical machine translation with context-sensitive bilingual semantic embedding model", "author": ["Wu et al.2014]Haiyang Wu", "Daxiang Dong", "Xiaoguang Hu", "Dianhai Yu", "Wei He", "Hua Wu", "Haifeng Wang", "Ting Liu"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "A Topic Similarity Model for Hierarchical Phrase-based Translation", "author": ["Xiao et al.2012]Xinyan Xiao", "Deyi Xiong", "Min Zhang", "Qun Liu", "Shouxun Lin"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2012}, {"title": "A topic-based coherence model for statistical machine translation", "author": ["Xiong", "Zhang2013]Deyi Xiong", "Min Zhang"], "venue": null, "citeRegEx": "Xiong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2013}, {"title": "Word Alignment Modeling with Context Dependent Deep Neural Network", "author": ["Yang et al.2013]Nan Yang", "Shujie Liu", "Mu Li", "Ming Zhou", "Nenghai Yu"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Bilingually-constrained phrase embeddings for machine translation", "author": ["Zhang et al.2014]Jiajun Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Chengqing Zong"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013]Will Y Zou", "Richard Socher", "Daniel Cer", "Christopher D Manning"], "venue": null, "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "In a conventional statistical machine translation (SMT) system, the translation model is constructed in two steps (Koehn et al. 2003).", "startOffset": 114, "endOffset": 133}, {"referenceID": 6, "context": "To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b).", "startOffset": 151, "endOffset": 205}, {"referenceID": 23, "context": "To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b).", "startOffset": 151, "endOffset": 205}, {"referenceID": 7, "context": "The above methods, however, neglect the information of local contexts, which has been proven to be useful for disambiguating translation candidates during decoding (He et al. 2008; Marton and Resnik 2008).", "startOffset": 164, "endOffset": 204}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching.", "startOffset": 13, "endOffset": 49}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching.", "startOffset": 13, "endOffset": 78}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. However, these discrete context features usually suffer the data sparseness problem. In addition, these models treated each word as a distinct feature, which can not leverage the semantic similarity between words as our model. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.", "startOffset": 13, "endOffset": 411}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. However, these discrete context features usually suffer the data sparseness problem. In addition, these models treated each word as a distinct feature, which can not leverage the semantic similarity between words as our model. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. However, they only focused on frequent phrase pairs and induced phrasal similarities by simply summing up the matching scores of all the embraced words. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston 2008). Another line of work focuses on capturing the document-level contexts via distributed representations. For instance, Xiao et al. (2012) and Cui et al.", "startOffset": 13, "endOffset": 1112}, {"referenceID": 3, "context": "(2012) and Cui et al. (2014) incorporated documentlevel topic information to select more semantically matched rules.", "startOffset": 11, "endOffset": 29}, {"referenceID": 5, "context": "For example, Gao et al. (2014) projected phrases from both source and target sides into a common, continuous space that is language independent.", "startOffset": 13, "endOffset": 31}, {"referenceID": 5, "context": "For example, Gao et al. (2014) projected phrases from both source and target sides into a common, continuous space that is language independent. Although Zhang et al. (2014) did not enforce the phrase embeddings from both sides to be in the same continuous space, they exploited a transformation between the two semantic embedding spaces.", "startOffset": 13, "endOffset": 174}, {"referenceID": 5, "context": "For example, Gao et al. (2014) projected phrases from both source and target sides into a common, continuous space that is language independent. Although Zhang et al. (2014) did not enforce the phrase embeddings from both sides to be in the same continuous space, they exploited a transformation between the two semantic embedding spaces. However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks.", "startOffset": 13, "endOffset": 697}, {"referenceID": 5, "context": "For example, Gao et al. (2014) projected phrases from both source and target sides into a common, continuous space that is language independent. Although Zhang et al. (2014) did not enforce the phrase embeddings from both sides to be in the same continuous space, they exploited a transformation between the two semantic embedding spaces. However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks.", "startOffset": 13, "endOffset": 714}, {"referenceID": 5, "context": "However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs.", "startOffset": 67, "endOffset": 88}, {"referenceID": 8, "context": "The model architecture, shown in Figure 1, is a variant of the convolutional architecture of Hu et al. (2014). It consists of two components:", "startOffset": 93, "endOffset": 110}, {"referenceID": 4, "context": "In this work, we use ReLu (Dahl et al. 2013) as the activation function;", "startOffset": 26, "endOffset": 44}, {"referenceID": 15, "context": "Typical word embeddings are trained on monolingual data (Mikolov et al. 2013), thus fails to capture the useful semantic relationship across languages.", "startOffset": 56, "endOffset": 77}, {"referenceID": 24, "context": "It has been shown that bilingual word embeddings represent a substantial step in better capturing semantic equivalence at the word level (Zou et al. 2013; Wu et al. 2014), thus could initialize our model with strong semantic information.", "startOffset": 137, "endOffset": 170}, {"referenceID": 19, "context": "It has been shown that bilingual word embeddings represent a substantial step in better capturing semantic equivalence at the word level (Zou et al. 2013; Wu et al. 2014), thus could initialize our model with strong semantic information.", "startOffset": 137, "endOffset": 170}, {"referenceID": 15, "context": "Typical word embeddings are trained on monolingual data (Mikolov et al. 2013), thus fails to capture the useful semantic relationship across languages. It has been shown that bilingual word embeddings represent a substantial step in better capturing semantic equivalence at the word level (Zou et al. 2013; Wu et al. 2014), thus could initialize our model with strong semantic information. Bilingual word embeddings refer to the semantic embeddings associated across two languages so that similar units in each language and across languages have similar representations. Zou et al. (2013) utilized MT word alignments to encourage pairs of frequently", "startOffset": 57, "endOffset": 589}, {"referenceID": 19, "context": "aligned words to have similar word embeddings, while Wu et al. (2014) improved bilingual word embeddings with discrete contextual information.", "startOffset": 53, "endOffset": 70}, {"referenceID": 17, "context": "For evaluation, case-insensitive NIST BLEU (Papineni et al. 2002) is used to measure translation performance.", "startOffset": 43, "endOffset": 65}, {"referenceID": 11, "context": "\u2022 Baseline: The baseline system is an open-source system of the phrase-based model \u2013 Moses (Koehn et al. 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model.", "startOffset": 91, "endOffset": 110}, {"referenceID": 6, "context": "\u2022 CICM (context-independent convolutional matching) model: Following the previous works (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b), we calculate the matching degree of a phrase pair without considering any contextual information.", "startOffset": 88, "endOffset": 142}, {"referenceID": 23, "context": "\u2022 CICM (context-independent convolutional matching) model: Following the previous works (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b), we calculate the matching degree of a phrase pair without considering any contextual information.", "startOffset": 88, "endOffset": 142}, {"referenceID": 15, "context": "We use the Word2Vec (Mikolov et al. 2013) to train the monolingual word embeddings.", "startOffset": 20, "endOffset": 41}, {"referenceID": 22, "context": "One possible reason is that bilingual and contextual information helps to capture the semantic relationships between words across languages (Yang et al. 2013), thus better phrasal similarities by using principle of compositionality.", "startOffset": 140, "endOffset": 158}, {"referenceID": 22, "context": "Zou et al. (2013) and Wu et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "(2013) and Wu et al. (2014) reported that word-level semantic relationships across languages, captured by the bilingual word embeddings, boost machine translation performance.", "startOffset": 11, "endOffset": 28}, {"referenceID": 23, "context": "2014b) or Recursive AutoEncoder (RAE) (Zhang et al. 2014).", "startOffset": 38, "endOffset": 57}, {"referenceID": 18, "context": "It has been observed in (Kalchbrenner and Blunsom 2013; Sutskever et al. 2014; Cho et al. 2014a) that the recursive approaches suffer from a significant drop in translation quality when translating long sentences.", "startOffset": 24, "endOffset": 96}, {"referenceID": 0, "context": "Previous works on bilingual phrase representations usually employ Recurrent Neural Network (RNN) (Cho et al. 2014b) or Recursive AutoEncoder (RAE) (Zhang et al. 2014). It has been observed in (Kalchbrenner and Blunsom 2013; Sutskever et al. 2014; Cho et al. 2014a) that the recursive approaches suffer from a significant drop in translation quality when translating long sentences. In contrast, Kalchbrenner et al. (2014) show that the convolutional model could represent the semantic content of a long sentence accurately.", "startOffset": 98, "endOffset": 422}], "year": 2015, "abstractText": "We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.", "creator": "LaTeX with hyperref package"}}}