{"id": "1503.02364", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Neural Responding Machine for Short-Text Conversation", "abstract": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.", "histories": [["v1", "Mon, 9 Mar 2015 02:54:29 GMT  (652kb)", "https://arxiv.org/abs/1503.02364v1", null], ["v2", "Mon, 27 Apr 2015 02:28:58 GMT  (652kb)", "http://arxiv.org/abs/1503.02364v2", "accepted as a full paper at ACL 2015"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["lifeng shang", "zhengdong lu", "hang li"], "accepted": true, "id": "1503.02364"}, "pdf": {"name": "1503.02364.pdf", "metadata": {"source": "CRF", "title": "Neural Responding Machine for Short-Text Conversation", "authors": ["Lifeng Shang", "Zhengdong Lu Hang Li"], "emails": ["Shang.Lifeng@huawei.com", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.02 364v 2 [cs.C L] 27 Apr 201 5"}, {"heading": "1 Introduction", "text": "Conversation with natural language is one of the most difficult problems with artificial intelligence, involving language comprehension, reasoning and the use of common sense knowledge. Previous work in this direction has mainly focused on rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000). These types of methods often rely on manual efforts in designing rules or automatic training of models with a particular learning algorithm and a small amount of data, making it difficult to develop an expandable open conversation system.Recently, due to the explosive growth of microblogging services such as Twitter1 and Weibo2, the amount of conversation data available on the Web has increased tremendously, making a data-driven approach to addressing the conversation problem possible (Ji et al., 2014; Ritter et al al., 2011). Instead of multiple conversations, the task at hand is called short conversation."}, {"heading": "1.1 Overview", "text": "In this paper, we take a probabilistic model to address the response generation problem and propose to use a neural encoder decoder called the Neural Responding Machine (NRM) for this task. As shown in Figure 1, the neural encoder decoder model first summarizes the contribution as a vector representation and then feeds this representation to the decoder to generate responses. We further generalize this scheme to dynamically alter the post-representation during the generation process, according to the idea in (Bahdanau et al., 2014) originally proposed for a neural network-based machine translation with automatic alignment. NRM essentially estimates the probability of an answer in the face of a post. The estimated probability should be clearly complex enough to represent all the appropriate answers. A similar framework has been applied for machine translation with notable success (Kalchbrenblunsom and we ascertain the probability of a post; 2013, 2014, Ai et al; 2014, Ai ever)."}, {"heading": "1.2 RoadMap", "text": "In the rest of this work we start with the introduction of the STC dataset in Section 2. Then we will discuss the NRM model in Section 3, followed by the details of the training in Section 4. Then we will report on the experimental results in Section 5. In Section 6 we will finish the work."}, {"heading": "2 The Dataset for STC", "text": "Our models are trained on a corpus of about 4.4 million conversation pairs on Weibo 3.3. The data set and its English translation (via machine translation system) will be published shortly."}, {"heading": "2.1 Conversations on Sina Weibo", "text": "Weibo is a popular Twitter-like microblogging service in China, where a user can post short messages (referred to as a post in memory of this article) that are visible to the public or a group of users who follow it. Other users comment on a published post that is called a reply. Like Twitter, Weibo also has a 140-character length limit for both posts and replies, making the post-response pair an ideal substitute for short text conversations."}, {"heading": "2.2 Dataset Description", "text": "To construct this million-fold dataset, we first sift through hundreds of millions of post-response pairs and then clean up the raw data in a similar way as suggested in (Wang et al., 2013), including 1) removing trivial answers such as \"wow,\" 2) filtering out potential ads, and 3) removing the answers after the first 30 on the consistency of the topic. Table 1 shows some statistics of the dataset used in this paper. You can see that each post has an average of 20 different answers. In addition to the semantic gap between post and its answers, this is another significant difference from a general parallel dataset used for traditional translations."}, {"heading": "3 Neural Responding Machines for STC", "text": "The basic idea of NRM is to create a hidden representation of a post and then generate the response based on it, as in Figure 2. In the respective illustration, the encoder converts the input sequence x = (x1, \u00b7 \u00b7 \u00b7, xT) into a series of high-dimensional hidden representations h = (h1, \u00b7 \u00b7 \u00b7, hT), which, together with the attention signal at a given time, are fed t (referred to as \u03b1t) to the context generator to form the context input to the encoder at a given time t (referred to as ct). Subsequently, ct is converted linearly from a matrix L (as part of the decoder) to a stimulus for generating RNN to generate the t-th word of the answer (referred to as yt). In the neural translation system, L converts the representation in the source language to that of the target language to that of neurodia."}, {"heading": "3.1 The Computation in Decoder", "text": "Figure 3 shows the graphical model of the decoder, which is essentially a standard RNN language model, except for context input c. The generation probability of the t-word is calculated by p (yt | yt \u2212 1, \u00b7 \u00b7, y1, x) = g (yt \u2212 1, st, ct), (1) where yt is a unified word representation, g (\u00b7) is a Softmax activation function and st is the hidden state of the decoder at a time t = f (yt \u2212 1, st \u2212 1, ct), (2) and f (\u00b7) is a nonlinear activation function and the transformation L is often assigned as a parameter of f (\u00b7). Here, f (\u00b7) can be a logistic function, the demanding long-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997) or the recently proposed gated recurrent unit (GRU) (Chet, Gt; 2014 GM), which is too long for STM storage."}, {"heading": "3.2 The Computation in Encoder", "text": "We consider three types of encoding schemes, namely 1) the global scheme, 2) the local scheme and the hybrid scheme combining 1) and 2).Global scheme: Figure 4 shows the graphical model of the RNN encoder and the associated context generator for a global encoding scheme. The hidden state at present t is calculated by ht = f (xt, ht \u2212 1) (i.e. silent GRU unit), and with a trivial context generation process we essentially use the final hidden state hT as the global representation of the set. The same strategy has been used in (Cho et al., 2014) and (Sutskever et al., 2014) to build the intermediate representation for machine translation. However, this scheme has its drawbacks: a vector summary of the entire set is often difficult to obtain and can lose important details for reaction generation, especially if the hidden dimension of the state is not large enough."}, {"heading": "3.3 Extensions: Local and Global Model", "text": "In the task of the STC, NRM-glo, NRM-glo has the summary of the entire post, while NRM-loc can adaptively select the important words in the post for various suitable answers. Since post-response pairs in the STC are not strictly parallel and a word in another context can have different meanings, we suspect that the global representation in NRM-glo can provide a useful context for the extraction of the local context, hence complementary to the scheme in NRM-loc. It is therefore a natural extension to link the two models by concatenating their hidden states to form an extended hidden representation for each time stamp, as shown in Figure 6. We can integrate the summary hgT in ct and \u03b1tj to provide a global context for local matching. With this hybrid method, we hope that both the local and the global information can be included in the generation of the response."}, {"heading": "4 Experiments", "text": "We evaluate three different NRM settings described in Section 3, namely NRM-glo, NRM-loc and NRM-hyb, and compare them with on-demand and SMT-based methods."}, {"heading": "4.1 Implementation Details", "text": "Although both posts and answers are written in the same language, the word distributions for the two words differ: the number of unique words in the posttext is 125,237, and the number of words in the reply text is 679,958. Therefore, we construct two separate words for posts and answers, using 40,000 common words on each page, using 97.8% of the words in the posttext and 96.2% in the reply text. All the5http: / / nlp.stanford.edu / software / segmenter.shtmlRemaining words are replaced by a special symbol \"UNK.\" The hidden states of the encoder and decoder are each 1,000, and the dimensions of the word embedding for post and reply 620. The model parameters are trained by random sampling from a uniform distribution between -0.1 and 0.1. All of our models have been trained on a NVIDIA Tesla-PP40 model with two mini gradations each."}, {"heading": "4.2 Competitor Models", "text": "Retrieval-based models: with retrieval-based models, the answer r * is often retrieved from a large post-response repository (p, r). Such models rely on three key components: a large repository, sets of feature functions \u03a6i (p, (p, r)), and a machine learning model to combine these features. In this thesis, the entire 4.4 million Weibo pairs are used as a repository, 14 features ranging from simple cosmic similarity to some in-depth matching models (Ji et al., 2014) to determine the suitability of a post for a particular post p * using the following linear model score (p, r)."}, {"heading": "5 Results and Analysis", "text": "The widely accepted evaluation methods in translation (e.g. BLUE score (Papineni et al., 2002)) do not apply, since the range of suitable answers is so wide that it is virtually impossible to provide a reference with adequate coverage. Nor is it useful to evaluate with perplexity, a commonly used measure in statistical language modeling, because the naturalness of the answer and the relationship to the contribution cannot be evaluated well. Therefore, we resort to human judgment, similar to that in (Ritter et al., 2011), but with important differences."}, {"heading": "5.1 Evaluation Methods", "text": "We accept human annotations to compare the performance of different models. Five labelers with at least three years of Sina Weibo experience are asked to perform human evaluations. Answers drawn from the five rated models are bundled for each labeler and randomly permutated. Labelers are instructed to imagine that they are the authors of the original entries and to judge whether a response (generated or retrieved) is appropriate and natural for an input item. \u2022 Three levels are assigned to an answer with results from 0 to 2: 6We use the default similarity function of Lucene 7 8Reports results showed that the new model outperformed the baseline SMT model 57.7% of the time. \u2022 Suitable (+ 2): The answer is obviously an appropriate and natural response to the post; \u2022 Neutral (+ 1): The response can be an appropriate response to a particular scenario."}, {"heading": "5.2 Results", "text": "Our test set consists of 110 posts that do not appear in the training, ranging in length from 6 to 22 words. Experimental results are based on human annotations and are summarized in Table 2, which shows the ratio of three categories and the correspondence between each model. The agreement is interpreted as a \"fair agreement\" by the other models."}, {"heading": "5.3 Case Study", "text": "Figure 8 shows some sample responses generated by our NRMs (only the most likely ones are given) and the comparable retrieval-based model. It is fascinating to note that three NRM variants provide suitable but quite different responses with different perspectives and wording, which we suspect is caused both by architectural variations between models and by variations in random effects such as initialization of parameters. Another interesting observation is in the fourth example, where the retrieval-based method returns a response with the inappropriate entity name \"WenShan,\" which is actually quite a common problem for retrieval-based models, where the inconsistency details (e.g. data, named entities) that often make the response inappropriate cannot be adequately considered in the matching function used when retrieving the responses. In contrast, we observe that NRMs tend to give a general response and rarely generate it."}, {"heading": "6 Conclusions and Future Work", "text": "Empirical studies confirm that the newly proposed NRMs, especially the hybrid coding scheme, can outperform state-of-the-art retrieval-based and SMT-based methods. Our preliminary study also shows that NRMs can generate multiple responses with great diversity to a particular post. In future work, we would consider adding the intention (or mood) of users as an external signal from the decoder to generate responses with specific objectives."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In EMNLP,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Measuring nominal scale agreement among many raters", "author": ["Joseph L Fleiss"], "venue": "Psychological bulletin,", "citeRegEx": "Fleiss.,? \\Q1971\\E", "shortCiteRegEx": "Fleiss.", "year": 1971}, {"title": "Generating sequences with recurrent neural networks. preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Fundamental Statistics for the Behavioral Sciences", "author": ["David C. Howell"], "venue": "PSY", "citeRegEx": "Howell.,? \\Q2010\\E", "shortCiteRegEx": "Howell.", "year": 2010}, {"title": "An information retrieval approach to short text conversation", "author": ["Ji et al.2014] Zongcheng Ji", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1408.6988", "citeRegEx": "Ji et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2014}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims"], "venue": "In SIGKDD,", "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Njfun: a reinforcement learning spoken dialogue system", "author": ["Litman et al.2000] Diane Litman", "Satinder Singh", "Michael Kearns", "Marilyn Walker"], "venue": "In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems,", "citeRegEx": "Litman et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Litman et al\\.", "year": 2000}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Reinforcement learning of question-answering dialogue policies for virtual museum guides", "author": ["Misu et al.2012] Teruhisa Misu", "Kallirroi Georgila", "Anton Leuski", "David Traum"], "venue": "In SIGDIAL,", "citeRegEx": "Misu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Misu et al\\.", "year": 2012}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011] Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In EMNLP,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "author": ["Karl Weilhammer", "Matt Stuttle", "Steve Young"], "venue": "The knowledge engineering review,", "citeRegEx": "Schatzmann et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A dataset for research on shorttext conversations", "author": ["Wang et al.2013] Hao Wang", "Zhengdong Lu", "Hang Li", "Enhong Chen"], "venue": "In EMNLP,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["Williams", "Young2007] Jason D Williams", "Steve Young"], "venue": "Computer Speech & Language,", "citeRegEx": "Williams et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 18, "context": "Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000).", "startOffset": 93, "endOffset": 184}, {"referenceID": 14, "context": "Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000).", "startOffset": 93, "endOffset": 184}, {"referenceID": 12, "context": "Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000).", "startOffset": 93, "endOffset": 184}, {"referenceID": 8, "context": "This makes a data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011) possible.", "startOffset": 69, "endOffset": 107}, {"referenceID": 17, "context": "This makes a data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011) possible.", "startOffset": 69, "endOffset": 107}, {"referenceID": 8, "context": "Previous methods for STC fall into two categories, 1) the retrieval-based method (Ji et al., 2014), and 2) the statistical machine translation (SMT) based method (Ritter et al.", "startOffset": 81, "endOffset": 98}, {"referenceID": 17, "context": ", 2014), and 2) the statistical machine translation (SMT) based method (Ritter et al., 2011).", "startOffset": 71, "endOffset": 92}, {"referenceID": 1, "context": "We further generalize this scheme to allow the post representation dynamically change during the generation process, following the idea in (Bahdanau et al., 2014) originally proposed for neural-network-based machine translation with automatic alignment.", "startOffset": 139, "endOffset": 162}, {"referenceID": 0, "context": "Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 82, "endOffset": 180}, {"referenceID": 19, "context": "Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 82, "endOffset": 180}, {"referenceID": 1, "context": "Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 82, "endOffset": 180}, {"referenceID": 20, "context": "2 Dataset Description To construct this million scale dataset, we first crawl hundreds of millions of post-response pairs, and then clean the raw data in a similar way as suggested in (Wang et al., 2013), including 1) removing trivial responses like \u201cwow\u201d, 2) filtering out potential advertisements, and 3) removing the responses after first 30 ones for topic consistency.", "startOffset": 184, "endOffset": 203}, {"referenceID": 13, "context": "We use Recurrent Neural Network (RNN) for both encoder and decoder, for its natural ability to summarize and generate word sequence of arbitrary lengths (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 153, "endOffset": 217}, {"referenceID": 19, "context": "We use Recurrent Neural Network (RNN) for both encoder and decoder, for its natural ability to summarize and generate word sequence of arbitrary lengths (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 153, "endOffset": 217}, {"referenceID": 2, "context": "We use Recurrent Neural Network (RNN) for both encoder and decoder, for its natural ability to summarize and generate word sequence of arbitrary lengths (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 153, "endOffset": 217}, {"referenceID": 3, "context": "Here f(\u00b7) can be a logistic function, the sophisticated long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997), or the recently proposed gated recurrent unit (GRU) (Chung et al., 2014; Cho et al., 2014).", "startOffset": 179, "endOffset": 217}, {"referenceID": 2, "context": "Here f(\u00b7) can be a logistic function, the sophisticated long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997), or the recently proposed gated recurrent unit (GRU) (Chung et al., 2014; Cho et al., 2014).", "startOffset": 179, "endOffset": 217}, {"referenceID": 3, "context": "We use GRU in this work, since it performs comparably to LSTM on squence modeling (Chung et al., 2014), but has less parameters and easier to train.", "startOffset": 82, "endOffset": 102}, {"referenceID": 2, "context": "The same strategy has been taken in (Cho et al., 2014) and (Sutskever et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 19, "context": ", 2014) and (Sutskever et al., 2014) for building the intermediate representation for machine translation.", "startOffset": 12, "endOffset": 36}, {"referenceID": 1, "context": "This local scheme is devised in (Bahdanau et al., 2014) for automatic alignment between the source sentence and the partial target sentence in machine translation.", "startOffset": 32, "endOffset": 55}, {"referenceID": 1, "context": "Local Scheme: Recently, Bahdanau et al. (2014) and Graves (2013) introduced an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence ct = \u2211T j=1 \u03b1tjhj , where weighting factors \u03b1tj determine which part should be selected to generate the new word yt, which in turn is a function of hidden states \u03b1tj = q(hj , st\u22121), as pictorially shown in Figure 5.", "startOffset": 24, "endOffset": 47}, {"referenceID": 1, "context": "Local Scheme: Recently, Bahdanau et al. (2014) and Graves (2013) introduced an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence ct = \u2211T j=1 \u03b1tjhj , where weighting factors \u03b1tj determine which part should be selected to generate the new word yt, which in turn is a function of hidden states \u03b1tj = q(hj , st\u22121), as pictorially shown in Figure 5.", "startOffset": 24, "endOffset": 65}, {"referenceID": 2, "context": "(2014) has to use 4, 000 dimension for satisfying performance on machine translation, while (Cho et al., 2014) with a smaller dimension perform poorly on translating an entire sentence.", "startOffset": 92, "endOffset": 110}, {"referenceID": 8, "context": "4 million Weibo pairs are used as the repository, 14 features, ranging from simple cosine similarity to some deep matching models (Ji et al., 2014) are used to determine the suitability of a post to a given post p\u2217 through the following linear model", "startOffset": 130, "endOffset": 147}, {"referenceID": 8, "context": "Following the ranking strategy in (Ji et al., 2014), we pick 225 posts and about 30 retrieved responses for each of them given by a baseline retriever6 from the 4.", "startOffset": 34, "endOffset": 51}, {"referenceID": 9, "context": "We use ranking SVM model (Joachims, 2006) for the parameters \u03c9i based on the labeled dataset.", "startOffset": 25, "endOffset": 41}, {"referenceID": 11, "context": "We use the most widely used open-source phrase-based translation modelMoses (Koehn et al., 2007).", "startOffset": 76, "endOffset": 96}, {"referenceID": 17, "context": "In (Ritter et al., 2011), the authors used a modified SMT model to obtain the \u201cResponse\u201d of Twitter \u201cStimulus\u201d.", "startOffset": 3, "endOffset": 24}, {"referenceID": 16, "context": "BLUE score (Papineni et al., 2002)) do not apply, since the range of the suitable responses is so large that it is practically impossible to give reference with adequate coverage.", "startOffset": 11, "endOffset": 34}, {"referenceID": 17, "context": "We therefore resort to human judgement, similar to that taken in (Ritter et al., 2011) but with important difference.", "startOffset": 65, "endOffset": 86}, {"referenceID": 4, "context": "The agreement is evaluated by Fleiss\u2019 kappa (Fleiss, 1971), as a statistical measure of inter-rater consistency.", "startOffset": 44, "endOffset": 58}, {"referenceID": 1, "context": "\u2022 NRM-loc outperforms NRM-glo, suggesting that a dynamically generated context might be more effective than a \u201cstatic\u201d fixed-length vector for the entire post, which is consistent with the observation made in (Bahdanau et al., 2014) for machine translation; \u2022 NRM-hyp outperforms NRM-loc and NRM-glo, suggesting that a global representation of post is complementary to dynamically generated local context.", "startOffset": 209, "endOffset": 232}, {"referenceID": 7, "context": "To test statistical significance, we use the Friedman test (Howell, 2010), which is a non-parametric test on the differences of several related samples, based on ranking.", "startOffset": 59, "endOffset": 73}], "year": 2015, "abstractText": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.", "creator": "LaTeX with hyperref package"}}}