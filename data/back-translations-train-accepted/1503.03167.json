{"id": "1503.03167", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2015", "title": "Deep Convolutional Inverse Graphics Network", "abstract": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.", "histories": [["v1", "Wed, 11 Mar 2015 04:08:42 GMT  (8277kb,D)", "http://arxiv.org/abs/1503.03167v1", "First two authors contributed equally"], ["v2", "Mon, 16 Mar 2015 04:57:24 GMT  (8267kb,D)", "http://arxiv.org/abs/1503.03167v2", "First two authors contributed equally"], ["v3", "Tue, 17 Mar 2015 02:22:07 GMT  (8267kb,D)", "http://arxiv.org/abs/1503.03167v3", "First two authors contributed equally"], ["v4", "Mon, 22 Jun 2015 02:10:00 GMT  (8878kb,D)", "http://arxiv.org/abs/1503.03167v4", "First two authors contributed equally"]], "COMMENTS": "First two authors contributed equally", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG cs.NE", "authors": ["tejas d kulkarni", "william f whitney", "pushmeet kohli", "joshua b tenenbaum"], "accepted": true, "id": "1503.03167"}, "pdf": {"name": "1503.03167.pdf", "metadata": {"source": "CRF", "title": "Deep Convolutional Inverse Graphics Network", "authors": ["Tejas D. Kulkarni", "Will Whitney", "Pushmeet Kohli", "Joshua B. Tenenbaum"], "emails": ["tejask@mit.edu", "wwhitney@mit.edu", "pkohli@microsoft.com", "jbt@mit.edu"], "sections": [{"heading": null, "text": "This work introduces the Deep Convolution Inverse Graphics Network (DC-IGN), which aims to learn an interpretable representation of images unraveled in relation to various transformations, such as object rotations, illumination variations, and texture. The DC-IGN model consists of several layers of folding and unfolding operators and is trained with the stochastic Gradient Variational Bayes (SGVB) algorithm [11]. We suggest training methods to encourage neurons in the graphics code layer to have semantic meaning and force each group to clearly represent a particular transformation (pose, light, texture, shape, etc.). Faced with a static face image, our model can recreate the input image in different poses, illumination, or even texture and shapes from the base."}, {"heading": "1 Introduction", "text": "This year, it has come to the point where there can only be a significant number of pus bags capable of taking the lead."}, {"heading": "2 Related Work", "text": "Unlike most RBM-based models [8, 23, 16], our approach is trained by means of back-propagation from the concept of data reconstruction and the varying boundary. In this work, an advanced neural network (encoder) is used to approximate the posterior distribution and a decoder network that serves to allow stochastic reconstructions of observations. In order to handle fine-grained geometries of faces, we work with relatively large images (150x150). Our approach extends to posterior distribution and a decoder network that allows stochastic reconstruction of observations."}, {"heading": "3 Model", "text": "In fact, it is so that one sees oneself in a position to put oneself and the others in the center. (...) In fact, it is so that most people are able to understand oneself. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world of the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world of the world, in the world of the world, in the world of the world, in the world of the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the, in the world in the world in the world, in the world in the world in the, in the world in the world in the, in the world in the, in the world in the world in the world in the, in the world in the, in the world in the, in the world in the world, in the world in the world in the, in the world in the, in the world in the, in the world in the world in the, in the world in the, in the world in the world in the, in the world in the world in the, in the world in the, in the, in the world in the, in the world in the world of the, in the world in the world in the, in the, in the"}, {"heading": "3.1 Random Transformations", "text": "For our simplest training method, we construct minicharges of randomly selected training samples and train the DC-IGN network in Figure 1 using the SGVB method. This training results in networks with extremely strong reconstruction performance. We examine various uses of the representations that these networks represent in Section 4."}, {"heading": "3.2 Specific Transformations", "text": "This year is the highest in the history of the country."}, {"heading": "3.2.1 Invariance targeting", "text": "In order to further improve the invariance of the method defined in Section 3.2, we propose a method for explicit training invariance of the latent variables against orthogonal transformations. As shown in Figure 4, we replace the zero gradients of zi 6 = ztrain from Section 3.2 with an error gradient pointing towards the greatest possible difference from the mean of the latent variables we want to be invariant in this minibatch. In training, this pushes the values we want to keep constant throughout the batch towards each other and reduces the variation these latents experience due to orthogonal variables. This regulating force must be scaled to be much smaller than the true training signal; empirically, a factor of 1 / 100 works well."}, {"heading": "4 Experiments", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "4.1 3D Statistical Shape Modeling", "text": "To test the contemporaneity of DC-IGN ratios, we use the model of DC-IGN ratios as a summary DC ratios in a generative model based on the 3DMM face model. (D) We are interested in the following question: How well can we recover from a compositional DC ratios? (D) We can use DC-IGN ratios in the way we apply these ratios, not in the way we apply them (e.g. L1). (D) in which we apply the contemporaneity of DC-IGN ratios. (D) We can use DC-IGN ratios. (D) DECODER Figure 5: Deep DC-IGN encoders with a domain-specific decoder: To test the contemporaneity of DC-IGNs ratios, we can use DC-IGNs ratios as marginal statistical ratios."}, {"heading": "4.2 Recognition", "text": "To test DC-IGN's invariance display properties, we performed a task to predict whether two given faces are equal or different. This experiment was first proposed by Yildrim et al. [28] and we adhere closely to this procedure for all neural network baselines. Yildrim et al. \"s test dataset consists of 96 pairs of faces with different poses, illumination, shape and texture. To ensure that the task is interesting and hard, they also obtained the accuracy of prediction from human subjects using Amazon Turk. Obtaining an invariant representation is difficult as the two faces can exhibit drastically different transformations. We also received a finetuned CNN on the SURF-W dataset al., which was fine-tuned with 400 famous people under different lighting conditions."}, {"heading": "4.3 Generalization of the rendering function", "text": "In fact, most of them will be able to feel as if they are able to survive on their own."}, {"heading": "5 Discussion", "text": "We have demonstrated that it is possible to learn a deep revolutionary inverse graphics network with interpretable graphics code layers only from raw images. We have proposed various training methods to force the network to learn unbundled representations. Using 3D facial analysis as a working example, we have demonstrated the invariant and equivariant properties of the learned representation. Furthermore, as highlighted in Section 4.1, we have been able to replace the deconventional network decoders with domain-specific decoding. To scale our approach to more complex scenes, it is probably important to experiment with deeper architectures to handle a large number of object categories within a single network architecture."}, {"heading": "Acknowledgements", "text": "We thank Thomas Vetter for the access to the Basel Face Model. T. Kulkarni was kindly supported by the Leventhal Fellowship. This research was supported by the ONR Prize N000141310333, ARO MURI W911NF-131-2012 and CBMM."}], "references": [{"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Thibodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Regression and classification using gaussian process priors", "author": ["J. Bernardo", "J. Berger", "A. Dawid", "A. Smith"], "venue": "In Bayesian Statistics 6: Proceedings of the sixth Valencia international meeting,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "A morphable model for the synthesis of 3d faces", "author": ["V. Blanz", "T. Vetter"], "venue": "In Proceedings of the 26th annual conference on Computer graphics and interactive techniques,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Learning the irreducible representations of commutative lie groups", "author": ["T. Cohen", "M. Welling"], "venue": "arXiv preprint arXiv:1402.4437,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Measuring invariances in deep networks. In Advances in neural information processing", "author": ["I. Goodfellow", "H. Lee", "Q.V. Le", "A. Saxe", "A.Y. Ng"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "The informed sampler: A discriminative approach to bayesian inference in generative computer vision models", "author": ["V. Jampani", "S. Nowozin", "M. Loper", "P.V. Gehler"], "venue": "arXiv preprint arXiv:1402.0859,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Inverse graphics with probabilistic cad models", "author": ["T.D. Kulkarni", "V.K. Mansinghka", "P. Kohli", "J.B. Tenenbaum"], "venue": "arXiv preprint arXiv:1407.1339,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Variational particle approximations", "author": ["T.D. Kulkarni", "A. Saeedi", "S. Gershman"], "venue": "arXiv preprint arXiv:1402.5715,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Opendr: An approximate differentiable renderer", "author": ["M.M. Loper", "M.J. Black"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Approximate bayesian image interpretation using generative probabilistic graphics programs", "author": ["V. Mansinghka", "T.D. Kulkarni", "Y.N. Perov", "J. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "In Artificial Neural Networks and Machine Learning\u2013", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Elliptical slice sampling", "author": ["I. Murray", "R.P. Adams", "D.J. MacKay"], "venue": "arXiv preprint arXiv:1001.0175,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Analysis-bysynthesis by learning to invert generative black boxes", "author": ["V. Nair", "J. Susskind", "G.E. Hinton"], "venue": "In Artificial Neural Networks-ICANN", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "A 3d face model for pose and illumination invariant face recognition", "author": ["P. Paysan", "R. Knothe", "B. Amberg", "S. Romdhani", "T. Vetter"], "venue": "Proceedings of the 6th IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS) for Security, Safety and Monitoring in Smart Environments,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Optimizing Neural Networks that Generate Images", "author": ["T. Tieleman"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "CoRR, abs/1412.4564,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Approximate bayesian computation (abc) gives exact results under the assumption of model error", "author": ["R.D. Wilkinson"], "venue": "Statistical applications in genetics and molecular biology,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Explaining monkey face patch system as deep inverse graphics", "author": ["I. Yildrim", "T. Kulkarni", "W. Freiwald", "J. Tenenbaum"], "venue": "In Computational and Systems Neuroscience,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [11].", "startOffset": 172, "endOffset": 176}, {"referenceID": 14, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 52, "endOffset": 60}, {"referenceID": 11, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 52, "endOffset": 60}, {"referenceID": 7, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 122, "endOffset": 133}, {"referenceID": 22, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 122, "endOffset": 133}, {"referenceID": 15, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 122, "endOffset": 133}, {"referenceID": 0, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 153, "endOffset": 164}, {"referenceID": 25, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 153, "endOffset": 164}, {"referenceID": 18, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 153, "endOffset": 164}, {"referenceID": 5, "context": "[6] have considered this problem by proposing a theoretical framework to learn irreducible representations having both invariances and equivariances, coming up with the best representation for any given task is an open question.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The Vision as inverse graphics approach offers an interesting perspective to various desiderata for a good representation [2, 6, 7]: invariance, meaningfulness of representations, abstractions and disentanglement.", "startOffset": 122, "endOffset": 131}, {"referenceID": 5, "context": "The Vision as inverse graphics approach offers an interesting perspective to various desiderata for a good representation [2, 6, 7]: invariance, meaningfulness of representations, abstractions and disentanglement.", "startOffset": 122, "endOffset": 131}, {"referenceID": 6, "context": "The Vision as inverse graphics approach offers an interesting perspective to various desiderata for a good representation [2, 6, 7]: invariance, meaningfulness of representations, abstractions and disentanglement.", "startOffset": 122, "endOffset": 131}, {"referenceID": 9, "context": "Recent work in inverse graphics [10, 18, 17, 13] follows a general strategy of first defining a probabilistic or nonprobabilistic model with latent parameters, followed by an inference or optimization algorithms to find the most appropriate set of latent parameters given observations.", "startOffset": 32, "endOffset": 48}, {"referenceID": 17, "context": "Recent work in inverse graphics [10, 18, 17, 13] follows a general strategy of first defining a probabilistic or nonprobabilistic model with latent parameters, followed by an inference or optimization algorithms to find the most appropriate set of latent parameters given observations.", "startOffset": 32, "endOffset": 48}, {"referenceID": 16, "context": "Recent work in inverse graphics [10, 18, 17, 13] follows a general strategy of first defining a probabilistic or nonprobabilistic model with latent parameters, followed by an inference or optimization algorithms to find the most appropriate set of latent parameters given observations.", "startOffset": 32, "endOffset": 48}, {"referenceID": 12, "context": "Recent work in inverse graphics [10, 18, 17, 13] follows a general strategy of first defining a probabilistic or nonprobabilistic model with latent parameters, followed by an inference or optimization algorithms to find the most appropriate set of latent parameters given observations.", "startOffset": 32, "endOffset": 48}, {"referenceID": 23, "context": "[24] moved beyond this two stage pipeline by using a generic encoder network with a domainspecific decoder network to approximate a 2D rendering function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "To achieve this, we employ a deep directed graphical model with many layers of convolution and de-convolution operators that is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 10, "context": "We follow the variational autoencoder[11] architecture with several variations.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "Unlike most RBM based models[8, 23, 16], our approach is trained using back-propagation from the data reconstruction term and the variational bound.", "startOffset": 28, "endOffset": 39}, {"referenceID": 22, "context": "Unlike most RBM based models[8, 23, 16], our approach is trained using back-propagation from the data reconstruction term and the variational bound.", "startOffset": 28, "endOffset": 39}, {"referenceID": 15, "context": "Unlike most RBM based models[8, 23, 16], our approach is trained using back-propagation from the data reconstruction term and the variational bound.", "startOffset": 28, "endOffset": 39}, {"referenceID": 10, "context": "[11] proposed the SGVB algorithm to learn generative models with continuous latent variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Generative stochastic networks [3] which learn the transi-", "startOffset": 31, "endOffset": 34}, {"referenceID": 28, "context": "[29] which is used to produce image representations using convolutional decomposition of images under a sparsity constraint.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "A number of inverse graphics inspired methods have recently been proposed in the literature [10, 18, 17].", "startOffset": 92, "endOffset": 104}, {"referenceID": 17, "context": "A number of inverse graphics inspired methods have recently been proposed in the literature [10, 18, 17].", "startOffset": 92, "endOffset": 104}, {"referenceID": 16, "context": "A number of inverse graphics inspired methods have recently been proposed in the literature [10, 18, 17].", "startOffset": 92, "endOffset": 104}, {"referenceID": 8, "context": "[9] (transforming autoencoders) and Tieleman [24] which uses a domain specific decoder to reconstruct input images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[9] (transforming autoencoders) and Tieleman [24] which uses a domain specific decoder to reconstruct input images.", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "Our work is similar in spirit to these works but has some key differences: (a) It uses a very generic convolutional architecture in the encoder and decoder networks to enable efficient learning on large datasets and image sizes, (b) it can handle single static frames as opposed to pair of images required in [9], and (c) it is generative.", "startOffset": 309, "endOffset": 312}, {"referenceID": 10, "context": "There are two reasons for using this parametrization: (1) Gradients of samples with respect to parameters \u03b8 of Q can be easily obtained using the reparametrization trick proposed in [11], and (2) Various statistical shape models trained on 3D scanner data such as faces have the same multivariate normal latent distribution (see section 4.", "startOffset": 182, "endOffset": 186}, {"referenceID": 1, "context": "[2] we would like only a small subset of the latent variables to change for sequences of inputs corresponding to real-world events.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] first proposed using 3D laser scanned face data to learn a deformable 3D face model (3DMM).", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "In all our experiments, we train using the synthetic data obtained from the 3DMM model and test it out on real face data with ground truth 3D annotations (geometry and texture) obtained from [22].", "startOffset": 191, "endOffset": 195}, {"referenceID": 23, "context": "We used rmsprop[24] learning algorithm during training and set the meta learning rate to be equal to 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 26, "context": "Since the likelihood function is not available in closed-form, we can simply resort to approximate Bayesian computation (ABC) based MCMC[27].", "startOffset": 136, "endOffset": 140}, {"referenceID": 9, "context": "Alternatively, we can also use the learnt representations from our model to learn data-driven proposals similar to [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "works and apply a variant of the probabilistic approximate MCMC algorithm [27] to do inference.", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "[21], where a domain-specific decoder is used along with a generic encoder to do analysis-by-synthesis based generative modeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Neal [4] and Murray et al.", "startOffset": 5, "endOffset": 8}, {"referenceID": 19, "context": "[20] studied generative models with Gaussian priors having zero mean and arbitrary covariance structure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "We also quantitatively compare DC-IGN\u2019s latent representation with a pre-trained deep convolutional network (trained on Imagenet [25] version:imagenet-caffe-ref ).", "startOffset": 129, "endOffset": 133}, {"referenceID": 27, "context": "This experiment was first proposed by Yildrim et al[28] and we closely follow this procedure for all neural network baselines.", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "In order to calculate test accuracy of the same vs different task for all baseline models, we follow the same-vsdifferent test procedure first proposed in [28].", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "We searched for a threshold correlation \u2208 [1, 1] such that the any given model\u2019s performance will be highest with respect to ground truth.", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "We searched for a threshold correlation \u2208 [1, 1] such that the any given model\u2019s performance will be highest with respect to ground truth.", "startOffset": 42, "endOffset": 48}, {"referenceID": 10, "context": "within a variational autoencoder formulation, our model can be jointly trained using back-propagation using the stochastic variational objective function [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "There has been recent work[14] on trying to approximate discrete distribution by using particle based variational approximations.", "startOffset": 26, "endOffset": 30}], "year": 2017, "abstractText": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [11]. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model\u2019s efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.", "creator": "LaTeX with hyperref package"}}}