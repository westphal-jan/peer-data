{"id": "1503.03244", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2015", "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences", "abstract": "Semantic matching is of central importance to many natural language tasks \\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.", "histories": [["v1", "Wed, 11 Mar 2015 09:46:36 GMT  (943kb,D)", "http://arxiv.org/abs/1503.03244v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["baotian hu", "zhengdong lu", "hang li", "qingcai chen"], "accepted": true, "id": "1503.03244"}, "pdf": {"name": "1503.03244.pdf", "metadata": {"source": "CRF", "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences", "authors": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "emails": ["baotianchina@gmail.com", "qingcai.chen@hitsz.edu.cn", "lu.zhengdong@huawei.com", "hangli.hl@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "The mapping of two potentially heterogeneous linguistic objects is central to many natural language applications [28, 2]. It generalizes the conventional notion of similarity (e.g. in paraphrase identification [19]) or relevance (e.g. in querying information [27]), since it aims to model the correspondence between \"linguistic objects\" of different nature at different levels of abstraction. Examples of this are the top k re-rankings in machine translation (e.g. comparing the meanings of a French sentence and an English sentence [5]) and dialogue (e.g. assessing the adequacy of an answer to a given utterance [26]. Natural language sentences have complex structures, both sequential and hierarchical, which are indispensable for understanding. A successful sentence matching algorithm must therefore capture not only the internal structures of sentences, but also the rich patterns in their interactions."}, {"heading": "2 Convolutional Sentence Model", "text": "In fact, most people are in a position to put themselves in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in fact, in which they, in the world, in which they, in which they, in the world, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which"}, {"heading": "2.1 Some Analysis on the Convolutional Architecture", "text": "The conventional unit in which one connects to Max Pooling can act as a complex operator with a recessive autoencoder [21]. Figure 2 gives an example of what can happen in the first two shifts. \"The cat was sitting on the mat,\" he says, \"but we have a drastic selection of parameters.\" Apart from turning off some elements in W (1), he needs to focus on different segments."}, {"heading": "3 Convolutional Matching Models", "text": "Based on the discussion in Section 2, we propose two related revolutionary architectures (ARC-I and ARC-II) to merge two sets."}, {"heading": "3.1 Architecture-I (ARC-I)", "text": "Architecture-I (ARC-I) takes a conventional approach, as shown in Figure 3: It first finds the representation of each sentence and then compares the representation of the two sentences with a multi-layered perceptron (MLP) [3]. It is essentially the Siamese architecture introduced in [2, 11], which has been applied as a non-linear similarity function to different tasks [23]. Although ARC-I enjoys the flexibility that the revolutionary sentence model brings with it, it suffers from a disadvantage inherited from the Siamese architecture: it postpones the interaction between two sentences (in the final MLP) until their individual representation matures (in the folding model), and therefore runs the risk of losing details (e.g. a city name) that are important for the appropriate task in the representation of the sentences. In other words, in the run-up phase (prediction) the representation of each sentence is formed without knowledge of each other."}, {"heading": "3.2 Architecture-II (ARC-II)", "text": "In view of the disadvantage of Architecture-I, we propose that the Architecture-II (ARC-II), which is built directly on the interaction space between two sentences, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2, (2), (2), (2, (2), (2), (2), (2, (2), (2, (2), (2), (2, (2), (2), (2, (2), (2, (2), (2), (2), (2, (2, (2), (2), (, (2), (2, (2), (2), (2), (2), (2), (2), (2), (, (2), (, (, (2), (2), (2), (2), (, (, (2), (2), (, ("}, {"heading": "4 Training", "text": "Let's assume that we get the following triples (x, y +, y \u2212) from the oracle, where x is better with y + than with y \u2212. We have the following rank-based loss as a target: e (x, y \u2212;) = max (0, 1 + s (x, y \u2212) \u2212 s (x, y +), where s (x, y) contains the appropriate values for (x, y) and the parameters for the folding layers and those for the MLP. Optimization is relatively simple for both architectures with the default backward propagation. The gating function (see section 2) can be easily incorporated into the gradients by subtracting the contribution from the folding units that are disabled by the gating function."}, {"heading": "5 Experiments", "text": "We report on the performance of the proposed models on three matching tasks of different nature and compare them with those of other competing models. Among them, the first two tasks (namely sentence completion and tweet-response matching) are the matching of speech objects of heterogeneous nature, while the third (paraphrase identification) is a natural example of the matching of homogeneous objects. In addition, the three tasks include two languages, different types of matching and different writing styles, proving the broad applicability of the proposed models."}, {"heading": "5.1 Competitor Methods", "text": "\u2022 WORDEMBED: We first represent each short text as the sum of the embedding of the words contained therein. The correspondence of two short texts is calculated with an MLP, with the two documents embedded as input. \u2022 DEEPMATCH: We take the matching model in [13] and train it on our data sets with 3 hidden levels and 1,000 hidden nodes in the first hidden level. \u2022 URAE + MLP: We use the recursive unfolding autoencode [19] 3 to obtain a 100-dimensional vector representation of each set and to put an MLP at the top as in WORDEMBED. \u2022 SENNA + MLP / SIM: We use the SENNA type sentence model for sentence representation. 2Our other experiments suggest that performance can be further improved with larger windows."}, {"heading": "5.2 Experiment I: Sentence Completion", "text": "clauses (with 8 x 28 words) divided by a comma and using the first sentence as SX and the second as SY. The task is then to restore the original second sentence for each given first sentence. Consistency is considered heterogeneous here, since the relationship between the two is not symmetrical, both at the lexical and semantic levels. We deliberately complicate the task by using negative second clauses similar to the original clauses 4, both in training and in the exam. A representative example is the following: Model P @ 1 (%)"}, {"heading": "SX : Although the state has only four votes in the Electoral College,", "text": "S + Y: Their loss would be a symbolic blow to Republican presidential candidate Bob Dole.S \u2212 Y: but he failed to garner enough votes to override an expected veto by President Clinton. All models are trained on 3 million triples (out of 600,000 positive pairs) and tested on 50K positive pairs, each accompanied by four negatives, with results obtained in Table 1. The two proposed models get almost half of the correct5, by a large margin over other set models and models without explicit sequence modeling. ARC-II performs significantly better than ARC-I and demonstrates the power of joint matching and sentence meaning modeling. As another revolutionary model, SENNA + MLP performs fairly well in this task, although it still lags behind the proposed revolutionary architectures because it is too flat to properly model the sentence. It is a bit surprising that URAE comes last in this task, which may be caused by the fact that the word model, including the judgment of Reuters, is not exercised on the basis of representation."}, {"heading": "5.3 Experiment II: Matching A Response to A Tweet", "text": "Compared to Experiment I, the writing style is obviously freer and more informal. For each positive pair, we find ten random answers as negative examples, resulting in 45 million triples for training; an example (translated into English) is listed below, with SX representing the tweet, S + Y representing the original answer, and S \u2212 Y representing the randomly selected answer: SX: Damn, I'll have to work overtime this weekend! S + Y: Try to have a rest buddy. S \u2212 Y: It's hard to find a job, better start polishing your resume. We keep 300K original pairs (tweet, answer) and test the appropriate model for its ability to select the original answer from four random negatives, with results reported in Table 2. This task is slightly easier than Experiment I, with more training instances and purely random negatives. It requires less grammatical rigor, but more detailed ARC models with large-format comparison models."}, {"heading": "5.4 Experiment III: Paraphrase Identification", "text": "Paraphrase identification is aimed at determining whether two sentences have the same meaning, a problem that serves as a touchstone for understanding natural language. This experiment also involves testing our methods for matching homogeneous objects, using the benchmark MSRP dataset [17], which contains 4,076 instances for training and 1,725 for testing. We use all training instances and report on the test performance from the outset. As previously mentioned, our model is not specifically tailored to synonymy modeling and usually requires \u2265 100K instances to work cheaply. Nevertheless, our generic matching models still manage to achieve reasonably well an accuracy and F1 value that comes close to the 2008 best value based on handmade features [17], but is still significantly lower than the prior art (76.8% / 83.6%) achieved with the deployment of RAE and other features designed for this task [19]."}, {"heading": "5.5 Discussions", "text": "Its superiority over ARC-I, however, is less significant if the sentences have a deep grammatical structure and the matching is less dependent on local matching patterns, as in Experiment I. This raises the interesting question of how to balance the representation of matching and the representation of objects, and whether we can guide the learning process through something like curriculum learning [4] Another important observation is that revolutionary models (ARC-I & II, SENNA + MLP) perform better than sack-of-words models, which indicates the importance of using sequential structures in understanding and matching sentences. Interestingly, as our other experiments have shown, ARC-I and ARC-II can acquire some ability to determine whether the words in a given sentence are in the correct order (with about 60% accuracy for both)."}, {"heading": "6 Related Work", "text": "The concordance of structured objects rarely goes beyond estimating the similarity of objects in the same area [23, 24, 19], with few exceptions such as [2, 18]. When dealing with linguistic objects, most methods still focus on searching for vector representations in a common latent space and calculating the concordance of score and internal product [18, 25]. Little work has been done on building a deep architecture based on the interaction space for text pairs, but it is largely based on a bag-of-words representation of text [13]. Our models refer to the long sentence representation workbook. Apart from the models with recursive character [15, 21, 19] (as discussed in Section 2.1), it is relatively common practice to use the sum of word embedding to represent a short text, mostly for classification [22]."}, {"heading": "7 Conclusion", "text": "Empirical studies show that our models can outperform competitors in a variety of matching tasks. Acknowledgements: B. Hu and Q. Chen are partially supported by the National Natural Science Foundation of China 61173075. Z. Lu and H. Li are partially supported by the China National 973 Project 2014CB340301."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "G. Penn"], "venue": "Proceedings of ICASSP", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["B. Antoine", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "Machine Learning, 94(2):233\u2013259", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., 2(1):1\u2013127", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradourand", "R. Collobert", "J. Weston"], "venue": "Proceedings of ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "S.A.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "Computational linguistics, 19(2):263\u2013311", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, 12:2493\u20132537", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "Proceedings of ICASSP", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "CoRR, abs/1207.0580", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "Proceedings of ACL, Baltimore and USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proceedings of EMNLP", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional networks for images", "author": ["Y. LeCun", "Y. Bengio"], "venue": "speech and time series. The Handbook of Brain Theory and Neural Networks, 3361", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Y. Lewis", "David D", "Yang", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A deep architecture for matching short texts", "author": ["Z. Lu", "H. Li"], "venue": "Advances in NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t"], "venue": "Proceedings of INTER- SPEECH", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Overfitting in neural nets: Backpropagation", "author": ["C. Rich", "L. Steve", "G. Lee"], "venue": "conjugate gradient, and early stopping. In Advances in NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Paraphrase identification with lexico-syntactic graph subsumption", "author": ["V. Rus", "P.M. McCarthy", "M.C. Lintean", "D.S. McNamara", "A.C. Graesser"], "venue": "Proceedings of FLAIRS Conference", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": "Proceedings of WWW", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "A.Y. Ng"], "venue": "Advances in NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["R. Socher", "C.C. Lin", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of EMNLP", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "On dataless hierarchical text classification", "author": ["Y. Song", "D. Roth"], "venue": "Proceedings of AAAI", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid deep learning for face verification", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "Proceedings of ICCV", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph kernels", "author": ["S.V.N. Vishwanathan", "N.N. Schraudolph", "R. Kondor", "K.M. Borgwardt"], "venue": "Journal of Machine Learning Research(JMLR), 11:1201\u20131242", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling semantic relevance for question-answer pairs in web social communities", "author": ["B. Wang", "X. Wang", "C. Sun", "B. Liu", "L. Sun"], "venue": "Proceedings of ACL", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "A dataset for research on short-text conversations", "author": ["H. Wang", "Z. Lu", "H. Li", "E. Chen"], "venue": "Proceedings of EMNLP, Seattle, Washington, USA", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning bilinear model for matching queries and documents", "author": ["W. Wu", "Z. Lu", "H. Li"], "venue": "The Journal of Machine Learning Research, 14(1):2519\u20132548", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Retrieval models for question and answer archives", "author": ["X. Xue", "J. Jiwoon", "C.W. Bruce"], "venue": "Proceedings of SIGIR \u201908, New York, NY, USA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Semantic matching is of central importance to many natural language tasks [2, 28].", "startOffset": 74, "endOffset": 81}, {"referenceID": 27, "context": "Semantic matching is of central importance to many natural language tasks [2, 28].", "startOffset": 74, "endOffset": 81}, {"referenceID": 27, "context": "Matching two potentially heterogenous language objects is central to many natural language applications [28, 2].", "startOffset": 104, "endOffset": 111}, {"referenceID": 1, "context": "Matching two potentially heterogenous language objects is central to many natural language applications [28, 2].", "startOffset": 104, "endOffset": 111}, {"referenceID": 18, "context": ", in paraphrase identification [19]) or relevance (e.", "startOffset": 31, "endOffset": 35}, {"referenceID": 26, "context": ", in information retrieval[27]), since it aims to model the correspondence between \u201clinguistic objects\u201d of different nature at different levels of abstractions.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": ", comparing the meanings of a French sentence and an English sentence [5]) and dialogue (e.", "startOffset": 70, "endOffset": 73}, {"referenceID": 25, "context": ", evaluating the appropriateness of a response to a given utterance[26]).", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "Towards this end, we propose deep neural network models, which adapt the convolutional strategy (proven successful on image [11] and speech [1]) to natural language.", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "Towards this end, we propose deep neural network models, which adapt the convolutional strategy (proven successful on image [11] and speech [1]) to natural language.", "startOffset": 140, "endOffset": 143}, {"referenceID": 12, "context": "This is part of our continuing effort1 in understanding natural language objects and the matching between them [13, 26].", "startOffset": 111, "endOffset": 119}, {"referenceID": 25, "context": "This is part of our continuing effort1 in understanding natural language objects and the matching between them [13, 26].", "startOffset": 111, "endOffset": 119}, {"referenceID": 10, "context": "As in most convolutional models [11, 1], we use convolution units with a local \u201creceptive field\u201d and shared weights, but we design a large feature map to adequately model the rich structures in the composition of words.", "startOffset": 32, "endOffset": 39}, {"referenceID": 0, "context": "As in most convolutional models [11, 1], we use convolution units with a local \u201creceptive field\u201d and shared weights, but we design a large feature map to adequately model the rich structures in the composition of words.", "startOffset": 32, "endOffset": 39}, {"referenceID": 6, "context": ", Sigmoid or Relu [7])", "startOffset": 18, "endOffset": 21}, {"referenceID": 20, "context": "The convolutional unit, when combined with max-pooling, can act as the compositional operator with local selection mechanism as in the recursive autoencoder [21].", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "Relation to Recursive Models Our convolutional model differs from Recurrent Neural Network (RNN, [15]) and Recursive Auto-Encoder (RAE, [21]) in several important ways.", "startOffset": 97, "endOffset": 101}, {"referenceID": 20, "context": "Relation to Recursive Models Our convolutional model differs from Recurrent Neural Network (RNN, [15]) and Recursive Auto-Encoder (RAE, [21]) in several important ways.", "startOffset": 136, "endOffset": 140}, {"referenceID": 20, "context": "First, unlike RAE, it does not take a single path of word/phrase composition determined either by a separate gating function [21], an external parser [19], or just natural sequential order [20].", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "First, unlike RAE, it does not take a single path of word/phrase composition determined either by a separate gating function [21], an external parser [19], or just natural sequential order [20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 19, "context": "First, unlike RAE, it does not take a single path of word/phrase composition determined either by a separate gating function [21], an external parser [19], or just natural sequential order [20].", "startOffset": 189, "endOffset": 193}, {"referenceID": 19, "context": "However, unlike recursive models [20, 21], the convolutional architecture has a fixed depth, which bounds the level of composition it could do.", "startOffset": 33, "endOffset": 41}, {"referenceID": 20, "context": "However, unlike recursive models [20, 21], the convolutional architecture has a fixed depth, which bounds the level of composition it could do.", "startOffset": 33, "endOffset": 41}, {"referenceID": 17, "context": "Relation to \u201cShallow\u201d Convolutional Models The proposed convolutional sentence model takes simple architectures such as [18, 10] (essentially the same convolutional architecture as SENNA [6]), which consists of a convolution layer and a max-pooling over the entire sentence for each feature map.", "startOffset": 120, "endOffset": 128}, {"referenceID": 9, "context": "Relation to \u201cShallow\u201d Convolutional Models The proposed convolutional sentence model takes simple architectures such as [18, 10] (essentially the same convolutional architecture as SENNA [6]), which consists of a convolution layer and a max-pooling over the entire sentence for each feature map.", "startOffset": 120, "endOffset": 128}, {"referenceID": 5, "context": "Relation to \u201cShallow\u201d Convolutional Models The proposed convolutional sentence model takes simple architectures such as [18, 10] (essentially the same convolutional architecture as SENNA [6]), which consists of a convolution layer and a max-pooling over the entire sentence for each feature map.", "startOffset": 187, "endOffset": 190}, {"referenceID": 2, "context": "1 Architecture-I (ARC-I) Architecture-I (ARC-I), as illustrated in Figure 3, takes a conventional approach: It first finds the representation of each sentence, and then compares the representation for the two sentences with a multi-layer perceptron (MLP) [3].", "startOffset": 255, "endOffset": 258}, {"referenceID": 1, "context": "It is essentially the Siamese architecture introduced in [2, 11], which has been applied to different tasks as a nonlinear similarity function [23].", "startOffset": 57, "endOffset": 64}, {"referenceID": 10, "context": "It is essentially the Siamese architecture introduced in [2, 11], which has been applied to different tasks as a nonlinear similarity function [23].", "startOffset": 57, "endOffset": 64}, {"referenceID": 22, "context": "It is essentially the Siamese architecture introduced in [2, 11], which has been applied to different tasks as a nonlinear similarity function [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": "(5) This could go on for more layers of 2D convolution and 2D max-pooling, analogous to that of convolutional architecture for image input [11].", "startOffset": 139, "endOffset": 143}, {"referenceID": 18, "context": "This pooling strategy resembles the dynamic pooling in [19] in a similarity learning context, but with two distinctions: 1) it happens on a fixed architecture and 2) it has much richer structure than just similarity.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "For regularization, we find that for both architectures, early stopping [16] is enough for models with medium size and large training sets (with over 500K instances).", "startOffset": 72, "endOffset": 76}, {"referenceID": 7, "context": "For small datasets (less than 10k training instances) however, we have to combine early stopping and dropout [8] to deal with the serious overfitting problem.", "startOffset": 109, "endOffset": 112}, {"referenceID": 13, "context": "We use 50-dimensional word embedding trained with the Word2Vec [14]: the embedding for English words (Section 5.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "We use ReLu [7] as the activation function for all of models (convolution and MLP), which yields comparable or better results to sigmoid-like functions, but converges faster.", "startOffset": 12, "endOffset": 15}, {"referenceID": 12, "context": "The matching score of two short-texts are calculated with an MLP with the embedding of the two documents as input; \u2022 DEEPMATCH: We take the matching model in [13] and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer; \u2022 URAE+MLP: We use the Unfolding Recursive Autoencoder [19]3 to get a 100dimensional vector representation of each sentence, and put an MLP on the top as in WORDEMBED; \u2022 SENNA+MLP/SIM: We use the SENNA-type sentence model for sentence representation;", "startOffset": 158, "endOffset": 162}, {"referenceID": 18, "context": "The matching score of two short-texts are calculated with an MLP with the embedding of the two documents as input; \u2022 DEEPMATCH: We take the matching model in [13] and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer; \u2022 URAE+MLP: We use the Unfolding Recursive Autoencoder [19]3 to get a 100dimensional vector representation of each sentence, and put an MLP on the top as in WORDEMBED; \u2022 SENNA+MLP/SIM: We use the SENNA-type sentence model for sentence representation;", "startOffset": 318, "endOffset": 322}, {"referenceID": 11, "context": "Basically, we take a sentence from Reuters [12]with two \u201cbalanced\u201d clauses (with 8\u223c 28 words) divided by one comma, and use the first clause as SX and the second as SY .", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "5 million original (tweet, response) pairs collected from Weibo, a major Chinese microblog service [26].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "Here we use the benchmark MSRP dataset [17], which contains 4,076 instances for training and 1,725 for test.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Nevertheless, our generic matching models still manage to perform reasonably well, achieving an accuracy and F1 score close to the best performer in 2008 based on hand-crafted features [17], but still significantly lower than the state-of-the-art (76.", "startOffset": 185, "endOffset": 189}, {"referenceID": 18, "context": "6%), achieved with unfolding-RAE and other features designed for this task [19].", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "This therefore raises the interesting question about how to balance the representation of matching and the representations of objects, and whether we can guide the learning process through something like curriculum learning [4].", "startOffset": 224, "endOffset": 227}, {"referenceID": 13, "context": "We noticed that simple sum of embedding learned via Word2Vec [14] yields reasonably good results on all three tasks.", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "This is in contrast with other bag-of-words models like DEEPMATCH [13].", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain [23, 24, 19], with few exceptions like [2, 18].", "startOffset": 103, "endOffset": 115}, {"referenceID": 23, "context": "Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain [23, 24, 19], with few exceptions like [2, 18].", "startOffset": 103, "endOffset": 115}, {"referenceID": 18, "context": "Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain [23, 24, 19], with few exceptions like [2, 18].", "startOffset": 103, "endOffset": 115}, {"referenceID": 1, "context": "Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain [23, 24, 19], with few exceptions like [2, 18].", "startOffset": 142, "endOffset": 149}, {"referenceID": 17, "context": "Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain [23, 24, 19], with few exceptions like [2, 18].", "startOffset": 142, "endOffset": 149}, {"referenceID": 17, "context": "When dealing with language objects, most methods still focus on seeking vectorial representations in a common latent space, and calculating the matching score with inner product[18, 25].", "startOffset": 177, "endOffset": 185}, {"referenceID": 24, "context": "When dealing with language objects, most methods still focus on seeking vectorial representations in a common latent space, and calculating the matching score with inner product[18, 25].", "startOffset": 177, "endOffset": 185}, {"referenceID": 12, "context": "Few work has been done on building a deep architecture on the interaction space for texts-pairs, but it is largely based on a bag-of-words representation of text [13].", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "Aside from the models with recursive nature [15, 21, 19] (as discussed in Section 2.", "startOffset": 44, "endOffset": 56}, {"referenceID": 20, "context": "Aside from the models with recursive nature [15, 21, 19] (as discussed in Section 2.", "startOffset": 44, "endOffset": 56}, {"referenceID": 18, "context": "Aside from the models with recursive nature [15, 21, 19] (as discussed in Section 2.", "startOffset": 44, "endOffset": 56}, {"referenceID": 21, "context": "1), it is fairly common practice to use the sum of word-embedding to represent a short-text, mostly for classification [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 5, "context": "In addition to [6, 18], there is a very recent model on sentence representation with dynamic convolutional neural network [9].", "startOffset": 15, "endOffset": 22}, {"referenceID": 17, "context": "In addition to [6, 18], there is a very recent model on sentence representation with dynamic convolutional neural network [9].", "startOffset": 15, "endOffset": 22}, {"referenceID": 8, "context": "In addition to [6, 18], there is a very recent model on sentence representation with dynamic convolutional neural network [9].", "startOffset": 122, "endOffset": 125}], "year": 2015, "abstractText": "Semantic matching is of central importance to many natural language tasks [2, 28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layerby-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.", "creator": "LaTeX with hyperref package"}}}