{"id": "1503.03585", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2015", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "abstract": "A central unsolved problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Previous approaches to this problem are subject to tradeoffs between flexibility and tractability. We develop a promising approach that simultaneously achieves both. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps.", "histories": [["v1", "Thu, 12 Mar 2015 04:51:37 GMT  (5395kb,D)", "https://arxiv.org/abs/1503.03585v1", null], ["v2", "Thu, 2 Apr 2015 06:48:02 GMT  (5397kb,D)", "http://arxiv.org/abs/1503.03585v2", null], ["v3", "Wed, 29 Apr 2015 06:00:20 GMT  (5403kb,D)", "http://arxiv.org/abs/1503.03585v3", null], ["v4", "Wed, 13 May 2015 01:57:49 GMT  (5409kb,D)", "http://arxiv.org/abs/1503.03585v4", null], ["v5", "Wed, 20 May 2015 03:19:10 GMT  (4586kb,D)", "http://arxiv.org/abs/1503.03585v5", null], ["v6", "Thu, 9 Jul 2015 16:16:33 GMT  (6085kb,D)", "http://arxiv.org/abs/1503.03585v6", null], ["v7", "Tue, 21 Jul 2015 19:44:20 GMT  (6092kb,D)", "http://arxiv.org/abs/1503.03585v7", null], ["v8", "Wed, 18 Nov 2015 21:50:51 GMT  (6095kb,D)", "http://arxiv.org/abs/1503.03585v8", null]], "reviews": [], "SUBJECTS": "cs.LG cond-mat.dis-nn q-bio.NC stat.ML", "authors": ["jascha sohl-dickstein", "eric a weiss", "niru maheswaranathan", "surya ganguli"], "accepted": true, "id": "1503.03585"}, "pdf": {"name": "1503.03585.pdf", "metadata": {"source": "META", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics ", "authors": ["Jascha Sohl-Dickstein", "Eric A. Weiss", "Niru Maheswaranathan", "Surya Ganguli"], "emails": ["JASCHA@STANFORD.EDU", "EAWEISS@BERKELEY.EDU", "NIRUM@STANFORD.EDU", "SGANGULI@STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "Historically, probabilistic models have suffered from a trade-off between two conflicting objectives: tractability and flexibility; models that are tractable can be analytically evaluated and easily adapted to data (e.g. a Gaussian or Laplace); Proceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015; JMLR: W & CP Volume 37; these models are unable to adequately describe structure in rich data sets; on the other hand, models that are flexible can be shaped to fit into arbitrary data; for example, we can define models based on arbitrary (non-negative) functionalities (x) that yield the flexible distribution p (x) = (x) Z, where Z is a normalization constant; however, the calculation of this normalization constant (T) is generally intractable; the evaluation, formation, or extraction of samples from such flexible models typically requires a very expensive Monte Carlo process."}, {"heading": "1.1. Diffusion probabilistic models", "text": "We present a novel method for defining probabilistic models that allows: 1. extreme flexibility in the model structure, 2. exact sampling, 1. non-parametric methods can be considered as a smooth transition between tractable and flexible models. For example, a non-parametric Gaussian mixing model will represent a small amount of data using a single Gaussian, but can represent infinite data as a mixture of an infinite number of Gaussian models.ar Xiv: 150 3.03 585v 8 [cs.L G] 13. simple multiplication with other distributions, e.g. to calculate a posterior, and4. the model logic probability and the probability of individual states that can be evaluated.Our method uses a Markov chain to gradually transform one distribution into another, an idea used in non-uniform statistical physics (Jarzynski, 1997) and the sequential stimulus probability of Neal 2001 (Monte Carlo)."}, {"heading": "1.2. Relationship to other work", "text": "The awake-sleep algorithms (Hinton, 1995; Dayan et al., 1995) introduced the idea of training networks and generative probability models against each other, an approach that has remained largely unexplored for almost two decades, with a few exceptions (Sminchisescu et al., 2006; Kavukcuoglu et al., 2010).Variational learning and inference algorithms have been developed that allow for a flexible generative model and posterior distribution of latent variables that are trained directly against each other.The variable distribution in these papers is similar to the one used in our training goals and in the earlier work of (Sminchisescu et al., 2006).However, our motivation and model form are both quite different, and the current work keeps the following technical advantages and benefits we are developing based on these:"}, {"heading": "2. Algorithm", "text": "Our goal is to define a diffusion process that converts complex data distributions into a simple, traceable distribution, and then to learn an apocalyptic reversal of this diffusion process that defines our generative model distribution (see Figure 1). First, we describe the process of forward and inference diffusion, then we show how the reverse generative diffusion process can be trained and used to evaluate probabilities, and we derive entropy boundaries for the reverse process, and show how the learned distributions can be multiplied by every second distribution (as would happen, for example, when calculating a posterior denoising an image)."}, {"heading": "2.1. Forward Trajectory", "text": "We label the data distribution q (x (0)). The data distribution is gradually converted into a well-restrained (analytically comprehensible) distribution \u03c0 (y) by repeatedly using a Markov diffusion core T\u03c0 (y | y \u2032; \u03b2) for \u03c0 (y), where \u03b2 is the diffusion rate, \u03c0 (y) = ady \u2032 T\u03c0 (y | y \u2032; \u03b2) \u03c0 (y \u2032) (1) q (x (t) | x (t \u2212 1)) = T\u03c0 (x (t) | x (t \u2212 1); \u03b2t). (2) The forward step corresponding to the data distribution and the execution of T diffusion steps is thusq (x (0 \u00b7 T)) = q (x (0)) T \u0394t = 1 q (t) | x (t \u2212 1) For the experiments shown below, q (x) | x (t \u2212 1)) corresponds to either the Gaussian diffusion table or the diffusion table."}, {"heading": "2.2. Reverse Trajectory", "text": "The generative distribution is trained to describe the same path, but vice versa, p (x (T)) = \u03c0 (x (T)) (4) p (x (0 \u00b7 \u00b7 T)) = p (x (T))) T-t = 1 p (x (t \u2212 1) | x (t))). (5) For both Gaussian and binomial diffusion, for continuous diffusion (limit of small step size \u03b2), the inversion of the diffusion process has the same functional form as the forward process (Feller, 1949). Since q (t) | x (t \u2212 1) is a Gaussian (binomical) distribution, and if \u00df is small, q (t \u2212 1) | x (t)) is also a Gaussian (binomical) distribution. The longer the trajectory is, the smaller the diffusion rate \u03b2 can be."}, {"heading": "2.3. Model Probability", "text": "The probability that the generative model assigns to the data is isp (x (0)) = \u00b7 \u00b7 q (1 \u00b7 \u00b7 T) p (x (0 \u00b7 \u00b7 T)). (6) Naively, this integral is insoluble - but using a keyword from the annealed importance test and the Jarzynski equality, we instead evaluate the relative probability of the forward and backward curves averaged over the forward curves, p (x (0)) = \"dx (1 \u00b7 \u00b7 T) p (x (0 \u00b7 \u00b7 T))) q (1) (x (1 \u00b7 \u00b7 T) | x (0) q (x (1 \u00b7 \u00b7 T), averaged over the forward curves, p (7) =\" dx (1 \u00b7 \u00b7 \u00b7 \u00b7 T) q (1 \u00b7 \u00b7 \u00b7 \u00b7 T) q (1) (x) p (x (0 \u00b7 \u00b7 T)."}, {"heading": "2.4. Training", "text": "In fact, the number of those who are able to hide, able to put themselves in a position, able to put themselves in a position, in a position in which they are able to put themselves in a position, in a position to put themselves in a position, in a position in which they are able to put themselves in a position, in a position in which they are able to put themselves in a position, in a position to put themselves in a position, in a position in which they are able to put themselves in a position, in a position in which they are able to entangle themselves, in a position in which they are able to survive themselves."}, {"heading": "2.5. Multiplying Distributions, and Computing Posteriors", "text": "Tasks such as calculating a posterior signal for denocialization or deduction of missing values require multiplying the model distribution p (x (0)) with a second entry or limited positive function r (x (0)), which generates a new distribution p (x (0)) p (x (0))) r (x (0)). Multiplying distributions is costly and difficult for many techniques, including variational autoencoders, GSNs, NADEs, and most graphical models. However, in a diffusion model, it is straightforward, since the second distribution can either be treated as a minor disturbance at each step of the diffusion process, or is often multiplied exactly with each diffusion step. Figures 3 and 5 show the use of a diffusion model to perform denosis and inpainting of natural images. The following sections describe how2Recent experiments suggest that it is just as effective to use the same diffusion context instead of the same modelling schedule for diffusion."}, {"heading": "2.5.1. MODIFIED MARGINAL DISTRIBUTIONS", "text": "First, we multiply each of the intermediate distributions by a corresponding function r (x (t). We use a tilde above a distribution or a Markov transition to indicate that it belongs to such a modified orbit. p (0 \u00b7 \u00b7 \u00b7 T) is the modified reverse orbit that begins at the distribution p (x (T) = 1 Z-T p (x (T))) r (x (T))) and continues through the sequence of intermediate distributions p (x (t) = 1 Z-t p (x (t))), (16), where Z-t is the normalizing constant for the tth intermediate distribution."}, {"heading": "2.5.2. MODIFIED DIFFUSION STEPS", "text": "The Markov Kernel p (t) Z (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t) S (t"}, {"heading": "2.6. Entropy of Reverse Process", "text": "Since the forward speed is known, we can derive upper and lower limits on the conditional entropy of each step in the reverse orbit and thus on the log probability, Hq (X (t \u2212 1) | X (t \u2212 1)) + Hq (X (t \u2212 1) | X (0) \u2212 Hq (X (t) | X (0))) \u2264 Hq (X (t \u2212 1) | X (t) \u2264 Hq (X (t) | X (t \u2212 1))), (24), whereby both the upper and the lower limits depend only on q (x (1 \u00b7 T) | x (0) and can be calculated analytically."}, {"heading": "3. Experiments", "text": "In all cases, objective function and gradient were calculated using Theano (Bergstra & Breuleux, 2010) and model training was performed using SFO (Sohl-Dickstein et al., 2014), with the exception of CIFAR-10. Results of CIFAR-10 were calculated using Theano (Bergstra & Breuleux, 2010) An earlier version of this paper reported higher log probability limits at CIFAR-10. These were the results of the model that learned 8-bit quantification of pixel values in the CIFAR-10 database. Log probability limits specified here refer to data previously processed by adding uniform noise to remove pixel quantization as recommended in (Theis et al., 2015)."}, {"heading": "3.1. Toy Problems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1. SWISS ROLL", "text": "Model Log Likelihood Dead LeavesMCGSM 1,244 bits / pixel Diffusion 1,489 bits / pixelMNIST Stacked CAE 174 \u00b1 2.3 bits DBN 199 \u00b1 2.9 bits Deep GSN 309 \u00b1 1.6 bits Diffusion 317 \u00b1 2.7 bits Adversarial net 325 \u00b1 2.9 bits Perfect model 349 \u00b1 3.3 bitsTable 2. Log probability comparisons with other algorithms. Dead LeavesMNIST images were evaluated with identical training and test data as in (Theis et al., 2012). MNIST logs probability comparisons with particles used in comparison with other particle models."}, {"heading": "3.1.2. BINARY HEARTBEAT DISTRIBUTION", "text": "A diffusion probability model was trained using simple binary sequences of length 20, where one occurs every five times and the rest of the vessels are 0, using a multi-layered perceptron to generate the Bernoulli rates fb (x (t), t) of the reverse trajectory. Log probability under the true distribution is log2 (1 5) = \u2212 2,322 bits per sequence. As shown in Figure 2 and Table 1, learning was almost perfect. See Appendix Section D.1.2 for more details."}, {"heading": "3.2. Images", "text": "We trained Gaussian diffusion probability models on multiple image datasets. The multi-scale revolutionary architecture shared by these experiments is described in Appendix Section D.2.1 and illustrated in Figure D.1."}, {"heading": "3.2.1. DATASETS", "text": "MNIST To allow direct comparison with previous work on a simple dataset, we have trained ourselves on MNIST digits (LeCun & Cortes, 1998).The probabilities of the protocol relative to (Bengio et al., 2012; Bengio & Thibodeau-Laufer, 2013; Goodfellow et al., 2014) are listed in Table 2. Examples of the MNIST model are listed in Appendix Figure App.1.Our training algorithm provides an asymptotically consistent lower limit on the likelihood of the protocol. However, most previous results of the continuous MNIST protocol are based on estimates of the Parzen window calculated from model samples. For this comparison, we estimate the likelihood of the MNIST protocol using the parameter window codes published with (Goodfellow et al., 2014)."}, {"heading": "4. Conclusion", "text": "We have introduced a novel probability distribution modeling algorithm that enables accurate scanning and evaluation of probabilities and demonstrates its effectiveness on a variety of toy and real-world datasets, including sophisticated natural image datasets. For each of these tests, we used a similar basic algorithm that shows that our method can accurately model a variety of distributions. Most existing density estimation techniques have to renounce modeling performance to remain comprehensible and efficient, and scanning or evaluation is often extremely costly. At the core of our algorithm is estimating the reversal of a Markov diffusion chain that maps data to a sound distribution; since the number of steps is large, the reversal distribution of each diffusion step becomes simple and easy to estimate. The result is an algorithm that is suitable for any data distribution, but remains tractable, can be taken from precise samples and evaluated, and is easy to manipulate with conditional and post-manipulation."}, {"heading": "Acknowledgements", "text": "We thank Lucas Theis, Subhaneil Lahiri, Ben Poole, Diederik P. Kingma, Taco Cohen, Philip Bachman and Aa \ufffd ron van den Oord for the extremely helpful discussion and Ian Goodfellow for the Parzen Window Code. We thank the Khan Academy and the Office of Naval Research for funding Jascha Sohl-Dickstein, and we thank the Office of Naval Research and the Burroughs-Wellcome, Sloan and James S. McDonnell Foundations for funding Surya Ganguli."}, {"heading": "A. Conditional Entropy Bounds Derivation", "text": "The conditional entropy Hq (t \u2212 1) | X (t)) of a step (q q q (q (q) (q (q) (q) (q) (q) (q (t \u2212 1)), X (t) = Hq (X (t \u2212 1), X (t \u2212 1))) (25) Hq (X (q \u2212 1) (q \u2212 1) | X (t \u2212 1) | X (t \u2212 1) (t \u2212 1) Hq (t \u2212 1) + Hq (X (t \u2212 1) + Hq (X (t \u2212 1) (26) Hq (t \u2212 1) (X \u2212 t) (X \u2212 1) (t \u2212 1) Hq (t \u2212 1) Hq (t \u2212 1) (t \u2212 1) Hq (t \u2212 1) (t \u2212 1) (t) (t \u2212 1) Hq (t \u2212 1) X (t) (t \u2212 1) | X (t) (1 \u2212 1) Hq (t \u2212 1) | X (t) (1 \u2212 1 \u2212 Hq (t) X (1 \u2212 1) X (t) | 1 \u2212 Hq (t) X (1 \u2212 1 \u2212 X (t) | 1 \u2212 X (t)) (1 \u2212 X (1 \u2212 1 \u2212 X (t)) | 1 \u2212 X (1 \u2212 X (1 \u2212 t)) | 1 \u2212 X (1 \u2212 X (1 \u2212 t) | 1 \u2212 X (1 \u2212 X (1 \u2212 t)) | 1 \u2212 X (1 \u2212 X (1 \u2212 t) \u2212 X (1 \u2212 t) | 1 \u2212 X (1 \u2212 t) \u2212 X (1 \u2212 t) | 1 \u2212 t (x (1 \u2212 t) | 1 \u2212 t (1 \u2212 t) (x (1 \u2212 t) \u2212 t) (1 \u2212 t) (x (1 \u2212 t) (x (1 \u2212 t) (1 \u2212 t) (x (1 \u2212 t) (1 \u2212 t) (x (1 \u2212 t) (1 \u2212 t) (x (1 \u2212 t) (1 \u2212 t) (x (1 \u2212 t) (1 \u2212 t) (1 \u2212 t) (x (1 \u2212 t) (x (x (1 \u2212 t) x (x (1 \u2212 t)"}, {"heading": "B. Log Likelihood Lower Bound", "text": "The bottom boundary on the log is isL \u2265 K (37) K = 2. \u00b7 T \u00b7 q (0 \u00b7 q (q) (1) (x (0 \u00b7 T)) q (x) q (x) q (x) p (x) p (x (T))) log [p (x) p (p) p (p) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (T) p (x) p (x) p (x) p (T) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (T) p (t) p (t) p (t) p (t) p (p) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t (t) p (t) p (t) p (t) p (t (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t (t) p (t) p (t) p (t) p (t) p (t) p (t (t) p (t) p (t) p (t (t) p (t) p (t) p (t) p (t) p (t (t) p (t) p (t) p (t) p (t (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t (t) p (t)"}, {"heading": "C. Perturbed Gaussian Transition", "text": "For notation, we can rewrite this in terms of energy functions, where He (y) = \u2212 log r (y), p (y) = log r (y), p (y) | x (t)), p (y), p (y), p (y), p (y), p (y), p (y). \u2212 exp [\u2212 E (y)] (54) E (y) = 12 (y \u2212 \u00b5) T (y \u2212 \u00b5) + Er (y). (55) If He (y) = \u2212 log r (y (y), p (t), p (t), p (t), p), p (y)] (54) E (y) T (y \u2212 \u00b5) + Er (y) + Er (y). (55) If He (y) is smooth relative to 12 (y), T (y \u2212 Er \u2212 p), then we can approximate it by (y)."}, {"heading": "D. Experimental Details", "text": "The generative model p (x \u00b7 T) consisted of 40 time steps of Gaussian diffusion initialized on an identity covariance of Gaussian distribution. A (normalized) radial base function network with a single hidden layer and 16 hidden units was used to generate the mean and covariance functions f\u00b5 (x (t), t) and a diago nal-f\u0440 (x (t), t) for the reverse trajectory. The top, readable layer for each function was learned independently for each time step, but for all other layers the weights were shared over all time steps and both functions. The top layer of the f\u03a3 (t), t), t) was learned to restrict between 0 and 1. As can be seen in Figure 1, the fast roll distribution was successfully learned."}], "references": [{"title": "Volumetric Semantic Segmentation Using Pyramid Context Features", "author": ["J.T. Barron", "M.D. Biggin", "P. Arbelaez", "D.W. Knowles", "S.V. Keranen", "J. Malik"], "venue": "IEEE International Conference on Computer Vision, pp. 3448\u20133455", "citeRegEx": "Barron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barron et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Thibodeau-Laufer"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio and Thibodeau.Laufer,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Thibodeau.Laufer", "year": 2013}, {"title": "Better Mixing via Deep Representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": "arXiv preprint arXiv:1207.4404,", "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra and Breuleux,? \\Q2010\\E", "shortCiteRegEx": "Bergstra and Breuleux", "year": 2010}, {"title": "Statistical Analysis of Non-Lattice Data", "author": ["J. Besag"], "venue": "The Statistician,", "citeRegEx": "Besag,? \\Q1975\\E", "shortCiteRegEx": "Besag", "year": 1975}, {"title": "GTM: The generative topographic mapping", "author": ["C. Bishop", "M. Svens\u00e9n", "C. Williams"], "venue": "Neural computation,", "citeRegEx": "Bishop et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1998}, {"title": "Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing", "author": ["Y. Burda", "R.B. Grosse", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Burda et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2014}, {"title": "The helmholtz machine", "author": ["P. Dayan", "G.E. Hinton", "R.M. Neal", "R.S. Zemel"], "venue": "Neural computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "NICE: Non-linear Independent Components", "author": ["L. Dinh", "D. Krueger", "Y. Bengio"], "venue": "Estimation. arXiv:1410.8516,", "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "A tutorial on Bayesian nonparametric models", "author": ["S.J. Gershman", "D.M. Blei"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "Gershman and Blei,? \\Q2012\\E", "shortCiteRegEx": "Gershman and Blei", "year": 2012}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["T. Gneiting", "A.E. Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gneiting and Raftery,? \\Q2007\\E", "shortCiteRegEx": "Gneiting and Raftery", "year": 2007}, {"title": "Generative Adversarial Nets", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Annealing between distributions by averaging moments", "author": ["R.B. Grosse", "C.J. Maddison", "R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grosse et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2013}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "The wake-sleep algorithm for unsupervised neural networks", "author": ["G.E. Hinton"], "venue": "Science,", "citeRegEx": "Hinton,? \\Q1995\\E", "shortCiteRegEx": "Hinton", "year": 1995}, {"title": "Estimation of non-normalized statistical models using score matching", "author": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hyv\u00e4rinen,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen", "year": 2005}, {"title": "Equilibrium free-energy differences from nonequilibrium measurements: A master-equation approach", "author": ["C. Jarzynski"], "venue": "Physical Review E,", "citeRegEx": "Jarzynski,? \\Q1997\\E", "shortCiteRegEx": "Jarzynski", "year": 1997}, {"title": "Equalities and inequalities: irreversibility and the second law of thermodynamics at the nanoscale", "author": ["C. Jarzynski"], "venue": "Annu. Rev. Condens. Matter Phys.,", "citeRegEx": "Jarzynski,? \\Q2011\\E", "shortCiteRegEx": "Jarzynski", "year": 2011}, {"title": "Dead leaves models: from space tesselation to random functions", "author": ["D. Jeulin"], "venue": "Proc. of the Symposium on the Advances in the Theory and Applications of Random Sets,", "citeRegEx": "Jeulin,? \\Q1997\\E", "shortCiteRegEx": "Jeulin", "year": 1997}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "arXiv preprint arXiv:1010.3467,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma and Welling,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Sur la th\u00e9orie du mouvement brownien", "author": ["P. Langevin"], "venue": "CR Acad. Sci. Paris,", "citeRegEx": "Langevin,? \\Q1908\\E", "shortCiteRegEx": "Langevin", "year": 1908}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Larochelle and Murray,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "A sparse texture representation using local affine regions", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Lazebnik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2005}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes", "year": 1998}, {"title": "Occlusion models for natural images: A statistical study of a scale-invariant dead leaves model", "author": ["A. Lee", "D. Mumford", "J. Huang"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lee et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2001}, {"title": "Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction", "author": ["S. Lyu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Lyu,? \\Q2011\\E", "shortCiteRegEx": "Lyu", "year": 2011}, {"title": "Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers", "author": ["D. MacKay"], "venue": "Detectors and Associated Equipment,", "citeRegEx": "MacKay,? \\Q1995\\E", "shortCiteRegEx": "MacKay", "year": 1995}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Murphy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Annealed importance sampling", "author": ["R. Neal"], "venue": "Statistics and Computing,", "citeRegEx": "Neal,? \\Q2001\\E", "shortCiteRegEx": "Neal", "year": 2001}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "High-Dimensional Probability Estimation with Deep Density Models", "author": ["O. Rippel", "R.P. Adams"], "venue": null, "citeRegEx": "Rippel and Adams,? \\Q2013\\E", "shortCiteRegEx": "Rippel and Adams", "year": 2013}, {"title": "Learning factorial codes by predictability minimization", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber", "year": 1992}, {"title": "Learning joint top-down and bottom-up processes for 3D visual inference", "author": ["C. Sminchisescu", "A. Kanaujia", "D. Metaxas"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Sminchisescu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sminchisescu et al\\.", "year": 2006}, {"title": "New Method for Parameter Estimation in Probabilistic Models: Minimum Probability Flow", "author": ["J. Sohl-Dickstein", "P. Battaglino", "M. DeWeese"], "venue": "Physical Review Letters,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2206\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2206}, {"title": "Minimum Probability Flow Learning", "author": ["J. Sohl-Dickstein", "P.B. Battaglino", "M.R. DeWeese"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2011}, {"title": "Fast largescale optimization by unifying stochastic gradient and quasi-Newton methods", "author": ["J. Sohl-Dickstein", "B. Poole", "S. Ganguli"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "Fluctuation Relations : A Pedagogical Overview", "author": ["R. Spinney", "I. Ford"], "venue": "arXiv preprint arXiv:1201.6381,", "citeRegEx": "Spinney and Ford,? \\Q2013\\E", "shortCiteRegEx": "Spinney and Ford", "year": 2013}, {"title": "Learning stochastic inverses", "author": ["A. Stuhlm\u00fcller", "J. Taylor", "N. Goodman"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Stuhlm\u00fcller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Stuhlm\u00fcller et al\\.", "year": 2013}, {"title": "Nonconvex optimization using a Fokker-Planck learning machine", "author": ["J. Suykens", "J. Vandewalle"], "venue": "In 12th European Conference on Circuit Theory and Design,", "citeRegEx": "Suykens and Vandewalle,? \\Q1995\\E", "shortCiteRegEx": "Suykens and Vandewalle", "year": 1995}, {"title": "Convergence condition of the TAP equation for the infinite-ranged Ising spin glass model", "author": ["T P"], "venue": "J. Phys. A: Math. Gen", "citeRegEx": "P.,? \\Q1971\\E", "shortCiteRegEx": "P.", "year": 1971}, {"title": "Mean-field theory of Boltzmann machine learning", "author": ["T. Tanaka"], "venue": "Physical Review Letters E,", "citeRegEx": "Tanaka,? \\Q1998\\E", "shortCiteRegEx": "Tanaka", "year": 1998}, {"title": "Mixtures of conditional Gaussian scale mixtures applied to multiscale image representations", "author": ["L. Theis", "R. Hosseini", "M. Bethge"], "venue": "PloS one,", "citeRegEx": "Theis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2012}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "RNADE: The real-valued neural autoregressive density-estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "A Deep and Tractable Density Estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": null, "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "A new learning algorithm for mean field Boltzmann machines", "author": ["M. Welling", "G. Hinton"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Welling and Hinton,? \\Q2002\\E", "shortCiteRegEx": "Welling and Hinton", "year": 2002}, {"title": "On the Equivalence Between Deep NADE and Generative Stochastic Networks", "author": ["L. Yao", "S. Ozair", "K. Cho", "Y. Bengio"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 43, "context": "A variety of analytic approximations exist which ameliorate, but do not remove, this tradeoff\u2013for instance mean field theory and its expansions (T, 1982; Tanaka, 1998), variational Bayes (Jordan et al.", "startOffset": 144, "endOffset": 167}, {"referenceID": 19, "context": "A variety of analytic approximations exist which ameliorate, but do not remove, this tradeoff\u2013for instance mean field theory and its expansions (T, 1982; Tanaka, 1998), variational Bayes (Jordan et al., 1999), contrastive divergence (Welling & Hinton, 2002; Hinton, 2002), minimum probability flow (Sohl-Dickstein et al.", "startOffset": 187, "endOffset": 208}, {"referenceID": 13, "context": ", 1999), contrastive divergence (Welling & Hinton, 2002; Hinton, 2002), minimum probability flow (Sohl-Dickstein et al.", "startOffset": 32, "endOffset": 70}, {"referenceID": 28, "context": ", 2011b;a), minimum KL contraction (Lyu, 2011), proper scoring rules (Gneiting & Raftery, 2007; Parry et al.", "startOffset": 35, "endOffset": 46}, {"referenceID": 15, "context": ", 2012), score matching (Hyv\u00e4rinen, 2005), pseudolikelihood (Besag, 1975), loopy belief propagation (Murphy et al.", "startOffset": 24, "endOffset": 41}, {"referenceID": 4, "context": ", 2012), score matching (Hyv\u00e4rinen, 2005), pseudolikelihood (Besag, 1975), loopy belief propagation (Murphy et al.", "startOffset": 60, "endOffset": 73}, {"referenceID": 30, "context": ", 2012), score matching (Hyv\u00e4rinen, 2005), pseudolikelihood (Besag, 1975), loopy belief propagation (Murphy et al., 1999), and many, many more.", "startOffset": 100, "endOffset": 121}, {"referenceID": 16, "context": "Our method uses a Markov chain to gradually convert one distribution into another, an idea used in non-equilibrium statistical physics (Jarzynski, 1997) and sequential Monte Carlo (Neal, 2001).", "startOffset": 135, "endOffset": 152}, {"referenceID": 31, "context": "Our method uses a Markov chain to gradually convert one distribution into another, an idea used in non-equilibrium statistical physics (Jarzynski, 1997) and sequential Monte Carlo (Neal, 2001).", "startOffset": 180, "endOffset": 192}, {"referenceID": 14, "context": "Relationship to other work The wake-sleep algorithm (Hinton, 1995; Dayan et al., 1995) introduced the idea of training inference and generative probabilistic models against each other.", "startOffset": 52, "endOffset": 86}, {"referenceID": 7, "context": "Relationship to other work The wake-sleep algorithm (Hinton, 1995; Dayan et al., 1995) introduced the idea of training inference and generative probabilistic models against each other.", "startOffset": 52, "endOffset": 86}, {"referenceID": 35, "context": "This approach remained largely unexplored for nearly two decades, though with some exceptions (Sminchisescu et al., 2006; Kavukcuoglu et al., 2010).", "startOffset": 94, "endOffset": 147}, {"referenceID": 20, "context": "This approach remained largely unexplored for nearly two decades, though with some exceptions (Sminchisescu et al., 2006; Kavukcuoglu et al., 2010).", "startOffset": 94, "endOffset": 147}, {"referenceID": 32, "context": "In (Kingma & Welling, 2013; Gregor et al., 2013; Rezende et al., 2014; Ozair & Bengio, 2014) variational learning and inference algorithms were developed which allow a flexible generative model and posterior distribution over latent variables to be directly trained against each other.", "startOffset": 3, "endOffset": 92}, {"referenceID": 35, "context": "The variational bound in these papers is similar to the one used in our training objective and in the earlier work of (Sminchisescu et al., 2006).", "startOffset": 118, "endOffset": 145}, {"referenceID": 49, "context": "Generative stochastic networks (Bengio & Thibodeau-Laufer, 2013; Yao et al., 2014) train a Markov kernel to match its equilibrium distribution to the data distribution.", "startOffset": 31, "endOffset": 82}, {"referenceID": 11, "context": "Adversarial networks (Goodfellow et al., 2014) train a generative model against a classifier which attempts to distinguish generated samples from true data.", "startOffset": 21, "endOffset": 46}, {"referenceID": 34, "context": "A similar objective in (Schmidhuber, 1992) learns a two-way mapping to a representation with marginally independent units.", "startOffset": 23, "endOffset": 42}, {"referenceID": 8, "context": "In (Rippel & Adams, 2013; Dinh et al., 2014) bijective deterministic maps are learned to a latent representation with a simple factorial density function.", "startOffset": 3, "endOffset": 44}, {"referenceID": 40, "context": "In (Stuhlm\u00fcller et al., 2013) stochastic inverses are learned for Bayesian networks.", "startOffset": 3, "endOffset": 29}, {"referenceID": 44, "context": "Mixtures of conditional Gaussian scale mixtures (MCGSMs) (Theis et al., 2012) describe a dataset using Gaussian scale mixtures, with parameters which depend on a sequence of causal neighborhoods.", "startOffset": 57, "endOffset": 77}, {"referenceID": 29, "context": "There is additionally significant work learning flexible generative mappings from simple latent distributions to data distributions \u2013 early examples including (MacKay, 1995) where neural networks are introduced as generative models, and (Bishop et al.", "startOffset": 159, "endOffset": 173}, {"referenceID": 5, "context": "There is additionally significant work learning flexible generative mappings from simple latent distributions to data distributions \u2013 early examples including (MacKay, 1995) where neural networks are introduced as generative models, and (Bishop et al., 1998) where a stochastic manifold mapping is learned from a latent space to the data space.", "startOffset": 237, "endOffset": 258}, {"referenceID": 16, "context": "Related ideas from physics include the Jarzynski equality (Jarzynski, 1997), known in machine learning as An-", "startOffset": 58, "endOffset": 75}, {"referenceID": 31, "context": "nealed Importance Sampling (AIS) (Neal, 2001), which uses a Markov chain which slowly converts one distribution into another to compute a ratio of normalizing constants.", "startOffset": 33, "endOffset": 45}, {"referenceID": 6, "context": "In (Burda et al., 2014) it is shown that AIS can also be performed using the reverse rather than forward trajectory.", "startOffset": 3, "endOffset": 23}, {"referenceID": 23, "context": "Langevin dynamics (Langevin, 1908), which are the stochastic realization of the Fokker-Planck equation, show how to define a Gaussian diffusion process which has any target distribution as its equilibrium.", "startOffset": 18, "endOffset": 34}, {"referenceID": 17, "context": "This corresponds to the case of a quasi-static process in statistical physics (Spinney & Ford, 2013; Jarzynski, 2011).", "startOffset": 78, "endOffset": 117}, {"referenceID": 12, "context": "In AIS, the right schedule of intermediate distributions can greatly improve the accuracy of the log partition function estimate (Grosse et al., 2013).", "startOffset": 129, "endOffset": 150}, {"referenceID": 17, "context": "In thermodynamics the schedule taken when moving between equilibrium distributions determines how much free energy is lost (Spinney & Ford, 2013; Jarzynski, 2011).", "startOffset": 123, "endOffset": 162}, {"referenceID": 18, "context": "The proposed framework trained on dead leaf images (Jeulin, 1997; Lee et al., 2001).", "startOffset": 51, "endOffset": 83}, {"referenceID": 27, "context": "The proposed framework trained on dead leaf images (Jeulin, 1997; Lee et al., 2001).", "startOffset": 51, "endOffset": 83}, {"referenceID": 44, "context": "(b) A sample from the previous state of the art natural image model (Theis et al., 2012) trained on identical data, reproduced here with permission.", "startOffset": 68, "endOffset": 88}, {"referenceID": 38, "context": "Model training was with SFO (Sohl-Dickstein et al., 2014), except for CIFAR-10.", "startOffset": 28, "endOffset": 57}, {"referenceID": 45, "context": "The log likelihood bounds reported here are instead for data that has been pre-processed by adding uniform noise to remove pixel quantization, as recommended in (Theis et al., 2015).", "startOffset": 161, "endOffset": 181}, {"referenceID": 25, "context": "(a) A bark image from (Lazebnik et al., 2005).", "startOffset": 22, "endOffset": 45}, {"referenceID": 44, "context": "Dead leaves images were evaluated using identical training and test data as in (Theis et al., 2012).", "startOffset": 79, "endOffset": 99}, {"referenceID": 11, "context": "MNIST log likelihoods were estimated using the Parzen-window code from (Goodfellow et al., 2014), with values given in bits, and show that our performance is comparable to other recent techniques.", "startOffset": 71, "endOffset": 96}, {"referenceID": 2, "context": "Log likelihoods relative to (Bengio et al., 2012; Bengio & Thibodeau-Laufer, 2013; Goodfellow et al., 2014) are given in Table 2.", "startOffset": 28, "endOffset": 107}, {"referenceID": 11, "context": "Log likelihoods relative to (Bengio et al., 2012; Bengio & Thibodeau-Laufer, 2013; Goodfellow et al., 2014) are given in Table 2.", "startOffset": 28, "endOffset": 107}, {"referenceID": 11, "context": "For this comparison we therefore estimate MNIST log likelihood using the Parzenwindow code released with (Goodfellow et al., 2014).", "startOffset": 105, "endOffset": 130}, {"referenceID": 18, "context": "Dead Leaf Images Dead leaf images (Jeulin, 1997; Lee et al., 2001) consist of layered occluding circles, drawn from a power law distribution over scales.", "startOffset": 34, "endOffset": 66}, {"referenceID": 27, "context": "Dead Leaf Images Dead leaf images (Jeulin, 1997; Lee et al., 2001) consist of layered occluding circles, drawn from a power law distribution over scales.", "startOffset": 34, "endOffset": 66}, {"referenceID": 25, "context": "Bark Texture Images A probabilistic model was trained on bark texture images (T01-T04) from (Lazebnik et al., 2005).", "startOffset": 92, "endOffset": 115}], "year": 2015, "abstractText": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.", "creator": "LaTeX with hyperref package"}}}