{"id": "1503.03712", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2015", "title": "On Graduated Optimization for Stochastic Non-Convex Problems", "abstract": "The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade. Despite its popularity, very little is known in terms of theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimiza- tion and analyze its performance. We characterize a parameterized family of non- convex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an {\\epsilon}-approximate solution within O(1/\\epsilon^2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of zero-order optimization, and devise a a variant of our algorithm which converges at rate of O(d^2/\\epsilon^4).", "histories": [["v1", "Thu, 12 Mar 2015 13:39:28 GMT  (231kb,D)", "http://arxiv.org/abs/1503.03712v1", "17 pages"], ["v2", "Wed, 8 Jul 2015 05:14:22 GMT  (231kb,D)", "http://arxiv.org/abs/1503.03712v2", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["elad hazan", "kfir yehuda levy", "shai shalev-shwartz"], "accepted": true, "id": "1503.03712"}, "pdf": {"name": "1503.03712.pdf", "metadata": {"source": "CRF", "title": "On Graduated Optimization for Stochastic Non-Convex Problems", "authors": ["Elad Hazan", "Kfir Y. Levy", "Shai Shalev-Swartz"], "emails": ["ehazan@cs.princeton.edu.", "kfiryl@tx.technion.ac.il.", "shais@cs.huji.ac.il."], "sections": [{"heading": null, "text": "In this paper, we describe a new first-order algorithm based on graduated optimization and analyze its performance. We characterize a parameterized family of nonconvex functions for which this algorithm is demonstrably converging to a global optimum. In particular, we demonstrate that the algorithm converges within O (1 / \u03b52) gradient-based steps to an \u03b5 approximate solution. We expand our algorithm and analysis to include setting stochastic nonconvex optimization with raucous gradient feedback that achieves the same convergence rate. In addition, we discuss setting \"zero-order optimization\" and develop a variant of our algorithm that converges with rate O (d2 / \u03b54)."}, {"heading": "1 Introduction", "text": "Of particular interest are non-convex optimization problems that occur in the formation of deep neural networks Bengio (2009). Frequently, such problems permit a multimodal structure, and therefore the use of convex optimization mechanisms can lead to local optimization. Gradual optimization (a.k.a. continuation), Blake and Zisserman (1987), is a methodology that attempts to overcome such numerous local optimization mechanisms. First, a coarse-grained version of the problem is generated by a local smoothing operation, which is easier to solve. Then, the method progresses in stages by gradually refining the problem versions, using the solution of the previous stage as the starting point for optimization."}, {"heading": "1.1 Related Work", "text": "The term \"Graduated Non-Convexity\" (GNC) was later coined in Yuille (1989); Yuille et al. (1990) and Terzopoulos (1988), which were the first to explicitly establish this idea. Similar attitudes in Machine Vision literature later appeared in Yuille (1989); Yuille et al. (1990). Over the last two decades, this concept has been successfully applied to numerous problems in computer vision; among others: Image Restoration Boccuto et al. (2002), Image Restoration Nikolova et al. (2010) and in the field of numerical analysis Allgower and Georg (1990)."}, {"heading": "2 Setting", "text": "We discuss an optimization of a nonconvex loss function f: K 7 \u2192 R, where K'Rd is a convex set. We assume that the optimization continues for T rounds; in each round t = 1,.., T we can query a point xt-K and receive feedback. After the last round we choose x-T-K, and our yardstick is the excess loss, defined as: f (x-T) \u2212 min x-K-f (x) In Section 4.2 we describe a family of nonconvex multimodal functions that we call \"chic.\" Faced with such a flattering loss f, we are interested in algorithms that are highly likely to ensure an excess loss within poly rounds. We consider two types of feedback: 1. Sound gradient feedback: When querying xt, we get f (xt) + vice versa, where {xt.} T2."}, {"heading": "3 Preliminaries and Notation", "text": "Notation: During this work we use B, S to denote the Euclidean ball / sphere in round, and also Br (x), Sr (x) as the Euclidean r-sphere / sphere in round centered on x. For a set A, Rd, u, A stands for a random variable that is evenly distributed over A."}, {"heading": "3.1 Strong-Convexity", "text": "Remember the definition of strongly convex functions: Definition 3.1. (Strong convexity) We say that a function F: Rn \u2192 R is convex across the set K, if it applies to all x, y, K that, F (y) \u2265 F (x) + E (x) > (y \u2212 x) + \u03c3 2 x \u2212 y \u00b2 2Let F be a convex F over the convex set K, and let x \u00b2 be a point in K at which F is minimized, then the following inequality is satisfied: \u03c3 2 x \u2212 x \u00b2 2 \u2264 F (x) \u2212 F (x) (x) (1) This is done directly by defining strong convexity combined with the convexity of F (x) > (x \u2212 x \u00b2)."}, {"heading": "4 Smoothing and \u03c3-Nice functions", "text": "The construction of ever finer approximations to the original objective function is at the heart of the continuation approach. In Section 4.1, we define the smoothed versions that we will use. In Section 4.1.1, we describe an efficient way to implicitly access the smoothed versions, allowing us to perform optimizations. Finally, in Section 4.2, we define a class of nonconvex multimodal functions that we call \u03c3-nice. As we will see in Section 7, these functions are rich enough to capture nonconvex structures that exist in natural data. In addition, these functions are suitable for efficient optimization, and we can ensure convergence to the \u03b5 solution within polar (1 / \u03b5) iterations, as described in Section 5.6."}, {"heading": "4.1 Smoothing", "text": "Smoothing by local averages is formally defined as the following definition: Definition 4.1. Faced with an L-Lipschitz function f: Rd 7 \u2192 R, it defines the \"smooth\" version of f (x) = Eu-B [f (x + \u03b4u)]. The next problem limits the inclination between f-Lipschitz function and f: Lemma 4.1. Let f-Lipschitz be the smoothed version of f, then the smoothed version of f (x) \u2212 f (x) \u2212 f (x)."}, {"heading": "4.1.1 Implicit Smoothing using Sampling", "text": "A much more efficient approach is to create an unbiased estimate for the gradients of the smoothed version by sampling the function gradients / values, which could then be used by a stochastic optimization algorithm such as SGD (Stochastic Gradient Descent). This approach is presented in Figures 1.2.The following two versions of Lemmas show that the resulting estimates are unbiased and limited: Lemma 4.2. Let x x x x x x x x x x x x x x x, and assume that f is the L-Lipschitz value, then the output of SGOG (Figure 1) is limited by L and is an unbiased estimate for the F-values (x).Proof. SGOG assumes that f x x x x x x x x x x."}, {"heading": "4.2 \u03c3-Nice Functions", "text": "Following is our main definitionDefinition 4.2. A function f: K 7 \u2192 R is called \u03c3-beautiful if the following two conditions apply: 1. Centering property: For each \u03b4 > 0 and each x-arg-arg-arg-minx-K f-3 (x) there is x-arg-minx-K f-3 / 2 (x), so that: x-arg-arg-arg-minx-K-2 (x), so that: x-arg-arg-arg-3 (x), so that: x-arg-minx-K-2 (x), so that: x-arg-minx-3 (x), so that the function f-arg-3 (x) is very convex-strong."}, {"heading": "5 Graduated Optimization with a Gradient Oracle", "text": "In this section, we assume that we can access an intoxicating gradient oracle for f, as guaranteed by Lemma 4.2. So, note that SGOG (Figure 1) is listed for easier notation using an exact gradient oracle for f. As described at the end of Section 4.1.1, this could be replaced by a loud gradient oracle for f, and Lemma 4.2 will still persist. Following is our main theorem 5.1. Let it happen that both f (0, 1) and p (0, 1 / e) K could be a convex set, and f will be a LLipschitz-nice function. Suppose we apply the algorithm 1, then round algorithm 1 m after O (1 / 2, 2), which yields a point x M + 1 that is more likely to be optimal than this optimization."}, {"heading": "5.1 Analysis", "text": "This algorithm performs projected gradients obtained from GradOptG. (Algorithm 1 GradOptG Input: Target Error, Maximum Failure Probability p, Decision Proposition K-K). (Sentence 2). (Sentence 1). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2). (Sentence 2.). (Sentence 2.). (Sentence 2.). (Sentence 2.). ("}, {"heading": "6 Graduated Optimization with a Value Oracle", "text": "In this section, we assume that we can access a noise value oracle for f. Therefore, at x-Rd, \u03b4 \u2265 0 SGOV (Figure 2) we can use SGOV (Figure 2) as an oracle that returns an unbiased and limited estimate for f-Rd (x), as guaranteed by Lemma 4.3. note that SGOV (Figure 2) will still be valid for easier notation using an exact value oracle for f. As described at the end of Section 4.1.1, this could be replaced by a loud value oracle for f, and Lemma 4.3 will still be valid. The following is our main theorem 6.1. Let us emerge > 0 and p-Ra (0, 1 / e), also K be a convex set, and f be an L-Lipschitz-nice function. Let us also assume that maxx | f (x) | \u2264 itz C. Let us assume that we apply algorithm 2, then lischnice-3, and then lischnice-4 (O / Rnice)."}, {"heading": "6.1 Analysis", "text": "Note that at each epoch m of GradOptV = 124m \u00b2 = 124m \u00b2 = 42.0 m \u00b2 = 42.0 m \u00b2 = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% = 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0% 42.0%"}, {"heading": "7 Experiments", "text": "Over the past two decades, performing complex learning tasks using Neural Network (NN) architectures has become an active and promising line of research. As learning NN architectures essentially requires solving a hard non-convex program, we have decided to focus our empirical study on these types of tasks. As a test case, we train an NN with a single hidden layer of 30 units over the MNIST dataset. We take over the experimental setup from Dauphin et al. (2014) and monitor a scaled-down version of the data, i.e. the original 28 x 28 images of MNIST have been sampled down to 10 x 10. We use a ReLU activation function and minimize square loss."}, {"heading": "7.1 Smoothing the NN", "text": "First, we were interested in researching the non-convex structure of the above NN learning task and checking whether our definition of \u03c3-nice is in line with this structure. We started by applying MSGD (Minibatch Stochastic Gradient Descent) to the problem, using a stack size of 100, and a step size rule for the shape \u03b7t = \u03b70 (1 + \u03b3t) -3 / 4, where \u03b70 = 0.01, \u03b3 = 10 \u2212 4. This choice of step size rule was the most effective of a grid we studied. We found that MSGD often \"stands\" in areas with a relatively high loss, here we refer to points at the end of such a run as stable points. To learn something about the non-convex nature of the problem, we examined the objective values along two directions around the stable points that mark stable x. The first direction was the junction line between the stable point, where the second direction x and the stable point appears, where the stable point x was the best direction."}, {"heading": "7.2 Graduated Optimization of NN", "text": "Here we present experiments demonstrating the effectiveness of GradOptG (algorithm 1) in the formation of the aforementioned NN. First, we wanted to learn whether smoothing can help us find vanishing points where MSGD comes to a halt. We used MSGD (\u03b4 = 0) to train the NN, and as before, we found that its progress slows down and leads to relatively high errors. Then, we took the point that MSGD reached after 5 \u00b7 104 iteration, and initialized an optimization via the smoothed versions of the loss; this was done using smoothing values of {1, 3, 5, 7}. In Figure 5, we present the results of the above experiment. As seen in Figure 5, small g's converge slower than large g's, but yield a much better solution. In addition, the initial optimization in leaps progresses because the jumps of large jumps are associated with the smaller ones, and we believe that the smaller ones are the 7 jumps."}, {"heading": "8 Discussion", "text": "We have described a family of nonconvex functions that allow efficient optimization via the tiered optimization method, and have provided the first rigorous analysis of a first-class algorithm in the stochastic environment.We consider this only as a first glimpse into the potential of tiered optimization to detectable nonconvex optimization, and as one of the interesting questions we still find. \u2022 Is \u03c3-neatness necessary to converge top-order methods to a global optimum? Is there a milder property that better captures the power of tiered optimization? \u2022 Can its parameters, in addition to the two properties \u03c3-neatness, be loosened with respect to the ratio of smoothing to strong convexity or centering? \u2022 Can second-class / other methods lead to better convergence rates / faster algorithms for stochastic or offline beautiful nonconvex optimization?"}], "references": [{"title": "Numerical continuation methods, volume 13", "author": ["E.L. Allgower", "K. Georg"], "venue": null, "citeRegEx": "Allgower and Georg.,? \\Q1990\\E", "shortCiteRegEx": "Allgower and Georg.", "year": 1990}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "A gnc algorithm for deblurring images with interacting discontinuities", "author": ["A. Boccuto", "M. Discepoli", "I. Gerace", "R. Pandolfi", "P. Pucci"], "venue": "Proc. VI SIMAI,", "citeRegEx": "Boccuto et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Boccuto et al\\.", "year": 2002}, {"title": "Large displacement optical flow: descriptor matching in variational motion estimation", "author": ["T. Brox", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Brox and Malik.,? \\Q2011\\E", "shortCiteRegEx": "Brox and Malik.", "year": 2011}, {"title": "Gradient descent optimization of smoothed information retrieval metrics", "author": ["O. Chapelle", "M. Wu"], "venue": "Information retrieval,", "citeRegEx": "Chapelle and Wu.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle and Wu.", "year": 2010}, {"title": "A continuation method for semi-supervised svms", "author": ["O. Chapelle", "M. Chi", "A. Zien"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Smoothing a program soundly and robustly", "author": ["S. Chaudhuri", "A. Solar-Lezama"], "venue": "In Computer Aided Verification,", "citeRegEx": "Chaudhuri and Solar.Lezama.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri and Solar.Lezama.", "year": 2011}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["D. Erhan", "P.-A. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In SODA,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "On the link between gaussian homotopy continuation and convex envelopes", "author": ["H. Mobahi", "J.W. Fisher III"], "venue": "In Energy Minimization Methods in Computer Vision and Pattern Recognition,", "citeRegEx": "Mobahi and III.,? \\Q2015\\E", "shortCiteRegEx": "Mobahi and III.", "year": 2015}, {"title": "A theoretical analysis of optimization by gaussian continuation", "author": ["H. Mobahi", "J.W. Fisher III"], "venue": null, "citeRegEx": "Mobahi and III.,? \\Q2015\\E", "shortCiteRegEx": "Mobahi and III.", "year": 2015}, {"title": "Fast nonconvex nonsmooth minimization methods for image restoration and reconstruction", "author": ["M. Nikolova", "M.K. Ng", "C.-P. Tam"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Nikolova et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nikolova et al\\.", "year": 2010}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "arXiv preprint arXiv:1109.5647,", "citeRegEx": "Rakhlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2011}, {"title": "The computation of visible-surface representations", "author": ["D. Terzopoulos"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Terzopoulos.,? \\Q1988\\E", "shortCiteRegEx": "Terzopoulos.", "year": 1988}, {"title": "The effective energy transformation scheme as a special continuation approach to global optimization with application to molecular conformation", "author": ["Z. Wu"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Wu.,? \\Q1996\\E", "shortCiteRegEx": "Wu.", "year": 1996}, {"title": "Energy functions for early vision and analog networks", "author": ["A. Yuille"], "venue": "Biological Cybernetics,", "citeRegEx": "Yuille.,? \\Q1989\\E", "shortCiteRegEx": "Yuille.", "year": 1989}, {"title": "Stereo integration, mean field theory and psychophysics", "author": ["A.L. Yuille", "D. Geiger", "H. B\u00fclthoff"], "venue": "In Computer Vision ECCV", "citeRegEx": "Yuille et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Yuille et al\\.", "year": 1990}, {"title": "A path following algorithm for the graph matching problem", "author": ["M. Zaslavskiy", "F. Bach", "J.-P. Vert"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Zaslavskiy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zaslavskiy et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Of particular interest are non-convex optimization problem that arise in the training of deep neural networks Bengio (2009). Often, such problems admit a multimodal structure, and therefore, the use of convex optimization machinery may lead to a local optima.", "startOffset": 110, "endOffset": 124}, {"referenceID": 1, "context": "Of particular interest are non-convex optimization problem that arise in the training of deep neural networks Bengio (2009). Often, such problems admit a multimodal structure, and therefore, the use of convex optimization machinery may lead to a local optima. Graduated optimization (a.k.a. continuation), Blake and Zisserman (1987), is a methodology that attempts to overcome such numerous local optima.", "startOffset": 110, "endOffset": 333}, {"referenceID": 5, "context": "For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) .", "startOffset": 66, "endOffset": 89}, {"referenceID": 5, "context": "For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) .", "startOffset": 66, "endOffset": 124}, {"referenceID": 5, "context": "For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) . However, in the general case, it is commonly suggested in the literature to convolve the original function with a gaussian kernel Wu (1996). Yet, this operation is prohibitively inefficient in high dimensions.", "startOffset": 66, "endOffset": 266}, {"referenceID": 1, "context": "Interestingly, the next question is raised in Bengio (2009) which reviews recent developments in the field of deep learning: \u201cCan optimization strategies based on continuation methods deliver significantly improved training of deep architectures?\u201d As an initial empirical study, we examine the task of training a NN (Neural Network) over the MNIST data set.", "startOffset": 46, "endOffset": 60}, {"referenceID": 16, "context": "Similar attitudes in the machine vision literature appeared later in Yuille (1989); Yuille et al.", "startOffset": 69, "endOffset": 83}, {"referenceID": 16, "context": "Similar attitudes in the machine vision literature appeared later in Yuille (1989); Yuille et al. (1990), and Terzopoulos (1988).", "startOffset": 69, "endOffset": 105}, {"referenceID": 15, "context": "(1990), and Terzopoulos (1988).", "startOffset": 12, "endOffset": 31}, {"referenceID": 7, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990).", "startOffset": 68, "endOffset": 78}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al.", "startOffset": 118, "endOffset": 144}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al.", "startOffset": 118, "endOffset": 301}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011).", "startOffset": 118, "endOffset": 344}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al.", "startOffset": 118, "endOffset": 384}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al.", "startOffset": 118, "endOffset": 549}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010).", "startOffset": 118, "endOffset": 590}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al.", "startOffset": 118, "endOffset": 626}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al.", "startOffset": 118, "endOffset": 644}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al.", "startOffset": 118, "endOffset": 741}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation.", "startOffset": 118, "endOffset": 762}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation. These approaches, in the spirit of the continuation method, offer no guarantees on the quality of the obtained solution, and are tailored to specific applications. A comprehensive survey of the graduated optimization literature can be found in Mobahi and Fisher III (2015a). A recent work Mobahi and Fisher III (2015b) advances our theoretical understanding, by analyzing a continuation algorithm in the general setting.", "startOffset": 118, "endOffset": 1063}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation. These approaches, in the spirit of the continuation method, offer no guarantees on the quality of the obtained solution, and are tailored to specific applications. A comprehensive survey of the graduated optimization literature can be found in Mobahi and Fisher III (2015a). A recent work Mobahi and Fisher III (2015b) advances our theoretical understanding, by analyzing a continuation algorithm in the general setting.", "startOffset": 118, "endOffset": 1108}, {"referenceID": 9, "context": "A proof of Equation (2) is found in Flaxman et al. (2005).", "startOffset": 36, "endOffset": 58}, {"referenceID": 14, "context": "Note the following result from Rakhlin et al. (2011) regarding stochastic optimization of \u03c3-strongly-convex functions, given such an oracle: Theorem 5.", "startOffset": 31, "endOffset": 53}, {"referenceID": 7, "context": "We adopt the experimental setup of Dauphin et al. (2014) and train over a down-scaled version of the data, i.", "startOffset": 35, "endOffset": 57}], "year": 2017, "abstractText": "The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade. Despite its popularity, very little is known in terms of theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimization and analyze its performance. We characterize a parameterized family of nonconvex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an \u03b5-approximate solution within O(1/\u03b52) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of \u201czero-order optimization\u201d, and devise a a variant of our algorithm which converges at rate of O(d2/\u03b54).", "creator": "LaTeX with hyperref package"}}}