{"id": "1503.03903", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2015", "title": "Approximating Sparse PCA from Incomplete Data", "abstract": "We study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements. We show that for a wide class of optimization problems, if the sketch is close (in the spectral norm) to the original data matrix, then one can recover a near optimal solution to the optimization problem by using the sketch. In particular, we use this approach to obtain sparse principal components and show that for \\math{m} data points in \\math{n} dimensions, \\math{O(\\epsilon^{-2}\\tilde k\\max\\{m,n\\})} elements gives an \\math{\\epsilon}-additive approximation to the sparse PCA problem (\\math{\\tilde k} is the stable rank of the data matrix). We demonstrate our algorithms extensively on image, text, biological and financial data. The results show that not only are we able to recover the sparse PCAs from the incomplete data, but by using our sparse sketch, the running time drops by a factor of five or more.", "histories": [["v1", "Thu, 12 Mar 2015 22:16:55 GMT  (1000kb)", "http://arxiv.org/abs/1503.03903v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IT cs.NA math.IT stat.ML", "authors": ["abhisek kundu", "petros drineas", "malik magdon-ismail"], "accepted": true, "id": "1503.03903"}, "pdf": {"name": "1503.03903.pdf", "metadata": {"source": "CRF", "title": "Approximating Sparse PCA from Incomplete Data", "authors": ["Abhisek Kundu", "Petros Drineas", "Malik Magdon-Ismail"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 3.03 903v 1 [cs.L G] 12 Mar 2"}, {"heading": "1 Introduction", "text": "The question we are addressing is not how we want the biological applications or assets in financial applications, but factors that have only a small number of the original variables. (The question we are addressing) is not how we should behave in the biological world. (The question we are addressing is not how we should behave in the biological world.) It is desirable to have factors that have only a small number of the original variables. (The question we are addressing is not how we can perform better; rather, it is whether we can provide incomplete data and whether we can provide incomplete data. (The question we are asking is not how we can provide better data.) (The question is not whether we can provide incomplete data, whether we can provide incomplete data, and whether we can provide incomplete data.)"}, {"heading": "2 Sparse PCA from a Sketch", "text": "We assume that f (V) f (V) f (V), X) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (X) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (F) f (V) f (V) f (V) f (V) f (X) f (X) f (X) f (X) f (X) f (X) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (X) f (V) f (X) f (X) f (X) f (X) f (V) f (X) f (X) f (X) f (X) f (X) f (V) f (X) f (X) f (X) f (X) f (X) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (V) f (X (V) f (V) f (X) f (V) f (X) f (V) f (X) f (V) f (X) f (X (V) f (V) f (X) f (X) f (V) f) f (V) f (V) f ("}, {"heading": "2.1 An (\u21131, \u21132)-Sampling Based Sketch", "text": "In the previous section, we created the sketch by deterministically setting the small data elements to zero. Instead, we could randomly select the data elements to hold them. It is natural to prefer this random sampling method toward the larger elements. Therefore, we define sampling probabilities for each data element that is proportional to a mixture of absolute value and square of the data element. Sampling probabilities were used in Kundu et al. [2015] to test data elements in independent experiments to obtain a sketch. We repeat the prototype algorithm for elemental matrix elements in algorithm 1. Note that unlike with deterministic erosion of small elements, in this sampling scheme, a sample of the element aij with probability and then the pipi results."}, {"heading": "3 Experiments", "text": "We will show the experimental performance of the sparse PCA using a sketch of several real data matrices. As already mentioned, the sparse PCA is NP-hard and therefore we have to apply heuristics. Next, these heuristics will be discussed, followed by the data, the experimental design and finally the results."}, {"heading": "3.1 Algorithms for Sparse PCA", "text": "We consider six heuristics to obtain economical main components.Gmax, r The r largest entries of the main components for the main components produced by G. Gsp (max. / r-sparse components using the spasm toolbox from Sjstrand et al. [2012] with A. Hmax, r The r largest entries of the main components for the (1, 2) -sampled sketch A (max.) -sampled sketch A (max. / r-sparse components using the spasm toolbox from Sjstrand et al. [2] -sampled sketch A, r largest entries of the main components for the (1, 2) -sampled sketch A (max. / r-sparse components using the spasm value).Umax, r largest entries of the main components for the (1, 2) -sampled sketch A (max. / r-sparse components using the spasm value)."}, {"heading": "3.2 Data Sets", "text": "Figure data (m = 2313, n = 256): We use Hull [1994] handwritten digit images (300 pixels / inch in 8-bit grayscale).Each pixel is a feature (normalized to be in [\u2212 1, 1].Each 16-digit image forms a series of data matrix A. We focus on three digits: \"6\" (664 samples), \"9\" (644 samples) and \"1\" (1005 samples).TechTC data (m = 139, n = 15170): We use the Technion Repository of Text Categorization Dataset (TechTC, see Gabrilovich and Markovitch [2004]) from the Open Directory Project (ODP).Each document is presented as a probability distribution over a bag of words."}, {"heading": "3.3 Results", "text": "We also report the speed of a factor of about 6 for the factor 2 most taken into account in the literature. \"We are very satisfied with the data.\" We also report the speed of a factor of about 6 for the factor 2. We report the speed of a factor most taken into account in the literature. \"We are very satisfied with the data.\" \"We are very satisfied with the data.\" \"We are very satisfied with the results.\" \"We are very satisfied with the results.\" \"We are very satisfied with the results.\" \"We are very satisfied with the results.\" \"We are very satisfied with the results.\" \"We are very satisfied with the results.\" \"We are very satisfied with the results.\" \"We are very satisfied with the results.\" \"We are also very satisfied with the results.\" \"We are also observing the speed of a factor of about 6 for the factor.\" We are very satisfied with the results. \""}, {"heading": "3.4 Performance of Other Sketches", "text": "It is about the question of to what extent it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which way it is about a way, in which it is about a way, in which it is about a way, in which way it is about a way, in which it is about a way, in which it is about a way and which it is about a way, in which it is about a way, in which it is about which it is about a way, in which it is about which it is about a way, in which it is about which it is about a way, in which it is about which it is about which it is about a way, in which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which it is about which it is about which it is about which it is about which it is about which is about which it is about which it is about which it is about which is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which is about which is about which is about which is about which is about which it is about which is about which is about which is about which"}], "references": [{"title": "Non-negative sparse PCA with provable guarantees", "author": ["M. Asteris", "D. Papailiopoulos", "A. Dimakis"], "venue": "In Proc. ICML,", "citeRegEx": "Asteris et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Asteris et al\\.", "year": 2014}, {"title": "Loadings and correlations in the interpretation of principal components", "author": ["J. Cadima", "I. Jolliffe"], "venue": "Applied Statistics,", "citeRegEx": "Cadima and Jolliffe.,? \\Q1995\\E", "shortCiteRegEx": "Cadima and Jolliffe.", "year": 1995}, {"title": "Coherent Matrix Completion", "author": ["Y Chen", "S Bhojanapalli", "S Sanghavi", "R Ward"], "venue": "Proceedings of International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["Alexandre d\u2019Aspremont", "Laurent El Ghaoui", "Michael I. Jordan", "Gert R.G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["Alexandre d\u2019Aspremont", "Francis Bach", "Laurent El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "Distinguishing Lung Tumours From Normal Lung Based on a Small Set of Genes", "author": ["T. Dracheva", "R. Philip", "W. Xiao", "AG Gee"], "venue": "In Lung Cancer,", "citeRegEx": "Dracheva et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dracheva et al\\.", "year": 2007}, {"title": "Text categorization with many redundant features: using aggressive feature selection to make SVMs competitive with C4.5", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "In Proceedings of International Conference on Machine Learning,", "citeRegEx": "Gabrilovich and Markovitch.,? \\Q2004\\E", "shortCiteRegEx": "Gabrilovich and Markovitch.", "year": 2004}, {"title": "Gene Expression-Based Classification of Non-Small Cell Lung Carcinomas and Survival Prediction", "author": ["J. Hou", "J. Aerts", "B. den Hamer"], "venue": "In PLoS One, page 5(4):e10312,", "citeRegEx": "Hou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2010}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hull.,? \\Q1994\\E", "shortCiteRegEx": "Hull.", "year": 1994}, {"title": "Recovering PCA from Hybrid-(l1, l2) Sparse Sampling of Data Elements", "author": ["A. Kundu", "P. Drineas", "M. Magdon-Ismail"], "venue": "In http://arxiv.org/pdf/1503.00547v1.pdf,", "citeRegEx": "Kundu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kundu et al\\.", "year": 2015}, {"title": "NP-hardness and inapproximability of sparse pca", "author": ["M. Magdon-Ismail"], "venue": "arxiv report: http://arxiv.org/abs/1502.05675,", "citeRegEx": "Magdon.Ismail.,? \\Q2015\\E", "shortCiteRegEx": "Magdon.Ismail.", "year": 2015}, {"title": "Generalized spectral bounds for sparse LDA", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "In Proc. ICML,", "citeRegEx": "Moghaddam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2006}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine,", "citeRegEx": "Pearson.,? \\Q1901\\E", "shortCiteRegEx": "Pearson.", "year": 1901}, {"title": "Sparse principal component analysis via regularized low rank matrix approximation", "author": ["Haipeng Shen", "Jianhua Z. Huang"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Shen and Huang.,? \\Q2008\\E", "shortCiteRegEx": "Shen and Huang.", "year": 2008}, {"title": "Spasm: A matlab toolbox for sparse statistical modeling", "author": ["K. Sjstrand", "L.H. Clemmensen", "R. Larsen", "B. Ersbll"], "venue": "In Journal of Statistical Software (Accepted for publication),", "citeRegEx": "Sjstrand et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sjstrand et al\\.", "year": 2012}, {"title": "A modified principal component technique based on the lasso", "author": ["N. Trendafilov", "I.T. Jolliffe", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Trendafilov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Trendafilov et al\\.", "year": 2003}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational & Graphical Statistics,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "The earliest reference to principal components analysis (PCA) is in Pearson [1901]. Since then, PCA has evolved into a classic tool for data analysis.", "startOffset": 68, "endOffset": 83}, {"referenceID": 10, "context": "The sparse PCA problem is itself a very hard problem that is not only NP-hard, but also inapproximable [Magdon-Ismail, 2015] There are many heuristics for obtaining sparse factors [Cadima and Jolliffe, 1995, Trendafilov et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 0, "context": ", 2006, Shen and Huang, 2008] including some approximation algorithms with provable guarantees Asteris et al. [2014]. The existing research typically addresses the task of getting just the top principal component (k = 1).", "startOffset": 95, "endOffset": 117}, {"referenceID": 9, "context": "To determine which elements to sample, and how to form the sketch, we leverage some recent results in elementwise matrix completion (Kundu et al. [2015]).", "startOffset": 133, "endOffset": 153}, {"referenceID": 9, "context": "Such a sampling probability was used in Kundu et al. [2015] to sample data elements in independent trials to get a sketch \u00c3.", "startOffset": 40, "endOffset": 60}, {"referenceID": 9, "context": "In particular, the hybrid-(l1, l2) distribution in (4) was analyzed in Kundu et al. [2015], where they suggest an optimal choice for the mixing parameter \u03b1\u2217 which minimizes the theoretical bound on \u2016A\u2212 \u00c3\u20162.", "startOffset": 71, "endOffset": 91}, {"referenceID": 9, "context": "In particular, the hybrid-(l1, l2) distribution in (4) was analyzed in Kundu et al. [2015], where they suggest an optimal choice for the mixing parameter \u03b1\u2217 which minimizes the theoretical bound on \u2016A\u2212 \u00c3\u20162. This algorithm to choose \u03b1\u2217 is summarized in Algorithm 2. Using the probabilities in (4) to create the sketch \u00c3 using Algorithm 1, with \u03b1\u2217 selected using Algorithm 2, one can prove a bound for \u2016A\u2212\u00c3\u20162. We state a simplified version of the bound from Kundu et al. [2015] in Theorem 4.", "startOffset": 71, "endOffset": 476}, {"referenceID": 9, "context": "In particular, the hybrid-(l1, l2) distribution in (4) was analyzed in Kundu et al. [2015], where they suggest an optimal choice for the mixing parameter \u03b1\u2217 which minimizes the theoretical bound on \u2016A\u2212 \u00c3\u20162. This algorithm to choose \u03b1\u2217 is summarized in Algorithm 2. Using the probabilities in (4) to create the sketch \u00c3 using Algorithm 1, with \u03b1\u2217 selected using Algorithm 2, one can prove a bound for \u2016A\u2212\u00c3\u20162. We state a simplified version of the bound from Kundu et al. [2015] in Theorem 4. Theorem 4 (Kundu et al. [2015]) Let A \u2208 Rm\u00d7n and let \u01eb > 0 be an accuracy parameter.", "startOffset": 71, "endOffset": 521}, {"referenceID": 9, "context": "Proof: Follows from the bound in Kundu et al. [2015]. \u22c4 Recall that k\u0303 is the stable rank of A.", "startOffset": 33, "endOffset": 53}, {"referenceID": 14, "context": "Gsp,r r-sparse components using the Spasm toolbox of Sjstrand et al. [2012] with A.", "startOffset": 53, "endOffset": 76}, {"referenceID": 8, "context": "Digit Data (m = 2313, n = 256): We use the Hull [1994] handwritten zip-code digit images (300 pixels/inch in 8-bit gray scale).", "startOffset": 43, "endOffset": 55}, {"referenceID": 6, "context": "TechTC Data (m = 139, n = 15170): We use the Technion Repository of Text Categorization Dataset (TechTC, see Gabrilovich and Markovitch [2004]) from the Open Directory Project (ODP).", "startOffset": 109, "endOffset": 143}, {"referenceID": 2, "context": "See Chen et al. [2014] for the detailed form of the leverage score sampling probabilities (which are known to work well", "startOffset": 4, "endOffset": 23}, {"referenceID": 10, "context": "In particular, it is pointed out in Magdon-Ismail and Boutsidis [2015] that though PCA optimizes variance, a more natural way to look at PCA is as the linear projection of the data that minimizes the information loss.", "startOffset": 36, "endOffset": 71}, {"referenceID": 10, "context": "In particular, it is pointed out in Magdon-Ismail and Boutsidis [2015] that though PCA optimizes variance, a more natural way to look at PCA is as the linear projection of the data that minimizes the information loss. Magdon-Ismail and Boutsidis [2015] gives efficient algorithms to find sparse linear dimension reduction that minimizes information loss \u2013 the information loss of sparse PCA can be considerably higher than optimal.", "startOffset": 36, "endOffset": 253}], "year": 2015, "abstractText": "We study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements. We show that for a wide class of optimization problems, if the sketch is close (in the spectral norm) to the original data matrix, then one can recover a near optimal solution to the optimization problem by using the sketch. In particular, we use this approach to obtain sparse principal components and show that for m data points in n dimensions, O(\u01eb\u22122k\u0303max{m,n}) elements gives an \u01eb-additive approximation to the sparse PCA problem (k\u0303 is the stable rank of the data matrix). We demonstrate our algorithms extensively on image, text, biological and financial data. The results show that not only are we able to recover the sparse PCAs from the incomplete data, but by using our sparse sketch, the running time drops by a factor of five or more.", "creator": "LaTeX with hyperref package"}}}