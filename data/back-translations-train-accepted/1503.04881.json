{"id": "1503.04881", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2015", "title": "Long Short-Term Memory Over Tree Structures", "abstract": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.", "histories": [["v1", "Mon, 16 Mar 2015 23:59:02 GMT  (242kb)", "http://arxiv.org/abs/1503.04881v1", "On February 6th, 2015, this work was submitted to the International Conference on Machine Learning (ICML)"]], "COMMENTS": "On February 6th, 2015, this work was submitted to the International Conference on Machine Learning (ICML)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["xiaodan zhu", "parinaz sobhani", "hongyu guo"], "accepted": true, "id": "1503.04881"}, "pdf": {"name": "1503.04881.pdf", "metadata": {"source": "META", "title": "Long Short-Term Memory Over Tree Structures", "authors": ["Xiaodan Zhu", "Parinaz Sobhani"], "emails": ["XIAODAN.ZHU@NRC-CNRC.GC.CA", "PSOBH090@UOTTAWA.CA", "HONGYU.GUO@NRC-CNRC.GC.CA"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.04 881v 1 [cs.C L] 1"}, {"heading": "1. Introduction", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "2. Related Work", "text": "In fact, it is a purely mental game, in which the aim is to put people at the centre of attention."}, {"heading": "3. The Model", "text": "In this paper, we expand on LSTM de structures, in which a memory cell can reflect the history of two different childhood memories. (1) In the illustration, it is as if the origin of the tree can in principle reflect the information from the past. (1) The illustration shows that the illustration shows a binary case, while in real models a soft version of gating is applied, where a gating signal in the range of [0, 1] is often enforced with a logistic sigmoid function. By learning the gating signals, as detailed later in this section, S-LSTM provides a principal way of considering long-distance relationships via the input structures."}, {"heading": "4. Experiment Set-up", "text": "As discussed earlier, recursion is a fundamental process inherent in many problems.In this paper, we are using the proposed model to solve the semantic composition for the meaning of parts of text, a fundamental problem in understanding human languages.Specifically, within the Stanford Sentiment Tree Bank benchmark data (Socher et al., 2013), we are trying to determine the feeling of different grains of phrases in a tree. In order to get the feeling of a long piece of text, early work has often factored the problem of considering smaller parts of words or phrases with words or phrase models (Pang & Lee, 2008; Liu & Zhang, 2012).Recent work has begun to model the composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; 2013; Kalchbrenner et al., 2014), a more principled approach to modelling the composition of the semantics proposed in this paper."}, {"heading": "4.1. Data Set", "text": "The Stanford Sentiment Tree Bank (Socher et al., 2013) contains approximately 11,800 sentences from the film reviews originally reviewed in (Pang & Lee, 2005), which were analyzed using the Stanford parser (Klein & Manning, 2003) and phrases on all tree nodes were commented manually with mood readings.We use the same breakdown of training and test data as in (Socher et al., 2013) to predict the mood categories of the roots (sentences) and all phrases (including sentences).For the basic mood, the training, development and test kits are 8544, 1101 and 2210, respectively. Phrasentiment task includes 318582, 41447 and 82600 phrases for the three sentences. In the following (Socher et al., 2013) we also use classification accuracy to measure performance."}, {"heading": "4.2. Training Details", "text": "As already mentioned, we follow (Socher et al., 2013) to minimize the cross-entropy error for all nodes or roots only, depending on the specific experimental settings. For all phrases, the error is calculated as a regulated summer. c is the number of classes or categories, and j-c denotes the j-th element of the multinomic target distribution; i iterates over nodes, \u03b8 are model parameters, and \u03bb is a regulation parameter. We have adjusted our model to the evolutionary data as a split-in (Socher et al., 2013)."}, {"heading": "5. Results", "text": "In fact, it is that most people are able to determine for themselves what they want and what they don't want. (...) In fact, it is that most people are able to determine for themselves. (...) It is as if they are able to determine for themselves. (...) It is as if they are able to determine for themselves what they want. (...) It is as if they are able to determine for themselves. (...) It is as if they are able to do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it. (...) It is as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if"}, {"heading": "6. Conclusions", "text": "In this paper, we particularly examine tree structures in which the proposed S-LSTM memory cell can reflect the history memories of multiple descendants through gated copy of memory vectors; the model provides a principal way of looking at the composition layers across the structures; we used the model to learn distributed sentiment representations for texts, and demonstrated that it surpasses a modern recursive model by replacing its sorted composition layers with the S-LSTM memory blocks; we showed that the structural information is useful to help S-LSTM achieve state-of-the-art performance.The research community seems to contain two lines of wisdom; one tries to learn distributed representation by using structures when available, and the other prefers to believe that recursive neural networks can implicitly grasp the structures through a linear-chain coding process. In this paper, we also try to answer the question that is least helpful at the experimental level."}], "references": [{"title": "Conkey, p. finding linguistic structure with recurrent neural networks. lawrence erlbaum assoc publ", "author": ["N Chater"], "venue": "In in: proceedings of the fourteenth annual conference of the cognitive science society. (pp", "citeRegEx": "Chater,? \\Q1992\\E", "shortCiteRegEx": "Chater", "year": 1992}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "translation. CoRR,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning with compositional semantics as structural inference for subsentential sentiment analysis", "author": ["Choi", "Yejin", "Cardie", "Claire"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Choi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2008}, {"title": "Finding temporal structure in music: Blues improvisation with lstm recurrent networks", "author": ["Eck", "Douglas", "Schmidhuber", "Jrgen"], "venue": "In NEURAL NETWORKS FOR SIGNAL PROCESSING XII, PROCEEDINGS OF THE 2002 IEEE WORKSHOP,", "citeRegEx": "Eck et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Eck et al\\.", "year": 2002}, {"title": "Learning the longterm structure of the blues", "author": ["Eck", "Douglas", "Schmidhuber", "Jrgen"], "venue": "In IN PROC. INTL. CONF, pp", "citeRegEx": "Eck et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Eck et al\\.", "year": 2002}, {"title": "Learning taskdependent distributed representations by backpropagation through structure", "author": ["Goller", "Christoph", "Kchler", "Andreas"], "venue": "In In Proc. of the ICNN-96,", "citeRegEx": "Goller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Goller et al\\.", "year": 1996}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "PhD thesis, Technische Universitat Munchen,", "citeRegEx": "Graves and Alex.,? \\Q2008\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2008}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex.,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "CoRR, abs/1303.5778,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "A general framework for unsupervised processing of structured", "author": ["Hammer", "Barbara", "Micheli", "Alessio", "Sperduti", "Alessandro", "Strickert", "Marc"], "venue": null, "citeRegEx": "Hammer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hammer et al\\.", "year": 2004}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Ozan", "Cardie", "Claire"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Irsoy et al\\.,? \\Q2096\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2096}, {"title": "A convolutional neural network for modelling sentences", "author": ["Kalchbrenner", "Nal", "Grefenstette", "Edward", "Blunsom", "Phil"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Dan", "Manning", "Christopher D"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "A survey of opinion mining and sentiment analysis", "author": ["Liu", "Bing", "Zhang", "Lei"], "venue": "Mining Text Data,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Manning", "Christopher D", "Sch\u00fctze", "Hinrich"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Identifying purpose behind electoral tweets", "author": ["Mohammad", "Saif M", "Kiritchenko", "Svetlana", "Martin", "Joel"], "venue": "In Proceedings of the 2nd International Workshop on Issues of Sentiment Discovery and Opinion Mining,", "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Sentiment composition", "author": ["Moilanen", "Karo", "Pulman", "Stephen"], "venue": "In Proceedings of RANLP", "citeRegEx": "Moilanen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moilanen et al\\.", "year": 2007}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["P.H.O. Pinheiro", "R. Collobert"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Pinheiro and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Pinheiro and Collobert", "year": 2014}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Ng", "Andrew Y", "Manning", "Christopher D"], "venue": "In Proceedings of the 26th International Conference on Machine Learning (ICML),", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher", "Richard", "Huval", "Brody", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "An empirical study on the effect of negation words on sentiment", "author": ["Zhu", "Xiaodan", "Guo", "Hongyu", "Mohammad", "Saif", "Kiritchenko", "Svetlana"], "venue": "In Proceedings of ACL,", "citeRegEx": "Zhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Recent years have seen a revival of the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), with its effectiveness being demonstrated on a wide range of problems such as speech recognition (Graves et al., 2013), machine translation (Sutskever et al.", "startOffset": 201, "endOffset": 222}, {"referenceID": 23, "context": ", 2013), machine translation (Sutskever et al., 2014; Cho et al., 2014), and image-to-text conversion (Vinyals et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 1, "context": ", 2013), machine translation (Sutskever et al., 2014; Cho et al., 2014), and image-to-text conversion (Vinyals et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 24, "context": ", 2014), and image-to-text conversion (Vinyals et al., 2014),", "startOffset": 38, "endOffset": 60}, {"referenceID": 21, "context": "Image understanding, as another example, benefits from recursive modeling over structures, which yielded the state-of-the-art performance on tasks like scene segmentation (Socher et al., 2011).", "startOffset": 171, "endOffset": 192}, {"referenceID": 0, "context": "In addition, the literature has also included many other efforts of applying feedforward-based neural network over structures, including (Goller & Kchler, 1996; Chater, 1992; Starzyk et al.; Hammer et al., 2004), amongst others.", "startOffset": 137, "endOffset": 211}, {"referenceID": 9, "context": "In addition, the literature has also included many other efforts of applying feedforward-based neural network over structures, including (Goller & Kchler, 1996; Chater, 1992; Starzyk et al.; Hammer et al., 2004), amongst others.", "startOffset": 137, "endOffset": 211}, {"referenceID": 8, "context": "LSTM replaces the hidden vector of a recurrent neural network with memory blocks which are equipped with gates; it can in principle keep longterm memory by training proper gating weights (refer to (Graves, 2008) for intuitive illustrations and good discussions), and it has practically showed to be very useful, achieving the state of the art on a range of problems including speech recognition (Graves et al., 2013), digit handwriting recognition (Liwicki et al.", "startOffset": 395, "endOffset": 416}, {"referenceID": 23, "context": ", 2007; Graves, 2012), and achieve interesting results on statistical machine translation (Sutskever et al., 2014; Cho et al., 2014) and music composition (Eck & Schmidhuber, 2002b;a).", "startOffset": 90, "endOffset": 132}, {"referenceID": 1, "context": ", 2007; Graves, 2012), and achieve interesting results on statistical machine translation (Sutskever et al., 2014; Cho et al., 2014) and music composition (Eck & Schmidhuber, 2002b;a).", "startOffset": 90, "endOffset": 132}, {"referenceID": 8, "context": "In (Graves et al., 2013), a deep LSTM network achieved the state-of-the-art results on the TIMIT phoneme recognition benchmark.", "startOffset": 3, "endOffset": 24}, {"referenceID": 23, "context": "In (Sutskever et al., 2014; Cho et al., 2014), a pair of LSTM networks are trained to encode and decode human language for automatic machine translation, which is in particular effective for the more challenging long sentence translation.", "startOffset": 3, "endOffset": 45}, {"referenceID": 1, "context": "In (Sutskever et al., 2014; Cho et al., 2014), a pair of LSTM networks are trained to encode and decode human language for automatic machine translation, which is in particular effective for the more challenging long sentence translation.", "startOffset": 3, "endOffset": 45}, {"referenceID": 22, "context": "More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; 2013; Kalchbrenner et al., 2014), a more principled approach to modeling the formation of semantics.", "startOffset": 50, "endOffset": 150}, {"referenceID": 12, "context": "More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; 2013; Kalchbrenner et al., 2014), a more principled approach to modeling the formation of semantics.", "startOffset": 50, "endOffset": 150}, {"referenceID": 25, "context": ", the study of the effect of negation in changing sentiment (Zhu et al., 2014).", "startOffset": 60, "endOffset": 78}, {"referenceID": 16, "context": "Phrase-level sentiment analysis is often defined over a very small subset of phrases of interest, such as in the phrase-level task defined in (Wilson et al., 2005; Mohammad et al., 2013).", "startOffset": 142, "endOffset": 186}], "year": 2015, "abstractText": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-theart recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.", "creator": "LaTeX with hyperref package"}}}