{"id": "1503.05034", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "$gen$CNN: A Convolutional Architecture for Word Sequence Prediction", "abstract": "We propose a novel convolutional architecture, named $gen$CNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and $n$-best re-ranking in machine translation show that $gen$CNN outperforms the state-of-the-arts with big margins.", "histories": [["v1", "Tue, 17 Mar 2015 13:26:08 GMT  (1815kb,D)", "http://arxiv.org/abs/1503.05034v1", "10 pages"], ["v2", "Fri, 24 Apr 2015 08:44:05 GMT  (1815kb,D)", "http://arxiv.org/abs/1503.05034v2", "Accepted by ACL as full paper(oral)"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mingxuan wang", "zhengdong lu", "hang li", "wenbin jiang", "qun liu"], "accepted": true, "id": "1503.05034"}, "pdf": {"name": "1503.05034.pdf", "metadata": {"source": "CRF", "title": "genCNN: A Convolutional Architecture for Word Sequence Prediction", "authors": ["Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Wenbin Jiang", "Qun Liu"], "emails": ["wangmingxuan@ict.ac.cn", "jiangwenbin@ict.ac.cn", "liuqun@ict.ac.cn", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "Both language modeling (Wu and Khudanpur, 2003; Mikolov et al., 2010; Bengio et al., 2003) and text generation (Axelrod et al., 2011) boil down to modelling the conditional probability of a word given the progressing words. Previously, this was mostly done using purely memory-based approaches, such as n-grams that cannot handle long sequences and require some heuristics (so-called smoothing) for rare words. Another family of methods is based on distributed word representations that are normally associated with a neural network architecture (NN) to estimate the conditional probabilities of words. Two categories of neural networks were used for speech modeling: 1) recurrent neural networks (RNN), and 2) feedfoward network (FFN): \u2022 The RNN-based models, including their variants such as LM, enjoy greater flexibility due mainly to their popularity structures."}, {"heading": "2 Overview", "text": "As shown in Figure 1, genCNN as a whole is recursive, consisting of CNN-based processing units of two types: \u2022 \u03b1CNN as a \"front-end\" that deals with the story closest to the prediction; \u2022 \u03b2CNNs (which can repeat themselves) that are responsible for an \"older\" story. Together, CNN uses the story e1: t of arbitrary length to predict the next word et + 1 with probability type (et + 1 | e1: t), (1) based on a representation procedure produced by CNN (e1: t) and a | V | -class soft-max: p (et + 1 | e1: t)."}, {"heading": "3.1 \u03b1CNN: Convolution", "text": "In contrast to conventional CNN, the weights of the folding units are only partially divided in \u03b1CNN. Specifically, there are two types of characteristic cards in the folding units: \"TIME-FLOW\" and \"TIME-ARROW,\" each with the unfilled nodes and filled nodes in Figure 2. The parameters for \"TIME-FLOW\" are divided between different folding units, while the parameters in \"TIME-ARROW\" are location-dependent. Intuitively, \"TIME-FLOW\" works more like a conventional CNN (e.g. that in (Hu et al., 2014), which aims to understand the general temporal structure in the word sequences; \"TIME-ARROW,\" works more like a traditional NN-based language model (Vaswani et al., 2013; Bengio et al., f f f x., 2003): with its location-dependent parameters, it focuses on the pre-recording of the time task and the direction of the time room."}, {"heading": "3.2 Gating Network", "text": "This is essentially a soft template that focuses on tasks such as classification, but is undesirable for maintaining the compositional functionality of the convolution. In this paper we propose to use separate gating networks to release the convolution, and let them focus on the composition."}, {"heading": "3.3 Recursive Architecture", "text": "As suggested in Section 2 and Figure 1, we use additional CNNs with conventional weight distribution, called \u03b2CNN, to summarize the story outside the scope of \u03b1CNN. Specifically, the output of \u03b2CNN (with the same dimension of word embedding) is placed before the first word as input to \u03b1CNN, as shown in Figure 4. Unlike \u03b2CNN, \u03b2CNN is only designed to summarize the story, with the weight divided across its folding units. In a sense, \u03b2CNN only has TIME FLOW feature maps. All \u03b2CNN are identical and recursively aligned, which enables CNN to handle sentences of any length. We place a special switch after each \u03b2CNN to turn them off (and replace a selection vector shown as \"/\" in Figure 4) when no story is assigned. As a result, if the story is shorter than L\u03b1, the recursive structure is reduced to a point (in Figure 4) more suitable than sentences contained in Figure 4."}, {"heading": "3.4 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.4.1 TIME-FLOW vs. TIME-ARROW", "text": "Both conceptually and systemically, CNN has highlighted two interwoven treatments of word history. By using the globally divided parameters in the folding units, TIME-FLOW summarizes what has been said. TIME-FLOW's hierarchical folding architecture in TIME-FLOW allows it to model the composition in language, allowing for the representation of segments in different intermediate layers. TIME-FLOW is aware of the sequential direction inherited from CNN's spatial consciousness, but it is not sensitive enough about the predictive task due to the uniform weighting in the confrontation. On the other hand, TIME-ARROW, which lives in location-dependent parameters of the folding units, acts like an arrow pointing to the predictive task. TIME-ARROW has a predictive force that focuses all on itself, but on capturing the direction of time, and consequently, the long-term dependence on modelling."}, {"heading": "4.1 Implementation Details", "text": "In all our experiments (Sections 5 and 6), we set the maximum words for \"CNN\" to 30 and for \"CNN\" to 20. \"CNN\" has two folding layers (both containing \"TIMEFLOW\" and \"TIME-ARROW\" folding) and two folding layers, followed by a fully bonded layer (400 dimensions) and then a \"Soft-Max\" layer. \"The numbers of the characteristic cards for\" TIME-FLOW \"are 150 (1st folding layer) and 100 (2nd folding layer), while\" TIME-ARROW \"has the same characteristic scale.\" \u03b2CNN \"is relatively simple, with two folding layers containing only TIME-FLOW with 150 characteristic cards, two folding layers and a fully bonded layer.\" \"We use ReLU as an activation function for\" Convolution \"layers and switch to\" Sigmoid for fully bonded layers. \""}, {"heading": "5 Experiments: Sentence Generation", "text": "In this experiment, we randomly generate sentences through recurring samples of? t + 1 \u0445 p (et + 1 | e1: t; \u044b) and write the newly generated word into history until EOS (end of sentence) is generated. We consider the generation of two types of sentences: 1) simple sentences and 2) sentences with dependency sparsing, which are discussed in Section 5.1 and 5.2 respectively."}, {"heading": "5.1 Natural Sentences", "text": "We train genCNN for a week on wiki data with 112M words, with some representative examples randomly generated in Table 1 (upper and middle blocks). We try to do this with two settings, asking genCNN to 1) finish a sentence started by a human (upper block), or 2) create a sentence from the beginning (middle block), or it is fairly clear that genCNN can generate sentences that are syntactically grammatically and semantically significant. Specifically, most sentences can be aligned to a parse tree with a reasonable structure. It is also worth noting that quotation marks (\"and\") are always generated in pairs and in the correct order, even over a relatively long distance, as the first sentence in the upper block illustrates."}, {"heading": "5.2 Sentences with Dependency Tags", "text": "For training, we first analyze the English sentences (Klein and Manning, 2002) and feed sequences with dependency labels as follows (I? like (red? apple)) to genCNN, with 1) each paired parenthesis containing a subtree and 2) the symbol \"?\" indicating that the word next to it is the head of the dependency in the corresponding subtree. Some representative examples generated by genCNN are listed in Table 1 (bottom block). As suggests, genCNN is fairly accurate in adhering to the rules of parenthesis, and probably more noteworthy is that it can correct the head of the dependency tree most of the time."}, {"heading": "6 Experiments: Language Modeling", "text": "We evaluate our model as a language model both in terms of helplessness (Brown et al., 1992) and in terms of its effectiveness in reclassifying the n-best candidates from state-of-the-art models in statistical machine translation, both compared to the following competitive models. Competitor Models we compare genCNN with the following competitive models. \u2022 5-gram: We use the SRI Language Modeling Toolkit (Stolcke et al., 2002) to train a 5-gram language model with modified Kneser-Ney smoothing. \u2022 FFN-LM: The neural language model based on a feedfoward network (Vaswani et al., 2013). We vary the input window size from 5 to 20, while performance ceases to improve according to window size 20. \u2022 RNN: We use the implementation 1 of the RNN-based hidden-size language model with 600 for its optimal performance. \u2022 LM: We use the best parameters for the GroundST2, but also the best for the hybrid code, including the SchmidST2."}, {"heading": "6.1 Perplexity", "text": "We are testing the performance of genCNN on PENN TREEBANK and FBIS, two public datasets of different sizes.6.1.1 On PENN TREEBANK Although it is a relatively small dataset 3, PENN TREEBANK is widely used as a benchmark for language modeling (Graves, 2013; Mikolov et al., 2010). It has 930,000 words in the training set, 74,000 words in the validation set, and 82,000 words in the test set. In addition to the traditional test strategy, where the models remain unchanged during the test, Mikolov et al. (2010) suggests updating the parameters of genCNN et al. (2010) in an online fashion when performing test sets called \"dynamic evaluation.\""}, {"heading": "6.1.2 On FBIS", "text": "The FBIS corpus (LDC2003E14) is relatively large, with 22.5K sentences and 8.6M English words. The validation set is NIST MT06 and the test set is NIST MT08. In order to train the neural network, we limit the vocabulary to the most common 40,000 words covering 99.4% of the corpus. Similar to the first experiment, all words from the vocabulary are replaced by unknown ones and the EOS token is counted lost in the sequence.1http: / / rnnlm.org / 2https: / github.com / lisa-groundhog / GroundHog 3http: / / www.fit.vutbr.cz / \u0445 imikolov / rnnlm / simple-examples.tgzFrom Table 3 (upper block), CNN once again gains significantly compared to its competitors, with over 25 points ahead of LSTM (in its optimal setting), the second best overall performance of IMtamper (the second IMtamper)."}, {"heading": "6.2 Re-ranking for Machine Translation", "text": "In this experiment, we rearrange the 1000 best English translation candidates for Chinese sentences generated by a statistical machine translation system (SMT) and compare them with other language models in the same settings.SMT setup The hierarchical phrase-based SMT system (Chines \u2192 English) was built with Moses, a widely recognized state of the art, with default settings.The bilingual training data comes from the NIST MT2012, with a reduced size of 1.1 million sentence pairs, which use selection strategies in (Axelrod et al., 2011).The starting point is based on a conventional 5 gram language model (LM), which is based on a modified Kneser-Ney smoothing (Chen and Goodman, 1996) on the English side of the 329Mword Xinhua part of the English Gigaword (LDC2011T07), on the conventional 5 gram language model of the LFFM 1996 (LFFM-07) with modified XinT1 gram (XinT1-2011gram)."}, {"heading": "7 Related Work", "text": "In addition to the long handbook on the neural network-based language model (Auli et al., 2013; Mikolov et al., 2010; Graves, 2013; Bengio et al., 2003; Vaswani et al., 2013), our work is also related to efforts to model long-range dependence in word sequence prediction (Wu and Khudanpur, 2003). Unlike working on craft features to include long-range dependencies, our model can uniformly assimilate relevant information in both the long and short range with the bottom-up flow of information and the revolutionary architecture. CNN is widely used in computer vision and language (Lawrence et al., 1997; Krizhevsky et al., 2012; LeCun and Bengio, 1995; Abdel-Hamid et al., 2012), and more recently in sentence representation (Kalchbrenner and Blunsom, 2013), the conformity (this word classification, Het al), and the conformity (this word classification)."}, {"heading": "8 Conclusion", "text": "We propose a revolutionary architecture for the generation and modeling of natural language. Our extensive experiments in sentence generation, confusion, and re-evaluation of machine translation show that our model can significantly improve the state of the art."}], "references": [{"title": "Hui Jiang", "author": ["Ossama Abdel-Hamid", "Abdel-rahman Mohamed"], "venue": "and Gerald Penn.", "citeRegEx": "Abdel.Hamid et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Chris Quirk", "author": ["Michael Auli", "Michel Galley"], "venue": "and Geoffrey Zweig.", "citeRegEx": "Auli et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Xiaodong He", "author": ["Amittai Axelrod"], "venue": "and Jianfeng Gao.", "citeRegEx": "Axelrod et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Pascal Vincent", "author": ["Yoshua Bengio", "Rjean Ducharme"], "venue": "and Christian Jauvin.", "citeRegEx": "Bengio et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Jennifer C", "author": ["Peter F. Brown", "Vincent J. Della Pietra", "Robert L. Mercer", "Stephen A. Della Pietra"], "venue": "Lai.", "citeRegEx": "Brown et al.1992", "shortCiteRegEx": null, "year": 1992}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Chen", "Goodman1996] Stanley F Chen", "Joshua Goodman"], "venue": "In Proceedings of the 34th annual meeting on Association for Computational Linguistics,", "citeRegEx": "Chen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1996}, {"title": "and Geoffrey E", "author": ["George E Dahl", "Tara N Sainath"], "venue": "Hinton.", "citeRegEx": "Dahl et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Richard Schwartz", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar"], "venue": "and John Makhoul.", "citeRegEx": "Devlin et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Elad Hazan", "author": ["John Duchi"], "venue": "and Yoram Singer.", "citeRegEx": "Duchi et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Hang Li", "author": ["Baotian Hu", "Zhengdong Lu"], "venue": "and Qingcai Chen.", "citeRegEx": "Hu et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Edward Grefenstette", "author": ["Nal Kalchbrenner"], "venue": "and Phil Blunsom.", "citeRegEx": "Kalchbrenner et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast exact inference with a factored model for natural language parsing", "author": ["Klein", "Manning2002] Dan Klein", "Christopher D Manning"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Klein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2002}, {"title": "Ilya Sutskever", "author": ["Alex Krizhevsky"], "venue": "and Geoffrey E Hinton.", "citeRegEx": "Krizhevsky et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Ah Chung Tsoi", "author": ["Steve Lawrence", "C Lee Giles"], "venue": "and Andrew D Back.", "citeRegEx": "Lawrence et al.1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Convolutional networks for images, speech, and time series", "author": ["LeCun", "Bengio1995] Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Ilya Sutskever", "author": ["Chris J. Maddison", "Aja Huang"], "venue": "and David Silver.", "citeRegEx": "Maddison et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Jan Cernocky", "author": ["Tomas Mikolov", "Martin Karafit", "Lukas Burget"], "venue": "and Sanjeev Khudanpur.", "citeRegEx": "Mikolov et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Greg Corrado", "author": ["Tomas Mikolov", "Kai Chen"], "venue": "and Jeffrey Dean.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Och", "Ney2002] Franz Josef Och", "Hermann Ney"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Och et al\\.", "year": 2002}, {"title": "and Christopher D", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng"], "venue": "Manning.", "citeRegEx": "Socher et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings of the international conference on spoken language processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc V Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Victoria Fossum", "author": ["Ashish Vaswani", "Yinggong Zhao"], "venue": "and David Chiang.", "citeRegEx": "Vaswani et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum entropy language modeling with non-local dependencies", "author": ["Wu", "Khudanpur2003] Jun Wu", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2003}], "referenceMentions": [], "year": 2017, "abstractText": "We propose a novel convolutional architecture, named genCNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the state-of-the-arts with big margins.", "creator": "LaTeX with hyperref package"}}}