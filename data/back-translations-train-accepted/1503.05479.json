{"id": "1503.05479", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2015", "title": "Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm", "abstract": "We consider the problem of recovering a low-rank tensor from a noisy observation. Previous work has shown $O(n^{\\lceil K/2\\rceil/2})$ recovery guarantee for recovering a $K$th order rank one tensor of size $n\\times ...\\times n$ by an algorithm called recursive unfolding. In this paper, we first improve this to $O(n^{K/4})$ by a much simpler approach but with a more careful analysis. Then we propose a new norm based on the Kronecker products of factors obtained by the proposed simple estimator. The imposed Kronecker structure of the new norm allows us to show a nearly ideal $O(\\sqrt{n}+\\sqrt{m})$ bound for the proposed subspace norm, in which the parameter $m$ controls the blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore we empirically demonstrate that with $m=O(1)$, the proposed norm achieves near ideal denoising performance.", "histories": [["v1", "Wed, 18 Mar 2015 16:45:04 GMT  (70kb,D)", "https://arxiv.org/abs/1503.05479v1", null], ["v2", "Tue, 27 Oct 2015 01:44:23 GMT  (79kb,D)", "http://arxiv.org/abs/1503.05479v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["qinqing zheng", "ryota tomioka"], "accepted": true, "id": "1503.05479"}, "pdf": {"name": "1503.05479.pdf", "metadata": {"source": "CRF", "title": "Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm", "authors": ["Qinqing Zheng", "Ryota Tomioka"], "emails": [], "sections": [{"heading": null, "text": "\u221a n + \u221a HK \u2212 1) limit, in which the parameter H controls the fading from the non-convex estimator to the mode-wise standard minimization. Furthermore, we show empirically that the subspace standard also achieves the almost ideal mitigation performance with H = O (1)."}, {"heading": "1 Introduction", "text": "The question of \"why\" and \"why,\" the question of \"why\" and \"why,\" the question of \"why,\" the question of \"why\" and \"why,\" the question of \"why,\" the question of \"why,\" the question of \"why,\" the question of \"why,\" the question of \"why,\" the question of \"why,\" the question of \"why,\" the question of \"why,\" the question of \"why,\" the question of \"how,\" the question of \"how,\" the question of \"why,\" the question of \"why,\" the question of \"why,\" the question of \"why.\""}, {"heading": "2 The power of ordinary unfolding", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 A perturbation bound for the left singular vector", "text": "We first determine a limit for restoring the left singular vector of a series n > > \u03b2-matrix (with m > \u03b2-matrix) disturbed by random Gaussian noise. Consider the following model, known as the information and noise model [4]: X = \u03b2uv > + \u03b2-matrix, (1) where u and v are unit vectors, \u03b2 is the signal strength, \u03c3 is the standard deviation of noise, and the noise matrix E is assumed to be random when the entries are evaluated from the standard normal distribution i.i.d. Our goal is to reduce the correlation between u and the uppermost left-ssingular vector u of the X-matrix index, and to enter the noise matrix index (mn) 1 / 4 with high probability.A direct application of the classic Wedin error theorem [28] to the desired matrix does not yield the desired result."}, {"heading": "2.2 Tensor Unfolding", "text": "Now we apply the above result to the tensor version of the information plus the noise model we are examining. We consider a ranking of n \u00b7 \u00b7 \u00b7 \u00b7 n tensor (signal) contaminated by Gaussian as follows: Y = X + \u03c3E = \u03b2u (1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (K), (2), where the factors u (k), k = 1,. \u2212 K, are vectors that are not necessarily identical, and the entries of E \u00b7 \u00b7 \u00b7 n are i.d samples from the normal distribution N (0, 1). Note: This is somewhat more general (and easier to analyze) than the symmetric setting investigated by [20]. Several estimates for restoring X from its noisy version Y have been proposed (see Table 1). Both the overlapped nuclear standard and latent nuclear standard are discussed in terms of the performance guarantee."}, {"heading": "3 Subspace norm for tensors", "text": "Suppose the true tensor X \u0445 Rn \u00b7 \u00b7 \u00b7 \u00b7 n permits a minimum Tucker decay (26] of rank (R,.., R) (26). (5) If the nuclear tensor C = (\u03b2i1... iK = 1 \u00b7 \u00b7 \u00b7 R is superdiagonal, the above decomposition is reduced to the canonical polyadic (CP) decay [15]. The mode-k unfolding of the true tensor X-II can be described as follows: X \u0445 (k) = U (K) C (k) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 K (U (k \u2212 1) of decomposition (1). (5) The mode-k unfolding of the true tensor X-II is inevitable."}, {"heading": "3.1 The subspace norm", "text": "Consider a tensor of the Kth order of magnitude n \u00b7 \u00b7 \u00b7 n.Definition 1. The subspace norm for a tensor of the Kth order X, associated with [[S (k)]] Kk = 1 is defined as | | X | | s: = inf {M (k)} Kk = 1 x x K = 1 x M (k) x, if X-Span ([S (k)} Kk = 1 x, if X-Span ([S (k)]} Kk = 1 x, if X-Span ([S (k)} Kk = 1 x,."}, {"heading": "3.2 Choosing the subspace", "text": "A natural question that arises is how to select the matrices S (1),., S (k).Lemma 2. Assume that R (k), K (k), K (k), K (k), K (k), K (k), K (k), K (k), K (1), R (n) and U (k) have a full column rank. It applies that for all k, i) U (k), K (k), K (k), K (k), K (1), K (1), K (1), K (1), P (k + 1), K (k). Proof: We prove the problem in Appendix D.4.Episode 1 that if the signal noise ratio is high enough, P (k \u2212 1), P (k \u2212 1), P (k), P (k + 1), P (k + 1), K (K), K (K), K (K), K (k), K (k, k), K (k, k), K (k), K (k, k, k), K (k), K (k, K, K (k), K (k)."}, {"heading": "3.3 Analysis", "text": "Let us know Y-Rn-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "4 Experiments", "text": "In this section, we perform tensor analyses on synthetic and real data sets to numerically confirm our analysis in earlier sections."}, {"heading": "4.1 Synthetic data", "text": "The true factors are generated as random matrices with orthonormal columns. Y is then generated by adding Gauss's error with standard deviation from the standard deviation to X *. We assume that CP knows that the true rank is 2. For the subspace standard, we use algorithm 2, which is described in section 3. We also select the top 2 singular vectors when constructing U (k) s. We calculated the solutions for 20 values of regulation parameters logarithmically between 1 and 100. For the overlapped standard and the latent standard, we use ADMM, which is described in [25]. We calculated the solutions for 20 values of regulation logarithmically."}, {"heading": "4.2 Amino acids data", "text": "The amino acid dataset [5] is a semi-realistic dataset commonly used as a benchmark for modelling low-level tensors. It consists of five laboratory samples, each containing different amounts of tyrosine, tryptophan and phenylalanine. As the true factors are these three acids, these data fit perfectly with the CP model. The true rank is fed into CP and the proposed asH = 3 approach. We have calculated CP's solutions for 20 different random initializations, and the solutions of other approaches with 20 different sizes of \u03bb. For the subspace and the overlapped approach, these data are logarithmically placed between 103 and 105. For the latent approach, CP's solutions are logarithmically divided between 104 and 106, although we have calculated the larger numbers compared to the smallest (2)."}, {"heading": "5 Conclusion", "text": "We have eliminated a conjecture raised by [20] and shown that the signal-to-noise ratio of O (nK / 4) is sufficient even for odd order tensors. Furthermore, our analysis shows an interesting two-phase behavior of the error. This realization led us to the development of the proposed subspace standard. The proposed standard is defined in terms of a series of orthonormal matrices P (1),..., P (K), which are estimated by fashion singular values. We have analyzed the denositive performance of the proposed standard and shown that the error can be limited by the sum of two terms, which can be interpreted as an approximation error date from the first (non-convex) step and as an estimated error date from the second (convex) step."}, {"heading": "A Maximum likelihood estimator", "text": "Let us have the following performance guarantees for X-K: X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X"}, {"heading": "B Details of optimization", "text": "To solve the problem (8) we follow the alternative direction method of multipliers described in [25]. We scale the objective function in (8) by 1 / \u03bb and consider the dual problem in D, (W (k)) Kk = 1\u03bb 2 | | | D | | | 2F \u2212 < D, Y > s.t. max k \u00b2 W (k). \u2212 Z (k) s are auxiliary variables introduced to limit the equality of problems.Algorithm 1: Tensor denoising via the subspace norm Input: noisy tensor Y, subspace dimension H, regularization constant for k = 1 to do P (k)."}, {"heading": "C Additional experiments", "text": "We report the experimental results, if the input rank of CP and the subspace approach are overspecified, on the same synthetic dataset as Section 4. We consider the case where the input rank is 8.algorithm 2: ADMM for subspace standard minimizationInput: Y, \u03bb, S (1),.., \u03b7, initializations D0, {M (1) 0,.., M (K) 0} t = 0 repeatDt + 1 = 1\u03bb + \u03b7K (Y + K\u03b7Dt \u2212 \u2211 k foldk (((((2M (k) t \u2212 M (k) t \u2212 1) S (k) >))) for k = 1 to K do M (k) t + 1 = prox tr \u03b7 (M (k) t + \u03b7D (k), t + 1S (k))."}, {"heading": "D Proofs", "text": "I would like to thank all those who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles."}, {"heading": "E Generalization of Theorem 1 to the higher rank case", "text": "Theorem 4 (\u03b2R) 2 (\u03b2R) 2 (\u03b2R / \u03b2R) 2 (\u03b2R / \u03b2R) 2 (\u03b2R / \u03b2R) 2 (\u03b2R / \u03b2R) 2 (\u03b2R / \u03b2S) 2 (\u03b2R / \u03b2S) 2 (\u03b2R / \u03b2S) 2 (\u03b2R / S) 2 (\u03b2R / S) 2 (\u03b2R / S) 2 (\u03b2R / S) 2 (S / S) 2 (S / S) 2 (S) 2 (S) 2 (S) 2 (U) 2 (U) 2 (U) 2 (S)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>We consider the problem of recovering a low-rank tensor from its noisy observation. Previ-<lb>ous work has shown a recovery guarantee with signal to noise ratioO(ndK/2e/2) for recovering<lb>a Kth order rank one tensor of size n\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 n by recursive unfolding. In this paper, we first<lb>improve this bound to O(nK/4) by a much simpler approach, but with a more careful analysis.<lb>Then we propose a new norm called the subspace norm, which is based on the Kronecker prod-<lb>ucts of factors obtained by the proposed simple estimator. The imposed Kronecker structure<lb>allows us to show a nearly ideal O(<lb>\u221a<lb>n+<lb>\u221a<lb>HK\u22121) bound, in which the parameter H controls<lb>the blend from the non-convex estimator to mode-wise nuclear norm minimization. Further-<lb>more, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising<lb>performance even with H = O(1).", "creator": "LaTeX with hyperref package"}}}