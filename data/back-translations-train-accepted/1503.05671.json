{"id": "1503.05671", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2015", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature", "abstract": "We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as factoring as Kronecker products between two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods such as Hessian-free methods, K-FAC works very well in highly stochastic optimization regimes.", "histories": [["v1", "Thu, 19 Mar 2015 08:30:24 GMT  (995kb,D)", "http://arxiv.org/abs/1503.05671v1", null], ["v2", "Fri, 3 Apr 2015 20:19:14 GMT  (977kb,D)", "http://arxiv.org/abs/1503.05671v2", "Fixed various typos. Tweaked experiments and updated figures. Expanded a few discussions and added a small subsection discussing approximation quality plots"], ["v3", "Tue, 28 Apr 2015 05:48:59 GMT  (1020kb,D)", "http://arxiv.org/abs/1503.05671v3", "Changes in this version: Tweaked experiments and updated figures (using uniform l_2 = 1e-5 now). Added a section with pseudocode. Revamped damping section, adding lots of discussion, and now recommending separate damping strength adjustment for approximate Fisher (see Section 6.6). Various other minor tweaks and fixes"], ["v4", "Thu, 21 May 2015 00:25:06 GMT  (1066kb,D)", "http://arxiv.org/abs/1503.05671v4", "Changes in this version: Redid matrix quality plots to use tanh units to better highlight difference between exact Fisher and approximation (previously used logistic units added a big low-rank component which was well-approximated). Added discussion to damping section, and a graph looking at the importance of re-scaling. Adding comparisons to momentumless K-FAC. Other minor fixes and tweaks"], ["v5", "Fri, 24 Jul 2015 02:30:35 GMT  (1090kb,D)", "http://arxiv.org/abs/1503.05671v5", "Added some diagrams of neural networks (original and transformed versions), and added a conclusions section. Made a few other minor tweaks and fixes"], ["v6", "Wed, 4 May 2016 00:29:33 GMT  (1090kb,D)", "http://arxiv.org/abs/1503.05671v6", "Various minor additions, corrections and tweaks. Added link to code"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["james martens", "roger b grosse"], "accepted": true, "id": "1503.05671"}, "pdf": {"name": "1503.05671.pdf", "metadata": {"source": "CRF", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature", "authors": ["James Martens", "Roger Grosse"], "emails": ["jmartens@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to go in search of a solution."}, {"heading": "2 Background and notation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural Networks", "text": "In this section, we will define the basic notation for the supplying neural networks that we use in this work = > Q = > Value. Note that this presentation is closely related to that of Martens (2014).A neural network transforms its a0 = x input into output f (x, \u03b8) = a 'through a series of \"objective\" layers, each of which consists of a bank of units / neurons, each receiving a weighted sum of outputs of units from the previous layer and calculating their output via a nonlinear \"activation function.\" We name the vector of these weighted sums for the i layer, and by ai the vector of unit outputs (aka \"activities\").The precise calculation performed on each layer i, \"is given as follows: si = Wia = Wia \u2212 1ai = (si), where it is an elementary nonlinear function."}, {"heading": "2.2 The Natural Gradient", "text": "Since our network defines a conditional model Py | x (\u03b8), it has an associated Fisher information matrix (which we will simply call \"the Fisher\"), which is given by F = E [d log p (y | x, \u03b8) d\u03b8d log p (y | x, \u03b8) d\u03b8 >] = E [D\u03b8D\u03b8 >]. Here, the expectation regarding the data distribution Qx via inputs x and the predictive distribution of the model Py | x (\u03b8) via y is taken. Since we do not normally have access to Qx, and the above expectation would probably be intractable, we calculate F using the training distribution Q-x via inputs x instead. The well-known natural gradient (Amari, 1998) is defined as F \u2212 1 \u0394h from the perspective of information geometry (Amari and Nagaoka, 2000)."}, {"heading": "3 A block-wise Kronecker-factored Fisher approximation", "text": "(D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i (D) i) i (D) i) i (D) i (D) i) i (D) i (D) i) i \"e\" e \"e\" e \"i\" i \"e\" e \"e\" e \"e\" e \"e\" e \"e\" e \"e\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" e \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" e \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"(D) i) i) i\" i \"i\" i \"e\" i \"(D) i) i) i (D) i) i (D) i) i) i (D) i) i) i) i (D) i) i (D) i) i) i) i (D) i (D) i) (D) i) i) i) i) i (D) (D) i) i) i) i (D) i) i) i) (D) (D) i) i) (D) i) (D) i) i) (D) i) i) (D) i) i) i) (D) i) (D) i) i) (D) i) (D) i"}, {"heading": "3.1 Interpretations of this approximation", "text": "The approximation given by eqn is equivalent to establishing the following approximation for each pair of weights: E [Wi] k3, k4] k3, and g [G] k4, and g [G] k4. The approximation given by eqn is equivalent to establishing the following approximation for each pair of weights: E [G] k3, g [G] k3, and g [G] k3, and g [G] k4. The approximation given by eqn is equivalent to establishing the following approximation for each pair of weights: E [W] k3, k4] k3, and g [G] k4, and g [G] k4, and g [G] k4."}, {"heading": "4 Additional approximations to F\u0303 and inverse computations", "text": "To the best of our knowledge, there is no efficient general method for reversing a khatri-rao product such as F. Therefore, if we hope to achieve an efficiently calculated approach to the reverse fisherman, we must make further approximations. In the following sections, we argue that the inverse of F-Rao can reasonably be approached as one of two special structures, each of which makes it efficiently calculable; the second will be slightly less restrictive than the first (and therefore a better approximation) at the expense of additional complexity; we will then show how matrix vector products can be efficiently calculated using these approximate reversals, creating an efficient algorithm for calculating an approximation to the natural gradient."}, {"heading": "4.1 Structured inverses and the connection to linear regression", "text": "Suppose we are faced with a multivariate distribution over some variables whose covariance matrix is DWS. Let us define the matrixB such that for i 6 = j [B] i, j the coefficient on the j-th variable is in the optimal, linear prediction of the i-th variable of all other variables, and [B] i, j = 0 for i = j. Then we define the matrix D to be the diagonal matrix where [D] i, i is the variance of error associated with the i-th variable.Pourahmadi (2011) showed that B can be achieved from the inverse covariance of the data \u2212 1 by the formula [B] i, j [D] i, j [D] i and [D] i = 1 [D] i [D] i [D] i] i, iFrom this it follows that the inverse covariance of the data can be expressed \u2212 1 (I \u2212 D)."}, {"heading": "4.2 Approximating F\u0303\u22121 as block-diagonal", "text": "A natural choice for such an approximation of F-1 as a block diagonal is to take the block diagonal of F-1 as that of F-1. This results in the matrix F-1 = diag (F-1,1, F-2,2,...) = diag (A-0,0 G1,1, A-1,1 G2,2,..., A-1-1 G, \"), whose inversion can easily be calculated as F-1 = diag (A-10,0 G-11,1, A-11,1 G-12,2,.., A-1 '-1 G-1',\"). Thus, the calculation of F-1 = diag (A-10,0 G-11,1, A-11,1 G-12,2, \") results."}, {"heading": "4.3 Approximating F\u0303\u22121 as block-tridiagonal", "text": "It's not as if it's going to be a way of doing it, but a way of doing it. (...) We're assuming that we're going to do it. (...) We're assuming that we're going to do it. (...) We're assuming that we're going to do it. (...) We're assuming that we're going to do it. (...) We're assuming that we're going to do it. (...) We're going to do it. (...) We're going to do it. (...) We're going to do it. (...) We're going to do it. (...) We're going to do it. (...) We're going to do it. (...) We're going to do it. (...) We're going to do it. (...) We're going to do it. (...) We're going to do it."}, {"heading": "5 Estimating the required A\u0304i,j\u2019s and Gi,j\u2019s", "text": "It is important that this expectation should not be taken into account in terms of training / data distribution (i.eQ)."}, {"heading": "6 Update damping", "text": "The idealized natural gradient approach is to follow the smooth path 3 in the belt mania multiplicity (implied by the Fisher information matrix, which is considered a metric tensor) that is generated by taking a series of infinitely small steps (in the original parameter space) toward the natural gradient (which is recalculated at each point). While this is clearly impracticable as a real optimization method, one can take larger steps and still roughly follow these paths. However, in our experience, in order to obtain an update that meets the minimal requirement not to degrade the objective functional value, one must make the step size so small that the resulting optimization algorithm is impracticable.The reason that the natural gradient is defined only as a short distance is that it is defined only as an optimal direction (the improvements in the objective versus changes in the predictable distribution) and not as a discrete actualization."}, {"heading": "6.1 A factored Tikhonov regularization technique", "text": "IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV"}, {"heading": "6.2 Re-scaling according to the exact F", "text": "Considering an update proposal generated by multiplying the negative gradient, the second stage of our proposed attenuation technology scales \"according to the square model M.\" Specifically, we will select \"\u03b1\" so that the square model \"M\" is optimized, calculated using an estimate of the exact Fisher-F \"(to which we also add the\" 2 regularization problem + Tikhonov term \"(\u03bb + \u03b7) I. In particular, we minimize the following function with respect to\" \u03b1 \":\" M \"(3) =\" M \"(\u03b1) =\" H \") I\" + \"(\" H \") +\" H. \"Since it is a 1-dimensional minimization, the formula for the optimal\" H \"x\" can be calculated."}, {"heading": "6.3 Adapting \u03bb", "text": "It is well known (e.g. Nocedal and Wright, 2006) that optimizing a quadratic function whose curvature matrix is modified by adding \u03bbI means the same thing without this modification, but subject to limitations that the solution lies within a spherical \"trust region\" of radius \u03c4. Relationship \u03c4 to \u03bb is complicated and depends on the properties of the curvature matrix (which is constantly changing), so it is often easier simply to adapt it directly. The Levenberg-Marquardt rule, which is used by HF for this purpose and which we will also adopt, exists if \u03c1 > 3 / 4 then \u03bb."}, {"heading": "7 Momentum", "text": "Sutskever et al. (2013) found that momentum (Polyak, 1964; Plaut et al., 1986) was very helpful in the context of stochastic gradient descend optimization of deep neural networks; a version of momentum is also present in the original RF method, and it probably plays an even more important role in the more \"stochastic\" versions of RF (Martens and Sutskever, 2012; Kiros, 2013); a natural method of adding an impulsive effect to the K-FAC, and one that we have found good in practice, is to take the update, the result of which is the same = \u03b1 + \u03b2 0, where \u03b40 is the final update calculated in the previous iteration, and where \u03b1 and \u03b2 are chosen to minimize M. This enables K-FAC to effectively build a better solution to the local quadratic optimization problem."}, {"heading": "8 Computational Costs and Efficiency Improvements", "text": "The significant calculation tasks required for the calculation of a single update / iteration of K-FAC, and the rough estimates of their associated calculation costs, are indeed as follows: 1. Standard forward and backward with random targets (as described in Section 5): C1 'd2m4. Updating the estimates of the required behavior of A, j and Gi, j's of quantities calculated in the forward passage and the additional randomized backward passage with random targets (as described in Section 5): C1' d2m4. Updating the estimates of the required behavior of A, j and Gi, j's of quantities used in the forward passage and the additional randomized backward passage with random targets (as described in Section 5): C1 'd2m4. Updating the estimates of the required behavior of A and Gi, quantities of passive, quantity j'j'j'j'j'j's, quantitative activities, and additional passj'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j'j's are used in the quantitative and auxiliary activities."}, {"heading": "9 Invariance Properties and the Relationship to Whitening and", "text": "This invariance means that the smooth path through the distribution space is only approximately maintained, as Martens (2014) has shown, the approximation error will go to zero, since the effects of the vamping-based optimization method and the repairing function will amount to a locally linear function. The latter will happen as the local region, which uses an approximation of natural gradients to update shrinkage to zero.Because K-FAC is not applicable."}, {"heading": "10 Related Work", "text": "The Hessian optimization method by Martens (2010) uses linear conjugate gradients (CG) to optimize local square models of the shape of eqn. 5 (subject to an adaptive Tikhonov attenuation technology) instead of direct solutions. As discussed in the introduction, the main advantages of K-FAC over RF are twofold. First, K-FAC uses an efficiently calculable direct solution for reversing the curve matrix, thus avoiding the costly matrix vector products associated with running CG within RF. Second, it can estimate the curve matrix from a set of data using exponentially decayed averages, as opposed to the relatively small fixed mini-batches used by RF."}, {"heading": "11 Experiments", "text": "In fact, most of them are able to survive themselves if they do not feel able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are not able to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves."}, {"heading": "Acknowledgments", "text": "We thank Google and NSERC for their support."}, {"heading": "A Derivation of the expression for the approximation from Sec-", "text": "In this section we shall show that E [a] a (1) a (2) a (2) a (2) a (2) b (2) b (2) b (2) b (2) b (2) b (2) b (2) b (1) b b (1) b (2) b (2) b (2) b (2) b (2) b (2) b (2) b (2) b) b (2) b (2) b (1) b) b (1) b) b (1) b (1) b) b (2) b (2) b) b (b) b (2) b) b (2) b) b (2) b (2) b) c (c) c (2) c (c) c (c) c (2) c c (c) c (c) c (c) c (2) c (c) c (c) c (c) c (c) c (c) c) c (c) c) c (c) (c) c) c (c) c) (c) (c) c) (c) (c) c) (c) (c) (c) c) (c) (c) c) (c) (c) (c) c) (c) (c) (c) c) (c) (c) c) c (c) c (c) (c) c (c) (c) c) (c) c (c) (c) c) c (c) (c) (c) c (c) (c) c) (c) c) (c) c (c) (c) (c) c) (c) c) (c) c (c) (c) (c) (c) (c) c) (c) c) (c) (c) (c) (c) (c) (c) (c) c) (c) (c) (c) c) c) (2) c) c) c (c) c (c) (c) (c) (c) c) (c) (c) (c) (c) (c) c) (c) c) (c) (c) c) c) ("}, {"heading": "B Efficient techniques for inverting A\u2297B \u00b1 C \u2297D", "text": "D urally, he says, \"it is as if it is a country where it is a country where it is not a country.\""}, {"heading": "D Proofs for Section 9", "text": "The proof of Theorem 1 first shows that the given mesh transformation can be regarded as the repair of the mesh according to an invertable linear function (using an invertable linear function).Let us define the function that we can prove by a simple induction. First, it must be noted that an invertable linear transformation.If the transformed network uses \"i\" instead of \"i\" (using \"i\" and \"i\") and \"s\" (using a simple induction) an invertable linear transformation.If the transformed network uses \"i\" instead of \"i,\" then we have \"i\" and \"s\" i \"(using\" i \"and\" i \") havea\" (using \"i\") havea \"havea\" havea \"havea\" haves \"havea\" haves \"havea\" have. \""}], "references": [{"title": "Methods of Information Geometry, volume 191 of Translations of Mathematical monographs", "author": ["S. Amari", "H. Nagaoka"], "venue": null, "citeRegEx": "Amari and Nagaoka.,? \\Q2000\\E", "shortCiteRegEx": "Amari and Nagaoka.", "year": 2000}, {"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural Computation,", "citeRegEx": "Amari.,? \\Q1998\\E", "shortCiteRegEx": "Amari.", "year": 1998}, {"title": "Information-geometric optimization algorithms: A unifying picture via invariance principles", "author": ["Ludovic Arnold", "Anne Auger", "Nikolaus Hansen", "Yann Ollivier"], "venue": null, "citeRegEx": "Arnold et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arnold et al\\.", "year": 2011}, {"title": "Improving the Convergence of Back-Propagation Learning with Second Order Methods", "author": ["Sue Becker", "Yann LeCun"], "venue": "Proceedings of the 1988 Connectionist Models Summer School,", "citeRegEx": "Becker and LeCun.,? \\Q1989\\E", "shortCiteRegEx": "Becker and LeCun.", "year": 1989}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M. Bishop"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Solution of the sylvester matrix equation AXB + CXD = E", "author": ["Judith D. Gardiner", "Alan J. Laub", "James J. Amato", "Cleve B. Moler"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "Gardiner et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Gardiner et al\\.", "year": 1992}, {"title": "Scaling up natural gradient by factorizing fisher information", "author": ["Roger Grosse", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Grosse and Salakhutdinov.,? \\Q2014\\E", "shortCiteRegEx": "Grosse and Salakhutdinov.", "year": 2014}, {"title": "On \u201cnatural\u201d learning and pruning in multilayered perceptrons", "author": ["Tom Heskes"], "venue": "Neural Computation,", "citeRegEx": "Heskes.,? \\Q2000\\E", "shortCiteRegEx": "Heskes.", "year": 2000}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Training neural networks with stochastic Hessian-free optimization", "author": ["Ryan Kiros"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kiros.,? \\Q2013\\E", "shortCiteRegEx": "Kiros.", "year": 2013}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["Nicolas Le Roux", "Pierre-antoine Manzagol", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Roux et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. M\u00fcller"], "venue": "Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sharpness in rates of convergence for cg and symmetric Lanczos methods", "author": ["R-C Li"], "venue": "Technical Report 05-01,", "citeRegEx": "Li.,? \\Q2005\\E", "shortCiteRegEx": "Li.", "year": 2005}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "Martens.,? \\Q2010\\E", "shortCiteRegEx": "Martens.", "year": 2010}, {"title": "New perspectives on the natural gradient method, 2014, arXiv:1411.7717", "author": ["J. Martens"], "venue": null, "citeRegEx": "Martens.,? \\Q2014\\E", "shortCiteRegEx": "Martens.", "year": 2014}, {"title": "Training deep and recurrent networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Martens and Sutskever.,? \\Q2012\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2012}, {"title": "Estimating the Hessian by backpropagating curvature", "author": ["J. Martens", "I. Sutskever", "K. Swersky"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Martens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2012}, {"title": "Riemannian metrics for neural networks. 2013, arXiv:1303.0818", "author": ["Yann Ollivier"], "venue": null, "citeRegEx": "Ollivier.,? \\Q2013\\E", "shortCiteRegEx": "Ollivier.", "year": 2013}, {"title": "An improved newton iteration for the generalized inverse of a matrix, with applications", "author": ["V. Pan", "R. Schreiber"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Pan and Schreiber.,? \\Q1991\\E", "shortCiteRegEx": "Pan and Schreiber.", "year": 1991}, {"title": "Adaptive natural gradient learning algorithms for various stochastic models", "author": ["H. Park", "S.-I. Amari", "K. Fukumizu"], "venue": "Neural Networks,", "citeRegEx": "Park et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Park et al\\.", "year": 2000}, {"title": "Revisiting natural gradient for deep networks", "author": ["Razvan Pascanu", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Pascanu and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu and Bengio.", "year": 2014}, {"title": "Experiments on learning by back propagation", "author": ["D. Plaut", "S. Nowlan", "G.E. Hinton"], "venue": "Technical Report CMU-CS-86-126,", "citeRegEx": "Plaut et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Plaut et al\\.", "year": 1986}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak.,? \\Q1964\\E", "shortCiteRegEx": "Polyak.", "year": 1964}, {"title": "Joint mean-covariance models with applications to longitudinal data: unconstrained parameterisation", "author": ["M. Pourahmadi"], "venue": null, "citeRegEx": "Pourahmadi.,? \\Q1999\\E", "shortCiteRegEx": "Pourahmadi.", "year": 1999}, {"title": "Covariance Estimation: The GLM and Regularization Perspectives", "author": ["M. Pourahmadi"], "venue": "Statistical Science,", "citeRegEx": "Pourahmadi.,? \\Q2011\\E", "shortCiteRegEx": "Pourahmadi.", "year": 2011}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["Tapani Raiko", "Harri Valpola", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "Raiko et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "No More Pesky Learning Rates", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Schaul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2013}, {"title": "Centering neural network gradient factors", "author": ["Nicol N. Schraudolph"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "Schraudolph.,? \\Q1998\\E", "shortCiteRegEx": "Schraudolph.", "year": 1998}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Nicol N. Schraudolph"], "venue": "Neural Computation,", "citeRegEx": "Schraudolph.,? \\Q2002\\E", "shortCiteRegEx": "Schraudolph.", "year": 2002}, {"title": "Computational methods for linear matrix equations", "author": ["V. Simoncini"], "venue": null, "citeRegEx": "Simoncini.,? \\Q2014\\E", "shortCiteRegEx": "Simoncini.", "year": 2014}, {"title": "Matrix equation XA+BX = C", "author": ["R.A. Smith"], "venue": "SIAM J. Appl. Math.,", "citeRegEx": "Smith.,? \\Q1968\\E", "shortCiteRegEx": "Smith.", "year": 1968}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "A tutorial on stochastic approximation algorithms for training restricted boltzmann machines and deep belief nets", "author": ["K. Swersky", "Bo Chen", "B. Marlin", "N. de Freitas"], "venue": "In Information Theory and Applications Workshop (ITA),", "citeRegEx": "Swersky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2010}, {"title": "Pushing stochastic gradient towards second-order methods \u2013 backpropagation learning with transformations in nonlinearities", "author": ["Tommi Vatanen", "Tapani Raiko", "Harri Valpola", "Yann LeCun"], "venue": null, "citeRegEx": "Vatanen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vatanen et al\\.", "year": 2013}, {"title": "Krylov subspace descent for deep learning", "author": ["O. Vinyals", "D. Povey"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Vinyals and Povey.,? \\Q2012\\E", "shortCiteRegEx": "Vinyals and Povey.", "year": 2012}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler.", "year": 2013}], "referenceMentions": [{"referenceID": 32, "context": "Despite work on layer-wise pretraining schemes, and various sophisticated optimization methods which try to approximate Newton-Raphson updates or natural gradient updates, stochastic gradient descent (SGD), possibly augmented with momentum, remains the method of choice for large-scale neural network training (Sutskever et al., 2013).", "startOffset": 310, "endOffset": 334}, {"referenceID": 14, "context": "From the work on Hessian-free optimization (HF) (Martens, 2010) and related methods (e.", "startOffset": 48, "endOffset": 63}, {"referenceID": 32, "context": "The extent to which neural network objective functions give rise to such quadratics is unclear, although (Sutskever et al., 2013) provides some preliminary evidence that they do.", "startOffset": 105, "endOffset": 129}, {"referenceID": 27, "context": "One such class of methods which have been widely studied are those which work by directly inverting a diagonal, block-diagonal, or low-rank approximation to the curvature matrix (e.g. Becker and LeCun, 1989; Schaul et al., 2013; Le Roux et al., 2008; Ollivier, 2013).", "startOffset": 178, "endOffset": 266}, {"referenceID": 18, "context": "One such class of methods which have been widely studied are those which work by directly inverting a diagonal, block-diagonal, or low-rank approximation to the curvature matrix (e.g. Becker and LeCun, 1989; Schaul et al., 2013; Le Roux et al., 2008; Ollivier, 2013).", "startOffset": 178, "endOffset": 266}, {"referenceID": 11, "context": "As discussed in Martens and Sutskever (2012), CG has the potential to be much faster at local optimization than gradient descent, when applied to quadratic objective functions.", "startOffset": 16, "endOffset": 45}, {"referenceID": 11, "context": "In the first, the rows and columns of the Fisher are divided into groups, each of which corresponds to all the weights in a given layer, and this gives rise to a block-partitioning of the matrix (where the blocks are much larger than those used by Le Roux et al. (2008) or Ollivier (2013)).", "startOffset": 251, "endOffset": 270}, {"referenceID": 11, "context": "In the first, the rows and columns of the Fisher are divided into groups, each of which corresponds to all the weights in a given layer, and this gives rise to a block-partitioning of the matrix (where the blocks are much larger than those used by Le Roux et al. (2008) or Ollivier (2013)).", "startOffset": 251, "endOffset": 289}, {"referenceID": 14, "context": "Note that this presentation closely follows the one from Martens (2014).", "startOffset": 57, "endOffset": 72}, {"referenceID": 1, "context": "The well-known natural gradient (Amari, 1998) is defined as F\u22121\u2207h(\u03b8).", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Motivated from the perspective of information geometry (Amari and Nagaoka, 2000), the natural gradient defines the direction in parameter space which gives the largest change in the objective per unit of change in the model, as measured by the KL-divergence.", "startOffset": 55, "endOffset": 80}, {"referenceID": 15, "context": "It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function.", "startOffset": 16, "endOffset": 57}, {"referenceID": 21, "context": "It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function.", "startOffset": 16, "endOffset": 57}, {"referenceID": 29, "context": "It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function.", "startOffset": 133, "endOffset": 181}, {"referenceID": 16, "context": "It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function.", "startOffset": 133, "endOffset": 181}, {"referenceID": 15, "context": "In particular, (Martens, 2014) showed that when the GGN is defined so that the network is linearized up to the loss function, and the loss function corresponds to the negative log probability of observations under an exponential family model Ry|z with natural parameters z, then the Fisher corresponds exactly to the GGN.", "startOffset": 15, "endOffset": 30}, {"referenceID": 0, "context": "Motivated from the perspective of information geometry (Amari and Nagaoka, 2000), the natural gradient defines the direction in parameter space which gives the largest change in the objective per unit of change in the model, as measured by the KL-divergence. This is to be contrasted with the standard gradient, which can be defined as the direction in parameter space which gives the largest change in the objective per unit of change in the parameters, as measured by the standard Euclidean metric. The natural gradient also has links to several classical ideas from optimization. It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function. Indeed, the GGN has served as the curvature matrix of choice in HF and related methods, and so in light of its equivalence to the Fisher, these methods can all be seen as approximate natural gradient methods. In particular, (Martens, 2014) showed that when the GGN is defined so that the network is linearized up to the loss function, and the loss function corresponds to the negative log probability of observations under an exponential family model Ry|z with natural parameters z, then the Fisher corresponds exactly to the GGN.1 For some good recent discussion and analysis of the natural gradient, see Arnold et al. (2011); Martens (2014); Pascanu and Bengio (2014).", "startOffset": 56, "endOffset": 1502}, {"referenceID": 0, "context": "Motivated from the perspective of information geometry (Amari and Nagaoka, 2000), the natural gradient defines the direction in parameter space which gives the largest change in the objective per unit of change in the model, as measured by the KL-divergence. This is to be contrasted with the standard gradient, which can be defined as the direction in parameter space which gives the largest change in the objective per unit of change in the parameters, as measured by the standard Euclidean metric. The natural gradient also has links to several classical ideas from optimization. It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function. Indeed, the GGN has served as the curvature matrix of choice in HF and related methods, and so in light of its equivalence to the Fisher, these methods can all be seen as approximate natural gradient methods. In particular, (Martens, 2014) showed that when the GGN is defined so that the network is linearized up to the loss function, and the loss function corresponds to the negative log probability of observations under an exponential family model Ry|z with natural parameters z, then the Fisher corresponds exactly to the GGN.1 For some good recent discussion and analysis of the natural gradient, see Arnold et al. (2011); Martens (2014); Pascanu and Bengio (2014).", "startOffset": 56, "endOffset": 1518}, {"referenceID": 0, "context": "Motivated from the perspective of information geometry (Amari and Nagaoka, 2000), the natural gradient defines the direction in parameter space which gives the largest change in the objective per unit of change in the model, as measured by the KL-divergence. This is to be contrasted with the standard gradient, which can be defined as the direction in parameter space which gives the largest change in the objective per unit of change in the parameters, as measured by the standard Euclidean metric. The natural gradient also has links to several classical ideas from optimization. It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function. Indeed, the GGN has served as the curvature matrix of choice in HF and related methods, and so in light of its equivalence to the Fisher, these methods can all be seen as approximate natural gradient methods. In particular, (Martens, 2014) showed that when the GGN is defined so that the network is linearized up to the loss function, and the loss function corresponds to the negative log probability of observations under an exponential family model Ry|z with natural parameters z, then the Fisher corresponds exactly to the GGN.1 For some good recent discussion and analysis of the natural gradient, see Arnold et al. (2011); Martens (2014); Pascanu and Bengio (2014).", "startOffset": 56, "endOffset": 1545}, {"referenceID": 14, "context": "Equivalently, one could linearize the network only up to the input s` to \u03c6` when computing the GGN (see Martens and Sutskever (2012)).", "startOffset": 104, "endOffset": 133}, {"referenceID": 6, "context": "In fact, the idea of approximating the distribution over loss gradients with a directed graphical model forms the basis of the recent FANG method (Grosse and Salakhutdinov, 2014).", "startOffset": 146, "endOffset": 178}, {"referenceID": 24, "context": "Following the work of Grosse and Salakhutdinov (2014), we use the block generalization of well-known \u201cCholesky\u201d decomposition of the precision matrix of DGGMs (Pourahmadi, 1999),", "startOffset": 159, "endOffset": 177}, {"referenceID": 6, "context": "Following the work of Grosse and Salakhutdinov (2014), we use the block generalization of well-known \u201cCholesky\u201d decomposition of the precision matrix of DGGMs (Pourahmadi, 1999),", "startOffset": 22, "endOffset": 54}, {"referenceID": 27, "context": "This kind of exponentially decaying averaging scheme is commonly used in methods involving diagonal or block-diagonal approximations (with much smaller blocks than ours) to the curvature matrix (e.g. Park et al., 2000; Schaul et al., 2013).", "startOffset": 194, "endOffset": 239}, {"referenceID": 23, "context": "While computing matrix-vector products with the Gi,j could be done exactly and efficiently for a given input x (or small mini-batch of x\u2019s) by adapting the methods of Schraudolph (2002), there doesn\u2019t seem to be a sufficiently efficient method for computing the entire matrix itself.", "startOffset": 167, "endOffset": 186}, {"referenceID": 14, "context": "Indeed, the hardness results of Martens et al. (2012) suggest that this would require, for each example x in the mini-batch, work that is asymptotically equivalent to matrix-matrix multiplication involving matrices the same size as Gi,j .", "startOffset": 32, "endOffset": 54}, {"referenceID": 14, "context": "Moreover, such a choice would not give rise to what is usually thought of as the natural gradient, and based on the findings of Martens (2010), would likely perform worse in practice as part of an optimization algorithm.", "startOffset": 128, "endOffset": 143}, {"referenceID": 14, "context": "Moreover, such a choice would not give rise to what is usually thought of as the natural gradient, and based on the findings of Martens (2010), would likely perform worse in practice as part of an optimization algorithm. See Martens (2014) for a more detailed discussion of the empirical Fisher.", "startOffset": 128, "endOffset": 240}, {"referenceID": 14, "context": "Fortunately, as observed by Martens (2014), the natural gradient can be understood using a more traditional optimizationtheoretic perspective which implies how it can be used to generate updates that will be useful over larger distances.", "startOffset": 28, "endOffset": 43}, {"referenceID": 14, "context": "Fortunately, as observed by Martens (2014), the natural gradient can be understood using a more traditional optimizationtheoretic perspective which implies how it can be used to generate updates that will be useful over larger distances. In particular, when Ry|z is an exponential family model with z as its natural parameters (as it will be in our experiments), Martens (2014) showed that the Fisher becomes equivalent to the Generalized Gauss-Newton matrix (GGN), which is a positive semi-definite approximation of the Hessian of h.", "startOffset": 28, "endOffset": 378}, {"referenceID": 14, "context": "3Which has the interpretation of being a geodesic in the Riemannian manifold from the current predictive distribution towards the training distribution when using a likelihood or KL-divergence based objective function (see Martens (2014)).", "startOffset": 223, "endOffset": 238}, {"referenceID": 13, "context": "And so as argued by Martens (2014), it is natural to make use of the various sophisticated \u201cdamping\u201d techniques that have been developed in the optimization literature for dealing with the breakdowns in local quadratic approximations.", "startOffset": 20, "endOffset": 35}, {"referenceID": 13, "context": "And so as argued by Martens (2014), it is natural to make use of the various sophisticated \u201cdamping\u201d techniques that have been developed in the optimization literature for dealing with the breakdowns in local quadratic approximations. These include techniques such as Tikhonov damping/regularization, line-searches, and trust regions, etc., all of which tend to be much more effective in practice than merely re-scaling the update. Indeed, a subset of these techniques were exploited in the work of Martens (2010), and have appeared implicitly in older works such as Becker and LeCun (1989).", "startOffset": 20, "endOffset": 514}, {"referenceID": 3, "context": "Indeed, a subset of these techniques were exploited in the work of Martens (2010), and have appeared implicitly in older works such as Becker and LeCun (1989). For detailed discussion of these and other damping techniques, we refer the reader to Martens and Sutskever (2012).", "startOffset": 135, "endOffset": 159}, {"referenceID": 3, "context": "Indeed, a subset of these techniques were exploited in the work of Martens (2010), and have appeared implicitly in older works such as Becker and LeCun (1989). For detailed discussion of these and other damping techniques, we refer the reader to Martens and Sutskever (2012). Methods like HF which use the exact Fisher seem to work reasonably well with an adaptive Tikhonov regularization technique where \u03bbI is added to F + \u03b7I , and where \u03bb is adapted according to Levenberg-Marquardt style adjustment rule.", "startOffset": 135, "endOffset": 275}, {"referenceID": 23, "context": "(2013) found that momentum (Polyak, 1964; Plaut et al., 1986) was very helpful in the context of stochastic gradient descent optimization of deep neural networks.", "startOffset": 27, "endOffset": 61}, {"referenceID": 22, "context": "(2013) found that momentum (Polyak, 1964; Plaut et al., 1986) was very helpful in the context of stochastic gradient descent optimization of deep neural networks.", "startOffset": 27, "endOffset": 61}, {"referenceID": 16, "context": "A version of momentum is also present in the original HF method, and it plays an arguably even more important role in more \u201cstochastic\u201d versions of HF (Martens and Sutskever, 2012; Kiros, 2013).", "startOffset": 151, "endOffset": 193}, {"referenceID": 10, "context": "A version of momentum is also present in the original HF method, and it plays an arguably even more important role in more \u201cstochastic\u201d versions of HF (Martens and Sutskever, 2012; Kiros, 2013).", "startOffset": 151, "endOffset": 193}, {"referenceID": 12, "context": "convex optimization theory, and with older empirical work done on neural network optimization (LeCun et al., 1998).", "startOffset": 94, "endOffset": 114}, {"referenceID": 19, "context": "And there are also ways of reducing matrix-inversion (and even matrix square-root) to a short sequence of matrix-matrix multiplications using iterative methods (Pan and Schreiber, 1991).", "startOffset": 160, "endOffset": 185}, {"referenceID": 14, "context": "One can similarly reduce the cost of task 7 by computing the (factored) matrix-vector product with F using such a subset, although we recommend caution when doing this, as using inconsistent sets of data for the quadratic and linear terms in M(\u03b4) can hypothetically cause instability problems which are avoided by using consistent data (see Martens and Sutskever (2012), Section 13.", "startOffset": 341, "endOffset": 370}, {"referenceID": 9, "context": "Some possible ways around this issue include computing the weight-decay contribution \u03bdF\u0306\u22121\u03b8 separately and refreshing it only occasionally, or using a different regularization method, such as drop-out (Hinton et al., 2012) or weight-magnitude constraints.", "startOffset": 201, "endOffset": 222}, {"referenceID": 14, "context": "As shown by Martens (2014), the approximation error will go to zero as the effects of damping diminish and the reparameterizing function \u03b6 tends to a locally linear function.", "startOffset": 12, "endOffset": 27}, {"referenceID": 14, "context": "Fortunately, as was shown by Martens (2014), one can establish invariance of an update direction with respect to a given reparameterization of the model by verifying certain simple properties of the curvature matrix C used to compute the update.", "startOffset": 29, "endOffset": 44}, {"referenceID": 14, "context": "While this corollary assumes that the \u03a9i\u2019s and \u03a6i\u2019s are fixed, if we relax this assumption so that they are allowed to vary smoothly with \u03b8, then \u03b6 will be a smooth function of \u03b8, and so as discussed in Martens (2014), invariance of the optimization path will hold approximately in a way that depends on the smoothness of \u03b6 (which measures how quickly the \u03a9i\u2019s and \u03a6i\u2019s change) and", "startOffset": 203, "endOffset": 218}, {"referenceID": 14, "context": "The Hessian-free optimization method of Martens (2010) uses linear conjugate gradient (CG) to optimize local quadratic models of the form of eqn.", "startOffset": 40, "endOffset": 55}, {"referenceID": 28, "context": "Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 26, "context": "Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.", "startOffset": 127, "endOffset": 169}, {"referenceID": 34, "context": "Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.", "startOffset": 127, "endOffset": 169}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit.", "startOffset": 3, "endOffset": 22}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit. By contrast, K-FAC uses much larger blocks, each of which corresponds to all the weights within a particular layer. The matrices which are inverted in K-FAC are roughly the same size as those which are inverted in TONGA, but rather than there being one per unit as in TONGA, there are only two per layer. Therefore, K-FAC is significantly less computationally intensive than TONGA, despite using what is arguably a much more accurate approximation to the Fisher. Note that to help mitigate the cost of the many matrix inversions it requires, TONGA approximates the blocks as being low-rank + a diagonal term, although this introduces further approximation error. Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.e. \u201ccentered\u201d), as they appear in the formula for the gradient. Typically, these methods require the introduction of additional \u201cskip\u201d connections (which bypass the nonlinearities of a given layer) in order to preserve the expressive power/efficiency of the network after these transformations are applied. It is argued by Raiko et al. (2012) that the application of centering methods moves the Fisher closer to a diagonal matrix, and thus makes the gradient a reasonable approximation to the natural gradient.", "startOffset": 3, "endOffset": 1535}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit. By contrast, K-FAC uses much larger blocks, each of which corresponds to all the weights within a particular layer. The matrices which are inverted in K-FAC are roughly the same size as those which are inverted in TONGA, but rather than there being one per unit as in TONGA, there are only two per layer. Therefore, K-FAC is significantly less computationally intensive than TONGA, despite using what is arguably a much more accurate approximation to the Fisher. Note that to help mitigate the cost of the many matrix inversions it requires, TONGA approximates the blocks as being low-rank + a diagonal term, although this introduces further approximation error. Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.e. \u201ccentered\u201d), as they appear in the formula for the gradient. Typically, these methods require the introduction of additional \u201cskip\u201d connections (which bypass the nonlinearities of a given layer) in order to preserve the expressive power/efficiency of the network after these transformations are applied. It is argued by Raiko et al. (2012) that the application of centering methods moves the Fisher closer to a diagonal matrix, and thus makes the gradient a reasonable approximation to the natural gradient. However, this argument uses the strong approximating assumption that the correlations between various network-dependent quantities, such as the activities of different units within a given layer, are zero. In our notation, this would be like assuming that the Gi,i\u2019s are diagonal, and that the \u0100i,i\u2019s are rank-1 plus a diagonal term. Indeed, using such an approximation within the block-diagonal version of K-FAC would yield an algorithm similar to standard centering, although without the need for skip connections. As shown in Corollary 3, K-FAC can be interpreted as automatically performing centering, along with the much stronger operation of whitening, of the unit activities and various other quantities which appear in the formula for the gradient. Intuitively, it is this whitening which accounts for the correlations between activities (or back-propagated gradients) within a given layer. Ollivier (2013) proposed a neural network optimization method which uses a block-diagonal approximation of the Fisher, with the blocks corresponding to the incoming weights (and bias) of each unit.", "startOffset": 3, "endOffset": 2618}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit. By contrast, K-FAC uses much larger blocks, each of which corresponds to all the weights within a particular layer. The matrices which are inverted in K-FAC are roughly the same size as those which are inverted in TONGA, but rather than there being one per unit as in TONGA, there are only two per layer. Therefore, K-FAC is significantly less computationally intensive than TONGA, despite using what is arguably a much more accurate approximation to the Fisher. Note that to help mitigate the cost of the many matrix inversions it requires, TONGA approximates the blocks as being low-rank + a diagonal term, although this introduces further approximation error. Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.e. \u201ccentered\u201d), as they appear in the formula for the gradient. Typically, these methods require the introduction of additional \u201cskip\u201d connections (which bypass the nonlinearities of a given layer) in order to preserve the expressive power/efficiency of the network after these transformations are applied. It is argued by Raiko et al. (2012) that the application of centering methods moves the Fisher closer to a diagonal matrix, and thus makes the gradient a reasonable approximation to the natural gradient. However, this argument uses the strong approximating assumption that the correlations between various network-dependent quantities, such as the activities of different units within a given layer, are zero. In our notation, this would be like assuming that the Gi,i\u2019s are diagonal, and that the \u0100i,i\u2019s are rank-1 plus a diagonal term. Indeed, using such an approximation within the block-diagonal version of K-FAC would yield an algorithm similar to standard centering, although without the need for skip connections. As shown in Corollary 3, K-FAC can be interpreted as automatically performing centering, along with the much stronger operation of whitening, of the unit activities and various other quantities which appear in the formula for the gradient. Intuitively, it is this whitening which accounts for the correlations between activities (or back-propagated gradients) within a given layer. Ollivier (2013) proposed a neural network optimization method which uses a block-diagonal approximation of the Fisher, with the blocks corresponding to the incoming weights (and bias) of each unit. This method is similar to TONGA, except that it approximates the Fisher instead of the empirical Fisher (see Martens (2014) for a discussion of the difference between these).", "startOffset": 3, "endOffset": 2924}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit. By contrast, K-FAC uses much larger blocks, each of which corresponds to all the weights within a particular layer. The matrices which are inverted in K-FAC are roughly the same size as those which are inverted in TONGA, but rather than there being one per unit as in TONGA, there are only two per layer. Therefore, K-FAC is significantly less computationally intensive than TONGA, despite using what is arguably a much more accurate approximation to the Fisher. Note that to help mitigate the cost of the many matrix inversions it requires, TONGA approximates the blocks as being low-rank + a diagonal term, although this introduces further approximation error. Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.e. \u201ccentered\u201d), as they appear in the formula for the gradient. Typically, these methods require the introduction of additional \u201cskip\u201d connections (which bypass the nonlinearities of a given layer) in order to preserve the expressive power/efficiency of the network after these transformations are applied. It is argued by Raiko et al. (2012) that the application of centering methods moves the Fisher closer to a diagonal matrix, and thus makes the gradient a reasonable approximation to the natural gradient. However, this argument uses the strong approximating assumption that the correlations between various network-dependent quantities, such as the activities of different units within a given layer, are zero. In our notation, this would be like assuming that the Gi,i\u2019s are diagonal, and that the \u0100i,i\u2019s are rank-1 plus a diagonal term. Indeed, using such an approximation within the block-diagonal version of K-FAC would yield an algorithm similar to standard centering, although without the need for skip connections. As shown in Corollary 3, K-FAC can be interpreted as automatically performing centering, along with the much stronger operation of whitening, of the unit activities and various other quantities which appear in the formula for the gradient. Intuitively, it is this whitening which accounts for the correlations between activities (or back-propagated gradients) within a given layer. Ollivier (2013) proposed a neural network optimization method which uses a block-diagonal approximation of the Fisher, with the blocks corresponding to the incoming weights (and bias) of each unit. This method is similar to TONGA, except that it approximates the Fisher instead of the empirical Fisher (see Martens (2014) for a discussion of the difference between these). Because computing blocks of the Fisher is expensive (it requires k backpropagations, where k is the number of output units), this method uses a biased deterministic approximation which can be computed more efficiently, and is similar in spirit to the deterministic approximation used by LeCun et al. (1998). Note that while such an approximation could hypothetically be used within K-FAC to compute the Gi,j\u2019s, we have found that our basic unbiased stochastic approximation works nearly", "startOffset": 3, "endOffset": 3282}, {"referenceID": 7, "context": "The work most closely related to ours is that of Heskes (2000), who proposed an approximation of the Fisher of feed-forward neural networks similar to our Kronecker-factored blockdiagonal approximation F\u0306 from Section 4.", "startOffset": 49, "endOffset": 63}, {"referenceID": 2, "context": "using the techniques of Arnold et al. (2011)).", "startOffset": 24, "endOffset": 45}, {"referenceID": 35, "context": "Due to their high difficulty, performance on these problems has become a standard benchmark for neural network optimization methods (e.g. Martens, 2010; Vinyals and Povey, 2012; Sutskever et al., 2013).", "startOffset": 132, "endOffset": 201}, {"referenceID": 32, "context": "Due to their high difficulty, performance on these problems has become a standard benchmark for neural network optimization methods (e.g. Martens, 2010; Vinyals and Povey, 2012; Sutskever et al., 2013).", "startOffset": 132, "endOffset": 201}, {"referenceID": 8, "context": "To investigate the practical performance of K-FAC we applied it to the 3 deep-autoencoder optimization problems from Hinton and Salakhutdinov (2006), which use the \u201cMNIST\u201d, \u201cCURVES\u201d, and \u201cFACES\u201d datasets respectively (see Hinton and Salakhutdinov (2006) for a complete description of the network architectures and datasets).", "startOffset": 117, "endOffset": 149}, {"referenceID": 8, "context": "To investigate the practical performance of K-FAC we applied it to the 3 deep-autoencoder optimization problems from Hinton and Salakhutdinov (2006), which use the \u201cMNIST\u201d, \u201cCURVES\u201d, and \u201cFACES\u201d datasets respectively (see Hinton and Salakhutdinov (2006) for a complete description of the network architectures and datasets).", "startOffset": 117, "endOffset": 254}, {"referenceID": 8, "context": "To investigate the practical performance of K-FAC we applied it to the 3 deep-autoencoder optimization problems from Hinton and Salakhutdinov (2006), which use the \u201cMNIST\u201d, \u201cCURVES\u201d, and \u201cFACES\u201d datasets respectively (see Hinton and Salakhutdinov (2006) for a complete description of the network architectures and datasets). Due to their high difficulty, performance on these problems has become a standard benchmark for neural network optimization methods (e.g. Martens, 2010; Vinyals and Povey, 2012; Sutskever et al., 2013). As a baseline we used the version of SGD with momentum described in Sutskever et al. (2013), which was calibrated to work well on these particular deep autoencoder problems.", "startOffset": 117, "endOffset": 620}, {"referenceID": 8, "context": "To investigate the practical performance of K-FAC we applied it to the 3 deep-autoencoder optimization problems from Hinton and Salakhutdinov (2006), which use the \u201cMNIST\u201d, \u201cCURVES\u201d, and \u201cFACES\u201d datasets respectively (see Hinton and Salakhutdinov (2006) for a complete description of the network architectures and datasets). Due to their high difficulty, performance on these problems has become a standard benchmark for neural network optimization methods (e.g. Martens, 2010; Vinyals and Povey, 2012; Sutskever et al., 2013). As a baseline we used the version of SGD with momentum described in Sutskever et al. (2013), which was calibrated to work well on these particular deep autoencoder problems. For each problem we followed the prescription given by Sutskever et al. (2013) for determining the learning rate, and the schedule for the decay constant \u03bc.", "startOffset": 117, "endOffset": 781}, {"referenceID": 14, "context": "Each method used the same initial parameter setting, which was generated using the \u201csparse initialization\u201d technique from Martens (2010) (which was also used by Sutskever et al.", "startOffset": 122, "endOffset": 137}, {"referenceID": 14, "context": "Each method used the same initial parameter setting, which was generated using the \u201csparse initialization\u201d technique from Martens (2010) (which was also used by Sutskever et al. (2013)).", "startOffset": 122, "endOffset": 185}], "year": 2017, "abstractText": "We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network\u2019s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as factoring as Kronecker products between two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods such as Hessian-free methods, K-FAC works very well in highly stochastic optimization regimes.", "creator": "LaTeX with hyperref package"}}}