{"id": "1503.06394", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2015", "title": "Large-scale log-determinant computation through stochastic Chebyshev expansions", "abstract": "Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of discrete graphical models, minimum-volume ellipsoids, metric learning and kernel learning. Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables, i.e., the matrix dimension, which makes it prohibitive for large-scale applications. We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation, called the Hutchinson method, coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications. We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix. In our experiments, the proposed algorithm can provide very high accuracy solutions at orders of magnitude faster time than the Cholesky decomposition and Schur completion, and enables us to compute log-determinants of matrices involving tens of millions of variables.", "histories": [["v1", "Sun, 22 Mar 2015 06:55:12 GMT  (670kb,D)", "http://arxiv.org/abs/1503.06394v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG cs.NA", "authors": ["insu han", "dmitry malioutov", "jinwoo shin"], "accepted": true, "id": "1503.06394"}, "pdf": {"name": "1503.06394.pdf", "metadata": {"source": "CRF", "title": "Large-scale Log-determinant Computation through Stochastic Chebyshev Expansions", "authors": ["Insu Han", "Dmitry Malioutov", "Jinwoo Shin"], "emails": ["hawki17@kaist.ac.kr", "dmaliout@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "One of the most important tasks of linear algebra, which occurs in a variety of machine learning problems, is the calculation of the log determinants of a large positive definitive matrix. For example, if they serve as a normalization constant for multivariate Gaussian models, log determinants of covariance (and precision) matrices play an important role in inference, model selection, and learning both the structure and parameters of Gaussian models and Gaussian processes. Log determinants also play an important role in a variety of Bayesian machine learning problems, including sampling and variable inferences [17]."}, {"heading": "2 Background", "text": "In this section we describe the preparatory work for our approach to approximating the log determinant of a positive definitive matrix. Our approach combines the following two techniques: (a) the development of a trace estimator for the log determinant of a positive definitive matrix using Chebyshev approximation [19] and (b) the approximation of the trace of a positive definitive matrix using Monte Carlo methods, e.g. the Hutchison method [14]."}, {"heading": "2.1 Chebyshev Approximation", "text": "The Chebyshev approximation technology is used to approximate the analytical function with certain orthonormal polynomials. We use pn (x) to denote the Chebyshev approximation of degree n for a specific function f: [\u2212 1, 1] \u2192 R: f (x) \u2248 pn (x) = n \u2211 j = 0 cjTj (x), where the coefficient ci and the i th Chebyshev polynomial Ti (x) are defined as asci = 1 n + 1 n \u2211 k = 0 f (xk) Ti (x), otherwise (1) Ti + 1 (x) \u2212 Ti \u2212 Ti \u2212 1 (x) \u2212 1 \u2212 Ti \u2212 1 \u2212 1 \u2212 empempempempempir for i functions (2), where xk = implicit = 1 (2) is implied."}, {"heading": "2.2 Trace Approximation via Monte-Carlo Method", "text": "The greatest challenge in calculating the log determinant of a positive definitive matrix in the previous section is to efficiently calculate the track of Tj (A) without evaluating the entire MatrixAk. We consider a Monte Carlo approach to estimating the track of a matrix. Firstly, a random vector z is drawn from a fixed distribution, so that the expectation of z > Az of the track of A. By scanning m such as Pr (+ 1) = Pr (\u2212 1) = 12, we obtain an estimate of tr (A).It is known that the Hutchinson method, in which the components of the random vectors Z i.i.d Rademacher are random variables, i.e. Pr (-1) = Pr (\u2212 1) = 12, has the smallest variance among such Monte Carlo methods."}, {"heading": "3 Log-determinant Approximation Scheme", "text": "Now we are ready to submit algorithms to approximate the absolute value of the log determinant to any non-singular square matrixC. Without loss of generality, we assume that singular values of C for some \u03c3min, \u03c3max > 0 lie in the interval [\u03c3min, \u03c3max], i.e., the condition number \u0445 (C) is at most \u03bamax: = \u03c3max / \u03c3min. The proposed algorithms are not sensitive to an exact knowledge of \u03c3min, \u03c3max, but some loose lower or upper limits on them suffice. We first present a log-determinant approximation scheme for positive definitive matrices in Section 3.1 and later for general non-singular ones in Section 3.2."}, {"heading": "3.1 Algorithm for Positive Definite Matrices", "text": "In this section, we describe our proposed algorithm for estimating the log determinant of a positive definitive matrix whose eigenvalues are less than one, i.e. it is used as a subroutine for estimating the log determinant of a generic non-singular matrix in the next paragraph.The formal description of the algorithm is given in the following paragraph.( Algorithm 1 log determinant approximation for positive definitive matrices with \u03c3max < 1 input: positive definitive matrix B, Rd \u00b7 d with eigenvalues in [3, 1 \u2212 3] for any procedure > 0, sampling number m and polynomial degree n initialization: A \u2190 I \u2212 B, 0 for i = 0 to n do ci, i-th coefficient of Chebyshev, Rd with eigenvalues in [4]. (1 \u2212 3) The log determinant of progression of m is 0 \u2212 implied."}, {"heading": "3.2 Algorithm for General Non-Singular Matrices", "text": "The idea is simple: Run algorithm 1 with normalization of positive defined matrix CTC. This is formally described as follows: Algorithm 2 Log-determinant approximation for general non-singular matrices Input: Matrix C + Log Rd \u00b7 d with singular values are in the interval [\u03c3min, \u03c3max] for some minute, \u03c3max] for some minute, \u03c3max > 0, sampling number m and polynomial degree nInitialize: B \u2190 2 + 2 maxCTC, ppi 2 minute values are in the interval."}, {"heading": "3.3 Application to Counting Spanning Trees", "text": "We apply algorithm 2 to a specific problem, in which we examine the number of overspanning trees in a simple undirected diagram G = (V, E), where there is a vertex i *, so that (i *, j) for all j * V\\ {i *). Counting trees is one of the classically well-studied counting problems and is also necessary in machine learning applications, e.g. tree mixture models [20, 1]. We designate the maximum and average degrees of vertex in V\\ {i *} by \"max\" and \"avg > 1,\" respectively. In addition, we have L (G) designate the laplac matrix of G. Then Kirchhoff's matrix-tree theorem shows that the number of overspanning tree trunks (G) is equal to \"to\" (G) = \"detL\" (i \"), where L (i\") is the (l \") the (V\" \u2212 1) \u00d7 (V \u2212 1) of matrix (G)."}, {"heading": "Pr [| log \u03c4(G)\u2212 \u0393| \u2264 \u03b5 log \u03c4(G)] \u2265 1\u2212 \u03b6", "text": "We note that the runtime of algorithm 2 with the inputs in the above theorem is O (nm \u2206 avg | V |). Therefore, one can select n, m = O (1) for \u03b5, \u0445 = \u044b (1) and \u0445 avg = O (1), i.e. G is sparse, so that the runtime of algorithm 2 is O (| V |)."}, {"heading": "4 Proof of Theorem 1", "text": "In order to prove Theorem 1, we first present some necessary background information and quotations on the error limits of the Chebyshev approximation and the Hutchinson method, which we have introduced in Section 2.1 and Section 2.2 respectively."}, {"heading": "4.1 Convergence Rate for Chebyshev Approximation", "text": "Intuitively, one can assume that the approximate Chebyshev polynomium is approaching its original function, since the degree n goes in both directions. Formally, the following error limit is known [7, 32]. \u2212 \u2212 Suppose f is analytical with the f (z) | \u2264 M defined in section 2.1 in the region limited by the ellipse with the center of gravity \u00b1 1 and the major and minor semiaxis. \u2212 Suppose f is the interpolation of f of the degree n n n n n n n n n n n in th Chebyshev points as defined in section 2.1, then for each n (n) 0, max x (f). \u2212 f (x) \u2212 pn (x) \u2212 pn is the interpolance of f of the degree n n n n n n n n n n n n n n in th Chebyshev points as defined in section 2.1, then for each n (n)."}, {"heading": "4.2 Approximation Error of Hutchinson Method", "text": "In this section, we use the same notation, e.g. f, pn, that was used in the previous section, and we analyze Hutchinson's tracker trm (\u00b7), which is defined in Section 2.2. First, we cite the following sentence, which is proven in [24]. Theorem 8 Let A, Rd, d be a positive, definitive, or negative matrix. Given \u03b50, 0, 0, 1, the above sentence provides a lower limit for the sample complexity of the Hutchinson method, which is independent of a given matrix (A) | \u2264 \u03b50 tr (A)] \u2265 1 \u2212 0if the sampling number m is not less than 6 \u03b5 \u2212 20 log (2 / \u04410). In general, however, we cannot know whether n, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, we, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, (we, c, c, c, c, c, c, c, c, c, c, c, that, c, c, c, c, c, c, we, c, c, c, c, c, c, we, c, c, c, c, c, c, c, c, c, and we, and we, c, and we, c, (), c, c, and we, c, c, c, and we, c, c, c, c, c, and we, c, c, and we, c, c, c, c, c, and we, c, c, c, and we, c, c, c, c, c, c, and we, c, c, c, and we, c, c, c, c, c, c, c, c, and we, c, c, c, and we, c, c, c, c, c, c, c, and we, c, c, c, c, c, and we, c, c, and we,"}, {"heading": "4.3 Proof of the Theorem 1", "text": "Now we are ready to prove theorem 1. First, we can verify that the sampling number n in the state of theorem 1 (A) is satisfactory. (3) It follows that pn (A) is defined negatively, whereas A = I \u2212 B and eigenvalues of B are in [B). Therefore, we can use theorem 8 asPr (A) \u2212 trm (pn (A) \u2212 trm (A) \u2212 trm (pn (A) | tr (A) | tr (pn (A)) | tr (4) for m."}, {"heading": "5 Experiments", "text": "We are now investigating our proposed algorithm for numerical experiments with simulated and real data."}, {"heading": "5.1 Performance Evaluation and Comparison", "text": "We first examine the empirical performance of our proposed algorithm using large, sparse random matrices. We create a random matrix C, Rd, i.e. the number of non-zero entries per line is about 10. We first select five non-zero entries in each line with evenly distributed values in [\u2212 1, 1]. To make the matrix symmetrical, we set the entries in transposed positions to the same values. Finally, to guarantee a positive definition, we set their diagonal entries to absolute line totals and add a small weight, 10 \u2212 3.Figure 1 (a) shows the runtime of the algorithm 2 from d = 103 to 3 \u00d7 107, using m = 10, n = 15, \u03c3min = 10 \u2212 3 and \u03c3max = C, 1. It roughly scales linearly over a wide range of sizes. We use a machine with 3.40 Ghz Intel I7 processor with 24 GB RAM. It only takes 500 seconds for an annotation time."}, {"heading": "5.2 Maximum Likelihood Estimation for GMRF", "text": "GMRF with 25 million variables for synthetic data. We now apply our proposed algorithm for maximum estimation of probability (GMRF) = > J (ML) estimation in Gaussian Markov Random Fields (GMRF) [25]. GMRF is a multivariable common Gaussian distribution defined in relation to a diagram. Each node of the diagram corresponds to a random variable in the Gaussian distribution, in which the diagram captures the conditional independence relationships (Markov properties) among the random variables. The model has been widely used in many applications in computer vision, spatial statistics and other fields. Inverse covariance matrix J (also called log information or precision matrix) is positively defined and sparsely: Jij is not zero only when the edge {i, j} is included in the diagram. First, we look at a GMRF on a quadratic raster of the precision matrix 5000 (x = 5000)."}, {"heading": "6 Conclusion", "text": "Tools from numerical linear algebra, such as determinants, matrix inversion and linear solvers, eigenvalue calculation and other matrix decompositions, play an important theoretical and computational role in machine learning. Although most matrix calculations allow polynomial time algorithms, they are often not feasible for large-scale or high-dimensional datasets. In this paper, we design and analyze a high-precision linear approximation algorithm for the logarithm of matrix determinants, the exact calculation of which requires cubic time. Furthermore, it is very easy to create parallels as it requires only (separable) matrix vector multiplications. We believe that the proposed algorithm will find numerous applications in machine learning problems."}, {"heading": "Acknowledgement", "text": "We would like to thank Haim Avron and Jie Chen for their fruitful comments on Chebyshev's approaches and Cho-Jui Hsieh for providing the code for Shur supplement-based log-det calculations."}, {"heading": "A Proof of Corollary 3", "text": "Since all eigenvalues of CTC are positive and less than 1, it follows that \"log det (CTC)\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" = \"log\" (\"detC |)\" dWe use \"\u03b50 instead of\" from Theorem 2, then \"log\" (\"detC |) \u2212\" log (\"detC |)."}, {"heading": "B Proof of Corollary 4", "text": "Similar to the detection of conclusion 3, \u03b50 = \u03b52 log \u03c3 2 min applies. Since the eigenvalues of CTC are greater than 1, we log \u03b50 into \u03b5 and Pr [| log detC \u2212 \u0432 | \u03b5 | log detC |] \u2265 1 \u2212 \u0438 if m and n are sufficient under condition."}, {"heading": "C Proof of Corollary 5", "text": "Theorem 2 provides for the following inequality for \u03b50 = \u03b5 (\u2206 avg \u2212 1) / 2, \u0445 equalization of equalization of equalization of equalization of equalization of equalization (0, 1): equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization of equalization (0, 1):"}, {"heading": "Pr (| log detL(i\u2217)\u2212 \u0393| \u2264 \u03b50(|V | \u2212 1)) \u2265 1\u2212 \u03b6.", "text": "Note that since the vertex i \u0445 connects all other vertex points, the number of surrounding trees, i.e. detL (i \u0445), is greater than 2 (| V | \u2212 1) (\u2206 avg \u2212 1) / 2. Therefore, we have Pr (| log detL (i \u0445) \u2212 \u0442 | \u2264 \u03b50 (| V | \u2212 1)) \u2264 Pr (| log detL (i \u0445) \u2212 \u0432 | \u2264 \u03b5 log detL (i \u0445)). This completes the proof of episode 5."}], "references": [{"title": "Learning mixtures of tree graphical models", "author": ["A. Anandkumar", "F. Huang", "D.J. Hsu", "S.M. Kakade"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Robust inversion, dimensionality reduction, and randomized sampling", "author": ["A. Aravkin", "M.P. Friedlander", "F.J. Herrmann", "T. Van Leeuwen"], "venue": "Mathematical Programming,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Parameter estimation in high dimensional gaussian distributions", "author": ["E. Aune", "D.P. Simpson", "J. Eidsvik"], "venue": "Statistics and Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Counting triangles in large graphs using randomized matrix trace estimation", "author": ["H. Avron"], "venue": "In Workshop on Largescale Data Mining: Theory and Applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix", "author": ["H. Avron", "S. Toledo"], "venue": "Journal of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "An estimator for the diagonal of a matrix", "author": ["C Bekas", "E Kokiopoulou", "Y. Saad"], "venue": "Applied numerical mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Barycentric lagrange interpolation", "author": ["J.P. Berrut", "L.N. Trefethen"], "venue": "SIAM Review,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A randomized algorithm for approximating the log determinant of a symmetric positive definite matrix", "author": ["Boutsidis", "Christos", "Drineas", "Petros", "Kambadur", "Prabhanjan", "Zouzias", "Anastasios"], "venue": "arXiv preprint arXiv:1503.00374,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Efficient estimation of eigenvalue counts in an interval", "author": ["E. Di Napoli", "E. Polizzi", "Y. Saad"], "venue": "arXiv preprint arXiv:1308.4275,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Uber die abgrenzung der eigenwerte einer matrix", "author": ["Gershgorin", "Semyon Aranovich"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1931}, {"title": "BIG & QUIC: Sparse inverse covariance estimation for a million variables", "author": ["C.J. Hsieh", "M.A. Sustik", "I.S. Dhillon", "P.K. Ravikumar", "R. Poldrack"], "venue": "In Adv. in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines", "author": ["M.F. Hutchinson"], "venue": "Communications in Statistics-Simulation and Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Computing an eigenvector with inverse iteration", "author": ["Ipsen", "Ilse CF"], "venue": "SIAM review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Learning planar ising models", "author": ["J.K. Johnson", "P. Netrapalli", "M. Chertkov"], "venue": "preprint arXiv:1011.3494,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Information theory, inference, and learning algorithms", "author": ["D.J.C. MacKay"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Low-rank variance estimation in large-scale gmrf models", "author": ["D.M. Malioutov", "J.K. Johnson", "A.S. Willsky"], "venue": "In IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 2006.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Learning with mixtures of trees", "author": ["M. Meila", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Bounds for norms of the matrix inverse and the smallest singular value", "author": ["N. Mora\u010da"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Chebyshev approximation of log-determinants of spatial weight matrices", "author": ["R.K. Pace", "J.P. LeSage"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen", "C.K. Williams"], "venue": "MIT press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Improved bounds on sample size for implicit matrix trace estimators", "author": ["F. Roosta-Khorasani", "U. Ascher"], "venue": "arXiv preprint arXiv:1308.2475,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Gaussian Markov random fields: theory and applications", "author": ["H. Rue", "L. Held"], "venue": "CRC Press,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Efficient exact inference in planar ising models", "author": ["N.N. Schraudolph", "D. Kamenetsky"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Stochastic approximation of score functions for gaussian processes", "author": ["M.L. Stein", "J. Chen", "M. Anitescu"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Random matrices: The distribution of the smallest singular values", "author": ["T. Tao", "V. Vu"], "venue": "Geometric And Functional Analysis,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Inverse littlewood-offord theorems and the condition number of random discrete matrices", "author": ["T. Tao", "V.H. Vu"], "venue": "Annals of Mathematics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Minimum volume ellipsoid", "author": ["S. Van Aelst", "P. Rousseeuw"], "venue": "Wiley Interdisciplinary Reviews: Computational Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Log-determinant relaxation for approximate inference in discrete markov random fields", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Signal Processing, IEEE Trans. on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Error bounds for approximation in chebyshev points", "author": ["Xiang", "Shuhuang", "Chen", "Xiaojun", "Wang", "Haiyong"], "venue": "Numerische Mathematik,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Approximate implementation of the logarithm of the matrix determinant in gaussian process regression", "author": ["Y. Zhang", "W.E. Leithead"], "venue": "Journal of Statistical Computation and Simulation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}], "referenceMentions": [{"referenceID": 22, "context": "For example, serving as the normalization constant for multivariate Gaussian models, log-determinants of covariance (and precision) matrices play an important role in inference, model selection and learning both the structure and the parameters for Gaussian Graphical models and Gaussian processes [25, 23, 10].", "startOffset": 298, "endOffset": 310}, {"referenceID": 20, "context": "For example, serving as the normalization constant for multivariate Gaussian models, log-determinants of covariance (and precision) matrices play an important role in inference, model selection and learning both the structure and the parameters for Gaussian Graphical models and Gaussian processes [25, 23, 10].", "startOffset": 298, "endOffset": 310}, {"referenceID": 15, "context": "Log-determinants also play an important role in a variety of Bayesian machine learning problems, including sampling and variational inference [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 8, "context": "In addition, metric and kernel learning problems attempt to learn quadratic forms adapted to the data, and formulations involving Bregman divergences of log-determinants have become very popular [9, 30].", "startOffset": 195, "endOffset": 202}, {"referenceID": 27, "context": "In addition, metric and kernel learning problems attempt to learn quadratic forms adapted to the data, and formulations involving Bregman divergences of log-determinants have become very popular [9, 30].", "startOffset": 195, "endOffset": 202}, {"referenceID": 17, "context": ", tree mixture models [20, 1] and Markov random fields [31].", "startOffset": 22, "endOffset": 29}, {"referenceID": 0, "context": ", tree mixture models [20, 1] and Markov random fields [31].", "startOffset": 22, "endOffset": 29}, {"referenceID": 28, "context": ", tree mixture models [20, 1] and Markov random fields [31].", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "In planar Markov random fields [26, 16] inference and learning involve log-determinants of general non-singular matrices.", "startOffset": 31, "endOffset": 39}, {"referenceID": 14, "context": "In planar Markov random fields [26, 16] inference and learning involve log-determinants of general non-singular matrices.", "startOffset": 31, "endOffset": 39}, {"referenceID": 12, "context": "We then use a stochastic trace-estimator, called the Hutchison method [14], to estimate the trace using multiplications between the input matrix and random vectors.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "We first apply our algorithm to obtain a randomized linear-time approximation scheme for counting the number of spanning trees in a certain class of graphs where it could be used for efficient inference in tree mixture models [20, 1].", "startOffset": 226, "endOffset": 233}, {"referenceID": 0, "context": "We first apply our algorithm to obtain a randomized linear-time approximation scheme for counting the number of spanning trees in a certain class of graphs where it could be used for efficient inference in tree mixture models [20, 1].", "startOffset": 226, "endOffset": 233}, {"referenceID": 11, "context": "In particular, the Schur method was used as a part of QUIC algorithm [13] for sparse inverse covariance estimation with over million variables, hence our algorithm could be used to further improve its speed and scale.", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "[6, 18] have used a stochastic trace estimator to compute the diagonal of a matrix or of matrix inverse.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[6, 18] have used a stochastic trace estimator to compute the diagonal of a matrix or of matrix inverse.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "Polynomial approximations to band-pass filters have been used to count the number of eigenvalues in certain intervals [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "Stochastic approximations of score equations have been applied in [27] to learn large-scale Gaussian processes.", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "The works closest to ours which have used stochastic trace estimators for Gaussian process parameter learning are [33] and [3] which instead use Taylor expansions and Cauchy integral formula, respectively.", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "The works closest to ours which have used stochastic trace estimators for Gaussian process parameter learning are [33] and [3] which instead use Taylor expansions and Cauchy integral formula, respectively.", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "A recent improved analysis using Taylor expansions has also appeared in [8].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "However, as reported in Section 5, our method using Chebyshev expansions provides much better accuracy in experiments than that using Taylor expansions, and [3] need Krylov-subspace linear system solver that is computationally expensive.", "startOffset": 157, "endOffset": 160}, {"referenceID": 19, "context": "[22] also use Chebyshev polynomials for log-determinant computation, but the method is deterministic and only applicable to polynomials of small degree.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": ", Hutchison method [14].", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": ", Pr(+1) = Pr(\u22121) = 12 , has the smallest variance among such Monte-Carlo methods [14, 5].", "startOffset": 82, "endOffset": 89}, {"referenceID": 4, "context": ", Pr(+1) = Pr(\u22121) = 12 , has the smallest variance among such Monte-Carlo methods [14, 5].", "startOffset": 82, "endOffset": 89}, {"referenceID": 3, "context": "It has been used extensively in many applications [4, 14, 2].", "startOffset": 50, "endOffset": 60}, {"referenceID": 12, "context": "It has been used extensively in many applications [4, 14, 2].", "startOffset": 50, "endOffset": 60}, {"referenceID": 1, "context": "It has been used extensively in many applications [4, 14, 2].", "startOffset": 50, "endOffset": 60}, {"referenceID": 13, "context": "\u03c3max = \u221a \u2016C\u20161\u2016C\u2016\u221e, or one can run the power iteration [15] to estimate a better bound.", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "3, and it is explicitly given as a parameter in many machine learning log-determinant applications [31].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "In general, one can use the inverse power iteration [15] to estimate it.", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "Furthermore, the smallest singular value is easy to compute for random matrices [29, 28] and diagonal-dominant matrices [12, 21].", "startOffset": 80, "endOffset": 88}, {"referenceID": 25, "context": "Furthermore, the smallest singular value is easy to compute for random matrices [29, 28] and diagonal-dominant matrices [12, 21].", "startOffset": 80, "endOffset": 88}, {"referenceID": 10, "context": "Furthermore, the smallest singular value is easy to compute for random matrices [29, 28] and diagonal-dominant matrices [12, 21].", "startOffset": 120, "endOffset": 128}, {"referenceID": 18, "context": "Furthermore, the smallest singular value is easy to compute for random matrices [29, 28] and diagonal-dominant matrices [12, 21].", "startOffset": 120, "endOffset": 128}, {"referenceID": 17, "context": ", tree mixture models [20, 1].", "startOffset": 22, "endOffset": 29}, {"referenceID": 0, "context": ", tree mixture models [20, 1].", "startOffset": 22, "endOffset": 29}, {"referenceID": 6, "context": "Formally, the following error bound is known [7, 32].", "startOffset": 45, "endOffset": 52}, {"referenceID": 29, "context": "Formally, the following error bound is known [7, 32].", "startOffset": 45, "endOffset": 52}, {"referenceID": 21, "context": "To begin with, we state the following theorem that is proven in [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "dimension, (b) relative accuracy, (c) comparison in running time with Cholesky decomposition and Schur complement and (d) comparison in accuracy with Taylor approximation in [33].", "startOffset": 174, "endOffset": 178}, {"referenceID": 11, "context": "estimation with over a million variables [13] and we run the code implemented by the authors.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "We also compare the accuracy of our algorithm to a related stochastic algorithm that uses Taylor expansions [33].", "startOffset": 108, "endOffset": 112}, {"referenceID": 22, "context": "We now apply our proposed algorithm for maximum likelihood (ML) estimation in Gaussian Markov Random Fields (GMRF) [25].", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "We use the data-set from [3] that provides satellite measurements of Ozone levels over the entire earth following the satellite tracks.", "startOffset": 25, "endOffset": 28}, {"referenceID": 22, "context": "We use a linear combination of the thinplate model and the thin-membrane models [25], with two parameters \u03b1 and \u03b2: J = \u03b1I + (\u03b2)Jtp + (1 \u2212 \u03b2)Jtm and", "startOffset": 80, "endOffset": 84}], "year": 2015, "abstractText": "Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of discrete graphical models, minimum-volume ellipsoids, metric learning and kernel learning. Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables, i.e., the matrix dimension, which makes it prohibitive for large-scale applications. We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation, called the Hutchinson method, coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications. We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix. In our experiments, the proposed algorithm can provide very high accuracy solutions at orders of magnitude faster time than the Cholesky decomposition and Schur completion, and enables us to compute log-determinants of matrices involving tens of millions of variables.", "creator": "LaTeX with hyperref package"}}}