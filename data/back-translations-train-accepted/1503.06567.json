{"id": "1503.06567", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "On some provably correct cases of variational inference for topic models", "abstract": "Variational inference is a very efficient and popular heuristic used in various forms in the context of latent variable models. It's closely related to Expectation Maximization (EM), and is applied when exact EM is computationally infeasible. Despite being immensely popular, current theoretical understanding of the effectiveness of variaitonal inference based algorithms is very limited. In this work we provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models.", "histories": [["v1", "Mon, 23 Mar 2015 09:20:39 GMT  (48kb)", "https://arxiv.org/abs/1503.06567v1", "47 pages"], ["v2", "Sat, 22 Aug 2015 11:24:43 GMT  (49kb)", "http://arxiv.org/abs/1503.06567v2", "46 pages, Compared to previous version: clarified notation, a number of typos fixed throughout paper"]], "COMMENTS": "47 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["pranjal awasthi", "andrej risteski"], "accepted": true, "id": "1503.06567"}, "pdf": {"name": "1503.06567.pdf", "metadata": {"source": "CRF", "title": "On some provably correct cases of variational inference for topic models", "authors": ["Pranjal Awasthi", "Andrej Risteski"], "emails": ["pawashti@cs.princeton.edu.", "risteski@cs.princeton.edu."], "sections": [{"heading": null, "text": "The characteristics that the word matrix needs to fulfill in our environment are related to the subject of expansion assumptions introduced in (Anandkumar et al., 2013), as well as the anchor word assumptions in (Arora et al., 2012b). The assumptions on the word matrix are related to the well-known dirichlet introduced in the field of theme modeling by (Blei et al., 2003). It is known that the initialization of the theme in (Arora et al., 2012b) plays a crucial role in how well variation-based algorithms work in practice. The initializations we use are quite natural, one of which is similar to what is currently used in LDA-c, the most popular implementation of variation-related inference for theme models. The other is an overlay of updates that are very efficient."}, {"heading": "1 Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Latent variable models and EM 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Topic models 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Variational relaxation for learning topic models 5", "text": "4.1 Simplified updates in the long document boundary...................................... 6 4.2 Alternately minimized KL and threshold updates....................... 6"}, {"heading": "5 Initializations 7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Case study 1: Sparse topic priors, support initialization 8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Case study 2: Dominating topics, seeded initialization 9", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 On common words 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 Discussion and open problems 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Notation throughout supplementary material 12", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Case study 1: Sparse topic priors, support initialization 12", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "C Case study 2: Dominating topics, seeded initialization 24", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "D Justification of prior assumptions 33", "text": "D.1 Thrift....................................................................................................................."}, {"heading": "E On common words 37", "text": "E.1 Phase I in general terms................................................................................................................................."}, {"heading": "F Estimates on number of documents 43", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "In recent years, the heuristics of non-convex optimization have proven to be one of the most fascinating phenomena of machine learning. Methods such as alternating minimization, EM, variable conclusions, and the like are very popular with ML practitioners, and for good reason: they are much more efficient than alternative methods available such as convex relaxation, and they are usually easy to modify. However, theoretical understanding is sparse, and we know of very few cases where these methods come with formal guarantees."}, {"heading": "2 Latent variable models and EM", "text": "We will briefly deal with latent variable models, in which Xi's observations are generated according to a method of distribution. \u2212 In view of Xi's observations, a common task in this context is to find the highest possible probability value of the parameter \u03b8: Argmaximizing Imbalance (P). The Expectation Maximization Algorithm (EM) is an iterative method to achieve this, going back to (Xi et al., 1977) and (Sundberg, 1974) in the 1970s. In the above context, it can be formulated in such a way that the following method, in which estimates are quantified, is not logical. \u2212 The method of variation P is not logical, but logical."}, {"heading": "3 Topic models", "text": "In fact, it is such that most of them will be able to interfere in politics in order to interfere in politics, namely in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy, in the economy"}, {"heading": "4 Variational relaxation for learning topic models", "text": "In this section, we will briefly consider the simplification of variation for topic models closely related to the description in (Lead q = q = al., 2003). Throughout the essay, we will use N to denote the total number of words and K to denote the number of topics. We will assume that we are working with an example set of D documents. We will also use f-d to denote the fraction of the word j in document d (i.e. f-d, j = number of topics (j) / Nd, where number (j) is the number of times word j appears in the document, and Nd the number of words in the document). For topic models, variation updates are such that the arithmetically intractable E level (Sontag and Roy, 2000), as in Section 2. Repeat the model parameters for topic models, the subject parameters are the previous parameter and the subject-word matrix \u03b2. The observation variables d are the list of words in the document."}, {"heading": "4.1 Simplified updates in the long document limit", "text": "Of the aforementioned updates, it is difficult to assign an intuitive meaning to the \"lei\" - \"lei\" - \"\u03b2\" - \"i\" updates. (In fact, it is not even clear what one would ideally like them to be in the global optimum.) However, we will work in the large document boundary - and this will simplify the updates. In particular, in the E-step, in the large document boundary, the first term in the updated equation for \"i\" has a vanishing contribution. In this case, we can simplify the e-update as: \"\u03c6d,\" \"i, i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i."}, {"heading": "4.2 Alternating KL minimization and thresholded updates", "text": "In a slightly modified form, these updates were used in an essay by (Lee and Seung, 2000) in the context of non-negative matrix factorization, where the authors demonstrated that among these updates, Dd = 1 KL (f, j, f, j, j) does not decrease. One can easily modify their arguments to show that the same property is retained when the E-step is replaced by a step. (Indeed, the listing of the above updates is one way to solve this problem.) D is the K-dimensional simplex - i.e. the minimization of the KL divergence between the counts and the \"predicted\" variables."}, {"heading": "5 Initializations", "text": "We look at two different strategies for initialization: First, we look at the case in which we initialize with the theme-word matrix, and the document priors that have the right support, and the analysis of tEM will be the cleanest in this case. While the paper focuses on tEM, we will show that this initialization can actually be done efficiently for our case. Second, we look at an initialization that is inspired by what the current LDA-c implementation is using. Specifically, we assume that for each topic i the user has a way to find a seed document in which the share of theme i is at least Cl. Then, when initializing, treat this document as if it were valuable: namely setting \u03b20i, j the fraction of the word in this document. We are not trying to design an algorithm to find these documents."}, {"heading": "6 Case study 1: Sparse topic priors, support initialization", "text": "Let's start with a simple case. As already mentioned, all our results only hold in the long document regimes: we will assume d for each document, the number of sampled words is large enough to approximate the expected frequencies of the words, i.e. you can find for each topic a topic that matches the topic-word matrix, and the topic-topic priors. Let's first consider the assumptions about the topic-word matrix. We will impose conditions to ensure that the topics do not overlap too much. We assume that the topics are discriminatory: Every word appears in o (K) topics. \u2022 Almost incompatible supports: \u2022 We will if the intersection of the supports is i and i."}, {"heading": "7 Case study 2: Dominating topics, seeded initialization", "text": "Next, we consider an initialization that is essentially what the current implementation of LDA-c uses."}, {"heading": "8 On common words", "text": "In this case, the above proofs as they are will not work, because common words do not have individual documents. However, if 1 \u2212 1\u03ba100 fractions of documents in which topic i is dominant contain topic i with proportions 1 \u2212 1\u0445100, and furthermore in each topic the weight of these words does not exceed 1\u0445100, then our proofs still work with both initializations4 The idea for the argument is simple: If the dominant topic is very large, we show that f * d, j ftd, j is very strongly correlated with \u03b2 * i, j \u03b2ti, j, so that these documents behave like anchor documents. Namely, we can show: Theorem 5. If we also have common words that satisfy the above properties, according to O (log (1 / 3) + logN) KL-EM updates in cases 1 + 1 and 2 + proportions in cases 1 + 1 and 1 \u00b2 in the studies."}, {"heading": "9 Discussion and open problems", "text": "In this paper, we provide the first characterization of sufficient conditions when variation conclusions lead to optimal parameter estimates for topic models. Our evidence also suggests possible hard cases for variation conclusions, namely cases with large dynamic range compared to the proportion of anchor words and / or correlated topic priorities. It is not difficult to create such cases by hand where the initialization of support works very poorly, even with only anchor words and ordinary words. We have not made any effort to investigate the optimal relationship between the dynamic range and the proportion of anchor words, as it is not clear what is the \"worst case\" for this trade, but the initialization sought works empirically much better. We have found that when Cl \u2265 0.6 and when the percentage of anchor words is as low as 0.2, variation-related conclusions restore the basic truth, even in cases with relatively large EM dynamic range. Our current evidence methods are too weak to capture this observation (in fact, even the largest topic is sometimes incorrect in the initial stages of the dynamics being identified)."}, {"heading": "Acknowledgements", "text": "We would like to thank Sanjeev Arora for the helpful discussions in various phases of this work. 3We stress that we want to analyze whether or not variation conclusions work. Algorithmic handling of ordinary words is simple: they can first be detected and \"filtered out,\" and then we can only update the variation conclusions using the rest of the words."}, {"heading": "A Notation throughout supplementary material", "text": "We will use \",., &\" to indicate that the corresponding (in) equality holds up to constants, we will use \u21d4 to denote equivalence, and we will say that an event with a high probability occurs when it occurs with a probability of 1 \u2212 1Kc or 1 \u2212 1Nc for a constant c."}, {"heading": "B Case study 1: Sparse topic priors, support initialization", "text": "\"We need to make sure that the issue with maximum impact, i is the dominant phase I: Getting constant multiplicative factor estimates.\" The general outline of the evidence will be the following: \u2022 Identifying dominant theme: For the modified tEM updates, we need to make sure that the subject-word matrix and the subject-proportions to multiplicative accuracy 1 + 2, for any other type of quantitative accuracy 1 + 3, for any other type of quantitative accuracy 1 + 3, for any other type of quantitative accuracy. \u2022 The general outline of the evidence will be the following: \u2022 Identifying dominant theme: For the modified tEM updates, we need to make sure that the issue with maximum quantitative changes, i is the dominant phase I: Getting constant multiplicative factor."}, {"heading": "C Case study 2: Dominating topics, seeded initialization", "text": "Reminder: Initialization is done in the following way: \u2022 For each topic, the user provides a document d, in which we identify the topic of satisfaction of case study 2, in which the number of documents is specified, the number of documents will be higher than the number of topics we have after answering the topic (1 / 2) + logN) of the KL-tEM updates, we recover from the topic-word matrix and the topic proportions to multiplicative accuracy. \u2022 Phase I: Anchor Identification: First, we will show that we can identify the dominant topic in each of the documents, the anchors will make progress in the sense that according to O (logN) the number of rounds, the values for the topic are almost zero."}, {"heading": "D Justification of prior assumptions", "text": "In this section, we provide a brief motivation for our choice of characteristics on the subject to which we are referring. Nothing in the other sections depends crucially on this section, so that it can be freely skipped to the first reading. Most of our characteristics on the subject of Priors are inspired by what is going on with the Dirichlet - especially the variants of all \"weak\" topics, but the other topics cannot be neglected either.) To prove the best of our knowledge, the lemmas proven topics are not otherwise derived, so that we include them for completeness. For all the following assertions, we will deal with the following scenario: (1, 2, 2)."}, {"heading": "E On common words", "text": "In this section, we show how to modify the evidence from the previous section to deal with common words as well. (We stress that common words are easy to handle if you are allowed to filter them out, but we want to analyze the conditions under which the variable conclusions could handle them on their own. (The difference from the previous sections is that it is not clear how to argue progress for common words: \u2022 On top of the assumptions we have in either Case Study 1 or Case Study 2, we assume that there are words that appear in all topics, but their probabilities are within a constant span of each other, B.) We will designate these common words as such, with no loss of generality. (If the claim is held to be a smaller span, then it certainly applies to 2.) We assume that the probability is within a constant span, B."}, {"heading": "F Estimates on number of documents", "text": "Finally, we give a few aids to estimate how many documents we need. Characteristics we need are that the empirical margins of a dominant topic in the documents in which it dominates are close to the actual numbers and that the empirical margins of the dominant topic are close to the actual numbers due to the total number of documents in which a discriminatory word does not exist. The first statement is: Lemma 50. Let Ei \u00b2 s dominate topic d, i \u00b2 s dominating topic. If the total number of documents D = 2 K + 2, and Di is the number of documents in which i is the dominant topic, then with high probability, for all topics i, (1 topic) Ei \u00b2 s dominating topic d, i \u00b2 s dominating topic is. If the total number of documents D = 2 K \u00b2 s does not exist, and Di is the number of documents in which i is the dominant topic, then with high probability, then for all documents."}], "references": [{"title": "Learning sparsely used overcomplete dictionaries via alternating minimization", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli"], "venue": "In Proceedings of The 27th Conference on Learning Theory (COLT),", "citeRegEx": "Agarwal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2013}, {"title": "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "author": ["A. Anandkumar", "S. Kakade", "D. Foster", "Y. Liu", "D. Hsu"], "venue": "Technical report,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Learning latent bayesian networks and topic models under expansion constraints", "author": ["A. Anandkumar", "D. Hsu", "A. Javanmard", "S. Kakade"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "Computing a nonnegative matrix factorization\u2013provably", "author": ["S. Arora", "R. Ge", "R. Kanna", "A. Moitra"], "venue": "In Proceedings of the forty-fourth annual ACM symposium on Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Learning topic models \u2013 going beyond svd", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "In Proceedings of the 53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "In Proceedings of The 27th Conference on Learning Theory (COLT),", "citeRegEx": "Arora et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Simple, efficient, and neural algorithms for sparse coding", "author": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"], "venue": "In Proceedings of The 28th Conference on Learning Theory (COLT),", "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Statistical guarantees for the em algorithm: From population to sample-based analysis", "author": ["S. Balakrishnan", "M.J. Wainwright", "B. Yu"], "venue": "arXiv preprint arXiv:1408.2156,", "citeRegEx": "Balakrishnan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2014}, {"title": "A provable svd-based algorithm for learning topics in dominant admixture corpus", "author": ["T. Bansal", "C. Bhattacharyya", "R. Kannan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A two-round variant of em for gaussian mixtures", "author": ["S. Dasgupta", "L. Schulman"], "venue": "In Proceedings of Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Dasgupta and Schulman.,? \\Q2000\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2000}, {"title": "A probabilistic analysis of em for mixtures of separated, spherical gaussians", "author": ["S. Dasgupta", "L. Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dasgupta and Schulman.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2007}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Topic discovery through data dependent and random projections", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "arXiv preprint arXiv:1303.3664,", "citeRegEx": "Ding et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2013}, {"title": "Efficient distributed topic modeling with provable guarantees", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "In Proceedings ot the 17th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Stochastic variational inference", "author": ["M. Hoffman", "D. Blei", "J. Paisley", "C. Wan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "An introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["A. Kumar", "R. Kannan"], "venue": "In Proceedings of Foundations of Computer Science (FOCS),", "citeRegEx": "Kumar and Kannan.,? \\Q2010\\E", "shortCiteRegEx": "Kumar and Kannan.", "year": 2010}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D. Lee", "S. Seung"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lee and Seung.,? \\Q2000\\E", "shortCiteRegEx": "Lee and Seung.", "year": 2000}, {"title": "Phase retrieval using alternating minimization", "author": ["P. Netrapalli", "P. Jain", "S. Sanghavi"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Netrapalli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Netrapalli et al\\.", "year": 2013}, {"title": "Complexity of inference in latent dirichlet allocation", "author": ["D. Sontag", "D. Roy"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sontag and Roy.,? \\Q2000\\E", "shortCiteRegEx": "Sontag and Roy.", "year": 2000}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["R. Sundberg"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "Sundberg.,? \\Q1974\\E", "shortCiteRegEx": "Sundberg.", "year": 1974}, {"title": "Dirichlet draws are sparse with high probability", "author": ["M. Telgarsky"], "venue": null, "citeRegEx": "Telgarsky.,? \\Q2013\\E", "shortCiteRegEx": "Telgarsky.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al.", "startOffset": 130, "endOffset": 155}, {"referenceID": 10, "context": "The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003).", "startOffset": 127, "endOffset": 146}, {"referenceID": 6, "context": "The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient.", "startOffset": 76, "endOffset": 96}, {"referenceID": 18, "context": "Among more classical results in this direction are the analyses of Lloyd\u2019s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007).", "startOffset": 174, "endOffset": 198}, {"referenceID": 11, "context": "Among more classical results in this direction are the analyses of Lloyd\u2019s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007).", "startOffset": 200, "endOffset": 229}, {"referenceID": 12, "context": "Among more classical results in this direction are the analyses of Lloyd\u2019s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007).", "startOffset": 231, "endOffset": 260}, {"referenceID": 8, "context": "The recent work of (Balakrishnan et al., 2014) also characterizes global convergence properties of the EM algorithm for more general settings.", "startOffset": 19, "endOffset": 46}, {"referenceID": 0, "context": "(Agarwal et al., 2013), (Arora et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": ", 2013), (Arora et al., 2015) prove that with appropriate initialization, alternating minimization can provably recover the ground truth.", "startOffset": 9, "endOffset": 29}, {"referenceID": 20, "context": "(Netrapalli et al., 2013) have proven similar results in the context of phase retreival.", "startOffset": 0, "endOffset": 25}, {"referenceID": 17, "context": "Another popular heuristic which has so far eluded such attempts is known as variational inference (Jordan et al., 1999).", "startOffset": 98, "endOffset": 119}, {"referenceID": 10, "context": "We provide the first characterization of global convergence of variational inference based algorithms for topic models (Blei et al., 2003).", "startOffset": 119, "endOffset": 138}, {"referenceID": 13, "context": "The expectation-maximization (EM) algorithm is an iterative method to achieve this, dating all the way back to (Dempster et al., 1977) and (Sundberg, 1974) in the 70s.", "startOffset": 111, "endOffset": 134}, {"referenceID": 22, "context": ", 1977) and (Sundberg, 1974) in the 70s.", "startOffset": 12, "endOffset": 28}, {"referenceID": 10, "context": "3 Topic models We will focus on a particular latent variable model, which is very often studied - topic models (Blei et al., 2003).", "startOffset": 111, "endOffset": 130}, {"referenceID": 10, "context": "(Originally introduced by (Blei et al., 2003).", "startOffset": 26, "endOffset": 45}, {"referenceID": 5, "context": ", 2012b),(Arora et al., 2013), as well as (Anandkumar et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 2, "context": ", 2013), as well as (Anandkumar et al., 2013), (Ding et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 14, "context": ", 2013), (Ding et al., 2013), (Ding et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 15, "context": ", 2013), (Ding et al., 2014) and (Bansal et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 9, "context": ", 2014) and (Bansal et al., 2014).", "startOffset": 12, "endOffset": 33}, {"referenceID": 5, "context": ", 2012b) and (Arora et al., 2013) assume that the topic-word matrix contains \u201canchor words\u201d.", "startOffset": 13, "endOffset": 33}, {"referenceID": 2, "context": "(Anandkumar et al., 2013) on the other hand work with a certain expansion assumption on the word-topic graph, which says that if one takes a subset S of topics, the number of words in the support of these topics should be at least |S|+ smax, where smax is the maximum support size of any topic.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "(Similar to the expansion in (Anandkumar et al., 2013), but only over constant sized subsets.", "startOffset": 29, "endOffset": 54}, {"referenceID": 10, "context": "(Originally introduced by (Blei et al., 2003).", "startOffset": 26, "endOffset": 45}, {"referenceID": 9, "context": ") The documents will also have a \u201cdominating topic\u201d, similarly as in (Bansal et al., 2014).", "startOffset": 69, "endOffset": 90}, {"referenceID": 10, "context": "4 Variational relaxation for learning topic models In this section we briefly review the variational relaxation for topic models following closely the description in (Blei et al., 2003).", "startOffset": 166, "endOffset": 185}, {"referenceID": 21, "context": "For topic models variational updates are way to approximate the computationally intractable E-step (Sontag and Roy, 2000) as described in Section 2.", "startOffset": 99, "endOffset": 121}, {"referenceID": 10, "context": "In (Blei et al., 2003) it\u2019s shown that for Dirichlet priors \u03b1 the optimal distributions q, q j are a Dirichlet distribution for q, with some parameter \u03b3\u0303, and multinomials for q j , with some parameters \u03c6j .", "startOffset": 3, "endOffset": 22}, {"referenceID": 10, "context": "The way we derived them, these updates appear to be an approximate form of the variational updates in (Blei et al., 2003).", "startOffset": 102, "endOffset": 121}, {"referenceID": 10, "context": "It is intuitively clear that in the large document limit, this approximation should not be much worse than the one in (Blei et al., 2003), as the posterior concentrates around the maximum likelihood value.", "startOffset": 118, "endOffset": 137}, {"referenceID": 19, "context": "In a slightly modified form, these updates were used in a paper by (Lee and Seung, 2000) in the context of non-negative matrix factorization.", "startOffset": 67, "endOffset": 88}, {"referenceID": 7, "context": "These are analogues of distributions that have been analyzed for dictionary learning (Arora et al., 2015).", "startOffset": 85, "endOffset": 105}, {"referenceID": 23, "context": "This was formally proven by (Telgarsky, 2013) - though sparsity there means a small number of large coordinates.", "startOffset": 28, "endOffset": 45}, {"referenceID": 7, "context": "Such assumptions also appear in the context of dictionary learning (Arora et al., 2015).", "startOffset": 67, "endOffset": 87}, {"referenceID": 6, "context": "This uses an idea inspired by (Arora et al., 2014) in the setting of dictionary learning.", "startOffset": 30, "endOffset": 50}, {"referenceID": 9, "context": "A similar assumption (a small fraction of almost pure documents) appeared in a recent paper by (Bansal et al., 2014).", "startOffset": 95, "endOffset": 116}, {"referenceID": 23, "context": "1 Sparsity To characterize the sparsity of the topic proportions in a document, we will need the following lemma from (Telgarsky, 2013): Lemma 38.", "startOffset": 118, "endOffset": 135}, {"referenceID": 23, "context": "(Telgarsky, 2013) For a Dirichlet distribution with parameters (C1/k , C2/k , .", "startOffset": 0, "endOffset": 17}], "year": 2015, "abstractText": "Variational inference is a very efficient and popular heuristic used in various forms in the context of latent variable models. It\u2019s closely related to Expectation Maximization (EM), and is applied when exact EM is computationally infeasible. Despite being immensely popular, current theoretical understanding of the effectiveness of variaitonal inference based algorithms is very limited. In this work we provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. More specifically, we show that variational inference provably learns the optimal parameters of a topic model under natural assumptions on the topic-word matrix and the topic priors. The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al., 2012b). The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003). It is well known that initialization plays a crucial role in how well variational based algorithms perform in practice. The initializations that we use are fairly natural. One of them is similar to what is currently used in LDA-c, the most popular implementation of variational inference for topic models. The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient. While our primary goal is to provide insights into when variational inference might work in practice, the multiplicative, rather than the additive nature of the variational inference updates forces us to use fairly non-standard proof arguments, which we believe will be of general interest. Our proofs rely on viewing the updates as an operation which, at each timestep, sets the new parameter estimates to be noisy convex combinations of the ground truth values, and a bounded error term which depends on the previous estimate. The weight on the ground truth values will be large, compared to the error term, which will cause the error term to eventually reach zero. The large weight on the ground truth values will be a byproduct of our model assumptions, which will imply a \u201clocal\u201d notion of anchor words for each document words which only appear in one topic in a given document, as well as a \u201clocal\u201d notion of anchor documents for each word documents where that word appears as part of a single topic. Princeton University, Computer Science Department. Email: pawashti@cs.princeton.edu. Supported by NSF grant CCF1302518. Princeton University, Computer Science Department. Email: risteski@cs.princeton.edu. Partially supported by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Sanjeev Arora\u2019s Simons Investigator Award, and a Simons Collaboration Grant.", "creator": "LaTeX with hyperref package"}}}