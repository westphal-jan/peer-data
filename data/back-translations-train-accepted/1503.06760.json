{"id": "1503.06760", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "Unsupervised POS Induction with Word Embeddings", "abstract": "Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the effect of various choices while inducing word embeddings on \"downstream\" POS induction results.", "histories": [["v1", "Mon, 23 Mar 2015 18:32:56 GMT  (37kb,D)", "http://arxiv.org/abs/1503.06760v1", "NAACL 2015"]], "COMMENTS": "NAACL 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chu-cheng lin", "waleed ammar", "chris dyer", "lori s levin"], "accepted": true, "id": "1503.06760"}, "pdf": {"name": "1503.06760.pdf", "metadata": {"source": "CRF", "title": "Unsupervised POS Induction with Word Embeddings", "authors": ["Chu-Cheng Lin", "Waleed Ammar", "Chris Dyer", "Lori Levin"], "emails": ["chuchenl@cs.cmu.edu", "wammar@cs.cmu.edu", "cdyer@cs.cmu.edu", "lsl@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Unattended POS embedding is the problem of assigning word marks to syntactic categories that have only a corpus of unmarked text. In this paper, we investigate the effects of replacing words with their vector space in two POS embedding models: the classic first-order HMM (Kupiec, 1992) and the newly introduced conditional autoencoder (Ammar et al., 2014). In each model, instead of a conditional multinomic distribution2, we use conditional Gaussian distribution to generate a word token wi-V with a POS tag ti-T and generate a d-dimensional word to embed vwi-Rd given ti.1In contrast to Yatbaz et al. (2014), we use easily accessible and widely used embedding of word types."}, {"heading": "2 Vector Space Word Embeddings", "text": "Word embeddings represent words in the vocabulary of a language as dots in a d-dimensional space, so that nearby words (dots) are similar in terms of their distribution properties. A variety of techniques for learning embeddings have been suggested, e.g. matrix factorization (Deerwester et al., 1990; Dhillon et al., 2011) and neural language modeling (Mikolov et al., 2011; Collobert and Weston, 2008).For the POS embeddings task we need embeddings that capture syntactical similarities."}, {"heading": "3 Models for POS Induction", "text": "In this section, we briefly look at two classes of models used for POS induction (HMMs and CRF autoencoders) and explain how to generate word embedding observations in each class. We represent a length set as w = < w1, w2,..., w '> \u0445V' and a sequence of tags as t = < t1, t2,..., t '> \u0435T'. The embedding of the w-V word is written as vw-Rd."}, {"heading": "3.1 Hidden Markov Models", "text": "The hidden Markov models with the multinomial emissions are a classic model for POS induction. This model assumes that a latent Markov process with discrete states representing the POS categories defines the individual words in the vocabulary by state (i.e., tag) specific emission distributions (i.e., tag) thus defines the following common distribution across the sequences of observations and tags: p (w, t) = sequential emission probability (ti \u2212 1) \u00b7 p (i) (1), where the distribution p (ti \u2212 1) is the probability of transition and p (wi \u2212 1) the probability of emission, the probability of a particular tag generating the word at position i.53https: / code.google.com / p / word2vec / 4https: / github.com / wlin12 / wang2vec / 5terms for start and stop probabilities."}, {"heading": "3.2 Conditional Random Field Autoencoders", "text": "The second class of models extending this work is called CRF autoencoders, which we recently proposed in q (Ammar et al., 2014). It is a scalable family of models for feature-rich learning from unlabeled examples. The model conditions on a copy of the structured input and generate a reconstruction of the input via a series of interdependent latent variables that represent the linguistic structure of interest. t is the linguistic structure of interest (e.g. a sequence of POS tags), and w is a generic reconstruction of the input. POS induction is structured input (e.g. a token sequence), t is the linguistic structure of interest (e.g. a sequence of POS tags), and w is a generic input reconstruction."}, {"heading": "4 Experiments", "text": "In this section we try to answer the following questions: \u2022 \u00a7 4.1: Do syntactically informed word embedding improve POS embedding? Which model performs best? \u2022 \u00a7 4.2: Which word embedding is suitable for POS embedding?"}, {"heading": "4.1 Choice of POS Induction Models", "text": "In fact, most of them are able to reform themselves, and they are able to reform themselves, \"he said in an interview with The New York Times."}, {"heading": "4.2 Choice of Embeddings", "text": "This could be because structured skip-gram embedding gives each position within the context window its own project matrix, so that the smear effect is not as pronounced as the window grows compared to the standard embedding. 1211In (Ammar et al., 2014) we found that careful initialization of the CRF model with multinomial reconstructions is necessary. 12In preliminary experiments we also compared standard skip-gram embedding with SENNA embedding (Collobert et al., 2011) (which is trained in a semi-supervised multi-task learning setup, with one task POS tagging) as a subset of the English POS word with 0.0."}, {"heading": "4.3 Discussion", "text": "We have shown that (re) generating word embeddings work much better than opaque word types in unattended POS embeddings. At a high level, this confirms previous findings that unattended word embeddings capture syntactic properties of words, and shows that different embeddings capture syntactically more meaningful information than others. As such, we maintain that unattended POS embeddings can be considered a diagnostic measure to evaluate the syntactic quality of embeddings. To get a better understanding of what the multivariate Gaussian models have learned, we are conducting a mountaineering experiment on our English dataset. We are much better at using SENNA embeddings, which results in a Vmeasure score of 0.57 compared to 0.51 for embeddings of skip programs. Since SENNA embeddings are only available in English, we do not include them in the comparison in the 1st category of each average VOS word randomly generated in the same category."}, {"heading": "5 Conclusion", "text": "We propose to use a multivariate Gaussian model to generate vector space representations of observed words in generative or hybrid models for POS induction, as a superior alternative to using multinomial distributions to generate categorical word types. We find the performance of a simple Gaussian HMM competitive with highly functional baselines. Wefurther show that substituting the emission part of the CRF autoencoder can bring further improvements. We also confirm previous results suggesting that smaller context windows in Skip-gram models lead to word embeddings that encode more syntactical information. It would be interesting to see if we can apply this approach to other tasks that require generative modeling of textual observations, such as speech modeling and grammar induction."}, {"heading": "Acknowledgements", "text": "This work was sponsored by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract / grant numbers W911NF-112-0042 and W911NF-10-1-0533."}], "references": [{"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["Ammar et al.2014] Waleed Ammar", "Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proc. of ACL", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Painless unsupervised learning with features", "author": ["Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein"], "venue": "In Proc. of NAACL", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Distance dependent Chinese restaurant processes. JMLR", "author": ["Blei", "Frazier2011] David M. Blei", "Peter I. Frazier"], "venue": null, "citeRegEx": "Blei et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2011}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Buchholz", "Marsi2006] Sabine Buchholz", "Erwin Marsi"], "venue": "In CoNLL-X", "citeRegEx": "Buchholz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proc. of ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Indexing by latent semantic analysis", "author": ["Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["Dean Foster", "Lyle Ungar"], "venue": "In NIPS,", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free transla", "author": ["Dyer et al.2010] Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Johnathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains", "author": ["Gauvain", "Lee1994] J. Gauvain", "Chin-Hui Lee"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "Gauvain et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Gauvain et al\\.", "year": 1994}, {"title": "The iconicity of the universal categories \u201cnoun\u201d and \u201cverb", "author": ["Hopper", "Thompson1983] Paul Hopper", "Sandra Thompson"], "venue": "Iconicity in Syntax: Proceedings of a symposium on iconicity in syntax", "citeRegEx": "Hopper et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Hopper et al\\.", "year": 1983}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["Mark Johnson"], "venue": "In Proc. of EMNLP", "citeRegEx": "Johnson.,? \\Q2007\\E", "shortCiteRegEx": "Johnson.", "year": 2007}, {"title": "Robust part-ofspeech tagging using a hidden Markov model", "author": ["Julian Kupiec"], "venue": "Computer Speech and Language,", "citeRegEx": "Kupiec.,? \\Q1992\\E", "shortCiteRegEx": "Kupiec.", "year": 1992}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso"], "venue": "In Proc. of NAACL", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2012] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "In Proc. of LREC,", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Corpus portal for search in monolingual corpora", "author": ["Matthias Richter", "Christian Biemann"], "venue": "In Proc. of LREC,", "citeRegEx": "Quasthoff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Quasthoff et al\\.", "year": 2006}, {"title": "V-measure: A conditional entropy-based external cluster evaluation", "author": ["Rosenberg", "Hirschberg2007] Andrew Rosenberg", "Julia Hirschberg"], "venue": null, "citeRegEx": "Rosenberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rosenberg et al\\.", "year": 2007}, {"title": "HMM training based on CV-EM and CV gaussian mixture optimization", "author": ["Shinozaki", "Kawahara2007] T. Shinozaki", "T. Kawahara"], "venue": "In Proc. of the 2007 ASRU Workshop,", "citeRegEx": "Shinozaki et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shinozaki et al\\.", "year": 2007}, {"title": "POS induction with distributional and morphological information using a distance-dependent Chinese restaurant process", "author": ["Sirts et al.2014] Kairit Sirts", "Jacob Eisenstein", "Micha Elsner", "Sharon Goldwater"], "venue": "In Proc. of ACL", "citeRegEx": "Sirts et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sirts et al\\.", "year": 2014}, {"title": "Ukwabelana: An open-source morphological Zulu corpus", "author": ["Andrew van der Spuy", "Peter A. Flach"], "venue": "In Proc. of COLING,", "citeRegEx": "Spiegler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Spiegler et al\\.", "year": 2010}, {"title": "Unsupervised instance-based part of speech induction using probable substitutes", "author": ["Enis R\u0131fat Sert", "Deniz Yuret"], "venue": "In Proc. of COLING", "citeRegEx": "Yatbaz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yatbaz et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "In this paper we explore the effect of replacing words with their vector space embeddings1 in two POS induction models: the classic first-order HMM (Kupiec, 1992) and the newly introduced conditional random field autoencoder (Ammar et al.", "startOffset": 148, "endOffset": 162}, {"referenceID": 0, "context": "In this paper we explore the effect of replacing words with their vector space embeddings1 in two POS induction models: the classic first-order HMM (Kupiec, 1992) and the newly introduced conditional random field autoencoder (Ammar et al., 2014).", "startOffset": 225, "endOffset": 245}, {"referenceID": 22, "context": "Unlike Yatbaz et al. (2014), we leverage easily obtainable and widely used embeddings of word types.", "startOffset": 7, "endOffset": 28}, {"referenceID": 21, "context": "Our results also confirm the conclusions of Sirts et al. (2014) who were likewise able to improve POS induction results, albeit using a custom clustering model based on the the distancedependent Chinese restaurant process (Blei and Frazier, 2011).", "startOffset": 44, "endOffset": 64}, {"referenceID": 8, "context": ", matrix factorization (Deerwester et al., 1990; Dhillon et al., 2011) and neural language modeling (Mikolov et al.", "startOffset": 23, "endOffset": 70}, {"referenceID": 9, "context": ", matrix factorization (Deerwester et al., 1990; Dhillon et al., 2011) and neural language modeling (Mikolov et al.", "startOffset": 23, "endOffset": 70}, {"referenceID": 16, "context": "\u2022 Skip-gram embeddings (Mikolov et al., 2013) are based on a log bilinear model that predicts an unordered set of context words given a target word.", "startOffset": 23, "endOffset": 45}, {"referenceID": 1, "context": "Bansal et al. (2014) found that smaller context window sizes tend to result in embeddings with more syntactic information.", "startOffset": 0, "endOffset": 21}, {"referenceID": 15, "context": "\u2022 Structured skip-gram embeddings (Ling et al., 2015) extend the standard skip-gram embeddings (Mikolov et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 16, "context": ", 2015) extend the standard skip-gram embeddings (Mikolov et al., 2013) by taking into account the relative positions of words in a given context.", "startOffset": 49, "endOffset": 71}, {"referenceID": 15, "context": "We use the tool word2vec3 and Ling et al. (2015)\u2019s modified version4 to generate both plain and structured skip-gram embeddings in nine languages.", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "\u2022 p(wi | ti) is parameterized as a multinomial logistic regression model with hand-engineered features as detailed in (Berg-Kirkpatrick et al., 2010).", "startOffset": 118, "endOffset": 149}, {"referenceID": 0, "context": "The second class of models this work extends is called CRF autoencoders, which we recently proposed in (Ammar et al., 2014).", "startOffset": 103, "endOffset": 123}, {"referenceID": 0, "context": "In (Ammar et al., 2014), we explored two kinds of reconstructions \u0175: surface forms and Brown clusters (Brown et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 4, "context": ", 2014), we explored two kinds of reconstructions \u0175: surface forms and Brown clusters (Brown et al., 1992), and used \u201cstupid multinomials\u201d as the underlying distributions for regenerating \u0175.", "startOffset": 86, "endOffset": 106}, {"referenceID": 14, "context": "\u2022 Baseline: HMM with multinomial emissions (Kupiec, 1992),", "startOffset": 43, "endOffset": 57}, {"referenceID": 0, "context": "\u2022 Baseline: CRF autoencoder with multinomial reconstructions (Ammar et al., 2014),7", "startOffset": 61, "endOffset": 81}, {"referenceID": 22, "context": ", 2007) (for Arabic, Basque, Greek, Hungarian and Italian), and the Ukwabelana corpus (Spiegler et al., 2010) (for Zulu).", "startOffset": 86, "endOffset": 109}, {"referenceID": 17, "context": "For evaluation, we obtain the corresponding gold-standard POS tags by deterministically mapping the language-specific POS tags in the aforementioned corpora to the corresponding universal POS tag set (Petrov et al., 2012).", "startOffset": 200, "endOffset": 221}, {"referenceID": 0, "context": "This is the same set up we used in (Ammar et al., 2014).", "startOffset": 35, "endOffset": 55}, {"referenceID": 18, "context": ", word2vec) embeddings with a context window size = 1 and with dimensionality d = 100, trained with the largest corpora for each language in (Quasthoff et al., 2006), in addition to the plain", "startOffset": 141, "endOffset": 165}, {"referenceID": 0, "context": "9 While the CRF autoencoder with multinomial reconstructions were carefully initialized as discussed in (Ammar et al., 2014), CRF autoencoder with Gaussian reconstructions were initialized uniformly at random in [\u22121, 1].", "startOffset": 104, "endOffset": 124}, {"referenceID": 10, "context": "sh script in the cdec decoder (Dyer et al., 2010) to tokenize the corpora from (Quasthoff et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 18, "context": ", 2010) to tokenize the corpora from (Quasthoff et al., 2006).", "startOffset": 37, "endOffset": 61}, {"referenceID": 13, "context": "We found the V-measure results to be consistent with the many-to-one evaluation metric (Johnson, 2007).", "startOffset": 87, "endOffset": 102}, {"referenceID": 0, "context": "In (Ammar et al., 2014), we found that careful initialization for the CRF autoencoder model with multinomial reconstructions is necessary.", "startOffset": 3, "endOffset": 23}, {"referenceID": 7, "context": "In preliminary experiments, we also compared standard skip-gram embeddings to SENNA embeddings (Collobert et al., 2011) (which are trained in a semi-supervised multi-task learning setup, with one task being POS tagging) on a subset of the English PTB corpus.", "startOffset": 95, "endOffset": 119}, {"referenceID": 1, "context": "This result confirms the observations of Bansal et al. (2014).", "startOffset": 41, "endOffset": 62}], "year": 2015, "abstractText": "Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the effect of various choices while inducing word embeddings on \u201cdownstream\u201d POS induction results.", "creator": "LaTeX with hyperref package"}}}