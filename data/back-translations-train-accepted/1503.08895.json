{"id": "1503.08895", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2015", "title": "End-To-End Memory Networks", "abstract": "In this paper we introduce a variant of Memory Networks that needs significantly less supervision to perform question and answering tasks. The original model requires that the sentences supporting the answer be explicitly indicated during training. In contrast, our approach only requires the answer to the question during training. We apply the model to the synthetic bAbI tasks, showing that our approach is competitive with the supervised approach, particularly when trained on a sufficiently large amount of data. Furthermore, it decisively beats other weakly supervised approaches based on LSTMs. The approach is quite general and can potentially be applied to many other tasks that require capturing long-term dependencies.", "histories": [["v1", "Tue, 31 Mar 2015 03:05:37 GMT  (119kb,D)", "http://arxiv.org/abs/1503.08895v1", null], ["v2", "Fri, 3 Apr 2015 02:23:20 GMT  (120kb,D)", "http://arxiv.org/abs/1503.08895v2", null], ["v3", "Sun, 12 Apr 2015 04:19:33 GMT  (120kb,D)", "http://arxiv.org/abs/1503.08895v3", null], ["v4", "Mon, 8 Jun 2015 21:42:20 GMT  (331kb,D)", "http://arxiv.org/abs/1503.08895v4", null], ["v5", "Tue, 24 Nov 2015 19:41:57 GMT  (332kb,D)", "http://arxiv.org/abs/1503.08895v5", "Accepted to NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["sainbayar sukhbaatar", "arthur szlam", "jason weston", "rob fergus"], "accepted": true, "id": "1503.08895"}, "pdf": {"name": "1503.08895.pdf", "metadata": {"source": "CRF", "title": "Weakly Supervised Memory Networks", "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we present a variant of Memory Networks [16] that requires significantly less supervision to perform question and answer tasks; the original model requires that the phrases that support the answer be explicitly stated during the training; in contrast, our approach only requires the answer to the question during the training; we apply the model to synthetic bAbI tasks and show that our approach competes with the monitored approach, especially if it is trained on a sufficiently large amount of data; and it clearly suggests other weakly monitored approaches based on LSTMs. The approach is quite general and can potentially be applied to many other tasks that require the capture of long-term dependencies."}, {"heading": "1 Introduction", "text": "The Memory Networks introduced by Weston et al. [16] investigate how explicit long-term storage can be combined with neural networks, applying their model to a series of synthetic question and answer tasks [15] that are designed to include many forms of thinking, including many that current models such as recursive neural networks (RNNNs) cannot execute. At test time, input into the model is a set of text sentences, along with a question, and the model output is a predicted answer. One limitation of the model is that it requires comprehensive monitoring, which requires not only the down-to-earth truth answer, but also explicit specification of the supporting sentences within the text. This monitoring level simplifies the formation of the read and write functions, since the correspondence is explicit: the model is told which part of the input it should be used in memory. In this paper, we examine a model that is similar to a network, [16] that the explicit response can only be provided with the explicit answer."}, {"heading": "2 Approach", "text": "Our model primarily addresses the bAbI question and answer proposed by Weston et al. but could easily be adapted to other text applications. A given bAbI task consists of a series of statements followed by a question whose answer is typically a single word (in some tasks answers are a set of words). Consider an example problem: Sam goes to the kitchen. Sam picks up an apple. Sam goes to the bedroom. Sam drops the apple."}, {"heading": "Q: Where is the apple?", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Bedroom", "text": "The answer is available to the model at training time, but must be predicted at test time. Note that for each question only a subset of the statements contains information required for the answer, and the others are essentially irrelevant distraction factors (e.g. the first sentence in the example above). In Weston et al. this supporting subset was explicitly communicated to the model during training, and the main difference between this work and this work is that this information is no longer available. Consequently, the model must derive for itself during training and testing which sentences are relevant and which are not. There are a total of 20 different types of bAbI tasks that examine different forms of reasoning and deduction. For a more complete description of these tasks, please refer to [15]. Formally, we are given sample problems for one of the 20 bAbI tasks, each having a set of I-sentences {xpi}, for which I have \u2264 320; a question qp and answer ap."}, {"heading": "2.1 Single Layer", "text": "We start by describing our model for a single layer that implements a single storage operation, then show that it can be stacked to give multiple hops in memory. Since some of our target tasks require an understanding of the temporal order, we then describe a way to integrate time into the model by introducing specific temporal features into lookup operations. Each storage layer has an input and output part that implements content-based addresses, with each location a unique output vector."}, {"heading": "2.2 Multiple Layers", "text": "rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "2.3 Sentence Representation", "text": "In our experiments, we examine two different representations of the sentences. The problem is that it cannot capture the order of words in the sentence, which is important for some tasks. lj is a column vector with the structure lkj = (1 \u2212 j / J) \u2212 (k / d) (1 \u2212 2j / J) (assuming a 1 \u2212 based indexing), where J is the number of words in the sentence, where \u00b7 is an element-by-element multiplication. lj is the dimension of the embedding. In other words, lj is a linear ramp that extends from 1 \u2212 2j / J (0 to 1) and J is the number of words in the sentence, and d is the dimension of the embedding."}, {"heading": "2.4 Temporal Encoding", "text": "Several of the bAbI tasks require a certain idea of the temporal context, i.e. Q: Where was Sam before he entered the bedroom? To enable our model to address these, we modify the embedding of the sentence xi so that mi = \u2211 j axij + TA (i), where TA (i) is the fourth line of a special matrix TA encoding temporal information, the output embedding is extended in the same way with a matrix Tc (e.g. ci = \u2211 j Cxij + TC (i))), both TA and TC are learned during the training, and they are subject to the same common constraints as A and C. Note that sentences are indexed in reverse order, reflecting their relative meaning, so that x1 is the last sentence of history."}, {"heading": "3 Training", "text": "The model was trained at a learning rate of \u03b7 = 0.01, with all 25 epochs annealed by \u03b7 / 2 until 100 epochs were reached. No impulse or weight loss was used. Weights were randomly initialized using a Gaussian distribution with a mean of zero and \u03c3 = 0.1. When all tasks were simultaneously practiced with 1k training samples (10k training samples), 60 epochs (20 epochs) with a learning rate of \u03b7 / 2 were annealed every 15 epochs (5 epochs). In each training, a stack size of 32 was used (but the cost is not averaged over a stack), and gradients with a \"2 standard greater than 40 are divided by a constant to have the standard. In some of our experiments, we examined the start of training with Softmax in each memory layer, thereby making the model fully linear, except for the final weight matrix W."}, {"heading": "4 Related Work", "text": "A number of recent efforts have led to a desertification of network structures. [3] The LSTM is based on local memory cells that were located in the past. [4] In practice, the performance is modest. [5] Our model is capable of creating a global storage zone."}, {"heading": "5 Experiments", "text": "The model is evaluated on the basis of the Babi question and answer data set proposed in [15], using two versions of the data, one with 1000 training problems per task and a second larger with 10,000 per task. All experiments used a 3-layer model that implements 3 memory searches. Unless otherwise specified, the weight distribution scheme used is adjacent. For all tasks that output a list, we take any possible combination of possible results and record them as a separate answer vocabulary. In some tasks we observed a large discrepancy in the performance of our model (i.e. sometimes they fail badly, other times not depending on the initialization). This is solved by averaging the probability output from 10 models that are identical except for different random initialization, which improves the mean performance by a few percent, but most importantly reduces the discrepancy significantly."}, {"heading": "5.1 Baselines", "text": "We compare our approach to a number of alternative models: \u2022 MemNN: The fully supervised AM + NG + NL Memory Networks approach proposed in [15]. \u2022 MemNN-WS: A weakly supervised version of MemNN in which the supporting sentence designations are not used in the training. Since we are unable to propagate the maximum operations in each level backwards, we force that the first hop store shares at least one word with the question and the second hop store shares at least one word with the first hop and at least one word with the answer. All those memories that are called valid memories and the goal during the training is to classify them higher than invalid stores, using the same ranking criteria as during the fully supervised training. \u2022 LSTM: A standard LSTM model that is only trained using question-answer pairs (i.e. also weakly supervised)."}, {"heading": "5.2 Model Variants", "text": "We explore a variety of design choices: (i) BoW vs Position Encoding (PE) sentence representation; (ii) Training on all 20 tasks together vs independent training (joint training uses an embedding dimension of d = 50, while independent training uses d = 20); (iii) Two-phase training: first without softmaxes, then with (linear start (LS) vs training with softmaxes from the outset. Results in all 20 tasks are given in Table 1 and Table 2 for the 1k and 10k workouts, respectively. They show a number of interesting points: \u2022 Set for both sizes of training, the best poorly supervised models are relatively close to the monitored models (e.g. 1k: 6.7% for MemNN and 11.3% for position coding + linear noise, trained together and 10k: 3.2% for MemNN and 6.4% for position coding."}, {"heading": "6 Conclusions and Future Work", "text": "In this paper, we presented an architecture for solving the tasks in [15] without supervision of supporting facts. By using smooth memory search, we are able to train with backpropagation, and the system is able to figure out what facts to focus on by using only the answers to the questions and the stories. The architecture performs significantly better than baselines (LSTM, storage networks trained with weak supervision).However, there is still much work to be done. Our model is still unable to match the performance of the storage networks trained under full supervision. Furthermore, frictionless research may not be able to scale well to very long stories or challenge answering tasks with large databases. For these settings, we plan to explore multi-layered notions of attention. Furthermore, as in [6] it may be possible to replace backpropagation with a form of reinforcement learning."}], "references": [{"title": "Memory-based neural networks for robot learning", "author": ["C.G. Atkeson", "S. Schaal"], "venue": "Neurocomputing, 9:243\u2013 269", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["S. Das", "C.L. Giles", "G.-Z. Sun"], "venue": "In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, abs/1308.0850", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "CoRR, abs/1410.5401", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "CoRR, abs/1503.01007", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning longer memory in recurrent neural networks", "author": ["T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato"], "venue": "CoRR, abs/1412.7753", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A connectionist symbol manipulator that discovers the structure of context-free languages", "author": ["M.C. Mozer", "S. Das"], "venue": "Advances in Neural Information Processing Systems, pages 863\u2013863", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "The induction of dynamical recognizers", "author": ["J. Pollack"], "venue": "Machine Learning, 7(2-3):227\u2013252", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning matrices and their applications", "author": ["K. Steinbuch", "U. Piske"], "venue": "IEEE Transactions on Electronic Computers, 12:846\u2013862", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1963}, {"title": "Pattern recognition by means of automatic analogue apparatus", "author": ["W.K. Taylor"], "venue": "Proceedings of The Institution of Electrical Engineers, 106:198\u2013209", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1959}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint: 1502.05698", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint: 1410.391v8", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "In this paper we introduce a variant of Memory Networks [16] that needs significantly less supervision to perform question and answering tasks.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "[16], explore how explicit long-term storage can be combined with neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "They apply their model to a set of synthetic question and answering tasks [15], constructed to encompass many forms of reasoning including many that cannot be performed by current models such as recurrent neural nets (RNNs).", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "In this paper, we explore a model, similar to Memory Networks [16], that is able to learn with weak supervision, i.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "We apply our model to the bAbI tasks [15], allowing for a direct comparison of our model to the fully supervised Memory Networks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "For a more complete description of them, please refer to [15].", "startOffset": 57, "endOffset": 61}, {"referenceID": 5, "context": "Other recently proposed forms of memory or attention take this approach, notably [6] and [2].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Other recently proposed forms of memory or attention take this approach, notably [6] and [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 14, "context": "Learning time invariance by injecting random noise: The original memory network model in [16] used a relative representation of time involving triples of memories.", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 4, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 8, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 6, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 0, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 8, "context": "In practice, the performance gains over carefully trained RNNs are modest [10].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "[13] and Taylor [14] considered a memory that performed nearest-neighbor operations on stored input vectors and then fit parametric models to the retrieved sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] and Taylor [14] considered a memory that performed nearest-neighbor operations on stored input vectors and then fit parametric models to the retrieved sets.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "Subsequent work in the 1990\u2019s explored other types of memory [12, 4, 11].", "startOffset": 61, "endOffset": 72}, {"referenceID": 3, "context": "Subsequent work in the 1990\u2019s explored other types of memory [12, 4, 11].", "startOffset": 61, "endOffset": 72}, {"referenceID": 9, "context": "Subsequent work in the 1990\u2019s explored other types of memory [12, 4, 11].", "startOffset": 61, "endOffset": 72}, {"referenceID": 3, "context": "[4, 11] introduced an explicit stack with push and pop operations which has been revisited recently by Joulin et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[4, 11] introduced an explicit stack with push and pop operations which has been revisited recently by Joulin et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[8] in the context of an RNN model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "However, as demonstrated by [8], stack-based memories are less well suited to the bAbI tasks where out of order retrieval is needed for some of them.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Closely related to our model is the Neural Turing Machine of [6], which also uses a continuous memory representation and can be trained without explicit supervision using reinforcement learning.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "Our model is also related to [2].", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "Our \u201cmemory\u201d is analogous to their attention mechanism, both using weak supervision during training, although [2] is only over a single sentence rather than many, as in our case.", "startOffset": 110, "endOffset": 113}, {"referenceID": 13, "context": "The model is evaluated on the Babi question & answer dataset, proposed in [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "\u2022 MemNN: The fully supervised AM+NG+NL Memory Networks approach, proposed in [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "In this work we presented an architecture for solving the tasks in [15] with no supervision of supporting facts.", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "Furthermore, as in [6], it may be possible to replace backpropagation with some form of reinforcement learning.", "startOffset": 19, "endOffset": 22}], "year": 2015, "abstractText": "In this paper we introduce a variant of Memory Networks [16] that needs significantly less supervision to perform question and answering tasks. The original model requires that the sentences supporting the answer be explicitly indicated during training. In contrast, our approach only requires the answer to the question during training. We apply the model to the synthetic bAbI tasks, showing that our approach is competitive with the supervised approach, particularly when trained on a sufficiently large amount of data. Furthermore, it decisively beats other weakly supervised approaches based on LSTMs. The approach is quite general and can potentially be applied to many other tasks that require capturing longterm dependencies.", "creator": "LaTeX with hyperref package"}}}