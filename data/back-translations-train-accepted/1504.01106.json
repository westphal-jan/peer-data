{"id": "1504.01106", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2015", "title": "Discriminative Neural Sentence Modeling by Tree-Based Convolution", "abstract": "This paper proposes a new convolutional neural architecture based on tree-structures, called the tree-based convolutional neural network (TBCNN). Two variants take advantage of constituency trees and dependency trees, respectively, to model sentences. Compared with traditional \"flat\" convolutional neural networks (CNNs), TBCNNs explore explicitly the structural information of sentences; compared with recursive neural networks, TBCNNs have much shorter propagation paths, enabling more effective feature learning and extraction. In the experiment of sentiment analysis, our two models consistently outperform CNNs and RNNs in a controlled setting. Our models are also competitive to most state-of-the-art results, including RNNs based on long short term memory and deep CNNs/RNNs.", "histories": [["v1", "Sun, 5 Apr 2015 10:18:32 GMT  (289kb,D)", "http://arxiv.org/abs/1504.01106v1", null], ["v2", "Tue, 7 Apr 2015 07:30:08 GMT  (289kb,D)", "http://arxiv.org/abs/1504.01106v2", null], ["v3", "Thu, 23 Apr 2015 17:16:32 GMT  (289kb,D)", "http://arxiv.org/abs/1504.01106v3", null], ["v4", "Mon, 1 Jun 2015 12:23:16 GMT  (409kb,D)", "http://arxiv.org/abs/1504.01106v4", null], ["v5", "Tue, 2 Jun 2015 05:56:06 GMT  (409kb,D)", "http://arxiv.org/abs/1504.01106v5", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["lili mou", "hao peng", "ge li", "yan xu", "lu zhang 0023", "zhi jin"], "accepted": true, "id": "1504.01106"}, "pdf": {"name": "1504.01106.pdf", "metadata": {"source": "CRF", "title": "Tree-based Convolution: A New Architecture for Sentence Modeling", "authors": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "emails": ["zhijin}@sei.pku.edu.cn", "penghao.pku@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "(Bengio et al., 2003; Mikolov et al., 2013) proposed uncontrolled approaches to learn word embedding and integrate discrete words into real-world vectors and activate neural networks to capture word meanings. (Collobert et al., 2011) proposed uniform, revolutionary models for sentences that apply them to a variety of tasks, e.g., part-of-speech tagging, named entity recognition, semantic role labeling, etc. (Socher et al., 2011) proposed classes of recursive neural networks that exceed traditional machine-based algorithms by almost 10% in a sentiment analysis task. These authors also contribute to this paper. \u2020 Corresponding Authority aims to capture sentence meanings, particularly important for various overarching classification tasks of interest. In revolutionary models such as (Collobert et al, 2011; Blunsom et al, 2014)."}, {"heading": "2 Background and Related Work", "text": "In this section we present the background and related work on the neural architecture of sentence modeling."}, {"heading": "2.1 Convolutional Neural Networks", "text": "Convolutionary neural networks (CNNs), first proposed for image processing (LeCun et al., 1995), also work with natural languages. Figure 1 (a) represents the architecture of the classical convolution process on a sentence (Collobert and Weston, 2008). A fixed window, the so-called convolution kernel, slides over the sentence and outputs the extracted characteristics. Let's take the window size and x1, \u00b7 \u00b7 \u00b7, xt-Rn are n-dimensional column vectors corresponding to the words in the current window. Output of the convolution, evaluated at the current position, isy = f (W \u00b7 [x1; \u00b7; xt] + b), where y-Rm (m is the number of convolution nuclei), W-Rm \u00d7 tn, b-Rm are parameters, and f is the activation function. Semicolons represent column vector concatenation."}, {"heading": "2.2 Recursive Neural Networks", "text": "Recursive neural networks (RNNs) have been proposed in (Socher et al., 2011) to model sets. In the original version, each node in the tree structure has a distributed, real-valued representation. Let node p be the parents of c1 and c2 in a constituency tree processing whose vector is called p, c1, c2, Rn (n is the dimension of embedding). Then, the representation of parents by their children is composed according to the following equation (W [c1; c2] + b), where W [Rn \u00d7 2n] n and b [n] n] are used as model parameters. Such a process, represented in Figure 1, is performed recursively along the tree; the root vector is then used for supervised classification, e.g. sentiment analysis. Dependency trees and combinatorial categorical grammar can also be used as RNN structures."}, {"heading": "3 Tree-based Convolution", "text": "In this section, we propose the novel tree-based Convolutionary Neural Network (TBCNN) for sentence modeling. Figure 1 (c) describes the process of tree folding. The dashed triangle represents a folding core and extracts structural features along the tree. Subsequently, the extracted features are packed into a fixed-size vector by max pooling, allowing a short propagation path between the precipitation layer and any position in the tree. Tree-based folding, together with the pooling method, enables effective learning of structural features. Later in this section, we describe two variants of TBCNN based on stem trees (Section 3.1) and dependency trees (Section 3.2). Several heuristics are presented in Section 3.3; the training goal is presented in Section 3.4."}, {"heading": "3.1 c-TBCNN", "text": "Figure 2 illustrates an example of the constituency tree corresponding to the phrase \"I like it.\" (See the blue nodes on the left.) Leaf nodes in a constituency tree are words in a sentence; non-leaf nodes represent a grammatical component, e.g. a noun phrase, a verbal phrase, etc. In our experiment, sentences from Stanford parser1 are analyzed; furthermore, the constituency trees are binarized to effectively process the constituency tree by folding, both leaf nodes and non-leaf nodes should be represented as real-valued vectors in the same semantic space. In our c-TBCNN model, leaf nodes have pre-formed nedimensional embedding by non-superior algorithms (Mikolov et al, 2013); non-leaf nodes are encoded by Equation 1."}, {"heading": "3.2 d-TBCNN", "text": "Each node in dependency trees corresponds to a word; an edge a \u2192 b indicates that a is governed by b in Figure 2 (b). Edges are labeled by the parser with grammatical conversion types, e.g. nsubj, (de Marneffe et al., 2006).In our d-TBCNN model, we also apply double-layered conversion cores to extract structural characteristics. in d-TBCNN, however, different nodes may have different child numbers. We assign a weight matrix Wr to each conversion type r. Rare conversion types (less than 3,000 occurrences) are assigned to a common weight matrix. Let p (ci, \u00b7 \u00b7 \u00b7 \u00b7, cn) the words and relationships in the conversion core evaluate p (c) s \u00b2 s \u00b2 s \u00b2 s s s s s s s s s s s s s s s s s s s (d) p \u00b7 p + n c \u00b2 c c \u00b2 s c \u00b2 s c c \u00b2 s s s s s (c) s \u00b2 s (cn)."}, {"heading": "3.3 Pooling Heuristics", "text": "Since different sets may have different lengths and tree structures, the extracted features by tree folding also vary in size and shape. Dynamic pooling is a dominant technique to address this problem. We present several heuristics for pooling in both c-TBCNN and d-TBCNN. \u2022 Global pooling. All features are combined into one vector, and we take the maximum value in each dimension. This simple heuristics applies to each structure, including cTBCNN and d-TBCNN. \u2022 3-way pooling for c-TBCNN. To get more information about different regions of constituency trees, we suggest 3-way pooling. If a tree has the maximum depth d, we bundle nodes of less than \u03b1 \u00b7 d layers into a TOP vector (in our experiment it is set to 0.6)."}, {"heading": "3.4 Training the Networks", "text": "After pooling, the information is packed into one or more fixed-size vectors that can be forwarded to a fully connected, hidden layer. We then apply a softmax layer that predicts the probability of each label, i.e., p (y | x) satisfactory p (y | x) satisfactory c i = 1 p (yi | x) = 1, where x refers to an input sample and c is the number of classes to be forecasted. The cost function of a sample is the standard cross entropy error J = c \u00b2 i = 1ti log yiwhere t \u00b2 Rc is the soil truth, with the target label being a uniform vector. We do not apply \"2 regulation for weights or embedding temporarily, but suspenders are used, which to a certain extent helps prevent match. Slopes are calculated by standard background propagation; stochastic lowering of the minibatch version is used for optimization, which is discussed further in Hyperparametering section 4.3."}, {"heading": "4 Experimental Results", "text": "In Section 4.1 we first describe the task and the dataset used for our model evaluation. In Section 4.2 we compare c-TBCNN and d-TBCNN with other current results. Hyperparameter tuning is described in Section 4.3."}, {"heading": "4.1 The Task and the Dataset", "text": "Our models are evaluated against a widely used benchmark for sentence modeling, which consists of more than 10,000 film reviews; our task is to predict the mood of the reviews under 5 target markers (strong positive, positive, neutral, negative and strongly negative). We use the standard split for training, validation and testing, which includes 8544, 1101 and 2210 sentences, respectively. In the training set, each sentence and phrase is marked with its sentiment label.We consider phrases as individual samples. For validation and testing, we consider only the feeling of a complete sentence (the root label).Both c-TBCNN and d-TBCNN use the parsing results of the Stanford parser. Some sentences (or phrases) that have been parsed abnormally are discarded (less than 1%)."}, {"heading": "4.2 Accuracy Compared with other Methods", "text": "As we can see, both c-TBCNN and d-TBCNN consistently outperform RNNs of the same interaction by a multiple (5-6%); they also consistently outperform the \"flat\" folding by more than 10%. These results show in a controlled environment that our proposed tree-based folding of both variants can effectively capture tree structure information, which is critical for sentence modeling. In addition, our model also outperforms elaborately designed RNNNs with matrix-vector interaction, tensor interaction by 2.5-4.7%. For RNNs with long-term short-term memory (LSTM) and deep architectures of RNNNNs / CNNs, the results are in the same ballpark - our accuracy, for example, is higher than with an implementation of LSTM-based RNN, but slightly lower than with other 2 implementations."}, {"heading": "4.3 Hyperparameter Tuning", "text": "In this section, we present some details of our hyperparameter adjustment.3 Activation function. We have tried Tanh and ReLU (Nair and Hinton, 2010) as activation functions. We have found that ReLU contributes to optimization and achieves high accuracy in both training and validation sets. One possible reason for this is that ReLU is more extensive. As this research is still ongoing, the results can be updated. Derivatives are more tanh and therefore easier to train. Abort. We have tried two settings for abort: (1) Abort the last hidden layer and (2) Abort all hidden layers, including the conventive layers. We have found that abort helps to prevent overwork to a large extent in terms of long-term performance during training - the decrease in validation accuracy due to the overhaul is much smaller than non-abort networks. The result is conservative: setting is better (2) than abort (which is no)."}, {"heading": "4.4 Conclusion and Future Work", "text": "In this paper, we propose a novel tree-based Convolutionary Architecture (TBCNN) for sentence modeling, which can be built on either constituency trees or dependency trees, and which uses tree structures effectively and efficiently. In a controlled environment, our model greatly outperforms existing \"flat\" Convolutionary and Recursive Neural Networks. Our models are also competitive with most current results. While this paper introduces the new TBCNN model for the first time, we strengthen the model by designing more meaningful interactions, such as bilinear or tensor interactions, to model exclamations or negations. We also try to stack multiple layers of tree evolution to improve information integration."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "Lapata2012] William Blacoe", "M. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Blunsom et al.2014] Phil Blunsom", "E. Grefenstette", "N. Kalchbrenner"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Blunsom et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks", "author": ["Collobert", "Weston2008] Ronan Collobert", "J. Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["B. MacCartney", "C. Manning"], "venue": "In Proceedings of Language Resource and Evaluation Conference,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan et al.2009] Dumitru Erhan", "P. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "In Proceedings of International Conference on Artificial Intelligence", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Hermann", "Blunsom2013] Karl Moritz Hermann", "P. Blunsom"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "D. Lu", "H. Li", "Q. Chen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "C. Cardie"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "T. Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Le", "Zuidema2015] Phong Le", "W. Zuidema"], "venue": "arXiv preprint arXiv:1503.02510", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Comparison of learning algorithms for handwritten digit", "author": ["LeCun et al.1995] Yann LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E. Sackinger"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Efficient estimation of word representations in vector space", "author": ["K. Chen", "G. Corrado", "J. Dean"], "venue": "In ICLR Workshop", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "TBCNN: A tree-based convolutional neural network for programming language processing", "author": ["Mou et al.2014a] Lili Mou", "G. Li", "Z. Jin", "L. Zhang", "T. Wang"], "venue": "arXiv preprint arXiv:1409.5718", "citeRegEx": "Mou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2014}, {"title": "Building program vector representations for deep learning", "author": ["Mou et al.2014b] Lili Mou", "G. Li", "X. Liu", "H. Peng", "Z. Jin", "Y. Xu", "L. Zhang"], "venue": "arXiv preprint arXiv:1409.3358", "citeRegEx": "Mou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "G. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang et al.2015] Lifeng Shang", "D. Lu", "H. Li"], "venue": "arXiv preprint arXiv:1503.02364", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["J. Pennington", "E. Huang", "A. Ng", "C. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["B. Huval", "C. Manning", "A. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Q. Le", "C. Manning", "A. Ng"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "In Proceedings of Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "R. Socher", "D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Long short-term memory over tree structures. arXiv preprint arXiv:1503.04881", "author": ["Zhu et al.2015] Xiaodan Zhu", "P. Sobhani", "Y. Guo"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "(Bengio et al., 2003; Mikolov et al., 2013) proposed unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors and enabling neural networks to capture word meanings.", "startOffset": 0, "endOffset": 43}, {"referenceID": 17, "context": "(Bengio et al., 2003; Mikolov et al., 2013) proposed unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors and enabling neural networks to capture word meanings.", "startOffset": 0, "endOffset": 43}, {"referenceID": 22, "context": "(Socher et al., 2011) proposed a class of recursive neural networks, outperforming traditional machine leaning algorithms by nearly 10% in a sentiment analysis task.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "In convolutional models like (Collobert et al., 2011; Blunsom et al., 2014; Hu et al., 2014), a fixed-size window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector\u2014usually taking the maximum value in each dimension\u2014for supervised learning.", "startOffset": 29, "endOffset": 92}, {"referenceID": 3, "context": "In convolutional models like (Collobert et al., 2011; Blunsom et al., 2014; Hu et al., 2014), a fixed-size window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector\u2014usually taking the maximum value in each dimension\u2014for supervised learning.", "startOffset": 29, "endOffset": 92}, {"referenceID": 11, "context": "In convolutional models like (Collobert et al., 2011; Blunsom et al., 2014; Hu et al., 2014), a fixed-size window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector\u2014usually taking the maximum value in each dimension\u2014for supervised learning.", "startOffset": 29, "endOffset": 92}, {"referenceID": 22, "context": "Recursive neural networks (RNNs), on the other hand, take advantage of the sentence\u2019s parsing trees (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013).", "startOffset": 100, "endOffset": 170}, {"referenceID": 8, "context": "Although RNNs encode prior knowledge on tree structures into networks to some extent, they may have difficulty in learning deep dependencies because of long propagation paths (Erhan et al., 2009).", "startOffset": 175, "endOffset": 195}, {"referenceID": 27, "context": "Currently, our accuracy is slightly lower than the most state-of-theart results by a deep RNN in (Irsoy and Cardie, 2014), two implementations of LSTM RNN (Tai et al., 2015; Le and Zuidema, 2015).", "startOffset": 155, "endOffset": 195}, {"referenceID": 16, "context": "Convolutional neural networks (CNNs), first proposed for image processing (LeCun et al., 1995), turn out to work with natural languages as well.", "startOffset": 74, "endOffset": 94}, {"referenceID": 3, "context": "(Blunsom et al., 2014) builds deep convolutional models so that local features can mix at high-level layers.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "Similar deep CNN architectures include (Kim, 2014; Hu et al., 2014).", "startOffset": 39, "endOffset": 67}, {"referenceID": 11, "context": "Similar deep CNN architectures include (Kim, 2014; Hu et al., 2014).", "startOffset": 39, "endOffset": 67}, {"referenceID": 13, "context": "As evidence in the literature shows, CNNs are particularly suitable for noisy and short texts, such as twitter or microblogs (Kim, 2014).", "startOffset": 125, "endOffset": 136}, {"referenceID": 22, "context": "Recursive neural networks (RNNs) were proposed in (Socher et al., 2011) to model sentences.", "startOffset": 50, "endOffset": 71}, {"referenceID": 23, "context": "Improvement for semantic compositionality in RNNs include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 27, "context": "Long short term memory (LSTM), first proposed for modeling temporal data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).", "startOffset": 157, "endOffset": 215}, {"referenceID": 28, "context": "Long short term memory (LSTM), first proposed for modeling temporal data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).", "startOffset": 157, "endOffset": 215}, {"referenceID": 0, "context": "A degraded variant of RNNs is the recurrent network (Bengio et al., 1994), whose architecture can be viewed as a right-most tree.", "startOffset": 52, "endOffset": 73}, {"referenceID": 21, "context": "As meaningful tree structures are not used, they are also particularly suitable for modeling short or noisy texts, similar to CNNs (Shang et al., 2015).", "startOffset": 131, "endOffset": 151}, {"referenceID": 17, "context": "In our c-TBCNN model, leaf nodes have pretrained ne-dimensional embeddings by unsupervised algorithms (Mikolov et al., 2013); non-leaf nodes are coded by Equation 1, similar to the RNN in (Socher et al.", "startOffset": 102, "endOffset": 124}, {"referenceID": 22, "context": ", 2013); non-leaf nodes are coded by Equation 1, similar to the RNN in (Socher et al., 2011).", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "4 (Blunsom et al., 2014) Deep CNN 48.", "startOffset": 2, "endOffset": 24}, {"referenceID": 3, "context": "5 (Blunsom et al., 2014)", "startOffset": 2, "endOffset": 24}, {"referenceID": 28, "context": "0 (Zhu et al., 2015) LSTM 49.", "startOffset": 2, "endOffset": 20}, {"referenceID": 27, "context": "6 (Tai et al., 2015) Deep RNN 49.", "startOffset": 2, "endOffset": 20}, {"referenceID": 26, "context": "For the sake of robustness, we adopt the dropout technique (Srivastava et al., 2014) with dropout probability 0.", "startOffset": 59, "endOffset": 84}, {"referenceID": 7, "context": "We tried AdaGrad (Duchi et al., 2011) for training.", "startOffset": 17, "endOffset": 37}], "year": 2017, "abstractText": "This paper proposes a new convolutional neural architecture based on treestructures, called the tree-based convolutional neural network (TBCNN). Two variants take advantage of constituency trees and dependency trees, respectively, to model sentences. Compared with traditional \u201cflat\u201d convolutional neural networks (CNNs), TBCNNs explore explicitly the structural information of sentences; compared with recursive neural networks, TBCNNs have much shorter propagation paths, enabling more effective feature learning and extraction. In the experiment of sentiment analysis, our two models consistently outperform CNNs and RNNs in a controlled setting. Our models are also competitive to most stateof-the-art results, including RNNs based on long short term memory and deep CNNs/RNNs.", "creator": "LaTeX with hyperref package"}}}