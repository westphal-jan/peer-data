{"id": "1504.01255", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2015", "title": "Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding", "abstract": "This paper presents a theoretical analysis of multi-view embedding -- feature embedding that can be learned from unlabeled data through the task of predicting one view from another. We prove its usefulness in supervised learning under certain conditions. The result explains the effectiveness of some existing methods such as word embedding. Based on this theory, we propose a new semi-supervised learning framework that learns a multi-view embedding of small text regions with convolutional neural networks. The method derived from this framework outperforms state-of-the-art methods on sentiment classification and topic categorization.", "histories": [["v1", "Mon, 6 Apr 2015 10:42:07 GMT  (251kb,D)", "http://arxiv.org/abs/1504.01255v1", null], ["v2", "Thu, 24 Sep 2015 11:32:44 GMT  (184kb,D)", "http://arxiv.org/abs/1504.01255v2", "The older version has a different title, and the results there are obsolete. The current version is to appear in NIPS 2015"], ["v3", "Sun, 1 Nov 2015 15:26:16 GMT  (184kb,D)", "http://arxiv.org/abs/1504.01255v3", "v1 has a different title, and the results there are obsolete. The current version is to appear in NIPS 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["rie johnson", "tong zhang 0001"], "accepted": true, "id": "1504.01255"}, "pdf": {"name": "1504.01255.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Learning with Multi-View Embedding: Theory and Application with Convolutional Neural Networks", "authors": ["Rie Johnson", "Tong Zhang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are in a position to put themselves in the world, to put themselves in a world in which they give themselves and others a chance, in which they give themselves and others a chance, in which they give themselves and others a chance, in which they give freedom, in which they live freedom, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live in which they, in which they, in which they, in which they, in which they, live in which they, in which they, in which they, in which they, in which they, in which they, in which they, live in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in fact, in which they, in fact, live, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in fact, in which they, in which they, live, live, in which they, in which they, in which they, live, in which they, live, in which they, in which they, in"}, {"heading": "2 Multi-View Embedding", "text": "In this section, we first present a theoretical analysis of multi-view embedding and then apply it to some existing methods."}, {"heading": "2.1 Theory of multi-view embedding", "text": "Let's XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"}, {"heading": "2.2 Case study of multi-view embedding", "text": "We focus on word2vec [18, 19] because of its popularity. To see the equivalent of embedding learning processes, we consider a token tagging task such as POS tagging. The function mappings would be useful for this task, the current word w with a word vector representing w as characteristics. The equivalent to embedding learning processes, we consider a token tagging task such as POS tagging. The function mappings uses the current word (view-1: X1) and its context (view-2: X2)."}, {"heading": "3 Multi-View Learning for CNN for Text", "text": "Based on our theoretical analysis, we propose a semi-supervised learning framework for CNN. As we will see below, CNN learns internally to embed small text regions into low-dimensional continuous vectors. This, together with the fact that it contains natural definitions of views that are integrated into the model structure, makes CNN particularly suitable for multi-dimensional embedding of learning content. CNN, originally developed for images, is a kind of neural network that can take advantage of the internal structure of data such as word order. An approach to adapt CNN from images (low-dimensional density data) to high-dimensional text data is to convert words first into word vectors before feeding them to CNN [7, 12, 11]. If word vectors are learned from large unlabeled data through multi-dimensional learning, this could be a very effective semi-supervised method, even though they have not been tested on text categorizations. 3 A newer approach of [10] consists of applying directly to a multidimensional view that leads CNN to a highly unified word."}, {"heading": "3.1 One-hot CNN for text [10]", "text": "As illustrated in Figure 1, a folding layer consists of computational units, each of which responds to a small region of input (e.g. tokens in a size 2 window), and the small regions collectively cover the entire data.3 This type of CNN was used for word sequence tagging tasks and classification of short sentences, but not for categorization of full-length documents in general. (e.g. a document) It is these small regions that we later consider to be views. A computing unit associated with the \"-th region of input x\" computes tasks and classification of short sentences, but not for categorization of whole text in general. (e.g. a document) It is these small regions that we later consider as views. A computing unit associated with the \"-th region of input x\" computes the voctors: \u03c3 (W \u00b7 r '( x) + b), (1) where r '( x), Rq is the region vector, the \"-th region\" we represent the \"-th\" -region."}, {"heading": "3.2 Multi-view semi-supervised CNN for text", "text": "The semi-supervised framework we propose consists of the following two steps: 1. Unsupervised Learning: Consider small regions of the folding layer of B as views. Define a task to predict neighboring regions from each region. Train CNN U for this task using unlabeled data. 2. Supervised Learning: Integrate the folding layer of U, which serves as a multi-view embedding, in B. Train this final model with labeled data of the intended task. These two steps are described in the next two sections."}, {"heading": "3.2.1 Unsupervised learning of multi-view embedding", "text": "We look at the small regions of B's folding layer as views and generate a task to predict adjacent regions from each region. As illustrated in Figure 2, we learn this task from CNN U with a folding layer. Faced with document x, the folding layer of U calculates for each text region indexed by 'u' (x) = \u03c3 (U) (W (U) \u00b7 r (U) '(x) + b (U)))), (4) which is the same as (1) except the high sentence \"(U)\" to indicate that these units belong to U. The top layer uses u' (U) as characteristics to predict the adjacent regions. W (U) and b (U) (and the top layer parameters) are learned through training. It is this folding layer that we transfer to supervised learning in the next level."}, {"heading": "3.2.2 Final supervised learning", "text": "The first option is to replace B's folding layer with U's, similar to pre-training in the neural network literature. The second option is the add-on that uses the output of U's folding layer as additional features. We do this by using the result of multi-view embedding in the \"-th region\" (x) + b, \"where u '( x) is defined by (4), i.e., the result of multi-view embedding in the\" -th region. \"We train this model with the described data of the task; that is, we update W's weights, bias, and bias in the top layer."}, {"heading": "3.3 Pre-trained word vector-based CNN", "text": "As mentioned above, one approach to adapting CNN to text is to convert words into word vectors first. However, note that the word vector conversion layer can also be considered a special folding layer with region sizes 1 and \u03c3 (x) = x, which takes unified vectors as input. In this interpretation, the word vector CNN can be considered a single CNN with two folding layers, as shown in Figure 3. Note that applying multiview embedding learning to the first folding layer (i.e. the word vector conversion layer) would then lead to the unattended task of predicting surrounding words 4 from each word. 4 Note that the example in Figure 2 uses only one adequate region on each page as a target, but more than one adequate region (in this case multiple words) can also be used."}, {"heading": "4 Experiments", "text": "We report on experiments for text categorization. Our code and the settings for reproducing the experiments are published on the Internet."}, {"heading": "4.1 Tasks and data", "text": "We used three sets of data used in [10]: IMDB, Elec, and RCV1. IMDB5 is a set of movie reviews, and Elec6 consists of Amazon Electronic Reviews [16]. The task associated with IMDB and Elec is the mood classification to assign positive / negative to each review. The task we tested on RCV17 was single-label categorization with the 55 second-level topics, using the same training and test sets as in Table 2 of [10]. Unchecked data IMDB comes with a set of unresolved 50K reviews. To facilitate comparison with [13], we used this set and the training set as unresolved data. For Elec, we chose 200K reviews from the same data source, so that they are separated from the training and test sets, and that their reviewed products are detached from the test set. 9 The uncleared set that we are using on the Internet."}, {"heading": "4.2 CNN with multi-view embedding", "text": "We experimented with semi-monitored CNN with multi-view embedding (short: mvCNN) of the following two types. \u2022 mvCNNo: It takes a hot CNN (Fig.1) with region size > 1 as base model and learns a region embedding from unlabeled data. \u2022 mvCNNw: It takes word vector CNN (Fig.3) as base model and learns word embedding from unlabeled data. 5 http: / / ai.stanford.edu / \u02dc amaas / data / sentiments / 6 http: / / riejohnson.com / cnn _ data.html 7 http: / / trec.nist.gov / data / reuters / reuters.html 8 Defined at http: / / riejohnson.com / cnn _ data.html. 9 This makes the task more difficult and realistic considering that new products come out after training."}, {"heading": "4.2.1 Implementation of mvCNN", "text": "Unsupervised training minimized the weighted square loss \u2211 i, j \u03b1i, j (zi [j] \u2212 pi [j]) 2, where i goes through the data points, z represents the adjacent regions (predictive destination), and p is the model output. (Remember that the goal of unsupervised training is to predict adjacent regions from each region.) Although there are several ways to encode the target regions, in our experiments we simply set z as a concatenation of two arc vectors of adjacent regions on the left and right, while we kept only the 30K most common words of unlabeled vocabulary lists with vocabulary control (see below). To this end, we used one region on each side for embedding two arc vectors of adjacent regions on the left and 5 words on each side for embedding words. The weights \u03b1j, we specified in the training, to exclude the loss of relevant words from each of the theory, and to assume that each of the two areas are related to the other."}, {"heading": "4.3 Model selection", "text": "It is important that in all tested methods the adjustment of the meta-parameters (i.e. the regularization parameters) took place by testing the models on the reserved part of the training data and then retraining the models with the selected meta-parameters using the entire training data."}, {"heading": "4.4 Results", "text": "The error rate is confirmed in Table 2 (except for CCCV1) significantly higher than the best performing effectiveness of these reports. All CNN models were forced to have exactly one conversion layer (except for the word vector level, if any) with 1000 neurons. Indeed, the supervised baselines are the most powerful CNN models within these constraints, which are a single CNNC selection (CNNC size 3; a max pooling unit) on IMDB and Elec and one-hot bow CNN format (region size 20; 10) CNNC words, the CNNC type of more complex CNN models from [10] later in Tables 3 and 4. With mvCNN, the dimensionality of multi-view embedding was set to 100. Performances with embedding of different dimensions are highlighted in Figure 4.The first thing highlighted from Table 2 is that on all three datasets, mvCNN models except for CNCV1 are significantly better mapped to CNVC models."}, {"heading": "4.5 Examples of predictive text regions", "text": "To get an insight into what has been learned from unlabeled data, we show the text regions (from the IMDB training set) that contribute most to activating the influential neurons (i.e., the neurons 11 http: / / svmlight.joachims.org / 12). Note that for feasibility, we only used the 30K most common n-grams in the TSVM experiments, i.e. the SVM results are also compared with 30K vocabulary, although SVM performance can be improved for some data sets by using all n-grams (e.g. 5 million n-grams on IMDB). This is because the computing costs of TSVM (single-core) have proven to be high, with even 30K vocabularies taking several days.13 Two types of visual differences have been tested: random splitting of vocabularies and splitting into the first and last half of each document. \""}, {"heading": "5 Conclusion", "text": "This paper presented a theoretical analysis of multi-view learning. The result explained the effectiveness of some existing word embedding methods and led to a new and more general semi-supervised learning framework that learns multi-view embedding of small text regions (not limited to a single word) with CNN for use in supervised CNN. Experimental results showed the effectiveness of this approach."}, {"heading": "Appendix A Proof of Theorem 1", "text": "Let us first prove that X1 contains d1 elements, and X2 contains d2 elements, and | H (= k) is of rank k (1), if we consider P (1) as a d2 \u00b7 d1 matrix (which we call A), then we can consider P (X2 | h) as a d2 \u00b7 k matrix (which we call B), and P (h) as a k \u00b7 d1 matrix (which we call C), and from the matrix equation A = BC we can obtain C = (B > B) \u2212 1B > A. Let us consider the k \u00b7 d2 matrix U = (B > B) \u2212 X2 h, then we know that its elements correspond to a function (h)."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Rie K. Ando", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Two-view feature generation model for semi-supervised learning", "author": ["Rie K. Ando", "Tong Zhang"], "venue": "In Proceedings of ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Marchine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["Avrim Blum", "Tom Mitchell"], "venue": "In Proceedings of COLT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "editors"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Transductive inference for text classification using support vector machines", "author": ["Thorsten Joachims"], "venue": "In Proceedings of ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A convolutional neural network for modeling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP, pages 1746\u20131751,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "Journal of Marchine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Hidden factors and hidden topics: Understanding rating dimensions with review text", "author": ["Julian McAuley", "Jure Leskovec"], "venue": "In RecSys,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Gr\u00e9goire Mesnil", "Tomas Mikolov", "Marc\u2019Aurelio Ranzato", "Yoshua Bengio"], "venue": "Feb 2015 version),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of Wordshop at ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Rainov", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D. Manning"], "venue": "In Proceedings of ACL (short paper),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Semi-supervised learning literature survey", "author": ["Xiaojin Zhu"], "venue": "Technical Report 1530,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "Among a number of semi-supervised methods that have been proposed [5, 23], there are two notable types of method.", "startOffset": 66, "endOffset": 73}, {"referenceID": 22, "context": "Among a number of semi-supervised methods that have been proposed [5, 23], there are two notable types of method.", "startOffset": 66, "endOffset": 73}, {"referenceID": 3, "context": ", EM, co-training [4], and transductive SVM [9].", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": ", EM, co-training [4], and transductive SVM [9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "In NLP, an empirically successful example is word embedding learned from unlabeled data through the tasks that essentially predict the neighboring words from words [3, 6, 20, 19].", "startOffset": 164, "endOffset": 178}, {"referenceID": 5, "context": "In NLP, an empirically successful example is word embedding learned from unlabeled data through the tasks that essentially predict the neighboring words from words [3, 6, 20, 19].", "startOffset": 164, "endOffset": 178}, {"referenceID": 19, "context": "In NLP, an empirically successful example is word embedding learned from unlabeled data through the tasks that essentially predict the neighboring words from words [3, 6, 20, 19].", "startOffset": 164, "endOffset": 178}, {"referenceID": 18, "context": "In NLP, an empirically successful example is word embedding learned from unlabeled data through the tasks that essentially predict the neighboring words from words [3, 6, 20, 19].", "startOffset": 164, "endOffset": 178}, {"referenceID": 20, "context": "When used as additional features, the obtained word vectors often improve the performance of supervised NLP systems [21].", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "Theoretically, this approach is related to two-view feature learning by [2], which originated from alternating structure optimization [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "Theoretically, this approach is related to two-view feature learning by [2], which originated from alternating structure optimization [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 13, "context": "[14] has shown that", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "one instance of word2vec [18] is implicit factorization of a word-context matrix of shifted PMI; however, [14] did not show why factorization of this matrix would be useful.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "one instance of word2vec [18] is implicit factorization of a word-context matrix of shifted PMI; however, [14] did not show why factorization of this matrix would be useful.", "startOffset": 106, "endOffset": 110}, {"referenceID": 1, "context": "[2] has given theoretical justification of two-view feature learning, but this analysis is limited to the linear case where SVD is used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "This analysis can be regarded as a generalization of [2] to non-linear cases, allowing it to handle neural network-based word embeddings and its extensions to text region embedding investigated in this work.", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "Furthermore, a recent study [10] has shown that CNN exceeds state-of-the-art supervised performances on text categorization due to its ability to directly embed small text regions into vectors in the supervised setting.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": "Like [2], we assume conditional independence of two views, but we relax1 it as follows.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "1 [2] assumed conditional independence of X1 and X2 given label Y .", "startOffset": 2, "endOffset": 5}, {"referenceID": 17, "context": "We focus on word2vec [18, 19] due to its popularity.", "startOffset": 21, "endOffset": 29}, {"referenceID": 18, "context": "We focus on word2vec [18, 19] due to its popularity.", "startOffset": 21, "endOffset": 29}, {"referenceID": 17, "context": "This might be the reason that cbow generally underperformed skip-gram in [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Paragraph vectors [13] proposed paragraph vectors (p-vec), which represent variable-sized text by lowdimensional vectors.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "One approach to adapting CNN from image (low-dimensional dense data) to high-dimensional text data is to first convert words into word vectors before feeding them to CNN [7, 12, 11].", "startOffset": 170, "endOffset": 181}, {"referenceID": 11, "context": "One approach to adapting CNN from image (low-dimensional dense data) to high-dimensional text data is to first convert words into word vectors before feeding them to CNN [7, 12, 11].", "startOffset": 170, "endOffset": 181}, {"referenceID": 10, "context": "One approach to adapting CNN from image (low-dimensional dense data) to high-dimensional text data is to first convert words into word vectors before feeding them to CNN [7, 12, 11].", "startOffset": 170, "endOffset": 181}, {"referenceID": 9, "context": "A more recent approach by [10] is to apply CNN directly to high-dimensional one-hot vectors, resulting in performances superior to other supervised methods on text categorization.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "In this section, we first focus on one-hot CNN of [10] to explore application of multi-view embedding learning and then later show that the proposed framework subsumes the CNN with pre-trained word vectors as well.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "1 One-hot CNN for text [10] CNN is a feed-forward network equipped with convolution layers interleaved with pooling layers.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "I it love r`(x) =[ 0 1 1 ] > (3)", "startOffset": 17, "endOffset": 26}, {"referenceID": 0, "context": "I it love r`(x) =[ 0 1 1 ] > (3)", "startOffset": 17, "endOffset": 26}, {"referenceID": 9, "context": "Here we focused on the convolution layer due to its relevance to our work; for other details, [10] should be consulted.", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "the model parameters have a smaller 2-norm) as it only needs to complement the predictor using the new features (which is simple due to low dimensionality), as analyzed in [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 9, "context": "On RCV1, we used the same training and test sets as in Table 2 of [10].", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "1 Tasks and data We used three datasets used in [10]: IMDB, Elec, and RCV1.", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "IMDB5 is a dataset of movie reviews, and Elec6 consists of Amazon electronics product reviews [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "The task we tested on RCV17 was singlelabel categorization with the 55 second-level topics, using the same training and test sets8 as in Table 2 of [10].", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "To facilitate comparison with [13], we used this set and the training set as unlabeled data.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "On RCV1, instead of a small list of function words, we used the stop-word list provided by [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "The rest basically follows [10].", "startOffset": 27, "endOffset": 31}, {"referenceID": 7, "context": "Supervised training was done with square loss and L2 regularization, and dropout [8] was optionally applied to the input of the top layer.", "startOffset": 81, "endOffset": 84}, {"referenceID": 9, "context": "The supervised baselines are the best-performing CNN within these constraints, which are one-hot seq-CNN (region size 3; one max-pooling unit) on IMDB and Elec and one-hot bow-CNN (region size 20; 10 average-pooling units) on RCV1, as in [10]10; we will review the performance of more complex CNN from [10] later in Tables 3 and 4.", "startOffset": 238, "endOffset": 242}, {"referenceID": 9, "context": "The supervised baselines are the best-performing CNN within these constraints, which are one-hot seq-CNN (region size 3; one max-pooling unit) on IMDB and Elec and one-hot bow-CNN (region size 20; 10 average-pooling units) on RCV1, as in [10]10; we will review the performance of more complex CNN from [10] later in Tables 3 and 4.", "startOffset": 302, "endOffset": 306}, {"referenceID": 9, "context": "This confirms the effectiveness of the 10 [10] reports 9.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "The difference is due to the use ([10]) and unuse (this work) of the stopword list.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "IMDB Elec RCV1 SVM 1-3grams (all) [10] 9.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "69 SVM 1-3grams (30K) [10] 10.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": "NB+SVM 1-2grams [22] 8.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "78 Ensemble NB-LM 1-3grams [17] 8.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "13 \u2013 seq2-CNN (1K\u00d72) [10] 8.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "04 \u2013 seq2-CNN (3K\u00d72) [10] 7.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "94 \u2013 seq2-bown-CNN [10] 7.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "67 \u2013 Paragraph vectors [13] 7.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "43 Ensemble+ + NB-LM [17] Unlabeled data mvCNNo&w [Ours] 6.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "SVM 1-3grams [10] 8.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "71 \u2013 NB-LM 1-3grams [10] 8.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "11 \u2013 seq2-CNN [10] 7.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "48 \u2013 seq2-bown-CNN [10] 7.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "46 by paragraph vectors [13], which used the same unlabeled data we used.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "[17] produced 7.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "models micro-F macro-F extra resource SVM [15] 81.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "7 \u2013 CNN [10] 84.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "RCV1: previous results To compare with the benchmark results in [15, 10], we tested mvCNNo on the multi-label task with the LYRL04 split [15] on RCV1, in which more than one out of 103 categories can be assigned to each document.", "startOffset": 64, "endOffset": 72}, {"referenceID": 9, "context": "RCV1: previous results To compare with the benchmark results in [15, 10], we tested mvCNNo on the multi-label task with the LYRL04 split [15] on RCV1, in which more than one out of 103 categories can be assigned to each document.", "startOffset": 64, "endOffset": 72}, {"referenceID": 14, "context": "RCV1: previous results To compare with the benchmark results in [15, 10], we tested mvCNNo on the multi-label task with the LYRL04 split [15] on RCV1, in which more than one out of 103 categories can be assigned to each document.", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "For this experiment, we used the stopword list as in [15, 10] so that the results are directly comparable.", "startOffset": 53, "endOffset": 61}, {"referenceID": 9, "context": "For this experiment, we used the stopword list as in [15, 10] so that the results are directly comparable.", "startOffset": 53, "endOffset": 61}, {"referenceID": 14, "context": "As shown in Table 5, mvCNNo outperforms the best SVM of [15] and the best CNN of [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "As shown in Table 5, mvCNNo outperforms the best SVM of [15] and the best CNN of [10].", "startOffset": 81, "endOffset": 85}], "year": 2016, "abstractText": "This paper presents a theoretical analysis of multi-view embedding \u2013 feature embedding that can be learned from unlabeled data through the task of predicting one view from another. We prove its usefulness in supervised learning under certain conditions. The result explains the effectiveness of some existing methods such as word embedding. Based on this theory, we propose a new semi-supervised learning framework that learns a multi-view embedding of small text regions with convolutional neural networks. The method derived from this framework outperforms state-of-the-art methods on sentiment classification and topic categorization.", "creator": "LaTeX with hyperref package"}}}