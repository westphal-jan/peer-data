{"id": "1504.01365", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2015", "title": "PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent", "abstract": "Stochastic Dual Coordinate Descent (SDCD) has become one of the most efficient ways to solve the family of $\\ell_2$-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of SDCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the SDCD algorithms in LIBLINEAR. In recent research, several synchronized parallel SDCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of asynchronous stochastic dual coordinate descent algorithms (ASDCD). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present the first {\\it backward error analysis} for ASDCD under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers.", "histories": [["v1", "Mon, 6 Apr 2015 19:25:47 GMT  (3084kb,D)", "http://arxiv.org/abs/1504.01365v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cho-jui hsieh", "hsiang-fu yu", "inderjit s dhillon"], "accepted": true, "id": "1504.01365"}, "pdf": {"name": "1504.01365.pdf", "metadata": {"source": "CRF", "title": "PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent", "authors": ["Cho-Jui Hsieh", "Hsiang-Fu Yu", "Inderjit S. Dhillon"], "emails": ["cjhsieh@cs.utexas.edu", "rofuyu@cs.utexas.edu", "inderjit@cs.utexas.edu"], "sections": [{"heading": null, "text": "Family of \"2-regulated empirical risk mitigation problems, including linear SVM, logistic regression, and many others. Vanilla implementation of DCD is quite slow; maintaining primary variables while updating dual variables can significantly reduce the time complexity of DCD. Such a strategy forms the core algorithm in the widely used LIBLINEAR package. In this paper, we propose parallels to the DCD algorithms in LIBLINEAR. Recent research has suggested several synchronized parallel DCD algorithms, but they do not achieve good acceleration in the multi-core environment of shared memory. In this paper, we propose a family of asynchronous stochastic dual coordinate descension algorithms (PASSCoDe). Each thread repeatedly selects a random dual variable and performs coordinate updates using primary variables that are applied to the shared memory when we analyze the different block properties."}, {"heading": "1 Introduction", "text": "Considering a number of instances couple (x-i, y-i problem), i = 1, \u00b7 n, x-i-i, y-i-R, we focus on the following empirical risk mitigation problems with \"2-regulatory problems: min w-RdP (w): = 12, w-2, h-2, h-2, h-2, c-3, c-3, c-2, c-2, b-2, b-2, c-2, c-3, c-3, c-3, c-3, c-c-3, c-c c-c, c-c-3, c-c-3, c-2, c-3, c-c-c, c-3, c-3, c-c-3, c-c-c-c, c-c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-4, 4-4, 4, 4-4, 4-4, 4, 4-4, c-4, 4, 4-4, c, 4-4, c, c-4, c-4, c-4, c-4, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3, c-3,"}, {"heading": "2 Related Work", "text": "It is a classical optimization technology that has long been studied (Bertsekas, 1999; Luo & Tseng, 1992); in terms of theoretical analysis, convergence of (cyclical) coordinate lineage has long been studied (Luo & Tseng, 1992; Bertsekas, 1999); and global linear convergence has recently been presented under certain conditions (Saha & Tewari, 2013; Wang & Lin, 2014).Stochastic dual lineage has long been studied (Hsieh et al, 2008; Yu et al, 2011; ShalevShwartz & Zhang, 2013)."}, {"heading": "3 Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Stochastic Dual Coordinate Descent", "text": "We first describe the stochastic dual coordinate descent (DCD) algorithm for solving the dual problem (2). At each iteration, DCD randomly selects a dual variable \u03b1i and updates it by minimizing the one variable sub-problem (Eq. (4) in algorithm 1. Without exploiting the structure of the square term, the sub-problem requires a considerable calculation (necessity O (nnz) time), where nnz is the total number of non-zero elements in the training data. However, ifw (\u03b1), which satisfies (3), can be maintained in memory, the sub-problem D (\u03b1 + \u03b4ei) can be written asD (necessity O (nnz) time, where nnz is the total number of non-zero elements in the training data."}, {"heading": "3.2 Asynchronous Stochastic Dual Coordinate Descent", "text": "The details are used in Algorithm 2.Algorithm 3.2 Parallel Asynchronous Stochastic dual Co-ordinate Descent (PASSCoDe) to ensure that the data is stored in a common memory. The threads do not have to coordinate or synchronize their iterations. The details are used in Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2 Parallel Asynchronous Stochastic dual Co-ordinate Descent (PASSCoDe) Input: Initial \u03b1 and w = 1 Number of threads Wild 2 98.03 / 0.27x 15.28s / 1.75x 14.08s / 1.90x 4 106.11s / 0.25x 8.35s / 3.20x 114.43s / 0.843.84x / 3.843.84x / 3.943.94x / 3.943.943.9x / 3.943.93.93.93.9x / 3.93.93.93.93.93.93.93.93.93.93.93.93.9x 93.93.93.93.93.93.93.93.93.93.93.93.93.9x 93.93.93.93.93.93.93.93.93.93.93.9x 93.93.93.93.93.93.93.93.93.93.93.93.9x 93.93.93.93.93.93.93.93.93.9x 93.93.93.93.93.93.93.9x 93.93.93.93.93.9x 93.93.93.93.93.9x 93.93.93.93.93.93.9x 93.93.93.93.93.9x 93.93.93.93.9x 93.93.93.9x 93.93.93.93.93.9x 93.93.93.93.93.93.9x 93.93.93.9x 93.93.93.93.93.9x 93.93.93.9x 93.93.93.93.93.9x 93.93.93.93.9x 93.93.93."}, {"heading": "3.3 Implementation Details", "text": "Without proper implementation, standstill in PASSCoDe-Lock can occur because a thread must acquire all locks connected to Ni. An easy way to avoid standstill is to link an order for all locks in such a way that each thread follows the same order to obtain the locks. Random permutation can be easily implemented asynchronously for algorithm 2 in LIBLINEAR. Initially, the index is replaced by the index due to random permutation, so that each thread can be selected in n steps instead of n logn steps in anticipation. Random permutation can be easily implemented for algorithm 2 as follows."}, {"heading": "4 Convergence Analysis", "text": "In this section, we formally analyze the convergence properties of our proposed algorithms in Section 3. We assume that all proofs can be found in the annex. We assign a global counter for the total number of updates and the index i (j) denotes the component that was selected in step j. However, we define the order generated by our algorithms, and the order in which the order in which the order in which the updates are made, in the order in which the order in which the order in which the order in which the order in which the order in which the order in which the order in which the order in which the order in which the order in which, in the order in which, in the order in which, in the order in which, in the order in which, in the order in which, in the order in the order in which, in the order in which, in the order in the order in which, in the order in the order in which, in the order in the"}, {"heading": "5 Experimental Results", "text": "We perform several experiments and show that the proposed PASSCoDe-Atomic and PASSCoDe-Wild have superior performance compared to other state-of-the-art parallel coordinate descend algorithms. We consider the hinge loss and five sets of data: news20, covtype, rcv1, webspam and kddb. Detailed information is given in Table 3. For a fair comparison, we implement all comparative methods in C + + using OpenMP as a parallel programming framework. All experiments will be performed on an Intel multi-core dual socket machine with 256GB of memory. Each socket is associated with 10 computational cores. We explicitly that all threads use cores from the same socket to avoid communication between the sockets. Our codes will be publicly available. We focus on solving the (hinge loss) SDCD (see (5) in the appendix."}, {"heading": "5.1 Convergence in terms of iterations.", "text": "The original objective functional value is used to determine the convergence. Note that we still use P (w) for PASSCoDe-Wild, although the true original target (16) should be. As long as w-T remains small enough, the trend of (16) and P (w-Wild) is similar. Figure 4 (a), 5 (a), 6 (a) shows the convergence results of PASScococode-Wild, PASScocococode-Atomic, CoCocoA and AsySCD with 10 threads in terms of the number of iterations. The horizontal line in gray indicates the original objective functional value achieved by LIBLINEAR using the standard stop condition. The result for LIBcocococococode-Wild is also included as a reference. We have the following observations \u2022 convergence of three PASSCode-Codecode-Wild codecodecodecode-codecodecode-codecodecode-codecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecocodecodecodecodecodecodecodecodecocococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococo1, which are almost identical and very close to the convergence behavior of the serial LINEcodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecodecococococococococodecococococococococococococococococococococococococococococococococococococococo"}, {"heading": "5.2 Efficiency.", "text": "In order to have a fair comparison, we have to include both the initialization and the calculation in the time results. In the initialization phase, AsySCD O (n) requires time and O (n2) space to form and store the Hessian matrix Q for (2). Thus, we only have the results in the AsySCD, as all other datasets are too large for AsySCD to fit into even 256 GB of memory."}, {"heading": "5.3 Speedup", "text": "We are interested in the following evaluation criterion: Acceleration: = time required by the target method with p-threads taken by the best serial reference method. This criterion differs from the scaling, where the denominator is replaced by \"time taken for the target method with a single thread.\" Note that one method can have perfect scaling but very poor acceleration. Figures 2 (d), 3 (d), 5 (d), 6 (d) show the acceleration results, with 1) DCD being used as the best serial reference; 2) the shrinking heuristics is turned off for all PASSCoDe and DCD to have a fair comparison; 3) the initialization time is excluded from the acceleration calculation. \u2022 PASSCoDe-Wild has a very good acceleration performance compared to other approaches. It dual reaches about 6 to 8 accelerations with 10 threads, we do not have (from) any data formation."}, {"heading": "6 Conclusions", "text": "In this thesis, we present a family of parallel asynchronous stochastic dual-coordinate descendancy algorithms in the shared memory multi-core configuration, in which each thread repeatedly selects a random dual variable and performs coordinate updates using the primary variables stored in the shared memory. We analyze the convergence properties when different latch / atomic mechanisms are used. For convergence with atomic updates, we show linear convergence under certain conditions. Experimental results show that our algorithms are much faster than previous parallel coordinate descendants. (a) Convergence (b) Target (c) Accuracy (d) Accuracy (d) Convergence (b) Accuracy (c) Accuracy (d) Accuracy (d) Accuracy (d) Accuracy (d) Speed (d) Speed (d) Convergence (b) Accuracy (c) Accuracy (c) Accuracy (c)"}, {"heading": "A.1 Notations and Prepositions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1.1 Notations", "text": "\u2022 For all i = 1,.., n we have the following definitions: hi (u): = \"1 + 2 + (u) 2 +\" 2 + (u) 2 + 2 + (u) 2 + hi (u) Ti (w, s): = arg min u1 2 + (u) 2 + (u \u2212 s) xi \u00b2 2 + \"3 +\" i (u) = arg min u12 [u \u2212 (s \u2212 wTxi \u00b2 2)] 2 + hi (u), whereas we also refer to prox (s) as the proximal operator from Rn to Rn, so that (prox (x)) i = proxi (si)."}, {"heading": "A.1.2 Prepositions", "text": "Preposition 1. Egg (j) (1 = 1 = 2) = 1n = 1 = 1 = 2 (17) proof. This can be proved by the definition of \u03b2 and the assumption that i (j) is uniformly randomly selected from {1,..., n}. Preposition 2."}, {"heading": "Proof.", "text": "This is why the United States and other countries of the world are unable to fulfill their obligations. (1) It can be proved by combining Ti (w, s) and Proxi (\u00b7 s) - Ti (w2, s2) \u2212 Ti (w2, \u2212 s1 \u2212 s2 + (w1 \u2212 w2 \u2212 w2) Txi (19). It can be proved by combining Ti (w, s) and Proxi (\u00b7 s). (1) and the non-expansivity of the preliminary operator. \u2212 Preposition 4. \u2212 Let M \u2212 n = 1, q = 6 (vq) and we (v2) - (v2) - (v2) and the non-expansivity of the preliminary operator. \u2212 Preposition 4. (ventiv \u2212 \u2212 \u2212 n) Proof proof. (19) It can be proved by combining Ti (w, s) and Proxi (\u00b7 s) and the non-expansivity of the preliminary operator."}, {"heading": "A.2 Proof of Lemma 1", "text": "Drawing on Liu & Wright (2014), we point out that we have a single factor for all two vectors a and b. \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1"}, {"heading": "A.3 Proof of Theorem 2", "text": "First we define T (w, \u03b1) as a n-dimensional vector so that (T, \u03b1) t = Tt (w, \u03b1t) for all t, we can then use the DistanceE (w, \u03b1j) \u2212 T (w, \u03b1j) \u2212 T (w, \u03b1j) \u2212 T (w, \u03b1j) \u00b2 (we disregard the expectation in the following derivative):"}], "references": [{"title": "Revisiting asynchronous linear solvers: Provable convergence rate through randomization", "author": ["H. Avron", "A. Druinsky", "A. Gupta"], "venue": "In IEEE International Parallel and Distributed Processing Symposium,", "citeRegEx": "Avron et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["Bertsekas", "Dimitri P", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Bertsekas et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas et al\\.", "year": 1989}, {"title": "A training algorithm for optimal margin classifiers", "author": ["Boser", "Bernhard E", "Guyon", "Isabelle", "Vapnik", "Vladimir"], "venue": "In Proceedings of the Fifth Annual Workshop on Computational Learning Theory,", "citeRegEx": "Boser et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Boser et al\\.", "year": 1992}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["Bradley", "Joseph K", "Kyrola", "Aapo", "Bickson", "Danny", "Guestrin", "Carlos"], "venue": "In ICML,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Coordinate descent method for large-scale L2-loss linear SVM", "author": ["Chang", "Kai-Wei", "Hsieh", "Cho-Jui", "Lin", "Chih-Jen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["Hsieh", "Cho-Jui", "Chang", "Kai-Wei", "Lin", "Chih-Jen", "Keerthi", "S. Sathiya", "Sundararajan", "Sellamanickam"], "venue": "In Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML),", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Jaggi", "Martin", "Smith", "Virginia", "Tak\u00e1\u010d", "Terhorst", "Jonathan", "Hofmann", "Thomas", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "A sequential dual method for large scale multi-class linear SVMs", "author": ["Keerthi", "S. Sathiya", "Sundararajan", "Sellamanickam", "Chang", "Kai-Wei", "Hsieh", "Cho-Jui", "Lin", "Chih-Jen"], "venue": "In Proceedings of the Forteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 408\u2013416,", "citeRegEx": "Keerthi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Keerthi et al\\.", "year": 2008}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["J. Liu", "S.J. Wright"], "venue": null, "citeRegEx": "Liu and Wright,? \\Q2014\\E", "shortCiteRegEx": "Liu and Wright", "year": 2014}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["J. Liu", "S.J. Wright", "C. Re", "V. Bittorf"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "On the convergence of coordinate descent method for convex differentiable minimization", "author": ["Luo", "Zhi-Quan", "Tseng", "Paul"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Luo et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Luo et al\\.", "year": 1992}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Nesterov", "Yurii E"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nesterov and E.,? \\Q2012\\E", "shortCiteRegEx": "Nesterov and E.", "year": 2012}, {"title": "HOGWILD!: a lock-free approach to parallelizing stochastic gradient descent", "author": ["Niu", "Feng", "Recht", "Benjamin", "R\u00e9", "Christopher", "Wright", "Stephen J"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2012}, {"title": "On the nonasymptotic convergence of cyclic coordinate descent methods", "author": ["Saha", "Ankan", "Tewari", "Ambuj"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Saha et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saha et al\\.", "year": 2013}, {"title": "Feature clustering for accelerating parallel coordinate descent", "author": ["C. Scherrer", "A. Tewari", "M. Halappanavar", "D. Haglin"], "venue": "In NIPS,", "citeRegEx": "Scherrer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Scherrer et al\\.", "year": 2012}, {"title": "Pegasos: primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "In ICML,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Iteration complexity of feasible descent methods for convex optimization", "author": ["Wang", "Po-Wei", "Lin", "Chih-Jen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "In NIPS,", "citeRegEx": "Yang,? \\Q2013\\E", "shortCiteRegEx": "Yang", "year": 2013}, {"title": "Dual coordinate descent methods for logistic regression and maximum entropy models", "author": ["Yu", "Hsiang-Fu", "Huang", "Fang-Lan", "Lin", "Chih-Jen"], "venue": "Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Zhang", "Tong"], "venue": "In Proceedings of the 21th International Conference on Machine Learning (ICML),", "citeRegEx": "Zhang and Tong.,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2004}], "referenceMentions": [{"referenceID": 16, "context": "Problem (1) is usually called the primal problem, and can usually be solved by Stochastic Gradient Descent (SGD) (Zhang, 2004; Shalev-Shwartz et al., 2007), second order methods (Lin et al.", "startOffset": 113, "endOffset": 155}, {"referenceID": 4, "context": ", 2007), or primal coordinate descent algorithms (Chang et al., 2008; Huang et al., 2009).", "startOffset": 49, "endOffset": 89}, {"referenceID": 3, "context": "The first thread focuses on synchronized algorithms, including synchronized CD (Richt\u00e1rik & Tak\u00e1\u010d, 2012; Bradley et al., 2011) and synchronized DCD algorithms (Yang, 2013; Jaggi et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 19, "context": ", 2011) and synchronized DCD algorithms (Yang, 2013; Jaggi et al., 2014).", "startOffset": 40, "endOffset": 72}, {"referenceID": 6, "context": ", 2011) and synchronized DCD algorithms (Yang, 2013; Jaggi et al., 2014).", "startOffset": 40, "endOffset": 72}, {"referenceID": 9, "context": "To overcome this problem, the other thread of work focuses on asynchronous CD algorithms in multi-core shared memory systems (Liu & Wright, 2014; Liu et al., 2014).", "startOffset": 125, "endOffset": 163}, {"referenceID": 4, "context": "By maintaining w in memory, Hsieh et al. (2008); Keerthi et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 4, "context": "By maintaining w in memory, Hsieh et al. (2008); Keerthi et al. (2008) showed that the time complexity of each coordinate update can be reduced from O(nnz) to O(nnz/n), where nnz is number of nonzeros in the training dataset.", "startOffset": 28, "endOffset": 71}, {"referenceID": 5, "context": "Recently it has enjoyed renewed interest due to the success of \u201cstochastic\u201d coordinate descent in real applications (Hsieh et al., 2008; Nesterov, 2012).", "startOffset": 116, "endOffset": 152}, {"referenceID": 5, "context": "Many recent papers (Hsieh et al., 2008; Yu et al., 2011; ShalevShwartz & Zhang, 2013) have shown that solving the dual problem using coordinate descent algorithms is faster on large-scale datasets.", "startOffset": 19, "endOffset": 85}, {"referenceID": 20, "context": "Many recent papers (Hsieh et al., 2008; Yu et al., 2011; ShalevShwartz & Zhang, 2013) have shown that solving the dual problem using coordinate descent algorithms is faster on large-scale datasets.", "startOffset": 19, "endOffset": 85}, {"referenceID": 9, "context": "By maintaining a primal solutionw while updating dual variables, our algorithm is much faster than the previous asynchronous coordinate descent methods of (Liu & Wright, 2014; Liu et al., 2014) for solving the dual problem (2).", "startOffset": 155, "endOffset": 193}, {"referenceID": 19, "context": "Our algorithms are also faster than synchronized dual coordinate descent methods (Yang, 2013; Jaggi et al., 2014) since the latest values ofw can be accessed by all the threads.", "startOffset": 81, "endOffset": 113}, {"referenceID": 6, "context": "Our algorithms are also faster than synchronized dual coordinate descent methods (Yang, 2013; Jaggi et al., 2014) since the latest values ofw can be accessed by all the threads.", "startOffset": 81, "endOffset": 113}, {"referenceID": 3, "context": "Recently it has enjoyed renewed interest due to the success of \u201cstochastic\u201d coordinate descent in real applications (Hsieh et al., 2008; Nesterov, 2012). In terms of theoretical analysis, the convergence of (cyclic) coordinate descent has been studied for a long time (Luo & Tseng, 1992; Bertsekas, 1999), and the global linear convergence is presented recently under certain condition (Saha & Tewari, 2013; Wang & Lin, 2014). Stochastic Dual Coordinate Descent. Many recent papers (Hsieh et al., 2008; Yu et al., 2011; ShalevShwartz & Zhang, 2013) have shown that solving the dual problem using coordinate descent algorithms is faster on large-scale datasets. The success of SDCD strongly relies on exploiting the primal-dual relationship (3) to speed up the gradient computation in the dual space. DCD has become the state-of-the-art solver implemented in LIBLINEAR (Fan et al., 2008). In terms of convergence of dual objective function, some standard theoretical guarantees for coordinate descent can be directly applied. Different from standard analysis, Shalev-Shwartz & Zhang (2013) presented the convergence rate in terms of duality gap.", "startOffset": 117, "endOffset": 1089}, {"referenceID": 3, "context": "Recently it has enjoyed renewed interest due to the success of \u201cstochastic\u201d coordinate descent in real applications (Hsieh et al., 2008; Nesterov, 2012). In terms of theoretical analysis, the convergence of (cyclic) coordinate descent has been studied for a long time (Luo & Tseng, 1992; Bertsekas, 1999), and the global linear convergence is presented recently under certain condition (Saha & Tewari, 2013; Wang & Lin, 2014). Stochastic Dual Coordinate Descent. Many recent papers (Hsieh et al., 2008; Yu et al., 2011; ShalevShwartz & Zhang, 2013) have shown that solving the dual problem using coordinate descent algorithms is faster on large-scale datasets. The success of SDCD strongly relies on exploiting the primal-dual relationship (3) to speed up the gradient computation in the dual space. DCD has become the state-of-the-art solver implemented in LIBLINEAR (Fan et al., 2008). In terms of convergence of dual objective function, some standard theoretical guarantees for coordinate descent can be directly applied. Different from standard analysis, Shalev-Shwartz & Zhang (2013) presented the convergence rate in terms of duality gap. Parallel Stochastic Coordinate Descent. In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al.", "startOffset": 117, "endOffset": 1262}, {"referenceID": 2, "context": "In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al. (2011) proposed a similar algorithm for `1-regularized problems.", "startOffset": 190, "endOffset": 212}, {"referenceID": 2, "context": "In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al. (2011) proposed a similar algorithm for `1-regularized problems. Scherrer et al. (2012) studied parallel greedy coordinate descent.", "startOffset": 190, "endOffset": 293}, {"referenceID": 2, "context": "In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al. (2011) proposed a similar algorithm for `1-regularized problems. Scherrer et al. (2012) studied parallel greedy coordinate descent. However, the above synchronized methods usually face a trade-off in choosing the block size. If the block size is small, the load balancing problem leads to slow running time. If the block size is large, the convergence speed becomes much slower or the algorithm even diverges. These problems can be resolved by developing an asynchronous algorithm. Asynchronous coordinate descent has been studied by (Bertsekas & Tsitsiklis, 1989), but they require the Hessian to be diagonal dominant in order to establish the convergence. Recently, Liu et al. (2014); Liu & Wright (2014) proved linear convergence of asynchronous stochastic coordinate descent algorithms under the essential strong convexity condition and a \u201cbounded staleness\u201d condition, where they consider both \u201cconsistent read\u201d and \u201cinconsistent read\u201d models.", "startOffset": 190, "endOffset": 891}, {"referenceID": 2, "context": "In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al. (2011) proposed a similar algorithm for `1-regularized problems. Scherrer et al. (2012) studied parallel greedy coordinate descent. However, the above synchronized methods usually face a trade-off in choosing the block size. If the block size is small, the load balancing problem leads to slow running time. If the block size is large, the convergence speed becomes much slower or the algorithm even diverges. These problems can be resolved by developing an asynchronous algorithm. Asynchronous coordinate descent has been studied by (Bertsekas & Tsitsiklis, 1989), but they require the Hessian to be diagonal dominant in order to establish the convergence. Recently, Liu et al. (2014); Liu & Wright (2014) proved linear convergence of asynchronous stochastic coordinate descent algorithms under the essential strong convexity condition and a \u201cbounded staleness\u201d condition, where they consider both \u201cconsistent read\u201d and \u201cinconsistent read\u201d models.", "startOffset": 190, "endOffset": 912}, {"referenceID": 0, "context": "Avron et al. (2014) showed linear rate of convergence for the asynchronous randomized Gaussian-Seidel updates, which is a special case of coordinate descent on linear systems.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Avron et al. (2014) showed linear rate of convergence for the asynchronous randomized Gaussian-Seidel updates, which is a special case of coordinate descent on linear systems. Parallel Stochastic Dual Coordinate Descent. For solving (5), each coordinate updates only requires the global primal variables w and one local dual variable \u03b1i, thus algorithms only need to synchronize w. Based on this observation, Yang (2013) proposed to update several coordinates or blocks simultaneously and update the globalw, and Jaggi et al.", "startOffset": 0, "endOffset": 421}, {"referenceID": 0, "context": "Avron et al. (2014) showed linear rate of convergence for the asynchronous randomized Gaussian-Seidel updates, which is a special case of coordinate descent on linear systems. Parallel Stochastic Dual Coordinate Descent. For solving (5), each coordinate updates only requires the global primal variables w and one local dual variable \u03b1i, thus algorithms only need to synchronize w. Based on this observation, Yang (2013) proposed to update several coordinates or blocks simultaneously and update the globalw, and Jaggi et al. (2014) showed that each block can be solved with other approaches under the same framework.", "startOffset": 0, "endOffset": 533}, {"referenceID": 20, "context": "For SVM problems, the subproblem has a closed form solution, while for logistic regression problems it has to be solved by an iterative solver (see Yu et al. (2012) for details).", "startOffset": 148, "endOffset": 165}, {"referenceID": 12, "context": "However, as shown in (Niu et al., 2011; Liu & Wright, 2014), the effect of using slightly stale values is usually limited in practice.", "startOffset": 21, "endOffset": 59}, {"referenceID": 5, "context": "Based on this property, a shrinking strategy was proposed by Hsieh et al. (2008) to further speed up DCD.", "startOffset": 61, "endOffset": 81}, {"referenceID": 2, "context": "We list several important machine learning problems that admit global error bounds: \u2022 Support Vector Machines (SVM) with hinge loss (Boser et al., 1992): `i(zi) = C max(1\u2212 zi, 0) `i (\u2212\u03b1i) = { \u2212\u03b1i if 0 \u2264 \u03b1i \u2264 C, \u221e otherwise.", "startOffset": 132, "endOffset": 152}, {"referenceID": 6, "context": "\u2022 CoCoA: We implement a multi-core version of CoCoA (Jaggi et al., 2014) with \u03b2K = 1 and DCD as its local dual method.", "startOffset": 52, "endOffset": 72}, {"referenceID": 9, "context": "\u2022 AsySCD: We follow the description in (Liu & Wright, 2014; Liu et al., 2014) to implement AsySCD with the step length \u03b3 = 1 2 and the shuffling period p = 10 as suggested in (Liu et al.", "startOffset": 39, "endOffset": 77}, {"referenceID": 9, "context": ", 2014) to implement AsySCD with the step length \u03b3 = 1 2 and the shuffling period p = 10 as suggested in (Liu et al., 2014).", "startOffset": 105, "endOffset": 123}, {"referenceID": 9, "context": "\u2022 From Figure 2(d), we can see that AsySCD does not have any \u201cspeedup\u201d over the serial reference, although it is shown to have almost linear scaling (Liu et al., 2014; Liu & Wright, 2014).", "startOffset": 149, "endOffset": 187}], "year": 2015, "abstractText": "Stochastic Dual Coordinate Descent (DCD) has become one of the most efficient ways to solve the family of `2-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of DCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the DCD algorithms in LIBLINEAR. In recent research, several synchronized parallel DCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of asynchronous stochastic dual coordinate descent algorithms (PASSCoDe). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present the first backward error analysis for PASSCoDe under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers.", "creator": "LaTeX with hyperref package"}}}