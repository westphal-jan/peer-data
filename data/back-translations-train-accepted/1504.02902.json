{"id": "1504.02902", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2015", "title": "Gradual Training Method for Denoising Auto Encoders", "abstract": "Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets.", "histories": [["v1", "Sat, 11 Apr 2015 17:51:41 GMT  (795kb,D)", "http://arxiv.org/abs/1504.02902v1", "arXiv admin note: substantial text overlap witharXiv:1412.6257"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1412.6257", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["alexander kalmanovich", "gal chechik"], "accepted": true, "id": "1504.02902"}, "pdf": {"name": "1504.02902.pdf", "metadata": {"source": "CRF", "title": "GRADUAL TRAINING METHOD FOR DENOISING AUTO ENCODERS", "authors": ["Alexander Kalmanovich"], "emails": ["sashakal@gmail.com,", "gal.chechik@biu.ac.il"], "sections": [{"heading": null, "text": "Stacked Denoising Auto Encoders (DAEs) are known to learn useful deep imaging that can be used to enhance supervised training by initializing a deep network. We are investigating a deep PCS training scheme, where PCS layers are added gradually and additional layers are added. We show that this gradual training in the regime of medium-sized datasets offers a small but consistent improvement over stacked training in both reconstruction quality and classification errors compared to stacked training on MNIST and CIFAR datasets."}, {"heading": "1 GRADUAL TRAINING OF DENOISING AUTOENCODERS", "text": "In order to allow continuous adjustment of the lower layers, noise is injected at the input level, which differs from the stack training of the auto-encoders (Vincent et al., 2010). Specifically, during the step-by-step training, the first layer of the deep PCS is trained as in stacked training, creating a layer of weights w1. Then, when adding the second layer autoencoder, the weights w2 are matched with the weights already trained w1. Considering a training example x, we create a loud version x, insert it into the two-layer PCS and calculate the activation in the following layers h1 = sigmoid (w > 1 x), h2 = sigmoid (w > 2 h1) and y = sigmoid (w > 3 h2). It is important that the loss is calculated over the subsequent layers h1 = sigmoid (w > 1 x), h2 = sigmoid (w > 3)."}, {"heading": "2 EXPERIMENTAL PROCEDURES", "text": "We compared the performance of gradual and stacked training in two learning setups: an unattended denoising task, and a supervised classification task initialized with the weights that were learned in an unattended manner. Assessments were made on three benchmarks: MNIST, CIFAR-10, and CIFAR100, but only show the learning rate, SGD batch size, dynamics, and weight decay. In the reviewed experiments, the training was \"stopped early,\" without the uniform distribution of classes. Hyper parameters were selected with a second level of cross-validation, including the learning rate, SGD batch size, and weight decay."}, {"heading": "3 RESULTS", "text": "We evaluate gradual and stacked training in unattended task of denozing the image, and then evaluated the quality of both methods of initializing a network in a supervised learning task. Unsupervised learning for denoising. We first evaluated gradual training in an unsupervised task of denoising the image. Here, the network is trained to minimize cross-entropy loss over corrupt images. In addition to stacked and gradual training, we also tested a hybrid method that outputs some epochs to tuning only the second layer (as in stacked training sessions), and then spends the rest of the training budget on both layers (as in gradual training sessions). We define the stacked vs-gradual fraction 0 \u2264 f \u2264 1 as a fraction of the weight updates that are stacked during. f = 1 is equivalent to pure stacked training, while f = 0 is equivalent to pure gradual training."}], "references": [{"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "This training procedure differs from stack-training of auto encoders (Vincent et al., 2010) More specifically, in gradual training, the first layer of the deep DAE is trained as in stacked training, producing a layer of weights w1.", "startOffset": 69, "endOffset": 91}], "year": 2015, "abstractText": "Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets. 1 GRADUAL TRAINING OF DENOISING AUTOENCODERS We test here gradual training of deep denoising auto encoders, training the network layer-by-layer, but lower layers keep adapting throughout training. To allow lower layers to adapt continuously, noise is injected at the input level. This training procedure differs from stack-training of auto encoders (Vincent et al., 2010) More specifically, in gradual training, the first layer of the deep DAE is trained as in stacked training, producing a layer of weights w1. Then, when adding the second layer autoencoder, its weights w2 are tuned jointly with the already-trained weights w1. Given a training sample x, we generate a noisy version x\u0303, feed it to the 2-layered DAE, and compute the activation at the subsequent layers h1 = Sigmoid(w > 1 x), h2 = Sigmoid(w > 2 h1) and y = Sigmoid(w \u2032> 3 h2). Importantly, the loss function is computed over the input x, and is used to update all the weights including w1. Similarly, if a 3rd layer is trained, it involves tuning w1 and w2 in addition to w3 and w\u2032 4. 2 EXPERIMENTAL PROCEDURES We compare the performance of gradual and stacked training in two learning setups: an unsupervised denoising task, and a supervised classification task initialized using the weights learned in an unsupervised way. Evaluations were made on three benchmarks: MNIST, CIFAR-10 and CIFAR100, but only show here MNIST results due to space constraints. We used a test subset of 10,000 samples and several sizes of training-set all maintaining the uniform distribution over classes. Hyper parameters were selected using a second level of cross validation, including the learning rate, SGD batch size, momentum and weight decay. In the supervised experiments, training was \u2019early stopped\u2019 after 35 epochs without improvement. The results reported below are averages over 3 train-validation splits. Since gradual training involves updating lower layers, every presentation of a sample involves more weight updates than in a single-layered DAE. To compare stacked and gradual training on a common ground, we limited gradual training to use the same budget of weight update steps as stacked training. For example, when training the second layer for n epochs in gradual training, we allocate 2n training epochs for stacked training (details in the full paper). 1 ar X iv :1 50 4. 02 90 2v 1 [ cs .L G ] 1 1 A pr 2 01 5 Accepted as a workshop contribution at ICLR 2015 a) Unsupervised Training b) Supervised training 0 0.25 0.5 10.4 10.5 10.6 10.7", "creator": "LaTeX with hyperref package"}}}