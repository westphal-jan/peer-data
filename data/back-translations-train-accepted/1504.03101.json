{"id": "1504.03101", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2015", "title": "Convex Learning of Multiple Tasks and their Structure", "abstract": "Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem.We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.", "histories": [["v1", "Mon, 13 Apr 2015 09:13:23 GMT  (619kb)", "https://arxiv.org/abs/1504.03101v1", "26 pages, 1 figure, 2 tables"], ["v2", "Fri, 17 Apr 2015 20:03:21 GMT  (891kb)", "http://arxiv.org/abs/1504.03101v2", "26 pages, 1 figure, 2 tables"]], "COMMENTS": "26 pages, 1 figure, 2 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["carlo ciliberto", "youssef mroueh", "tomaso a poggio", "lorenzo rosasco"], "accepted": true, "id": "1504.03101"}, "pdf": {"name": "1504.03101.pdf", "metadata": {"source": "CRF", "title": "Convex Learning of Multiple Tasks and their Structure", "authors": ["Carlo Ciliberto", "Youssef Mroueh", "Tomaso Poggio", "Lorenzo Rosasco"], "emails": ["(cciliber@mit.edu)", "(tp@ai.mit.edu)", "(lrosasco@mit.edu)"], "sections": [{"heading": null, "text": "ar Xiv: 150 4.03 101v 2 [cs.L G] 1"}, {"heading": "1 Introduction", "text": "In fact it is true that most people are able to reform themselves. (...) It is not as if they are able to reform themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2 Background", "text": "We assume that T monitors the scalar learning problems (or tasks), each with a \"training set\" of input output observations St = {(xit, yit) nti = 1 with input space and yit output Space1. Faced with a loss function L: R \u2192 R + that measures the pro task predictions, we have limited ourselves to the typical situation where all tasks share the same input and output Space1, i.e., Xt = X and Yt R.s We want to solve the following regularized learning problems that occur in the notation, we have limited ourselves to the typical situation where all tasks share the same input and output Space2."}, {"heading": "2.1 Learning Multiple Tasks with RKHSvv", "text": "Interestingly, the theory of RKHSvv and the corresponding Tikhonov regularization scheme closely follows the derivative in the case of scalar. Definition 2.2. Let (H, < \u00b7, > H) be a Hilbert function space from X to RT. A symmetrical, positive, definitive, matrix-weighted function is called a reproducing core for H if we have it for all x-X, c-RT and f-H tasks. (x, \u00b7) c-H and the following reproducing property holds < f (x), c > RT = < f, (x, \u00b7) c > H.In analogy to the scalar setting it can be demonstrated (see [25]) that the representer theorem also applies to the regularization in RKHSvv."}, {"heading": "2.2 Separable Kernels", "text": "Separable (reproducible) cores are functions of formulas 1 (x, x) = k (x, x) A (x, x), where k: X \u00b7 X \u2192 R is a scalar reproducing kernel and a positive semi-definitive (PSD) matrix. (P) In this case, the representer theorem allows to rewrite problem (1) in a more compact matrix notation containing the output points; K-Sn + is the empirical kernel matrix associated with k and V: Rn \u00b7 T \u00b7 Rn \u00b7 T \u2192 R + generalizes the loss in (1) and consists in a linear combination of the input form of L. Notice that this formula also describes the situation in which not all values provided for the training are related to (CA \u00b7 R + generalized)."}, {"heading": "2.3 Incorporating Known Tasks Structure", "text": "The question that arises is to what extent it is actually about a way and a way in which it is about the question, to what extent it is actually about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about and a way in which it is about a way and a way in which it is about and a way in which it is about and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about which it is about a way in which it is"}, {"heading": "3 Learning the Tasks and their Structure", "text": "In this case, an advantageous approach is to derive the task relationships directly from the data. To this end, we propose to consider the following extension of problem (P) to minimize the specific task structures encoded in Matrix A. The above regulation is generic enough to cover a large number of already suggested approaches by simply specifying a selection of the scalar core and the penalty F. A detailed discussion of these relationships is moved to Section 4. In this section, we will focus on computational aspects. During this time, we will limit ourselves to calculating convex loss functions V and convex (and coercive) penalties F. In this case, the objective problems in Block 4 will be deferred. In this section, we will focus on computational aspects to convert the Svex method."}, {"heading": "3.1 Characterization of Minima and A Barrier Method", "text": "We begin, in Section 3.1.1, with a characterization of the solution to the problem (Q) by showing that there is an equivalent formulation regarding the minimization of a convex objective function, namely problem (R). Depending on the behavior of the objective function at the boundary of the optimization domain, problem (R) may not be solved by standardized optimization techniques. This possible problem motivates the introduction, in Section 3.1.2, of a barrier method; a family of \"perturbed\" convex programs is introduced whose solutions approach those of the problem (R) (and hence of the original (Q)). 3.1.1 An equivalent formulation for (Q) The objective functional solution in (Q) is therefore in principle difficult to find a global minimizer solution. However, since it is possible to work around this problem and efficiently find a global solution for (Q)."}, {"heading": "3.2 Block Coordinate Descent Methods", "text": "The characteristic block structure of the objective function in problem solving (S\u03b4) suggests that it may be beneficial to use block coordination methods (BCM) (see also: BCM) to solve it. (Let's look at how the various methods used in our environment are processed in different steps of optimization to C, with optimization to A, B, and C.) A metaphor of problem solving algorithms in algorithms 1, C, and each optimization stage of A, B in the sense that it includes the inputs, but not the outputs. In fact, when the structure matrix A is fixed, it boils down to the standard superordinate multi-task frameworks, where a prioritized knowledge of the task structure is available."}, {"heading": "3.2.1 Closed Form solutions for Alternating Minimization: Examples", "text": "Here we focus on the alternating minimization case and discuss some settings in which it is possible to obtain a closed mold solution for the procedures SUPERVISEDSTEP and UNSUPERVISEDSTEP. (SUPERVISEDSTEP) Least Square Loss. If the loss function V is selected as least square (i.e. V (Y, Z) = and the structure matrix A), a closed mold solution for the coefficient matrix C returned by the SUPERVISEDSTEP procedure can be easily derived (see for example [1]): vec (C) = (IT K + \u03bbA \u2212 1 In) \u2212 1vec (Y). The symbol denotes the Kronecker product, while the notation form vec (M) Rnm for a matrix M Rn \u00d7 m identifies a problem."}, {"heading": "4 Previous Work: Comparison and Discussion", "text": "This year it is more than ever before."}, {"heading": "5 Experiments", "text": "We empirically investigated the effectiveness of the strategy proposed in this paper for optimizing block coordinates for both artificial and real data sets. Synthetic experiments were conducted to evaluate the computational aspects of the approach, while evaluating the quality of the solutions found by the system under realistic conditions."}, {"heading": "5.1 Computational Times", "text": "As discussed in Sec. 4, several methods previously proposed in the literature, such as Multi-task Cluster Learning (MTCL) [19] and Multi-task Feature Learning (MTFL [3]), can be formulated as special problem cases (Q) or (R). It is natural to compare the proposed alternating minimization strategy with the optimization solution originally proposed for each method. In order to evaluate the performance of the system in terms of different dimensions of attribute space and an increasing number of tasks, we opted for this comparison in an artificial environment. We considered a linear setting in which the input data is in Rd and distributed according to a normal distribution with zero mean and identity covariance matrix. T linear models wt Rd for t = 1,...., T were then generated according to a normal distribution in order to test T different training sets, each consisting of 30 examples (x (t) i, y (t) i) i) ii)."}, {"heading": "5.2 Real dataset", "text": "In particular, we considered the following algorithms: Single Task Learning (STL) as baseline, Multi-task Feature Learning (MTFL) [3], Multi-task Relation Learning (MTRL) [37], Output Kernel Learning (OKL) [14]. We used at least square losses for all experiments. Sarcos2 is a regression dataset for evaluating machine learning solutions for inverse dynamics problems in robotics. It consists of a collection of 21-dimensional inputs, i.e. the common positions, velocities and acceleration of a robot arm with 7 degrees of freedom and 7 outputs (the tasks) measuring the corresponding torque frequencies on each joint."}, {"heading": "6 Conclusions", "text": "For a large family of models, the problem of shared learning of tasks and their relationships can be viewed as a convex program that generalizes previous results for specific cases [3, 14]. Such optimization can, of course, be addressed by block coordinate minimization, which can be seen as alternating between supervised and unsupervised learning steps that optimize the tasks or their structure. We evaluated the real data of our method and confirmed the utility of multi-task learning when tasks have similar characteristics. From an optimization perspective, future work will focus on investigating the theoretical properties of block coordinate methods, especially in terms of convergence rates. Indeed, the empirical evidence we report suggests that similar strategies can be remarkably efficient in the multi-task environment. From a modeling perspective, future work will focus on investigating broader families of matrix-rated cores, overcoming the boundaries of separable strategies."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Imposing Known Structure on the Tasks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Coding and Embedding", "text": "It is indeed the case that we are able to go in search of a solution that is in the position in which we find ourselves."}, {"heading": "Output Metric", "text": "In the multi-output settings, another approach to the implicit modeling of task relationships is to change the metric to the output space RT. (> Q = > Q) Specifically, we can define a matrix, as called in Sec. 6, and the induced internal product on RT as < y, y, y > RT for all y, y. \"For the loss functions L such as those in Sec. 6 (e.g. hinge, logistics, etc.), which depend only on the inner product between observations and predictions, we have defined the new loss as.\" K (x) = L (< y, f (x) > L (< y, f) > RT) and induces a learning problem of the minimized C Rn \u00b7 T (Y, KCh)."}, {"heading": "Proof. (Theorem 3.1)", "text": "We need to prove that C is a convex group and that tr (A) C (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K) K (A) K (A) K) K (A) K (A) K (K) K) K (A) K (K) K) K (A) K (A) K (K) K) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A) K (A"}, {"heading": "Spectral Regularization", "text": "Proposition 3.6 follows directly from the following resultProposition 6.3. Let's A, M, Sn + with Ran (A), Ran (M), Rank (M), Rank (M), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A), Rank (A) (A)."}, {"heading": "Linear Multi-task Learning", "text": "(W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W)."}, {"heading": "Proof. Theorem 6.4.", "text": "From the discussion in [3] we can minimize problem (R) to the equivalent formula B (A) \u00b7 R \u00b7 T (A) \u00b7 ST +, Ran (A) \u00b7 Ran (B) T (B, A) = V (Y, K) + tr (A) \u00b7 A (B) \u00b7 p (U) Therefore, it is sufficient to prove that problem (T) and (U) are equivalent. Let us suppose without loss of generality T \u2264. Let us consider an arbitrary matrix B (B) R T and a singular value decomposition B = V (V) U, where 0 R (\u2212 T) \u00b7 Tidentifies a matrix of all zeros, V O, U OT and vice versa."}], "references": [{"title": "Kernels for vector-valued functions: a review", "author": ["M. \u00c1lvarez", "N. Lawrence", "L. Rosasco"], "venue": "Foundations and Trends in Machine Learning, 4(3):195\u2013266,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning the Graph of Relations Among Multiple Tasks", "author": ["A. Argyriou", "S. Cl\u00e9men\u00e7on", "R. Zhang"], "venue": "Research report, Oct.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, 73,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "An algorithm for transfer learning in a heterogeneous environment", "author": ["A. Argyriou", "A. Maurer", "M. Pontil"], "venue": "ECML/PKDD (1), pages 71\u201385,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 25\u201332. MIT Press, Cambridge, MA,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Predicting structured data", "author": ["G.H. Bakir", "T. Hofmann", "B. Scholkopf", "A.J. Smola", "B. Taskar", "S.V.N. Vishwanathan"], "venue": "MIT Press,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Convexity, classification, and risk bounds", "author": ["P.L. Bartlett", "M.I. Jordan", "J.D. McAuliffe"], "venue": "Journal of the American Statistical Association, 101(473):138\u2013156,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "On the convergence of block coordinate descent type methods", "author": ["A. Beck", "L. Tetruashvili"], "venue": "Technion, Israel Institute of Technology, Haifa, Israel, Tech. Rep,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning incoherent sparse and low-rank patterns from multiple tasks", "author": ["J. Chen", "J. Liu", "J. Ye"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), 5(4):22,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "On the learnability and design of output codes for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, pages 35\u201346,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Relations between invex properties", "author": ["B. Craven"], "venue": "WORLD SCIENTIFIC SERIES IN APPLI- CABLE ANALYSIS, 5:25\u201334,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning output kernels for multi-task problems", "author": ["F. Dinuzzo"], "venue": "Neurocomputing, 118:119\u2013 126,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning output kernels with block coordinate descent", "author": ["F. Dinuzzo", "C.S. Ong", "P. Gehler", "G. Pillonetto"], "venue": "International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, pages 615\u2013637,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Wordnet: An electronic lexical database", "author": ["C. Fellbaum"], "venue": "1998. MIT Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Semantic label sharing for learning with many categories", "author": ["R. Fergus", "H. Bernal", "Y. Weiss", "A. Torralba"], "venue": "European Conference on Computer Vision,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Trace lasso: a trace norm regularization for correlated designs", "author": ["E. Grave", "G.R. Obozinski", "F. Bach"], "venue": "J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2187\u20132195.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Clustered multi-task learning: a convex formulation", "author": ["L. Jacob", "F. Bach", "J.-P. Vert"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Decorrelating semantic visual attributes by resisting the urge to share", "author": ["D. Jayaraman", "F. Sha", "K. Grauman"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting structured objects with support vector machines", "author": ["T. Joachims", "T. Hofmann", "Y. Yue", "C.-N. Yu"], "venue": "Commun. ACM, 52(11):97\u2013104, Nov.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daume III"], "venue": "arXiv preprint arXiv:1206.6417,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Discovering structure by learning sparse graphs", "author": ["B.M. Lake", "J.B. Tenenbaum"], "venue": "Proceedings of the 32nd Cognitive Science Conference,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Block variable selection in multivariate regression and highdimensional causal inference", "author": ["A. Lozano", "V. Sindhwani"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernels for multi-task learning", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Vector-valued manifold regularization", "author": ["H.Q. Minh", "V. Sindhwani"], "venue": "International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-category and taxonomy learning: A regularization approach", "author": ["Y. Mroueh", "T. Poggio", "L. Rosasco"], "venue": "NIPS Workshop on Challenges in Learning Hierarchical Models: Transfer Learning and Optimization,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiclass learning with simplex coding", "author": ["Y. Mroueh", "T. Poggio", "L. Rosasco", "J.-j. Slotine"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization, 22(2):341\u2013362,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo"], "venue": "SIAM Journal on Optimization, 23(2):1126\u20131153,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable matrix-valued kernel learning and high-dimensional nonlinear causal inference", "author": ["V. Sindhwani", "A.C. Lozano", "H.Q. Minh"], "venue": "CoRR, abs/1210.4792,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "Springer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Convergence of block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications, 109:475\u2013494,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "International Conference on Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "CVPR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three, pages 2764\u20132770. AAAI Press,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "A convex formulation for learning task relationships in multitask learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-10), pages 733\u2013742, Corvallis, Oregon,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex multitask learning with flexible task clusters", "author": ["W. Zhong", "J. Kwok"], "venue": "J. Langford and J. Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML \u201912, pages 49\u201356, New York, NY, USA, July", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 24, "context": "This idea has motivated a variety of methods, including frequentist [25, 3, 4] and Bayesian methods (see e.", "startOffset": 68, "endOffset": 78}, {"referenceID": 2, "context": "This idea has motivated a variety of methods, including frequentist [25, 3, 4] and Bayesian methods (see e.", "startOffset": 68, "endOffset": 78}, {"referenceID": 3, "context": "This idea has motivated a variety of methods, including frequentist [25, 3, 4] and Bayesian methods (see e.", "startOffset": 68, "endOffset": 78}, {"referenceID": 0, "context": "[1] and references therein), with connections to structured learning [6, 34].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[1] and references therein), with connections to structured learning [6, 34].", "startOffset": 69, "endOffset": 76}, {"referenceID": 33, "context": "[1] and references therein), with connections to structured learning [6, 34].", "startOffset": 69, "endOffset": 76}, {"referenceID": 24, "context": "Following [25, 15] we consider a setting where tasks are modeled as the components of a vector-valued function and their", "startOffset": 10, "endOffset": 18}, {"referenceID": 14, "context": "Following [25, 15] we consider a setting where tasks are modeled as the components of a vector-valued function and their", "startOffset": 10, "endOffset": 18}, {"referenceID": 24, "context": "Exploiting the theory of reproducing kernel Hilbert spaces for vector-valued functions (RKHSvv) [25], we consider and analyze a flexible regularization framework, within which a variety of previously proposed approaches can be recovered as special cases, see e.", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 23, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 25, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 36, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 30, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 32, "context": "Our approach is based on a barrier method that is combined with block coordinate descent techniques [33, 30].", "startOffset": 100, "endOffset": 108}, {"referenceID": 29, "context": "Our approach is based on a barrier method that is combined with block coordinate descent techniques [33, 30].", "startOffset": 100, "endOffset": 108}, {"referenceID": 2, "context": "In this sense our analysis generalizes the results in [3] for which a low-rank assumption was considered; however the extension is not straightforward, since we consider a much larger class of regularization schemes (any convex penalty).", "startOffset": 54, "endOffset": 57}, {"referenceID": 13, "context": "The RKHSvv setting allows to naturally deal both with linear and non-linear models and the approach we propose provides a general computational framework for learning output kernels as formalized in [14].", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "In particular, we focus on a class of reproducing kernels (known as separable kernels) that can be designed to encode specific tasks structures (see [15, 2] and Sec.", "startOffset": 149, "endOffset": 156}, {"referenceID": 1, "context": "In particular, we focus on a class of reproducing kernels (known as separable kernels) that can be designed to encode specific tasks structures (see [15, 2] and Sec.", "startOffset": 149, "endOffset": 156}, {"referenceID": 31, "context": "In general, due to discrete nature of the output space, these problems cannot be solved directly; hence, a so-called surrogate problem is often introduced, which is computationally tractable and whose solution allows to recover the solution of the original problem [32, 7, 28].", "startOffset": 265, "endOffset": 276}, {"referenceID": 6, "context": "In general, due to discrete nature of the output space, these problems cannot be solved directly; hence, a so-called surrogate problem is often introduced, which is computationally tractable and whose solution allows to recover the solution of the original problem [32, 7, 28].", "startOffset": 265, "endOffset": 276}, {"referenceID": 27, "context": "In general, due to discrete nature of the output space, these problems cannot be solved directly; hence, a so-called surrogate problem is often introduced, which is computationally tractable and whose solution allows to recover the solution of the original problem [32, 7, 28].", "startOffset": 265, "endOffset": 276}, {"referenceID": 24, "context": "In analogy to the scalar setting, it can be proved (see [25]) that the Representer Theorem holds also for regularization in RKHSvv.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "The choice of kernel \u0393 induces a joint representation of the inputs as well as a structure among the output components [1]; In the rest of the paper we will focus on so-called separable kernels, where these two aspects are factorized.", "startOffset": 119, "endOffset": 122}, {"referenceID": 14, "context": "Tasks relations can be enforced by devising suitable regularizers [15].", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "Interestingly, for a large class of such methods it can be shown that this is equivalent to the choice of the matrix A (or rather its pseudoinverse) [25].", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "If we consider the squared norm of a function f = \u2211n i=1 k(xi, \u00b7)Aci \u2208 H we have (see [15])", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "A different approach to model tasks relatedness consists in choosing a suitable metric on the output space to reflect the tasks structure [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 16, "context": ") [17, 21, 11].", "startOffset": 2, "endOffset": 14}, {"referenceID": 20, "context": ") [17, 21, 11].", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": ") [17, 21, 11].", "startOffset": 2, "endOffset": 14}, {"referenceID": 11, "context": "V and the penalty F are differentiable, this is exactly the definition of a convexifiable function, which in particular implies invexity [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 13, "context": "This result was originally proved in [14] for the special case of V the least-squares loss and F (\u00b7) = \u2016 \u00b7 \u2016F the Frobenius norm; Here we have proved its generalization to all convex losses V and penalties F .", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "First, we note that, while the objective function in Problem (R) is convex, the corresponding minimization problem might not be a convex program (in the sense that the feasible set C is not identified by a set of linear equalities and non-linear convex inequalities [9]).", "startOffset": 266, "endOffset": 269}, {"referenceID": 2, "context": "Here we propose a barrier approach inspired by the work in [3] by introducing a perturbation of problem (R) that enforces the objective functions to be equal to +\u221e on the boundary of R \u00d7 S +.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "The proposed barrier method is similar in spirit to the approach developed in [3] and indeed Theorem 3.", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "4 are a generalization over the two main results in [3] to any convex penaltyF on the cone of PSD matrices.", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "However, notice that since we are considering a much wider family of penalties (than the trace norm as in [3]) our results cannot directly derived from those in [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "However, notice that since we are considering a much wider family of penalties (than the trace norm as in [3]) our results cannot directly derived from those in [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "The characteristic block variable structure of the objective function in problem (S\u03b4), suggests that it might be beneficial to use block coordinate methods (BCM) (see [8]) to solve it.", "startOffset": 167, "endOffset": 170}, {"referenceID": 22, "context": "Instead, when the coefficient matrix C is fixed, the problem of learning A can be interpreted as an unsupervised setting in which the goal is to actually find the underlying task structure [23].", "startOffset": 189, "endOffset": 193}, {"referenceID": 29, "context": "Different strategies to choose which direction minimize at each step have been proposed: pre-fixed cyclic order, greedy search [30] or randomly, according to a predetermined distribution [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 28, "context": "Different strategies to choose which direction minimize at each step have been proposed: pre-fixed cyclic order, greedy search [30] or randomly, according to a predetermined distribution [29].", "startOffset": 187, "endOffset": 191}, {"referenceID": 29, "context": "For a review of several BCD algorithms we refer the reader to [30] and references therein.", "startOffset": 62, "endOffset": 66}, {"referenceID": 32, "context": "1) and has been studied extensively in [33] in the abstract setting where an oracle provides a block-wise minimizer at each iteration.", "startOffset": 39, "endOffset": 43}, {"referenceID": 32, "context": "1 in [33], while for part (b) we refer to Theorem 2 in [30].", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "1 in [33], while for part (b) we refer to Theorem 2 in [30].", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "However, up to our knowledge, so far only the authors in [3] have considered the issue of convergence to a global optimum.", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "V (Y, Z) = \u2016Y \u2212 Z\u2016F for any two matrices Y, Z \u2208 R) and the structure matrix A is fixed, a closed form solution for the coefficient matrix C returned by the SUPERVISEDSTEP procedure can be easily derived (see for instance [1]):", "startOffset": 221, "endOffset": 224}, {"referenceID": 25, "context": "In [26] the authors proposed a faster approach to solve this problem in closed form based on Sylvester\u2019s method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Figure 1: Comparison of the computational performance of the alternating minimization strategy studied in this paper with respect to the optimization methods proposed for MTCL in [19] and MTFL [3] in the original papers.", "startOffset": 179, "endOffset": 183}, {"referenceID": 2, "context": "Figure 1: Comparison of the computational performance of the alternating minimization strategy studied in this paper with respect to the optimization methods proposed for MTCL in [19] and MTFL [3] in the original papers.", "startOffset": 193, "endOffset": 196}, {"referenceID": 2, "context": "6 generalizes a similar result originally proved in in [3] for the special case p = 1 and provides an explicit formula for the UNSUPERVISEDSTEP of Algorithm 1.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "The penalty F = \u2016 \u00b7 \u2016F was considered in [14], together with a least squares loss function and the non convex problem (Q) is solved directly by alternating minimization.", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "This latter approach can shown to be equivalent to the Multi-Task Feature Learning setting of [3] (see supplementary material).", "startOffset": 94, "endOffset": 97}, {"referenceID": 18, "context": "In [19], the authors studied a multi-task setting where tasks are assumed to be organized in a fixed number r of unknown disjoint clusters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [19] the authors considered a regularization setting of the form of (R) where the structure matrix A is parametrized by the matrix M in order to reflect the cluster structure of the tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "Starting from a multi-task Gaussian Process setting, in [37], authors propose a model where the covariance among the coefficient vectors of the T individual tasks is controlled by a matrix A \u2208 S ++ in the form of a prior.", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "For instance [2] requires A to be a graph Laplacian, or [13] imposes a low-rank factorization of A in two smaller matrices.", "startOffset": 13, "endOffset": 16}, {"referenceID": 12, "context": "For instance [2] requires A to be a graph Laplacian, or [13] imposes a low-rank factorization of A in two smaller matrices.", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "In [27, 22] different sparsity models are proposed.", "startOffset": 3, "endOffset": 11}, {"referenceID": 21, "context": "In [27, 22] different sparsity models are proposed.", "startOffset": 3, "endOffset": 11}, {"referenceID": 18, "context": "4, several methods previously proposed in the literature, such as Multi-task Cluster Learning (MTCL) [19] and Multi-task Feature Learning (MTFL [3]]), can be formulated as special cases of problem (Q) or (R).", "startOffset": 101, "endOffset": 105}, {"referenceID": 2, "context": "4, several methods previously proposed in the literature, such as Multi-task Cluster Learning (MTCL) [19] and Multi-task Feature Learning (MTFL [3]]), can be formulated as special cases of problem (Q) or (R).", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "In particular we considered the following algorithms: Single Task Learning (STL) as a baseline, Multi-task Feature Learning (MTFL) [3], Multi-task Relation Learning (MTRL) [37], Output Kernel Learning (OKL) [14].", "startOffset": 131, "endOffset": 134}, {"referenceID": 36, "context": "In particular we considered the following algorithms: Single Task Learning (STL) as a baseline, Multi-task Feature Learning (MTFL) [3], Multi-task Relation Learning (MTRL) [37], Output Kernel Learning (OKL) [14].", "startOffset": 172, "endOffset": 176}, {"referenceID": 13, "context": "In particular we considered the following algorithms: Single Task Learning (STL) as a baseline, Multi-task Feature Learning (MTFL) [3], Multi-task Relation Learning (MTRL) [37], Output Kernel Learning (OKL) [14].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "We represented images using LLC coding [35] and trained the system on a training set comprising 50, 100 and 150 examples per class.", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "For a wide family of models, the problem of jointly learning the tasks and their relations can be cast as a convex program, generalizing previous results for special cases [3, 14].", "startOffset": 172, "endOffset": 179}, {"referenceID": 13, "context": "For a wide family of models, the problem of jointly learning the tasks and their relations can be cast as a convex program, generalizing previous results for special cases [3, 14].", "startOffset": 172, "endOffset": 179}], "year": 2015, "abstractText": "Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem. We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.", "creator": "LaTeX with hyperref package"}}}