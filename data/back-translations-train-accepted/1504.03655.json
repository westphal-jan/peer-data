{"id": "1504.03655", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2015", "title": "Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients", "abstract": "Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, and they serve as invaluable preprocessing tools for various purposes such as data exploration, dimension reduction and feature extraction.", "histories": [["v1", "Tue, 14 Apr 2015 18:34:03 GMT  (126kb,D)", "https://arxiv.org/abs/1504.03655v1", null], ["v2", "Tue, 23 Jun 2015 02:47:45 GMT  (583kb,D)", "http://arxiv.org/abs/1504.03655v2", "updated figures and new experiments"], ["v3", "Sun, 12 Jul 2015 23:09:21 GMT  (192kb,D)", "http://arxiv.org/abs/1504.03655v3", null], ["v4", "Sun, 10 Jan 2016 22:54:59 GMT  (566kb,D)", "http://arxiv.org/abs/1504.03655v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bo xie 0002", "yingyu liang", "le song"], "accepted": true, "id": "1504.03655"}, "pdf": {"name": "1504.03655.pdf", "metadata": {"source": "CRF", "title": "Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients", "authors": ["Bo Xie", "Yingyu Liang", "Le Song"], "emails": ["bo.xie@gatech.edu"], "sections": [{"heading": null, "text": "We propose a simple, computationally efficient, and memory-friendly algorithm based on the \"double stochastic gradient\" to extend a range of non-linear core component analyses such as kernel PCA, CCA, and SVD. Despite the non-convex nature of these problems, our method enjoys theoretical guarantees that it will approach the global optimum at the rate O \u00b2 (1 / t), even for the uppermost k-own subspace. Unlike many alternatives, our algorithm does not require explicit orthogonization, which is impracticable on large datasets. We demonstrate the effectiveness and scalability of our algorithm on large-scale synthetic and real datasets."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Related work", "text": "Many efforts have been made to increase the number of random features, but the Random Function approach [22, 23] approaches the core function with explicit random attributes and solves the problem in primordial form, bypassing square computational complexity. However, it has been applied to several core methods [15, 8, 16], most of which are related to our work, is Randomized Component Analysis [16]. One disadvantage of Randomized Component Analysis is that its theoretical guarantees apply only to the core matrix approach: it does not say how close the Randomized PCA solution is to the true solution. In contrast, we offer a finite time convergence rate as our solution approaches the true solution. Moreover, although a moderate size of random attributes can work well for tens of thousands of data points, datasets with tens of millions of data points require many more random attributes."}, {"heading": "3 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Kernels and Covariance Operators", "text": "A kernel k (x, y): X \u00b7 X 7 \u2192 R is a function that is positively defined (PD), i.e. for all n > 1, c1,. < F to X is a Hilbert space of functions from X to R. F is an RKHS if and only if there is a k (x, x): X \u00b7 X 7 \u2192 R so that an x (x, x) Hilbert space (RKHS) F is on X, < f (), k (x, \u00b7) > F = f (x). If such a k (x, x) exists, it is unique and it is a PD kernel (x, \u00b7 F)."}, {"heading": "3.2 Kernel PCA", "text": "The kernel PCA aims to identify the uppermost k-self-functions V = (v1 (\u00b7), vk (\u00b7) for the covariance operator A, whereby V can also be called the uppermost k-subspace for a series of self-functions {vi} and associated eigenvalues {\u03bbi}, where < vi, vj > F = \u03b4ij. We can define the intrinsic value of A asA = V \u0430kV > + V Gew\u00e4n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n"}, {"heading": "3.3 Random feature approximation", "text": "The use of random features to approximate a kernel function is motivated by the following theory.Theorem 1 (Bochner): A continuous, real-rated, symmetric and shift-invariant function k (x \u2212 x \u2032) on Rd is a PD kernel if and only if there is a finite, non-negative measurement P (\u03c9) on Rd, so that k (x \u2212 x \u2032) = even distribution on [0, 2 \u2212 x \u2032) dP (\u03c9) = equente, non-negative measurement P (x) throughs (y) (P (b)), where P (b) is an even distribution on [0, 2\u03c0] and percent (x).The theorem states that any shift is invariant core function k (x, y) = k (x \u2212 y), i.e. the kernel explicitly."}, {"heading": "4 Algorithm", "text": "In this section, we describe an efficient algorithm based on the \"double stochastic gradients\" to update the core PCA functions. KPCA is essentially a eigenvalue problem in a functional space. Traditional approaches convert it to the dual form, resulting in another eigenvalue problem, the size of which corresponds to the number of training points that are not scalable. Other approaches solve the problem in its original form with stochastic functional gradients. However, these algorithms must store all previously seen training points. They quickly encounter memory problems when working with hundreds of thousands of data points. We suggest tackling the problem with \"double stochastic gradients,\" where we make two unbiased stochastic approximations. Stochasticity stems from the fact that data points descend as in stochastic gradients. Another source of stochasticity is random features to approximate the core."}, {"heading": "4.1 Stochastic functional gradient update", "text": "Kernel PCA can be formulated as follows: non-convex optimization problem max Gtr (G > AG) s.t.G > G = I, (7), where G: = (g1,.., gk) and gi is the i-th function. The lagranger containing the constraint is L (G > AG) = tr (G > AG) + tr (G > G \u2212 I). Furthermore, the optimal conditions result in 2AG + G (G >) = 0, G > G \u2212 I = 0, the slope of the lagranger w.r.t G = GL = 2AG + G (E + E >).If these are inserted into the gradients, the following updating ruleGt + 1 = GtG > t (G > t) AGt results."}, {"heading": "4.2 Doubly stochastic update", "text": "The update rule (9) has a basic arithmetic error. At each step t, a new basis k (xt, \u00b7) is added to Gt, and it is therefore a linear combination of feature mappings of all data points up to t. To do this, the algorithm must store all data points it has seen so far, which is not practical for large datasets. To solve this problem, we use the random feature approximation k (x, \u00b7) throughp (x) throughp >, (10) where ht is the evaluation of Ht at the current data point: ht = [h1t (xt),.., h k t (xt) \u2264 \u2212 txt = V0, we can explicitly evaluate Ht at the current data point: the evaluation of Ht at the current data point: ht = [h1t (xt),.., h k t (xt) \u2264 \u2212 txt = V0, we can explicitly evaluate Ht at the current data point: the evaluation of Ht is the current data point: the evaluation of Ht = 1ht."}, {"heading": "5 Analysis", "text": "In this section, we provide finite time convergence guarantees for our algorithm. As discussed in the previous section, explicit orthogonalization is not scalable for the core case, so we must provide guarantees for updates without orthogonalization. This challenge is even more pronounced when using random characteristics, since it introduces additional variance.In addition, our guarantees w.r.t. are the uppermost k-dimensional subspace. Although convergence without normalization for a uppermost eigenvector was established before [19, 20], the subspace case is required by algorithm 1: {\u03b1i} t1 = DSGD-KPCA (P (x), k): P (\u03c9), applicable (x). 1: for i = 1,.., t do 2: Example xi \u00b2 P (x). 3: Example \u00b2 P (\u043e) with seeds i."}, {"heading": "5.1 Notations", "text": "To analyze the convergence of our double stochastic kernel PCA algorithm, we need to define a few spaces. To simplify, we assume that the mini-batch size for the data points.1 Let Ft: = (f1t,..., f k t) be the space estimated with stochastic gradients and explicit orthogonalizing. \u2212 Let us see the space estimated with stochastic update rule without orthogonalizing: Gt + 1 (F- > t + 1F-t-t-t-t + 1) \u2212 1 / 2. Let us let Gt: (g1t,...) estimate the space with stochastic update rule without orthogonalizing: Gt + 1 \u2190 Gt + g-Ht (I-GtG > t) AtGt.where AtGt > t-AtGt-t-t-t-t-t-t-t-t-can be written-equal-to-computation."}, {"heading": "5.2 Conditions and Assumptions", "text": "We will focus on the case where a good initialization V0 is given: V > 0 V0 = I, cos 2 \u03b8 (V, V0) \u2265 1 / 2. (13) In other words, we will analyze the later stage of convergence that is typical in the literature (e.g. [28]). The early stage can be analyzed using established techniques (e.g. [4]). Throughout the paper, we assume that | k (x, x \u2032) | \u2264 \u0445, | \u03c6\u03c9 (x) | \u2264 \u03c6, considering \u0432 and \u03c6 as constants. Note that this applies to all considered nuclei and random characteristics. Furthermore, we will consider the eigengap \u03bbk \u2212 speck + 1 as a constant, which also applies to typical applications and datasets."}, {"heading": "5.3 Update without random features", "text": "Our warranty is based on the cosine of the main angle between the calculated subspace and the basic truth proper subspace (also referred to as potential function): cos2 \u03b8 (V, Gt) = minw \u0445 V > Gtw-2, Gtw-2, Gtw-2. Consider the two different rules of update, one with explicit orthogonalization and another with explicit orthogonalization. Our final warranty for Gt-3, is the following warranty for Gt-3, orth.theorem-2, Gt-2, \u2212 Gt-2, Gt-2, and assume that the mini batch batch size gap for each 1 \u2264 i \u2264 t, a-Lemcos-A \u2212 Ai, < (Gt-2), which is not suitable. We exist increments, i = O (1 / i)."}, {"heading": "5.4 Doubly stochastic update", "text": "The Ht = Htw, and z = G = tw. Then the error can be compiled as x (x = x) (x = x)."}, {"heading": "6 Extensions", "text": "The proposed algorithm is a general technique for solving eigenvalue problems in the functional space. Numerous machine learning algorithms boil down to this basic operation. Therefore, our method can be easily expanded to solve many related tasks, including latent variable estimation, kernel CCA, spectral clustering, etc. In the following sections, we briefly illustrate how it can be extended to various machine learning algorithms."}, {"heading": "6.1 Locating individual eigenfunctions", "text": "The proposed algorithm finds the subspace spanned by the upper k eigenfunctions, but it does not isolate the individual eigenfunctions. If we need to find these individual eigenfunctions, we can use a modified version, the Generalized Hebbic Algorithm (GHA) [25]. Its updating rule is Gt + 1 = Gt + \u03b7tAtGt \u2212 \u03b7tGt UT [G > t AtGt], (16) where UT [\u00b7] is an operator that sets the lower triangular parts to zero. To understand the effect of the upper triangular operator, we can see that UT [\u00b7] is the updating rule for the first function of Gt > Atg 1 t, (17) exactly the same as that of a one-dimensional subspace; all contributions of the other functions are overwritten with zero."}, {"heading": "6.2 Latent variable models and kernel SVD", "text": "Latent variable models are probabilistic models based on unobserved or latent structures in the data. They appear in specific forms such as Gaussian Mixture Models (GMM), Hidden Markov Models (HMM) and Latent Dirichlet Allocations (LDA), etc. Algorithm 3: {\u03b1i, \u03b2i} t1 = DSGD-KSVD (P (x), P (y), k) Required: P (\u03c9), throughs (x). 1: for i = independent,., t do 2: Sample xi \u0445 P (x). Example yi \u0445 P (y). 3: Sample perspeci \u0445 P (\u03c9), k). 4: ui = Evaluate (xi, {\u03b1j} i \u2212 1j = 1).R k.5: vi = Evaluate (Vyi, {\u03b2j} i \u2212 1j = k."}, {"heading": "6.3 Kernel CCA and generalized eigenvalue problem", "text": "Kernel CCA and ICA [3] can also be solved within the proposed framework because they can be considered a generalized eigenvalue problem. In view of two variables X and Y, CCA finds two projections, so that the correlations between the two projected variables are maximized. In view of the covariance matrices CXX, CY and CXY, CCA corresponds to the following problem [CXX CXY CY] = (1 + \u03c32) [CXX CY] [gY gY] [gY gY gY], where gX and gY are the uppermost canonical correlation functions for variables X and Y, and the corresponding canonical correlation. This is a generalized eigenvalue problem. It can be reformed as the following non-convex optimization YY: YY (Gtr (G > AG), (21) s.t."}, {"heading": "6.4 Kernel sliced inverse regression", "text": "The Kernel Sliced Inverse Regression [14] aims at sufficiently reducing dimensions, in which the found representation of low dimensions maintains the statistical correlation to the targets. It is also reduced to a generalized eigenvalue problem and demonstrably finds the same subspace as KCCA [14]."}, {"heading": "7 Experiments", "text": "We demonstrate the effectiveness and scalability of our algorithm on both synthetic and real data sets."}, {"heading": "7.1 Synthetic dataset with analytical solution", "text": "We first check the convergence rate of DSGD-KPCA on a synthetic dataset with an analytical solution of eigenfunctions [31]. If the data follows a Gaussian distribution and we use a Gaussian kernel, the eigenfunctions are given by the Hermite polynomials. We have created 1 million data points and executed DSGD-KPCA with a total of 262,144 random features. In each iteration, we use a data mini-batch of size 512 and a random feature batch of size 128. After all the random features have been generated, we check and adjust the coefficients of existing random features. The kernel bandwidth is set as the true bandwidth of the datas.The step size is scheduled according to t = \u044501 + \u04451t, (27) generating two random features. We use a small procedural size so that the step size is large enough to reach a good initial solution."}, {"heading": "7.2 Nonparametric Latent Variable Model", "text": "In [29], the authors proposed a multiview non-parametric latent variable model solved by kernel SVD, followed by tensor power iterations; the algorithm can separate latent variables without imposing specific parametric assumptions of conditional probabilities; however, the scalability of the algorithm was limited by kernel SVD. Here, we show that with DSGD-KSVD, we can learn latent variable models with one million data, achieving a higher quality of learned components compared to two other approaches. DSGD-KSVD uses a total of 8192 random features, and in each iteration, it uses a feature mini-batch of size 256 and a data mini-batch of size 512. We compare with 1) random Fourier features with fixed 2048 functions and 2) random Nystrom features with fixed 2048 functions. Nystrom features are calculated by the first uniform data sample of 2048."}, {"heading": "7.3 KCCA MNIST8M", "text": "We then demonstrate the scalability and effectiveness of our algorithm on a large-scale real dataset. MNIST8M consists of 8.1 million handwritten digits and their transformations. Each digit is of size 28 \u00d7 28. We divide each image into the left and right parts and learn their correlations with KCCA. Such is the dimension of the input functions 392.The evaluation criteria are the total correlations at the top k = 50 canonical correlation directions, which are calculated on a separate test set of size 10000. From the 8.1 million training data we randomly select 10000 as the evaluation set.We compare with 1) random Fourier and 2) random Nystrom characteristics on both total correlations and runtime. We vary the number of random characteristics used for both methods. Our algorithm uses a total of 20480 characteristics. In each iteration we use minibatches of size 2048 and minibatches of size 1024."}, {"heading": "7.4 Kernel PCA visualization on molecular space dataset", "text": "The MolecularSpace dataset contains 2.3 million molecular motifs [8]. We are interested in visualizing the dataset with KPCA. The data is represented by sorted coulomb matrices of size 75 x 75 [18]. Each molecule also has an attribute called Power Conversion Efficiency (PCE). We use a Gaussian kernel with bandwidth chains by the \"median trick.\" We performed kernel PCA with a total of 16384 random features, with a feature minibatch size of 512, and data minibatch size of 1024. We performed 4000 iterations with the step size \u03b7t = 1 / (1 + 0.001 \u0445 t). Figure 4 shows the visualization by projecting the data onto the top two main components. Compared with linear PCA, KPCA shrinks the distances between clusters and produces the important structures in the dataset."}, {"heading": "7.5 Kernel sliced inverse regression on KUKA dataset", "text": "After performing SDRs, we adjust a linear regression model using the projected input data and evaluate the average square error (MSE).The data set records rhythmic movements of a KUKA arm at different speeds and represents realistic settings for robots [17].We use a variant that contains 2 million data points generated by the SL simulator.The KUKA robot has 7 joints, and the problem with high-dimensional regression is to predict the torques from positions, speeds, and accelerations.The input has 21 dimensions, while the output is 7 dimensions.Since there are seven independent joints, we set the reduced dimension to seven. We randomly select 20% as a test set, and from the remaining training set, we select 5000 as validation."}, {"heading": "8 Conclusions", "text": "We have proposed a general and scalable approach to solving non-linear component analysis based on double stochastic gradients. It is simple, efficient and scalable. In addition, we have theoretical guarantees that the entire subspace converges at the rate O value (1 / t) into the real subspace. As its core is an algorithm for eigenvalue problems in functional space, it can be applied to various other tasks and models. Finally, we demonstrate the scalability and effectiveness of our algorithm on both synthetic and real data sets."}, {"heading": "A Setting", "text": "Notations Given a distribution P (x) is a kernel function k (x, x) with RKHS F = > functions (= > functions) with RKHS F = > functions (= > functions) with RKHS F = > peculiar operator A: < g, Af > F = Ex [f (x) g (x) g (x))], Ex [f (x), F (), f2 (),., fk () is a list of k functions in the RKHS, and we define matrix-like notation AF (\u00b7): = (Af1)., Afk (\u00b7), (29) and F > AF is a k > matrix whose (i, j) -th element < fi, Afj > F."}, {"heading": "B Analysis Roadmap", "text": "To analyze the convergence of our double stochastic kernel PCA algorithms, we need to define a few intermediate subranges that we use for the simplicity of notation and explicit orthogonalizing. (F + 1) Ft + 1 (F + 1) Ft + 1 (F + 1) Ft + 1 (F + 1) Ft + 1 (F + 1) Ft + 1 (F + 1) Ft + 1 (F + 1) Ft + 1 (F + 1). Let us leave Gt: (GtG > t) AtGt (40), where AtGt and GtG > t are equivalent."}, {"heading": "C Stochastic Update", "text": "u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u > u"}, {"heading": "D Doubly Stochastic Update", "text": "In this section we will consider the double stochastic refresh rule. Suppose in step t a mini-group consisting of Bx, t random data points x r (1 \u2264 r \u2264 Bx, t) and B\u03c9, t random characters (1 \u2264 s \u2264 B\u03c9, t) is used, then the refresh rule isHt + 1 = Ht + 4 random data points x (2 \u2212 x x x) + random characters x (2 \u2212 x x x x) + (2 \u2212 x random characters) is applied. (72) = Ht (I \u2212 \u03b7tEt [ht) > ht (xt) > x random characters x (xt). (8) \u2212 tEt (xt) \u2212 n random characters (xt). (73) ht (xt) > ht (xt). (xt)]]]] (72) = Ht (I \u2212 increht tEt [ht (xt) > Et (xt) > Ext (xt) > hht (xt) argument."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel<lb>Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis,<lb>but they can not scale up to big datasets. Recent attempts have employed random feature approximations<lb>to convert the problem to the primal form for linear computational complexity. However, to obtain high<lb>quality solutions, the number of random features should be the same order of magnitude as the number<lb>of data points, making such approach not directly applicable to the regime with millions of data points.<lb>We propose a simple, computationally efficient, and memory friendly algorithm based on the \u201cdoubly<lb>stochastic gradients\u201d to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA<lb>and SVD. Despite the non-convex nature of these problems, our method enjoys theoretical guarantees<lb>that it converges at the rate \u00d5(1/t) to the global optimum, even for the top k eigen subspace. Unlike<lb>many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big<lb>datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and<lb>real world datasets.", "creator": "LaTeX with hyperref package"}}}