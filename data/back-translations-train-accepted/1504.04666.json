{"id": "1504.04666", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2015", "title": "Unsupervised Dependency Parsing: Let's Use Supervised Parsers", "abstract": "We present a self-training approach to unsupervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms. Our approach, called `iterated reranking' (IR), starts with dependency trees generated by an unsupervised parser, and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees. Our system achieves 1.8% accuracy higher than the state-of-the-part parser of Spitkovsky et al. (2013) on the WSJ corpus.", "histories": [["v1", "Sat, 18 Apr 2015 00:23:16 GMT  (83kb,D)", "http://arxiv.org/abs/1504.04666v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["phong le", "willem h zuidema"], "accepted": true, "id": "1504.04666"}, "pdf": {"name": "1504.04666.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Dependency Parsing: Let\u2019s Use Supervised Parsers", "authors": ["Phong Le", "Willem Zuidema"], "emails": ["p.le@uva.nl", "zuidema@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "This year, we have reached the point where we feel we can take the lead without having to try to find a solution, in order to find a solution that is capable of finding a solution."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Unsupervised Dependency Parsing", "text": "The first breakthrough was achieved by Klein and Manning (2004) with their dependency model with valence (DMV), the first model to exceed the right-branched baseline on the DDA metric: 43.2% versus 33.6% for sentences up to length 10 in the WSJ corpus. Nine years later, Spitkovsky et al. (2013) achieved much higher DDAs: 72.0% for sentences up to length 10 and 64.4% for all sentences in Section 23. During this period, many approaches were proposed to try the challenge. Naseem and Barzilay (2011), Tu and Honavar (2012), Spitkovsky et al. (2012), Spitkovsky et al. (2013) and Marecek and Straka (2013) use extensions of the DMV, but with different learning strategies. Naseem and Barzilay Reliquity use semantic cubes that are event annotations from an inherited domain."}, {"heading": "2.2 Reranking", "text": "Our work is based on re-anking, a technique widely used in (semi-) supervised parsing. re-anking requires two components: a k-best parser and a re-anchor. In one sentence, the parser creates a list of the k-best candidates, the re-anchor then sorts these candidates and selects the one with the highest score. re-anking was first successfully applied to supervised constituent parsing (Collins, 2000; Charniak and Johnson, 2005), and then applied in the approaches of supervised dependency analysis by Sangati et al. (2009), Hayashi et al. (2013) and Le and Zuidema (2014). The closest to our work is the work on semi-supervised constituent parsing by McClosky and colleagues, e.g. McClosky et al. (2006), on the use of selfie training."}, {"heading": "3 The IR Framework", "text": "Existing training methods for the unattended dependency task, such as Blunsom and Cohn (2010), Gillenwater et al. (2011) and Tu and Honavar (2012), are hypothesis-oriented searches using the EM algorithm or its variants: Training consists in moving from one point, which is a model hypothesis, to another. This approach is feasible for optimizing models with simple features, since existing dynamic programming algorithms can calculate expectations that are sums over all possible parses, or find the best parse in the entire parser space with low complexity. However, complexity increases rapidly when rich, complex features are used. One way to reduce computing costs is to use approximation methods as in Blunsom and Cohn (2010)."}, {"heading": "3.1 Treebank-oriented Greedy Search", "text": "Believing that the difficulty of using EM stems from the fact that treebanks are \"hidden,\" which leads to the need for the sum total (or max) of total possible treebanks, we propose a greedy local search scheme based on a different training philosophy: treebankoriented search. The key idea is the explicit search for concrete treebanks that are used to train parsing models.D is a set of raw sentences, the search space consists of all possible treebanksD = {d (s) | s, where d (s) is a dependency tree of the set. The goal of the search is the optimal treebank D, which is as good as human annotations."}, {"heading": "3.2 Iterated Reranking", "text": "We begin the greedy search scheme by iterating re-ranking, which requires two components: a k-best parser P and a re-anchor R. First, D1 is used to train these two components, leading to P1 and R1. Parser P1 then generates a series of lists of k candidates kD1 (whose cartesian product leads to N (D1) for the set of training sets S. The best candidates are collected according to re-anchors R1 to form D2 for the next iteration. However, this process is stopped when a predefined stop criterion met.1It is certain that, as in the work of Spitkovsky et al. (2010b) and many bootstrapping approaches, only parser P. re-anking gives us two advantages: First, it allows us to use highly expressive models such as the generative model proposed by Le and Zuidema (2014) if it embodies an idea similar to the roles of R \u00d7 (1) and P (1)."}, {"heading": "3.3 Multi-phase Iterated Reranking", "text": "In machine learning training, one often begins with a large set, i.e. all training data is consumed at the same time. Elman (1993), however, suggests that in some cases one should start by training simple models on small data and then gradually increase the complexity of the model and add more difficult data. In the unattended analysis of dependencies, starting with small sentences is intuitive. For example, learning the fact that the head of a sentence is its main verb is difficult because a long set always contains many syntactic category.It would be much easier if we started with only longer sentences, e.g. \"Look!,\" as there is only one choice that is normally a verb. This training program has been successfully applied by Spitkovsky et al. (2010a) under the name: Baby Step. We take small beginnings to construct the multi-phase, iterated sentence (e.g. \"Look!\"), as there is only one choice, which is normally a simple phase b of a short series of S is trained successfully in a short series of S."}, {"heading": "4 Le and Zuidema (2014)\u2019s Reranker", "text": "The re-anchor of Le and Zuidema (2014) is an exception among the monitored parsers because it uses an extremely expressive model whose features are \u221e - order2. To overcome the problem of scarcity, they introduced the architecture of the recursive neural network (IORNN) from the inside out, which can estimate tree-generating models, including those proposed by Eisner (1996) and Collins (2003a)."}, {"heading": "4.1 The\u221e-order Generative Model", "text": "Le and Zuidema (2014) \"s reranchors use the generative model proposed by Eisner (1996). Intuitively, this model is top-down: starting from ROOT, 2In fact, the order is finite but boundless. We create the left dependencies and the right dependencies. We then create dependencies for the dependencies of each ROOT. The generative process continues recursively until there are no more dependencies to create. Formally, this model is described by the following formula: P (d (H)) = L-l = 1 P (HLl | C (HLl)) P (d (HLl))) \u00b7 R-r = 1 P (HRr | C (HRr)) P (3), where H is the current head, d (N) is the fragment of the dependency sparse rooted in N, and C (N) is the context for generating N-HL, HR are each H's left dependencies and right dependencies, d (is the fragment of the N) is rooted in N."}, {"heading": "4.2 Estimation with the IORNN", "text": "An IORNN (Figure 1) is a recursive neural network whose topology is a tree. What distinguishes this network from traditional RNNs (Socher et al., 2010) is that each tree node represents two vectors: iu - the inner representation, represents the content of the phrase covered by the node, and ou - the outer representation, represents the context around that phrase. Furthermore, information in an IORNN is allowed to flow not only from the bottom up, as in RNNs, but also from the top down. This makes IORNs a natural tool for estimating top-down tree-generating models. Applying the IORNN architecture to dependency analyses is straightforward, along the generative history of the generating model. First of all, the \"inside\" part of this IORNN model is an approach that is simpler than what is presented in Figure 1: the inner representation of a phrase is often assumed to be the inner meaning of its head representation."}, {"heading": "4.3 The Reranker", "text": "The (generative) reanchor of Le and Zuidema is given by d * = arg max d * kDep (s) P (d), where P (Eq.3) is calculated by the \u221e order generative model estimated by an IORNN; and kDep (s) is the k-best list."}, {"heading": "5 Complete System", "text": "Our system is based on the multi-phase IR. Generally, any third-party parser for unattended dependency savers in phase 0 can be used, and any third-party parser that can generate k-best lists can be used in the other phases. In our experiments for phase 0, we select the parser using an extension of the DMV model with stop probability estimates calculated on a large corpus proposed by Marecek and Straka (2013).This system has a moderate performance3 on the WSJ corpus: 57.1% compared to the SOTA 64.4% DDA from Spitkovsky et al. (2013).For the other phases, we use the MSTParser4 (with the second-order mode of operation) (McDonald and Pereira, 2006).Our system uses Le and Zuidema (2014)'s anchor (Section 4.3). It is worth noting that in this case each step is considered to be an equal ranking D."}, {"heading": "5.1 Tuning Parser P", "text": "The position and shape of N (Di) is therefore determined by two factors: how well Pi can match Di, and k. Intuitively, we use their source code at http: / / ufal.mff.cuni.cz / udp with the setting presented in Section 6.1, the larger3Marecek and Straka (2013) have not reported any experimental results on the WSJ corpus. We use their source code at http: / / ufal.mff.cz / udp with the setting presented in Section 6.1. Because the parser does not offer the option to parse invisible sentences, we merge the training sets (up to length 15) to evaluate its performance. Note that this result is close to the DDA (55.4%) that the authors are using CoNLL 2007 to date English, which is part of the WSJ."}, {"heading": "5.2 Tuning Reranker R", "text": "By tuning the Reranchor R values for dimIORNN, the dimensions of internal and external representations and itersIORNN, the number of epochs in which the IORNN is to be trained, are determined. Since the \u221e order model is very meaningful and neural networks are universal approximators (Cybenko, 1989), the Reranchor can perfectly remember all training pars. To avoid this, we set dimIORNN = 50 and set itersIORNN = 5 for very early stopping."}, {"heading": "5.3 Tuning multi-phase IR", "text": "Since the parser of Marecek and Straka (2013) does not distinguish between training data and test data, we postulate S0 = S1. Our system has N-phases, so that S0, S1 all records up to length l1 = 15, Si (i = 2.. N) all records up to length li = li \u2212 1 + 1 and SN all records up to length 25. Phase 1 ends after 100 iterations, while all subsequent phases run with one iteration. Note that we force the local search in phase 1 to intensive processes, because we hypothesize that most important patterns for the analysis of dependence can be found within short sentences."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Setting", "text": "We use Penn Treebank WSJ Corpus: Sections 02-21 for training and Section 23 for testing. Then we apply the standard pre-processing 5 for unattended dependency analysis (Klein and Manning, 2004): we remove all blank sub-trees, punctuation, and terminals (marked # and $) that are not pronounced where they appear; we convert the remaining trees into dependencies by using Collins \"header rules (Collins, 2003b). Both word forms and golden POS tags are used. To evaluate the metric of directed dependency accuracy (DDA) is used. The vocabulary is taken as a list of words that occur more than twice in the training data. All other words are referred to as\" UNKNOWN \"and each digit is replaced by\" 0. \"We initialize the IORNN with the 50-twilight embedding from Collobert et al. (2011) and 6 train it with the learning rate of 0.1."}, {"heading": "6.2 Results", "text": "We compare our system with current systems (Table 1 and Section 2.1). Our system with the two encouragement levels MinEnc and MaxEnc achieves the highest reported DDAs in Section 23: 1.8% and 1.2% higher respectively than Spitkovsky et al. (2013) in all judgments and up to length 10. Our improvements over the initializer of the system (Marecek and Straka, 2013) are 9.1% and 4.4% respectively."}, {"heading": "6.3 Analysis", "text": "It is about the question to what extent it is a matter of a way in which people in the world move into another world. (...) It is about the question to what extent the world slips into another world. (...) It is about the question to what extent the world slips into another world. (...) It is about the question to what extent the world slips into another world. (...) It is about the question to what extent the world slips into another world. (...) It is about the question to what extent the world slips into another world. (...) It is about the question to what extent the world slips into another world. (...) It is about the question to what extent the world slips into another world. (...) It is about the world, in which it is about the world, in which it is about. (...) It is about the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world."}, {"heading": "7 Discussion", "text": "Our system differs from the other systems shown in Table 1 because it uses an extremely expressive model, the so-called \"bedbedbedbedative model,\" in which the contexts are very large. Only the work of Blunsom and Cohn (2010), whose resulting grammar rules can contain large fragments of trees, shares this characteristic. The difference is that their work involves a predefined, hierarchical, non-parametric node of the Pitman-Yor process to avoid and smooth large, rare fragments, whereas the IORNs of our system do so automatically, learning how to deal with remote nodes, which are often less informative than close conditioning nodes on computing P (x | C). Furthermore, smoothing is free: we can map recursive onal networks to assign \"similar\" fragments to narrow points (Socher et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al)."}, {"heading": "8 Conclusion", "text": "Our system, which uses the unattended parser from Marecek and Straka (2013) as initiator, the k-best MSTParser, and the reanchor from Le and Zuidema (2014), achieved 1.8% higher DDA than the SOTA parser from Spitkovsky et al. (2013) on the WSJ corpus. Furthermore, we have shown that unattended parsing offers lexical semantics benefits through the use of word embeddings. Our future work is to use other existing monitored parsers that fit our framework. In addition, taking into account the rapid evolution of the word, we will embed research (Mikolov et al., 2013; Pennington et al., 2014) to try different word conditions."}, {"heading": "Acknowledgments", "text": "We thank Remko Scha and three anonymous reviewers for helpful comments. Le thanks Milos \"Stanojevic\" for helpful debate.8In an experiment we used the parser of Marecek and Straka (2013) as initiator for the parser of Gillenwater et al. (2011), which, as expected, was not able to use."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Simple robust grammar induction with combinatory categorial grammars", "author": ["Yonatan Bisk", "Julia Hockenmaier."], "venue": "AAAI.", "citeRegEx": "Bisk and Hockenmaier.,? 2012", "shortCiteRegEx": "Bisk and Hockenmaier.", "year": 2012}, {"title": "Combining labeled and unlabeled sata with co-training", "author": ["Avrim Blum", "Tom M. Mitchell."], "venue": "COLT, pages 92\u2013100.", "citeRegEx": "Blum and Mitchell.,? 1998", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Phil Blunsom", "Trevor Cohn."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204\u20131213. Association for Computational Lin-", "citeRegEx": "Blunsom and Cohn.,? 2010", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2010}, {"title": "Coarse-tofine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Discriminative reranking for natural language parsing", "author": ["Michael Collins."], "venue": "ICML, pages 175\u2013182.", "citeRegEx": "Collins.,? 2000", "shortCiteRegEx": "Collins.", "year": 2000}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins."], "venue": "Computational linguistics, 29(4):589\u2013637.", "citeRegEx": "Collins.,? 2003a", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins."], "venue": "Computational Linguistics, 29(4):589\u2013637.", "citeRegEx": "Collins.,? 2003b", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u2013 2537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko."], "venue": "Mathematics of control, signals and systems, 2(4):303\u2013314.", "citeRegEx": "Cybenko.,? 1989", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M Eisner."], "venue": "Proceedings of the 16th conference on Computational linguistics-Volume 1, pages 340\u2013345. Association for Computational Linguistics.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["Jeffrey L Elman."], "venue": "Cognition, 48(1):71\u201399.", "citeRegEx": "Elman.,? 1993", "shortCiteRegEx": "Elman.", "year": 1993}, {"title": "Posterior sparsity in unsupervised dependency parsing", "author": ["Jennifer Gillenwater", "Kuzman Ganchev", "Jo\u00e3o Gra\u00e7a", "Fernando Pereira", "Ben Taskar."], "venue": "The Journal of Machine Learning Research, 12:455\u2013490.", "citeRegEx": "Gillenwater et al\\.,? 2011", "shortCiteRegEx": "Gillenwater et al\\.", "year": 2011}, {"title": "Efficient stacked dependency parsing by forest reranking", "author": ["Katsuhiko Hayashi", "Shuhei Kondo", "Yuji Matsumoto."], "venue": "Transactions of the Association for Computational Linguistics, 1(1):139\u2013150.", "citeRegEx": "Hayashi et al\\.,? 2013", "shortCiteRegEx": "Hayashi et al\\.", "year": 2013}, {"title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "ACL, pages 478\u2013485.", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Efficient thirdorder dependency parsers", "author": ["Terry Koo", "Michael Collins."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1\u201311. Association for Computational Linguistics.", "citeRegEx": "Koo and Collins.,? 2010", "shortCiteRegEx": "Koo and Collins.", "year": 2010}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.", "citeRegEx": "Le and Zuidema.,? 2014", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing", "author": ["David Marecek", "Milan Straka."], "venue": "ACL (1), pages 281\u2013290.", "citeRegEx": "Marecek and Straka.,? 2013", "shortCiteRegEx": "Marecek and Straka.", "year": 2013}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Andr\u00e9 FT Martins", "Miguel B Almeida", "Noah A Smith."], "venue": "Proc. of ACL.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Effective self-training for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics, pages", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["Ryan T. McDonald", "Fernando C.N. Pereira."], "venue": "EACL.", "citeRegEx": "McDonald and Pereira.,? 2006", "shortCiteRegEx": "McDonald and Pereira.", "year": 2006}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Using semantic cues to learn syntax", "author": ["Tahira Naseem", "Regina Barzilay."], "venue": "AAAI.", "citeRegEx": "Naseem and Barzilay.,? 2011", "shortCiteRegEx": "Naseem and Barzilay.", "year": 2011}, {"title": "Linguistically Motivated Models for Lightly-Supervised Dependency Parsing", "author": ["Tahira Naseem."], "venue": "Ph.D. thesis, Massachusetts Institute of Technology.", "citeRegEx": "Naseem.,? 2014", "shortCiteRegEx": "Naseem.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A generative re-ranking model for dependency parsing", "author": ["Federico Sangati", "Willem Zuidema", "Rens Bod."], "venue": "Proceedings of the 11th International Conference on Parsing Technologies, pages 238\u2013241. Association for Computational Linguistics.", "citeRegEx": "Sangati et al\\.,? 2009", "shortCiteRegEx": "Sangati et al\\.", "year": 2009}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.", "citeRegEx": "Socher et al\\.,? 2010", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "From Baby Steps to Leapfrog: How \u201cLess is More\u201d in unsupervised dependency parsing", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky."], "venue": "Proc. of NAACL-HLT.", "citeRegEx": "Spitkovsky et al\\.,? 2010a", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Viterbi training improves unsupervised dependency parsing", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky", "Christopher D. Manning."], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010).", "citeRegEx": "Spitkovsky et al\\.,? 2010b", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky."], "venue": "Proceedings of the 11th International Conference on Grammatical Inference.", "citeRegEx": "Spitkovsky et al\\.,? 2012", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2012}, {"title": "Breaking out of local optima with count transforms and model recombination: A study in grammar induction", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky."], "venue": "EMNLP, pages 1983\u20131995.", "citeRegEx": "Spitkovsky et al\\.,? 2013", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2013}, {"title": "Unambiguity regularization for unsupervised learning of probabilistic grammars", "author": ["Kewei Tu", "Vasant Honavar."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learn-", "citeRegEx": "Tu and Honavar.,? 2012", "shortCiteRegEx": "Tu and Honavar.", "year": 2012}], "referenceMentions": [{"referenceID": 28, "context": "8% accuracy higher than the stateof-the-part parser of Spitkovsky et al. (2013) on the WSJ corpus.", "startOffset": 55, "endOffset": 80}, {"referenceID": 16, "context": "Many recent supervised parsers use thirdorder (or higher order) features (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014)", "startOffset": 73, "endOffset": 140}, {"referenceID": 19, "context": "Many recent supervised parsers use thirdorder (or higher order) features (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014)", "startOffset": 73, "endOffset": 140}, {"referenceID": 17, "context": "Many recent supervised parsers use thirdorder (or higher order) features (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014)", "startOffset": 73, "endOffset": 140}, {"referenceID": 22, "context": "Surprisingly, however, there have been no reported attempts to reuse supervised approaches to tackle the unsupervised parsing problem (an idea briefly mentioned in Spitkovsky et al. (2010b)).", "startOffset": 164, "endOffset": 190}, {"referenceID": 3, "context": "tent patterns in data (Naseem, 2014, page 23), and to avoid overfitting (Blunsom and Cohn, 2010).", "startOffset": 72, "endOffset": 96}, {"referenceID": 31, "context": "3% on the WSJ corpus (Spitkovsky et al., 2013), compared to over 93% actual performance of the SOTA supervised parsers.", "startOffset": 21, "endOffset": 46}, {"referenceID": 16, "context": "(2014), Le and Zuidema (2014), and Chen and Manning (2014).", "startOffset": 8, "endOffset": 30}, {"referenceID": 5, "context": "(2014), Le and Zuidema (2014), and Chen and Manning (2014). Lexical semantics is a source for handling rare words and syntactic ambiguities.", "startOffset": 35, "endOffset": 59}, {"referenceID": 17, "context": "Using this framework, we can employ the work of Le and Zuidema (2014) to build a new system that outperforms the SOTA unsupervised parser of Spitkovsky et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 17, "context": "Using this framework, we can employ the work of Le and Zuidema (2014) to build a new system that outperforms the SOTA unsupervised parser of Spitkovsky et al. (2013) on the WSJ corpus.", "startOffset": 48, "endOffset": 166}, {"referenceID": 15, "context": "The first breakthrough was set by Klein and Manning (2004) with their dependency model with valence (DMV), the first model to outperform the right-branching baseline on the DDA metric: 43.", "startOffset": 34, "endOffset": 59}, {"referenceID": 15, "context": "The first breakthrough was set by Klein and Manning (2004) with their dependency model with valence (DMV), the first model to outperform the right-branching baseline on the DDA metric: 43.2% vs 33.6% on sentences up to length 10 in the WSJ corpus. Nine years later, Spitkovsky et al. (2013) achieved much higher DDAs: 72.", "startOffset": 34, "endOffset": 291}, {"referenceID": 18, "context": "(2013), and Marecek and Straka (2013) employ extensions of the DMV but with different learning strategies.", "startOffset": 12, "endOffset": 38}, {"referenceID": 18, "context": "(2013), and Marecek and Straka (2013) employ extensions of the DMV but with different learning strategies. Naseem and Barzilay (2011) use semantic cues, which are event annotations from an outof-domain annotated corpus, in their model during training.", "startOffset": 12, "endOffset": 134}, {"referenceID": 18, "context": "(2013), and Marecek and Straka (2013) employ extensions of the DMV but with different learning strategies. Naseem and Barzilay (2011) use semantic cues, which are event annotations from an outof-domain annotated corpus, in their model during training. Relying on the fact that natural language grammars must be unambiguous in the sense that a sentence should have very few correct parses, Tu and Honavar (2012) incorporate unambiguity regu-", "startOffset": 12, "endOffset": 411}, {"referenceID": 27, "context": "Spitkovsky et al. (2012) bootstrap the learning by slicing up all input sentences at punctuation.", "startOffset": 0, "endOffset": 25}, {"referenceID": 27, "context": "Spitkovsky et al. (2012) bootstrap the learning by slicing up all input sentences at punctuation. Spitkovsky et al. (2013) propose a complete deterministic learning framework for breaking out of local optima using count transforms and model recombination.", "startOffset": 0, "endOffset": 123}, {"referenceID": 18, "context": "Marecek and Straka (2013) make use of a large raw text corpus (e.", "startOffset": 0, "endOffset": 26}, {"referenceID": 1, "context": "Differing from those works, Bisk and Hockenmaier (2012) rely on Combinatory Categorial Gram-", "startOffset": 28, "endOffset": 56}, {"referenceID": 3, "context": "mars with a small number of hand-crafted general linguistic principles; whereas Blunsom and Cohn (2010) use Tree Substitution Grammars with a hierarchical non-parametric Pitman-Yor process prior biasing the learning to a small grammar.", "startOffset": 80, "endOffset": 104}, {"referenceID": 6, "context": "supervised constituent parsing (Collins, 2000; Charniak and Johnson, 2005).", "startOffset": 31, "endOffset": 74}, {"referenceID": 4, "context": "supervised constituent parsing (Collins, 2000; Charniak and Johnson, 2005).", "startOffset": 31, "endOffset": 74}, {"referenceID": 4, "context": "supervised constituent parsing (Collins, 2000; Charniak and Johnson, 2005). It was then employed in the supervised dependency parsing approaches of Sangati et al. (2009), Hayashi et al.", "startOffset": 47, "endOffset": 170}, {"referenceID": 4, "context": "supervised constituent parsing (Collins, 2000; Charniak and Johnson, 2005). It was then employed in the supervised dependency parsing approaches of Sangati et al. (2009), Hayashi et al. (2013), and Le and Zuidema (2014).", "startOffset": 47, "endOffset": 193}, {"referenceID": 4, "context": "supervised constituent parsing (Collins, 2000; Charniak and Johnson, 2005). It was then employed in the supervised dependency parsing approaches of Sangati et al. (2009), Hayashi et al. (2013), and Le and Zuidema (2014).", "startOffset": 47, "endOffset": 220}, {"referenceID": 20, "context": "McClosky et al. (2006), using selftraining.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Existing training methods for the unsupervised dependency task, such as Blunsom and Cohn (2010), Gillenwater et al.", "startOffset": 72, "endOffset": 96}, {"referenceID": 3, "context": "Existing training methods for the unsupervised dependency task, such as Blunsom and Cohn (2010), Gillenwater et al. (2011), and Tu and Honavar (2012), are hypothesis-oriented search with the EM algorithm or its variants: training is to move from a point which represents a model hypothesis to another point.", "startOffset": 72, "endOffset": 123}, {"referenceID": 3, "context": "Existing training methods for the unsupervised dependency task, such as Blunsom and Cohn (2010), Gillenwater et al. (2011), and Tu and Honavar (2012), are hypothesis-oriented search with the EM algorithm or its variants: training is to move from a point which represents a model hypothesis to another point.", "startOffset": 72, "endOffset": 150}, {"referenceID": 3, "context": "Existing training methods for the unsupervised dependency task, such as Blunsom and Cohn (2010), Gillenwater et al. (2011), and Tu and Honavar (2012), are hypothesis-oriented search with the EM algorithm or its variants: training is to move from a point which represents a model hypothesis to another point. This approach is feasible for optimising models using simple features since existing dynamic programming algorithms can compute expectations, which are sums over all possible parses, or to find the best parse in the whole parse space with low complexities. However, the complexity increases rapidly if rich, complex features are used. One way to reduce the computational cost is to use approximation methods like sampling as in Blunsom and Cohn (2010).", "startOffset": 72, "endOffset": 760}, {"referenceID": 20, "context": "Semi-supervised parsing using reranking (McClosky et al., 2006).", "startOffset": 40, "endOffset": 63}, {"referenceID": 29, "context": "Unsupervised parsing with hard-EM (Spitkovsky et al., 2010b) In hard-EM, the target is to maximise the following objective function with respect to a parameter set \u0398", "startOffset": 34, "endOffset": 60}, {"referenceID": 28, "context": "It is certain that we can, as in the work of Spitkovsky et al. (2010b) and many bootstrapping approaches, employ only parser P .", "startOffset": 45, "endOffset": 71}, {"referenceID": 2, "context": "Second, it embodies a similar idea to co-training (Blum and Mitchell, 1998): P and R play roles as two views of the data.", "startOffset": 50, "endOffset": 75}, {"referenceID": 16, "context": "erative model proposed by Le and Zuidema (2014). Second, it embodies a similar idea to co-training (Blum and Mitchell, 1998): P and R play roles as two views of the data.", "startOffset": 26, "endOffset": 48}, {"referenceID": 12, "context": "However, Elman (1993) suggests that in some cases, learning should start by training simple models on small data and then gradually increase the model complexity and add more difficult data.", "startOffset": 9, "endOffset": 22}, {"referenceID": 28, "context": "This training scheme was successfully applied by Spitkovsky et al. (2010a) under the name: Baby Step.", "startOffset": 49, "endOffset": 75}, {"referenceID": 17, "context": "4 Le and Zuidema (2014)\u2019s Reranker", "startOffset": 2, "endOffset": 24}, {"referenceID": 8, "context": "To overcome the problem of sparsity, they introduced the inside-outside recursive neural network (IORNN) architecture that can estimate treegenerating models including those proposed by Eisner (1996) and Collins (2003a).", "startOffset": 186, "endOffset": 200}, {"referenceID": 6, "context": "To overcome the problem of sparsity, they introduced the inside-outside recursive neural network (IORNN) architecture that can estimate treegenerating models including those proposed by Eisner (1996) and Collins (2003a).", "startOffset": 204, "endOffset": 220}, {"referenceID": 11, "context": "Le and Zuidema (2014)\u2019s reranker employs the generative model proposed by Eisner (1996). Intuitively, this model is top-down: starting with ROOT,", "startOffset": 74, "endOffset": 88}, {"referenceID": 27, "context": "different from traditional RNNs (Socher et al., 2010) is that each tree node u caries two vectors: iu - the inner representation, represents the content of the", "startOffset": 32, "endOffset": 53}, {"referenceID": 9, "context": "are initially borrowed from Collobert et al. (2011)), the POS-tag and capitalisation feature.", "startOffset": 28, "endOffset": 52}, {"referenceID": 18, "context": "In our experiments, for phase 0, we choose the parser using an extension of the DMV model with stop-probability estimates computed on a large corpus proposed by Marecek and Straka (2013). This system has a moderate performance3", "startOffset": 161, "endOffset": 187}, {"referenceID": 21, "context": "For the other phases, we use the MSTParser4 (with the second-order feature mode) (McDonald and Pereira, 2006).", "startOffset": 81, "endOffset": 109}, {"referenceID": 27, "context": "4% DDA of Spitkovsky et al. (2013). For the other phases, we use the MSTParser4 (with the second-order feature mode) (McDonald and Pereira, 2006).", "startOffset": 10, "endOffset": 35}, {"referenceID": 17, "context": "Our system uses Le and Zuidema (2014)\u2019s reranker (Section 4.", "startOffset": 16, "endOffset": 38}, {"referenceID": 10, "context": "approximators (Cybenko, 1989), the reranker is capable of perfectly remembering all training parses.", "startOffset": 14, "endOffset": 29}, {"referenceID": 18, "context": "Because Marecek and Straka (2013)\u2019s parser does not distinguish training data from test data, we postulate S0 = S1.", "startOffset": 8, "endOffset": 34}, {"referenceID": 15, "context": "We then apply the standard pre-processing5 for unsupervised dependency parsing task (Klein and Manning, 2004): we strip off all empty sub-trees, punctuation, and terminals (tagged # and $) not pronounced where they appear; we then convert the remaining trees to dependencies using Collins\u2019s head rules (Collins, 2003b).", "startOffset": 84, "endOffset": 109}, {"referenceID": 8, "context": "We then apply the standard pre-processing5 for unsupervised dependency parsing task (Klein and Manning, 2004): we strip off all empty sub-trees, punctuation, and terminals (tagged # and $) not pronounced where they appear; we then convert the remaining trees to dependencies using Collins\u2019s head rules (Collins, 2003b).", "startOffset": 302, "endOffset": 318}, {"referenceID": 9, "context": "We initialise the IORNN with the 50-dim word embeddings from Collobert et al. (2011) 6 , and train it with the learning rate 0.", "startOffset": 61, "endOffset": 85}, {"referenceID": 18, "context": "Our improvements over the system\u2019s initialiser (Marecek and Straka, 2013) are 9.", "startOffset": 47, "endOffset": 73}, {"referenceID": 27, "context": "2% higher than Spitkovsky et al. (2013) on all sentences and up to length 10, respectively.", "startOffset": 15, "endOffset": 40}, {"referenceID": 18, "context": "Marecek and Straka (2013).", "startOffset": 0, "endOffset": 26}, {"referenceID": 13, "context": "cek and Straka, 2013), GGGPT (Gillenwater et al., 2011), and Harmonic (Klein and Manning, 2004).", "startOffset": 29, "endOffset": 55}, {"referenceID": 15, "context": ", 2011), and Harmonic (Klein and Manning, 2004).", "startOffset": 22, "endOffset": 47}, {"referenceID": 13, "context": "2013), the parser used in the previous experiments, (ii) GGGPT (Gillenwater et al., 2011)7 employing", "startOffset": 63, "endOffset": 89}, {"referenceID": 15, "context": "an extension of the DMV model and posterior regularization framework for training, and (iii) Harmonic, the harmonic initializer proposed by Klein and Manning (2004).", "startOffset": 140, "endOffset": 165}, {"referenceID": 18, "context": "We compare the quality of the treebank resulted in the end of phase 1 against the quality of the treebank given by the initialier Marecek and Straka (2013). Figure 5 shows precision (top) and recall (bottom)", "startOffset": 130, "endOffset": 156}, {"referenceID": 3, "context": "Only the work of Blunsom and Cohn (2010), whose resulting grammar rules can contain large tree frag-", "startOffset": 17, "endOffset": 41}, {"referenceID": 27, "context": "In addition, smoothing is given free: recursive neural nets are able to map \u2018similar\u2019 fragments onto close points (Socher et al., 2010) thus an unseen fragment tends to be mapped onto a point close to points corresponding to \u2018similar\u2019 seen fragments.", "startOffset": 114, "endOffset": 135}, {"referenceID": 28, "context": "Spitkovsky et al. (2013) also exploit lexical semantics but in a limited way, using a context-based polysemous unsuper-", "startOffset": 0, "endOffset": 25}, {"referenceID": 23, "context": "Naseem and Barzilay (2011)\u2019s system uses semantic cues from an out-of-domain annotated corpus, thus is not fully unsupervised.", "startOffset": 0, "endOffset": 27}, {"referenceID": 28, "context": "Our system is thus related to the works of Spitkovsky et al. (2013) and Tu and Honavar (2012).", "startOffset": 43, "endOffset": 68}, {"referenceID": 28, "context": "Our system is thus related to the works of Spitkovsky et al. (2013) and Tu and Honavar (2012). However, what we have proposed is more than that: IR is a general framework that we can have", "startOffset": 43, "endOffset": 94}, {"referenceID": 27, "context": "The results shown in Figure 4 suggest that if phase 0 uses a better parser which uses less expressive model and/or less external knowledge than our model, such as the one proposed by Spitkovsky et al. (2013), we can expect even a higher performance.", "startOffset": 183, "endOffset": 208}, {"referenceID": 3, "context": "The other systems, except Blunsom and Cohn (2010), however, might not benefit from using good existing parsers as initializers because their models are not significantly more expressive than others 8.", "startOffset": 26, "endOffset": 50}, {"referenceID": 17, "context": "ploying Marecek and Straka (2013)\u2019s unsupervised parser as the initialiser, the k-best MSTParser, and Le and Zuidema (2014)\u2019s reranker, achieved 1.", "startOffset": 8, "endOffset": 34}, {"referenceID": 17, "context": "ploying Marecek and Straka (2013)\u2019s unsupervised parser as the initialiser, the k-best MSTParser, and Le and Zuidema (2014)\u2019s reranker, achieved 1.", "startOffset": 102, "endOffset": 124}, {"referenceID": 17, "context": "ploying Marecek and Straka (2013)\u2019s unsupervised parser as the initialiser, the k-best MSTParser, and Le and Zuidema (2014)\u2019s reranker, achieved 1.8% DDA higher than the SOTA parser of Spitkovsky et al. (2013) on the WSJ corpus.", "startOffset": 102, "endOffset": 210}, {"referenceID": 22, "context": "Besides, taking into account the fast development of the word embedding research (Mikolov et al., 2013; Pennington et al., 2014), we will try different word embeddings.", "startOffset": 81, "endOffset": 128}, {"referenceID": 25, "context": "Besides, taking into account the fast development of the word embedding research (Mikolov et al., 2013; Pennington et al., 2014), we will try different word embeddings.", "startOffset": 81, "endOffset": 128}, {"referenceID": 17, "context": "In an experiment, we used the Marecek and Straka (2013)\u2019s parser as an initializer for the Gillenwater et al.", "startOffset": 30, "endOffset": 56}, {"referenceID": 13, "context": "In an experiment, we used the Marecek and Straka (2013)\u2019s parser as an initializer for the Gillenwater et al. (2011)\u2019s parser.", "startOffset": 91, "endOffset": 117}], "year": 2015, "abstractText": "We present a self-training approach to unsupervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms. Our approach, called \u2018iterated reranking\u2019 (IR), starts with dependency trees generated by an unsupervised parser, and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees. Our system achieves 1.8% accuracy higher than the stateof-the-part parser of Spitkovsky et al. (2013) on the WSJ corpus.", "creator": "TeX"}}}