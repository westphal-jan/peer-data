{"id": "1504.04788", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2015", "title": "Compressing Neural Networks with the Hashing Trick", "abstract": "As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.", "histories": [["v1", "Sun, 19 Apr 2015 04:24:15 GMT  (503kb,D)", "http://arxiv.org/abs/1504.04788v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["wenlin chen", "james t wilson", "stephen tyree", "kilian q weinberger", "yixin chen"], "accepted": true, "id": "1504.04788"}, "pdf": {"name": "1504.04788.pdf", "metadata": {"source": "META", "title": "Compressing Neural Networks with the Hashing Trick", "authors": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "emails": ["WENLINCHEN@WUSTL.EDU", "J.WILSON@WUSTL.EDU", "STYREE@NVIDIA.COM", "KILIAN@WUSTL.EDU", "CHEN@CSE.WUSTL.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in a phase, in which we are in which we are in a phase, in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in a phase, in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we in which we are in which we are in which we in which we are in which we in which we are in which we are in which we in which we are in which we in which we are in which we are in which we in which we in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we are we in which we are in which we in which we are in which we in which we are in which we in which we in which we in which we are in which we are in which we in which we in which we are in which we are in which we in which we in which we are in which we are in which we in which we in which we are in which we are in which we in which we are in which we in which we are in which we are in which we in which we are in which we in which we are in which we are in which we in which we are in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we are in which we in which we in which we in which we in which we in which we are in which we are in which we in which we in which we in which we in which"}, {"heading": "2. Feature Hashing", "text": "Feature-hashing (or the hashing trick) (Weinberger et al., 2009; Shi et al., 2009) is a technique for mapping high-dimensional text documents directly into word pockets (Salton & Buckley, 1988) Vectors that otherwise require the use of memory-intensive dictionaries to store indexes that correspond to specific input terms. Formally, an input vector x is mapped into a feature space with a mapping function: Rk \u2192 Rk, where k d. The mapping task is based on two (approximately uniform) hash functions h: h: N \u2192., k} and vice versa: N \u2192 {\u2212 1, + 1} and the kest dimension of the hated input x is defined as x."}, {"heading": "3. Notation", "text": "During this work we type vectors bold (x), scalars regular (C or b) and matrices bold (X.) Specific entries in vectors or matrices are scalars and follow the corresponding convention, i.e. the ith dimension of the vector x is xi and the (i, j) th entry of matrix V is Vij.Feed Forward Neural Networks. We define the forward propagation of the 'th layer in a neural network as a' + 1i = f (z '+ 1 i), where z' + 1 i = n '\u2211 j = 0 V' ija 'j, (2) where V' is the (virtual) weight matrix in the 'th layer. The vectors z', a 'Rn' denote the activation units before and after transformation by the transition function f (\u00b7). Typical activation functions are rectifier linear unit (ReLU) (Nair & Cun, 2010 or Lesigton) (2012)."}, {"heading": "4. HashedNets", "text": "In this section, we introduce HashedNets, a novel variant of neural networks with drastically reduced model sizes (and memory requirements). First, we present our approach as a method of random weight distribution between network connections, and then describe how to simplify it with the hashing trick to avoid additional memory consumption."}, {"heading": "4.1. Random weight sharing", "text": "In a fully networked neural network, there are (n '+ 1) \u00b7 n' + 1 weighted connections between two layers, each with a corresponding free parameter in the weight matrix V '. We assume a finite memory budget per layer that cannot be exceeded, K' (n '+ 1) \u00b7 n' + 1. The obvious solution is to adjust the neural network within the budget by reducing the number of nodes n ', n' + 1 in layers, '+ 1, or by reducing the bit accuracy of the weight matrices (Courbariaux et al., 2014). However, if K' is sufficiently small, both approaches reduce the neural network's ability to generalize values (see Section 6). Instead, we propose an alternative: We keep the size of V 'unaffected, but reduce its effective storage requirement by weight partitioning. We only allow precisely K' different weight proportions within net vector that we store in a weight component K '."}, {"heading": "4.2. Hashed Neural Nets (HashedNets)", "text": "A naive implementation of random weight allocation can be achieved trivially by maintaining a secondary matrix consisting of the group allocation of each connection. Unfortunately, this explicit representation sets an undesirable limit to potential memory savings. We propose to implement the random weight allocation using the hashing trick. In this way, the combined weight of each connection is determined by a hash function that does not require storage costs with the model. Specifically, we assign V'ij an element of w 'indexed by a hash function h h' (i, j), as follows: V'ij = w'h '(i, j), (3) where the (approximately uniform) hash function h h' (\u00b7, \u00b7) maps a key (i, j) to a natural number within {1,...., K '."}, {"heading": "4.3. Feature hashing versus weight sharing", "text": "This section focuses on a single level, in which there is a simplification of notation. (...) We will characterize the input activation (...) as a way, as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...), as they (...)."}, {"heading": "4.4. Training HashedNets", "text": "Training HashedNets is tantamount to training a standard neural network with equality constraints for the virtual weight distribution. Here, we show how to (a) calculate the output of a hash layer during the feed phase, (b) propagate gradients from the output layer back to the input layer, and (c) calculate the gradient over the divided weights w '. We use dedicated hash functions between the layers' and '+ 1 and designate them as h' and '. Output. add the hash functions h' (\u00b7, \u00b7) and the weight vectors w '(\u00b7) and the weight vectors w' to the feed update (2), the following propagation rule results: a '+ 1i = f' j w'h '(i, j)."}, {"heading": "5. Related Work", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "6. Experimental Results", "text": "In fact, most of them will be able to move to another world, to move to another world, to move to another world, to move to another world."}, {"heading": "7. Conclusion", "text": "HashedNets exploit this feature to create neural networks with \"virtual\" connections that seem to exceed the memory limits of the trained model, which can have surprising effects. Figure 4 in Section 6 shows that the neural network test error can drop almost 50%, from 3% to 1.61%, by \"practically\" increasing the number of weights by a factor of 8. Although the collisions (or weight distribution) could serve as a form of regulation, we can probably safely ignore this effect, since both networks (with and without expansion) were also regulated with suspensions (Srivastava et al., 2014) and the hyperparameters were carefully refined by Bayesian optimization. So why should additional virtual layers help? One answer is that they really increase the expressivity of the neural network."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Agarwal", "Alekh", "Chapelle", "Olivier", "Dud\u0131\u0301k", "Miroslav", "Langford", "John"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Do deep nets really need to be deep", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": "In NIPS, pp", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural Networks for Pattern Recognition", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M.", "year": 1995}, {"title": "Sparse feature learning for deep belief networks", "author": ["Boureau", "Y-lan", "Cun", "Yann L"], "venue": "In NIPS, pp", "citeRegEx": "Boureau et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2008}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["Chen", "Minmin", "Weinberger", "Kilian Q", "Sha", "Fei", "Bengio", "Yoshua"], "venue": "In ICML, pp", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Augmented smartphone applications through clone cloud execution", "author": ["Chun", "Byung-Gon", "Maniatis", "Petros"], "venue": "In HotOS,", "citeRegEx": "Chun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chun et al\\.", "year": 2009}, {"title": "Highperformance neural networks for visual object classification", "author": ["Cire\u015fan", "Dan C", "Meier", "Ueli", "Masci", "Jonathan", "Gambardella", "Luca M", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1102.0183,", "citeRegEx": "Cire\u015fan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak"], "venue": "In AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Deep learning with cots hpc systems", "author": ["Coates", "Adam", "Huval", "Brody", "Wang", "Tao", "Wu", "David", "Catanzaro", "Bryan", "Andrew", "Ng"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Low precision storage for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "de Freitas", "Nando"], "venue": "In NIPS,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1310.1531,", "citeRegEx": "Donahue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "Small statistical models by random feature mixing", "author": ["Ganchev", "Kuzman", "Dredze", "Mark"], "venue": "In Workshop on Mobile NLP at ACL,", "citeRegEx": "Ganchev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2008}, {"title": "Bayesian optimization with inequality constraints", "author": ["Gardner", "Jacob", "Kusner", "Matt", "Weinberger", "Kilian", "Cunningham", "John"], "venue": "In ICML,", "citeRegEx": "Gardner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Deep sparse rectifier networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In ICML, pp", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "A-R", "Hinton", "Geoffrey"], "venue": "In ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish"], "venue": "arXiv preprint arXiv:1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "NIPS workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Client vs. server architecture: Why google voice search is also much faster than siri @ONLINE", "author": ["A.W. Kosner"], "venue": "URL http://tinyurl.com/ c2d2otr", "citeRegEx": "Kosner,? \\Q2012\\E", "shortCiteRegEx": "Kosner", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["Larochelle", "Hugo", "Erhan", "Dumitru", "Courville", "Aaron C", "Bergstra", "James", "Bengio", "Yoshua"], "venue": "In ICML, pp", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Le", "Quoc V"], "venue": "In ICASSP,", "citeRegEx": "Le and V.,? \\Q2013\\E", "shortCiteRegEx": "Le and V.", "year": 2013}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A", "Howard", "Richard E", "Jackel", "Lawrence D"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "A low-power processor with configurable embedded machine-learning accelerators for high-order and adaptive analysis of medicalsensor signals. Solid-State Circuits", "author": ["Lee", "Kyong Ho", "Verma", "Naveen"], "venue": "IEEE Journal of,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["Mohamed", "Abdel-rahman", "Sainath", "Tara N", "Dahl", "George", "Ramabhadran", "Bhuvana", "Hinton", "Geoffrey E", "Picheny", "Michael A"], "venue": "In ICASSP,", "citeRegEx": "Mohamed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In ICML, pp", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Simplifying neural networks by soft weight-sharing", "author": ["Nowlan", "Steven J", "Hinton", "Geoffrey E"], "venue": "Neural computation,", "citeRegEx": "Nowlan et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Nowlan et al\\.", "year": 1992}, {"title": "Generalization of back-propagation to recurrent neural networks", "author": ["Pineda", "Fernando J"], "venue": "Physical review letters,", "citeRegEx": "Pineda and J.,? \\Q1987\\E", "shortCiteRegEx": "Pineda and J.", "year": 1987}, {"title": "Cnn features off-theshelf: an astounding baseline for recognition", "author": ["Razavian", "Ali Sharif", "Azizpour", "Hossein", "Sullivan", "Josephine", "Carlsson", "Stefan"], "venue": "In CVPR Workshop,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Learning ordered representations with nested dropout", "author": ["Rippel", "Oren", "Gelbart", "Michael A", "Adams", "Ryan P"], "venue": "arXiv preprint arXiv:1402.0915,", "citeRegEx": "Rippel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2014}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Salton", "Gerard", "Buckley", "Christopher"], "venue": "Information processing & management,", "citeRegEx": "Salton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1988}, {"title": "Speech recognition for mobile devices at google", "author": ["Schuster", "Mike"], "venue": "In PRICAI 2010: Trends in Artificial Intelligence,", "citeRegEx": "Schuster and Mike.,? \\Q2010\\E", "shortCiteRegEx": "Schuster and Mike.", "year": 2010}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Hash kernels for structured data", "author": ["Shi", "Qinfeng", "Petterson", "James", "Dror", "Gideon", "Langford", "John", "Smola", "Alex", "S.V.N. Vishwanathan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Simard", "Patrice Y", "Steinkraus", "Dave", "Platt", "John C"], "venue": "In ICDAR,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Feature hashing for large scale multitask learning", "author": ["Weinberger", "Kilian", "Dasgupta", "Anirban", "Langford", "John", "Smola", "Alex", "Attenberg", "Josh"], "venue": "In ICML,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1301.3557,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In ECCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "These include object classification (Krizhevsky et al., 2012; Sermanet et al., 2013), speech recognition (Hinton et al.", "startOffset": 36, "endOffset": 84}, {"referenceID": 39, "context": "These include object classification (Krizhevsky et al., 2012; Sermanet et al., 2013), speech recognition (Hinton et al.", "startOffset": 36, "endOffset": 84}, {"referenceID": 44, "context": ", 2012), image caption generation (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014) and domain adaptation (Glorot et al.", "startOffset": 34, "endOffset": 82}, {"referenceID": 8, "context": "As data sets increase in size, so do the number of parameters in these neural networks in order to absorb the enormous amount of supervision (Coates et al., 2013).", "startOffset": 141, "endOffset": 162}, {"referenceID": 8, "context": "Increasingly, these networks are trained on industrial-sized clusters (Le, 2013) or high-performance graphics processing units (GPUs) (Coates et al., 2013).", "startOffset": 134, "endOffset": 155}, {"referenceID": 23, "context": "This approach is problematic, as it only works when sufficient bandwidth is available and incurs artificial delays through network traffic (Kosner, 2012).", "startOffset": 139, "endOffset": 153}, {"referenceID": 11, "context": "Recent work by Denil et al. (2013) demonstrates that there is a surprisingly large amount of redundancy among the weights of neural networks.", "startOffset": 15, "endOffset": 35}, {"referenceID": 11, "context": "Recent work by Denil et al. (2013) demonstrates that there is a surprisingly large amount of redundancy among the weights of neural networks. The authors show that a small subset of the weights are sufficient to reconstruct the entire network. They exploit this by training low-rank decompositions of the weight matrices. Ba & Caruana (2014) show that deep neural networks can be successfully compressed http://en.", "startOffset": 15, "endOffset": 342}, {"referenceID": 10, "context": "Courbariaux et al. (2014) train neural networks with reduced bit precision, and, long predating this work, LeCun et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 10, "context": "Courbariaux et al. (2014) train neural networks with reduced bit precision, and, long predating this work, LeCun et al. (1989) investigated dropping unimportant weights in neural networks.", "startOffset": 0, "endOffset": 127}, {"referenceID": 45, "context": "Our parameter hashing is akin to prior work in feature hashing (Weinberger et al., 2009; Shi et al., 2009; Ganchev & Dredze, 2008) and is similarly fast and requires no additional memory overhead.", "startOffset": 63, "endOffset": 130}, {"referenceID": 40, "context": "Our parameter hashing is akin to prior work in feature hashing (Weinberger et al., 2009; Shi et al., 2009; Ganchev & Dredze, 2008) and is similarly fast and requires no additional memory overhead.", "startOffset": 63, "endOffset": 130}, {"referenceID": 28, "context": "The backpropagation algorithm (LeCun et al., 2012) can naturally tune the hash bucket parameters and take into account the random weight sharing within the neural network architecture.", "startOffset": 30, "endOffset": 50}, {"referenceID": 11, "context": "Under the same memory constraint, HashedNets have more adjustable free parameters than the lowrank decomposition methods suggested by Denil et al. (2013), leading to smaller drops in descriptive power.", "startOffset": 134, "endOffset": 154}, {"referenceID": 28, "context": ", 2014), activation functions (Glorot et al., 2011a; LeCun et al., 2012), or weight sparsity (Coates et al.", "startOffset": 30, "endOffset": 72}, {"referenceID": 7, "context": ", 2012), or weight sparsity (Coates et al., 2011).", "startOffset": 28, "endOffset": 49}, {"referenceID": 45, "context": "Feature hashing (or the hashing trick) (Weinberger et al., 2009; Shi et al., 2009) is a technique to map high-dimensional text documents directly into bag-ofword (Salton & Buckley, 1988) vectors, which would otherwise require use of memory consuming dictionaries for storage of indices corresponding with specific input terms.", "startOffset": 39, "endOffset": 82}, {"referenceID": 40, "context": "Feature hashing (or the hashing trick) (Weinberger et al., 2009; Shi et al., 2009) is a technique to map high-dimensional text documents directly into bag-ofword (Salton & Buckley, 1988) vectors, which would otherwise require use of memory consuming dictionaries for storage of indices corresponding with specific input terms.", "startOffset": 39, "endOffset": 82}, {"referenceID": 40, "context": "This problem is less severe for sparse data sets and can be counteracted through multiple hashing (Shi et al., 2009) or larger hash tables (Weinberger et al.", "startOffset": 98, "endOffset": 116}, {"referenceID": 45, "context": ", 2009) or larger hash tables (Weinberger et al., 2009).", "startOffset": 30, "endOffset": 55}, {"referenceID": 45, "context": "The second hash function, \u03be, guarantees that inner products are unbiased in expectation (Weinberger et al., 2009); that is,", "startOffset": 88, "endOffset": 113}, {"referenceID": 45, "context": "Finally, Weinberger et al. (2009) also show that the hashing trick can be used to learn multiple classifiers within the same hashed space.", "startOffset": 9, "endOffset": 34}, {"referenceID": 28, "context": "Typical activation functions are rectifier linear unit (ReLU) (Nair & Hinton, 2010), sigmoid or tanh (LeCun et al., 2012).", "startOffset": 101, "endOffset": 121}, {"referenceID": 10, "context": "The obvious solution is to fit the neural network within budget by reducing the number of nodes n, n in layers `, `+ 1 or by reducing the bit precision of the weight matrices (Courbariaux et al., 2014).", "startOffset": 175, "endOffset": 201}, {"referenceID": 45, "context": "Alternatively and more in line with previous work (Weinberger et al., 2009), we may interpret HashedNets in terms of feature hashing.", "startOffset": 50, "endOffset": 75}, {"referenceID": 45, "context": "For the same reasons we multiply (3) by the sign factor \u03be(i, j) for parameterizing V (Weinberger et al., 2009):", "startOffset": 85, "endOffset": 110}, {"referenceID": 45, "context": "Weinberger et al. (2009) introduce an additional sign factor \u03be(i, j) to remove the bias of hashed inner-products due to collisions.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": ", 2011a) or through specialized regularization (Chen et al., 2014; Boureau et al., 2008).", "startOffset": 47, "endOffset": 88}, {"referenceID": 3, "context": ", 2011a) or through specialized regularization (Chen et al., 2014; Boureau et al., 2008).", "startOffset": 47, "endOffset": 88}, {"referenceID": 36, "context": "As pointed out in Shi et al. (2009) and Weinberger et al.", "startOffset": 18, "endOffset": 36}, {"referenceID": 36, "context": "As pointed out in Shi et al. (2009) and Weinberger et al. (2009), feature hashing is most effective on sparse feature vectors since the number of hash collisions is minimized.", "startOffset": 18, "endOffset": 65}, {"referenceID": 10, "context": "All weights can be stored with low bit precision (Courbariaux et al., 2014; Gupta et al., 2015), edges could be removed (Cire\u015fan et al.", "startOffset": 49, "endOffset": 95}, {"referenceID": 20, "context": "All weights can be stored with low bit precision (Courbariaux et al., 2014; Gupta et al., 2015), edges could be removed (Cire\u015fan et al.", "startOffset": 49, "endOffset": 95}, {"referenceID": 6, "context": ", 2015), edges could be removed (Cire\u015fan et al., 2011) and HashedNets can be trained on the outputs of larger networks (Ba & Caruana, 2014) \u2014 yielding further reductions in memory requirements.", "startOffset": 32, "endOffset": 54}, {"referenceID": 24, "context": "Deep neural networks have achieved great progress on a wide variety of real-world applications, including image classification (Krizhevsky et al., 2012; Donahue et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2014), object detection (Girshick et al.", "startOffset": 127, "endOffset": 220}, {"referenceID": 13, "context": "Deep neural networks have achieved great progress on a wide variety of real-world applications, including image classification (Krizhevsky et al., 2012; Donahue et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2014), object detection (Girshick et al.", "startOffset": 127, "endOffset": 220}, {"referenceID": 39, "context": "Deep neural networks have achieved great progress on a wide variety of real-world applications, including image classification (Krizhevsky et al., 2012; Donahue et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2014), object detection (Girshick et al.", "startOffset": 127, "endOffset": 220}, {"referenceID": 16, "context": ", 2013; Zeiler & Fergus, 2014), object detection (Girshick et al., 2014; Vinyals et al., 2014), image retrieval (Razavian et al.", "startOffset": 49, "endOffset": 94}, {"referenceID": 44, "context": ", 2013; Zeiler & Fergus, 2014), object detection (Girshick et al., 2014; Vinyals et al., 2014), image retrieval (Razavian et al.", "startOffset": 49, "endOffset": 94}, {"referenceID": 35, "context": ", 2014), image retrieval (Razavian et al., 2014), speech recognition (Hinton et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 19, "context": ", 2014), speech recognition (Hinton et al., 2012; Graves et al., 2013; Mohamed et al., 2011), and text representation (Mikolov et al.", "startOffset": 28, "endOffset": 92}, {"referenceID": 31, "context": ", 2014), speech recognition (Hinton et al., 2012; Graves et al., 2013; Mohamed et al., 2011), and text representation (Mikolov et al.", "startOffset": 28, "endOffset": 92}, {"referenceID": 30, "context": ", 2011), and text representation (Mikolov et al., 2013).", "startOffset": 33, "endOffset": 55}, {"referenceID": 41, "context": "Arguably the most popular method is the widely used convolutional neural network (Simard et al., 2003).", "startOffset": 81, "endOffset": 102}, {"referenceID": 26, "context": "Instead of sharing weights, LeCun et al. (1989) introduce \u201coptimal brain damage\u201d to directly drop unimportant weights.", "startOffset": 28, "endOffset": 48}, {"referenceID": 6, "context": "Cire\u015fan et al. (2011) demonstrate in their experiments that randomly removing connections leads to superior empirical performance, which shares the same spirit of HashedNets.", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": "16-bit fixed-point representation (Gupta et al., 2015) for a compression factor of 14 over double-precision floating point).", "startOffset": 34, "endOffset": 54}, {"referenceID": 11, "context": "A recent study by Denil et al. (2013) demonstrates significant redundancy in neural network parameters by directly learning a low-rank decomposition of the weight matrix within each layer.", "startOffset": 18, "endOffset": 38}, {"referenceID": 11, "context": "A recent study by Denil et al. (2013) demonstrates significant redundancy in neural network parameters by directly learning a low-rank decomposition of the weight matrix within each layer. They demonstrate that networks composed of weights recovered from the learned decompositions are only slightly less accurate than networks with all weights as free parameters, indicating heavy overparametrization in full weight matrices. A follow-up work by Denton et al. (2014) uses a similar technique to speed up test-time evaluation of convolutional neural networks.", "startOffset": 18, "endOffset": 468}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network.", "startOffset": 8, "endOffset": 29}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network.", "startOffset": 8, "endOffset": 53}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network. Specifically, Hinton et al. (2014) and Ba & Caruana (2014) train a large network on the original training labels, then learn a much smaller \u201cdistilled\u201d model on a weighted combination of the original labels and the (softened) softmax output of the larger model.", "startOffset": 8, "endOffset": 232}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network. Specifically, Hinton et al. (2014) and Ba & Caruana (2014) train a large network on the original training labels, then learn a much smaller \u201cdistilled\u201d model on a weighted combination of the original labels and the (softened) softmax output of the larger model.", "startOffset": 8, "endOffset": 256}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network. Specifically, Hinton et al. (2014) and Ba & Caruana (2014) train a large network on the original training labels, then learn a much smaller \u201cdistilled\u201d model on a weighted combination of the original labels and the (softened) softmax output of the larger model. The authors show that the distilled model has better generalization ability than a model trained on just the labels. In our experimental results, we show that our approach is complementary by learning HashedNets with soft targets. Rippel et al. (2014) propose a novel dropout method, nested dropout, to give an order of importance for hidden neurons.", "startOffset": 8, "endOffset": 711}, {"referenceID": 45, "context": "With the help of feature hashing (Weinberger et al., 2009), Vowpal Wabbit, a large-scale learning system, is able to scale to terafeature datasets (Agarwal et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 0, "context": ", 2009), Vowpal Wabbit, a large-scale learning system, is able to scale to terafeature datasets (Agarwal et al., 2014).", "startOffset": 96, "endOffset": 118}, {"referenceID": 25, "context": "Datasets consist of the original MNIST handwritten digit dataset, along with four challenging variants (Larochelle et al., 2007).", "startOffset": 103, "endOffset": 128}, {"referenceID": 25, "context": "In addition, we include two binary image classification datasets: CONVEX and RECT (Larochelle et al., 2007).", "startOffset": 82, "endOffset": 107}, {"referenceID": 6, "context": "Random Edge Removal (RER) (Cire\u015fan et al., 2011) reduces the total number of model parameters by randomly removing weights prior to training.", "startOffset": 26, "endOffset": 48}, {"referenceID": 11, "context": "Low-Rank Decomposition (LRD) (Denil et al., 2013) decomposes the weight matrix into two low-rank matrices.", "startOffset": 29, "endOffset": 49}, {"referenceID": 21, "context": "In a similar manner, we examine Dark Knowledge (DK) (Hinton et al., 2014; Ba & Caruana, 2014) by training a distilled model to optimize the cross entropy with both the original labels and soft targets generated by the corresponding full neural network (compression factor 1).", "startOffset": 52, "endOffset": 93}, {"referenceID": 9, "context": "HashedNets and all accompanying baselines were implemented using Torch7 (Collobert et al., 2011) and run on NVIDIA GTX TITAN graphics cards with 2688 cores and 6GB of global memory.", "startOffset": 72, "endOffset": 96}, {"referenceID": 10, "context": "We use 32 bit precision throughout but note that the compression rates of all methods may be improved with lower precision (Courbariaux et al., 2014; Gupta et al., 2015).", "startOffset": 123, "endOffset": 169}, {"referenceID": 20, "context": "We use 32 bit precision throughout but note that the compression rates of all methods may be improved with lower precision (Courbariaux et al., 2014; Gupta et al., 2015).", "startOffset": 123, "endOffset": 169}, {"referenceID": 42, "context": "Hyperparameters are selected for all algorithms with Bayesian optimization (Snoek et al., 2012) and hand tuning on 20% validation splits of the training sets.", "startOffset": 75, "endOffset": 95}, {"referenceID": 9, "context": "HashedNets and all accompanying baselines were implemented using Torch7 (Collobert et al., 2011) and run on NVIDIA GTX TITAN graphics cards with 2688 cores and 6GB of global memory. We use 32 bit precision throughout but note that the compression rates of all methods may be improved with lower precision (Courbariaux et al., 2014; Gupta et al., 2015). We verify all implementations by numerical gradient checking. Models are trained via stochastic gradient descent (minibatch size of 50) with dropout and momentum. ReLU is adopted as the activation function for all models. Hyperparameters are selected for all algorithms with Bayesian optimization (Snoek et al., 2012) and hand tuning on 20% validation splits of the training sets. We use the open source Bayesian Optimization MATLAB implementation \u201cbayesopt.m\u201d from Gardner et al. (2014).3", "startOffset": 73, "endOffset": 841}, {"referenceID": 11, "context": "Prior work shows that weights learned in neural networks can be highly redundant (Denil et al., 2013).", "startOffset": 81, "endOffset": 101}], "year": 2015, "abstractText": "As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb everincreasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.", "creator": "LaTeX with hyperref package"}}}