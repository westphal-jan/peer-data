{"id": "1504.05477", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2015", "title": "Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition", "abstract": "We re-analyze Simultaneous Power Iteration and the Block Lanczos methods, two classical iterative algorithms for the singular value decomposition (SVD). We are interested in convergence bounds that *do not depend* on properties of the input matrix (e.g. singular value gaps).", "histories": [["v1", "Tue, 21 Apr 2015 15:48:44 GMT  (34kb)", "http://arxiv.org/abs/1504.05477v1", null], ["v2", "Sat, 6 Jun 2015 23:43:50 GMT  (50kb)", "http://arxiv.org/abs/1504.05477v2", null], ["v3", "Wed, 1 Jul 2015 03:55:11 GMT  (50kb)", "http://arxiv.org/abs/1504.05477v3", null], ["v4", "Fri, 30 Oct 2015 19:35:08 GMT  (54kb)", "http://arxiv.org/abs/1504.05477v4", "Neural Information Processing Systems 2015"]], "reviews": [], "SUBJECTS": "cs.DS cs.LG cs.NA", "authors": ["cameron musco", "christopher musco"], "accepted": true, "id": "1504.05477"}, "pdf": {"name": "1504.05477.pdf", "metadata": {"source": "CRF", "title": "Stronger Approximate Singular Value Decomposition via the Block Lanczos and Power Methods", "authors": ["Cameron Musco"], "emails": ["cnmusco@mit.edu", "cpmusco@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 4.05 477v 1 [cs.D S] 2 1Simultaneous iteration is known to yield an approximation of low rank within (1 + 1) the optimum iteration for spectral standard errors in O (1 / 2) iterations. We reinforce this result and prove that it finds approximate major components that are very close to those of an exact SVD. Our work bridges a gap between classical analysis, which can show similar limitations but critically depends on singular value gaps, and more recent work that focuses only on low rank approximations. In addition, we extend our limitations to the Block Lanczos method, which we show obtains the same approximation guarantees in only O-value (1 / 2) iterations, which results in the fastest known algorithm for spectral standard approximation of low rank and major component approximation. Despite its popularity, cranial methods did not come to independent subspace analysis as early as Lancylow and"}, {"heading": "1 Introduction", "text": "The SVD is also responsible for the main component A (SVD). < A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component.A (SVD).A (SVD) is responsible for the main component.A (SVD).A (SVD) is responsible for the main component.A (SVD).A (SVD) is responsible for the main component.A (SVD).A (SVD) is responsible for the main component A (SVD).VD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD) is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD) is responsible for the main component A (SVD is responsible for the main component A (SVD).A (SVD) is responsible for the main component A (SVD) is responsible for the main component A (SVD is responsible for the main component (SVD).A (SVD)."}, {"heading": "1.1 Comparing Guarantees", "text": "The Frobenius Standard Guarantee (1) is well researched and there are now algorithms that (1 + 1) achieve errors in O (nnz (A)) time, plus lower terms depending on A, where nnz (A) is the number of non-zero entries in A [CW13]. However, as in previous work [RST09, HMT11, SKT14] Frobenius Standard Error is often insufficient, especially in data analysis and machine learning applications. While A has a \"heavy tail\" of singular values, as is common with noisy data, A \u2212 Ak \u00b2 2F = \u2211 r i = k \u2212 1 \u00b2 i can be huge, potentially greater than even A \u2212 s largest singular value. These renders (1) are meaningless as Z does not need to align with any large singular values to obtain good multiplicative values."}, {"heading": "1.2 Our Contributions", "text": "s methods, combined with simple randomized initializations, for guarantees (1), (2), and (3). Algorithm 1: Simultaneous Iteration Input: A-Rn \u00b7 d, Error (0, 1), Error (0, 1), Error (1), Error (0, 1), Error (2), Error (3), Error (3), Error (3), Error (3), Error (3), Error (3), Error (3), Error (3), Error (3), Error (3), Error (3), Error (3), Error (3), Error (3)."}, {"heading": "2 Background and Intuition", "text": "The aim of this section is to 1) provide background information on algorithms for approximate decomposition of singular values and 2) provide intuition for simultaneous power iteration and the Block Lanczos method to explain why they can provide strong, gapless error guarantees."}, {"heading": "2.1 Frobenius Norm Error", "text": "The advances in the algorithms for Frobenius Normal Errors Low Rank Approximation (06) have been the most significant, and the work in this direction goes back to the strong rank revealing QR factorizations of Gu and Eisenstat [GE96], which give deterministic algorithms running in approximately O (ndk) time vs. O (ndmin (n, d) 1 for a complete SVD and roughly achieve the constant factor Frobenius Norm Error. Randomization has recently been applied to achieve even faster algorithms with (1 +) errors. The paradigm is to calculate a linear sketch of A in very few dimensions, using either a slit sampling matrix or Johnson-Lindenstrauss random projection matrix to calculate even faster algorithms with (1 +) errors."}, {"heading": "2.2 Spectral Norm Error", "text": "The answer is yes, and indeed this is exactly the intuition behind the Famed Power method."}, {"heading": "2.3 Beating Simultaneous Iteration with Lanczos", "text": "Numerous studies point to the possibility of beating simultaneous iteration with the Block Lanczos method (CD74, GU77, GLO81), a well-studied variant of Lanczos iteration [Lan50], which is the canonical Krylov subspace method for large singular value problems. (RST09), [HMST11] and [Hal12] surprisingly point out and experimentally confirm the potential of the Randomized Block Lanczos (Algorithm 2) for beating simultaneous iteration for low approximation. (ME11) also points to the difficulty of using state-of-the-art Lanczos implementations [Lar01, Lar05] with simultaneous iteration.The intuition behind Block Lanczos corresponds to that of many accelerated iterative methods. Simply put, there are better polynomials than Aq for denominating the lower polynomena variations, especially we can use."}, {"heading": "2.4 Per Vector Error", "text": "Achieving the pro-vector guarantee of (3) requires a more sophisticated understanding of how simultaneous iteration and block lancets denounce the spectrum of A. The analysis for low-rank spectral standard approximation is based on the fact that Aq and p \u221a q (A) inflate any singular value corresponding to these large singular values."}, {"heading": "3 Preliminaries", "text": "Before proceeding to the full technical analysis, we provide an overview of the required results from linear algebra, polynomial approximation, and low-level randomized approximation."}, {"heading": "3.1 Singular Value Decomposition and Low Rank Approximation", "text": "As already mentioned, the single value of the A-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "3.2 Other Linear Algebra Tools", "text": "In the course of this work, we will use span (M) to denote the column span of matrix M and say that a matrix Q is an orthonormous basis for the column span of M if Q has orthonormous columns and QQ M = M. That is, if the columns of M and N have the same dimensions and MN = 0, then QQ is the orthogonal projection matrix on the span of Q. (QQ) = QIQ. If two matrices have M + N and MN = 0, then it is the orthogonal projection matrix of M 2F + N 2F. This pythagorean theorem results from the fact that the matrices M + N 2F = tr (M + N) (M + N) kk lie."}, {"heading": "3.3 Randomized Low Rank Approximation", "text": "As already mentioned, our proofs are based on well-known sketch-based algorithms to approximate the low rank with the Frobenius standard error. A short proof for the following Lemma is in Appendix A: Lemma 4 (Frobenius Standard Low Rank Approximation). Let us take any A-Rn-d and Rd-k values where the entries are independent Gaussians drawn from N (0, 1). If we allow Z as the orthonorthonormal basis for the span (A-Rn), then with a probability of at least 99 / 100 for any fixed constant c, i.e. A-ZZ A-2F \u2264 c-dk-A-Ak-2F."}, {"heading": "3.4 Chebyshev Polynomials", "text": "As described in Section 2.3, our proof also requires polynomials to denounce the tail of A. As is usual with Krylov subspace methods, we use a variation of the Chebyshev polynomials. The proof for the following Lemma is referred to in Appendix A.Lemma 5 (Chebyshev Minimizing Polynomial). At a specified value \u03b1 > 0, Gap \u03b3 (0, 1] and Grade q \u2265 1, there is a degree q Polynomial p (x), so that: 1. p (((1 + \u03b3) \u03b1) = (1 + \u03b3) \u03b12. p (x) \u2265 x for all x \u2265 (1 + \u03b3) \u03b13. p (x) \u2264 \u03b13. p (\u03b1 2q \u0430 - 1 for all x [0, \u03b1]"}, {"heading": "4 Implementation and Runtimes", "text": "In this section, we briefly discuss runtime and implementation considerations for algorithms 1 and 2, our randomized variants of simultaneous power iteration, and the Block Lanczos methods."}, {"heading": "4.1 Simulatenous Iteration", "text": "Algorithm 1 can be modified in various ways, so it can be replaced by a random character matrix, or by any matrix that reaches the warranty of Lemma 4. Besides, it can be chosen with p > k columns. We will discuss in detail how this approach can lead to improved accuracy in Section 5.3. This is necessary to obtain per vector guarantees for an approximate PCA. However, if we are only interested in calculating a near-optimal approximation of lower order, we can simply Z = Q. Projection from A to QU-k is equivalent to projection of Q, since these two matrices have the same spans.Theorem 6 (Simultaneous Iteration Runtime) e.g."}, {"heading": "4.2 Block Lanczos", "text": "As with simultaneous iteration, we can replace \"Q\" with any matrix that achieves the Q-Q guarantee and use p > k columns to improve accuracy (see Section 5.3). In addition, Q can be calculated in various ways. In the traditional Lanczos block algorithm, one begins by calculating an orthonormal base for \"K,\" the first block in the crylov subspace. Basics for subsequent blocks are calculated from previous blocks with a three-dimensional repetition that ensures that \"Q\" AA \"is tridiagonal, with blocks of k size [GU77]. This technique can be useful when\" qk \"is large, as it is faster to calculate the uppermost singular vectors of a three-dimensional matrix. However, calculating\" Q \"using a repetition can introduce a number of stability problems, and additional steps may be required to ensure that the matrix remains orthogonal."}, {"heading": "5 Error Bounds", "text": "We now prove that both algorithms 1 and 2 provide a basis Z that yields a relative error Frobenius (1) and a spectral standard (2) an approximation error of low order as well as the guarantees per vector (3)."}, {"heading": "5.1 Main Approximation Lemma", "text": "Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition:: Definition: Definition:: Definition:: Definition:: Definition:: Definition:: Definition:: Definition::: Definition::: Definition:: Definition:::: Definition::: Definition:::: Definition:::: Definition:::: Definition::::: Definition:::::: Definition:::: Definition::::::: Definition:::::::: Definition:::: Definition::::: Definition::: Definition::::::: Definition::::::::: Definition::::::::::::::::: Definition::"}, {"heading": "5.2 Error Bounds for Simultaneous Iteration and Block Lanczos", "text": "With Lemma 9 in place, we can easily prove that Simultaneous Iteration and Block Lanczos both reach the low ranking and PCA guarantees (1), (2), and (3).Theorem 10 (Near Optimal Spectral Error Approximation).Probably 99 / 100, algorithms 1 and 2 yield Z satisfactory (2).\u2212 Theorem 10 (1 + 2). \u2212 A \u2212 Ak 2.Proof. Leave m the number of singular values with A (1 + 2).A \u2212 Z 2 Z 2 \u2212 Z 2 \u2264 A \u2212 Z 2 \u2264 A \u2212 A \u2212 Ak 2.Proof. Leave m the number of singular values with i (1 + 2)."}, {"heading": "5.3 Improved Convergence With Spectral Decay", "text": "In addition to the traditional simultaneous Iteration and Block Lanczos methods (algorithms 1 and 2), Iteration and Block Lanczos values (algorithms q and 2), our analysis refers to the joint modification of the algorithms with algorithms Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs Higgs"}, {"heading": "6 Acknowledgements", "text": "We thank David Woodruff, Aaron Sidford and Richard Peng for some valuable conversations. In addition, Michael Cohen has been very helpful in discussing many details of this project, including the ultimate form of Lemma 9. This work was supported in part by the NSF Graduate Research Fellowship No. 1122374, the AFOSR Scholarship FA9550-13-1-0042, the DARPA Scholarship FA8650-11-C-7192 and the NSF Center for Science of Information."}, {"heading": "A Appendix", "text": "It is not that we would be able to find a solution that would enable us to find a solution that would enable us to find a solution that would enable us to find a solution that would enable us to find a solution that would enable us to find a solution that would enable us to find a solution that would enable us to find a solution that would enable us to find a solution that would enable us to find a solution that would enable us to find a solution that we would be able to find a solution."}], "references": [{"title": "Das verfahren der treppeniteration und verwandte verfahren zur l\u00f6sung algebraischer eigenwertprobleme", "author": ["Friedrich L. Bauer"], "venue": "Zeitschrift fu\u0308r angewandte Mathematik und Physik ZAMP,", "citeRegEx": "Bauer.,? \\Q1957\\E", "shortCiteRegEx": "Bauer.", "year": 1957}, {"title": "Near-optimal columnbased matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Boutsidis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2014}, {"title": "A block Lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace of large, sparse, real symmetric matrices", "author": ["Jane Cullum", "W.E. Donath"], "venue": "In IEEE Conference on Decision and Control including the 13th Symposium on Adaptive Processes,", "citeRegEx": "Cullum and Donath.,? \\Q1974\\E", "shortCiteRegEx": "Cullum and Donath.", "year": 1974}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["Michael B. Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu"], "venue": "In Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Clustering large graphs via the singular value decomposition", "author": ["Petros Drineas", "Alan Frieze", "Ravi Kannan", "Santosh Vempala", "V Vinay"], "venue": "Machine Learning,", "citeRegEx": "Drineas et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2004}, {"title": "Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM J. Comput.,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Adaptive sampling and fast low-rank matrix approximation", "author": ["Amit Deshpande", "Santosh Vempala"], "venue": "In Proceedings of the 10th International Workshop on Randomization and Computation (RANDOM),", "citeRegEx": "Deshpande and Vempala.,? \\Q2006\\E", "shortCiteRegEx": "Deshpande and Vempala.", "year": 2006}, {"title": "Fast Monte Carlo algorithms for finding low-rank approximations", "author": ["Alan Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": "Journal of the ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "Efficient algorithms for computing a strong rankrevealing qr factorization", "author": ["Ming Gu", "Stanley C. Eisenstat"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Gu and Eisenstat.,? \\Q1996\\E", "shortCiteRegEx": "Gu and Eisenstat.", "year": 1996}, {"title": "A block Lanczos method for computing the singular values and corresponding singular vectors of a matrix", "author": ["Gene H. Golub", "Franklin T. Luk", "Michael L. Overton"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "Golub et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1981}, {"title": "The block Lanczos method for computing eigenvalues", "author": ["Gene Golub", "Richard Underwood"], "venue": "Mathematical Software,", "citeRegEx": "Golub and Underwood.,? \\Q1977\\E", "shortCiteRegEx": "Golub and Underwood.", "year": 1977}, {"title": "Subspace iteration randomization and singular value problems", "author": ["Ming Gu"], "venue": "Computing Research Repository (CoRR),", "citeRegEx": "Gu.,? \\Q2014\\E", "shortCiteRegEx": "Gu.", "year": 2014}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan.,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan.", "year": 1996}, {"title": "Randomized methods for computing low-rank approximations of matrices", "author": ["Nathan P Halko"], "venue": "PhD thesis, University of Colorado,", "citeRegEx": "Halko.,? \\Q2012\\E", "shortCiteRegEx": "Halko.", "year": 2012}, {"title": "An algorithm for the principal component analysis of large data sets", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Yoel Shkolnisky", "Mark Tygert"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM Review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "An iteration method for the solution of the eigenvalue problem of linear differential and integral operators1", "author": ["Cornelius Lanczos"], "venue": "Journal of Research of the National Bureau of Standards,", "citeRegEx": "Lanczos.,? \\Q1950\\E", "shortCiteRegEx": "Lanczos.", "year": 1950}, {"title": "PROPACK: Software for large and sparse SVD calculations", "author": ["Rasmus Munk Larsen"], "venue": "Stanford University,", "citeRegEx": "Larsen.,? \\Q2005\\E", "shortCiteRegEx": "Larsen.", "year": 2005}, {"title": "Fast algorithms for approximating the singular value decomposition", "author": ["Aditya Krishna Menon", "Charles Elkan"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "Menon and Elkan.,? \\Q2011\\E", "shortCiteRegEx": "Menon and Elkan.", "year": 2011}, {"title": "Symmetric gauge functions and unitarily invariant norms", "author": ["L. Mirsky"], "venue": "The Quarterly Journal of Mathematics,", "citeRegEx": "Mirsky.,? \\Q1960\\E", "shortCiteRegEx": "Mirsky.", "year": 1960}, {"title": "Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression", "author": ["Michael W Mahoney", "Xiangrui Meng"], "venue": "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Mahoney and Meng.,? \\Q2013\\E", "shortCiteRegEx": "Mahoney and Meng.", "year": 2013}, {"title": "A randomized algorithm for the approximation of matrices", "author": ["Per-Gunnar Martinsson", "Vladimir Rokhlin", "Mark Tygert"], "venue": "Technical Report 1361,", "citeRegEx": "Martinsson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Martinsson et al\\.", "year": 2006}, {"title": "OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L. Nguyen"], "venue": "In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguyen.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguyen.", "year": 2013}, {"title": "Latent semantic indexing: A probabilistic analysis", "author": ["Christos H. Papadimitriou", "Hisao Tamaki", "Prabhakar Raghavan", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Papadimitriou et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 2000}, {"title": "A randomized algorithm for principal component analysis", "author": ["Vladimir Rokhlin", "Arthur Szlam", "Mark Tygert"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Rokhlin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rokhlin et al\\.", "year": 2009}, {"title": "Simultaneous iteration method for symmetric matrices", "author": ["H. Rutishauser"], "venue": "Numerische Mathematik,", "citeRegEx": "Rutishauser.,? \\Q1970\\E", "shortCiteRegEx": "Rutishauser.", "year": 1970}, {"title": "Non-asymptotic theory of random matrices: extreme singular values", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "In Proceedings of the International Congress of Mathematicians 2010 (ICM),", "citeRegEx": "Rudelson and Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2010}, {"title": "On the rates of convergence of the Lanczos and the Block-Lanczos methods", "author": ["Y. Saad"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Saad.,? \\Q1980\\E", "shortCiteRegEx": "Saad.", "year": 1980}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["T\u00e1mas Sarl\u00f3s"], "venue": "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Sarl\u00f3s.,? \\Q2006\\E", "shortCiteRegEx": "Sarl\u00f3s.", "year": 2006}, {"title": "An implementation of a randomized algorithm for principal component analysis", "author": ["Arthur Szlam", "Yuval Kluger", "Mark Tygert"], "venue": "Computing Research Repository (CoRR),", "citeRegEx": "Szlam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szlam et al\\.", "year": 2014}, {"title": "On the minimum rank of a generalized matrix approximation problem in the maximum singular value norm", "author": ["Kin Cheong Sou", "Anders Rantzer"], "venue": "In Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems (MTNS),", "citeRegEx": "Sou and Rantzer.,? \\Q2010\\E", "shortCiteRegEx": "Sou and Rantzer.", "year": 2010}, {"title": "Numerical Linear Algebra", "author": ["Lloyd N. Trefethen", "David Bau"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Trefethen and Bau.,? \\Q1997\\E", "shortCiteRegEx": "Trefethen and Bau.", "year": 1997}, {"title": "Randomized algorithms for low-rank matrix factorizations: Sharp performance", "author": ["Rafi Witten", "Emmanuel J. Cand\u00e8s"], "venue": "bounds. Algorithmica,", "citeRegEx": "Witten and Cand\u00e8s.,? \\Q2014\\E", "shortCiteRegEx": "Witten and Cand\u00e8s.", "year": 2014}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P. Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Woodruff.", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "We re-analyze Simultaneous Power Iteration and the Block Lanczos method, two classical iterative algorithms for the singular value decomposition (SVD). We are interested in convergence bounds that do not depend on properties of the input matrix (e.g. singular value gaps). Simultaneous Iteration is known to give a low rank approximation within (1 + \u01eb) of optimal for spectral norm error in \u00d5(1/\u01eb) iterations. We strengthen this result, proving that it finds approximate principal components very close in quality to those given by an exact SVD. Our work bridges a divide between classical analysis, which can give similar bounds but depends critically on singular value gaps, and more recent work, which only focuses on low rank approximation Furthermore, we extend our bounds to the Block Lanczos method, which we show obtains the same approximation guarantees in just \u00d5(1/ \u221a \u01eb) iterations, giving the fastest known algorithm for spectral norm low rank approximation and principal component approximation. Despite their popularity, Krylov subspace methods like Block Lanczos previously seemed more difficult to analyze and did not come with rigorous gap-independent guarantees. Finally, we give insight beyond the worst case, justifying why Simultaneous Power Iteration and Block Lanczos can run much faster in practice than predicted. We clarify how simple techniques can potentially accelerate both algorithms significantly.", "creator": "LaTeX with hyperref package"}}}