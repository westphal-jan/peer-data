{"id": "1504.05539", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2015", "title": "Temporal-Difference Networks", "abstract": "We introduce a generalization of temporal-difference (TD) learning to networks of interrelated predictions. Rather than relating a single prediction to itself at a later time, as in conventional TD methods, a TD network relates each prediction in a set of predictions to other predictions in the set at a later time. TD networks can represent and apply TD learning to a much wider class of predictions than has previously been possible. Using a random-walk example, we show that these networks can be used to learn to predict by a fixed interval, which is not possible with conventional TD methods. Secondly, we show that if the inter-predictive relationships are made conditional on action, then the usual learning-efficiency advantage of TD methods over Monte Carlo (supervised learning) methods becomes particularly pronounced. Thirdly, we demonstrate that TD networks can learn predictive state representations that enable exact solution of a non-Markov problem. A very broad range of inter-predictive temporal relationships can be expressed in these networks. Overall we argue that TD networks represent a substantial extension of the abilities of TD methods and bring us closer to the goal of representing world knowledge in entirely predictive, grounded terms.", "histories": [["v1", "Tue, 21 Apr 2015 18:33:39 GMT  (80kb,D)", "http://arxiv.org/abs/1504.05539v1", "8 pages, 3 figures, presented at the 2004 conference on Neural Information Processing Systems. in Advances in Neural Information Processing Systems 17 (proceedings of the 2004 conference), Saul, L. K., Weiss, Y., and Bottou, L. (Eds)"]], "COMMENTS": "8 pages, 3 figures, presented at the 2004 conference on Neural Information Processing Systems. in Advances in Neural Information Processing Systems 17 (proceedings of the 2004 conference), Saul, L. K., Weiss, Y., and Bottou, L. (Eds)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard s sutton", "brian tanner"], "accepted": true, "id": "1504.05539"}, "pdf": {"name": "1504.05539.pdf", "metadata": {"source": "CRF", "title": "Temporal-Difference Networks", "authors": ["Richard S. Sutton"], "emails": ["sutton@cs.ualberta.ca", "btanner@cs.ualberta.ca"], "sections": [{"heading": null, "text": "In this context, TD learning is often easier and more data-efficient than other methods, but the idea of TD learning can be used in a more general way than in reinforcement learning. TD learning is a general method of learning predictions when multiple predictions of the same event are made over time, with value functions being just one example. Of the more general uses of TD learning, the most relevant were learning models of an environment or task area (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999). In this work, TD learning is used to predict future values of many observations or state variables of a dynamic system. The essential idea of TD learning can be described as \"learning a conjecture from a conjecture.\" In all previous work, the two conjectures were associated with predictions of the same quantity."}, {"heading": "1 The Learning-to-predict Problem", "text": "The problem we are looking at in this paper is a general one of learning to predict aspects of the interaction between a policymaker and his or her environment. While A is an arbitrary discrete proposition, we assume, without loss of generality, that ot can be represented as a vector of bits. Action and observation events occur successively, o1, a1, o2, a2, o3 \u00b7 \u00b7, each event, of course, only depends on the preceding one. This sequence is called experience. We are interested in predicting not only every next observation, but more general, action-related functions of future experience, as discussed in the next section. In this paper, we are using a seven-state random walk problem, with left and right actions available in each state."}, {"heading": "2 TD Networks", "text": "A TD network is a network of nodes, each representing a single scalar prediction; the nodes are connected by links that represent the TD relationships between the predictions and the observations and actions; these links determine the extensive semantics of each prediction - its desired or targeted relationship to the data; they represent what we want to predict about the data, as opposed to how we try to predict it. We regard these links as determining a series of questions that are asked about the data, and accordingly we call them the Question Network. A separate set of interactions determines the updated process of prediction at each node from its previous values and current action and observation. We think that this process provides the answers to the questions, and accordingly we call it the Answer Network. The Question Network provides goals for a learning process that shapes the behavior of the TD network and does not otherwise affect it."}, {"heading": "4 Experiment 2: Action-conditional Prediction", "text": "The advantage of the TD methods should be greater for predictions that only apply when the sequence of experiences unfolds in a certain way, e.g. when a certain sequence of actions is performed. In a second experiment, we tried to learn n-step-ahead predictions that depend on the selection of actions. The question network for learning all 2-step-forward predictions is shown in Figure 1b. The top two nodes predict the observation bit when a left action (L) or a right action (R) is taken. The bottom four nodes correspond to the two-step predictions, e.g. the second lower node is the prediction of what the observation bit will be when an L action is followed by an R action. These predictions are the same as the E tests used in some of the work on predictive state representations (Littman, Sutton & Singh, 2002; Rudary & Singh, 2003). In this experiment, we used measures from the network + 8, except for the one in each of the four frames."}, {"heading": "5 Experiment 3: Learning a Predictive State Representation", "text": "Experiments 1 and 2 showed advantages for TD learning methods in Markov problems. The feature vectors in both experiments provided complete information about the nominal state of the random walk. In Experiment 3, on the other hand, we applied TD networks to a non-Markov version of the random walk example, in particular, in which only the specific observation bit was visible and not the state number. In this case, it is not possible to make accurate predictions based solely on the current action and observation; the predictions of the previous time step must also be used. As in the previous experiment, we tried n-step prediction using actional fragment networks of depth 2, 3 and 4. The feature vector xt consisted of three parts: a constant 1, four binary features to represent the action pair at \u2212 1 and observation bit ot and nmore features corresponding to the components of yt \u2212 1."}, {"heading": "6 Conclusion", "text": "Our results show that TD methods can have significant advantages in learning TD-defined predictions, even in a fully observable environment. Our results show that TD methods can learn dramatically faster than other methods. TD networks allow the expression of many new types of predictions whose comprehensive semantics are not immediately clear, but which ultimately are based entirely on data. It may be fruitful to further explore the expressive potential of TD-defined predictions. Although most of our experiments concern the representative meaningfulness and efficiency of TD-defined predictions, it is also natural to consider their use as a state, as in predictive state representations. Our experiments suggest that this is a promising direction and that TD learning algorithms may have advantages over earlier learning methods. Finally, we find that adding nodes to a network of questions produces new predictions and thus may be a way to solve the predictive representation problem."}, {"heading": "Acknowledgments", "text": "The authors gratefully acknowledge the ideas and encouragement they have received in this work from Satinder Singh, Doina Precup, Michael Littman, Mark Ring, Vadim Bulitko, Eddie Rafols, Anna Koop, Tao Wang and all members of the rlai.net group."}], "references": [{"title": "Technical update: Least-squares temporal difference learning. Machine Learning 49:233\u2013246", "author": ["J.A. Boyan"], "venue": null, "citeRegEx": "Boyan,? \\Q2000\\E", "shortCiteRegEx": "Boyan", "year": 2000}, {"title": "Linear least-squares algorithms for temporal difference learning. Machine Learning 22(1/2/3):33\u201357", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": null, "citeRegEx": "Bradtke and Barto,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto", "year": 1996}, {"title": "Improving generalization for temporal difference learning: The successor representation", "author": ["P. Dayan"], "venue": "Neural Computation", "citeRegEx": "Dayan,? \\Q1993\\E", "shortCiteRegEx": "Dayan", "year": 1993}, {"title": "Learning and discovery of predictive state representations in dynamical systems with reset", "author": ["M. James", "S. Singh"], "venue": "In Proceedings of the Twenty-First International Conference on Machine Learning,", "citeRegEx": "James and Singh,? \\Q2004\\E", "shortCiteRegEx": "James and Singh", "year": 2004}, {"title": "Hierarchical learning in stochastic domains: Preliminary results", "author": ["L.P. Kaelbling"], "venue": "In Proceedings of the Tenth International Conference on Machine Learning,", "citeRegEx": "Kaelbling,? \\Q1993\\E", "shortCiteRegEx": "Kaelbling", "year": 1993}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research 4(Dec):1107\u20131149", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Predictive representations of state", "author": ["M.L. Littman", "R.S. Sutton", "S. Singh"], "venue": "In Advances In Neural Information Processing Systems", "citeRegEx": "Littman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2002}, {"title": "A nonlinear predictive state representation", "author": ["M.R. Rudary", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Rudary and Singh,? \\Q2004\\E", "shortCiteRegEx": "Rudary and Singh", "year": 2004}, {"title": "Learning predictive state representations", "author": ["S. Singh", "M.L. Littman", "N.K. Jong", "D. Pardoe", "P. Stone"], "venue": "In Proceedings of the Twentieth Int. Conference on Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2003}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Sutton,? \\Q1995\\E", "shortCiteRegEx": "Sutton", "year": 1995}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}], "referenceMentions": [{"referenceID": 2, "context": "The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999).", "startOffset": 121, "endOffset": 195}, {"referenceID": 4, "context": "The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999).", "startOffset": 121, "endOffset": 195}, {"referenceID": 10, "context": "The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999).", "startOffset": 121, "endOffset": 195}, {"referenceID": 0, "context": "The predictions learned under batch updating are also the same as would be computed by least squares algorithms such as LSTD(\u03bb) (Bradtke & Barto, 1996; Boyan, 2000; Lagoudakis & Parr, 2003).", "startOffset": 128, "endOffset": 189}, {"referenceID": 9, "context": "TD methods appear to have a significant data-efficiency advantage over non-TD methods in this prediction-by-n context (and this task) just as they do in conventional multi-step prediction (Sutton, 1988).", "startOffset": 188, "endOffset": 202}], "year": 2015, "abstractText": "We introduce a generalization of temporal-difference (TD) learning to networks of interrelated predictions. Rather than relating a single prediction to itself at a later time, as in conventional TD methods, a TD network relates each prediction in a set of predictions to other predictions in the set at a later time. TD networks can represent and apply TD learning to a much wider class of predictions than has previously been possible. Using a random-walk example, we show that these networks can be used to learn to predict by a fixed interval, which is not possible with conventional TD methods. Secondly, we show that if the interpredictive relationships are made conditional on action, then the usual learning-efficiency advantage of TD methods over Monte Carlo (supervised learning) methods becomes particularly pronounced. Thirdly, we demonstrate that TD networks can learn predictive state representations that enable exact solution of a non-Markov problem. A very broad range of inter-predictive temporal relationships can be expressed in these networks. Overall we argue that TD networks represent a substantial extension of the abilities of TD methods and bring us closer to the goal of representing world knowledge in entirely predictive, grounded terms. Temporal-difference (TD) learning is widely used in reinforcement learning methods to learn moment-to-moment predictions of total future reward (value functions). In this setting, TD learning is often simpler and more data-efficient than other methods. But the idea of TD learning can be used more generally than it is in reinforcement learning. TD learning is a general method for learning predictions whenever multiple predictions are made of the same event over time, value functions being just one example. The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999). In these works, TD learning is used to predict future values of many observations or state variables of a dynamical system. The essential idea of TD learning can be described as \u201clearning a guess from a guess\u201d. In all previous work, the two guesses involved were predictions of the same quantity at two points in time, for example, of the discounted future reward at successive time steps. In this paper we explore a few of the possibilities that open up when the second guess is allowed to be different from the first. ar X iv :1 50 4. 05 53 9v 1 [ cs .L G ] 2 1 A pr 2 01 5 To be more precise, we must make a distinction between the extensive definition of a prediction, expressing its desired relationship to measurable data, and its TD definition, expressing its desired relationship to other predictions. In reinforcement learning, for example, state values are extensively defined as an expectation of the discounted sum of future rewards, while they are TD defined as the solution to the Bellman equation (a relationship to the expectation of the value of successor states, plus the immediate reward). It\u2019s the same prediction, just defined or expressed in different ways. In past work with TD methods, the TD relationship was always between predictions with identical or very similar extensive semantics. In this paper we retain the TD idea of learning predictions based on others, but allow the predictions to have different extensive semantics. 1 The Learning-to-predict Problem The problem we consider in this paper is a general one of learning to predict aspects of the interaction between a decision making agent and its environment. At each of a series of discrete time steps t, the environment generates an observation ot \u2208 O, and the agent takes an action at \u2208 A. Whereas A is an arbitrary discrete set, we assume without loss of generality that ot can be represented as a vector of bits. The action and observation events occur in sequence, o1, a1, o2, a2, o3 \u00b7 \u00b7 \u00b7, with each event of course dependent only on those preceding it. This sequence will be called experience. We are interested in predicting not just each next observation but more general, action-conditional functions of future experience, as discussed in the next section. In this paper we use a random-walk problem with seven states, with left and right actions available in every state: 1 1 1 2 3 4 5 6 7 0 0 0 0 0 The observation upon arriving in a state consists of a special bit that is 1 only at the two ends of the walk and, in the first two of our three experiments, seven additional bits explicitly indicating the state number (only one of them is 1). This is a continuing task: reaching an end state does not end or interrupt experience. Although the sequence depends deterministically on action, we assume that the actions are selected randomly with equal probability so that the overall system can be viewed as a Markov chain. The TD networks introduced in this paper can represent a wide variety of predictions, far more than can be represented by a conventional TD predictor. In this paper we take just a few steps toward more general predictions. In particular, we consider variations of the problem of prediction by a fixed interval. This is one of the simplest cases that cannot otherwise be handled by TD methods. For the seven-state random walk, we will predict the special observation bit some numbers of discrete steps in advance, first unconditionally and then conditioned on action sequences.", "creator": "TeX"}}}