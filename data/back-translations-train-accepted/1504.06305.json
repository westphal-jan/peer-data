{"id": "1504.06305", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2015", "title": "Regularization-Free Estimation in Trace Regression with Symmetric Positive Semidefinite Matrices", "abstract": "Over the past few years, trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In the present paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (\\textsf{spd}) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an \\textsf{spd} constraint may perform as well as regularization-based approaches with a proper choice of the regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.", "histories": [["v1", "Thu, 23 Apr 2015 19:30:38 GMT  (191kb)", "http://arxiv.org/abs/1504.06305v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["martin slawski", "ping li 0001", "matthias hein 0001"], "accepted": true, "id": "1504.06305"}, "pdf": {"name": "1504.06305.pdf", "metadata": {"source": "CRF", "title": "Regularization-free estimation in trace regression with symmetric positive semidefinite matrices", "authors": ["Martin Slawski", "Ping Li", "Matthias Hein"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 4.06 305v 1 [stat.ML] 2 3"}, {"heading": "1 Introduction", "text": "It is not as if this is a real problem that has been focused in recent years in these areas on a setting with few measurements. [7, 21] Compressed sensitization [14] and phase retrieval [14] and phase retrieval [9]. A common thread in this work is the use of the nuclear standard as an environmental variant. [8, 26], Compressed sensitization [7, 21], quantum tomography [14] and phase retrieval. [9] A common thread in this work is the use of the nuclear standard of an environmental variant. [10], Compressed sensitization [11], [14] and retrieval."}, {"heading": "2 Analysis", "text": "In this section we will consider a special model (1), in which we each behave in a certain way (1). (6) The assumption that we can assume that we can assume without loss of generality that the {Xi} ni = 1 are symmetrical. (6) Any M-M sym + M skew, where M-M sym + M sym = 2 and M-2denote are the Euclidean projections of M-M sym + M skew, where M sym = M sym and M skew, where M sym = M-2denote are the Euclidean."}, {"heading": "2.1 Negative results", "text": "First, we will discuss examples of X for which the difference between the top and bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers / bottom layers."}, {"heading": "2.2 Slow rate bound on the prediction error", "text": "We now turn to the first positive result on the spd-restricted lowest square cost catalogue, which is sufficient for a slow choice and which is only suitable for a slow choice. (...) Specifically, the prediction error will be limited. (...) The rate in (13) can be a significant improvement of what is achieved when the rate in (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (..., (...), (..., (...), (...), (...,..., (...), (...,...,..., (...), (...,..., (...), (...), (..., (...), (...,...), (...,...,...), (...,...), (...,...,...), (...,...,...), (...), (...,...,...,...), (...,...), (...), (...), (...), (...,...,...), (...,...),...,...,...,...,..., (...,...,...,...), (...), (...),...,...,...), (...,...),..., (...,...),..., (...,...),..., (...,...,...),...), (...,...), (...,...), (...),...,..., (...),..., (...,...), (...,...), (...,...), (...), (...),..., (...), (...), (..., (...),...), (...), (...),..., (...), (...,...),...,..., (...),..., (...,...), (...),...,...,...,...,...), (...), (...), (...),...),...,...,"}, {"heading": "2.3 Bound on the estimation error", "text": "In the previous subsection, we have made no assumptions about the effects of climate change, apart from the effects of climate change on global warming. Henceforth, we assume that the effects of climate change on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on warming on"}, {"heading": "3 Numerical results", "text": "In this section, we provide a set of empirical results regarding the properties of the estimator. Similarly, we expect the fourth. In particular, its performance in relation to regularization-based methods is examined. We also provide an application to increased covariance estimation for the CBCL image sets and share prices of NASDAQ.3.1 Scaling the constants of 2 (T) for X and T given it is possible to evaluate high-quality 2 (T) by solving a convex optimization problem. This differs from other conditions used in the literature such as limited strong convexity [21], 1-RIP [10] or restricted uniform curvity [5], which is a non-convex optimization problem even for fixed T.Here, we are looking at sampling operators with random i.i.d. measurements Xi = ziz i, where zi N (0, I) is a standard Gaussian vector in Requivalence (Xi = 1), which we expect to follow a Wii quisition."}, {"heading": "3.2 Comparison with regularization-based approaches", "text": "In this subsection, we evaluate empirically that we compare approaches to regulation based on the methods proposed in the literature. Setup. we consider the measurement matrices of Wishart as in the previous subsection. Here, too, we expect a similar behavior for (most) other random designs of the ensemble M (\u03c0m, q). We fix m = 50 and leave n = 0.24, 0.26,. \u2212 s4,..., 0.56} \u00b7 m2 and r {1, 2,., 10} to take into account different methods. For each configuration of n and r, we consider 50 replications. In each of these replications, we generate Datayi = tr (Xi\u03a3) + \u03c3\u03b5i = 0.1, i = 1,.,., n, (18) where the approaches of the rWishart matrices and the {\u03b5.N} matrices are i.i.d.N (0, 1). We compare the approaches of Aryanization based on Aryanization."}, {"heading": "3.3 Real data examples", "text": "We conclude this section by explaining the relevance and popularity of the mentioned covariance matrices."}, {"heading": "4 Conclusion", "text": "In this paper, we have examined trace regression in the situation that the underlying matrix is symmetrically positively semidefinite. We have shown that the estimator with limited least squares exhibits excellent statistical properties under certain design constraints, similar to methods that apply nuclear standards regulation, which may be surprising as regulation is widely considered necessary in small sample environments. On the application side, we have highlighted the usefulness of our results for restoring spiked covariance matrices from square measurements."}, {"heading": "Acknowledgement", "text": "The work of Martin Slawski and Ping Li is partially supported by NSF-DMS-1444124, NSF-III-1360971, ONR-N00014-13-1-0764 and AFOSR-FA9550-13-1-0137."}, {"heading": "A Proof of Proposition 1", "text": "According to the rotational invariance of the Gaussian distribution of \u03b5, it is sufficient to consider the canonical orthonormal base of Sm, which is defined by X1 = e1e 1, X2 = 1 \u221a 2 (e1e 2 + e2e 1),.., Xm = 1 \u00b0 2 (e1e m + eme 1), Xm + 1 = e2e 2, Xm \u2212 2 = 1 \u00b0 2 (e2e 3 + e3e 2),.., Xm \u2212 1 = 1 \u00b0 2 (em \u2212 1e m + eme m \u2212 1), X\u03b4m = eme m, where {ej} mj = 1 \u00b0 2 is the canonical base vectors of Rm. Equivalent is the corresponding map X: Sm \u2192 R\u03b4m = = 1 \u00b0, the symmetrical vectorization of Sm."}, {"heading": "B Proof of Proposition 2", "text": "The proof for Proposition 2 comes from the results in [2].Definition B.1. Let C'Rd be a convex cone. The statistical dimension of C is defined as \u03b4 (C) = E [\u0441\u0430\u0441\u043a\u043e\u0441\u043a\u043e\u0441\u0442C denotes the Euclidean projection on C and the entries of g are i.i.d. N (0, 1).Theorem B.1. [2] Let f: Rd \u2192 R's be a correct convex function. Suppose A'Rn \u00b7 d has an i.i.d. N (0, 1) entries and let z0 = Ax0 \u2212 dimension Rd. Let's consider the convex optimization f (x) subject to Ax = z0. (25) and let D (f, x0) = t > 0 \u2212 Provit."}, {"heading": "C Proof of Proposition 3", "text": "Proposition 3 arises from the dual problem of the convex optimization problem in connection with \"A\" (X, R). \u2212 Proposition 3 arises from the dual problem of the convex optimization problem in connection with \"A\" (X, R). \u2212 Proposition 3 arises from the dual problem of the convex optimization problem in connection with \"A\" (X, R). \u2212 Proposition 2 arises from the dual optimization problem in A, B1n1 / 2 \u2212 X (A) \u2212 X (A). \u2212 X (A) \"n\" n \"n\" n \"I,\" X \"n\" I, \"a\" n. \"(27) The assertion of Proposition 3 arises directly from (27) by using\" n \"n\" min \"(n \u2212 1 / 2X\" (a) and \"n.\""}, {"heading": "D Proof of Corollary 1", "text": "The logical conclusion follows from sentence 3 by selecting a = 1 / \u221a n, so that n \u2212 1 / 2X \u0445 (a) = 1 n \u0445 n \u0445 n i = 1 Xi, and the use of this sentence implies that the specific values of R * and \u03c42 * are determined by selecting B = 2 in sentence 3."}, {"heading": "E Proof of Theorem 1", "text": "(U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) + (U) (U) + (U) + (U) + (U) (U) (U) (U) (U) (U), U (U) (U) (U) (U) (U) (U), U (U) (U) (U) (U) (U), U (U) (U) (U) (U) (U), U (U) (U) (U) (U), U (U) (U) (U) (U) (U) (U), U (U) (U) (U) (U) (U) (U) (U) (U) (U), U (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U) (U (U) (U) + (U) (U (U) + (U) (U) (U (U) (U) (U) (U) (U) (U) (U (U), U (U (U) (U), U (U (U), U (U (U (U) (U) (U) (U) (U), U (U (U (U) (U) (U) (U) (U (U) (U) (U) (U) (U) (U) (U, U (U) (U) (U (U) (U) (U) (U (U) (U) (U) (U) (U (U) (U) (U) (U)"}, {"heading": "F Proof of Theorem 1, Remark 3", "text": "The tied clues relate to the following concentration results for the extreme intrinsic values of the sample covariance of a Gaussian sample. Theorem F. 1. [11] Let z1,. \u2212 SN = 1 = 2 = 2 = 2 = 2 = 3 = 4 = 4 = 4 = 4 = 4 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5: 5 (5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5).In evidence we also use the following facts.Lemma F. 1. Let it be: 5 = 1 = 5: 5 + 5 + 5: 5 + 5: 5 + 5 = 5 = 5: 5 + 5 = 5: 5 = 5: 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5: 5 = 5 = 5: 5 = 5: 5 = 5: 5 = 5: 5 = 5: 5: 5: 5: 5: 5: 5 + 5 = 5 = 5 = 5: 5 = 5: 5 = 5 = 5 = 5: 5 = 5: 5, 5 = 5 = 5: 5: 5 = 5: 5: 5 = 5: 5: 5 = 5: 5 = 5: 5: 5 = 5 = 5: 5 = 5: 5 = 5: 5: 5 = 5 = 5: 5: 5 = 5: 5 = 5 = 5: 5: 5: 5 = 5 = 5: 5, 5 = 5: 5 = 5: 5: 5 = 5 = 5: 5 = 5 = 5: 5 = 5 = 5: 5: 5 = 5: 5, 5: 5: 5 = 5: 5 = 5 = 5: 5 = 5: 5 = 5 = 5: 5, 5: 5: 5 = 5: 5 = 5 = 5 = 5: 5: 5 = 5: 5 = 5: 5 = 5 = 5: 5, 5 = 5: 5: 5 = 5 = 5: 5: 5 = 5 = 5 = 5: 5, 5: 5: 5 = 5 = 5: 5 = 5 = 5 = 5: 5: 5 = 5 ="}, {"heading": "G Proof of Proposition 4", "text": "Subsequently, we write \"T\" and \"T\" for the orthogonal projections on T = \"T\" and \"T\" = \"T.\" Note first that since the \"Y\" ni = 1 are zero, each minimizer \"X\" (\"T\") = X \"(\" T \") = 0\" X \"(\" T \") + X\" (\"T\") = 0 \"32,\" where \"T\" = \"T\" and \"T\" = \"T\" (\"T\") = 0. \"Then there are\" T \"and\" S + 1 \"(m)\" T, \"so that X (\" T \") and\" T \"(\" T \") cannot be X (\" O \") + X\" (T) + X \"O\" (O \"O\" O \"), T\" O \"(O\" O \"), T\" O. \""}, {"heading": "H Proof of Theorem 2", "text": "We begin with the following analogy to (31) 1 n n n n n n n n n n n n n n n n n n n n n n n n n n (0) n n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1 (1) (1) (1) (1 (1) (1) (1) (1) (1 (1) (1) (1) (1) (1 (1) (1 (1) (1) (1) (1) (1 (1 (1) (1 (1) (1) (1 (1) (1) (1 (1) (1) (1) (1 (1 (1) (1 (1) (1) (1 (1) (1 (1) (1) (1 (1) (1) (1 (1"}], "references": [{"title": "Living on the edge: phase transitions in convex programs with random data", "author": ["D. Amelunxen", "M. Lotz", "M. McCoy", "J. Tropp"], "venue": "Information and Inference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "On the uniqueness of nonnegative sparse solutions to underdetermined systems of equations", "author": ["A. Bruckstein", "M. Elad", "M. Zibulevsky"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "ROP: Matrix recovery via rank-one projections", "author": ["T. Cai", "A. Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Solving quadratic equations via PhaseLift when there are about as many equations as unknowns", "author": ["E. Candes", "X. Li"], "venue": "Foundation of Computational Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Tight oracle bounds for low-rank matrix recovery from a minimal number of noisy measurements", "author": ["E. Candes", "Y. Plan"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Candes", "B. Recht"], "venue": "Foundation of Computational Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "PhaseLift: exact and stable signal recovery from magnitude measurements via convex programming", "author": ["E. Candes", "T. Strohmer", "V. Voroninski"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Handbook of the Geometry of Banach Spaces, volume 1, chapter Local operator theory, random matrices and Banach spaces, pages 317\u2013366", "author": ["K. Davidson", "S. Szarek"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Counting the faces of randomly-projected hypercubes and orthants, with applications", "author": ["D. Donoho", "J. Tanner"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Quantum State Tomography via Compressed Sensing", "author": ["D. Gross", "Y.-K. Liu", "S. Flammia", "S. Becker", "J. Eisert"], "venue": "Physical Review Letters,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Matrix Analysis", "author": ["R. Horn", "C. Johnson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1985}, {"title": "On the distribution of the largest eigenvalue in principal components analysis", "author": ["I. Johnstone"], "venue": "The Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "The phase retrieval problem", "author": ["M. Klibanov", "P. Sacks", "A. Tikhonarov"], "venue": "Inverse Problems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Von Neumann entropy penalization and low-rank matrix estimation", "author": ["V. Koltchinskii"], "venue": "The Annals of Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion", "author": ["V. Koltchinskii", "K. Lounici", "A. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Sign-constrained least squares estimation for high-dimensional regression", "author": ["N. Meinshausen"], "venue": "The Electronic Journal of Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["S. Negahban", "M. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear normminimization", "author": ["B. Recht", "M. Fazel", "P. Parillo"], "venue": "SIAM Review,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Estimation of high-dimensional low-rank matrices", "author": ["A. Rohde", "A. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Learning with kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization", "author": ["M. Slawski", "M. Hein"], "venue": "The Electronic Journal of Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Maximum margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Topics in Random Matrix Theory", "author": ["T. Tao"], "venue": "American Mathematical Society,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Regression shrinkage and variable selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1996}, {"title": "User-friendly tools for random matrices: An introduction", "author": ["J. Tropp"], "venue": "http://users.cms.caltech.edu/~jtropp/", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "How close is the sample covariance matrix to the actual covariance matrix ", "author": ["R. Vershynin"], "venue": "Journal of Theoretical Probability,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Conditions for a Unique Non-negative Solution to an Underdetermined System", "author": ["M. Wang", "A. Tang"], "venue": "In Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "A unique \u2019nonnegative\u2019 solution to an underdetermined system: from vectors to matrices", "author": ["M. Wang", "W. Xu", "A. Tang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}], "referenceMentions": [{"referenceID": 6, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 78, "endOffset": 85}, {"referenceID": 22, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 78, "endOffset": 85}, {"referenceID": 5, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 106, "endOffset": 113}, {"referenceID": 17, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 106, "endOffset": 113}, {"referenceID": 10, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 18, "context": "A common thread in these works is the use of the nuclear norm of a matrix as a convex surrogate for its rank [22] in regularized estimation amenable to modern optimization techniques.", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "This approach can be seen as natural generalization of l1-norm (aka lasso) regularization for the standard linear regression model [28] that arises as a special case of model (1) in which both \u03a3 and the measurement matrices {Xi}i=1 are diagonal.", "startOffset": 131, "endOffset": 135}, {"referenceID": 20, "context": "The set S+ deserves specific interest as it includes covariance matrices and Gram matrices in kernel-based learning methods [24].", "startOffset": 124, "endOffset": 128}, {"referenceID": 29, "context": "It is rather common for these matrices to be of low rank (at least approximately), given the widespread use of principal components analysis and low-rank kernel approximations [33].", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for high-dimensional linear regression with non-negative parameters [20, 25].", "startOffset": 183, "endOffset": 191}, {"referenceID": 21, "context": "Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for high-dimensional linear regression with non-negative parameters [20, 25].", "startOffset": 183, "endOffset": 191}, {"referenceID": 2, "context": "In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to l1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.", "startOffset": 250, "endOffset": 261}, {"referenceID": 9, "context": "In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to l1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.", "startOffset": 250, "endOffset": 261}, {"referenceID": 27, "context": "In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to l1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.", "startOffset": 250, "endOffset": 261}, {"referenceID": 28, "context": "In [32], the problem of exactly recovering \u03a3 being low-rank from noiseless observations (\u03b5i = 0, i = 1, .", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Apart from the fact that we primarily study a noisy setting, we shall argue below that in the setup of compressed sensing the measurement matrices studied in [32] constitute an unfavourable choice relative to those recommended in the present paper.", "startOffset": 158, "endOffset": 162}, {"referenceID": 3, "context": "In [5], rank-one measurements are considered for general \u03a3 \u2208 R12 .", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "On the other hand, in [5], no specific attention is given to the spd constraint: the convex program proposed therein, which can be seen as a modification of the approach in [10], applies to general symmetric matrices and does not enforce positive semidefiniteness.", "startOffset": 22, "endOffset": 25}, {"referenceID": 13, "context": "yi = |xi \u03c3| + \u03b5i (5) which (with complex-valued \u03c3) is relevant to the problem of phase retrieval [17] that has received some attention recently.", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "The approach of [9] treats (5) as an instance of (1) and uses nuclear norm regularization to enforce rank-one solutions.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "In followup work [6], the authors show a refined recovery result stating that imposing an spd constraint \u2212 without regularization \u2212 suffices.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "However, the results in both [6] and [12] only concern model (5).", "startOffset": 29, "endOffset": 32}, {"referenceID": 14, "context": "In [18], \u03a3 is assumed to be a complex Hermitian positive semidefinite matrix of unit trace, which is the setting in quantum state tomography.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "While the setting as well as the measurement matrices under consideration are different from ours, a notable point of contact to our work can be seen in the fact that the negative von Neumann entropy, which is the proposed regularizer in [18], does not promote low rankedness, but constitutes one possible way of enforcing positive definiteness.", "startOffset": 238, "endOffset": 242}, {"referenceID": 14, "context": "At the same time, adaptivity of the approach to low rankedness is established in [18].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 17, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "[7, 21].", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[7, 21].", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "The following statement, which follows from results in [2], points to a serious limitation associated with the use of such measurements.", "startOffset": 55, "endOffset": 58}, {"referenceID": 28, "context": "In [32], the following noiseless analog to the constrained least squares problem (7) is considered: find \u03a3 \u2208 S+ such thatX (\u03a3) = y = X (\u03a3), (12) where Xi \u223c GOE(m), i = 1, .", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "It is of interest to relate Proposition 2 to corresponding results on the vector case (equivalent to having diagonal {Xi}i=1 and diagonal \u03a3) in [13].", "startOffset": 144, "endOffset": 148}, {"referenceID": 9, "context": "Compared to Proposition 2, the corresponding result in [13] applies to a much wider class of random measurement matrices including all random matrices with i.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "It is thus natural to ask whether Proposition 2 holds more generally for all Wigner matrices [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "The fact that the threshold 12\u03b4m for the number measurements in Proposition 2 equals (up to the scaling factor \u03c3) the asymptotic prediction error of Example 2 is not a coincidence; this is part of a wider phenomenon as pointed out in [2].", "startOffset": 234, "endOffset": 237}, {"referenceID": 0, "context": "In the framework of [2], 12\u03b4m is the \u201cstatistical dimension\u201d of S m + .", "startOffset": 20, "endOffset": 23}, {"referenceID": 19, "context": "Theorem 1 in [23].", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "Theorem 1 in [25].", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "[11]) that for m,N large, \u03bbmax(\u0393\u0302n) and \u03bbmin(\u0393\u0302n) concentrate sharply around (1+ \u03b7n) 2 and (1\u2212 \u03b7n), respectively, where \u03b7n = \u221a m/N .", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Similar though weaker concentration results for \u2016\u0393\u2212 \u0393\u0302n\u2016\u221e are available for the broad class of distributions \u03c0m having finite fourth moments [30].", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "This is different from other conditions employed in the literature such as restricted strong convexity [21], 1-RIP [10] or restricted uniform boundedness [5] that involve a non-convex optimization problem even for fixed T.", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "This is different from other conditions employed in the literature such as restricted strong convexity [21], 1-RIP [10] or restricted uniform boundedness [5] that involve a non-convex optimization problem even for fixed T.", "startOffset": 154, "endOffset": 157}, {"referenceID": 0, "context": "The results point to the existence of a phase transition as it is typical for problems related to that under study [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 17, "context": "5, 1, 2, 4, 8, 16}, where \u03bb\u2217 = \u03c3 \u221a m/n as recommended in [21] and pick \u03bb so that the prediction error on a separate validation data set of size n generated from (18) is minimized.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "In addition, we have assessed the performance of the approach in [5], which does not impose an spd constraint but adds one more constraint to the formulation (19).", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "Therefore, instead of doing a grid search over a 2Dgrid, we use fixed values as specified in [5] given the knowledge of \u03c3.", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "The results are similar or worse than those of (19) (note in particular that positive semidefiniteness is not taken advantage of in the approach of [5]) and are hence not reported here.", "startOffset": 148, "endOffset": 151}, {"referenceID": 12, "context": "We conclude this section by presenting an application to recovery of spiked covariance matrices, a notion due to [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "It is well-known [3] that the projection of a symmetric matrix on the positive semidefinite cone is obtained by setting all its negative eigenvalues to zero, i.", "startOffset": 17, "endOffset": 20}, {"referenceID": 23, "context": "[27]), which is symmetric", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The proof of Proposition 2 follows from results in [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "[2] Let f : R \u2192 R\u222a{\u2212\u221e,+\u221e} be a proper convex function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "It is shown in [2], Proposition 3.", "startOffset": 15, "endOffset": 18}, {"referenceID": 11, "context": ",m ([15], \u00a74.", "startOffset": 4, "endOffset": 8}, {"referenceID": 25, "context": "1 in [29].", "startOffset": 5, "endOffset": 9}, {"referenceID": 25, "context": "[29] Consider a sequence {Xi}i=1 of fixed matrices in S and let {\u03b5i}i=1 i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] Let z1, .", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Over the past few years, trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularizationbased approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In the present paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of the regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.", "creator": "LaTeX with hyperref package"}}}