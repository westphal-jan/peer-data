{"id": "1504.06580", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2015", "title": "Classifying Relations by Ranking with Convolutional Neural Networks", "abstract": "Relation classification is an important semantic processing task whose state-of-the-art systems still rely on the use of costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which encodes the task of classifying the relationship between two nominals marked in a sentence. Using CR-CNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results also evidence that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.", "histories": [["v1", "Fri, 24 Apr 2015 17:50:33 GMT  (57kb,D)", "https://arxiv.org/abs/1504.06580v1", "Accepted as a long paper in the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)"], ["v2", "Sun, 24 May 2015 13:58:05 GMT  (55kb,D)", "http://arxiv.org/abs/1504.06580v2", "Accepted as a long paper in the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)"]], "COMMENTS": "Accepted as a long paper in the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["c\u00edcero nogueira dos santos", "bing xiang", "bowen zhou"], "accepted": true, "id": "1504.06580"}, "pdf": {"name": "1504.06580.pdf", "metadata": {"source": "CRF", "title": "Classifying Relations by Ranking with Convolutional Neural Networks", "authors": ["C\u0131\u0301cero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou"], "emails": ["cicerons@br.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Relation classification is an important Natural Language Processing (NLP) task that is normally used as an intermediate step in many complex NLP applications such as answering questions and automated knowledge base construction. Over the last decade, there has been an increasing interest in the application of machine learning approaches to this task (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010), one reason for which is the availability of benchmark datasets such as the SemEval-2010task 8 dataset (Hendrickx et al., 2010), which perform the task of classifying the relationship between two nominals marked in a sentence. The following sentence contains an example of the component-hole relationship between the nominals \"introduction\" and \"book.\" [Introduction] e1 in [book] e2 is a summary of what is written in the text."}, {"heading": "2 The Proposed Neural Network", "text": "CR-CNN calculates a score for each relationship class c-C. For each class c-C, the network learns a distributed vector representation encoded as a column of matrix W classes embedded in the class. As described in Figure 1, the only input for the network is the tokenized text sequence of the set. In the first step, CR-CNN transforms words into real character vectors. Next, a revolutionary layer is used to construct a distributed vector representation of the set, rx. Finally, CR-CNN calculates a score for each relationship class c-C by performing a point product between rx and W classes."}, {"heading": "2.1 Word Embeddings", "text": "The first level of the network transforms words into representations that capture syntactical and semantic information about the words. In the face of a sentence x consisting of N words x = {w1, w2,..., wN}, each word is converted into a real vector rwn. Therefore, input to the next level is a sequence of real vectors embx = {rw1, rw2,..., rwN} word representations are encoded by column vectors in an embedding matrixWwrd, Rdw \u00d7 | V |, where V is a fixed-size vocabulary. Each column Wwrdi \u0445 Rd w corresponds to the word embedding of the i-th word in the vocabulary. We transform a word w into its word embedded rw by using the matrix vector product."}, {"heading": "2.2 Word Position Embeddings", "text": "In the task of relation classification, information necessary to determine the class of a relationship between two target nouns is normally used by words that are close to the target nouns. Zeng et al. (2014) suggest the use of word position embeddings (position features), which help CNN by tracking how close words are to the target nouns. These features are similar to the position features that Collobert et al. (2011) suggest for the semantic role description task. In this thesis, we also experiment with the word position embeddings (WPE) proposed by Zeng et al. (2014). The WPE is derived from the relative distances of the current word to the target noun1 and Noun2. For example, in Figure 1, the relative distances from left to the car and plant -1 and 2 respectively, as in Collobert et al. (2011), any relative distance to a vector of the dimension constant constant initials is mapped to the target noun1, with the objective number layer 1 being random."}, {"heading": "2.3 Sentence Representation", "text": "The main challenges in this step are the variability of sentence size and the fact that important information can appear at any point in the sentence. Recent work has used revolutionary approaches to address these problems in creating representations for text segments of different sizes (Zeng et al., 2014; Hu et al., 2014; dos Santos and Gatti, 2014) and character representations of words of different sizes (dos Santos and Zadrozny, 2014). Here, we use a revolutionary layer to calculate distributed vector representations of the sentence. First, the revolutionary layer generates local characteristics around each word in the sentence, and then combines these local characteristics with a maximum operation to create a fixed vector for the input set."}, {"heading": "2.4 Class embeddings and Scoring", "text": "Considering the distributed vector representation of the input set x, the network with the parameter set \u03b8 calculates the score for a class name c \u0441C using the point products \u03b8 (x) c = r x [W classes] cwo W classes are an embedding matrix whose columns encode the distributed vector representations of the various class names, and [W classes] c is the column vector containing the embedding of class c. Note that the number of dimensions in each embedding must correspond to the size of the sentence representation defined by DC. The embedding matrix W classes is a parameter to be learned from the network. It is initialized by randomly selecting each value from a uniform distribution: U (\u2212 r, r), where r = \u221a 6 | C | + dc."}, {"heading": "2.5 Training Procedure", "text": "We have a network in which we can apply the results of each class to the results of each class. (1) We have a new logistic loss function with respect to these results in order to train CR-CNN: L = log (1 + exp) (m + exp) (m + exp) (m + exp) (m + exp)). We have a scaling factor that increases the difference between the scores and the marginals, which increases the difference between the scores and the marginals, which increases the difference between the scores and the marginals, which increases the difference between the scores and the marginalization factors and helps punish more on the predictive error errors. The first term class in Equation 1 decreases the scores of the scores. (1) We have a scaling factor that increases the difference between the scores and the marginalization factors, which increases the difference between the scores and the marginalizing factors, and we increase the difference between the differences between the scores and the marginalizing factors, and the marginalizing factors."}, {"heading": "3 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset and Evaluation Metric", "text": "We use the data set from SemEval-2010 Task 8 to conduct our experiments. This data set contains 10,717 examples with 9 different relation types and an artificial relation Other, which is used to indicate that the relationship in the example does not belong to one of the nine most important relationship types. Nine relationships are cause-effect, component-whole, content container, EntityDestination, entity-origin, instrument agency, member-collection, message-topic and productProducer. Each example contains a sentence marked with two nominals e1 and e2, and the task consists of predicting the relationship between the two nominals with respect to the direction.This means that the relationship CauseEffect (e1, e2) differs from the relationship CauseEffect (e2, e1) as shown in the following examples. More information about this data set can be found in (Hendrickal et al, 2010)."}, {"heading": "3.2 Word Embeddings Initialization", "text": "The word embeddings used in our experiments are initialized by means of unattended pre-training. We conduct pre-training using the NN architecture available in the word2vec tool with the skip program (Mikolov et al., 2013) to train word embeddings with word2vec. We edit the Wikipedia text using the steps described in (dos Santos et al., 2014): (1) removal of paragraphs that are not in English; (2) replacement of non-Western characters with a special character; (3) tokenization of the text with the tokenizer available with the Stanford POS Tagger (Toutanova et al., 2003); (4) removal of sentences that are less than 20 characters long (including spaces) or have less than 5 tokens. (5) Lower case letters replace all words and each digit with a 0. The resulting clean corpus contains about 1.75 billion tokens."}, {"heading": "3.3 Neural Network Hyper-parameter", "text": "The learning rates in the range of 0.03 and 0.01 provide relatively similar results. Depending on the CR-CNN configuration, the best results are achieved in 10 to 15 training epochs. In Table 1, we show the selected hyperparameter values. In addition, we use a learning rate plan that reduces the learning rate \u03bb according to the training epoch t. The learning rate for epoch t, \u03bbt, is calculated according to the equation: \u03bbt = \u03bbt."}, {"heading": "4 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Word Position Embeddings and Input Text Span", "text": "In the experiments discussed in this section, we evaluate the impact of the use of word embedding (WPE) and also propose a simpler alternative approach that is almost as effective as WPEs. The main idea behind the use of WPEs in relation to the classification task is to give an indication at the revolutionary level of how close a word is to the target terms, based on the assumption that closer words have more impact than distant words. Here, we suspect that most of the information needed to classify the relationship between the two target terms. Based on this hypothesis, we are conducting an experiment in which the input for the revolutionary level consists of the word embedding the sequence {we1 \u2212 1}, we2 + 1, where e2 correspond to the positions of the first and second target terms. In Table 2, we compare the results, we compare the results of the various CR-CNN configurations."}, {"heading": "4.3 CR-CNN versus CNN+Softmax", "text": "In this section, we report on experimental results that compare CR-CNN with CNN + Softmax. To make a fair comparison, we have implemented a CNN + Softmax and trained it with the same data, Word embedding and WPEs used in CR-CNN. Specifically, our CNN + Softmax consists in obtaining the output of the Convolutionary Layer, which is the rx vector in Figure 1, and giving it as input for a Softmax classifier. We adjust the parameters of CNN + Softmax by using a quadruple cross validation with the training set. Compared to the hyperparameter values for CR-CNN, which are shown in Table 1, the only difference for CNN + Softmax is the number of Convolutionary Units dc, which is set to 400. In Table 4, we compare the results of CRCNN and CNN + Softmax with the training set. CR-CNN surpasses CNN + Softmax both in precision as well as in memory and in the table of the max, and the max of the formula of CNN is improved by 1.6. In this section, we report CNN + Softmax improves the result in both the precision and the memory of the Z4 and the max of the formula of the formula of the max by 1.6."}, {"heading": "4.4 Comparison with the State-of-the-art", "text": "In Table 5, we compare the CR-CNN results with the recently published results of the SemEval-2010 Task 8 dataset. Rink and Harabagiu (2010) present a support vector machine (SVM) classifier that is fed with a rich (traditional) functionality. It gets an F1 of 82.2, which is the best result in SemEval-2010 Task 8. Socher et al. (2012) present results for a recursive neural network (MVRNN) that achieves a matrix vector representation for each node in a parse tree to compose the distributed vector representation for the complete set. Their method is called matrix vector recursive neural network (MVRNN) and achieves an F1 of 82.4 when using POS, NER and WordNet functions. In (Zeng et al., 2014), the authors present results for a CNN + soft-max classifier, excellent and excellent system."}, {"heading": "4.5 Most Representative Trigrams for each Relation", "text": "In Table 6, for each type of conversion, we present the five trigrams of the test set that have contributed the most to evaluating correctly classified examples. Remember that in CR-CNN, for a set x, the score for class c is calculated by s\u03b8 (x) c = rx [W classes] c. To calculate the most representative trigram of a set x, we trace each position in rx to find the trigram responsible for it. For each trigram t, we calculate its particular contribution to the score by adding the terms in the score, using the positions in rx that trace back to t. The most representative trigram in x is the trigram with the greatest contribution to improving the score. To create the results presented in Table 6, we classify the trigrams selected as the most representative of all sets in decreasing order of the contribution value. If a trigram as the largest contributor for more than one set appears, its value for each set of origins will be given."}, {"heading": "5 Related Work", "text": "Over the years, various approaches to classifying relationships have been proposed (Zhang, 2004; Qian et al., 2009; Hendrickx et al., 2010; Rink and Harabagiu, 2010), most of which treat it as a problem of multi-class classification and apply a variety of machine learning methods to the task to achieve high accuracy. Recently, deep learning (Bengio, 2009) has become an attractive area for multiple applications, including computer vision, speech recognition and natural language processing. Among the various deep learning strategies, revolutionary neural networks have been successfully applied to various NLP tasks such as part-of-speech tagging (dos Santos and Zadrozny, 2014), sensation analysis (Kim, 2014; dos Santos and Gatti, 2014), classification of words (Kalchbrenner et al., 2014), semantic role labeling (Collobert et al., 2011, Vorsage and Westhertag, 2014)."}, {"heading": "6 Conclusion", "text": "The main contributions of this paper are: (1) the definition of a new state-of-the-art for the SemEval 2010 Task 8 dataset without expensive handcrafted features; (2) the suggestion of a new CNN for classification, class embedding and a new rank-loss function; (3) an effective way to deal with artificial classes by omitting their embedding in CR-CNN; (4) the demonstration that using only the text between the target nomenclatures is almost as effective as using WPEs; and (5) a method to extract from the CR-CNN model the most representative contexts of each relationship type. Although we apply CR-CNN to relationship classification, this method can be used for any classification task."}, {"heading": "Acknowledgments", "text": "The authors thank Nina Wacholder for her valuable suggestions for improving the final version of the essay."}], "references": [{"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["dos Santos", "Ma\u0131\u0301ra Gatti"], "venue": "In Proceedings of the 25th International Conference on Computational Linguistics", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao et al.2014] Jianfeng Gao", "Patrick Pantel", "Michael Gamon", "Xiaodong He", "Li Deng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Semeval-2010 task 8: Multi-way classification", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3. S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Proceedings of the Conference on Neural Information Processing", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A convolutional neural netork for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods for Natural Language Processing,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Proceedings of Workshop at ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Semi-supervised learning for semantic relation classification using stratified sampling strategy", "author": ["Qian et al.2009] Longhua Qian", "Guodong Zhou", "Fang Kong", "Qiaoming Zhu"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Qian et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2009}, {"title": "Utd: Classifying semantic relations by combining lexical and semantic resources", "author": ["Rink", "Harabagiu2010] Bryan Rink", "Sanda Harabagiu"], "venue": "In Proceedings of International Workshop on Semantic Evaluation,", "citeRegEx": "Rink et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the Conference of the North American Chapter of the Associa-", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston et al.2011] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "tagspace: Semantic embeddings from hashtags", "author": ["Weston et al.2014] Jason Weston", "Sumit Chopra", "Keith Adams"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Factor-based compositional embedding models", "author": ["Yu et al.2014] Mo Yu", "Matthew Gormley", "Mark Dredze"], "venue": "In Proceedings of the 2nd Workshop on Learning Semantics,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of the 25th International Conference on Computational Linguistics (COLING),", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Weakly-supervised relation classification for information extraction", "author": ["Zhu Zhang"], "venue": "In Proceedings of the ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}], "referenceMentions": [{"referenceID": 19, "context": "Since the last decade there has been increasing interest in applying machine learning approaches to this task (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010).", "startOffset": 110, "endOffset": 168}, {"referenceID": 11, "context": "Since the last decade there has been increasing interest in applying machine learning approaches to this task (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010).", "startOffset": 110, "endOffset": 168}, {"referenceID": 6, "context": "One reason is the availability of benchmark datasets such as the SemEval-2010 task 8 dataset (Hendrickx et al., 2010), which encodes the task of classifying the relationship between two nominals marked in a sentence.", "startOffset": 93, "endOffset": 117}, {"referenceID": 13, "context": "focused on the use of deep neural networks with the aim of reducing the number of handcrafted features (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014).", "startOffset": 103, "endOffset": 160}, {"referenceID": 18, "context": "focused on the use of deep neural networks with the aim of reducing the number of handcrafted features (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014).", "startOffset": 103, "endOffset": 160}, {"referenceID": 17, "context": "focused on the use of deep neural networks with the aim of reducing the number of handcrafted features (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014).", "startOffset": 103, "endOffset": 160}, {"referenceID": 2, "context": "These features are similar to the position features proposed by Collobert et al. (2011) for the Semantic Role Labeling task.", "startOffset": 64, "endOffset": 88}, {"referenceID": 2, "context": "As in (Collobert et al., 2011), each relative distance is mapped to a vector of dimension dwpe, which is initialized with random numbers.", "startOffset": 6, "endOffset": 30}, {"referenceID": 17, "context": "In this work we also experiment with the word position embeddings (WPE) proposed by Zeng et al. (2014). The WPE is derived from the relative distances of the current word to the target noun1 and noun2.", "startOffset": 84, "endOffset": 103}, {"referenceID": 18, "context": "when creating representations for text segments of different sizes (Zeng et al., 2014; Hu et al., 2014; dos Santos and Gatti, 2014) and characterlevel representations of words of different sizes (dos Santos and Zadrozny, 2014).", "startOffset": 67, "endOffset": 131}, {"referenceID": 7, "context": "when creating representations for text segments of different sizes (Zeng et al., 2014; Hu et al., 2014; dos Santos and Gatti, 2014) and characterlevel representations of words of different sizes (dos Santos and Zadrozny, 2014).", "startOffset": 67, "endOffset": 131}, {"referenceID": 15, "context": "Like some other ranking approaches that only update two classes/examples at every training round (Weston et al., 2011; Gao et al., 2014), we can efficiently train the network for tasks which have a very large number of classes.", "startOffset": 97, "endOffset": 136}, {"referenceID": 5, "context": "Like some other ranking approaches that only update two classes/examples at every training round (Weston et al., 2011; Gao et al., 2014), we can efficiently train the network for tasks which have a very large number of classes.", "startOffset": 97, "endOffset": 136}, {"referenceID": 1, "context": "In our experiments, we implement the CR-CNN architecture and the backpropagation algorithm using Theano (Bergstra et al., 2010).", "startOffset": 104, "endOffset": 127}, {"referenceID": 6, "context": "More information about this dataset can be found in (Hendrickx et al., 2010).", "startOffset": 52, "endOffset": 76}, {"referenceID": 10, "context": "We perform pre-training using the skip-gram NN architecture (Mikolov et al., 2013) available in the word2vec tool.", "startOffset": 60, "endOffset": 82}, {"referenceID": 14, "context": "tion of non-western characters for a special character; (3) tokenization of the text using the tokenizer available with the Stanford POS Tagger (Toutanova et al., 2003); (4) removal of sentences that are less than 20 characters long (including white spaces) or have less than 5 tokens.", "startOffset": 144, "endOffset": 168}, {"referenceID": 18, "context": "This effect of WPEs is reported by (Zeng et al., 2014).", "startOffset": 35, "endOffset": 54}, {"referenceID": 17, "context": "0 reported in (Yu et al., 2014) for the SemEval2010 Task 8 dataset.", "startOffset": 14, "endOffset": 31}, {"referenceID": 18, "context": "The third line in Table 4 shows the result reported by Zeng et al. (2014) when only word embeddings and WPEs are used as input to the network (similar to our CNN+Softmax).", "startOffset": 55, "endOffset": 74}, {"referenceID": 18, "context": "9 (Zeng et al., 2014)", "startOffset": 2, "endOffset": 21}, {"referenceID": 13, "context": "Socher et al. (2012) present results for a recursive neural network (RNN) that employs a matrix-vector representation to every node in a parse tree in order to compose the distributed vector representation for the complete sentence.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "In (Zeng et al., 2014), the authors present results for a CNN+Softmax classifier which employs lexical and sentencelevel features.", "startOffset": 3, "endOffset": 22}, {"referenceID": 17, "context": "Yu et al. (2014) present the Factor-", "startOffset": 0, "endOffset": 17}, {"referenceID": 17, "context": "6), which is produced by the FCM system of Yu et al. (2014), is 2.", "startOffset": 43, "endOffset": 60}, {"referenceID": 19, "context": "Over the years, various approaches have been proposed for relation classification (Zhang, 2004; Qian et al., 2009; Hendrickx et al., 2010; Rink and Harabagiu, 2010).", "startOffset": 82, "endOffset": 164}, {"referenceID": 11, "context": "Over the years, various approaches have been proposed for relation classification (Zhang, 2004; Qian et al., 2009; Hendrickx et al., 2010; Rink and Harabagiu, 2010).", "startOffset": 82, "endOffset": 164}, {"referenceID": 6, "context": "Over the years, various approaches have been proposed for relation classification (Zhang, 2004; Qian et al., 2009; Hendrickx et al., 2010; Rink and Harabagiu, 2010).", "startOffset": 82, "endOffset": 164}, {"referenceID": 0, "context": "Recently, deep learning (Bengio, 2009) has become an attractive area for multiple applications,", "startOffset": 24, "endOffset": 38}, {"referenceID": 8, "context": "2014; dos Santos and Gatti, 2014), question classification (Kalchbrenner et al., 2014), semantic role labeling (Collobert et al.", "startOffset": 59, "endOffset": 86}, {"referenceID": 2, "context": ", 2014), semantic role labeling (Collobert et al., 2011), hashtag prediction (Weston et al.", "startOffset": 32, "endOffset": 56}, {"referenceID": 16, "context": ", 2011), hashtag prediction (Weston et al., 2014), sentence completion and response matching (Hu et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 7, "context": ", 2014), sentence completion and response matching (Hu et al., 2014).", "startOffset": 51, "endOffset": 68}, {"referenceID": 13, "context": "In (Socher et al., 2012), the authors tackle relation classification", "startOffset": 3, "endOffset": 24}, {"referenceID": 13, "context": "Some recent work on deep learning for relation classification include Socher et al. (2012), Zeng et al.", "startOffset": 70, "endOffset": 91}, {"referenceID": 13, "context": "Some recent work on deep learning for relation classification include Socher et al. (2012), Zeng et al. (2014) and Yu et al.", "startOffset": 70, "endOffset": 111}, {"referenceID": 13, "context": "Some recent work on deep learning for relation classification include Socher et al. (2012), Zeng et al. (2014) and Yu et al. (2014). In (Socher et al.", "startOffset": 70, "endOffset": 132}, {"referenceID": 13, "context": "8 (Socher et al., 2012) word embeddings, POS, NER, WordNet 77.", "startOffset": 2, "endOffset": 23}, {"referenceID": 13, "context": "1 (Socher et al., 2012) word embeddings, POS, NER, WordNet 82.", "startOffset": 2, "endOffset": 23}, {"referenceID": 18, "context": "7 (Zeng et al., 2014) word pair, words around word pair, WordNet FCM word embeddings 80.", "startOffset": 2, "endOffset": 21}, {"referenceID": 17, "context": "6 (Yu et al., 2014) word embeddings, dependency parse, NER 83.", "startOffset": 2, "endOffset": 19}, {"referenceID": 18, "context": "It achieves slightly higher accuracy on the same dataset than (Zeng et al., 2014), but only when syntactic information is used.", "startOffset": 62, "endOffset": 81}, {"referenceID": 13, "context": "There are two main differences between the approach proposed in this paper and the ones proposed in (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014): (1) CR-CNN uses a pair-wise ranking method, while other approaches apply multiclass classification by using the softmax function on the top of the CNN/RNN; and (2) CR-CNN employs an effective method to deal with artificial classes by omitting their embeddings, while other approaches treat all classes equally.", "startOffset": 100, "endOffset": 157}, {"referenceID": 18, "context": "There are two main differences between the approach proposed in this paper and the ones proposed in (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014): (1) CR-CNN uses a pair-wise ranking method, while other approaches apply multiclass classification by using the softmax function on the top of the CNN/RNN; and (2) CR-CNN employs an effective method to deal with artificial classes by omitting their embeddings, while other approaches treat all classes equally.", "startOffset": 100, "endOffset": 157}, {"referenceID": 17, "context": "There are two main differences between the approach proposed in this paper and the ones proposed in (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014): (1) CR-CNN uses a pair-wise ranking method, while other approaches apply multiclass classification by using the softmax function on the top of the CNN/RNN; and (2) CR-CNN employs an effective method to deal with artificial classes by omitting their embeddings, while other approaches treat all classes equally.", "startOffset": 100, "endOffset": 157}], "year": 2015, "abstractText": "Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.", "creator": "LaTeX with hyperref package"}}}