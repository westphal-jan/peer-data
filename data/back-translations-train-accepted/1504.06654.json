{"id": "1504.06654", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2015", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space", "abstract": "There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.", "histories": [["v1", "Fri, 24 Apr 2015 22:12:14 GMT  (203kb,D)", "http://arxiv.org/abs/1504.06654v1", "In Conference on Empirical Methods in Natural Language Processing, 2014"]], "COMMENTS": "In Conference on Empirical Methods in Natural Language Processing, 2014", "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["arvind neelakantan", "jeevan shankar", "alexandre passos", "andrew mccallum"], "accepted": true, "id": "1504.06654"}, "pdf": {"name": "1504.06654.pdf", "metadata": {"source": "CRF", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space", "authors": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "emails": ["arvind@cs.umass.edu", "jshankar@cs.umass.edu", "apassos@cs.umass.edu", "mccallum@cs.umass.edu"], "sections": [{"heading": null, "text": "Interest in vector-space word embedding and its use in NLP is growing, especially in light of newer methods for its rapid estimation on a very large scale. However, almost all of this work assumes a single vector per word type - ignoring polysemy and thereby jeopardizing its usefulness for downstream tasks. We present an extension of the Skip-gram model, which efficiently learns multiple embedding per word type. It differs from the more recent related work in that it jointly performs word discrimination and embeds learning, by non-parametric estimation of the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in terms of word similarity in context tasks and demonstrate its scalability by training with a machine on a body of nearly 1 billion tokens in less than 6 hours."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able, are able, are able, to be able to move, to be able to move, to be able to move, to feel able, to feel able, to feel able, to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to act themselves."}, {"heading": "2 Related Work", "text": "Our work is based on neural language models proposed by Bengio et al. (2003), which expand the traditional idea of N-gram language models by replacing conditional probability calculation with a neural network that symbolizes each word by representing a small vector instead of an indicator variable, and which together estimate the parameters of the neural network and these vectors. As the Bengio et al. (2003) model is quite expensive to train, research focuses on optimizing this word. Collobert and Weston (2008) replace the maximum probability of the model with a max margin approach, in which the network is encouraged to value the correct N-grams higher than randomly selected incorrect N-grams. Mnih and Hinton (2007) replace the global normalization of the Bengio model with a tree-structured probability distribution."}, {"heading": "3 Background: Skip-gram model", "text": "The Skip-gram model learns word embeddings in such a way that they are useful in predicting the surrounding words in a sentence. In the Skip-gram model, v (w) \"Rd\" is the vector representation of the word w \"W,\" where \"W\" is the word vocabulary and \"d\" is the embedding of words into dimensionality. In the case of a word pair (wt, c), the probability that the word \"c\" in the context of the word \"W\" is by, P (D = 1 | v (wt), v (c) = 11 + e \u2212 v (wt) T v (c) (1) is the probability that the word \"c\" will not be observed in the context of the word \"W\" by, P (wt), v (c) = 1 \u2212 P (v (c)) = P (v (v (v (v (v))))))))."}, {"heading": "4 Multi-Sense Skip-gram (MSSG) model", "text": "To expand the model of the global vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-word-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-"}, {"heading": "5 Non-Parametric MSSG model (NP-MSSG)", "text": "The MSSG model learns a fixed number of senses per word type. In this section we describe a non-parametric version of MSSG, the NP-MSSG model, which learns a varying number of senses per word type. Our approach is closely related to the non-parametric cluster method described in Meyerson (2001). We create a new cluster (sense) for a word type with a probability proportional to the distance of its context to the next cluster (sense). Each word is associated with sense vectors, context clusters and a global vector vg (w), as in the MSSG model. The number of senses for a word is unknown and is learned during the training. First, the words have no sense vectors and context clusters. We create the first sense vector and context cluster for each word on its first appearance in the training data."}, {"heading": "6 Experiments", "text": "To evaluate our algorithms, we train embedding with the same corpus and vocabulary as in Huang et al (2012), the snapshot of the Wikipedia corpus from April 2010 (Shaoul and Westbury, 2010), which contains approximately 2 million articles and 990 million characters. In all our experiments, we remove all words with less than 20 occurrences and use a maximum context window (N) of the length 5 (5 words before and after word occurrence). We set the number of senses (K) for the MSSG model to 3, unless otherwise specified. Our hyperparameter values were selected by a small amount of manual exploration on a validation set. In NP-MSSG, we set the skip gram model, MSSG and NP-MSSG models to a loud context word (S) for each of the contextual words observed."}, {"heading": "6.1 Nearest Neighbors", "text": "Table 2 shows qualitatively the results of the discovery of multiple senses by presenting the closest neighbors associated with different embeddings; the closest neighbors of a word are calculated by comparing the cosinal similarity between the embeddedness of each sense of the word and the context embeddedness of all other words in the vocabulary; note that each of the discovered senses is actually semantically coherent and that an appropriate number of senses is generated by the nonparametric method; Table 3 shows the closest neighbors of the word plant for Skip-gram, MSSG, NP-MSSG and the Haung model (Huang et al, 2012)."}, {"heading": "6.2 Word Similarity", "text": "We evaluate our embedding on two related datasets: the WordSim-353 (Finkelstein et al, 2001) Dataset and the Contextual Word Similarity (SCWS) dataset Huang et al (2012).WordSim-353 is a standard dataset for the evaluation of word vector representations. It consists of a list of word types whose similarity is evaluated on a holistic scale from 1 to 10. These results include both monosemical and polysemitic words. These results for each word pair are given without any contextual information, making them difficult to interpret. To overcome this problem, Stanford's contextual word similarities (SCWS) dataset were developed by Huang et al (2012). The dataset consists of 2003 word pairs and their entential contexts."}, {"heading": "7 Conclusion", "text": "We present an extension of the Skip-gram model, which efficiently learns multiple embeddings per word type. Together, the model discriminates and embeds learning, and non-parametrically estimates the number of senses per word type. Our method achieves new state-of-the-art results in word similarity in the context task and learns multiple senses, training it on nearly one billion tokens in less than 6 hours. Global vectors, sense vectors and cluster centers of our model and codes for learning these are available at https: / / people.cs.umass.edu / \u02dc arvind / emnlp2014wordvectors. In the future, we plan to use the multiple embeddings per word type in downstream NLP tasks."}, {"heading": "Acknowledgments", "text": "This work has been supported in part by the Center for Intelligent Information Retrieval and in part by DARPA under contract number FA8750-13-20020. The U.S. government is authorized to reproduce and distribute copies for government purposes, regardless of copyright notices contained therein. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}], "references": [{"title": "Tailoring Continuous Word Representations for Dependency Parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research (JMLR).", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Class-based N-gram models of natural language Computational Linguistics", "author": ["Peter F. Brown", "Peter V. Desouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "International Conference on Machine learning (ICML).", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Multi-View Learning of Word Embeddings via CCA", "author": ["Paramveer S. Dhillon", "Dean Foster", "Lyle Ungar."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Dhillon et al\\.,? 2011", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Adaptive sub- gradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research (JMLR).", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Placing search in context: the concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "International Conference on World Wide Web (WWW).", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Computing semantic relatedness using wikipediabased explicit semantic analysis", "author": ["Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "International Joint Conference on Artificial Intelligence (IJCAI).", "citeRegEx": "Gabrilovich and Markovitch.,? 2007", "shortCiteRegEx": "Gabrilovich and Markovitch.", "year": 2007}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Association of Computational Linguistics (ACL).", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Simple Semi-supervised Dependency Parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Quoc V. Le", "Tomas Mikolov"], "venue": "International Conference on Machine Learning", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning Word Vectors for Sentiment Analysis Association for Computational Linguistics (ACL", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Online Facility Location", "author": ["Adam Meyerson"], "venue": "IEEE Symposium on Foundations of Computer Science", "citeRegEx": "Meyerson.,? \\Q2001\\E", "shortCiteRegEx": "Meyerson.", "year": 2001}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Workshop at International Conference on Learning Representations (ICLR).", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."], "venue": "arXiv.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Name tagging with word clusters and discriminative training", "author": ["Scott Miller", "Jethran Guinness", "Alex Zamanian."], "venue": "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).", "citeRegEx": "Miller et al\\.,? 2004", "shortCiteRegEx": "Miller et al\\.", "year": 2004}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "International Conference on Machine learning (ICML).", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Learning Dictionaries for Named Entity Recognition using Minimal Supervision", "author": ["Arvind Neelakantan", "Michael Collins."], "venue": "European Chapter of the Association for Computational Linguistics (EACL).", "citeRegEx": "Neelakantan and Collins.,? 2014", "shortCiteRegEx": "Neelakantan and Collins.", "year": 2014}, {"title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution", "author": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."], "venue": "Conference on Natural Language Learning (CoNLL).", "citeRegEx": "Passos et al\\.,? 2014", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Design Challenges and Misconceptions in Named Entity Recognition", "author": ["Lev Ratinov", "Dan Roth."], "venue": "Conference on Natural Language Learning (CoNLL).", "citeRegEx": "Ratinov and Roth.,? 2009", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "Dynamic and Static Prototype Vectors for Semantic Composition", "author": ["Siva Reddy", "Ioannis P. Klapaftis", "Diana McCarthy."], "venue": "International Joint Conference on Artificial Intelligence (IJCNLP).", "citeRegEx": "Reddy et al\\.,? 2011", "shortCiteRegEx": "Reddy et al\\.", "year": 2011}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J. Mooney."], "venue": "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)", "citeRegEx": "Reisinger and Mooney.,? 2010a", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "A mixture model with sharing for lexical semantics", "author": ["Joseph Reisinger", "Raymond Mooney."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Reisinger and Mooney.,? 2010b", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "The Westbury lab wikipedia", "author": ["Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul and Westbury.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul and Westbury.", "year": 2010}, {"title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Jakob Uszkoreit."], "venue": "North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2012", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010).", "startOffset": 82, "endOffset": 125}, {"referenceID": 3, "context": "This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010).", "startOffset": 193, "endOffset": 241}, {"referenceID": 20, "context": "There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T\u00e4ckstr\u00f6m et al, 2012).", "startOffset": 232, "endOffset": 276}, {"referenceID": 20, "context": "There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T\u00e4ckstr\u00f6m et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons.", "startOffset": 253, "endOffset": 574}, {"referenceID": 20, "context": "There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T\u00e4ckstr\u00f6m et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons. Similarly Bansal et al (2014) show results in dependency parsing using Skip-gram embeddings.", "startOffset": 253, "endOffset": 804}, {"referenceID": 22, "context": "Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012).", "startOffset": 81, "endOffset": 110}, {"referenceID": 22, "context": "Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012). They both pre-cluster the contexts of a word type\u2019s tokens into discriminated senses, use the clusters to re-label the corpus\u2019 tokens according to sense, and then learn embeddings for these re-labeled words.", "startOffset": 81, "endOffset": 133}, {"referenceID": 22, "context": "Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012). They both pre-cluster the contexts of a word type\u2019s tokens into discriminated senses, use the clusters to re-label the corpus\u2019 tokens according to sense, and then learn embeddings for these re-labeled words. The second paper improves upon the first by employing an earlier pass of non-discriminated embedding learning to obtain vectors used to represent the contexts. Note that by pre-clustering, these methods lose the opportunity to jointly learn the sense-discriminated vectors and the clustering. Other weaknesses include their fixed number of sense per word type, and the computational expense of the two-step process\u2014the Huang et al (2012) method took one week of computation to learn multiple embeddings for a 6,000 subset of the 100,000 vocabulary on a corpus containing close to billion tokens.", "startOffset": 81, "endOffset": 780}, {"referenceID": 12, "context": "In the non-parametric version of our method, we build on facility location (Meyerson, 2001): a new cluster is created with probability proportional to the distance from the context to the", "startOffset": 75, "endOffset": 91}, {"referenceID": 3, "context": "Collobert and Weston (2008) replaces the max-likelihood character of the model with a max-margin approach, where the network is encouraged to score the correct n-grams higher than randomly chosen incorrect n-grams.", "startOffset": 0, "endOffset": 28}, {"referenceID": 3, "context": "Collobert and Weston (2008) replaces the max-likelihood character of the model with a max-margin approach, where the network is encouraged to score the correct n-grams higher than randomly chosen incorrect n-grams. Mnih and Hinton (2007) replaces the global normalization of the Bengio model with a tree-structured probability distribution, and also considers multiple positions for each word in the tree.", "startOffset": 0, "endOffset": 238}, {"referenceID": 18, "context": "Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014).", "startOffset": 111, "endOffset": 182}, {"referenceID": 10, "context": "Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014).", "startOffset": 419, "endOffset": 441}, {"referenceID": 20, "context": "The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks.", "startOffset": 134, "endOffset": 178}, {"referenceID": 22, "context": "Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words.", "startOffset": 0, "endOffset": 29}, {"referenceID": 22, "context": "Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks.", "startOffset": 0, "endOffset": 151}, {"referenceID": 22, "context": "Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final outputs are still one-to-one mappings between word types and word embeddings.", "startOffset": 0, "endOffset": 515}, {"referenceID": 22, "context": "Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final outputs are still one-to-one mappings between word types and word embeddings.", "startOffset": 0, "endOffset": 538}, {"referenceID": 22, "context": "To extend the Skip-gram model to learn multiple embeddings per word we follow previous work (Huang et al, 2012; Reisinger and Mooney, 2010a)", "startOffset": 92, "endOffset": 140}, {"referenceID": 12, "context": "Our approach is closely related to the online non-parametric clustering procedure described in Meyerson (2001). We create a new cluster (sense) for a word type with probability proportional to the distance of its context to the nearest cluster (sense).", "startOffset": 95, "endOffset": 111}, {"referenceID": 24, "context": "To evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in Huang et al (2012), which is the April 2010 snapshot of the Wikipedia corpus (Shaoul and Westbury, 2010).", "startOffset": 172, "endOffset": 199}, {"referenceID": 24, "context": "To evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in Huang et al (2012), which is the April 2010 snapshot of the Wikipedia corpus (Shaoul and Westbury, 2010). It contains approximately 2 million articles and 990 million tokens. In all our experiments we remove all the words with less than 20 occurrences and use a maximum context window (N ) of length 5 (5 words before and after the word occurrence). We fix the number of senses (K) to be 3 for the MSSG model unless otherwise specified. Our hyperparameter values were selected by a small amount of manual exploration on a validation set. In NP-MSSG we set \u03bb to -0.5. The Skip-gram model, MSSG and NP-MSSG models sample one noisy context word (S) for each of the observed context words. We train our models using AdaGrad stochastic gradient decent (Duchi et al, 2011) with initial learning rate set to 0.025. Similarly to Huang et al (2012), we don\u2019t use a regularization penalty.", "startOffset": 173, "endOffset": 935}, {"referenceID": 3, "context": "C&W refers to the language model by Collobert and Weston (2008) and HLBL model is the method described in Mnih and Hinton (2007).", "startOffset": 36, "endOffset": 64}, {"referenceID": 3, "context": "C&W refers to the language model by Collobert and Weston (2008) and HLBL model is the method described in Mnih and Hinton (2007). On WordSim-353 task, we see that our model performs significantly better than the previous neural network model for learning multi-representations per word (Huang et al, 2012).", "startOffset": 36, "endOffset": 129}, {"referenceID": 22, "context": "Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations.", "startOffset": 14, "endOffset": 43}, {"referenceID": 7, "context": "Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations.", "startOffset": 49, "endOffset": 83}, {"referenceID": 23, "context": "Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations.", "startOffset": 102, "endOffset": 131}], "year": 2015, "abstractText": "There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type\u2014ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.", "creator": "TeX"}}}