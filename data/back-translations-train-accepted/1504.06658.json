{"id": "1504.06658", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2015", "title": "Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods", "abstract": "Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction. In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention. Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method.", "histories": [["v1", "Fri, 24 Apr 2015 22:32:40 GMT  (349kb,D)", "http://arxiv.org/abs/1504.06658v1", "North American Chapter of the Association for Computational Linguistics- Human Language Technologies, 2015"]], "COMMENTS": "North American Chapter of the Association for Computational Linguistics- Human Language Technologies, 2015", "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["arvind neelakantan", "ming-wei chang"], "accepted": true, "id": "1504.06658"}, "pdf": {"name": "1504.06658.pdf", "metadata": {"source": "CRF", "title": "Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods", "authors": ["Arvind Neelakantan", "Ming-Wei Chang"], "emails": ["arvind@cs.umass.edu", "minchang@microsoft.com"], "sections": [{"heading": null, "text": "Due to the novelty of this task, we construct a large data set and design an automatic evaluation method. Our Knowledge Base Completion Method uses information within existing KBs and external information from Wikipedia. We show that individual methods trained with a global goal that takes into account unobserved cells from both the entity and type side consistently provide higher quality predictions than basic methods. We also perform manual evaluations on a small portion of the data to verify the effectiveness of our methods for completing the knowledge base and the accuracy of our proposed automatic evaluation method."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Inferring Entity Types", "text": "In our experiments, we look at a KB that contains entity type information from the form (e, t), where e-E (E is the totality of all entities) is a unit in the KB with type t-T (T is the totality of all types). For example, e could be Tiger Woods and t an athlete. Since a single entity may have several types, entities in the freebase often overlook some of their types. The aim of this work is to infer missing entity types in the KB. In view of an unnoticed fact (an entity type pair) in the training data (e, t) 6, the entity e-E and entity t-T is to infer whether the KB currently overlooks the fact, i.e., whether (e, t). We look entities at the intersection of freebase and Wikipedia in our experiments."}, {"heading": "2.1 Information Resources", "text": "We use information in the freebase and external information from Wikipedia to complete the KB. \u2022 Entity Type Features: The entity types observed in the training data can be a useful signal to infer missing entity type instances. \u2022 Freebase Description: Almost all entities in the freebase have a short 1-paragraph description of the entity. Figure 1 shows Jean Metellus \"freebase description, which can be used to infer the type / book / author that the freebase does not contain as the date of writing this article. \u2022 Wikipedia: For external information, we include the Wikipedia full-text article of an entity in its feature representation. We look at entities in freebase that have a link to their Wikipedia article."}, {"heading": "3 Evaluation Framework", "text": "In this section, we propose an evaluation methodology for the task of identifying missing entity type instances in a KB. While we focus on recovering entity types, the proposed framework can also be easily adapted to the extraction of relations. First, we discuss our strategy for creating two snapshots of data sets. Then, we motivate the importance of a global evaluation of KBC algorithms and describe the evaluation metrics we use."}, {"heading": "3.1 Two Snapshots Construction", "text": "eiD eeisrcnlhsrtee\u00fcGsrtee\u00fccnh rf\u00fc ide rf\u00fc ide rf\u00fc ide eeisrteeeirsrVnlrteee\u00fceegnn rf\u00fc ide rf\u00fc ide eeirsrteeee\u00fcg\u00dfnlrVnlrtaeu ni red eeisrsrteeeitlrBnlrgne\u00fc\u00fc\u00fc\u00fce ni rde nlrf\u00fc-eaeaJnlrgneaeeaeBnlrsrlrrrgneaeetnrFnlrsrsrsrteeeoiuiuiuiuiuiuiuiuiuiueetnlrsn ni ni ni ni ni rf\u00fc red.rf\u00fc"}, {"heading": "3.2 Global Evaluation Metric", "text": "Mean average precision (MAP) (Manning et al., 2008) is now commonly used to evaluate methods for evaluating the completion of KB (Mintz et al., 2009; Riedel et al., 2013). MAP is defined as the mean of average precision across all entity types (or relation types). MAP treats each entity type equally (without explicitly considering their distribution). However, some types occur much more frequently than others. For example, in our large-scale experiment with 500 entity types, there are many entity types with only 5 instances in the test kit, while the most common entity type has tens of missing instances. In addition, MAP only measures the ability of methods to correctly rank predictions within a font to account for high variance in the distribution of entity types-label label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label label-label-label-label label-label-label-label label-label-label-label-label-label-label label-label-label-label label-label-label label-label-label label label-label-label-label label-label-label-label-label label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label"}, {"heading": "4 Global Objective for Knowledge Base Completion", "text": "In this section, we describe our approach to predicting missing entity types in a KB. While this paper focuses on recovering entity types, the methods we have developed can easily be extended to other KB completion tasks."}, {"heading": "4.1 Global Objective Framework", "text": "During the training, only positive examples are considered in the following two sets: NE, e. \"Similar to the previous paper (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013), we obtain negative examples by treating the unobserved data in the KB as negative examples. As the number of unobserved examples is much greater than the number of facts in the KB, we follow previous methods and few unobserved negative examples for each positive example. Previous methods are largely neglected. The proposed global object structure allows us to systematically investigate the effects of the different sampling methods in order to obtain negative data, since the performance of the model for different evaluation metrics depends on the sampling methods. We consider a training recording of the KB 0, which contains facts of the form (e, t) where e is an entity in the KB with type. Given a fact (e, t) in the KB, we consider two types of negative examples constructed."}, {"heading": "4.2 Algorithms", "text": "We propose three different algorithms based on the global objective framework for predicting missing entity types."}, {"heading": "4.3 Relationship to Existing Methods", "text": "Many existing methods of relation extraction and entity type prediction can be considered a special case within the global objective framework. For example, we can consider the work in relation extraction (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205. These models are trained only on negative entities that we call the Negative Entity (NE) lens. The entity type prediction model developed in Ling and Weld (2012) is also a linear model with NE (e, t) = \u2205 of what we call the Negative Type (NT) lens. The embedding model for image recovery developed in Weston et al. (2011) is a special case of our model trained with the NT lens. While the NE or NT lens functions may be suitable for some classification tasks (Weston et al, the choice of functions was not motivated)."}, {"heading": "5 Experiments", "text": "In this section, we will give details about our data set and discuss our experimental results. Finally, we will perform a manual analysis of a small portion of the data."}, {"heading": "5.1 Data", "text": "First, we evaluate our methods on 70 entity types with the most commonly observed facts in the training data. [5] We also perform large-scale evaluations by testing the methods on 500 types with the most commonly observed facts in the training data.Table 1 shows statistics from our data set. The number of positive examples is much greater in the training data than in the test data, as the test set only contains facts added to the newer snapshot.5 An additional effect of this is that we have removed a few entity types that were trivial to predict in the test data. Most facts in the test data are about entities that are not well known or notorious.The high ratio of negative to positive examples in the test data makes this data set very difficult."}, {"heading": "5.2 Automatic Evaluation Results", "text": "Table 2 shows automatic evaluation results, where we give results on 70 types and 500 types. We empirically compare different aspects of the system on 70 types. Adagrad Vs DCD We first examine the linear models by comparing Linear.DCD and Linear.AdaGrad. Table 2a shows that Linear.AdaGrad consistently performs better for our task. Impact of Features We compare the impact of different features on final performance using Linear.AdaGrad in Table 2b. Types are represented by Boolean features, while Freebase description and Wikipedia full text are represented by tfidf weighting.The best MAP results are obtained by using all the information (T + D + W), while the best GAP results are obtained by using the freebase description and the Wikipedia article of the unit. Note that the features are simply bundled when multiple resources are used."}, {"heading": "5.3 Human Evaluation", "text": "In order to verify the effectiveness of our KBC algorithms and the accuracy of our automatic evaluation method, we perform a manual evaluation of the 100 best predictions of the results of two different experimental settings, and the results are presented in Table 3. Although the automatic evaluation provides pessimistic results, as the KB test is also incompleted6, the results suggest that the automatic evaluation correlates with the manual evaluation. Even more exciting, of the 179 unique cases we evaluated manually, 17 are missing in Freebase, underscoring the effectiveness of our approach."}, {"heading": "5.4 Error Analysis", "text": "\u2022 Impact of the training data: We find that the performance of the models of a type strongly depends on the number of training instances for that type. For example, the linear classification model performs 24.86% better when evaluating 70 types for the most common 35 types than for the rarest 35 types, suggesting that bootstrapping or active learning techniques can be used profitably to better monitor the methods. In this case, G @ k would be a useful metric to compare the effectiveness of the different methods. \u2022 Flat linguistic characteristics: Some of the false positive predictions were caused by the use of flat linguistic characteristics. For example, a unit that acts in a movie and composes music only for television shows is wrongly marked with the type / movie / composer, since words like \"movie,\" \"\" \"composer\" and \"music\" often appear in the company's Wikipedia article (http: / / de.wikipedia. org / wiki / J. Abrams)."}, {"heading": "6 Related Work", "text": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of prediction entity type at the sentence level. Yao et al. (2013) develop a method based matrix factorization for entity type prediction in a KB using information within the KB and New York Times articles. However, the method has still only been evaluated at the sentence level. Toral and Munoz (2006), Kazama and Torisawa (2007) use the first line of an entity's Wikipedia article to perform named entity recognition on three entity types.Knowledge Base Completion Much of valuable work in KB completion has focused on the problem of relationship extraction. Majority of the methods conclude missing relationship facts by means of information within the KB (Nickel et al., 2011; Socher et al., 2013; Bordes et al.)."}, {"heading": "7 Conclusion and Future Work", "text": "We have confirmed that our automatic evaluation is correlated with human evaluation, and our data sets and evaluation scripts are available to the public.8 Experimental results show that models trained with our proposed global training target produce higher quality within and between types compared to basic methodologies. In future work, we plan to use information from linked entity documents to improve performance and also explore active reading methods and other human-the-loop methods in order to obtain more training data."}], "references": [{"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Vivek Srikumar", "PeiChun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Christopher D. Manning"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": null, "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Toward an architecture for never-ending language learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Chang et al.2014] Kai-Wei Chang", "Wen tau Yih", "Bishan Yang", "Christopher Meek"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "Language indep endent named entity recognition combining morphological and contextual evidence", "author": ["Cucerzan", "Yarowsky1999] Silviu Cucerzan", "David Yarowsky"], "venue": "In oint SIGDAT Conference on Empirical Methods in Natural Language", "citeRegEx": "Cucerzan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cucerzan et al\\.", "year": 1999}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Entity linking on microblogs with spatial and temporal signals", "author": ["Fang", "Chang2014] Yuan Fang", "Ming-Wei Chang"], "venue": "In Transactions of the Association for Computational Linguistics", "citeRegEx": "Fang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "To link or not to link? a study on end-to-end tweet entity linking", "author": ["Guo et al.2013] Stephen Guo", "Ming-Wei Chang", "Emre Kiciman"], "venue": "In The North American Chapter of the Association for Computational Linguistics.,", "citeRegEx": "Guo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2013}, {"title": "Joint coreference resolution and named-entity linking with multi-pass sieves", "author": ["Leila Zilles", "Daniel S. Weld", "Luke Zettlemoyer"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Hajishirzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hajishirzi et al\\.", "year": 2013}, {"title": "A dual coordinate descent method for largescale linear svm", "author": ["Hsieh et al.2008] Cho-Jui Hsieh", "Kai-Wei Chang", "ChihJen Lin", "S. Sathiya Keerthi", "S. Sundararajan"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Exploiting wikipedia as external knowledge for named entity recognition", "author": ["Kazama", "Torisawa2007] Jun\u2019ichi Kazama", "Kentaro Torisawa"], "venue": null, "citeRegEx": "Kazama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kazama et al\\.", "year": 2007}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Eunsol Choi", "Yoav Artzi", "Luke. Zettlemoyer"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W. Cohen"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "A sequential algorithm for training text classifiers", "author": ["Lewis", "Gale1994] David D. Lewis", "William A. Gale"], "venue": "In ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "Lewis et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 1994}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S. Weld"], "venue": "In Association for the Advancement of Artificial Intelligence", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Introduction to information retrieval", "author": ["Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Association for Computational Linguistics and International Joint Conference on Natural Language Processing", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Mining entity types from query logs via user intent modeling", "author": ["Thomas Lin", "Michael Gamon"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Pantel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pantel et al\\.", "year": 2012}, {"title": "Learning-based multi-sieve co-reference resolution with knowledge", "author": ["Ratinov", "Roth2012] Lev Ratinov", "Dan Roth"], "venue": "In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Ratinov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2012}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In The North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A proposal to automatically", "author": ["Toral", "Munoz2006] Antonio Toral", "Rafael Munoz"], "venue": null, "citeRegEx": "Toral et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Toral et al\\.", "year": 2006}, {"title": "Knowledge base completion via search-based question answering", "author": ["West et al.2014] Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin"], "venue": "In Proceedings of the 23rd international conference on World wide web,", "citeRegEx": "West et al\\.,? \\Q2014\\E", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston et al.2011] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In International Joint Conference on Artificial Intelligence", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["Yao", "Durme2014] Xuchen Yao", "Benjamin Van Durme"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Universal schema for entity type prediction", "author": ["Yao et al.2013] Limin Yao", "Sebastian Riedel", "Andrew McCallum"], "venue": "In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "There is now increasing interest in the construction of knowledge bases like Freebase (Bollacker et al., 2008) and NELL (Carlson et al.", "startOffset": 86, "endOffset": 110}, {"referenceID": 4, "context": ", 2008) and NELL (Carlson et al., 2010) in the natural language processing community.", "startOffset": 17, "endOffset": 39}, {"referenceID": 5, "context": "Entity type information is crucial in KBs and is widely used in many NLP tasks such as relation extraction (Chang et al., 2014), coreference resolution (Ratinov and Roth, 2012; Hajishirzi et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 13, "context": "2013), entity linking (Fang and Chang, 2014), semantic parsing (Kwiatkowski et al., 2013; Berant et al., 2013) and question answering (Bordes et al.", "startOffset": 63, "endOffset": 110}, {"referenceID": 0, "context": "2013), entity linking (Fang and Chang, 2014), semantic parsing (Kwiatkowski et al., 2013; Berant et al., 2013) and question answering (Bordes et al.", "startOffset": 63, "endOffset": 110}, {"referenceID": 3, "context": ", 2013) and question answering (Bordes et al., 2014; Yao and Durme, 2014).", "startOffset": 31, "endOffset": 73}, {"referenceID": 5, "context": "For example, adding entity type information improves relation extraction by 3% (Chang et al., 2014) and entity linking by 4.", "startOffset": 79, "endOffset": 99}, {"referenceID": 9, "context": "2 F1 points (Guo et al., 2013).", "startOffset": 12, "endOffset": 30}, {"referenceID": 18, "context": "Most of previous KBC datasets (Mintz et al., 2009; Riedel et al., 2013) are constructed using a single snapshot of the KB and methods are evaluated on a subset of facts that are hidden during training.", "startOffset": 30, "endOffset": 71}, {"referenceID": 22, "context": "Most of previous KBC datasets (Mintz et al., 2009; Riedel et al., 2013) are constructed using a single snapshot of the KB and methods are evaluated on a subset of facts that are hidden during training.", "startOffset": 30, "endOffset": 71}, {"referenceID": 18, "context": "generally type-based (Mintz et al., 2009; Riedel et al., 2013), measuring the quality of the predictions by aggregating scores computed within a type.", "startOffset": 21, "endOffset": 62}, {"referenceID": 22, "context": "generally type-based (Mintz et al., 2009; Riedel et al., 2013), measuring the quality of the predictions by aggregating scores computed within a type.", "startOffset": 21, "endOffset": 62}, {"referenceID": 18, "context": "In most previous work on KB completion to predict missing relation facts (Mintz et al., 2009; Riedel et al., 2013), the methods are evaluated on a subset of facts from a single KB snapshot, that are hidden while training.", "startOffset": 73, "endOffset": 114}, {"referenceID": 22, "context": "In most previous work on KB completion to predict missing relation facts (Mintz et al., 2009; Riedel et al., 2013), the methods are evaluated on a subset of facts from a single KB snapshot, that are hidden while training.", "startOffset": 73, "endOffset": 114}, {"referenceID": 5, "context": "Notably, Chang et al. (2014) use a two-snapshot strategy for constructing a dataset for relation extraction using automatically constructed", "startOffset": 9, "endOffset": 29}, {"referenceID": 17, "context": "Mean average precision (MAP) (Manning et al., 2008) is now commonly used to evaluate KB completion methods (Mintz et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 18, "context": ", 2008) is now commonly used to evaluate KB completion methods (Mintz et al., 2009; Riedel et al., 2013).", "startOffset": 63, "endOffset": 104}, {"referenceID": 22, "context": ", 2008) is now commonly used to evaluate KB completion methods (Mintz et al., 2009; Riedel et al., 2013).", "startOffset": 63, "endOffset": 104}, {"referenceID": 25, "context": "We use average precision instead of mean reciprocal rank since MRR could be biased to the top predictions of the method (West et al., 2014)", "startOffset": 120, "endOffset": 139}, {"referenceID": 2, "context": "Prior to us, Bordes et al. (2013) use mean reciprocal rank as a global evaluation metric for a KBC task.", "startOffset": 13, "endOffset": 34}, {"referenceID": 18, "context": "Similar to previous work (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013), we get negative training examples by treating the unobserved data in the KB as negative", "startOffset": 25, "endOffset": 87}, {"referenceID": 2, "context": "Similar to previous work (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013), we get negative training examples by treating the unobserved data in the KB as negative", "startOffset": 25, "endOffset": 87}, {"referenceID": 22, "context": "Similar to previous work (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013), we get negative training examples by treating the unobserved data in the KB as negative", "startOffset": 25, "endOffset": 87}, {"referenceID": 11, "context": "Our first algorithm is obtained by using the dual coordinate descent algorithm (Hsieh et al., 2008) to optimize Eq.", "startOffset": 79, "endOffset": 99}, {"referenceID": 7, "context": "Therefore, we adopt an online algorithm, Adagrad (Duchi et al., 2011).", "startOffset": 49, "endOffset": 69}, {"referenceID": 18, "context": "For example, we can consider the work in relation extraction (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205.", "startOffset": 61, "endOffset": 123}, {"referenceID": 2, "context": "For example, we can consider the work in relation extraction (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205.", "startOffset": 61, "endOffset": 123}, {"referenceID": 22, "context": "For example, we can consider the work in relation extraction (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205.", "startOffset": 61, "endOffset": 123}, {"referenceID": 2, "context": ", 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205. These models are trained only using negative entities which we refer to as Negative Entity (NE) objective. The entity type prediction model in Ling and Weld (2012) is a linear model with NE(e, t) = \u2205 which", "startOffset": 8, "endOffset": 252}, {"referenceID": 26, "context": "The embedding model described in Weston et al. (2011)", "startOffset": 33, "endOffset": 54}, {"referenceID": 26, "context": "While the NE or NT objective functions could be suitable for some classification tasks (Weston et al., 2011), the choice of objective functions for the KBC tasks has not been well motivated.", "startOffset": 87, "endOffset": 108}, {"referenceID": 20, "context": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of predicting entity types at the sentence level.", "startOffset": 68, "endOffset": 110}, {"referenceID": 19, "context": "Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al.", "startOffset": 85, "endOffset": 166}, {"referenceID": 14, "context": "Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al.", "startOffset": 85, "endOffset": 166}, {"referenceID": 23, "context": "Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al.", "startOffset": 85, "endOffset": 166}, {"referenceID": 2, "context": "Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al.", "startOffset": 85, "endOffset": 166}, {"referenceID": 15, "context": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of predicting entity types at the sentence level. Yao et al. (2013) develop a method based on matrix factorization for entity type prediction in a KB using information within the KB and New York Times articles.", "startOffset": 69, "endOffset": 229}, {"referenceID": 15, "context": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of predicting entity types at the sentence level. Yao et al. (2013) develop a method based on matrix factorization for entity type prediction in a KB using information within the KB and New York Times articles. However, the method was still evaluated only at the sentence level. Toral and Munoz (2006), Kazama and Torisawa (2007) use the first line of an entity\u2019s Wikipedia article to perform named entity recognition on three entity types.", "startOffset": 69, "endOffset": 463}, {"referenceID": 15, "context": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of predicting entity types at the sentence level. Yao et al. (2013) develop a method based on matrix factorization for entity type prediction in a KB using information within the KB and New York Times articles. However, the method was still evaluated only at the sentence level. Toral and Munoz (2006), Kazama and Torisawa (2007) use the first line of an entity\u2019s Wikipedia article to perform named entity recognition on three entity types.", "startOffset": 69, "endOffset": 491}, {"referenceID": 2, "context": ", 2013; Bordes et al., 2013) while methods such as Mintz et al. (2009) use information in text documents.", "startOffset": 8, "endOffset": 71}, {"referenceID": 2, "context": ", 2013; Bordes et al., 2013) while methods such as Mintz et al. (2009) use information in text documents. Riedel et al. (2013) use both information within and outside the KB to complete the KB.", "startOffset": 8, "endOffset": 127}, {"referenceID": 26, "context": "Linear Embedding Model Weston et al. (2011) is one of first work that developed a supervised linear embedding model and applied it to image retrieval.", "startOffset": 23, "endOffset": 44}], "year": 2015, "abstractText": "Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction. In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention. Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method.", "creator": "LaTeX with hyperref package"}}}