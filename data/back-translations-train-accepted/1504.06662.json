{"id": "1504.06662", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2015", "title": "Compositional Vector Space Models for Knowledge Base Completion", "abstract": "Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z) -&gt; containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recursive neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%.", "histories": [["v1", "Fri, 24 Apr 2015 23:06:10 GMT  (68kb,D)", "https://arxiv.org/abs/1504.06662v1", "The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing, 2015"], ["v2", "Wed, 27 May 2015 21:23:45 GMT  (68kb,D)", "http://arxiv.org/abs/1504.06662v2", "The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing, 2015"]], "COMMENTS": "The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing, 2015", "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["arvind neelakantan", "benjamin roth", "andrew mccallum"], "accepted": true, "id": "1504.06662"}, "pdf": {"name": "1504.06662.pdf", "metadata": {"source": "CRF", "title": "Compositional Vector Space Models for Knowledge Base Completion", "authors": ["Arvind Neelakantan", "Benjamin Roth", "Andrew McCallum"], "emails": ["arvind@cs.umass.edu", "beroth@cs.umass.edu", "mccallum@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to change the world, in which they are able to integrate themselves, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives,"}, {"heading": "2 Background", "text": "We provide background information on PRA, which we use to obtain a set of paths that connect the entity pairs to the RNN model that we use to model the composition function."}, {"heading": "2.1 Path Ranking Algorithm", "text": "Since it is not practical to fully identify all the paths that connect an entity pair in the large KB diagram, we use PRA (Lao et al., 2011) to obtain a set of paths that connect the entity pairs. In the face of a training set of entity pairs for a relationship, PRA heuristically finds a set of paths by taking random walks from the source and target nodes that retain the most common paths. We use PRA to find millions of different paths per relationship type."}, {"heading": "2.2 Recurrent Neural Networks", "text": "Recurrent Neural Network (RNN) (Werbos, 1990) is a neural network that constructs vector representations for sequences (arbitrary length). For example, an RNN model can be used to construct vector representations for phrases or sentences (arbitrary length) in natural language using a composition function (Mikolov et al., 2010; Sutskever et al., 2014; Vinyals et al., 2014). The vector representation of a phrase (w1, w2) consisting of words w1 and w2 is given by f (W [v (w1); v (w2)], where v (w)... Rd is the vector representation of w, f is an elementary non-linearity function, [a; b] the concatenation of two vectors a and b together with a distorted term, and W... Rd x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x is."}, {"heading": "3 Recurrent Neural Networks for KB Completion", "text": "The vector representations of paths (of any length) in the KB diagram are calculated by using the composition function recursively, as in Figure 2. To calculate the vector representations for the higher nodes in the tree, the composition function uses the vector representation of the two child nodes of the node and prints a new vector of the same dimension. Predictions of missing relation types are made by comparing the vector representation of the path with the vector representation of the relationship using the sigmoid function. We represent each binary relationship with a dimensional real weighted vector. We model the composition with recurrent neural networks (Werbos, 1990). We learn a separate composition matrix for each relationship, the vated.Let vr vatervr vatervr vatervatervaterity d caturale representation of the relationship and vp \u00b2 d caturale representation of the relationship."}, {"heading": "3.1 Model Training", "text": "We train the model with the existing facts in a KB with them as positive and negative examples, treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013). In contrast to previous work that used RNNs (Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), one challenge with them for our task is not only to observe a series of paths that connect an entity pair that is the path (s) predictive of a relationship. We select the path that is closest to the type of relationship to be predicted in vector space. This not only allows faster training (compared to marginalization), but also gives improved performance. This technique has been successfully used in models other than RNs (Weston et al., 2013; Neelakantal et al.)."}, {"heading": "4 Zero-shot KB Completion", "text": "Considering the vector representation of relationships, we show that our model described in the previous section is capable of predicting relational facts without explicitly training for the target (or test) relation types (zero-shot learning). In zero-shot or zero-data learning (Larochelle et al., 2008; Palatucci et al., 2009), some labels or classes are not available during model training and only a description of these classes are given in the prediction time. We make two modifications to the model described in the previous section, (1) learn a general composition matrix, and (2) fix relation vectors with pre-formed vectors so that we can predict relationships that are not seen during training. This ability of the model to generalize to invisible relationships goes beyond the capabilities of all predicted methods for KB inference."}, {"heading": "5 Experiments", "text": "All neural network models are trained for 150 iterations with 50 dimensional relation vectors, and we set the L2 regulator and learning rate to 0.0001 and 0.1, respectively. We halved the learning rate after 60 iterations and used size 20 minibatches. Neural networks and classifiers were optimized with AdaGrad (Duchi et al., 2011)."}, {"heading": "5.1 Data", "text": "We have conducted experiments with Freebase (Bollacker et al., 2008), enriched with information from ClueWeb. We use the publicly available entity links to Freebase in the ClueWeb dataset (Orr et al., 2013). Therefore, we create nodes only for freebase entities in our KB chart. We remove facts that contain / type / object / type as they do not provide useful predictive information for our task. We get triples from ClueWeb by looking at sentences that contain two entities associated with Freebase. We extract the phrase between the two entities and treat them as relationship types. For formulations longer than four, we keep only the first and last word. This helps us avoid the time-consuming step of dependence by analyzing the sentence to get the relationship type. These triples resemble the facts that we choose by type 50, the most common (ie.) To evaluate the relatation type (ie."}, {"heading": "5.2 Predictive Paths", "text": "Table 2 shows predictive paths for 4 relationships learned through the RNN model. The high quality of invisible paths is indicative of the fact that the RNN model is able to generalize paths that are never seen during training."}, {"heading": "5.3 Results", "text": "This year, we will be able to put ourselves at the top, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "5.3.1 Zero-shot", "text": "Table 4 shows the results of the zero-shot model described in Section 4 compared to the fully monitored RNN model (Section 3) and a baseline that produces a random order of test facts. We evaluate randomly selected 10 (of 46) relationship types, so we train for the fully monitored version 10 RNNs, one for each relationship type. To evaluate the zero-shot model, we randomly split the relationships into two groups of the same size and train a zero-shot model on one side and test on the other. In this case, we have two RNNNs that make predictions about relationship types that they have never seen during training. As expected, the fully monitored RNN performs the zero-shot model at a great distance, but the zero-shot model without direct monitoring shows significantly better results than a random baseline."}, {"heading": "5.3.2 Discussion", "text": "To investigate whether the performance of the RNNs was affected by multiple local Optima problems, we combined the predictions of five different RNNs trained on all paths. Apart from RNN and RNN random, we trained three other RNNNs with different random initialization and the performance of the three RNNs is individual 57.09, 57.011 and 56.01. The performance of the ensemble is 59.16 and their performance stopped improving after using three RNNNs. Thus, this suggests that although several local Optima affect performance, it is probably not the only problem, since the performance of the ensemble is still lower than the performance of the RNN + PRA classifier-b. We suspect that the RNN model does not capture some important local structures as well as the classifier that uses Bigram features. To overcome this disadvantage, in future work we plan to investigate compositional models that have a longer memory (Cholov, 1997, Cholov, 2014, and 2014)."}, {"heading": "6 Related Work", "text": "KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011), which learn conclusions rules of length one. Schoenmackers et al. (2010) learn general conclusions rules by considering the amount of all paths in KB and selecting paths that meet a certain precision threshold. Their method does not correspond to modern CBs and also depends on carefully calibrated thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as characteristics to perform KB completion, while Gardner et al. (2013) and Gardner et al al al al al. (2014) extend them by using pre-trained relation vectors to overcome feature sparseness."}, {"heading": "7 Conclusion", "text": "We develop a compositional vector space model to complete the knowledge base using relapsing neural networks. In our sophisticated, large-scale dataset available at http: / / iesl.cs.umass.edu / downloads / inferencerules / release.tar.gz, our method outperforms two basic methods and performs competitively with a modified stronger baseline. The best results are obtained by combining the predictions of our model with the predictions of the modified baseline, which achieves a 15% improvement over Gardner et al. (2013)."}, {"heading": "Acknowledgments", "text": "We thank Matt Gardner for publishing the PRA code and for answering numerous questions about the code and the data. We also thank Stanford NLP Group for publishing the code for neural networks. This work was supported in part by the Center for Intelligent Information Retrieval, in part by DARPA under contract number FA8750-13-20020, in part by a prize from Google, and in part by NSF funding # CNS-0958392. The U.S. government is authorized to reproduce and distribute reprints for government purposes, regardless of the copyright notices contained therein. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ArXiv.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Open information extraction from the web", "author": ["Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matt Broadhead", "Oren Etzioni."], "venue": "International Joint Conference on Artificial Intelligence.", "citeRegEx": "Banko et al\\.,? 2007", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Global learning of typed entailment rules", "author": ["Jonathan Berant", "Ido Dagan", "Jacob Goldberger."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Berant et al\\.,? 2011", "shortCiteRegEx": "Berant et al\\.", "year": 2011}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the ACM SIGMOD International Conference on Management of", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Translating embeddings for modeling multirelational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garc\u0131\u0301aDur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Recursive neural networks for learning logical semantics", "author": ["Samuel R. Bowman", "Christopher Potts", "Christopher D Manning."], "venue": "CoRR.", "citeRegEx": "Bowman et al\\.,? 2014", "shortCiteRegEx": "Bowman et al\\.", "year": 2014}, {"title": "Toward an architecture for never-ending language learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka", "A."], "venue": "In AAAI.", "citeRegEx": "Carlson et al\\.,? 2010", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Investigating the role of prior disambiguation in deep-learning compositional models of meaning", "author": ["Cheng", "Jianpeng Kartsaklis", "Edward Grefenstette."], "venue": "In Learning Semantics workshop NIPS.", "citeRegEx": "Cheng et al\\.,? 2014", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Workshop on Syntax, Semantics and Structure in Statistical Translation.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Devise: A deep visualsemantic embedding model", "author": ["Andrea Frome", "Gregory S. Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Improving learning and inference in a large knowledge-base using latent syntactic cues", "author": ["Matt Gardner", "Partha Pratim Talukdar", "Bryan Kisiel", "Tom M. Mitchell."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Gardner et al\\.,? 2013", "shortCiteRegEx": "Gardner et al\\.", "year": 2013}, {"title": "Incorporating vector space similarity in random walk inference over knowledge bases", "author": ["Matt Gardner", "Partha Talukdar", "Jayant Krishnamurthy", "Tom Mitchell."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Gardner et al\\.,? 2014", "shortCiteRegEx": "Gardner et al\\.", "year": 2014}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas K\u00fcchler."], "venue": "IEEE Transactions on Neural Networks.", "citeRegEx": "Goller and K\u00fcchler.,? 1996", "shortCiteRegEx": "Goller and K\u00fcchler.", "year": 1996}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "ArXiv.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Iyyer et al\\.,? 2014", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Ni Lao", "Tom Mitchell", "William W. Cohen."], "venue": "Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Lao et al\\.,? 2011", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Reading the web with learned syntactic-semantic inference rules", "author": ["Ni Lao", "Amarnag Subramanya", "Fernando Pereira", "William W. Cohen."], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Lao et al\\.,? 2012", "shortCiteRegEx": "Lao et al\\.", "year": 2012}, {"title": "Zero-data learning of new tasks", "author": ["Hugo Larochelle", "Dumitru Erhan", "Yoshua Bengio."], "venue": "National Conference on Artificial Intelligence.", "citeRegEx": "Larochelle et al\\.,? 2008", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Dirt - discovery of inference rules from text", "author": ["Dekang Lin", "Patrick Pantel."], "venue": "International Conference on Knowledge Discovery and Data Mining.", "citeRegEx": "Lin and Pantel.,? 2001", "shortCiteRegEx": "Lin and Pantel.", "year": 2001}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Annual Conference of the International Speech Communication Association.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Learning longer memory in recurrent neural networks. In CoRR", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Micha\u00ebl Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction with an incomplete knowledge base", "author": ["Bonan Min", "Ralph Grishman", "Li Wan", "Chang Wang", "David Gondek."], "venue": "HLT-NAACL, pages 777\u2013782.", "citeRegEx": "Min et al\\.,? 2013", "shortCiteRegEx": "Min et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing.", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel."], "venue": "International Conference on Machine Learning.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg Corrado", "Jeffrey Dean."], "venue": "International Conference on Learning", "citeRegEx": "Norouzi et al\\.,? 2014", "shortCiteRegEx": "Norouzi et al\\.", "year": 2014}, {"title": "11 billion clues in 800 million documents: A web research corpus annotated with freebase concepts", "author": ["Dave Orr", "Amarnag Subramanya", "Evgeniy Gabrilovich", "Michael Ringgaard."], "venue": "http://googleresearch.blogspot.com/2013/07/11-", "citeRegEx": "Orr et al\\.,? 2013", "shortCiteRegEx": "Orr et al\\.", "year": 2013}, {"title": "Zero-shot learning with semantic output codes", "author": ["Mark Palatucci", "Dean Pomerleau", "Geoffrey Hinton", "Tom Mitchell."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Palatucci et al\\.,? 2009", "shortCiteRegEx": "Palatucci et al\\.", "year": 2009}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M. Marlin."], "venue": "HLT-NAACL.", "citeRegEx": "Riedel et al\\.,? 2013", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Learning first-order horn clauses from web text", "author": ["Stefan Schoenmackers", "Oren Etzioni", "Daniel S. Weld", "Jesse Davis."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Schoenmackers et al\\.,? 2010", "shortCiteRegEx": "Schoenmackers et al\\.", "year": 2010}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff Chiung-Yu Lin", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML).", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natu-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Richard Socher", "Milind Ganjoo", "Christopher D Manning", "Andrew Ng."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Conference on Empirical Methods in", "citeRegEx": "Socher et al\\.,? 2013c", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Yago: A core of semantic knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum."], "venue": "Proceedings of the 16th International Conference on World Wide Web.", "citeRegEx": "Suchanek et al\\.,? 2007", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "CoRR.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul Werbos."], "venue": "IEEE.", "citeRegEx": "Werbos.,? 1990", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Nonlinear latent factorization by embedding multiple user interests", "author": ["Jason Weston", "Ron Weiss", "Hector Yee."], "venue": "ACM International Conference on Recommender Systems.", "citeRegEx": "Weston et al\\.,? 2013", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "CoRR.", "citeRegEx": "Yang et al\\.,? 2014", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Unsupervised resolution of objects and relations on the web", "author": ["Alexander Yates", "Oren Etzioni."], "venue": "North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Yates and Etzioni.,? 2007", "shortCiteRegEx": "Yates and Etzioni.", "year": 2007}, {"title": "Compositional matrix-space models for sentiment analysis", "author": ["Ainur Yessenalina", "Claire Cardie."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Yessenalina and Cardie.,? 2011", "shortCiteRegEx": "Yessenalina and Cardie.", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "For this reason KBs have been of increasing interest in both industry and academia (Bollacker et al., 2008; Suchanek et al., 2007; Carlson et al., 2010).", "startOffset": 83, "endOffset": 152}, {"referenceID": 40, "context": "For this reason KBs have been of increasing interest in both industry and academia (Bollacker et al., 2008; Suchanek et al., 2007; Carlson et al., 2010).", "startOffset": 83, "endOffset": 152}, {"referenceID": 7, "context": "For this reason KBs have been of increasing interest in both industry and academia (Bollacker et al., 2008; Suchanek et al., 2007; Carlson et al., 2010).", "startOffset": 83, "endOffset": 152}, {"referenceID": 25, "context": "However, even the largest KBs are woefully incomplete (Min et al., 2013), missing many important facts, and therefore damaging their usefulness in downstream tasks.", "startOffset": 54, "endOffset": 72}, {"referenceID": 1, "context": "(2012) greatly increase available raw material for paths by augmenting KB-schema relations with relations defined by the text connecting mentions of entities in a large corpus (also known as OpenIE relations (Banko et al., 2007)).", "startOffset": 208, "endOffset": 228}, {"referenceID": 31, "context": "For example, Schoenmackers et al. (2010) learns Horn clauses predictive of new binary relations by exhausitively exploring relational paths of increasing length, and selecting those surpassing an accuracy threshold.", "startOffset": 13, "endOffset": 41}, {"referenceID": 18, "context": ") Lao et al. (2011) introduced the Path Ranking Algorithm (PRA), which greatly improves efficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier.", "startOffset": 2, "endOffset": 20}, {"referenceID": 18, "context": ") Lao et al. (2011) introduced the Path Ranking Algorithm (PRA), which greatly improves efficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier. A typical predictive feature learned by PRA is that CountryOfHeadquarters(X, Y) is implied by IsBasedIn(X,A) and StateLocatedIn(A, B) and CountryLocatedIn(B, Y). Given IsBasedIn(Microsoft, Seattle), StateLocatedIn(Seattle, Washington) and CountryLocatedIn(Washington, USA), we can infer the fact CountryOfHeadquarters(Microsoft, USA) using the predictive feature. In later work, Lao et al. (2012) greatly increase available raw material for paths by augmenting KB-schema relations with relations defined by the text connecting mentions of entities in a large corpus (also known as OpenIE relations (Banko et al.", "startOffset": 2, "endOffset": 640}, {"referenceID": 33, "context": "Universal schema (Riedel et al., 2013) learns to perform relation prediction cast as matrix completion (likewise using vector embeddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities.", "startOffset": 17, "endOffset": 38}, {"referenceID": 5, "context": "For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations.", "startOffset": 13, "endOffset": 34}, {"referenceID": 5, "context": "For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly Nickel et al. (2011) and Socher et al.", "startOffset": 13, "endOffset": 389}, {"referenceID": 5, "context": "For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly Nickel et al. (2011) and Socher et al. (2013a) perform KB completion by learning embeddings of relations, but based on matrices or tensors.", "startOffset": 13, "endOffset": 415}, {"referenceID": 43, "context": "Our method uses recurrent neural networks (RNNs) (Werbos, 1990) to compose the semantics of relations in an arbitrary-length path.", "startOffset": 49, "endOffset": 63}, {"referenceID": 12, "context": "Related to our work, new versions of PRA (Gardner et al., 2013; Gardner et al., 2014) use pre-trained vector representations of relations to alleviate its feature explosion problem\u2014but the core mechanism continues to be a classifier based on atomic-path features.", "startOffset": 41, "endOffset": 85}, {"referenceID": 13, "context": "Related to our work, new versions of PRA (Gardner et al., 2013; Gardner et al., 2014) use pre-trained vector representations of relations to alleviate its feature explosion problem\u2014but the core mechanism continues to be a classifier based on atomic-path features.", "startOffset": 41, "endOffset": 85}, {"referenceID": 4, "context": "The dataset is build from the combination of Freebase (Bollacker et al., 2008) and Google\u2019s entity linking in ClueWeb (Orr et al.", "startOffset": 54, "endOffset": 78}, {"referenceID": 31, "context": ", 2008) and Google\u2019s entity linking in ClueWeb (Orr et al., 2013).", "startOffset": 47, "endOffset": 65}, {"referenceID": 20, "context": "On this challenging large-scale dataset our compositional method outperforms PRA (Lao et al., 2012), and Cluster PRA (Gardner et al.", "startOffset": 81, "endOffset": 99}, {"referenceID": 12, "context": ", 2012), and Cluster PRA (Gardner et al., 2013) by 11% and 7% respectively.", "startOffset": 25, "endOffset": 47}, {"referenceID": 12, "context": ", 2012), and Cluster PRA (Gardner et al., 2013) by 11% and 7% respectively. A further contribution of our work is a new, surprisingly strong baseline method using classifiers of path bigram features, which beats PRA and Cluster PRA, and statistically ties our compositional method. Our analysis shows that our method has substantially different strengths than the new baseline, and the combination of the two yields a 15% improvement over Gardner et al. (2013). We also show that our zeroshot model is indeed capable of predicting new unseen relation types.", "startOffset": 26, "endOffset": 461}, {"referenceID": 19, "context": "Since it is impractical to exhaustively obtain the set of all paths connecting an entity pair in the large KB graph, we use PRA (Lao et al., 2011) to obtain a set of paths connecting the entity pairs.", "startOffset": 128, "endOffset": 146}, {"referenceID": 43, "context": "Recurrent neural network (RNN) (Werbos, 1990) is a neural network that constructs vector representation for sequences (of any length).", "startOffset": 31, "endOffset": 45}, {"referenceID": 23, "context": "For example, a RNN model can be used to construct vector representations for phrases or sentences (of any length) in natural language by applying a composition function (Mikolov et al., 2010; Sutskever et al., 2014; Vinyals et al., 2014).", "startOffset": 169, "endOffset": 237}, {"referenceID": 41, "context": "For example, a RNN model can be used to construct vector representations for phrases or sentences (of any length) in natural language by applying a composition function (Mikolov et al., 2010; Sutskever et al., 2014; Vinyals et al., 2014).", "startOffset": 169, "endOffset": 237}, {"referenceID": 42, "context": "For example, a RNN model can be used to construct vector representations for phrases or sentences (of any length) in natural language by applying a composition function (Mikolov et al., 2010; Sutskever et al., 2014; Vinyals et al., 2014).", "startOffset": 169, "endOffset": 237}, {"referenceID": 43, "context": "We model composition using recurrent neural networks (Werbos, 1990).", "startOffset": 53, "endOffset": 67}, {"referenceID": 43, "context": "This makes our model a recurrent neural network (Werbos, 1990).", "startOffset": 48, "endOffset": 62}, {"referenceID": 26, "context": "We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013).", "startOffset": 176, "endOffset": 256}, {"referenceID": 19, "context": "We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013).", "startOffset": 176, "endOffset": 256}, {"referenceID": 33, "context": "We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013).", "startOffset": 176, "endOffset": 256}, {"referenceID": 5, "context": "We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013).", "startOffset": 176, "endOffset": 256}, {"referenceID": 35, "context": "Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation.", "startOffset": 37, "endOffset": 102}, {"referenceID": 18, "context": "Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation.", "startOffset": 37, "endOffset": 102}, {"referenceID": 17, "context": "Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation.", "startOffset": 37, "endOffset": 102}, {"referenceID": 44, "context": "This technique has been successfully used in models other than RNNs previously (Weston et al., 2013; Neelakantan et al., 2014).", "startOffset": 79, "endOffset": 126}, {"referenceID": 28, "context": "This technique has been successfully used in models other than RNNs previously (Weston et al., 2013; Neelakantan et al., 2014).", "startOffset": 79, "endOffset": 126}, {"referenceID": 14, "context": "We train the network using backpropagation through structure (Goller and K\u00fcchler, 1996).", "startOffset": 61, "endOffset": 87}, {"referenceID": 21, "context": "In zero-shot or zero-data learning (Larochelle et al., 2008; Palatucci et al., 2009), some labels or classes are not available during training the model and only a description of those classes are given at prediction time.", "startOffset": 35, "endOffset": 84}, {"referenceID": 32, "context": "In zero-shot or zero-data learning (Larochelle et al., 2008; Palatucci et al., 2009), some labels or classes are not available during training the model and only a description of those classes are given at prediction time.", "startOffset": 35, "endOffset": 84}, {"referenceID": 34, "context": "This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014).", "startOffset": 128, "endOffset": 218}, {"referenceID": 19, "context": "This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014).", "startOffset": 128, "endOffset": 218}, {"referenceID": 12, "context": "This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014).", "startOffset": 128, "endOffset": 218}, {"referenceID": 13, "context": "This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014).", "startOffset": 128, "endOffset": 218}, {"referenceID": 33, "context": "We initialize the vector representations of the binary relations (vr) using the representations learned in Riedel et al. (2013) and do not update them during training.", "startOffset": 107, "endOffset": 128}, {"referenceID": 10, "context": "The neural networks and the classifiers were optimized using AdaGrad (Duchi et al., 2011).", "startOffset": 69, "endOffset": 89}, {"referenceID": 4, "context": "We ran experiments on Freebase (Bollacker et al., 2008) enriched with information from ClueWeb.", "startOffset": 31, "endOffset": 55}, {"referenceID": 31, "context": "We use the publicly available entity links to Freebase in the ClueWeb dataset (Orr et al., 2013).", "startOffset": 78, "endOffset": 96}, {"referenceID": 1, "context": "These triples are similar to facts obtained by OpenIE (Banko et al., 2007).", "startOffset": 54, "endOffset": 74}, {"referenceID": 17, "context": "Using our dataset, we compare the performance of the following methods: PRA Classifier is the method in Lao et al. (2012) which trains a logistic regression classifier by creating a feature for every path type.", "startOffset": 104, "endOffset": 122}, {"referenceID": 12, "context": "Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step.", "startOffset": 40, "endOffset": 62}, {"referenceID": 12, "context": "Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type.", "startOffset": 40, "endOffset": 276}, {"referenceID": 12, "context": "Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gardner et al.", "startOffset": 40, "endOffset": 423}, {"referenceID": 12, "context": "Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gardner et al. (2013). Composition-Add uses a simple element-wise addition followed by sigmoid non-linearity as the composition function similar to Yang et al.", "startOffset": 40, "endOffset": 528}, {"referenceID": 12, "context": "Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gardner et al. (2013). Composition-Add uses a simple element-wise addition followed by sigmoid non-linearity as the composition function similar to Yang et al. (2014). RNN-random is the supervised RNN model described in section 3 with the relation vectors initialized randomly.", "startOffset": 40, "endOffset": 673}, {"referenceID": 12, "context": "Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gardner et al. (2013). Composition-Add uses a simple element-wise addition followed by sigmoid non-linearity as the composition function similar to Yang et al. (2014). RNN-random is the supervised RNN model described in section 3 with the relation vectors initialized randomly. RNN is the supervised RNN model described in section 3 with the relation vectors initialized using the method in Riedel et al. (2013). PRA Classifier-b is our simple extension to the method in Lao et al.", "startOffset": 40, "endOffset": 918}, {"referenceID": 12, "context": "Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gardner et al. (2013). Composition-Add uses a simple element-wise addition followed by sigmoid non-linearity as the composition function similar to Yang et al. (2014). RNN-random is the supervised RNN model described in section 3 with the relation vectors initialized randomly. RNN is the supervised RNN model described in section 3 with the relation vectors initialized using the method in Riedel et al. (2013). PRA Classifier-b is our simple extension to the method in Lao et al. (2012) which additionally uses bigrams in the path as features.", "startOffset": 40, "endOffset": 995}, {"referenceID": 12, "context": "Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gardner et al. (2013). Composition-Add uses a simple element-wise addition followed by sigmoid non-linearity as the composition function similar to Yang et al. (2014). RNN-random is the supervised RNN model described in section 3 with the relation vectors initialized randomly. RNN is the supervised RNN model described in section 3 with the relation vectors initialized using the method in Riedel et al. (2013). PRA Classifier-b is our simple extension to the method in Lao et al. (2012) which additionally uses bigrams in the path as features. We add a special start and stop symbol to the path before computing the bigram features. Cluster PRA Classifier-b is our simple extension to the method in Gardner et al. (2013) which additionally uses bigram features computed as previously described.", "startOffset": 40, "endOffset": 1229}, {"referenceID": 12, "context": "First, we show that it is better to train the models using all the path types instead of using only the top 1, 000 path types as done in previous work (Gardner et al., 2013; Gardner et al., 2014).", "startOffset": 151, "endOffset": 195}, {"referenceID": 13, "context": "First, we show that it is better to train the models using all the path types instead of using only the top 1, 000 path types as done in previous work (Gardner et al., 2013; Gardner et al., 2014).", "startOffset": 151, "endOffset": 195}, {"referenceID": 12, "context": "The method described in Gardner et al. (2014) is not included in the table since the publicly available implementation does not scale to our large dataset.", "startOffset": 24, "endOffset": 46}, {"referenceID": 12, "context": "The method described in Gardner et al. (2014) is not included in the table since the publicly available implementation does not scale to our large dataset. First, we show that it is better to train the models using all the path types instead of using only the top 1, 000 path types as done in previous work (Gardner et al., 2013; Gardner et al., 2014). We can see that the RNN model performs significantly better than the baseline methods of Lao et al. (2012) and Gardner et al.", "startOffset": 24, "endOffset": 460}, {"referenceID": 12, "context": "The method described in Gardner et al. (2014) is not included in the table since the publicly available implementation does not scale to our large dataset. First, we show that it is better to train the models using all the path types instead of using only the top 1, 000 path types as done in previous work (Gardner et al., 2013; Gardner et al., 2014). We can see that the RNN model performs significantly better than the baseline methods of Lao et al. (2012) and Gardner et al. (2013). The performance of the RNN model is not affected by initialization since using random vectors and pre-trained vectors results in similar performance.", "startOffset": 24, "endOffset": 486}, {"referenceID": 16, "context": "To overcome this drawback, in future work, we plan to explore compositional models that have a longer memory (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Mikolov et al., 2014).", "startOffset": 109, "endOffset": 183}, {"referenceID": 9, "context": "To overcome this drawback, in future work, we plan to explore compositional models that have a longer memory (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Mikolov et al., 2014).", "startOffset": 109, "endOffset": 183}, {"referenceID": 24, "context": "To overcome this drawback, in future work, we plan to explore compositional models that have a longer memory (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Mikolov et al., 2014).", "startOffset": 109, "endOffset": 183}, {"referenceID": 8, "context": "We also plan to include vector representations for the entities and develop models that address the issue of polysemy in verb phrases (Cheng et al., 2014).", "startOffset": 134, "endOffset": 154}, {"referenceID": 27, "context": "Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011).", "startOffset": 120, "endOffset": 206}, {"referenceID": 2, "context": "Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011).", "startOffset": 120, "endOffset": 206}, {"referenceID": 47, "context": "Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011).", "startOffset": 120, "endOffset": 206}, {"referenceID": 23, "context": "Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al.", "startOffset": 82, "endOffset": 104}, {"referenceID": 41, "context": ", 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 42, "context": ", 2014) and parsing (Vinyals et al., 2014).", "startOffset": 20, "endOffset": 42}, {"referenceID": 35, "context": "Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al.", "startOffset": 126, "endOffset": 147}, {"referenceID": 36, "context": ", 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al.", "startOffset": 34, "endOffset": 101}, {"referenceID": 39, "context": ", 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al.", "startOffset": 34, "endOffset": 101}, {"referenceID": 17, "context": ", 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al.", "startOffset": 34, "endOffset": 101}, {"referenceID": 18, "context": ", 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 6, "context": ", 2014) and natural language logical semantics (Bowman et al., 2014).", "startOffset": 47, "endOffset": 68}, {"referenceID": 13, "context": "KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 13, "context": "KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al.", "startOffset": 39, "endOffset": 87}, {"referenceID": 2, "context": "KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one.", "startOffset": 91, "endOffset": 112}, {"referenceID": 2, "context": "KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold.", "startOffset": 91, "endOffset": 182}, {"referenceID": 2, "context": "KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al.", "startOffset": 91, "endOffset": 435}, {"referenceID": 2, "context": "KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al.", "startOffset": 91, "endOffset": 565}, {"referenceID": 2, "context": "KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity.", "startOffset": 91, "endOffset": 591}, {"referenceID": 2, "context": "KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function.", "startOffset": 91, "endOffset": 698}, {"referenceID": 0, "context": "similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction.", "startOffset": 31, "endOffset": 68}, {"referenceID": 15, "context": "similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction.", "startOffset": 31, "endOffset": 68}, {"referenceID": 38, "context": "(2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014).", "startOffset": 130, "endOffset": 194}, {"referenceID": 11, "context": "(2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014).", "startOffset": 130, "endOffset": 194}, {"referenceID": 30, "context": "(2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014).", "startOffset": 130, "endOffset": 194}, {"referenceID": 0, "context": "similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery.", "startOffset": 32, "endOffset": 247}, {"referenceID": 0, "context": "similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al.", "startOffset": 32, "endOffset": 317}, {"referenceID": 12, "context": "The best results are obtained by combining the predictions of our model with the predictions of the modified baseline which achieves a 15% improvement over Gardner et al. (2013). We also show that our model has the ability to perform zero-shot inference.", "startOffset": 156, "endOffset": 178}], "year": 2015, "abstractText": "Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z)\u2192 containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%.", "creator": "TeX"}}}