{"id": "1504.06937", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2015", "title": "Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits", "abstract": "We study contextual bandits with budget and time constraints under discrete contexts, referred to as constrained contextual bandits. The budget and time constraints significantly increase the complexity of exploration-exploitation tradeoff because they introduce coupling among contexts. Such coupling effects make it difficult to obtain oracle solutions that assume known statistics of bandits. To gain insight, we first study unit-cost systems, where the costs of all actions under any context are identical. We develop near-optimal approximations of the oracle, which are then combined with upper confidence bound (UCB) method in the general case where the expected rewards are unknown a priori. We show that our proposed algorithms, named UCB-PB and UCB-ALP, achieve logarithmic regret except in certain boundary cases. Last, we discuss the extension of the proposed algorithms into general-cost systems.", "histories": [["v1", "Mon, 27 Apr 2015 06:03:50 GMT  (194kb,D)", "https://arxiv.org/abs/1504.06937v1", "23 pages, 4 figures; under submission; partly presented at Information Theory and Applications (ITA) workshop, 2015, San Diego, CA"], ["v2", "Thu, 23 Jul 2015 17:55:35 GMT  (235kb,D)", "http://arxiv.org/abs/1504.06937v2", "36 pages, 4 figures; under submission; partly presented at Information Theory and Applications (ITA) workshop, 2015, San Diego, CA"], ["v3", "Mon, 19 Oct 2015 16:47:20 GMT  (751kb,D)", "http://arxiv.org/abs/1504.06937v3", "36 pages, 4 figures; accepted by the 29th Annual Conference on Neural Information Processing Systems (NIPS), Montr\\'eal, Canada, Dec. 2015"]], "COMMENTS": "23 pages, 4 figures; under submission; partly presented at Information Theory and Applications (ITA) workshop, 2015, San Diego, CA", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["huasen wu", "r srikant", "xin liu 0002", "chong jiang"], "accepted": true, "id": "1504.06937"}, "pdf": {"name": "1504.06937.pdf", "metadata": {"source": "CRF", "title": "Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits", "authors": ["Huasen Wu", "Xin Liu", "Chong Jiang"], "emails": ["hswu@ucdavis.edu", "rsrikant@illinois.edu", "liu@cs.ucdavis.edu", "jiang17@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to survive on their own, and they will be able to survive on their own if they are not able to survive on their own."}, {"heading": "2 System Model", "text": "We look at a contextual bandit problem with a context set X = {1, 2,.,., J} and an action set A = {1, 2,., K}. In each turn comes a context Xt independently with identical distribution P {Xt = j} = \u03c0j, j \u00b2 X, and each action k \u00b2 A generates a non-negative reward Yk, t. In a given context is a context Xt = j, the reward Yk, t's are independent random variables in [0, 1]. The conditional expectation E [Yk, t | Xt = j] = uj, k is unknown to the agent. In addition, costs arise when an action T = j, the reward Yk's is taken under context j. To gain insight into limited contextual bandits, we look at fixed and known costs in this paper, where the cost is cj, k > 0 if the action is k. Similar to traditional contextual bandits."}, {"heading": "3 Approximations of the Oracle", "text": "In this section, we will examine approximations of the oracle where the bandit's statistics are known to the agent. This will provide a yardstick for regret analysis and insights into the design of limited contextual bandit algorithms. As a starting point, we will focus on unit cost systems, i.e., cj, k = 1 for each j and k, from section 3 to section 5, which is in section 6. In unit cost systems, the quality of action k under context j will be fully determined by their expected reward uj, k. Let u-j be the highest expected reward under context j, and k-j be the best action under context j, i.e. u-j = maxk-A uj, k and k-j = arg maxk-A uj, k. To simplify the presentation, we will assume that the best action under each context is unique, i.e. uj, k < u-j = 6 k."}, {"heading": "3.1 Upper Bound: Static Linear Programming", "text": "We propose a cap on the expected total remuneration by loosening the hard constraint to an average constraint and solving the corresponding limited LP problem. Specifically, pj should be the probability that the agent will take action in the context j, and 1 \u2212 pj the probability that the agent will skip the context j (i.e., take action At = 0). Let's call the probability vector p = (p1, p2,., pJ). Let's consider the following LP problem for a time horizon T and budget B: (LPT, B) maximizep J = 1 pj\u03c0ju."}, {"heading": "B, then U\u0302(T,B) \u2265 U\u2217(T,B).", "text": "The proof for Lemma 1 can be found in Appendix A. With Lemma 1 we can tie the regret of any algorithm by comparing its performance with the upper limit U-1 (T, B) rather than U-2 (T, B). Since U-2 (T, B) has a simple expression, as we will see later, it significantly reduces the complexity of the regret analysis."}, {"heading": "3.2 Adaptive Linear Programming", "text": "Although the solution (4) provides a cap on the expected reward, we will not perform well with such a fixed algorithm since the ratio b\u03c4 / \u03c4, referred to as the average residual budget, varies over time. We propose an Adaptive Linear Programming (ALP) algorithm that sets the threshold and randomization probability according to the instantaneous value of b / \u03c4. Specifically, if the remaining time is and the remaining budget can be replaced by a replacement of an LP problem LPT that is the same as LPT, B except that B / T in Eq. (2) is replaced by b / \u0432. Then, the optimal solution for LPT can be achieved by working in Eqs. (3), (4), and (5) with b. The ALP algorithm then makes decisions based on this optimal solution.ALP Algorithm: In each round, the remaining budget is used."}, {"heading": "4 UCB-ALP Algorithm for Constrained Contextual Bandits", "text": "We assume that the agent knows the context distribution as [17], which is relaxed in Section 5. Thanks to the desirable properties of ALP, the maxim of \"optimism under uncertainty\" [8] is still applicable, and ALP can be extended to bandit contexts in combination with estimation guidelines that can quickly provide a correct ranking with a high probability. In combination with the UCB method [4], we propose a UCB-ALP algorithm for restricted context bandits."}, {"heading": "4.1 UCB: Notations and Property", "text": "Let Cj, k (t) be the number of cases in which the action k (t) is under the context j until round t. If Cj, k (t \u2212 1) > 0, u (t) is the empirical reward of the action k under the context j, i.e., u (), k (t) = 1 Cj, k (t) = 1 Yt \u00b2 1 (Xt \u00b2 1), where 1 (\u00b7) is the indicator function. We define the UCBof uj, k (t) = 1 as u (t), k (t) = u (t), k (t) + \u221a t 2Cj, k (t \u2212 1) for Cj, k (t \u2212 1) > 0, and u (t) j (t), k (t) = 1 for Cj (t), k (t \u2212 1) = 0. Furthermore, we define the UCB of the maximum expected reward under the context j &ltj, k (t \u2212 1) > 0, and u (t) > 1, and u (t), and u (t) = 1 for Cj (t), k (t = 1 for Cj (t), k (t) = 0."}, {"heading": "4.2 UCB-ALP Algorithm", "text": "We propose a UCB-based adaptive linear programming algorithm (UCB-ALP) as shown in Algorithm 1. As indicated by the name, the UCB-ALP algorithm maintains estimates of the expected benefits for all context action pairs and then implements the ALP algorithm based on these estimates. Note that the UCB estimates cannot decrease u-ALP algorithms in j (t). Therefore, we use p-j (\u00b7) instead of pj (\u00b7) to indicate this difference. Algorithm 1 UCB-ALP input: time horizon T, budget B and context distribution j (t)'s and may deviate from Equation (4). We use p-j (\u00b7) instead of pj (\u00b7) to indicate this difference."}, {"heading": "4.3 Regret of UCB-ALP", "text": "Specific representations of the regret limits and evidence information can be found in the supplementary facts. (1 \u2264 j \u2264 J) The limits are defined in Section 3. We show that as the budget B and the time horizon T grow to infinity, the proposed UCB-ALP algorithm achieves logarithmic regret, except for the limit cases. Theorem 2. In view of this, uj, k's and a fixed number of UCB-ALP cases, the regret of the UCB-ALP fulfills: 1) (Non-boundary cases), if we have 6 = qj for all j-cases {1, 2,., J \u2212 1}, then the regret of the UCB-ALP-ALP is."}, {"heading": "5 Bandits with Unknown Context Distribution", "text": "If the context distribution is not known, a reasonable heuristic is to replace the probability \u03c0j in ALP with its empirical estimate, i.e., \u03c0 \u0445j (t) = 1t \u2211 t \u2032 = 1 (Xt \u2032 = j). We refer to this modified ALP algorithm as Empirical ALP (EALP) and its combination with UCB as UCB-EALP. Empirical distribution provides a maximum probability estimate of the context distribution, and the EALP and UCB-EALP algorithms achieve similar performance to ALP and UCB-ALP, respectively, as observed in numerical simulations. However, rigorous analysis for EALP and UCBEALP is much more difficult due to the dependence introduced by empirical distribution. To address this problem, our rigorous analysis focuses on a truncated version of EALP, where we stop updating the empirical distribution after a given round."}, {"heading": "6 Bandits with Heterogeneous Costs", "text": "We generalize the ALP algorithm to approximate the oracle and adjust it to the case with unknown expected rewards. For simplicity, we assume that the context distribution is known here, while the empirical estimate can be used to replace the actual context distribution if it is unknown, as discussed in the previous section. At heterogeneous costs, the quality of an action k under a context j is roughly captured by its normalized expected reward, defined as \u03b7j, k = uj, k / cj, k. However, the actor cannot focus only on the \"best\" action, i.e., k = arg maxk, k, for context j. This is because there is another action k \u00b2, so that another action k \u00b2, k \u00b2, k \u00b2 j, k \u00b2 j, k \u00b2 j, but uj > mij, if a problem can be solved in that context."}, {"heading": "7 Conclusion", "text": "Using simplified but practical assumptions, we show that the close interactions between information gathering and decision-making in context-dependent bandits can be decoupled through adaptive linear relaxation. If system statistics are known, the ALP approach achieves near-optimal performance while tolerating certain errors in estimation of system parameters. If the expected rewards are unknown, the proposed UCB-ALP algorithm takes advantage of the benefits of ALP and UCB and achieves O (log T) repentance, except in specific borderline cases where it achieves O (T) repentance. Our study provides an efficient approach to addressing the challenges posed by budget constraints and could potentially be extended to more general context-based bandits."}, {"heading": "Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Proof of Lemma 1: Upper Bound", "text": "Let Cj be the number of rounds that an action in the context j is undertaken for each realization under any possible algorithm with known statistics. Let's let pj = E [Cj] / (\u03c0jD) which fulfills 0 \u2264 pj \u2264 1. Then, the expected total reward becomes \u2211 J = 1 u \u0445 jE [Cj] = T \u0445 J = 1 pj\u03c0ju \u0445 j. Furthermore, since the hard budget constraints for all realizations are met, i.e., for each of these results we have an upper limit of J = 1 pj\u03c0j = \u0445j E [Cj] / T \u2264 B / T. Thus, the expected total reward achieved by any feasible algorithm, including the oracle algorithm, is limited upwards by U (T, B)."}, {"heading": "B Proof of Theorem 1: Near Optimality of ALP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Lemma 2: Evolution of Remaining Budget", "text": "We prove Lemma 2 by pouring ALP into a non-replacement sampling problem. By implementing the ALP, we can verify that when the remaining time is up and the remaining budget is non-replacement, the system consumes a budget unit with the probability b / \u03c4 and uses nothing with the probability 1 \u2212 b / \u03c4. So, if we focus on the remaining budget, we can consider the ALP algorithm as a non-replacement sampling problem. Mapping ALP to non-replacement sampling: Consider T-balls in an urn, including B black balls and T \u2212 B white balls. Running ALP is equivalent to randomly pulling a ball without a replacement. To perform an action under > 0 is equivalent to pulling a black ball and performing the dummy action At = 0 is equivalent to pulling a white ball."}, {"heading": "B.2 Part 1: Non-Boundary Cases", "text": "According to Lemma 1, U-function (T, B) is an upper limit for the expected total remuneration. Therefore, U-plan adoption (T, B) \u2212 plan adoption (T, B) \u2212 plan adoption (T) \u2212 plan adoption (T) \u2212 plan adoption (B, B) = plan adoption (T) \u2212 plan adoption (B) \u2212 plan adoption (B) \u2212 plan adoption (B, B) \u2212 plan adoption (B) \u2212 plan adoption (B, B) \u2212 plan adoption (B, B) \u2212 plan adoption (B) \u2212 plan adoption (B) \u2212 plan adoption (B, B) \u2212 plan adoption (B, B) \u2212 plan adoption (B) \u2212 plan adoption (B, B) \u2212 plan adoption (B, B, B, B () \u2212 (B) \u2212 plan adoption (B, B, B, B () \u2212 plan adoption (B, B) \u2212 plan adoption (T, B) \u2212 plan adoption (T, B) \u2212 plan adoption (T, B) \u2212 plan adoption (T) \u2212 plan adoption (T, B) \u2212 plan adoption (T, B) \u2212 plan adoption (T, B) \u2212 plan adoption (T) \u2212 plan adoption (T, B) \u2212 plan adoption (T) \u2212 plan adoption (T, B) \u2212 plan adoption (T, B) \u2212 plan adoption (B) \u2212 plan adoption (B) \u2212 plan adoption (B, B) \u2212 plan adoption (B, B) \u2212 plan adoption (B, B) \u2212 plan adoption (B, B, B, B, B (B, B, B, B) \u2212 plan adoption (B, B, B, B (B, B, B, B) \u2212 plan adoption (B, B, B (B, B, B) \u2212 plan adoption (B, B, B, B)."}, {"heading": "B.3 Part 2: Boundary Cases", "text": "The proof of Part 2 of Theorem 1 is comparable to that of Part 1. Specifically, we know if either (Part 1) or (Part 2) we (Part 2) we (Part 2) know (Part 2) or (Part 1) we (Part 2) we (Part 2). (Part 2 of Theorem 1 is comparable to Part 1. (Part 1) we know if we know that (Part 1) or (Part 2) we know that (Part 2) or (Part 2) we have (Part 2). (Part 1) We know that (Part 1) or (Part 2) we (Part 1) or (Part 2) we have (Part 2). (Part 2) or (Part 2) we have (Part 2) or (Part 2) or (Part 2). (Part 2) or (Part 2) or (Part 2) we have (Part 2) or (Part 2) we have (Part 2) or (Part 2)."}, {"heading": "C Proof of Theorem 2: Regret of UCB-ALP", "text": "To reach this upper limit, we first divide the regret according to the sources and then include each part of the regret accordingly. Before we provide the proof, we first introduce a notation that will be widely used later on. For contexts j and j \u00b2 and an action k, let us determine the difference between the expected reward for action k in the context j \u00b2 and the highest expected reward in the context j \u00b2, i.e., (j \u00b2) j, k = u \u0445 j \u00b2 \u2212 uj, k. If j \u00b2, (j \u00b2) j \u00b2, k is the difference of the expected reward between the suboptimal action k and the best action in the context j."}, {"heading": "C.1 Step 1: Partition the Regret", "text": "Note that the total reward of the oracle solution U-ALP (T, B) \u2264 U-ALP (T, B) can be compared with U-ALP (T, B), i.e., RUCB \u2212 ALP (T, B) = Tv (T, B) \u2212 J-ALP (T, B) = U-ALP (T, B) \u2212 UUCB \u2212 ALP (T, B) \u2212 ALP (T) = Tv (T) \u2212 J-ALP = 1 K-K (T) = 1 uj, kE [Cj, k (T)]. (15) The total reward of the UCB-ALP can be further divided into UCB \u2212 ALP (T, B) = J-ALP (T). (K-ALK-K = 1 Cj-K (T) \u2212 K \u2212 kJ-ALP can be further divided."}, {"heading": "C.2 Step 2: Bound Each Part of Regret", "text": "C.2.1 Step 2.1: Limit of R (a) UCB-ALP (T, B) = j = j = j (j = j) For regretting mistakes we show in Lemma 4 that R (a) UCB-ALP (T, B) = O (log T) using similar techniques for traditional UCB methods [25].Lemma 4. Under UCB-ALP we consider regret due to errors within the context j \u2212 fiesR (a) UCB-ALP (T) \u2264 J (T, B) \u2264 J (p = 1).Lemma 4. Under UCB-ALP we consider regret due to errors within the context j \u2212 fiesR (a) UCB-ALP (T). For k 6 = k-ALP (T) \u2264 j) j, k = 2 log T (j) j (j), k = 2 log T (j) j (j), k), k), k), k (k), k (T), k), k (k), k (T)."}, {"heading": "2) If V \u2032t = 0, let S\u2032t = 0.", "text": "We can verify that (V) t, S \u2212 t, 1 \u2264 t = 1 \u2264 T) has the same distribution as (1 (E \u00b2 t), St, 1 \u2264 t \u2264 T), so P {CS (T) \u2264 (p \u2212) N, C (T) \u2265 N} = P {p \u00b2 t = 1 S \u00b2 t (p \u2212) N, V \u00b2 t = 1 V \u00b2 n \u00b2. On the other hand, generation S \u00b2 t implies that we T = 1 S \u00b2 t (T) t \u00b2 (T) t \u00b2 (T) t = 1 Wt \u00b2 (T = 1 Wt), the event {p \u00b2 t (T = 1 S \u00b2 t)."}, {"heading": "D Two-Context Systems with Unit-Cost", "text": "As a special case, the oracle algorithm can be used for two-context systems with unit costs. If the context distribution and the expected rewards are unknown, the oracle algorithm can be combined with the UCB method to achieve logarithmic repentance in both borderline and non-borderline cases."}, {"heading": "D.1 Oracle Algorithm: Procrastinate-for-the-Better-context", "text": "If there are only two contexts, the oracle algorithm is trivial. Assuming unit costs, omitting the worse context does not waste opportunities when it comes to a better context. Thus, the agent can reserve the budget for the better context, unless there is a sufficient budget; i.e., we have the following algorithm: Procrastinate-for-the-Better (PB): If Xt = 1 and b\u0442 > 0, or if b\u03c4 \u2264, act At = k \u0445 Xt; otherwise, we can verify that the above PB algorithm achieves the highest expected reward for realizing the context arrival process. Therefore, the PB algorithm is optimal in secondary context systems. We note that the PB algorithm does not need to know the context distribution and only requires the order of the expected rewards. This property allows us to extend it to the case where the context distribution or unknown rewards are expected."}, {"heading": "D.2 UCB-PB: Logarithmic Regret Algorithm for Two-Context Bandits with Unit-Cost", "text": "If the context distribution and expected rewards are unknown, we propose the UCB-based Procrastinate-for-the-Better (UCB-PB) algorithm to solve the contextual bandit problem with two contexts and unit costs. Algorithm 2 UCB-PB input: time horizon T, budget B; init: residual time T = T, residual budget B = B; Cj, k (0), u-j, k (0) = 0, u-j, k (0), for all j-horizons T, budget B; u-T, residual time T = T, residual budget B = B; Cj, k (0), u-j (0), arg maxk u-j (t), k (t), j-j (t), u-j (t), j-j (t), u-j (t), j-j (t), c-j (t), c-j (t)."}, {"heading": "E Constrained Contextual Bandits with Unknown Context Distribution", "text": "In this section we relax the assumption of the known context distribution and study cost systems with unknown context distribution. Since the arrival of the contexts is independent of the measures taken by the agencies, it is a natural idea to implement the ALP or UCB-ALP algorithms based on the empirical distribution as follows: EALP and UCB-EALP algorithms: the agent maintains the empirical distribution of the contexts referred to by the other EALP algorithms. (If the expected rewards are known) or UCB-ALP (if the expected rewards are unknown) algorithms with the context distribution in LPT-1 (Xt). These algorithms are replaced by Empirical ALP (EALP) and UCB-ALP (if the expected rewards are unknown) algorithms with the context distribution in LPT."}, {"heading": "F Constrained Contextual Bandits with Heterogeneous Costs", "text": "In this section, we look at the case where the cost of each action k in the context j is set to cj, k, which can be different for different j and k. We discuss how we can use the insights from unit cost systems in heterogeneous cost systems."}, {"heading": "F.1 Approximation of the Oracle Algorithm", "text": "Similar to unit cost systems, we first examine the case on the basis of known statistics. In Section 3, we generalize the upper limit and the ALP algorithm to general cost systems."}, {"heading": "F.1.1 Upper Bound", "text": "With known statistics, the agent knows the context distribution \u03c0j's, the cost cj's, k's, and the expected rewards uj, k's. In heterogeneous cost systems, the agent cannot focus only on the \"best\" action with the highest normalized reward, i.e., k'j = arg maxk, k = uj, k / cj, k's. This is because there can be another action k's, that there is another action k, k < g < k < p > uj > reward j (and of course cj > cj, k > cj). If there is sufficient budget for context j, the agent can take action to maximize the expected reward."}, {"heading": "F.1.2 ALP Algorithm", "text": "Similar to the unit cost systems, the ALP algorithm replaces the average constraint B / T in LP \"T, B with the average remaining budget B / IV, thus obtaining the probability pj, k (B / IV). In context j, the ALP algorithm can continue to take measures k with the probability pj, k (B / IV). Unlike the unit cost systems, the remaining budget does not follow a classical distribution in heterogeneous cost systems. However, we can show that the concentration property still applies in this general case by applying the method of averaged boundary differences [23]. Lemma 12. For 0 < T, there is a positive number, so that within the ALP algorithm the remaining budget differences are satisfied."}, {"heading": "F.2 -First ALP Algorithm", "text": "If the expected rewards are unknown, it is difficult to combine the UCB method with the proposed ALP method, since the ALP method in this case requires not only the ordering of the ALP algorithms, but also the ordering of the ALP algorithms."}, {"heading": "F.3 Deciding (T ) without Prior Information", "text": "() () () () () () () () () ()) () () () () ()) () () ()) () () ()) () ()) () ()) () ()) () () () ()) () () ()) () () () ()) () () ()) () () () ()) () ()) () () () () () () () ()) () () () () ()) () () ()) () ()) () () () () ()) () () () () ()) () () () () () () () () () () () () () () () () ()) () () () ()) () () () () () () () ()) () () () ()) () () () ()) () () () ()) () () () ()) () ()) () () () ()) () ()) () () ()) () () () ()) () () ()) () () ()) () () ()) () () () () () ()) () () ()) () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () (() () () (() () () () () ((() () () () (() () () (() () () (() (() () (() (() () (((() () () (((() () (() () () ((("}, {"heading": "G Numerical Experiments", "text": "In this section, we evaluate the regret of the proposed algorithms through numerical simulations. We examine the performance of the proposed algorithms here for unit cost systems, as the parameter setting is relatively easy to control and provides us with useful insights. Performance in heterogeneous cost systems is similar to what we theoretically demonstrated and omitted here. In the case of known statistics, we compare the proposed BP (dual context case) and ALP algorithms with the Fixed LP (FLP) algorithm, which uses a fixed average budget constraint B / T, since both [17] and [20] use a fixed average budget constraint. Then, the UCB-based FLP, i.e. UCB-FLP, is evaluated in the case without knowledge of the expected rewards. We also evaluate algorithms in the case without knowledge of the context distribution. If the context distribution is unknown to the agent, we use the Empirical ALP (EUCB-FLP) algorithm, which is known for the empirical distributions used for the case."}, {"heading": "G.1 Two-Context Systems", "text": "0 2 4 6 10x 10 5050100150200Horizon TR egre tFLP ALP EALP (a) 0 2 4 6 10x 10 5050100150200Horizon TR egre tFLP ALP EALP (b) 0 2 4 6 8 10x 10 5050100150200Horizon TR egre tFLP EALP (c) Figure 1 \u2212 \u2212 Comparison of algorithms for UCP systems with perfect knowledge (\u03c01 = 0.4, \u03c02 = 0.6), (a) \u03c1 = 0.39, (b) \u03c1 TR egre ALP EALP (c).Let us first consider a UCP context scenario with K = 3 UCP arms and Bernoulli rewards: the context distribution vector is \u03c0 = [0.4, 0.6], the expected rewards are u1 = 0.8 \u00d7 100ms [1, 2 / 3, 1] for the context, u2 = 0.4 / 3]."}, {"heading": "G.2 Multi-Context Systems", "text": "Next, we examine a multi-context scenario with J = 10 contexts, K = 5 poor and Bernoulli rewards. Specifically, the context distribution vector is \u03c0 = [0.025, 0.05, 0.075, 0.15, 0.2, 0.15, 0.075, 0.05, 0.025]. The expected reward of the action k in the context j is uj, k = jkJK. One limit in this system is q5 = 0.5. We examine cases with average budget \u03c1 = 0.49, 0.5, and 0.51, respectively. In this case, it is difficult to calculate the expected total reward obtained by the oracle solution. Therefore, we calculate the regret by comparing different algorithms in the case with the upper limit, i.e., U \u043d\u043e\u043d\u043e\u043b\u0435\u043b\u0435\u043d\u0435\u043d\u0435\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u0439 (T, B) = TV. Thus, we calculate the regret of different algorithms in the case with both known and non-known ALP rewards in the context."}], "references": [{"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["J. Langford", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 817\u2013824,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Contextual multi-armed bandits", "author": ["T. Lu", "D. P\u00e1l", "M. P\u00e1l"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 485\u2013492,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey on contextual multi-armed bandits", "author": ["L. Zhou"], "venue": "arXiv preprint arXiv:1508.03326,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, 47(2-3):235\u2013256,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "ACM International Conference on World Wide Web (WWW), pages 661\u2013670,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Contextual bandits with similarity information", "author": ["A. Slivkins"], "venue": "The Journal of Machine Learning Research, 15(1):2533\u20132568,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["A. Agarwal", "D. Hsu", "S. Kale", "J. Langford", "L. Li", "R.E. Schapire"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["P. Auer", "R. Ortner"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 49\u201356,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning on a budget: posted price mechanisms for online procurement", "author": ["A. Badanidiyuru", "R. Kleinberg", "Y. Singer"], "venue": "ACM Conference on Electronic Commerce, pages 128\u2013145,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient adaptive randomization and stopping rules in multi-arm clinical trials for testing a new treatment", "author": ["T.L. Lai", "O.Y.-W. Liao"], "venue": "Sequential Analysis, 31(4):441\u2013457,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Knapsack based optimal policies for budget-limited multi-armed bandits", "author": ["L. Tran-Thanh", "A.C. Chapman", "A. Rogers", "N.R. Jennings"], "venue": "AAAI Conference on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Bandits with knapsacks", "author": ["A. Badanidiyuru", "R. Kleinberg", "A. Slivkins"], "venue": "IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS), pages 207\u2013216,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Bandits with budgets", "author": ["C. Jiang", "R. Srikant"], "venue": "IEEE 52nd Annual Conference on Decision and Control (CDC), pages 5345\u20135350,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic ad allocation: Bandits with budgets", "author": ["A. Slivkins"], "venue": "arXiv preprint arXiv:1306.0155,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Thompson sampling for budgeted multi-armed bandits", "author": ["Y. Xia", "H. Li", "T. Qin", "N. Yu", "T.-Y. Liu"], "venue": "International Joint Conference on Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Bandits with budgets: Regret lower bounds and optimal algorithms", "author": ["R. Combes", "C. Jiang", "R. Srikant"], "venue": "ACM Sigmetrics,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Resourceful contextual bandits", "author": ["A. Badanidiyuru", "J. Langford", "A. Slivkins"], "venue": "Conference on Learning Theory (COLT),", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal rate sampling in 802.11 systems", "author": ["R. Combes", "A. Proutiere", "D. Yun", "J. Ok", "Y. Yi"], "venue": "In IEEE INFOCOM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Approximate linear programming for average cost MDPs", "author": ["M.H. Veatch"], "venue": "Mathematics of Operations Research, 38(3):535\u2013544,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Bandits with concave rewards and convex knapsacks", "author": ["S. Agrawal", "N.R. Devanur"], "venue": "ACM Conference on Economics and Computation, pages 989\u20131006. ACM,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual bandits with global constraints and objective", "author": ["S. Agrawal", "N.R. Devanur", "L. Li"], "venue": "arXiv preprint arXiv:1506.03374,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Linear contextual bandits with global constraints and objective", "author": ["S. Agrawal", "N.R. Devanur"], "venue": "arXiv preprint arXiv:1507.06738,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Concentration of measure for the analysis of randomized algorithms", "author": ["D.P. Dubhashi", "A. Panconesi"], "venue": "Cambridge University Press,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "Conference on Learning Theory (COLT), pages 359\u2013376,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6(1):4\u201322,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1985}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction The contextual bandit problem [1, 2, 3] is an important extension of the classic multi-armed bandit (MAB) problem [4], where the agent can observe a set of features, referred to as context, before making a decision.", "startOffset": 45, "endOffset": 54}, {"referenceID": 1, "context": "1 Introduction The contextual bandit problem [1, 2, 3] is an important extension of the classic multi-armed bandit (MAB) problem [4], where the agent can observe a set of features, referred to as context, before making a decision.", "startOffset": 45, "endOffset": 54}, {"referenceID": 2, "context": "1 Introduction The contextual bandit problem [1, 2, 3] is an important extension of the classic multi-armed bandit (MAB) problem [4], where the agent can observe a set of features, referred to as context, before making a decision.", "startOffset": 45, "endOffset": 54}, {"referenceID": 3, "context": "1 Introduction The contextual bandit problem [1, 2, 3] is an important extension of the classic multi-armed bandit (MAB) problem [4], where the agent can observe a set of features, referred to as context, before making a decision.", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "Existing works try to reduce the regret of contextual bandits by leveraging the structure of the context-reward models such as linearity [5] or similarity [6], and more recent work [7] focuses on computationally efficient algorithms with minimum regret.", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "Existing works try to reduce the regret of contextual bandits by leveraging the structure of the context-reward models such as linearity [5] or similarity [6], and more recent work [7] focuses on computationally efficient algorithms with minimum regret.", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "Existing works try to reduce the regret of contextual bandits by leveraging the structure of the context-reward models such as linearity [5] or similarity [6], and more recent work [7] focuses on computationally efficient algorithms with minimum regret.", "startOffset": 181, "endOffset": 184}, {"referenceID": 7, "context": "For Markovian context arrivals, algorithms such as UCRL [8] for more general reinforcement learning problem can be used to achieve logarithmic regret.", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "Taking crowdsourcing [9] as an example, the budget constraint for a given set of tasks will limit the number of workers that an employer can hire.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "Another example is the clinical trials [10], where each treatment is usually costly and the budget of a trial is limited.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "Although budget constraints have been studied in non-contextual bandits where logarithmic or sublinear regret is achieved [11, 12, 13, 14, 15, 16], as we will see later, these results are inapplicable in the case with observable contexts.", "startOffset": 122, "endOffset": 146}, {"referenceID": 11, "context": "Although budget constraints have been studied in non-contextual bandits where logarithmic or sublinear regret is achieved [11, 12, 13, 14, 15, 16], as we will see later, these results are inapplicable in the case with observable contexts.", "startOffset": 122, "endOffset": 146}, {"referenceID": 12, "context": "Although budget constraints have been studied in non-contextual bandits where logarithmic or sublinear regret is achieved [11, 12, 13, 14, 15, 16], as we will see later, these results are inapplicable in the case with observable contexts.", "startOffset": 122, "endOffset": 146}, {"referenceID": 13, "context": "Although budget constraints have been studied in non-contextual bandits where logarithmic or sublinear regret is achieved [11, 12, 13, 14, 15, 16], as we will see later, these results are inapplicable in the case with observable contexts.", "startOffset": 122, "endOffset": 146}, {"referenceID": 14, "context": "Although budget constraints have been studied in non-contextual bandits where logarithmic or sublinear regret is achieved [11, 12, 13, 14, 15, 16], as we will see later, these results are inapplicable in the case with observable contexts.", "startOffset": 122, "endOffset": 146}, {"referenceID": 15, "context": "Although budget constraints have been studied in non-contextual bandits where logarithmic or sublinear regret is achieved [11, 12, 13, 14, 15, 16], as we will see later, these results are inapplicable in the case with observable contexts.", "startOffset": 122, "endOffset": 146}, {"referenceID": 16, "context": "The above constrained contextual bandit problem can be viewed as a special case of Resourceful Contextual Bandits (RCB) [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "In [17], RCB is studied under more general settings with possibly infinite contexts, random costs, and multiple budget constraints.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "However, the benchmark for the definition of regret in [17] is restricted to within a finite policy set.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "This simplified model is justified in many scenarios such as clinical trials [10] and rate selection in wireless networks [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "This simplified model is justified in many scenarios such as clinical trials [10] and rate selection in wireless networks [18].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": ", [17, 19]), the design and analysis of ALP here is quite different.", "startOffset": 2, "endOffset": 10}, {"referenceID": 18, "context": ", [17, 19]), the design and analysis of ALP here is quite different.", "startOffset": 2, "endOffset": 10}, {"referenceID": 3, "context": "In this paper, we propose a UCB-ALP algorithm by combining ALP with the upper-confidence-bound (UCB) method [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "We note that UCB-type algorithms are proposed in [20] for non-contextual bandits with concave rewards and convex constraints, and further extended to linear contextual bandits.", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "However, [20] focuses on static contexts1 and achieves O( \u221a T ) regret in our setting since it uses a fixed budget constraint in each round.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "In comparison, we consider random context arrivals and use an adaptive After the online publication of our preliminary version, two recent papers [21, 22] extend their previous work [20] to the dynamic context case, where they focus on possibly infinite contexts and achieve O( \u221a T ) regret, and [21] restricts to a finite policy set as [17].", "startOffset": 146, "endOffset": 154}, {"referenceID": 21, "context": "In comparison, we consider random context arrivals and use an adaptive After the online publication of our preliminary version, two recent papers [21, 22] extend their previous work [20] to the dynamic context case, where they focus on possibly infinite contexts and achieve O( \u221a T ) regret, and [21] restricts to a finite policy set as [17].", "startOffset": 146, "endOffset": 154}, {"referenceID": 19, "context": "In comparison, we consider random context arrivals and use an adaptive After the online publication of our preliminary version, two recent papers [21, 22] extend their previous work [20] to the dynamic context case, where they focus on possibly infinite contexts and achieve O( \u221a T ) regret, and [21] restricts to a finite policy set as [17].", "startOffset": 182, "endOffset": 186}, {"referenceID": 20, "context": "In comparison, we consider random context arrivals and use an adaptive After the online publication of our preliminary version, two recent papers [21, 22] extend their previous work [20] to the dynamic context case, where they focus on possibly infinite contexts and achieve O( \u221a T ) regret, and [21] restricts to a finite policy set as [17].", "startOffset": 296, "endOffset": 300}, {"referenceID": 16, "context": "In comparison, we consider random context arrivals and use an adaptive After the online publication of our preliminary version, two recent papers [21, 22] extend their previous work [20] to the dynamic context case, where they focus on possibly infinite contexts and achieve O( \u221a T ) regret, and [21] restricts to a finite policy set as [17].", "startOffset": 337, "endOffset": 341}, {"referenceID": 0, "context": "Under a given context Xt = j, the reward Yk,t\u2019s are independent random variables in [0, 1].", "startOffset": 84, "endOffset": 90}, {"referenceID": 0, "context": "Specifically, let pj \u2208 [0, 1] be the probability that the agent takes action k\u2217 j for context j, and 1\u2212 pj be the probability that the agent skips context j (i.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "p \u2208 [0, 1] .", "startOffset": 4, "endOffset": 10}, {"referenceID": 3, "context": "This highly desirable feature allows us to combine ALP with classic MAB algorithms such as UCB [4] for the case without knowledge of expected rewards.", "startOffset": 95, "endOffset": 98}, {"referenceID": 22, "context": "Thus, we can show that b\u03c4 follows the hypergeometric distribution [23] and has the following properties: Lemma 2.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "We assume the agent knows the context distribution as [17], which will be relaxed in Section 5.", "startOffset": 54, "endOffset": 58}, {"referenceID": 7, "context": "[8] is still applicable and ALP can be extended to the bandit settings when combined with estimation policies that can quickly provide correct ranking with high probability.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Here, combining ALP with the UCB method [4], we propose a UCB-ALP algorithm for constrained contextual bandits.", "startOffset": 40, "endOffset": 43}, {"referenceID": 23, "context": "As suggested in [24], we use a smaller coefficient in the exploration term \u221a log t 2Cj,k(t\u22121) than the traditional UCB algorithm [4] to achieve better performance.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "As suggested in [24], we use a smaller coefficient in the exploration term \u221a log t 2Cj,k(t\u22121) than the traditional UCB algorithm [4] to achieve better performance.", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "This property has been widely applied in the analysis of UCBbased algorithms [4, 13], and its proof can be found in [13, 25] with a minor modification on the coefficients.", "startOffset": 77, "endOffset": 84}, {"referenceID": 12, "context": "This property has been widely applied in the analysis of UCBbased algorithms [4, 13], and its proof can be found in [13, 25] with a minor modification on the coefficients.", "startOffset": 77, "endOffset": 84}, {"referenceID": 12, "context": "This property has been widely applied in the analysis of UCBbased algorithms [4, 13], and its proof can be found in [13, 25] with a minor modification on the coefficients.", "startOffset": 116, "endOffset": 124}, {"referenceID": 24, "context": "For the non-boundary cases, UCB-ALP is order-optimal because obtaining the correct action ranking under each context will result in O(log T ) regret [26].", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "Note that our results do not contradict the lower bound in [17] because we consider discrete contexts and actions, and focus on instance-dependent regret.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "References [1] J.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Based on the above mapping and using its symmetric property, we know that b\u03c4 follows the hypergeometric distribution [23] and complete the proof of Lemma 2.", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "Similar to the analysis of UCB in [4], we have", "startOffset": 34, "endOffset": 37}, {"referenceID": 22, "context": "To show the concentration of b\u03c4/\u03c4 , we first use the coupling argument to show the following lemma and then use the method of averaged bounded differences [23].", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "To use the method of averaged bounded differences [23], we note that T\u2212\u03c4 \u2211 t\u2032=T1 [ 2 ( \u03c4 T \u2212 t\u2032 )1\u2212\u03c3]2 \u2264 4\u03c42\u22122\u03b1 \u00b7 [ 1 \u03c41\u22122\u03c3 \u2212 1 (T \u2212 T1 + 1)1\u22122\u03c3 ] \u2264 4\u03c4.", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "1 in [23] and Lemma 9, we have P { |b\u03c4 \u2212 E[b\u03c4 ]| \u2265 \u03b4\u03c4/4 } \u2264 2 exp ( \u2212 2(\u03b4\u03c4/4) 2", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "pj,k \u2208 [0, 1].", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "pj,k \u2208 [0, 1].", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "where p\u0303j,kj,a \u2208 [0, 1], and p\u0303j,kj,a \u2265 p\u0303j,kj,a+1 for 1 \u2264 a \u2264 Kj \u2212 1.", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "a=1 p\u0303j,kj,a c\u0303j,kj,a \u2264 \u03c1j , p\u0303j,kj,a \u2265 p\u0303j,kj,a+1 , 1 \u2264 a \u2264 Kj \u2212 1, (61) p\u0303j,kj,a \u2208 [0, 1], \u2200a, where", "startOffset": 85, "endOffset": 91}, {"referenceID": 0, "context": "a=1 \u03c0j p\u0303j,kj,a c\u0303j,kj,a \u2264 B/T, p\u0303j,kj,a \u2208 [0, 1], \u2200j, and 1 \u2264 a \u2264 Kj .", "startOffset": 43, "endOffset": 49}, {"referenceID": 22, "context": "However, we can show that the concentration property still holds for this general case by using the method of averaged bounded differences [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "We prove the lemma using the method of averaged bounded differences [23].", "startOffset": 68, "endOffset": 72}, {"referenceID": 22, "context": "1 in [23], except that we consider the remaining budget and the successive differences of the remaining budget are bounded by cmax.", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "3 in [23], and noting \u03c4 = T \u2212 t+ 1, E[b\u03c4 ] = \u03c1\u03c4 , we have P{b\u03c4 > E[b\u03c4 ] + \u03b4\u03c4} \u2264 e \u2212 2T (\u03b4\u03c1\u03c4) 2 4cmax(T\u2212t+1)(t\u22121) \u2264 e T\u03b42B2\u03c4 2cmaxT 2(t\u22121) \u2264 e \u03b42\u03c12 2cmax \u03c4 , (66) and similarly, P{b\u03c4 < E[b\u03c4 ]\u2212 \u03b4\u03c4} \u2264 e \u2212 \u03b4 2\u03c12 2cmax \u03c4 , (67) Choosing \u03ba = \u03c1 2 2cmax concludes the proof.", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "In the case with known statistics, we compare the proposed PB (two-context case) and ALP algorithms with Fixed LP (FLP) algorithm that uses a fixed average budget constraint B/T since both [17] and [20] use fixed average budget constraint.", "startOffset": 189, "endOffset": 193}, {"referenceID": 19, "context": "In the case with known statistics, we compare the proposed PB (two-context case) and ALP algorithms with Fixed LP (FLP) algorithm that uses a fixed average budget constraint B/T since both [17] and [20] use fixed average budget constraint.", "startOffset": 198, "endOffset": 202}, {"referenceID": 5, "context": ", similarity [6] and linearity [5], to reduce the exploration time is part of our future work.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": ", similarity [6] and linearity [5], to reduce the exploration time is part of our future work.", "startOffset": 31, "endOffset": 34}], "year": 2015, "abstractText": "We study contextual bandits with budget and time constraints, referred to as constrained contextual bandits. The time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time. To gain insight, we first study unit-cost systems with known context distribution. When the expected rewards are known, we develop an approximation of the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves near-optimality and only requires the ordering of expected rewards. With these highly desirable features, we then combine ALP with the upper-confidence-bound (UCB) method in the general case where the expected rewards are unknown a priori. We show that the proposed UCB-ALP algorithm achieves logarithmic regret except for certain boundary cases. Further, we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs. To the best of our knowledge, this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits. Moreover, this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits.", "creator": "LaTeX with hyperref package"}}}