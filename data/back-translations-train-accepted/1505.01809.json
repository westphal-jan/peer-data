{"id": "1505.01809", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2015", "title": "Language Models for Image Captioning: The Quirks and What Works", "abstract": "Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of the different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of both the ME and RNN methods, we achieve a new record performance on the benchmark COCO dataset.", "histories": [["v1", "Thu, 7 May 2015 18:36:14 GMT  (684kb,D)", "https://arxiv.org/abs/1505.01809v1", "Seethis http URLfor project information"], ["v2", "Mon, 20 Jul 2015 22:10:49 GMT  (1039kb,D)", "http://arxiv.org/abs/1505.01809v2", "Seethis http URLfor project information"], ["v3", "Wed, 14 Oct 2015 22:03:40 GMT  (1033kb,D)", "http://arxiv.org/abs/1505.01809v3", "Seethis http URLfor project information"]], "COMMENTS": "Seethis http URLfor project information", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["jacob devlin", "hao cheng", "hao fang", "saurabh gupta", "li deng", "xiaodong he", "geoffrey zweig", "margaret mitchell"], "accepted": true, "id": "1505.01809"}, "pdf": {"name": "1505.01809.pdf", "metadata": {"source": "CRF", "title": "Language Models for Image Captioning: The Quirks and What Works", "authors": ["Jacob DevlinF", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong HeF", "Geoffrey ZweigF", "Margaret MitchellF"], "emails": ["jdevlin@microsoft.com", "xiaohe@microsoft.com", "gzweig@microsoft.com", "memitc@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Two leading approaches have been studied for this task: the first breaks down the problem into a first step that uses a revolutionary neural network to predict a handful of words likely to be present in a caption; then, in a second step, uses a maximum entropy language model (ME LM) to generate a sentence that includes a minimum number of words captured (Fang et al., 2015); and the second approach uses activations from the last hidden layer of an object that CNN uses as input for a recursive neural language model (RNN LM), referred to as a multimodal recurrent network (MRNN) and Fei file, 2015; Mao et al., 2015; Chen and Zitnick, 2015."}, {"heading": "2 Models", "text": "All the language models compared here are trained using the same state-of-the-art CNN. CNN uses the 16-layer variant of VGGNet (Simonyan and Zisserman, 2014), which was first trained for the ILSVRC2014 classification task (Russakovsky et al., 2015) and then refined on the Microsoft COCO dataset (Fang et al., 2015; Lin et al., 2014)."}, {"heading": "2.1 Detector Conditioned Models", "text": "We examine the effect of using an explicit recognition step to find key objects / attributes in images before creating them, and examine both a ME LM approach, as described in previous work (Fang et al., 2015), and a novel LSTM approach introduced here. Both use a CNN that is trained to output words likely to appear in a caption, and both use a beam search to find a sentence with the highest score containing a subset of words. This set of words is dynamically adjusted to remove words as they are mentioned. We refer the reader to Fang et al. (2015) for a full description of their ME LM approach, the 500-best results of which we analyze here.4 We also include the results from their ME LM, which uses results from a Deep Multimodal Similarity Model (DMSM), while the n-best re-ranking of both words is a neural model, with the SM not the SM-generative one."}, {"heading": "2.2 Multimodal Recurrent Neural Network", "text": "In this section, we examine a model that is conditioned directly on CNN activations and not on a set of word identifiers. Our implementation is very similar to the subtitle models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al. (2015), and Donahue et al. (2014). This common vision language RNN is called a multimodal recursive neural network (MRNN). In this model, we feed each image into our CNN and pick up the 4096-dimensional last hidden layer, called fc7. The fc7 vector is then fed into a hidden layer H to obtain a 500-dimensional representation that serves as an initial hidden state to a gated recurrent neural network (GRNN) (Cho et al., 2014). The GRNN is trained together with H to produce the word at a time."}, {"heading": "3 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Microsoft COCO Dataset", "text": "We work with the Microsoft COCO dataset (Lin et al., 2014) with 82,783 training images and the validation set split into 20,243 validation images and 20,244 test images. Most images contain multiple objects and important context information, and each image is accompanied by 5 Human-6Each training image has 5 captions."}, {"heading": "LM PPLX BLEU METEOR", "text": "Commented Captions: The images provide a sophisticated test environment for captions and have recently been widely used in automatic captioning."}, {"heading": "3.2 Metrics", "text": "The quality of the generated captions is automatically measured using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). BLEU roughly measures the fraction of N -gram (up to 4 grams) common between a hypothesis and one or more references, punishing short hypotheses with a short sentence. 7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extends exact word matches by similar words based on WordNet synonyms and retained tokens. We also report on the perplexity (PPLX) of detected conditioned LMs examined. The PPLX is in many respects the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2013)."}, {"heading": "3.3 Model Comparison", "text": "Table 1 summarizes the generation performance of our various models, and the discrete detection models are preceded by \"D.\" Some exemplarily generated results show in Table 2. We see that the detection-based LSTM LM produces much lower PPLX than the detection-based ME LM, but its BLEU score is no better. MRNN has the lowest PPLX and the highest BLEU of all LMs stud-7We use the length of reference closest to the length of the hypothesis to calculate the Brevity Penalty.ied in our experiments, which significantly improves BLEU by 2.1 absolutely above the D-ME LM baseline. METEOR is comparable in all three LM-based methods.Perhaps most astoundingly, the k-next neighbor algorithm achieves a higher BLEU score than all other models, but as we will show in Section 3.5, the descriptions produced are significantly better than the comparable neighbor descriptions in terms of human quality."}, {"heading": "3.5 Human Evaluation", "text": "Since automatic measures do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human assessments using the same procedure as in Fang et al. (2015). Here, human judges were confronted with an image, a system-generated caption and a human-generated caption and asked which caption was \"better.\" 9 For each condition, 5 assessments were obtained for 1000 images from the test set. 8The MRNN does not provide a diverse n-score. 9Captions were randomized and users were not informed which one was better. The results are in Table 4. The DME + DMSM exceeds the MRNN by 5 percentage points for the \"Better or Equal to Human\" verdict, even though both systems achieved the same BLEU score. The k-Nearest Neighbor system performs 1.4 percentage points worse than the MRNN, although it slightly exceeds the EU + BLNME + 5 score, we do not match the SME + 5 score accurately."}, {"heading": "4 Language Analysis", "text": "The D-ME system has difficulties with the anaphora, especially within the phrase \"about,\" as shown in Examples (1), (2) and (3). This is probably due to the fact that a local context window is maintained. In contrast, the MRNN approach tends to create such anaphorical relationships correctly. However, the D-ME LM maintains an explicit coverage state trace of which attributes have already been emitted. MRNN implicitly maintains the full state by using its recursive layer, which sometimes results in multiple emission errors where the same attribute is emitted more than once, which is particularly evident when coordination (\"and\") is present (Examples (4) and (5)))."}, {"heading": "4.1 Repeated Captions", "text": "All of our models produce a large number of captions that can be seen in training and are repeated for different images in the test kit, as shown in Table 6 (which is also observed by Vinyals et al. (2014) for their LSTM-based model).There are at least two possible causes for this repetition.First, the systems often produce generic captions such as \"a close-up of a plate of food\" that can be applied to many publicly available images, which could indicate a deeper problem in training and evaluating our models, which should be discussed more in future work. Second, although the COCO dataset and evaluation server10 have facilitated rapid progress in captioning, there may be a lack of data diversity. We also point out that captions duplication is a problem in all systems, but a bigger problem in the MRNN than the D-ME + DMSM."}, {"heading": "5 Image Diversity", "text": "The strong performance of the k-next object category algorithm and the large number of repetitive captions produced by the systems here indicate a lack of diversity in the training and test data. 11We believe that one reason to work on captions is to be able to create new captions where the individual components of the image can be seen in training, but the overall composition often does not match. 11To evaluate the results for only compositively novel images, we sort the test images based on visual overlaps with the training data. For each test image, we calculate the cosmic similarity fc7 with each training image and the mean of the 50 closest images. We then calculate BLEU based on the 20% lowest overlaps and 20% most10http: / / mscococo.org / dataset / 11This is partially an artifact of the way the Microsoft COCO data was created based on the object category, each image was pre-selected from a set of 80."}, {"heading": "6 Conclusion", "text": "We have shown that a gated RNN conditioned directly on CNN activations (an MRNN) achieves better BLEU performance than a ME LM or LSTM conditioned on a series of discrete activations; and a BLEU performance similar to that of a ME LM in combination with a DMSM. However, the ME LM + DMSM method significantly outperforms the MRNN in terms of human quality assessments. We suspect that this is partly due to the lack of novelty in the captions created by MRNN. In fact, a k-next neighbor retrieval algorithm presented in this paper behaves both in terms of automatic metrics and human assessments similar to the MRNN system. If we use the MRNN system alongside the DMSM to achieve additional values in the MERT ranking of the n-best produced by the imageconditioned LME EU, we do not achieve an advance of 1.6 points on the previously published BLCO results."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig."], "venue": "Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 1044\u20131054.", "citeRegEx": "Auli et al\\.,? 2013", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Re-evaluation the role of bleu in machine translation research", "author": ["Chris Callison-Burch", "Miles Osborne", "Philipp Koehn."], "venue": "EACL, volume 6, pages 249\u2013256.", "citeRegEx": "Callison.Burch et al\\.,? 2006", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2006}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["Xinlei Chen", "C. Lawrence Zitnick."], "venue": "Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).", "citeRegEx": "Chen and Zitnick.,? 2015", "shortCiteRegEx": "Chen and Zitnick.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "CoRR.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Meteor universal: language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proc. EACL 2014 Workshop Statistical Machine Translation.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "arXiv:1411.4389 [cs.CV].", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "Proc. Conf. Comput. Vision", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "From captionons to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Doll\u00e1", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig."], "venue": "Proc. Conf. Comput. Vision and Pat-", "citeRegEx": "Fang et al\\.,? 2015", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Every picture tells a story: generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth."], "venue": "Proc. European Conf. Comput. Vision (ECCV),", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Framing image description as a ranking task: data models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "J. Artificial Intell. Research, pages 853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel."], "venue": "Proc. Int. Conf. Machine Learning (ICML).", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Microsoft COCO: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick."], "venue": "arXiv:1405.0312", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L. Yuille."], "venue": "Proc. Int. Conf. Learning Representations (ICLR).", "citeRegEx": "Mao et al\\.,? 2015", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Domainspecific image captioning", "author": ["Rebecca Mason", "Eugene Charniak."], "venue": "CoNLL.", "citeRegEx": "Mason and Charniak.,? 2014", "shortCiteRegEx": "Mason and Charniak.", "year": 2014}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig."], "venue": "SLT, pages 234\u2013239.", "citeRegEx": "Mikolov and Zweig.,? 2012", "shortCiteRegEx": "Mikolov and Zweig.", "year": 2012}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "ACL, ACL \u201903.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Im2Text: Describing images using 1 million captioned photogrphs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg."], "venue": "Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS).", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. Assoc. for Computational Linguistics (ACL), pages 311\u2013 318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei."], "venue": "Interna-", "citeRegEx": "Russakovsky et al\\.,? 2015", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "arXiv preprint.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Show and tell: a neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "The first decomposes the problem into an initial step that uses a convolutional neural network to predict a bag of words that are likely to be present in a caption; then in a second step, a maximum entropy language model (ME LM) is used to generate a sentence that covers a minimum number of the detected words (Fang et al., 2015).", "startOffset": 311, "endOffset": 330}, {"referenceID": 10, "context": "This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015).", "startOffset": 68, "endOffset": 138}, {"referenceID": 13, "context": "This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015).", "startOffset": 68, "endOffset": 138}, {"referenceID": 2, "context": "This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015).", "startOffset": 68, "endOffset": 138}, {"referenceID": 2, "context": ", 2015; Chen and Zitnick, 2015). Similar in spirit is the the log-bilinear (LBL) LM of Kiros et al. (2014).", "startOffset": 8, "endOffset": 107}, {"referenceID": 12, "context": "We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training set.", "startOffset": 136, "endOffset": 154}, {"referenceID": 8, "context": "In fact, a simple k-nearest neighbor approach, which is common in earlier related work (Farhadi et al., 2010; Mason and Charniak, 2014), performs similarly to the MRNN.", "startOffset": 87, "endOffset": 135}, {"referenceID": 14, "context": "In fact, a simple k-nearest neighbor approach, which is common in earlier related work (Farhadi et al., 2010; Mason and Charniak, 2014), performs similarly to the MRNN.", "startOffset": 87, "endOffset": 135}, {"referenceID": 3, "context": "In our case, a gated recurrent neural network (GRNN) is used (Cho et al., 2014), similar to an LSTM.", "startOffset": 61, "endOffset": 79}, {"referenceID": 3, "context": "In our case, a gated recurrent neural network (GRNN) is used (Cho et al., 2014), similar to an LSTM. This is the largest image captioning dataset to date. As described by Fang et al. (2015). ar X iv :1 50 5.", "startOffset": 62, "endOffset": 190}, {"referenceID": 20, "context": "The CNN used is the 16-layer variant of VGGNet (Simonyan and Zisserman, 2014) which was initially trained for the ILSVRC2014 classification task (Russakovsky et al.", "startOffset": 47, "endOffset": 77}, {"referenceID": 19, "context": "The CNN used is the 16-layer variant of VGGNet (Simonyan and Zisserman, 2014) which was initially trained for the ILSVRC2014 classification task (Russakovsky et al., 2015), and then finetuned on the Microsoft COCO data set (Fang et al.", "startOffset": 145, "endOffset": 171}, {"referenceID": 7, "context": ", 2015), and then finetuned on the Microsoft COCO data set (Fang et al., 2015; Lin et al., 2014).", "startOffset": 59, "endOffset": 96}, {"referenceID": 12, "context": ", 2015), and then finetuned on the Microsoft COCO data set (Fang et al., 2015; Lin et al., 2014).", "startOffset": 59, "endOffset": 96}, {"referenceID": 7, "context": "We study the effect of leveraging an explicit detection step to find key objects/attributes in images before generation, examining both an ME LM approach as reported in previous work (Fang et al., 2015), and a novel LSTM approach introduced here.", "startOffset": 183, "endOffset": 202}, {"referenceID": 7, "context": "We refer the reader to Fang et al. (2015) for a full description of their ME LM approach, whose 500-best outputs we analyze here.", "startOffset": 23, "endOffset": 42}, {"referenceID": 15, "context": "We incorporate this information within the LSTM by adding an additional input encoded to represent the remaining visual attributes D \\ {h} as a continuous valued auxiliary feature vector (Mikolov and Zweig, 2012).", "startOffset": 187, "endOffset": 212}, {"referenceID": 8, "context": "Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al.", "startOffset": 69, "endOffset": 97}, {"referenceID": 8, "context": "Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al.", "startOffset": 69, "endOffset": 120}, {"referenceID": 8, "context": "Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al. (2015), and Donahue et al.", "startOffset": 69, "endOffset": 139}, {"referenceID": 5, "context": "(2015), and Donahue et al. (2014). This joint vision-language RNN is referred to as a Multimodal Recurrent Neural Network (MRNN).", "startOffset": 12, "endOffset": 34}, {"referenceID": 3, "context": "The fc7 vector is then fed into a hidden layer H to obtain a 500dimensional representation that serves as the initial hidden state to a gated recurrent neural network (GRNN) (Cho et al., 2014).", "startOffset": 174, "endOffset": 192}, {"referenceID": 5, "context": "Both Donahue et al. (2015) and Karpathy and FeiFei (2015) present a 1-nearest neighbor baseline.", "startOffset": 5, "endOffset": 27}, {"referenceID": 5, "context": "Both Donahue et al. (2015) and Karpathy and FeiFei (2015) present a 1-nearest neighbor baseline.", "startOffset": 5, "endOffset": 58}, {"referenceID": 17, "context": ", (Ordonez et al., 2011; Hodosh et al., 2013).", "startOffset": 2, "endOffset": 45}, {"referenceID": 9, "context": ", (Ordonez et al., 2011; Hodosh et al., 2013).", "startOffset": 2, "endOffset": 45}, {"referenceID": 12, "context": "We work with the Microsoft COCO dataset (Lin et al., 2014), with 82,783 training images, and the validation set split into 20,243 validation images and 20,244 testval images.", "startOffset": 40, "endOffset": 58}, {"referenceID": 7, "context": "\u2020: From (Fang et al., 2015).", "startOffset": 8, "endOffset": 27}, {"referenceID": 18, "context": "The quality of generated captions is measured automatically using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014).", "startOffset": 71, "endOffset": 94}, {"referenceID": 4, "context": ", 2002) and METEOR (Denkowski and Lavie, 2014).", "startOffset": 19, "endOffset": 46}, {"referenceID": 4, "context": "7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens.", "startOffset": 9, "endOffset": 36}, {"referenceID": 0, "context": "The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2013).", "startOffset": 106, "endOffset": 125}, {"referenceID": 7, "context": "\u2020: previously reported and reconfirmed BLEU scores from (Fang et al., 2015).", "startOffset": 56, "endOffset": 75}, {"referenceID": 16, "context": "8 We then re-rank the hypotheses using MERT (Och, 2003).", "startOffset": 44, "endOffset": 55}, {"referenceID": 7, "context": "As in previous work (Fang et al., 2015), model weights were optimized to maximize BLEU score on the validation set.", "startOffset": 20, "endOffset": 39}, {"referenceID": 7, "context": "We further extend this combination approach to the D-ME model with DMSM scores included during re-ranking (Fang et al., 2015).", "startOffset": 106, "endOffset": 125}, {"referenceID": 1, "context": "Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al.", "startOffset": 71, "endOffset": 121}, {"referenceID": 9, "context": "Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al.", "startOffset": 71, "endOffset": 121}, {"referenceID": 1, "context": "Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al. (2015). Here, human judges were presented with an image, a system generated caption, and a human generated caption, and were asked which caption was \u201cbetter\u201d.", "startOffset": 72, "endOffset": 209}, {"referenceID": 21, "context": "All of our models produce a large number of captions seen in the training and repeated for different images in the test set, as shown in Table 6 (also observed by Vinyals et al. (2014) for their LSTM-based model).", "startOffset": 163, "endOffset": 185}], "year": 2015, "abstractText": "Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-ofthe-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.", "creator": "TeX"}}}