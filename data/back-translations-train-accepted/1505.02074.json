{"id": "1505.02074", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2015", "title": "Exploring Models and Data for Image Question Answering", "abstract": "This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use recurrent neural networks and visual semantic embeddings without intermediate stages such as object detection and image segmentation. Our model performs 1.8 times better than the recently published results on the same dataset. Another main contribution is an automatic question generation algorithm that converts the currently available image description dataset into QA form, resulting in a 10 times bigger dataset with more evenly distributed answers.", "histories": [["v1", "Fri, 8 May 2015 15:59:44 GMT  (2151kb,D)", "http://arxiv.org/abs/1505.02074v1", "10 pages, 7 figures, 5 tables. Under review as Deep Learning Workshop paper at ICML 2015"], ["v2", "Fri, 19 Jun 2015 19:55:07 GMT  (4971kb,D)", "http://arxiv.org/abs/1505.02074v2", "11 pages. To Appear in Deep Learning Workshop at ICML 2015"], ["v3", "Thu, 25 Jun 2015 06:44:44 GMT  (4971kb,D)", "http://arxiv.org/abs/1505.02074v3", "11 pages. To Appear in Deep Learning Workshop at ICML 2015"], ["v4", "Sun, 29 Nov 2015 22:45:12 GMT  (3256kb,D)", "http://arxiv.org/abs/1505.02074v4", "12 pages. Conference paper at NIPS 2015"]], "COMMENTS": "10 pages, 7 figures, 5 tables. Under review as Deep Learning Workshop paper at ICML 2015", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.CV", "authors": ["mengye ren", "ryan kiros", "richard s zemel"], "accepted": true, "id": "1505.02074"}, "pdf": {"name": "1505.02074.pdf", "metadata": {"source": "META", "title": "Image Question Answering: A Visual Semantic Embedding Model and a New Dataset", "authors": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "emails": ["MREN@CS.TORONTO.EDU", "RKIROS@CS.TORONTO.EDU", "ZEMEL@CS.TORONTO.EDU"], "sections": [{"heading": "1. Introduction", "text": "Combining image understanding and interaction with natural language is one of the great dreams of artificial intelligence. We are interested in the problem of learning image and text together through a question and answer task. Recently, the generation of captions (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us practical ways in which we can learn image and text together to form higher representations from models such as Convolutionary Neural Networks (CNNs) trained in object recognition and word embedding that are trained on large-format text corpus. Image QA involves an additional layer of human-computer interaction. Here, the inputs are combined with image and text, and the model must give an answer that is geared to the question."}, {"heading": "2. Problem Formulation", "text": "The input of the problem is an image and a question, and the output is an answer. In this thesis, we assume that the answers consist of only one word, which allows us to treat the problem as a classification problem, which also makes the evaluation of the models easier and more robust."}, {"heading": "3. Related Work", "text": "Malinowski & Fritz (2014a) published a dataset of images and question-and-answer pairs, the DAtaset for QUestion Answering on Real World Images (DAQUAR). All images are taken from the NYU Depth v2 dataset (Silberman et al., 2012), and are taken from indoors. There are mainly three types of questions in this dataset: object type, object color, and number of objects. Some questions are simple, but many questions are very difficult to answer, even for humans. Figure 2 shows some examples of simple and hard questions. Since DAQUAR is the only publicly available image-based dataset, it is the dataset, object color, and number of objects."}, {"heading": "4. Proposed Methodology", "text": "The methodology presented here is twofold: On the model side, we applied recurrent neural networks and visual-semantic embedding for this task, and on the dataset side, we proposed new ways to synthesize QA pairs from currently available image description data sets."}, {"heading": "4.1. Models", "text": "In recent years, recurring neural networks (RNNs) have achieved some successes in the field of natural language processing (NLP). Short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) is a form of RNN that is easier to learn than standard RNNNs due to its linear error propagation and multiplicative genres. There is a growing interest in using LSTM as a record-level encoder and decoder. Our model builds directly on the LSTM record model and is called the \"VIS + LSTM\" model. It treats the image as a word of the question. We have adopted this idea of treating the image as a word from the work done by Vinyals et al. (2014) to generate captions as a word generation. The difference to image caption generation is that here we are just responding to the last timestep.1 We have fed the last hidden layer of the Oxford Conv image network (2014 & Zman)."}, {"heading": "4.2. Question-Answer Generation", "text": "The DAQUAR dataset currently available contains approximately 1500 images, with 7000 images divided into 37 common object classes, which may not be enough to train large complex models. Another problem with the current dataset is that merely guessing the mode can lead to very good accuracy. We aim to create another dataset in order to achieve a much larger number of QA pairs and a more even distribution of responses. While collecting human-generated QA pairs is one possible approach, and another is to generate questions based on image descriptions, we instead propose automatically using descriptions in QS forms. As a starting point, we used the Microsoft COCO dataset (Lin et al., 2014), but the same method can be applied to any other image description dataset, such as Flickr (Hodosh et al., 2013), SBU (Ordonez et al., 2011) or even the use of the Internet to create it first."}, {"heading": "4.2.1. COMMON STRATEGIES", "text": "1. Composite sentences into simple sentences Here we consider only one simple case, in which two sentences are connected to a conjunctive word. We divide the original sentences into two independent sentences. For example, \"There is a cat and the cat runs.\" is divided into \"There is a cat\" and \"The cat runs.\" 2. Undetermined determinants become definitive determinants. Questions on a specific case of the subject require that the determinator be brought into definitive form. \"For example, the algorithm must have\" a boy playing baseball. \"in its question form\" that \"instead of\" a \":\" What is the boy playing? \"3. Wh movement restrictions In English, questions tend to begin with question words like\" What. \"The algorithm must move the verb just as much as the\" Wen \"to the front of the sentence. In this thesis, we consider the following two simple limitations: (a) A-over-A principle The b principle of the word limits the word NP-law that is contained in another h-bill."}, {"heading": "4.2.2. PRE-PROCESSING", "text": "We used the Stanford parser (Klein & Manning, 2003) to obtain the syntatic structure of the original image description.4.2.3. Object QUESTIONFirst, we look at the question of an object with \"what.\" To do this, we need to replace the actual object with a \"what\" in the sentence and then transform the sentence structure so that the \"what\" appears before the sentence. The entire algorithm has the following levels: 1. Split long sets into simple satences2. Change indefinite determinants to define determinants. 3. Traverse the sentence and identify potential answers and replace it with \"what.\" While traversing the object-typical question generation, we currently ignore all prepositional phrases (PP) components. 4. Run wh-movementFigure 4 illustrates an example with tree diagrams. To identify a possible answer word, we use WordNet (Fellbaum, 1998) and NLK-Software (Bird, 2006) to get the object in the category."}, {"heading": "4.2.7. POST-PROCESSING", "text": "One of our design requirements for the new dataset is to avoid over-frequent responses. To achieve this goal, we applied heuristics to reject the answers that occur too rarely or too often in our generated dataset. First, answers that appear less than a frequency threshold are discarded. In COCO-QA, the threshold is about 20 in the training set and 10 in the test set. Then, all QA pairs are shuffled to reduce dependence between adjacent pairs. We formulate the rejection process as a Bernoulli random process. The probability of recording the next QA pair (q, a) is: p (q, a) = {1 if count (a) \u2264 Kexp (count (a) \u2212 KKK2) otherwise (1), where counting (a) denotes the current number of inscribed QA pairs that have one as a basic truth answer, and K, select some 2 mode constants with K = K in the KCO-24."}, {"heading": "5. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets", "text": "Table 1 summarizes the difference between DAQUAR and COCO-QA. It should be noted that since the application of the QA pair rejection process, COCO-QA mode rates have performed very poorly, but COCO-QA questions are actually easier to answer from a human point of view, encouraging the model to take advantage of outstanding object relationships rather than finalizing all possible relationships. Table 2 shows a breakdown of all four question types included in the COCO-QA dataset with turn / test split information."}, {"heading": "5.2. Model Details", "text": "We trained two versions of the architectures that we believe work best: the first model is CNN and LSTM with a weight matrix in the middle; we call this \"VIS + LSTM\" in our tables and figures; the second model has two image functions at the beginning and end of the set with different learned linear transformations; and it also has LSTMs that go both forward and backward; both LSTMs output to the Softmax level in the last time step; and the second model is called \"VIS + LSTM-2.\""}, {"heading": "5.3. Baselines", "text": "In order to evaluate the effectiveness of our models, we have designed some baselines."}, {"heading": "5.3.1. GUESS MODEL", "text": "A very simple baseline is the prediction of the mode based on the question type. For example, if the question contains \"how many,\" the model outputs \"two.\" In DAQUAR the modes \"table,\" \"two\" and \"white\" and in COCOQA the modes \"cat,\" \"two,\" \"white\" and \"space.\" This baseline actually works unexpectedly well in the DAQUAR dataset."}, {"heading": "5.3.2. BOW MODEL", "text": "To avoid over-interpretation of the results, we have designed \"blind\" models that are given only the questions without the images. One of the simplest blind models is to combine all the word vectors of the question in a sack-of-word vector (BOW), and then use logistical regression to classify the answers with an optional hidden tanh layer."}, {"heading": "5.3.3. LSTM MODEL", "text": "Another blind model we experimented with simply enters the question words into the LSTM."}, {"heading": "5.3.4. VIS+BOW MODEL", "text": "This is the same as the blind BOW model, but replaces the feature layer with bag-of-word typesetting attributes that are linked to image attributes from the last hidden layer of Oxford Net (4096 dimension) after a linear transformation (to 300 or 500 dimensions)."}, {"heading": "5.3.5. VIS MODEL", "text": "Note that this model knows the type of question in order to make its performance comparable to the models that can take into account the words used to narrow the answer box. However, the model does not know anything about the question except the type."}, {"heading": "5.4. Performance Metrics", "text": "To evaluate the performance of the model, we used both the accuracy of the simple answer and the Wu-Palmer similarity (WUPS, 1994; Malinowski & Fritz, 2014b). WUPS calculates the similarity between two words based on their longest common subsequence in the taxonomy tree. The similarity function takes into account a threshold parameter. If the similarity between two words is less than the threshold, the candidate's response gets a value of zero. It is reduced to the accuracy if the threshold is equal to 1.0. In the following (Malinowski & Fritz, 2014b) we measure all models in terms of accuracy, WUPS 0.9 and WUPS 0.00.Table 3 summarizes the learning results on DAQUAR. Here we compare our results with (Malinowski & Fritz, 2014b)."}, {"heading": "6. Discussion", "text": "It is interesting to note that the blind model on the DAQUAR dataset is actually very strong, comparable to the VIS + LSTM model. We speculate that it is likely that the ImageNet images are very different from the indoor images, which consist largely of furniture. Therefore, the VIS + LSTM model cannot really use the image characteristics unless the question arises about the largest object, i.e. the distinction between sofas, beds and tables. However, the VIS + LSTM model outperforms the blind model by a large margin on the COCO-QA dataset. There are three possible reasons for this: firstly, the objects in MS-COCO are more similar to the objects in ImageNet; secondly, MS-COCO images show fewer objects, while the indoor scenes show considerable confusion; and thirdly, COQA has more data to train complex models."}, {"heading": "6.1. Model Selection", "text": "The best model for DAQUAR uses a randomly initialized embedding of 300 dimensions, while the best model trained for COCO-QA uses a problem-specific embedding of skip programs for initialization. We observed that fine-tuning the word embedding and normalizing CNN's hidden image functions lead to better results, and the bi-directional LSTM model can increase the result a little."}, {"heading": "6.2. Object Questions", "text": "Since the original CNN was trained on the ImageNet challenge, the VIS + LSTM largely benefited from its ability to detect single objects. However, for some simple images in COCO-QA, the VIS + LSTM and VIS + BOW can simply deduce the correct answer from the image characteristics. However, the challenge is to consider spatial relationships between multiple objects and focus on the details of the image. Some qualitative results in Figure 5 show that the VIS + LSTM only does a reasonably acceptable job on them. Sometimes, it does not make the right decision, but gives out the most striking object, while the blind model can guess the most likely objects based on the question alone (e.g., there should be chairs around the dining table)."}, {"heading": "6.3. Counting", "text": "In DAQUAR, we found no advantage in the counting capability of the VIS + LSTM model compared to other blind baselines. In COCO-QA, there are some observable counting capabilities in very clean images with a single object type. The model can sometimes count up to five or six. However, as shown in Figure 6, the capability is quite weak as it does not count correctly when different object types are present. As the VIS + LSTM model beats the blind by only 3% when counting, there is a lot of room for improvement in counting, and in fact, this could be a separate computer vision problem in itself."}, {"heading": "6.4. Color", "text": "In COCO-QA, there is a significant advantage for the VIS + LSTM model over the LSTM model in color type issues. Furthermore, we found that the model is not only able to recognize the dominant color of the image, but sometimes associates different colors with different objects, as in Figure 7. However, the model is not yet particularly robust and fails on a number of simple examples."}, {"heading": "6.5. Limitations and Future Work", "text": "Answering image questions is a relatively new research topic, and the approach we are presenting here has a number of limitations. Firstly, the model is only an answer classifier. Ideally, we would like to allow longer answers that include a complex text generation model or structured output. Secondly, our question generation algorithm also assumes that all answers consist of one word and that the implementation of the algorithm depends heavily on the type of question. Furthermore, the algorithm is currently only applicable to the English language. Finally, it is difficult to interpret why the model delivers a particular answer. By comparing the model with some baseline lines, we can roughly conclude whether the model understood the image, but humans are prone to over-interpretation of the results. Visual attention is another future direction that could both improve the results (based on recent successes in caption (Xu et al., 2015) and help explain the model output by examining the attention output at any point in time."}, {"heading": "7. Conclusion", "text": "In this article, we look at the problem of image quality and present our VIS + LSTM model, which combines CNN and LSTM (s) with visual-semantic embedding. As the currently available dataset is not large enough, we have developed an algorithm that helps us capture large-scale image quality data from image descriptions. Our model shows a reasonable understanding of the question and a rough understanding of the image, but is still very naive in many situations. Our algorithm for generating questions is expandable to many image description datasets and can be automated without much human effort. We hope that the release of the new dataset will encourage more data-driven approaches to this problem in the future."}, {"heading": "Acknowledgments", "text": "We would like to thank Nitish Srivastava for the support of Toronto Conv Net, from which we extracted the CNN image functions, and Zhi Hao Luo for the processing of suggestions."}], "references": [{"title": "NLTK: the natural language toolkit", "author": ["Bird", "Steven"], "venue": "ACL", "citeRegEx": "Bird and Steven.,? \\Q2006\\E", "shortCiteRegEx": "Bird and Steven.", "year": 2006}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["Chen", "Xinlei", "Zitnick", "C. Lawrence"], "venue": "CoRR, abs/1411.5654,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Conditions on Transformations", "author": ["Chomsky", "Noam"], "venue": null, "citeRegEx": "Chomsky and Noam.,? \\Q1973\\E", "shortCiteRegEx": "Chomsky and Noam.", "year": 1973}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue", "Jeff", "Hendricks", "Lisa Anne", "Guadarrama", "Sergio", "Rohrbach", "Marcus", "Venugopalan", "Subhashini", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "ed.). WordNet An Electronic Lexical Database", "author": ["Fellbaum", "Christiane"], "venue": null, "citeRegEx": "Fellbaum and Christiane,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and Christiane", "year": 1998}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["Hodosh", "Micah", "Young", "Peter", "Hockenmaier", "Julia"], "venue": "metrics. J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Karpathy", "Andrej", "Joulin", "Armand", "Fei-Fei", "Li"], "venue": "CoRR, abs/1406.5679,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": "CoRR, abs/1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Dan", "Manning", "Christopher D"], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Phrase-based image captioning", "author": ["Lebret", "R\u00e9mi", "Pinheiro", "Pedro O", "Collobert", "Ronan"], "venue": "CoRR, abs/1502.03671,", "citeRegEx": "Lebret et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2015}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang", "Percy", "Jordan", "Michael I", "Klein", "Dan"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Microsoft COCO: common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C. Lawrence"], "venue": "In Computer Vision - ECCV 2014 - 13th European Conference,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Towards a visual turing challenge", "author": ["Malinowski", "Mateusz", "Fritz", "Mario"], "venue": "CoRR, abs/1410.8027,", "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Yuille", "Alan L"], "venue": "CoRR, abs/1410.1090,", "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Ordonez", "Vicente", "Kulkarni", "Girish", "Berg", "Tamara L"], "venue": "In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Indoor segmentation and support inference from RGBD images", "author": ["Silberman", "Nathan", "Hoiem", "Derek", "Kohli", "Pushmeet", "Fergus", "Rob"], "venue": "In Computer Vision - ECCV 2012 - 12th European Conference on Computer", "citeRegEx": "Silberman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberman et al\\.", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Verb semantics and lexical selection", "author": ["Wu", "Zhibiao", "Palmer", "Martha"], "venue": "Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wu et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Wu et al\\.", "year": 1994}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.", "startOffset": 35, "endOffset": 219}, {"referenceID": 8, "context": "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.", "startOffset": 35, "endOffset": 219}, {"referenceID": 7, "context": "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.", "startOffset": 35, "endOffset": 219}, {"referenceID": 14, "context": "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.", "startOffset": 35, "endOffset": 219}, {"referenceID": 3, "context": "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.", "startOffset": 35, "endOffset": 219}, {"referenceID": 21, "context": "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.", "startOffset": 35, "endOffset": 219}, {"referenceID": 10, "context": "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.", "startOffset": 35, "endOffset": 219}, {"referenceID": 17, "context": "All images are from the NYU depth v2 dataset (Silberman et al., 2012), and are taken from indoor scenes.", "startOffset": 45, "endOffset": 69}, {"referenceID": 11, "context": "In the natural language part of their work, they used a semantic parser (Liang et al., 2013) to convert sentences into latent logical forms.", "startOffset": 72, "endOffset": 92}, {"referenceID": 19, "context": "We borrowed this idea of treating the image as a word from caption generation work done by Vinyals et al. (2014). The difference with caption generation is that here we only output the answer at the last timestep.", "startOffset": 91, "endOffset": 113}, {"referenceID": 15, "context": "We experimented with several different word embedding models: randomly initialized embedding, dataset-specific skip-gram embedding and generalpurpose skip-gram embedding model (Mikolov et al., 2013).", "startOffset": 176, "endOffset": 198}, {"referenceID": 12, "context": "As a starting point we used the Microsoft-COCO dataset (Lin et al., 2014), but the same method can be applied to any other image description dataset, such as Flickr (Hodosh et al.", "startOffset": 55, "endOffset": 73}, {"referenceID": 6, "context": ", 2014), but the same method can be applied to any other image description dataset, such as Flickr (Hodosh et al., 2013), SBU (Ordonez et al.", "startOffset": 99, "endOffset": 120}, {"referenceID": 16, "context": ", 2013), SBU (Ordonez et al., 2011), or even the internet.", "startOffset": 13, "endOffset": 35}, {"referenceID": 21, "context": "Visual attention is another future direction, which could both improve the results (based on recent successes in image captioning (Xu et al., 2015)) as well as help explain the model output by examining the attention output at every timestep.", "startOffset": 130, "endOffset": 147}], "year": 2015, "abstractText": "This work aims to address the problem of imagebased question-answering (QA) with new models and datasets. In our work, we propose to use recurrent neural networks and visual semantic embeddings without intermediate stages such as object detection and image segmentation. Our model performs 1.8 times better than the recently published results on the same dataset. Another main contribution is an automatic question generation algorithm that converts the currently available image description dataset into QA form, resulting in a 10 times bigger dataset with more evenly distributed answers.", "creator": "LaTeX with hyperref package"}}}