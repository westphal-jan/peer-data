{"id": "1505.02294", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2015", "title": "Estimation with Norm Regularization", "abstract": "Analysis of non-asymptotic estimation error and structured statistical recovery based on norm regularized regression, such as Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise model. This paper presents generalizations of such estimation error analysis on all four aspects compared to the existing literature. We characterize the restricted error set where the estimation error vector lies, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to any norm. Precise characterizations of the bound is presented for a variety of design matrices, including sub-Gaussian, anisotropic, and correlated designs, noise models, including both Gaussian and sub-Gaussian noise, and loss functions, including least squares and generalized linear models. A key result from the analysis is that the sample complexity of all such estimators depends on the Gaussian width of the spherical cap corresponding to the restricted error set. Further, once the number of samples $n$ crosses the required sample complexity, the estimation error decreases as $\\frac{c}{\\sqrt{n}}$, where $c$ depends on the Gaussian width of the unit dual norm ball.", "histories": [["v1", "Sat, 9 May 2015 17:25:14 GMT  (1218kb,D)", "https://arxiv.org/abs/1505.02294v1", null], ["v2", "Mon, 18 May 2015 16:24:01 GMT  (643kb,D)", "http://arxiv.org/abs/1505.02294v2", "Fixed minor typos"], ["v3", "Mon, 30 Nov 2015 20:47:14 GMT  (660kb,D)", "http://arxiv.org/abs/1505.02294v3", "Fixed technical issues. Generalized some results"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["arindam banerjee", "sheng chen", "farideh fazayeli", "vidyashankar sivakumar"], "accepted": true, "id": "1505.02294"}, "pdf": {"name": "1505.02294.pdf", "metadata": {"source": "CRF", "title": "Estimation with Norm Regularization", "authors": ["Arindam Banerjee", "Sheng Chen", "Farideh Fazayeli", "Vidyashankar Sivakumar"], "emails": ["banerjee@cs.umn.edu}", "shengc@cs.umn.edu}", "farideh@cs.umn.edu}", "sivakuma@cs.umn.edu}"], "sections": [{"heading": null, "text": "n, where c depends on the Gaussian width of the standard sphere."}, {"heading": "1 Introduction", "text": "In the last decade, progress has been made in the development of non-asymptotic limits based on the estimation of structural parameters based on norm-regulated regression. (1) Such estimates are usually found in the form [39, 29, 9] in which they are a suitable norm, L () is a suitable loss function, Zn = (yi, Xi) ni = 1, where yi-R, Xi, Rp has formed, and it is a regulation parameter. (2) The optimal parameter is often considered \"structured\" or a small value according to any norm R (\u00b7). Recent work has considered such characterizations in relation to atomic norms that relate to the strict relativization of a structured atom."}, {"heading": "2 Restricted Error Set and Recovery Guarantees", "text": "In this section, we will characterize the restricted error fixer in which the error vector \u0445 \u0435\u043d\u0430 = (\u03b8 \u0431\u043d\u0438\u043cn \u2212 \u0432) lies, establish clear relationships between the error sets for regulated and restricted problems, and finally define upper limits for the estimation error. The error limit is deterministic, but has quantities that are \u03b8, X, \u03c9, for which we will develop high probability limits in sections 3, 4, and 6."}, {"heading": "2.1 The Restricted Error Set and the Error Cone", "text": "We start with a characterization of the limited margin of error He who includes any margin of error. Lemma \u03b2 1 For any margin of error (> 1, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adopting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting, Adapting,"}, {"heading": "2.2 Recovery Guarantees", "text": "To establish the restoration guarantees, we first assume that the restricted strong convexity (RSC) is satisfied by the loss function in Er, the error set, so that there is a suitable constant for each of them. (20) In Sections 4 and 6, we define precise forms of the RSC condition for a variety of design matrices and loss functions. (21) Since the constant number of design matrices and loss functions is objective, we focus on the quantitative F conditions.2 To establish the restoration guarantees, we focus on the quantitative F conditions.2 The quantitative condition of the Er condition is clearly F-shaped."}, {"heading": "2.3 A Special Case: Decomposable Norms", "text": "In more recent work, [29] is considered a regularized regression with the special case of decompatible norms (defined in the form of a pair of subspaceM'M's of Rp. The model is assumed in relation to subspaceM, and the definition considers the so-called perturbation subspace M's, which is the orthogonal complement of M. A norm R's is considered decompatible in relation to subspaces (M's, M's) if R's (\"disturbance subspace M's\") = R's (\"disturbance subspace M's\"). We show that in decompatible norms the error defined in our analysis Er is contained in the error cone defined in [29]. In the present context, we leave \u03b2 = 2, the trade conditionality between M's (\"inequality\" M's \") and then for any other inequality (\" inequality M's \") as compatible M's (\" M's)."}, {"heading": "3 Bounds on the Regularization Parameter", "text": "In this section we describe the expectation E [R] [R] [R] [R] [R] [R] [R] [R] [S] [S] [S] [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S [S] S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S"}, {"heading": "4 Least Squares Models: Restricted Eigenvalue Conditions", "text": "The erroneous analysis in Theorem 2 depends on the Limited Strong Convexity (RSC). (RSC) In this section, we will determine the RSC conditions for Sub-Gaussian design matrices if the loss function is squared loss. (RSC) In this section, we will consider the RSC conditions for Sub-Gaussian design matrices (20) to be equivalent to the Restricted eigenvalue (RE) condition [6, 29], since\u03b4L (RD) = 1 n -X (RO) -X (RO) + 1 n sample of the RSC condition (1 n). (XT \u2212 XE) condition for the entire condition (RE), sinceL (RO) = 1 n-Gropherth i = 1 < Xi, Xi, Xi > 2), so that the condition for the condition is simplified (so that Er & Er) condition-Er-1."}, {"heading": "5 Examples and Applications", "text": "In this section we give examples of analysis from previous sections for three standards: L1 standard, group and compatibility constants (14, 29), and more general ways to bind the Gaussian constants have been developed in [25, 16]. L1 standard: Suppose the statistical parameter is \"s-sparse,\" and note that the Gaussian constants and norm compatibility constants are \"s-sparse,\" and note that the Gaussian constants are \"s-sparse.\" \""}, {"heading": "6 Generalized Linear Models: Restricted Strong Convexity", "text": "In this section, we extend our results to estimation with norm regularization in the context of generalized linear models (GLMs) [4, 8]. We assume that the conditional distribution of the response yi to the covariates Xi Xi Xi is an exponential family distribution: p (yi | Xi; \u03b8) = p (yi | \u03b8) = p (yi | Xi, \u03b8 >) = exp {yi < Xi, Xi Xi - division function [8, 4, 44].2 In GLMs, the conditional distribution of the response yi is characterized by an exponential family distribution (yiexp {yi < Xi, \u03b8 Xi - division function [8, 44].2 In GLMs, the conditional distribution of the response yi is characterized by an exponential family distribution."}, {"heading": "7 Conclusions", "text": "The paper presents a general set of results and tools for characterizing non-asymptotic estimation errors in norm-regulated regression problems. It applies to each norm and summarizes much of the existing literature focusing on structured sparseness and related topics. It can be considered a direct generalization of the results in [29], which presents related results for decomposable norms. Our analysis illustrates the important role that Gaussian latitudes play in measuring the size of suitable sets for such results. Furthermore, the error sets for regulated and restricted versions of such problems will be closely related [6]. While the paper represents a unified geometric treatment of non-asymptotically structured estimates with regulated estimators, several technical issues need to be examined further. The focus of the analysis is on thin and limited versions of such problems, and the RE / RSC type analysis really represents two-sided boundaries, i.e., it shows that thin-skinned distributions meet the general phase of satisfying the 27 and lower conditions differently."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Background and Preliminaries", "text": "We will begin with a review of some definitions and known results used in our evidence."}, {"heading": "A.1 Gaussian Width", "text": "In several of our proofs we use the concept of Gaussian latitude [20, 14], which is defined as a two-dimensional unit. (Definition 1 (Gaussian latitude) For each quantity A-Rp, the Gaussian latitude of set A is defined as: w (A) = Eg [sup u-A < g, u >], (68), where the expectation is above g-N (0, Ip \u00b7 p), a vector of the independent zero-mean variance Gaussian random variance. Gaussian latitude w (A) provides a geometric characterization of the magnitude of set A. We consider three perspectives of Gaussian latitude and provide some properties used in our analysis. First, we consider the Gaussian process {To}, where the constituent Gaussian random variable Zu = < t, g > are indexed by u-Rp."}, {"heading": "A.2 Sub-Gaussian and Sub-exponential Random Variables (Vectors)", "text": "\u2212 Definition 2 Sub-Gaussian (sub-exponential) random variable: We say that a random variable x (sub-exponential) is subGaussian (sub-exponential) when the moments [E | x | p] 1 p (sub-exponential) p (p] 1 p \u2264 K1p) (69) for each p \u00b2 1 with a constant K2 (K1). The minimum value of K2 (K1) is called a sub-Gaussian (subexponential) p \u00b2 standard of x (X-x-exponential) p."}, {"heading": "B Restricted Error Set and Recovery Guarantees", "text": "Section 2 is about the limited error set. Lemma 1 characterizes the limited error set. Theorem 1 establishes the relationship between the limited and the restricted error set. In particular, we prove that the Gaussian width of the regulated and limited error sets (cones) is in the same order. Starting from the assumption that the RSC condition is met, Lemma 2 and Theorem 2 derive results at the upper limit of the L2 standard of the error. In this section, we collect the evidence for the different results."}, {"heading": "B.1 The Restricted Error Set", "text": "Lemma 1 in section 2 characterizes the sentence to which the error vector belongs. We give the proof for Lemma 1 below: Lemma 1 For each \u03b2 > 1, assuming that the error vector applies to each \u03b2 > 1. \u2212 Er (Er) = \"Er\" (\"Er,\" \"Zn\") (75), where \"R\" (\"\u00b7) is the dual norm of\" R \"(\u00b7). (76) Proof: Through the optimality of\" Er \"(\" Er, \"\" Zn \") =\" Er \"(\" Rp, \"\" R \"(\" O \") (\" O \") +\" O \"(\" O \"). (76) Proof: Through the optimality of\" Er \"(\" O \"O\") + \"O\" (\"O\") + \"R\" (\"O\") + \"R\" (\"O\") \u2212 R \"(\" O \") (\" O \") \u2212 R\" (\"O\") \u2212 R \"(\") \u2212 (\"O\") \u2212 \"(\" O) \u2212 \"(\" O \") \u2212\" proof of the \"O \u2212\" (O \")."}, {"heading": "B.2 Relation between the Constrained and Regularized Error Cones", "text": "In this section we show that the quantities of regularized and restricted error sets are of the same magnitude (\u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 (\u03b2 \u03b2 \u03b2). We point out that the error set for the restricted setting for atomic standards is a cone that is replaced by: Cc = Cc (\u03b8) = cone (Ec) = cone (\u0445 Rp | R (\u0432) \u2264 R (\u0432)}}. (82) The error set is replaced by: He = Er = Ec (\u03b8, \u03b2) = {\u0445 Rp (Ec) = cone (\u0432) + \u0432 (\u0445 Rp | R (\u0432) \u2264 R (\u0432)). (1) In the following, we provide the proof for Theorem 1.Theorem 1 Let A (\u0432) c = Ec \u00b2 component 2, A (\u0432) r = Er (Rp)."}, {"heading": "B.3 Recovery Guarantees", "text": "Lemma 2 and Theorem 2 in the work are results with which we are satisfied ourselves. The result in Lemma 2 depends on the results in Lemma 2, which are not known. On the other hand, Theorem 2 gives the result in terms of quantities such as size and the compatibility of the norm. (Er) = Supu-Er R (u), which are easier to calculate or bind. (In this section we provide evidence for Lemma 2 and Theorem 2.Lemma 2 assume that the RSC condition in Er is due to the loss L (\u00b7) with the parameter Er R (u), with the parameter L (u). (With any other part of the norm R (\u00b7) we have evidence for the quality of Lemma 2 and Theorem 2.Lemma 2. \u2212 1 We assume that the RSC condition in Er (), (91), where we have the quality of R (\u00b7)."}, {"heading": "C Bounds on the Regularization Parameter", "text": "In this section we prove theorems 3 and 4 in section 3. The regularization parameter should fulfill the condition \u03bbn \u2265 \u03b2R * (\u0432 L (\u03b8 \u043a; Zn)). In theorem 3 we determine the upper limit of the expectation E [R * (\u0432 L (\u03b8; Zn))] in relation to the Gaussian width of the standard sphere for least square loss and Gaussian designs. In theorem 4 we show that R * [R * L (\u03b8; Zn)) is strongly concentrated around its expectation."}, {"heading": "C.1 Proof of Theorem 3", "text": "To prove this, we first need the following theory from the general chain reaction. (...) Theorem 8 Let us ask the question whether we have the unity of R (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...). (...). () (...). (). (...). (). (...). (). (...). (). (). (...). (). (). (...). (). (). (). (...). (). (). (). (...). (). (). (). (...). (). (). (). ().). (). (). ().). (). ().). (). ().). (). ().). (). ().).). ().). (). (). (). ().). ().).). (). ().). (). ().). ().).). ().). ().). (). (). (). ().). (). (). ().).). ().).). (). (). (). ().). (). ().).). (). (). (). ().). ().).). (). ().). ().).).). (). ().). ().). (). ().). (). ().)."}, {"heading": "C.2 Proof of Theorem 4", "text": "To prove Theorem 4, we also need the following result of Gaussian. (...) Theorem 9 (...) Let us (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) () (...) () () () (...) () (...) () () () (...) () () () (... () () (...) () () () () () () () (...) () () () ()... () () () () ()...) () () () () ()...) () () () () ()...) () () ()...) () () ()...) () () ()...) () () () ()...) () ()...) () () ()...) () () () ()...) () () () ()...) () () () () () () () ()...) () () ()...) () () () ()... () () () () () ()... () () ()... () () ()... () () ()... () () ()... () () () () ()... () () () ()... () () () () () () () () () () ()... () () () () () () () () ()... () () () () () () () () () ("}, {"heading": "C.3 Proof of Lemma 3", "text": "Lemma 3 Let us conclude that the Gaussian Latitude is the invariable, the Gaussian Latitude is the invariable, the Gaussian Latitude the invariable, the Gaussian Latitude the invariable, the Gaussian Latitude the invariable, the Gaussian Latitude the invariable, the invariable, the invariant, the invariable, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariable, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the invariant, the, the, the invariant, the, the, the invariant, the, the, the invariant, the, the, the, the, the invariant, the, the, the, the, the, the, the, the, the, the, the, the, the, the invariant, the, the, the, the, the, the, the, the, the, the, the, the, the, the invariant, the, the, the, the, the, the, the, the, the, the, the invariant, the, the, the, the, the, the, the, the, the, the, the"}, {"heading": "D Restricted Eigenvalue Conditions: Sub-Gaussian Designs", "text": "We focus on the results in Section 4. In particular, we consider the RE conditions for sub-Gaussian design matrices for three different cases: (i) the design matrix has independent sub-Gaussian lines Xi with two different Untergaussian elements, (ii) the design matrix has independent lines with sub-Gaussian elements xij, so that (ii) is a special case of (i), but we highlight this particular case because of its practical significance and past literature on RE conditions for anisotropic sub-Gaussian designs [33, 34]. Our results simply use a general treatment developed in [28] and based on Talagand's general concatenation."}, {"heading": "Further, if F is symmetric, then", "text": "[1] We will use the above results and associated arguments to determine the RIP conditions for the cases of interest. [2] We will consider the case in which the design matrix X-Rn \u00b7 p has independent subGaussian rows in which each row meets the RIP conditions. [3] We will consider the case in which the design matrix X-Rn \u00b7 p has independent subGaussian rows in which each row meets the RIP conditions for the cases of interest. [4] We will consider the case in which the design matrix X-Rn \u00b7 p has independent subGaussian rows in which each row meets the RIP conditions."}, {"heading": "D.2 Anisotropic Sub-Gaussian Designs", "text": "I'm not going to comment on this because I don't want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it."}, {"heading": "E Generalized Linear Models: Restricted Strong Convexity", "text": "We set limits on the regularization parameter and the RSC condition for GLMs, as described in Section 6, along with some specific examples."}, {"heading": "E.1 Generalized Linear Models", "text": "Loss functions for GLMs are derived as maximum probability estimates for the exponential distributions family. < The canonical density function of exponential family distributions is given by [4, 8, 44]: Xi (y | \u03b7). The log partition function ensures that P (y | \u03b7) remains a probability distribution. The gradient of the log partition function is the response function, i.e., g (\u00b7). The inversion of the response function is the so-called connecting function h (\u00b7) = g \u2212 1 (\u00b7). The mean value of the distribution can be derived from the gradient of the log partition function at the natural parameters."}, {"heading": "E.2 RSC condition for GLMs", "text": "Any Convex Loss Function to Meet the RSC Condition on Each A Sp \u2212 1 > thereafter shall be subject to the following yardsticks: < < < < \"L\" (\"L\") (\"L\") (\"L\") (< \"L\") (\"L\") (\"L\") (\"L\") (\"L\") (\"X\") (\"X\") (\"L\") (\"X\") (\"X\") (\"X\") (\"X\") (X \") (X\") () (X () (X) (X) () (X) () () (X) () (X) (\"X\") \"(\") \"(X\") (\") (X) (\") (X) (\") (X) (\") (X) (\") (X) (\") (X) (\"(\") \"(X) (\") (X) (\"(X) (\") (X) (\"(\") (X) (X) (\"(\") (X) (X) (\"(\") (X) (\"(\") (X) \"(X) (X) (\" (\") (X) (\" (\")\" (X) (X) (X) (X) (\"(\" (\") (\") \"(X) (X)\" (\"(X)\" (X) \"(X)\" (\"(X)\" (\"(\") \"(\" (X) \"(\") \"(X)\" (\"(\") \"(X)\" (\"(X)\" (X) \"(\" (\")\" (X) \"(\" (X) \")\" (X) \"(X)\" (\"(X)\" (X) (X) (X) (X) () (\"(X) () (X) () (X) () () (X) (X () (X) (X) (X () () () (X () () (X () (X) () () (X) () () (X (X) () ("}], "references": [{"title": "Living on the edge: Phase transitions in convex programs with random data", "author": ["D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp"], "venue": "Inform. Inference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Estimation with norm regularization", "author": ["A. Banerjee", "S. Chen", "F. Fazayeli", "V. Sivakumar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Clustering with Bregman Divergences", "author": ["A Banerjee", "S Merugu", "I Dhillon", "J Ghosh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Information and Exponential Families in Statistical Theory", "author": ["O Barndorff-Nielsen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1978}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Fundamentals of Statistical Exponential Families", "author": ["L Brown"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Statistics for High Dimensional Data: Methods, Theory and Applications", "author": ["P. Buhlmann", "S. van de Geer"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "The restricted isometry property and its implications for compressed sensing", "author": ["E. Candes"], "venue": "Comptes Rendus Mathematique,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["E. Candes", "T Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Candes", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Decoding by linear programming", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "The convex geometry of linear inverse problems", "author": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Generalized dantzig selector: Application to the k-support norm", "author": ["S. Chatterjee", "S. Chen", "A. Banerjee"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Structured estimation with atomic norms: General bounds and applications", "author": ["S. Chen", "A. Banerjee"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Struct. Algorithms,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Sparse estimation with strongly correlated variables using ordered weighted l1 regularization", "author": ["M.A.T. Figueiredo", "R.D. Nowak"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Some inequalities for gaussian processes and applications", "author": ["Y. Gordon"], "venue": "Israel Journal of Mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1985}, {"title": "On milmans inequality and random subspaces which escape through a mesh in rn", "author": ["Y. Gordon"], "venue": "In Geometric Aspects of Functional Analysis,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Empirical processes and random projections", "author": ["B. Klartag", "S. Mendelson"], "venue": "Journal of Functional Analysis,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Bounding the smallest singular value of a random matrix without concentration", "author": ["V. Koltchinskii", "S. Mendelson"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Sparse recovery under weak moment assumptions", "author": ["G. Lecu\u00e9", "S. Mendelson"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "author": ["M. Ledoux", "M. Talagrand"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning", "author": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Lasso-type recovery of sparse representations for high-dimensional data", "author": ["N. Meinshausen", "B. Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Learning Without Concentration", "author": ["S. Mendelson"], "venue": "In Journal of the ACM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Reconstruction and subGaussian operators in asymptotic geometric analysis", "author": ["S. Mendelson", "A. Pajor", "N. Tomczak-Jaegermann"], "venue": "Geometric and Functional Analysis,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "A unified framework for the analysis of regularized M -estimators", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Statistical Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "The lower tail of random quadratic forms, with applications to ordinary least squares and restricted eigenvalue properties", "author": ["R.I. Oliveira"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "The Squared-Error of Generalized Lasso: A Precise Analysis", "author": ["S. Oymak", "C. Thrampoulidis", "B. Hassibi"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach", "author": ["Y. Plan", "R. Vershynin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Restricted Eigenvalue Properties for Correlated Gaussian Designs", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Reconstruction from anisotropic random measurements", "author": ["Z. Rudelson", "S. Zhou"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Principles of Mathematical Analysis", "author": ["Walter Rudin"], "venue": "International Series in Pure & Applied Mathematics. McGraw-Hill,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1976}, {"title": "Beyond sub-gaussian measurements: High-dimensional structured estimation with sub-exponential designs", "author": ["V. Sivakumar", "A. Banerjee", "P. Ravikumar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "The Generic Chaining", "author": ["M. Talagrand"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Upper and Lower Bounds for Stochastic Processes", "author": ["M. Talagrand"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1996}, {"title": "Convex recovery of a structured signal from independent random linear measurements. In Sampling Theory, a Renaissance", "author": ["J.A. Tropp"], "venue": "(To Appear),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Compressed Sensing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Estimation in high dimensions: A geometric perspective", "author": ["R. Vershynin"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Sharp thresholds for noisy and high-dimensional recovery of sparsity using `1constrained quadratic programming(Lasso)", "author": ["M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M J Wainwright", "M I Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2006}, {"title": "Restricted eigenvalue conditions on subgaussian random matrices", "author": ["S. Zhou"], "venue": "Technical report, Department of Mathematics, ETH Zurich, December", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}], "referenceMentions": [{"referenceID": 38, "context": "Such estimators are usually of the form [39, 29, 9]: \u03b8\u0302\u03bbn = argmin \u03b8\u2208Rp L(\u03b8;Z) + \u03bbnR(\u03b8) , (1)", "startOffset": 40, "endOffset": 51}, {"referenceID": 28, "context": "Such estimators are usually of the form [39, 29, 9]: \u03b8\u0302\u03bbn = argmin \u03b8\u2208Rp L(\u03b8;Z) + \u03bbnR(\u03b8) , (1)", "startOffset": 40, "endOffset": 51}, {"referenceID": 8, "context": "Such estimators are usually of the form [39, 29, 9]: \u03b8\u0302\u03bbn = argmin \u03b8\u2208Rp L(\u03b8;Z) + \u03bbnR(\u03b8) , (1)", "startOffset": 40, "endOffset": 51}, {"referenceID": 13, "context": "Recent work has viewed such characterizations in terms of atomic norms, which give the tightest convex relaxation of a structured set of atoms in which \u03b8\u2217 belongs [14].", "startOffset": 163, "endOffset": 167}, {"referenceID": 44, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 12, "endOffset": 24}, {"referenceID": 42, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 12, "endOffset": 24}, {"referenceID": 25, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 12, "endOffset": 24}, {"referenceID": 12, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 134, "endOffset": 142}, {"referenceID": 11, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 134, "endOffset": 142}, {"referenceID": 5, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 185, "endOffset": 196}, {"referenceID": 28, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 185, "endOffset": 196}, {"referenceID": 32, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 185, "endOffset": 196}, {"referenceID": 32, "context": "While much of the development has focussed on isotropic Gaussian design matrices, recent work has extended the analysis for L1 norm to correlated Gaussian designs [33] as well as anisotropic sub-Gaussian design matrices [34].", "startOffset": 163, "endOffset": 167}, {"referenceID": 33, "context": "While much of the development has focussed on isotropic Gaussian design matrices, recent work has extended the analysis for L1 norm to correlated Gaussian designs [33] as well as anisotropic sub-Gaussian design matrices [34].", "startOffset": 220, "endOffset": 224}, {"referenceID": 28, "context": "Building on such development, [29] presents a unified framework for the case of decomposable norms and also considers generalized linear models (GLMs) for certain norms such as L1.", "startOffset": 30, "endOffset": 34}, {"referenceID": 28, "context": "Two key insights are offered in [29]: first, the error vector \u2206\u0302n lies in a restricted set, a cone or a star, for suitably large \u03bbn, and second, the loss function needs to satisfy restricted strong convexity (RSC), a generalization of the RE condition, on the restricted error set for the analysis to work out.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon\u2019s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon\u2019s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.", "startOffset": 157, "endOffset": 169}, {"referenceID": 19, "context": "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon\u2019s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.", "startOffset": 157, "endOffset": 169}, {"referenceID": 23, "context": "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon\u2019s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.", "startOffset": 157, "endOffset": 169}, {"referenceID": 30, "context": "[31] considers three related formulations for generalized Lasso problems, establish recovery guarantees based on Gordon\u2019s inequality, and quantities related to the Gaussian width.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Sharper analysis for recovery has been considered in [1], yielding a precise characterization of phase transition behavior using quantities related to the Gaussian width.", "startOffset": 53, "endOffset": 56}, {"referenceID": 31, "context": "[32] consider a linear programming estimator in a 1-bit compressed sensing setting and, interestingly, the concept of Gaussian width shows up in the analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "In spite of the advances, with a few notable exceptions [40, 42], most existing results are restricted to isotropic Gaussian design matrices.", "startOffset": 56, "endOffset": 64}, {"referenceID": 41, "context": "In spite of the advances, with a few notable exceptions [40, 42], most existing results are restricted to isotropic Gaussian design matrices.", "startOffset": 56, "endOffset": 64}, {"referenceID": 12, "context": "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(\u03b8) under suitable constraints determined by the noise (y \u2212 X\u03b8) and/or the design matrix X [13, 6, 14, 15].", "startOffset": 249, "endOffset": 264}, {"referenceID": 5, "context": "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(\u03b8) under suitable constraints determined by the noise (y \u2212 X\u03b8) and/or the design matrix X [13, 6, 14, 15].", "startOffset": 249, "endOffset": 264}, {"referenceID": 13, "context": "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(\u03b8) under suitable constraints determined by the noise (y \u2212 X\u03b8) and/or the design matrix X [13, 6, 14, 15].", "startOffset": 249, "endOffset": 264}, {"referenceID": 14, "context": "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(\u03b8) under suitable constraints determined by the noise (y \u2212 X\u03b8) and/or the design matrix X [13, 6, 14, 15].", "startOffset": 249, "endOffset": 264}, {"referenceID": 14, "context": "A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15], which generalizes the Dantzig selector [11] corresponding to the L1 norm, and is given by: \u03b8\u0302\u03b3n = argmin \u03b8\u2208Rp R(\u03b8) s.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15], which generalizes the Dantzig selector [11] corresponding to the L1 norm, and is given by: \u03b8\u0302\u03b3n = argmin \u03b8\u2208Rp R(\u03b8) s.", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "One can show [14, 15] that the restricted error set for such constrained estimators are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 13, "endOffset": 21}, {"referenceID": 14, "context": "One can show [14, 15] that the restricted error set for such constrained estimators are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 13, "endOffset": 21}, {"referenceID": 23, "context": "Then, with Ar = Er \u2229 B p 2 , Ac = Ec \u2229 B p 2 , and \u0100c = cone(Ec) \u2229 B p 2 , assuming \u2016\u03b8\u20162 = 1, \u03b2 = 2, we show that w(Ac) \u2264 w(Ar) \u2264 3w(\u0100c) , (5) where w(A) = Eg[supa\u2208A\u3008a, g\u3009], with g \u223c N(0, Ip\u00d7p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].", "startOffset": 271, "endOffset": 286}, {"referenceID": 4, "context": "Then, with Ar = Er \u2229 B p 2 , Ac = Ec \u2229 B p 2 , and \u0100c = cone(Ec) \u2229 B p 2 , assuming \u2016\u03b8\u20162 = 1, \u03b2 = 2, we show that w(Ac) \u2264 w(Ar) \u2264 3w(\u0100c) , (5) where w(A) = Eg[supa\u2208A\u3008a, g\u3009], with g \u223c N(0, Ip\u00d7p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].", "startOffset": 271, "endOffset": 286}, {"referenceID": 13, "context": "Then, with Ar = Er \u2229 B p 2 , Ac = Ec \u2229 B p 2 , and \u0100c = cone(Ec) \u2229 B p 2 , assuming \u2016\u03b8\u20162 = 1, \u03b2 = 2, we show that w(Ac) \u2264 w(Ar) \u2264 3w(\u0100c) , (5) where w(A) = Eg[supa\u2208A\u3008a, g\u3009], with g \u223c N(0, Ip\u00d7p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].", "startOffset": 271, "endOffset": 286}, {"referenceID": 36, "context": "Then, with Ar = Er \u2229 B p 2 , Ac = Ec \u2229 B p 2 , and \u0100c = cone(Ec) \u2229 B p 2 , assuming \u2016\u03b8\u20162 = 1, \u03b2 = 2, we show that w(Ac) \u2264 w(Ar) \u2264 3w(\u0100c) , (5) where w(A) = Eg[supa\u2208A\u3008a, g\u3009], with g \u223c N(0, Ip\u00d7p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].", "startOffset": 271, "endOffset": 286}, {"referenceID": 5, "context": "For the special case of L1 norm, [6] considered a simultaneous analysis of the Lasso and the Dantzig selector, and characterized the structure of the error sets for regularized and constrained sets for the special case of L1 norm.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Further, while the characterization in [6] was also geometric, it was not based on Gaussian widths.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "The second assumption is that the design matrix X \u2208 Rn\u00d7p satisfies the restricted strong convexity (RSC) condition [6, 29] in the error set Er, in particular, there exists a suitable constant \u03ba > 0 so that \u03b4L(\u2206, \u03b8\u2217) , L(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217)\u2212 \u3008\u2207L(\u03b8\u2217),\u2206\u3009 \u2265 \u03ba\u2016\u2206\u20162 \u2200\u2206 \u2208 Er .", "startOffset": 115, "endOffset": 122}, {"referenceID": 28, "context": "The second assumption is that the design matrix X \u2208 Rn\u00d7p satisfies the restricted strong convexity (RSC) condition [6, 29] in the error set Er, in particular, there exists a suitable constant \u03ba > 0 so that \u03b4L(\u2206, \u03b8\u2217) , L(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217)\u2212 \u3008\u2207L(\u03b8\u2217),\u2206\u3009 \u2265 \u03ba\u2016\u2206\u20162 \u2200\u2206 \u2208 Er .", "startOffset": 115, "endOffset": 122}, {"referenceID": 28, "context": "where \u03c8(Er) = supu\u2208Er R(u) \u2016u\u20162 is a norm compatibility constant [29], and c > 0 is a constant.", "startOffset": 65, "endOffset": 69}, {"referenceID": 35, "context": "Recent work in [36] has extended the analyses for sub-exponential distributions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 36, "context": "Interestingly, for sub-Gaussian designs, one obtains the results in terms of the \u2018sub-Gaussian width\u2019 of the unit norm-ball, which can be upper bounded by a constant times the Gaussian width using generic chaining [37].", "startOffset": 214, "endOffset": 218}, {"referenceID": 36, "context": "Further, one can get high-probability versions of these bounds using related advances in generic chaining [37, 38].", "startOffset": 106, "endOffset": 114}, {"referenceID": 37, "context": "Further, one can get high-probability versions of these bounds using related advances in generic chaining [37, 38].", "startOffset": 106, "endOffset": 114}, {"referenceID": 28, "context": "For the special case of L1 regularization, \u03a9R is the unit L1 norm ball, and the corresponding Gaussian width w(\u03a9R) \u2264 c1 \u221a log p, which explains the \u221a log p term one finds in existing bounds for Lasso [29, 9].", "startOffset": 200, "endOffset": 207}, {"referenceID": 8, "context": "For the special case of L1 regularization, \u03a9R is the unit L1 norm ball, and the corresponding Gaussian width w(\u03a9R) \u2264 c1 \u221a log p, which explains the \u221a log p term one finds in existing bounds for Lasso [29, 9].", "startOffset": 200, "endOffset": 207}, {"referenceID": 36, "context": "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].", "startOffset": 74, "endOffset": 82}, {"referenceID": 37, "context": "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].", "startOffset": 74, "endOffset": 82}, {"referenceID": 20, "context": "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].", "startOffset": 127, "endOffset": 135}, {"referenceID": 27, "context": "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].", "startOffset": 127, "endOffset": 135}, {"referenceID": 28, "context": "is determined by the Restricted Strong Convexity (RSC) condition [29].", "startOffset": 65, "endOffset": 69}, {"referenceID": 28, "context": "The result is thus a considerable generalization of earlier results on convex losses, such as GLMs, which had looked at specific norms and associated cones and/or did not express the results in terms of the Gaussian width of A [29].", "startOffset": 227, "endOffset": 231}, {"referenceID": 28, "context": "Further, note that the condition in (15) is similar to that in [29] for \u03b2 = 2, but the above characterization holds for any norm, not just decomposable norms [29].", "startOffset": 63, "endOffset": 67}, {"referenceID": 28, "context": "Further, note that the condition in (15) is similar to that in [29] for \u03b2 = 2, but the above characterization holds for any norm, not just decomposable norms [29].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].", "startOffset": 132, "endOffset": 147}, {"referenceID": 5, "context": "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].", "startOffset": 132, "endOffset": 147}, {"referenceID": 13, "context": "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].", "startOffset": 132, "endOffset": 147}, {"referenceID": 14, "context": "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].", "startOffset": 132, "endOffset": 147}, {"referenceID": 14, "context": "A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15] given by: \u03b8\u0302\u03b3n = argmin \u03b8\u2208Rp R(\u03b8) s.", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 13, "endOffset": 21}, {"referenceID": 14, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 13, "endOffset": 21}, {"referenceID": 13, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 84, "endOffset": 96}, {"referenceID": 14, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 84, "endOffset": 96}, {"referenceID": 39, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 84, "endOffset": 96}, {"referenceID": 5, "context": "Related observations have been made for the special case of the L1 norm [6], although past work did not provide an explicit characterization in terms of Gaussian widths.", "startOffset": 72, "endOffset": 75}, {"referenceID": 28, "context": "In recent work, [29] considered regularized regression with the special case of decomposable norms, defined in terms of a pair of subspacesM \u2286 M\u0304 of Rp.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "We show that for decomposable norms, the error set Er in our analysis is included in the error cone defined in [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "The last inequality is precisely the error cone in [29] for \u03b8\u2217 \u2208 M.", "startOffset": 51, "endOffset": 55}, {"referenceID": 28, "context": "where \u03a8(M\u0304) is the subspace compatibility in M\u0304, as used in [29].", "startOffset": 60, "endOffset": 64}, {"referenceID": 40, "context": "We also recall the definition of the sub-Gaussian norm for a sub-Gaussian variable x, |||x|||\u03c82 = supp\u22651 1 \u221a p(E[|x| p])1/p [41].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "The motivation behind getting an upper bound of the Gaussian width w(\u03a9R) of the unit norm ball in terms of the Gaussian width of such a cone is because considerable advances have been made in recent years in upper bounding Gaussian widths of such cones [14, 1].", "startOffset": 253, "endOffset": 260}, {"referenceID": 0, "context": "The motivation behind getting an upper bound of the Gaussian width w(\u03a9R) of the unit norm ball in terms of the Gaussian width of such a cone is because considerable advances have been made in recent years in upper bounding Gaussian widths of such cones [14, 1].", "startOffset": 253, "endOffset": 260}, {"referenceID": 5, "context": ", L(\u03b8;Zn) = 1 n\u2016y \u2212 X\u03b8\u2016 2, the RSC condition (20) becomes equivalent to the Restricted Eigenvalue (RE) condition [6, 29], since", "startOffset": 113, "endOffset": 120}, {"referenceID": 28, "context": ", L(\u03b8;Zn) = 1 n\u2016y \u2212 X\u03b8\u2016 2, the RSC condition (20) becomes equivalent to the Restricted Eigenvalue (RE) condition [6, 29], since", "startOffset": 113, "endOffset": 120}, {"referenceID": 5, "context": "Analysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34].", "startOffset": 147, "endOffset": 158}, {"referenceID": 32, "context": "Analysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34].", "startOffset": 147, "endOffset": 158}, {"referenceID": 33, "context": "Analysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34].", "startOffset": 147, "endOffset": 158}, {"referenceID": 9, "context": "The RE/RIP conditions for independent isotropic Gaussian designs have been widely studied for the case of L1 norm [10, 6].", "startOffset": 114, "endOffset": 121}, {"referenceID": 5, "context": "The RE/RIP conditions for independent isotropic Gaussian designs have been widely studied for the case of L1 norm [10, 6].", "startOffset": 114, "endOffset": 121}, {"referenceID": 32, "context": "The generalization to RE condition for correlated Gaussian designs for the special of L1 norm was studied in [33].", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "[14] consider the more general context of atomic norms, and RE condition analysis applies to any spherical cap A, with sample complexity results in terms of w(A), the Gaussian width of A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "However, the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 52, "endOffset": 64}, {"referenceID": 19, "context": "However, the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 52, "endOffset": 64}, {"referenceID": 23, "context": "However, the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 52, "endOffset": 64}, {"referenceID": 45, "context": "Progress has been made on establishing RE conditions for sub-Gaussian designs for error sets/caps corresponding to specific norms such as L1 [46].", "startOffset": 141, "endOffset": 145}, {"referenceID": 33, "context": "In recent work, RE conditions were developed for anisotropic sub-Gaussian designs for the L1 norm [34].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "Further, recent work have pointed out the differences between the RE and the RIP condition, which gives a two-sided bound on quadratic forms of random matrices [29].", "startOffset": 160, "endOffset": 164}, {"referenceID": 32, "context": "In fact, all existing RE results do implicitly have the width term, but in a form specific to the chosen norm [33, 34].", "startOffset": 110, "endOffset": 118}, {"referenceID": 33, "context": "In fact, all existing RE results do implicitly have the width term, but in a form specific to the chosen norm [33, 34].", "startOffset": 110, "endOffset": 118}, {"referenceID": 13, "context": "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 113, "endOffset": 125}, {"referenceID": 19, "context": "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 113, "endOffset": 125}, {"referenceID": 23, "context": "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 113, "endOffset": 125}, {"referenceID": 36, "context": "The proof technique we use is an application of generic chaining [37, 38].", "startOffset": 65, "endOffset": 73}, {"referenceID": 37, "context": "The proof technique we use is an application of generic chaining [37, 38].", "startOffset": 65, "endOffset": 73}, {"referenceID": 20, "context": "The specific form we utilize was originally developed in [21, 28].", "startOffset": 57, "endOffset": 65}, {"referenceID": 27, "context": "The specific form we utilize was originally developed in [21, 28].", "startOffset": 57, "endOffset": 65}, {"referenceID": 32, "context": "The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use", "startOffset": 165, "endOffset": 173}, {"referenceID": 28, "context": "The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use", "startOffset": 165, "endOffset": 173}, {"referenceID": 33, "context": "The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use", "startOffset": 214, "endOffset": 218}, {"referenceID": 16, "context": "Further, the RIP results can be viewed as a generalization of the celebrated Johnson-Lindenstrauss (JL) lemma [17], and the interested reader can explore these connections in [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "Further, the RIP results can be viewed as a generalization of the celebrated Johnson-Lindenstrauss (JL) lemma [17], and the interested reader can explore these connections in [21].", "startOffset": 175, "endOffset": 179}, {"referenceID": 9, "context": "More generally, choosing = cw(A)/ \u221a n, one can write the result in a traditional RIP form [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 32, "context": "Finally, it is instructive to compare the above result to existing characterizations of the RE condition for anisotropic Gaussian [33] and anisotropic sub-Gaussian [34] designs, focused on the L1 norm.", "startOffset": 130, "endOffset": 134}, {"referenceID": 33, "context": "Finally, it is instructive to compare the above result to existing characterizations of the RE condition for anisotropic Gaussian [33] and anisotropic sub-Gaussian [34] designs, focused on the L1 norm.", "startOffset": 164, "endOffset": 168}, {"referenceID": 13, "context": "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].", "startOffset": 133, "endOffset": 141}, {"referenceID": 28, "context": "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].", "startOffset": 133, "endOffset": 141}, {"referenceID": 24, "context": "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].", "startOffset": 250, "endOffset": 258}, {"referenceID": 15, "context": "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].", "startOffset": 250, "endOffset": 258}, {"referenceID": 13, "context": "where (a) is obtained from the fact that Gaussian width ofG(\u03b8\u0303) with \u03b8\u0303 be a s-sparse vector is \u221a 2s log(ps ) + 5 4s [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "which is similar to the results obtained in well known results [14, 29].", "startOffset": 63, "endOffset": 71}, {"referenceID": 28, "context": "which is similar to the results obtained in well known results [14, 29].", "startOffset": 63, "endOffset": 71}, {"referenceID": 28, "context": "As shown in [29] Group norm is a decomposable norm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "where m = max t |Gt| and (a) is obtained from the fact that Gaussian width of G(\u03b8\u0303) where \u03b8\u0303 has k active group is \u221a 2k(m+ log(T \u2212 k)) + k [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "which is similar to the results obtained in previous works [14, 29].", "startOffset": 59, "endOffset": 67}, {"referenceID": 28, "context": "which is similar to the results obtained in previous works [14, 29].", "startOffset": 59, "endOffset": 67}, {"referenceID": 3, "context": "In this section, we extend our results to estimation with norm regularization in the context of generalized linear models (GLMs) [4, 8].", "startOffset": 129, "endOffset": 135}, {"referenceID": 7, "context": "In this section, we extend our results to estimation with norm regularization in the context of generalized linear models (GLMs) [4, 8].", "startOffset": 129, "endOffset": 135}, {"referenceID": 7, "context": "is the log-partition function [8, 4, 44].", "startOffset": 30, "endOffset": 40}, {"referenceID": 3, "context": "is the log-partition function [8, 4, 44].", "startOffset": 30, "endOffset": 40}, {"referenceID": 43, "context": "is the log-partition function [8, 4, 44].", "startOffset": 30, "endOffset": 40}, {"referenceID": 3, "context": "the natural parameter \u03b7i = \u3008Xi, \u03b8\u2217\u3009 gives the expectation of the response [4, 8], i.", "startOffset": 74, "endOffset": 80}, {"referenceID": 7, "context": "the natural parameter \u03b7i = \u3008Xi, \u03b8\u2217\u3009 gives the expectation of the response [4, 8], i.", "startOffset": 74, "endOffset": 80}, {"referenceID": 3, "context": "Note that for GLMs over discrete responses yi, the integration needs to be suitably changed to summation [4, 8].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "Note that for GLMs over discrete responses yi, the integration needs to be suitably changed to summation [4, 8].", "startOffset": 105, "endOffset": 111}, {"referenceID": 0, "context": "i=1 \u22072\u03c6(\u3008\u03b8\u2217, Xi\u3009+ \u03b3i\u3008u,Xi\u3009)\u3008u,Xi\u3009 , where \u03b3i \u2208 [0, 1], and where the last equality follows from a direct application of the mean value theorem [35].", "startOffset": 47, "endOffset": 53}, {"referenceID": 34, "context": "i=1 \u22072\u03c6(\u3008\u03b8\u2217, Xi\u3009+ \u03b3i\u3008u,Xi\u3009)\u3008u,Xi\u3009 , where \u03b3i \u2208 [0, 1], and where the last equality follows from a direct application of the mean value theorem [35].", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "Since the log-partition function \u03c6 is of Legendre type [3, 4, 8], the second derivative \u22072\u03c6(\u00b7) is always positive.", "startOffset": 55, "endOffset": 64}, {"referenceID": 3, "context": "Since the log-partition function \u03c6 is of Legendre type [3, 4, 8], the second derivative \u22072\u03c6(\u00b7) is always positive.", "startOffset": 55, "endOffset": 64}, {"referenceID": 7, "context": "Since the log-partition function \u03c6 is of Legendre type [3, 4, 8], the second derivative \u22072\u03c6(\u00b7) is always positive.", "startOffset": 55, "endOffset": 64}, {"referenceID": 41, "context": "Assuming X has isotropic sub-Gaussian rows with |||Xi|||\u03c62 \u2264 \u03ba, \u3008Xi, \u03b8 \u2217\u3009 and \u3008Xi, u\u3009 are sub-Gaussian random variables with sub-Gaussian norm at most C\u03ba [42].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "Note that RSC analysis for GLMs was considered in [29] for specific norms, especially L1, whereas our analysis applies to any set A \u2286 Sp\u22121, hence to any norm, and the result is in terms of the Gaussian width w(A) of A.", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "The work can be viewed as a direct generalization of results in [29], which presented related results for decomposable norms.", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "Further, the error sets for regularized and constrained versions of such problems are shown to be closely related [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 29, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 95, "endOffset": 103}, {"referenceID": 26, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 95, "endOffset": 103}, {"referenceID": 21, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 263, "endOffset": 275}, {"referenceID": 22, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 263, "endOffset": 275}, {"referenceID": 26, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 263, "endOffset": 275}, {"referenceID": 17, "context": "Since real-world several problems, including spatial and temporal problems, do have correlated observations, it will be important to investigate estimators which perform well in such settings [18].", "startOffset": 192, "endOffset": 196}, {"referenceID": 19, "context": "In several of our proofs, we use the concept of Gaussian width [20, 14], which is defined as follows.", "startOffset": 63, "endOffset": 71}, {"referenceID": 13, "context": "In several of our proofs, we use the concept of Gaussian width [20, 14], which is defined as follows.", "startOffset": 63, "endOffset": 71}, {"referenceID": 36, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].", "startOffset": 179, "endOffset": 194}, {"referenceID": 37, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].", "startOffset": 179, "endOffset": 194}, {"referenceID": 6, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].", "startOffset": 179, "endOffset": 194}, {"referenceID": 23, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].", "startOffset": 179, "endOffset": 194}, {"referenceID": 40, "context": "The following definitions and lemmas are from [41].", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "Recall from [14], that the error set for the constrained setting for atomic norms is a cone given by: Cc = Cc(\u03b8 \u2217) = cone(Ec) = cone {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "Also, recall that any norm is convex, since by triangle inequality, for t \u2208 [0, 1], we have R(t\u03b81 + (1\u2212 t)\u03b82) \u2264 R(t\u03b81) +R((1\u2212 t)\u03b82) = tR(\u03b81) + (1\u2212 t)R(\u03b82) .", "startOffset": 76, "endOffset": 82}, {"referenceID": 32, "context": "One can view (ii) as a special case of (i), but we highlight this special case because of its practical importance and past literature on RE conditions for anisotropic subGaussian designs [33, 34].", "startOffset": 188, "endOffset": 196}, {"referenceID": 33, "context": "One can view (ii) as a special case of (i), but we highlight this special case because of its practical importance and past literature on RE conditions for anisotropic subGaussian designs [33, 34].", "startOffset": 188, "endOffset": 196}, {"referenceID": 27, "context": "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand\u2019s generic chaining [37, 38].", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand\u2019s generic chaining [37, 38].", "startOffset": 74, "endOffset": 78}, {"referenceID": 36, "context": "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand\u2019s generic chaining [37, 38].", "startOffset": 118, "endOffset": 126}, {"referenceID": 37, "context": "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand\u2019s generic chaining [37, 38].", "startOffset": 118, "endOffset": 126}, {"referenceID": 27, "context": "We specifically focus on results in [28] which provide uniform bounds on the supremum of certain empirical processes.", "startOffset": 36, "endOffset": 40}, {"referenceID": 27, "context": "The results in [28], and more generally in generic chaining [37, 38], are based on certain \u03b3-functionals which we briefly introduce below.", "startOffset": 15, "endOffset": 19}, {"referenceID": 36, "context": "The results in [28], and more generally in generic chaining [37, 38], are based on certain \u03b3-functionals which we briefly introduce below.", "startOffset": 60, "endOffset": 68}, {"referenceID": 37, "context": "The results in [28], and more generally in generic chaining [37, 38], are based on certain \u03b3-functionals which we briefly introduce below.", "startOffset": 60, "endOffset": 68}, {"referenceID": 27, "context": "Theorem 10 (Mendelson, Pajor, Tomczak-Jaegermann [28]) There exist absolute constants c1, c2, c3 for which the following holds.", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "The canonical density function of exponential family distributions is given by [4, 8, 44]:", "startOffset": 79, "endOffset": 89}, {"referenceID": 7, "context": "The canonical density function of exponential family distributions is given by [4, 8, 44]:", "startOffset": 79, "endOffset": 89}, {"referenceID": 43, "context": "The canonical density function of exponential family distributions is given by [4, 8, 44]:", "startOffset": 79, "endOffset": 89}, {"referenceID": 7, "context": "(144) The interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44].", "startOffset": 125, "endOffset": 135}, {"referenceID": 3, "context": "(144) The interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44].", "startOffset": 125, "endOffset": 135}, {"referenceID": 43, "context": "(144) The interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44].", "startOffset": 125, "endOffset": 135}, {"referenceID": 7, "context": "Considering the negative log-likelihood, the GLM corresponding to the Gaussian distribution yields least squares regression [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "Considering the negative log-likelihood, the GLM corresponding to the Bernoulli distribution yields logistic regression [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "Considering the negative log-likelihood, the GLM corresponding to the Poisson distribution yields Poisson regression [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 0, "context": "for suitable \u03b3i \u2208 [0, 1].", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": "where \u03b3i \u2208 [0, 1].", "startOffset": 11, "endOffset": 17}, {"referenceID": 1, "context": "Acknowledgements: We thank the reviewers of the conference version [2] for helpful comments and suggestions on related work.", "startOffset": 67, "endOffset": 70}], "year": 2015, "abstractText": "Analysis of non-asymptotic estimation error and structured statistical recovery based on norm regularized regression, such as Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise model. This paper presents generalizations of such estimation error analysis on all four aspects compared to the existing literature. We characterize the restricted error set where the estimation error vector lies, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to any norm. Precise characterizations of the bound is presented for isotropic as well as anisotropic subGaussian design matrices, subGaussian noise models, and convex loss functions, including least squares and generalized linear models. Generic chaining and associated results play an important role in the analysis. A key result from the analysis is that the sample complexity of all such estimators depends on the Gaussian width of a spherical cap corresponding to the restricted error set. Further, once the number of samples n crosses the required sample complexity, the estimation error decreases as c \u221a n , where c depends on the Gaussian width of the unit norm ball.", "creator": "LaTeX with hyperref package"}}}