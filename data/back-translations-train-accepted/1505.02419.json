{"id": "1505.02419", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2015", "title": "Improved Relation Extraction with Feature-Rich Compositional Embedding Models", "abstract": "Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings. We propose a Feature-rich Compositional Embedding Model (\\fct{}) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) hand-crafted features with learned word embeddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a log-linear classifier with hand-crafted features gives state-of-the-art results.", "histories": [["v1", "Sun, 10 May 2015 18:47:06 GMT  (2701kb,D)", "https://arxiv.org/abs/1505.02419v1", "10 pages"], ["v2", "Tue, 12 May 2015 01:01:34 GMT  (2701kb,D)", "http://arxiv.org/abs/1505.02419v2", "10 pages"], ["v3", "Tue, 15 Sep 2015 02:01:34 GMT  (2738kb,D)", "http://arxiv.org/abs/1505.02419v3", "12 pages for EMNLP 2015"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["matthew r gormley", "mo yu", "mark dredze"], "accepted": true, "id": "1505.02419"}, "pdf": {"name": "1505.02419.pdf", "metadata": {"source": "CRF", "title": "Improved Relation Extraction with Feature-Rich Compositional Embedding Models", "authors": ["Matthew R. Gormley", "Mo Yu", "Mark Dredze"], "emails": ["gflfof@gmail.com,", "mdredze}@cs.jhu.edu"], "sections": [{"heading": "1 Introduction", "text": "There are two types of terms that have lexical properties of words and blank words; 2010, 2008 and 2010, as well as the interactions between words. Prior to working on relation extraction, there was extensive research on how to design such features by combining individual lexical properties (e.g. whether there is between two entities or on a dependency path between them), while this help makes learning invisible words more difficult. An alternative approach to collecting lexical information relies on persistent words, but generalizably on new words. Embedding features have improved many tasks, including those that parse dependence, semantic role labeling, and relation extraction (Miller et al, 2004; Turian al al al al al al al al al al)."}, {"heading": "2 Relation Extraction", "text": "With respect to extraction, we are given a sentence as input with the aim of identifying, for all pairs of entities mentioned, what relationship exists between them, if any. For each pair of entities is mentioned in a sentence S., we construct an instance (y, x) where x = (M1, M2, S, A). S = {w1, w2,..., wn} is a sentence of length n, which expresses a relationship of type y between two entities mentioned M1 and M2, where M1 and M2 strings of words in S. A are the associated notes of the sentence S, such as part-of-speech tags, a dependency model n, which expresses a relationship between two entities. We consider directed relationships: for a relationship type Rel, y = Rel (M1, M2) and y \u2032 = Rel (M2, M1) are different relationships. Table 1 shows relationships between entities, and has a strong label bias to negative examples."}, {"heading": "3 A Feature-rich Compositional Embedding Model for Relations", "text": "We propose a general framework for constructing the embedding of a sentence with annotations to its individual words. While we focus on the task of relation extraction, the framework applies to any task that benefits from both embedding and typical handmade lexical features."}, {"heading": "3.1 Combining Features with Embeddings", "text": "We begin by describing a precise method of constructing substructure embeddings and commented sentence embeddings from existing (usually immaculate) features and embeddings. Note that these embeddings can be incorporated directly into a log-linear model as features - this leads to in4Since the focus of this essay is on the extraction of relationships, we take over the evaluation scenery of previous work in which gold-named entities are used to facilitate a better comparison. A specific case of our complete model is presented in the next subsection. An additional sentence is first broken down into substructures. The type of substructures may vary according to the task; for the extraction of relationships we consider one substructure per word5. For each substructure in the sentence, we have a handmade feature vector fwi and a dense embedding vector ewi. We represent each substructure as the outer product between these two vectors to create a matrix that is called hwi = wi."}, {"heading": "3.2 The Log-Bilinear Model", "text": "In fact, it is such that it is a very selective and unpredictable world, in which most people are able to put themselves in the world, and in which people put themselves in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "3.3 Discussion of the Model", "text": "Substructure Embedding Similar words (i.e. words with similar embedding) with similar functions in the sentence (i.e. those with similar properties) have similar matrix representations. To understand our selection of the outer product, consider the example in Fig. 1. The word \"drive\" can indicate the GTR relationship when it refers to the 6Other popular log-bilinear models are the log-bilinear language models (Mnih and Hinton, 2007; Mikolov et al., 2013).Dependency path between M1 and M2. Suppose that the third feature in fwi indicates this on-path function. Our model can now learn parameters that give the third row a high weight for the GTR label. Other words with embedding appearing on the dependency path between the mentions will receive a high weight for the GTR label."}, {"heading": "4 Hybrid Model", "text": "We present a hybrid model that combines the FCM with an existing log-linear model. We do this by defining a new model: pFCM + loglin (y | x) = 1Z pFCM (y | x) ploglin (y | x) (3) The log-linear model has the usual form: ploglin (y | x) and exp (\u03b8 \u00b7 f (x, y)), where \u03b8 are the model parameters and f (x, y) is a vector of characteristics. Integration treats each model as providing a value that we multiply with each other. A constant Z ensures a normalized distribution."}, {"heading": "5 Training", "text": "The FCM training optimizes a cross-entropy target: \"(D; T, e) = \u2211 (x, y) \u0445 D logP (y; x; T, e), where D is the set of all training data and e is the set of word embeddings. To optimize the target, we perform stochastic training for each instance (y, x) via the loss function\" = \"(y, x; T, e) = logP (y; x; T, e). The gradients of the model parameters are determined by backpropagation (i.e. repeated application of the chain rule). We define the vector s = [zi i Ty (fwi ewi)] 1 \u2264 y \u2264 L, from which the\" X-word \"[(I [y \u2032 = y] \u2212 P (y \u2032; T, e) 1 \u2264 y model results."}, {"heading": "6 Experimental Settings", "text": "Features Our FCM characteristics (Table 2) use a feature vector fwi over the word wi, the two target units M1, M2, and their dependence path. Here h1, h2 are the indexes of the two header words of M1, M2, \u00d7 refers to the Cartesian product between two sentences, th1 and th2 are entity types (named entity tags for ACE 2005 or WordNet supertags for SemEval 2010) of the header words of two units, and \u03c6 stands for the empty attribute."}, {"heading": "6.1 Datasets and Evaluation", "text": "ACE 2005 We evaluate our relation extraction system on the English part of the ACE 2005 corpus (Walker et al., 2006).8 There are 6 domains: Newswire (nw), Broadcast Conversation (bc), Broadcast News (bn), Telephone Speech (cts), Usenet Newsgroups (un), and Weblogs (wl).After previous work, we focus on the domain adaptation settings where we train on one sentence (the union of news domains (bn + nw), match hyperparameters on a dev domain (half of bc), and evaluate on the rest (cts, wl, and the rest of bc) (Plank and Moschitti, 2013; Nguyen and Grishman, 2014).We assume that gold entity spans and types are available for turn and test. We use all pairs of entity eras to achieve 43,518 total relationships in the training set."}, {"heading": "7 Results", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "7.1 Effects of the embedding sub-models", "text": "Next, we will examine the impact of different types of characteristics on FCM using ablation tests on ACE 2005 (Table 5.) We will focus solely on FCM with the characteristics templates in Table 2. In addition, we will show results from using only the header embedding characteristics of Nguyen and Grishman (2014) (HeadOnly). Unsurprisingly, the HeadOnly model performs poorly (F1 value = 14.30%), demonstrating the importance of our extensive binary feature set. Of all the feature templates, the removal of HeadEmb results in the greatest deterioration. The second most important feature template is in-between, while context characteristics have little impact. Removing all the characteristics of the entity type (thi) performs significantly worse than the full model, showing the value of our entity type characteristics."}, {"heading": "7.2 Effects of the word embeddings", "text": "In this section we show the results of FCM with embedding used to initialize other current state-of-the-art models. These embedding include the 300-d embedding based on the English Wikipedia (w2venwiki-d300) and the 100-d task-specific embedding (task-d100) 10 from the RelEmb paper (Hashimoto et al., 2015), the 400-d embedding from the CR CNN paper (dos Santos et al., 2015). In addition, we list the best result (DepNN) in Liu et al. (2015), which uses the same embedding as ours. Table 6 shows the impact of word embedding on FCM and provides relative comparisons between FCM and the other state-of-the-art models. We use the same hyperparameters and the number of iterations in Table 4.The results show that different embedding on FCM can achieve better results than FCM."}, {"heading": "8 Related Work", "text": "However, there are other approaches that could take into account the order of the words (as in \"Convolutional Neural Networks\"). (But there are also other approaches that are oriented by the order of the words.) There are also other approaches that are oriented by the order of the terms. (However, there are also other approaches that are oriented by the order of the terms.) There are also other approaches that are oriented by the order of the terms. (But there are also other approaches that are oriented by the order of the terms. (There are also other approaches that are oriented by the order of the terms.) There are also other approaches that are oriented by the order of the terms. \"(However, there are also other approaches that are oriented by the order of the order of the terms, oriented by the order of the concepts."}, {"heading": "9 Conclusion", "text": "We have introduced FCM, a new compositional model for deriving sentence layers and substructure embeddings from Word embeddings. Compared to existing compositional models, FCM can easily process arbitrary types of input and handle global information for composition while remaining easy to implement. We have shown that FCM alone performs almost state-of-the-art in several relation extraction tasks, and achieves cutting-edge results in combination with traditional, function-based loglinear models. Our next steps to improve FCM focus on enhancements to task-specific embeddings or loss functions such as in Hashimoto et al. (2015; dos Santos et al. (2015). Moreover, as the model provides a general idea for displaying both sentences and substructures in the language, it has the potential to contribute useful components to various tasks, such as dependency sparing, SRL, and multiplying. Also, as kindly mentioned by one year of an anonymous TBP, our review can be applied)."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their comments and Nicholas Andrews, Francis Ferraro and Benjamin Van Durme for their contributions. We thank Kazuma Hashimoto, C\u0131 \u0301 cero Nogueira dos Santos, Bing Xiang and Bowen Zhou for their word embedding and many helpful discussions. Mo Yu is supported by the China Scholarship Council and NSFC 61173073."}], "references": [{"title": "A semantic matching", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["Massimiliano Ciaramita", "Yasemin Altun."], "venue": "EMNLP2006, pages 594\u2013602, July.", "citeRegEx": "Ciaramita and Altun.,? 2006", "shortCiteRegEx": "Ciaramita and Altun.", "year": 2006}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "JMLR, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Cicero dos Santos", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Santos et al\\.,? 2015", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Task-oriented learning of word embeddings for semantic relation classification", "author": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka."], "venue": "arXiv preprint arXiv:1503.00095.", "citeRegEx": "Hashimoto et al\\.,? 2015", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2015}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Association for Computational Linguistics, pages 894\u2013904.", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Semantic frame identification with distributed word representations", "author": ["Karl Moritz Hermann", "Dipanjan Das", "Jason Weston", "Kuzman Ganchev."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1:", "citeRegEx": "Hermann et al\\.,? 2014", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Overview of the tac 2010 knowledge base population track", "author": ["Heng Ji", "Ralph Grishman", "Hoa Trang Dang", "Kira Griffitt", "Joe Ellis."], "venue": "Third Text Analysis Conference (TAC 2010).", "citeRegEx": "Ji et al\\.,? 2010", "shortCiteRegEx": "Ji et al\\.", "year": 2010}, {"title": "Simple semi-supervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of ACL-08: HLT, pages 595\u2013603, Columbus, Ohio, June. Association for Computational Linguistics.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Incremental joint extraction of entity mentions and relations", "author": ["Qi Li", "Heng Ji."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 402\u2013412, Baltimore, Maryland, June.", "citeRegEx": "Li and Ji.,? 2014", "shortCiteRegEx": "Li and Ji.", "year": 2014}, {"title": "A dependency-based neural network for relation classification", "author": ["Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng WANG."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bowen Zhou", "Bing Xiang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1310.4546.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Name tagging with word clusters and discriminative training", "author": ["Scott Miller", "Jethran Guinness", "Alex Zamanian."], "venue": "Susan Dumais, Daniel Marcu, and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings. Association for Compu-", "citeRegEx": "Miller et al\\.,? 2004", "shortCiteRegEx": "Miller et al\\.", "year": 2004}, {"title": "Ace 2004 multilingual training corpus", "author": ["Alexis Mitchell", "Stephanie Strassel", "Shudong Huang", "Ramez Zakhary."], "venue": "Linguistic Data Consortium, Philadelphia.", "citeRegEx": "Mitchell et al\\.,? 2005", "shortCiteRegEx": "Mitchell et al\\.", "year": 2005}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proceedings of the 24th international conference on Machine learning, pages 641\u2013648. ACM.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Employing word representations and regularization for domain adaptation of relation extraction", "author": ["Thien Huu Nguyen", "Ralph Grishman."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short", "citeRegEx": "Nguyen and Grishman.,? 2014", "shortCiteRegEx": "Nguyen and Grishman.", "year": 2014}, {"title": "Relation extraction: Perspective from convolutional neural networks", "author": ["Thien Huu Nguyen", "Ralph Grishman."], "venue": "Proceedings of NAACL Workshop on Vector Space Modeling for NLP.", "citeRegEx": "Nguyen and Grishman.,? 2015", "shortCiteRegEx": "Nguyen and Grishman.", "year": 2015}, {"title": "English gigaword fifth edition, june", "author": ["Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda."], "venue": "Linguistic Data Consortium, LDC2011T07.", "citeRegEx": "Parker et al\\.,? 2011", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction", "author": ["Barbara Plank", "Alessandro Moschitti."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Plank and Moschitti.,? 2013", "shortCiteRegEx": "Plank and Moschitti.", "year": 2013}, {"title": "Utd: Classifying semantic relations by combining lexical and semantic resources", "author": ["Bryan Rink", "Sanda Harabagiu."], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 256\u2013259, Uppsala, Sweden, July. Association for", "citeRegEx": "Rink and Harabagiu.,? 2010", "shortCiteRegEx": "Rink and Harabagiu.", "year": 2010}, {"title": "Composition of word representations improves semantic role labelling", "author": ["Michael Roth", "Kristian Woodsend."], "venue": "EMNLP.", "citeRegEx": "Roth and Woodsend.,? 2014", "shortCiteRegEx": "Roth and Woodsend.", "year": 2014}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "In Proceedings of the ACL conference. Citeseer.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Empirical Methods in Natural Language", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Semi-supervised relation extraction with large-scale word clustering", "author": ["Ang Sun", "Ralph Grishman", "Satoshi Sekine."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages", "citeRegEx": "Sun et al\\.,? 2011", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning."], "venue": "Proceedings of the 2012 Joint Conference on Empirical", "citeRegEx": "Surdeanu et al\\.,? 2012", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Association for Computational Linguistics, pages 384\u2013394.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "ACE 2005 multilingual training corpus", "author": ["Christopher Walker", "Stephanie Strassel", "Julie Medero", "Kazuaki Maeda."], "venue": "Linguistic Data Consortium, Philadelphia.", "citeRegEx": "Walker et al\\.,? 2006", "shortCiteRegEx": "Walker et al\\.", "year": 2006}, {"title": "Learning composition models for phrase embeddings", "author": ["Mo Yu", "Mark Dredze."], "venue": "Transactions of the Association for Computational Linguistics, 3:227\u2013 242.", "citeRegEx": "Yu and Dredze.,? 2015", "shortCiteRegEx": "Yu and Dredze.", "year": 2015}, {"title": "Combining word embeddings and feature embeddings for fine-grained relation extraction", "author": ["Mo Yu", "Matthew R. Gormley", "Mark Dredze."], "venue": "Proceedings of NAACL.", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Zeng et al\\.,? 2014", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Exploring various knowledge in relation extraction", "author": ["GuoDong Zhou", "Jian Su", "Jie Zhang", "Min Zhang."], "venue": "Association for Computational Linguistics, pages 427\u2013434.", "citeRegEx": "Zhou et al\\.,? 2005", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "Linear+Emb\u201d is the implementation of our method", "author": ["Plank", "Moschitti"], "venue": "(Nguyen et al.,", "citeRegEx": "Plank and Moschitti,? \\Q2013\\E", "shortCiteRegEx": "Plank and Moschitti", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014).", "startOffset": 138, "endOffset": 295}, {"referenceID": 29, "context": "Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014).", "startOffset": 138, "endOffset": 295}, {"referenceID": 9, "context": "Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014).", "startOffset": 138, "endOffset": 295}, {"referenceID": 23, "context": "Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014).", "startOffset": 138, "endOffset": 295}, {"referenceID": 27, "context": "Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014).", "startOffset": 138, "endOffset": 295}, {"referenceID": 21, "context": "Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014).", "startOffset": 138, "endOffset": 295}, {"referenceID": 18, "context": "Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014).", "startOffset": 138, "endOffset": 295}, {"referenceID": 29, "context": "Various feature sets used in prior work (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014) are cap-", "startOffset": 40, "endOffset": 135}, {"referenceID": 18, "context": "Various feature sets used in prior work (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014) are cap-", "startOffset": 40, "endOffset": 135}, {"referenceID": 7, "context": "Various feature sets used in prior work (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014) are cap-", "startOffset": 40, "endOffset": 135}, {"referenceID": 23, "context": "Various feature sets used in prior work (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014) are cap-", "startOffset": 40, "endOffset": 135}, {"referenceID": 30, "context": "The result is a state-of-the-art relation extractor for unseen domains from ACE 2005 (Walker et al., 2006) and the relation classification dataset from SemEval-2010 Task 8 (Hendrickx et al.", "startOffset": 85, "endOffset": 106}, {"referenceID": 5, "context": ", 2006) and the relation classification dataset from SemEval-2010 Task 8 (Hendrickx et al., 2010).", "startOffset": 73, "endOffset": 97}, {"referenceID": 4, "context": "Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos", "startOffset": 80, "endOffset": 104}, {"referenceID": 4, "context": "Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos", "startOffset": 80, "endOffset": 132}, {"referenceID": 32, "context": "Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015), which focuses on fine-grained relation extraction for ACE and ERE.", "startOffset": 87, "endOffset": 104}, {"referenceID": 31, "context": "(2015), Yu and Dredze (2015) and Yu et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 31, "context": "(2015), Yu and Dredze (2015) and Yu et al. (2015). Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 24, "context": "SemEval 2010 Task 8) has focused on short sentences with at most one relation per sentence (Socher et al., 2012; Zeng et al., 2014).", "startOffset": 91, "endOffset": 131}, {"referenceID": 33, "context": "SemEval 2010 Task 8) has focused on short sentences with at most one relation per sentence (Socher et al., 2012; Zeng et al., 2014).", "startOffset": 91, "endOffset": 131}, {"referenceID": 21, "context": "For relation extraction, where negative examples abound, prior work has assumed that only the named entity boundaries and not their types were available (Plank and Moschitti, 2013; Nguyen et al., 2015).", "startOffset": 153, "endOffset": 201}, {"referenceID": 18, "context": "sumed that the order of two entities in a relation are given while the relation type itself is unknown (Nguyen and Grishman, 2014; Nguyen and Grishman, 2015).", "startOffset": 103, "endOffset": 157}, {"referenceID": 19, "context": "sumed that the order of two entities in a relation are given while the relation type itself is unknown (Nguyen and Grishman, 2014; Nguyen and Grishman, 2015).", "startOffset": 103, "endOffset": 157}, {"referenceID": 30, "context": "The standard relation extraction task, as adopted by ACE 2005 (Walker et al., 2006), uses long sentences containing multiple named entities with known types4 and unknown relation directions.", "startOffset": 62, "endOffset": 83}, {"referenceID": 18, "context": "The closest work is that of Nguyen and Grishman (2014), who use a loglinear model for relation extraction with embeddings as features for only the entity heads.", "startOffset": 28, "endOffset": 55}, {"referenceID": 29, "context": "In fact, we find that the feature sets used in prior work for many other NLP tasks are special cases of this simple construction (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014).", "startOffset": 129, "endOffset": 224}, {"referenceID": 18, "context": "In fact, we find that the feature sets used in prior work for many other NLP tasks are special cases of this simple construction (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014).", "startOffset": 129, "endOffset": 224}, {"referenceID": 7, "context": "In fact, we find that the feature sets used in prior work for many other NLP tasks are special cases of this simple construction (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014).", "startOffset": 129, "endOffset": 224}, {"referenceID": 23, "context": "In fact, we find that the feature sets used in prior work for many other NLP tasks are special cases of this simple construction (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014).", "startOffset": 129, "endOffset": 224}], "year": 2015, "abstractText": "Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings. We propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) handcrafted features with learned word embeddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a loglinear classifier with hand-crafted features gives state-of-the-art results. We made our implementation available for general use1.", "creator": "TeX"}}}