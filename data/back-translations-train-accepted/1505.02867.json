{"id": "1505.02867", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2015", "title": "The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning", "abstract": "We describe a new instance-based learning algorithm called the Boundary Forest (BF) algorithm, that can be used for supervised and unsupervised learning. The algorithm builds a forest of trees whose nodes store previously seen examples. It can be shown data points one at a time and updates itself incrementally, hence it is naturally online. Few instance-based algorithms have this property while being simultaneously fast, which the BF is. This is crucial for applications where one needs to respond to input data in real time. The number of children of each node is not set beforehand but obtained from the training procedure, which makes the algorithm very flexible with regards to what data manifolds it can learn. We test its generalization performance and speed on a range of benchmark datasets and detail in which settings it outperforms the state of the art. Empirically we find that training time scales as O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N the amount of data,", "histories": [["v1", "Tue, 12 May 2015 03:45:11 GMT  (6519kb,D)", "http://arxiv.org/abs/1505.02867v1", "7 pages, 4 figs, 1 page supp. info"]], "COMMENTS": "7 pages, 4 figs, 1 page supp. info", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IR stat.ML", "authors": ["charles mathy", "nate derbinsky", "jos\u00e9 bento", "jonathan rosenthal", "jonathan s yedidia"], "accepted": true, "id": "1505.02867"}, "pdf": {"name": "1505.02867.pdf", "metadata": {"source": "META", "title": "The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning", "authors": ["Charles Mathy", "Nate Derbinsky", "Jos\u00e9 Bento", "Jonathan Rosenthal", "Jonathan Yedidia"], "emails": ["@disneyresearch.com", "@wit.edu", "@bc.edu", "@disneyresearch.com", "@disneyresearch.com"], "sections": [{"heading": "Introduction", "text": "The ability to learn from a large number of examples, where the examples themselves are often high-dimensional, is critical in many areas of machine learning. It is clear that the ability to generalize from training examples to test queries is a key feature that every learning algorithm must have, but there are several other features that are also critical in many practical situations. Specifically, we are looking for a learning algorithm that is: (i) quick to train, (ii) quick to query, (iii) able to deal with arbitrary data allocations, and (iv) able to learn incrementally in an online environment. Algorithms that meet all of these characteristics, especially (iv), are hard to come by, but they are directly relevant to problems such as real-time computer vision, robot control, and general issues that involve learning from and responding quickly to streaming data."}, {"heading": "Related work", "text": "There are several existing methods, including KD Trees (Friedman, Bentley and Finkel 1977), Geometric Nearneighbor Access Trees (Brin 1995), and Nearest Vector Trees (Lejsek, Jo \u0301 nsson, and Amsaleg 2011), which build tree species on large datasets (see Samet 2006). These algorithms usually need to access the entire dataset before they build their trees, exceeding the BF, but we are interested in an online environment that allows an online listing."}, {"heading": "The Boundary Forest algorithm", "text": "A boundary forest is a collection of nT rooted trees. Each tree consists of nodes that always do not fulfill this function, with edges between the nodes created during the training, as described below. The root node of each tree is the starting point for all queries that use this tree. Each tree is an example of the next query in the test period, so you can trivially parallel training and querying. Each example has a D-class real position x and a \"label\" vector c (x) associated with it (for retrieval problems, you can think of c (x) as equal to x, as we will explain below). For example, if you deal with a 10-dimensional classification, we could use a 10-dimensional indicator vector c (x) with each point x.You have to specify a metric associated with the positions x, the two data points x and y, and outputs a real number d (x) y."}, {"heading": "Scaling properties", "text": "In order to examine the scaling properties of the BF algorithm, we must now focus on its use for the entire line of application. Let's look at examples we have drawn in D dimensions within a hypercube. The qualitative results we are discussing are general: We have tested a mixture of Gaussians of arbitrary size and orientation, and real datasets like MNIST are treated as a retrieval problem, and we will consider the results for a uniformly sampled hypercube and unlabeled MNIST that we interpret raw pixel intensity values as vectors for MNIST without any pre-processing, and in this paper the Euclidean metric is used for all datasets. We will present scaling adjustments to the various rows that exclude one scaling law from another. In these cases, our method to take the first half of the data points was to fit them separately to a scaling law and to try to exclude the one."}, {"heading": "Numerical results", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "35.47 13.8 380 404 11.5 51.4 625", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "16.20 5.2 433 485 65.7 172.5 1.32", "text": "points, which slows down testing. Note that our main point was the comparison with algorithms that perform metric comparisons, but even these comparisons are informative."}, {"heading": "Conclusion and future work", "text": "We have described and investigated a novel online learning algorithm with empirical nlog (N) training and log (N) scaling with data set N and performance similar to k \u2212 NN. The speed of this algorithm makes it suitable for applications such as real-time machine learning and metric learning (Weinberger, Blitzer and Saul 2006). Interesting future possibilities would be: combining the BF with random projections, analyzing acceleration and performance effects; testing a real-time tracking scenario in which the raw pixels may first be guided through a feature extractor."}, {"heading": "Additional results for CT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Additional results for k \u2212NN and RF", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Instancebased learning algorithms", "author": ["D.W. Aha", "D. Kibler", "M.K. Albert"], "venue": "Machine Learning 6:37\u201366.", "citeRegEx": "Aha et al\\.,? 1991", "shortCiteRegEx": "Aha et al\\.", "year": 1991}, {"title": "Cover tree for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "Proceedings of the 2006 23rd International Conference on Machine Learning.", "citeRegEx": "Beygelzimer et al\\.,? 2006", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2006}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning 45:5\u201332.", "citeRegEx": "Breiman,? 2001", "shortCiteRegEx": "Breiman", "year": 2001}, {"title": "Near neighbor search in large metric spaces", "author": ["S. Brin"], "venue": "Proceedings of the 21st International Conference on Very Large Data Bases, 574\u2013584.", "citeRegEx": "Brin,? 1995", "shortCiteRegEx": "Brin", "year": 1995}, {"title": "LIBSVM Data: Classification, Regression, and Multi-label", "author": ["Chang", "C.-C.", "Lin", "C.-J."], "venue": "http://www.csie.ntu.edu.tw/ \u0303cjlin/ libsvmtools/datasets/. [Online; accessed 4June-2014].", "citeRegEx": "Chang et al\\.,? 2008", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "Cover-Tree", "author": ["D.N. Crane"], "venue": "http://www.github. com/DNCrane/Cover-Tree. [Online; accessed 1-July2014].", "citeRegEx": "Crane,? 2011", "shortCiteRegEx": "Crane", "year": 2011}, {"title": "An algorithm for finding best matches in logarithmic expected time", "author": ["J. Friedman", "J. Bentley", "R. Finkel"], "venue": "ACM Trans. on Mathematical Software 3:209\u2013226.", "citeRegEx": "Friedman et al\\.,? 1977", "shortCiteRegEx": "Friedman et al\\.", "year": 1977}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter 11(1):10\u2013", "citeRegEx": "Hall et al\\.,? 2009", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Extensions of lipschitz maps into a hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Contemp. Math. 26:189\u2013 206.", "citeRegEx": "Johnson and Lindenstrauss,? 1984", "shortCiteRegEx": "Johnson and Lindenstrauss", "year": 1984}, {"title": "Online learning of robust object detectors during unstable tracking", "author": ["Z. Kalal", "J. Matas", "K. Mikolajczyk"], "venue": "IEEE Transactions on Online Learning for Computer Vision 1417 \u2013 1424.", "citeRegEx": "Kalal et al\\.,? 2009", "shortCiteRegEx": "Kalal et al\\.", "year": 2009}, {"title": "NV-tree: nearest neighbors at the billion scale", "author": ["H. Lejsek", "B.P. J\u00f3nsson", "L. Amsaleg"], "venue": "Proceedings of the 1st ACM International Conference on Multimedia.", "citeRegEx": "Lejsek et al\\.,? 2011", "shortCiteRegEx": "Lejsek et al\\.", "year": 2011}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP (1), 331\u2013340.", "citeRegEx": "Muja and Lowe,? 2009", "shortCiteRegEx": "Muja and Lowe", "year": 2009}, {"title": "Five balltree construction algorithms", "author": ["S.M. Omohundro"], "venue": "International Computer Science Institute Berkeley.", "citeRegEx": "Omohundro,? 1989", "shortCiteRegEx": "Omohundro", "year": 1989}, {"title": "Foundations of Multidimensional and Metric Data Structures", "author": ["H. Samet"], "venue": "Morgan Kaufman.", "citeRegEx": "Samet,? 2006", "shortCiteRegEx": "Samet", "year": 2006}, {"title": "A two-dimensional interpolation function for irregularly-spaced data", "author": ["D. Shepard"], "venue": "Proceedings of the 1968 23rd ACM national conference, 517\u2013524. ACM.", "citeRegEx": "Shepard,? 1968", "shortCiteRegEx": "Shepard", "year": 1968}, {"title": "Optimised kd-trees for fast image descriptor matching", "author": ["C. Silpa-Anan", "R. Hartley"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 1\u20138. IEEE.", "citeRegEx": "Silpa.Anan and Hartley,? 2008", "shortCiteRegEx": "Silpa.Anan and Hartley", "year": 2008}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "J. Blitzer", "L. Saul"], "venue": "Advances in neural information processing systems 18:1473.", "citeRegEx": "Weinberger et al\\.,? 2006", "shortCiteRegEx": "Weinberger et al\\.", "year": 2006}, {"title": "Reduction techniques for instance-based learning algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Machine Learning 38:257\u2013286.", "citeRegEx": "Wilson and Martinez,? 2000", "shortCiteRegEx": "Wilson and Martinez", "year": 2000}, {"title": "i.e. the number of features of the dataset rounded to the closest integer (the value recommended by (Breiman", "author": [], "venue": null, "citeRegEx": "\u221a,? \\Q2001\\E", "shortCiteRegEx": "\u221a", "year": 2001}], "referenceMentions": [{"referenceID": 3, "context": "There are several existing methods, including KD-trees (Friedman, Bentley, and Finkel 1977), Geometric Nearneighbor Access Trees (Brin 1995), and Nearest Vector trees (Lejsek, J\u00f3nsson, and Amsaleg 2011) that build tree search structures on large datasets (see (Samet 2006) for an extensive bibliography).", "startOffset": 129, "endOffset": 140}, {"referenceID": 13, "context": "There are several existing methods, including KD-trees (Friedman, Bentley, and Finkel 1977), Geometric Nearneighbor Access Trees (Brin 1995), and Nearest Vector trees (Lejsek, J\u00f3nsson, and Amsaleg 2011) that build tree search structures on large datasets (see (Samet 2006) for an extensive bibliography).", "startOffset": 260, "endOffset": 272}, {"referenceID": 12, "context": "The ball tree online insertion algorithm (Omohundro 1989) is rather costly, requiring a volume minimizing step at each addition.", "startOffset": 41, "endOffset": 57}, {"referenceID": 8, "context": "can immediately be combined with random projections to obtain speedup, as it is known by the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss 1984) that the number of projections needed to maintain metric distances only grows as log(D) as the data dimensionality D grows.", "startOffset": 121, "endOffset": 153}, {"referenceID": 11, "context": "In fact, kd trees typically scale no better than brute force in higher than 20 dimensions (Muja and Lowe 2009) , but multiple random kd trees have been shown to overcome this difficulty.", "startOffset": 90, "endOffset": 110}, {"referenceID": 17, "context": "In particular, for classification the Condensed Nearest Neighbor algorithm (Wilson and Martinez 2000) only adds a point if the previously seen points misclassify it.", "startOffset": 75, "endOffset": 101}, {"referenceID": 14, "context": "Many options exist, but in this paper we use a Shepard weighted average (Shepard 1968), where the weights are the inverse distances, so that the estimate is", "startOffset": 72, "endOffset": 86}, {"referenceID": 11, "context": "We compare to the highly optimized FLANN (Muja and Lowe 2009) implementation of multiple random kd trees (Rkd).", "startOffset": 41, "endOffset": 61}, {"referenceID": 7, "context": "For 1\u2212NN , 3\u2212NN andRF we use the Weka(Hall et al. 2009) implementation.", "startOffset": 37, "endOffset": 55}, {"referenceID": 2, "context": "trees and \u221a D features per node (recommended in (Breiman 2001)), andCT with b = 1.", "startOffset": 48, "endOffset": 62}], "year": 2015, "abstractText": "We describe a new instance-based learning algorithm called the Boundary Forest (BF) algorithm, that can be used for supervised and unsupervised learning. The algorithm builds a forest of trees whose nodes store previously seen examples. It can be shown data points one at a time and updates itself incrementally, hence it is naturally online. Few instance-based algorithms have this property while being simultaneously fast, which the BF is. This is crucial for applications where one needs to respond to input data in real time. The number of children of each node is not set beforehand but obtained from the training procedure, which makes the algorithm very flexible with regards to what data manifolds it can learn. We test its generalization performance and speed on a range of benchmark datasets and detail in which settings it outperforms the state of the art. Empirically we find that training time scales as O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N the amount of data.", "creator": "TeX"}}}