{"id": "1505.03410", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2015", "title": "Mind the duality gap: safer rules for the Lasso", "abstract": "Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the so-called $\\textit{safe rules}$ for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.", "histories": [["v1", "Wed, 13 May 2015 14:50:34 GMT  (263kb,D)", "http://arxiv.org/abs/1505.03410v1", "to appear in ICML 2015"], ["v2", "Tue, 25 Aug 2015 14:52:12 GMT  (263kb,D)", "http://arxiv.org/abs/1505.03410v2", "to appear in ICML 2015"], ["v3", "Thu, 3 Dec 2015 21:12:34 GMT  (263kb,D)", "http://arxiv.org/abs/1505.03410v3", "erratum to ICML 2015, \"The authors would like to thanks Jalal Fadili and Jingwei Liang for helping clarifying some misleading statements on the equicorrelation set\""]], "COMMENTS": "to appear in ICML 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC stat.CO", "authors": ["olivier fercoq", "alexandre gramfort", "joseph salmon"], "accepted": true, "id": "1505.03410"}, "pdf": {"name": "1505.03410.pdf", "metadata": {"source": "META", "title": "Mind the duality gap: safer rules for the Lasso", "authors": ["Olivier Fercoq", "Alexandre Gramfort", "Joseph Salmon"], "emails": ["OLIVIER.FERCOQ@TELECOM-PARISTECH.FR", "ALEXANDRE.GRAMFORT@TELECOM-PARISTECH.FR", "JOSEPH.SALMON@TELECOM-PARISTECH.FR"], "sections": [{"heading": "1. Introduction", "text": "Since the mid-1990s, high-dimensional statistics have attracted considerable attention, especially in the context of linear regression with more explanatory variables than observations: the so-called phalanx n case. In such a context, the fewest squares are with '1 consecutive strategies, referred to as the Lasso Method (Tibshirani, 1996) in statistics, or as a basic solution (Chen et al., 1998) in signal processing, one of the most popular tools. It enjoys theoretical guarantees (Bickel et al., 2009), as well as practical advantages: it offers economical solutions and fast convex solutions are available, which has made the Lasso Method a popular method in modern data science with toolkits. Among the successful fields in which it has been applied are procedures of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W & CP Volume 37. Copyright 2015."}, {"heading": "1.1. Model and notation", "text": "Our observation vector is y P Rn and the design matrix X \"rx1,\" \"xps P Rnq\" has column-by-column explanatory variables (or characteristics).Our goal is to approach y as a linear combination of a few variables xj, \"i.e. to express y as X\u03b2, where \u03b2 P Rp is a sparse vector. Euclideans is written as\" 1, \"1 as the\" 1 norm, \"8 as the\" 8 norm, \"and the matrix tranposition of a matrix Q is called QJ. We call ptq\" maxp0, tq.For such a task, the lasso is often regarded as \"1 norm\" (see Bu \"hlmann\" and van de Geer \"(2011) as the matrix tranposition of a matrix Q as QJ."}, {"heading": "1.2. A KKT detour", "text": "For the lasso problem, a primary solution \u03b2-p\u03bbq P Rp and the dual solution \u03b8-p\u03bbq P Rn are linked by the relation: y \"X\u03b2-p\u03bbq\" \u03bb\u043d-p\u03bbq. (3) The Karush-Khun-Tucker (KKT) conditions are: @ j P rps, xJj \u03b8-p\u03bbq P # tsignp\u03b2-p\u03bbqj qu if \u03b2-p\u03bbq j \u0445 0, r '1, 1s if \u03b2-p\u00e7qj \"0. (4) See for example (Xiang et al., 2014). The KKT conditions mean that all the cases considered are a primary solution. It can be considered the mother of all safe screening rules."}, {"heading": "2. Safe rules", "text": "Safe rules make use of the KKT condition (4). This equation implies that \u03b2-p\u03bbqj \"0 as soon as | xJj \u00da p\u03bbq | \u0103 1. The main challenge is that the double optimal solution is unknown. Therefore, a safe rule aims to form a group C-Rn containing \u03b8-p\u03bbq. We refer to such a group C as a safe region. Safe regions are all the more helpful because for many J-Rs, \u00b5Cpxjq, applies:\" sup\u03b8PC | xJj-1, hence for many J-Rn, \u03b2-p\u03bbqj \"0.Practical advantages arise when one can construct a region C for which it is easy to calculate its support function, which is denoted by \u03c3C and defined for each x-P-Rn with: \u03c3Cpxq\" max successPCxJTA. \"(5) Differently structured, for each safe region C, each j-Rps, and each optimal solution-Rm\u03b2-q-PP applies:"}, {"heading": "If \u00b5Cpxjq \u201c maxp\u03c3Cpxjq, \u03c3Cp\u00b4xjqq \u0103 1 then \u03b2\u0302p\u03bbqj \u201c 0.", "text": "(6) We call a safe test or a safe rule, a test associated with C and sorting out explanatory variables thanks to Eq. (6) Note 1. Mention that the support function of a set is the same as the support function of its closed convex fuselage (Hiriart-Urruty & Lemare \u0301 chal, 1993) [Note V.2.2.1], we limit our search to closed convex safe regions. On the basis of a safe region C, the explanatory variables can be divided into a safe active set A\u03bbpCq and a safe zero set Z\u03bbpCq, where: Ap\u03bbqpCq \"tj P rps: \u00b5Cpxjq \u011b 1u, (7) Zp\u043aqpCq\" tj P rps: \"tj P rps:\" \u00b5Cpcitiq \"and a safe zero set Z\u03bbpCq.\" (8) Note that for nested safe regions \"C1\" and \"PICC\" are safe."}, {"heading": "2.1. Sphere tests", "text": "After previous work on safe rules, we call ball tests, tests based on bullets as safe regions. For a ball test, a ball with center c and radius r, i.e. C \"Bpc, rq, is chosen. Due to its simplicity, safe bullets are the most frequently examined safe regions (see table 1 for a brief overview). The corresponding test is defined as follows: If \u00b5Bpc, rqpxjq\" | xJj c |'r} xj \"1, then \u03b2\" p\u03bbq j \"0.\" (9) Note that for a fixed center, the smaller the radius, the better the safe shielding strategy. Example 1. The first ball test introduced (El Ghaoui et al., 2012) consists of the center c \"y\" and the radius r \"| 1\" max. \""}, {"heading": "2.2. Dome tests", "text": "Other popular safe regions are domes, the intersection of a ball and half a space, which is a type of safe region that has been considered, for example (El Ghaoui et al., 2012; Xiang & Ramadge, 2012; Xiang et al., 2014; Bonnefoy et al., 2014b). We refer to DPC, r, \u03b1, wq as the dome with spherical center c, spherical radius r, oriented hyperplane with normal vector w and parameter \u03b1, so that c'\u03b1rw is the projection of c on the hyperplane (see Figure 1 for an illustration in the interesting case \u0430 0). Note 3. The dome is not trivial whenever \u03b1 P '1, 1s. If \u03b1 \"0, you simply get a hemisphere.For the dome test, you have to calculate the support function for C\" DPC, r, \u03b1, wq., as for spheres, it can be obtained in closed form."}, {"heading": "2.3. Dynamic safe rules", "text": "To approach a solution to the lasso primary problem, iterative q-q algorithms are generally used. We refer to \u03b2k P-Rp as the current estimate after k-iterations of each iterative algorithm (see Section 4 for a specific study of coordinate parentage). Dynamic safe rules aim to find safe regions that become narrower when k-canola canola canola rape (see also (Bonnefoy et al., 2014a)). This can be achieved by a simple transformation of the current residues."}, {"heading": "3. New contributions on safe rules", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Support discovery in finite time", "text": "First, let us consider the concepts of convergence of safe regions and convergence of safe tests.Definition 1. Let pCkqkPN be a sequence of closed convex sets in Rn, the convergence of safe regions for the lasso with parameters \u03bb when the diameters of the sets converge to zero. The associated safe screening rules are called convergent safe tests.Not only is the convergence of safe regions crucial for accelerating the calculation, but they are also helpful for achieving accurate active set identification in a finite number of steps. Specifically, we prove that the equicorrelation theorem of the lasso (cf. Remark 2) can be restored in finite time with each convergence strategy."}, {"heading": "3.2. GAP SAFE regions: leveraging the duality gap", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.3. GAP SAFE rules : sequential for free", "text": "As a by-product, our dynamic screening tests provide a warm-start strategy for safe regions, which makes our CAP \u03b2 \u03b2 q q q rules inherently sequential; the next proposal shows their efficiency if they attack a new tuning parameter after they have solved the lasso for a previous solution, even remotely. Handling approximate solutions is a critical problem for producing safe sequential strategies: without taking into account the approximation error, screening could ignore relevant variables, especially those close to the borders of safe regions; in practice, it is unrealistic to assume that exact solutions can be planned."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Coordinate Descent", "text": "We chose the coordination method because it is well suited for machine learning tasks, especially for sparse and / or unstructured design matrix X. Coordinate descent requires efficiently extracting columns of X, which is typically not easy in signal processing applications where X is usually an implicit operator (e.g. Fourier or Wavelets).Algorithm 1 Coordinate Descent with GAP SAFE rules Input X, y, K, f, p\u03bbtqtPrT '1sInitialization: 0 \"max \u03b20\" for t P \"1s do \u03b2 \u03b2\" solution for k P rKs doif k mod f \"1 then Compute and C thanks to (11) and (19) Get A\u03bbtpCq\" tj P rps: \"We are Jxp.\""}, {"heading": "4.2. Number of screened variables", "text": "Figure 3 shows the proportion of variables screened by several safe rules of the standard leukemia dataset. Screening percentage is displayed as a function of the number of iterations K. Since the SAFE screening rule of El Ghaoui et al. (2012) is sequential but not dynamic, the proportion of screened variables for a particular \u03bb does not depend on K. The rules of Bonnefoy et al. (2014a) are more efficient on this dataset, but they do not particularly benefit from the dynamic framework. Our proposed GAP-SAFE tests screen out many more variables, especially when the tuning parameter \u03bb becomes small, which is particularly relevant in practice. Furthermore, the figure shows that the GAP-SAFE dome test brings only a marginal improvement over the sphere."}, {"heading": "4.3. Gains in the computation of Lasso paths", "text": "In fact, the time required to calculate the screening itself should not be greater than the gains it provides. Therefore, we compared the time required to calculate lasso paths with prescribed accuracy for different safe rules. Figures 4, 5 and 6 illustrate the results on three sets of data. Figure 4 presents results on the dense, small scale, leukemia data set. Figure 5 presents results on a medium-sized sparse data set extracted with words from the 20Newsgroup screening data set (comp.graphics vs. talk.religion.misc using TF-IDF, which removes English stopwords and words that occur only once or more than 95% of the time). Extraction of text properties was performed with Scikit-Learn. Figure 6 focuses on the large sparse RCV1 (Reuters Corpus Volume 1) data set, cf."}, {"heading": "5. Conclusion", "text": "We have presented new findings on safe rules for accelerating algorithms to solve the lasso problem (see Appendix on extending the Elastic Network). Firstly, we have introduced the framework for the convergence of safe rules, a key concept regardless of the chosen implementation; secondly, we have used duality gap calculations to create two safer rules that meet the above-mentioned convergence characteristics; and finally, we have demonstrated the important practical benefits of these new rules by applying them to standard dense and sparse datasets using a coordinate descendant solver."}, {"heading": "Acknowledgment", "text": "We appreciate the support of the Chair of Machine Learning for Big Data at Te'le'com ParisTech and the Orange / Te'le'com ParisTech think tank phi-TAB. This work benefited from the support of the \"Gaspard Monge FMJH Programme in Optimisation and Operations Research\" and EDF's support for this programme."}, {"heading": "A. Supplementary materials", "text": "In this appendix, we have provided some further details about the theoretical results in the main part. \"We should look at the case in which safe region C is the dome formula.\" \"The calculation of the dome test formula proceeds as follows:\" # cJxj, \"\" # cJxj, \"\" # cJxj, \"\" # cJxj, \"\" \"# cJxj,\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\""}], "references": [{"title": "Bolasso: model consistent Lasso estimation through the bootstrap", "author": ["F. Bach"], "venue": "In ICML,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": "Ann. Statist.,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "A dynamic screening principle for the lasso", "author": ["A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval"], "venue": "In EUSIPCO,", "citeRegEx": "Bonnefoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bonnefoy et al\\.", "year": 2014}, {"title": "Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso", "author": ["A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval"], "venue": "ArXiv e-prints,", "citeRegEx": "Bonnefoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bonnefoy et al\\.", "year": 2014}, {"title": "Statistics for highdimensional data", "author": ["P. B\u00fchlmann", "S. van de Geer"], "venue": null, "citeRegEx": "B\u00fchlmann and Geer,? \\Q2011\\E", "shortCiteRegEx": "B\u00fchlmann and Geer", "year": 2011}, {"title": "Enhancing sparsity by reweighted l1 minimization", "author": ["E.J. Cand\u00e8s", "M.B. Wakin", "S.P. Boyd"], "venue": "J. Fourier Anal. Applicat.,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2008}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["A. Chambolle", "T. Pock"], "venue": "J. Math. Imaging Vis.,", "citeRegEx": "Chambolle and Pock,? \\Q2011\\E", "shortCiteRegEx": "Chambolle and Pock", "year": 2011}, {"title": "Atomic decomposition by basis pursuit", "author": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Chen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1998}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I.M. Johnstone", "R. Tibshirani"], "venue": "Ann. Statist.,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Safe feature elimination in sparse supervised learning", "author": ["L. El Ghaoui", "V. Viallon", "T. Rabbani"], "venue": "J. Pacific Optim.,", "citeRegEx": "Ghaoui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ghaoui et al\\.", "year": 2012}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Fan and Li,? \\Q2001\\E", "shortCiteRegEx": "Fan and Li", "year": 2001}, {"title": "Sure independence screening for ultrahigh dimensional feature space", "author": ["J. Fan", "J. Lv"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Fan and Lv,? \\Q2008\\E", "shortCiteRegEx": "Fan and Lv", "year": 2008}, {"title": "Pathwise coordinate optimization", "author": ["J. Friedman", "T. Hastie", "H. H\u00f6fling", "R. Tibshirani"], "venue": "Ann. Appl. Stat.,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "Mixednorm estimates for the M/EEG inverse problem using accelerated gradient methods", "author": ["A. Gramfort", "M. Kowalski", "M. H\u00e4m\u00e4l\u00e4inen"], "venue": "Physics in Medicine and Biology,", "citeRegEx": "Gramfort et al\\.,? \\Q1937\\E", "shortCiteRegEx": "Gramfort et al\\.", "year": 1937}, {"title": "TIGRESS: Trustful Inference of Gene REgulation using Stability Selection", "author": ["Haury", "A.-C", "F. Mordelet", "P. Vera-Licona", "J.P. Vert"], "venue": "BMC systems biology,", "citeRegEx": "Haury et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Haury et al\\.", "year": 2012}, {"title": "Convex analysis and minimization algorithms. I, volume 305", "author": ["Hiriart-Urruty", "J.-B", "C. Lemar\u00e9chal"], "venue": "SpringerVerlag, Berlin,", "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 1993}, {"title": "An interior-point method for large-scale l1regularized least squares", "author": ["Kim", "S.-J", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky"], "venue": "IEEE J. Sel. Topics Signal Process.,", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "Local linear convergence of forward\u2013backward under partial smoothness", "author": ["J. Liang", "J. Fadili", "G. Peyr\u00e9"], "venue": "In NIPS, pp. 1970\u20131978,", "citeRegEx": "Liang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2014}, {"title": "Sparse MRI: The application of compressed sensing for rapid MR imaging", "author": ["M. Lustig", "D.L. Donoho", "J.M. Pauly"], "venue": "Magnetic Resonance in Medicine,", "citeRegEx": "Lustig et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lustig et al\\.", "year": 2007}, {"title": "Sparse coding for machine learning, image processing and computer vision", "author": ["J. Mairal"], "venue": "PhD thesis, E\u0301cole normale supe\u0301rieure de Cachan,", "citeRegEx": "Mairal,? \\Q2010\\E", "shortCiteRegEx": "Mairal", "year": 2010}, {"title": "Complexity analysis of the lasso regularization path", "author": ["J. Mairal", "B. Yu"], "venue": "In ICML,", "citeRegEx": "Mairal and Yu,? \\Q2012\\E", "shortCiteRegEx": "Mairal and Yu", "year": 2012}, {"title": "A new approach to variable selection in least squares problems", "author": ["M.R. Osborne", "B. Presnell", "B.A. Turlach"], "venue": "IMA J. Numer. Anal.,", "citeRegEx": "Osborne et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2000}, {"title": "Variational analysis, volume 317 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences", "author": ["R.T. Rockafellar", "Wets", "R.J.-B"], "venue": null, "citeRegEx": "Rockafellar et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Rockafellar et al\\.", "year": 1998}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M. Schmidt", "N. Le Roux", "F. Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R.J. Tibshirani"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Tibshirani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2012}, {"title": "The lasso problem and uniqueness", "author": ["R.J. Tibshirani"], "venue": "Electron. J. Stat.,", "citeRegEx": "Tibshirani,? \\Q2013\\E", "shortCiteRegEx": "Tibshirani", "year": 2013}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl.,", "citeRegEx": "Tseng,? \\Q2001\\E", "shortCiteRegEx": "Tseng", "year": 2001}, {"title": "Smallsample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering", "author": ["G. Varoquaux", "A. Gramfort", "B. Thirion"], "venue": "In ICML,", "citeRegEx": "Varoquaux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Varoquaux et al\\.", "year": 2012}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"], "venue": "In NIPS,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Fast lasso screening tests based on correlations", "author": ["Z.J. Xiang", "P.J. Ramadge"], "venue": "In ICASSP, pp", "citeRegEx": "Xiang and Ramadge,? \\Q2012\\E", "shortCiteRegEx": "Xiang and Ramadge", "year": 2012}, {"title": "Learning sparse representations of high dimensional data on large scale dictionaries", "author": ["Z.J. Xiang", "H. Xu", "P.J. Ramadge"], "venue": "In NIPS,", "citeRegEx": "Xiang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2011}, {"title": "Screening tests for lasso problems", "author": ["Z.J. Xiang", "Y. Wang", "P.J. Ramadge"], "venue": "arXiv preprint arXiv:1405.4897,", "citeRegEx": "Xiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2014}, {"title": "Three structural results on the lasso problem", "author": ["P. Xu", "P.J. Ramadge"], "venue": "In ICASSP, pp", "citeRegEx": "Xu and Ramadge,? \\Q2013\\E", "shortCiteRegEx": "Xu and Ramadge", "year": 2013}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["Zhang", "C.-H"], "venue": "Ann. Statist.,", "citeRegEx": "Zhang and C..H.,? \\Q2010\\E", "shortCiteRegEx": "Zhang and C..H.", "year": 2010}, {"title": "A general theory of concave regularization for high-dimensional sparse estimation problems", "author": ["Zhang", "C.-H", "T. Zhang"], "venue": "Statistical Science,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "The adaptive lasso and its oracle properties", "author": ["H. Zou"], "venue": "J. Am. Statist. Assoc.,", "citeRegEx": "Zou,? \\Q2006\\E", "shortCiteRegEx": "Zou", "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Zou and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie", "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "In such a context, the least squares with `1 regularization, referred to as the Lasso (Tibshirani, 1996) in statistics, or Basis Pursuit (Chen et al.", "startOffset": 86, "endOffset": 104}, {"referenceID": 8, "context": "In such a context, the least squares with `1 regularization, referred to as the Lasso (Tibshirani, 1996) in statistics, or Basis Pursuit (Chen et al., 1998) in signal processing, has been one of the most popular tools.", "startOffset": 137, "endOffset": 156}, {"referenceID": 2, "context": "It enjoys theoretical guarantees (Bickel et al., 2009), as well as practical benefits: it provides sparse solutions and fast convex solvers are available.", "startOffset": 33, "endOffset": 54}, {"referenceID": 20, "context": "one can mention dictionary learning (Mairal, 2010), biostatistics (Haury et al.", "startOffset": 36, "endOffset": 50}, {"referenceID": 15, "context": "one can mention dictionary learning (Mairal, 2010), biostatistics (Haury et al., 2012) and medical imaging (Lustig et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 19, "context": ", 2012) and medical imaging (Lustig et al., 2007; Gramfort et al., 2012) to name a few.", "startOffset": 28, "endOffset": 72}, {"referenceID": 0, "context": "For stability selection methods (Meinshausen & B\u00fchlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved.", "startOffset": 32, "endOffset": 98}, {"referenceID": 29, "context": "For stability selection methods (Meinshausen & B\u00fchlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved.", "startOffset": 32, "endOffset": 98}, {"referenceID": 37, "context": "For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Cand\u00e8s et al., 2008).", "startOffset": 133, "endOffset": 186}, {"referenceID": 6, "context": "For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Cand\u00e8s et al., 2008).", "startOffset": 133, "endOffset": 186}, {"referenceID": 22, "context": "Among possible algorithmic candidates for solving the Lasso, one can mention homotopy methods (Osborne et al., 2000), LARS (Efron et al.", "startOffset": 94, "endOffset": 116}, {"referenceID": 9, "context": ", 2000), LARS (Efron et al., 2004), and approximate homotopy (Mairal & Yu, 2012), that provide solutions for the full Lasso path, i.", "startOffset": 14, "endOffset": 34}, {"referenceID": 13, "context": "More recently, particularly for p \u0105 n, coordinate descent approaches (Friedman et al., 2007) have proved to be among the best methods to tackle large scale problems.", "startOffset": 69, "endOffset": 92}, {"referenceID": 33, "context": "We refer to (Xiang et al., 2014) for a concise introduction on safe rules.", "startOffset": 12, "endOffset": 32}, {"referenceID": 26, "context": "This is for instance the strategy adopted for the strong rules (Tibshirani et al., 2012).", "startOffset": 63, "endOffset": 88}, {"referenceID": 0, "context": "For stability selection methods (Meinshausen & B\u00fchlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved. For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Cand\u00e8s et al., 2008). Among possible algorithmic candidates for solving the Lasso, one can mention homotopy methods (Osborne et al., 2000), LARS (Efron et al., 2004), and approximate homotopy (Mairal & Yu, 2012), that provide solutions for the full Lasso path, i.e., for all possible choices of tuning parameter \u03bb. More recently, particularly for p \u0105 n, coordinate descent approaches (Friedman et al., 2007) have proved to be among the best methods to tackle large scale problems. Following the seminal work by El Ghaoui et al. (2012), screening techniques have emerged as a way to exploit the known sparsity of the solution by discarding features prior to starting a Lasso solver.", "startOffset": 63, "endOffset": 847}, {"referenceID": 30, "context": "This road has been pursued in (Wang et al., 2013; Xu & Ramadge, 2013; Xiang et al., 2014), and can be thought of as a \u201cwarm start\u201d of the screening (in addition to the warm start of the solution itself).", "startOffset": 30, "endOffset": 89}, {"referenceID": 33, "context": "This road has been pursued in (Wang et al., 2013; Xu & Ramadge, 2013; Xiang et al., 2014), and can be thought of as a \u201cwarm start\u201d of the screening (in addition to the warm start of the solution itself).", "startOffset": 30, "endOffset": 89}, {"referenceID": 17, "context": "Denoting \u2206X \u201c \u03b8 P R : \u2223\u2223xJj \u03b8\u2223\u2223 \u010f 1,@j P rps( the dual feasible set, a dual formulation of the Lasso reads (see for instance Kim et al. (2007) or Xiang et al.", "startOffset": 125, "endOffset": 143}, {"referenceID": 17, "context": "Denoting \u2206X \u201c \u03b8 P R : \u2223\u2223xJj \u03b8\u2223\u2223 \u010f 1,@j P rps( the dual feasible set, a dual formulation of the Lasso reads (see for instance Kim et al. (2007) or Xiang et al. (2014)):", "startOffset": 125, "endOffset": 166}, {"referenceID": 32, "context": ", 2012) y{\u03bb q R\u03bbp y \u03bbmax q \u03bbmax \u201c }X y}8\u201c|xj\u2039y| Dynamic ST3 (Xiang et al., 2011) y{\u03bb \u0301 \u03b4xj\u2039 p q R\u03bbp\u03b8kq  \u0301 \u03b4q 1 2 \u03b4 \u201c ` \u03bbmax \u03bb \u0301 1 \u0306", "startOffset": 60, "endOffset": 80}, {"referenceID": 30, "context": ", as in (11) ) Sequential (Wang et al., 2013) \u03b8\u0302p\u03bbt \u03011q \u02c7", "startOffset": 26, "endOffset": 45}, {"referenceID": 33, "context": "See for instance (Xiang et al., 2014) for more details.", "startOffset": 17, "endOffset": 37}, {"referenceID": 27, "context": "If C \u201c t\u03b8\u0302p\u03bbqu, the safe active set is the equicorrelation set Ap\u03bbqpCq \u201c E\u03bb :\u201c tj P rps : |xj \u03b8\u0302p\u03bbq| \u201c 1u (in most cases (Tibshirani, 2013) it is exactly the active set of \u03b2\u0302p\u03bbq).", "startOffset": 121, "endOffset": 139}, {"referenceID": 27, "context": "If it is not unique, the equicorrelation set contains all the solutions\u2019 supports and there exists a Lasso solution whose support is exactly this set (Tibshirani, 2013)[Lemma 12].", "startOffset": 150, "endOffset": 168}, {"referenceID": 33, "context": "For simplicity we focus only on balls and domes, though more complicated regions could be investigated (Xiang et al., 2014).", "startOffset": 103, "endOffset": 123}, {"referenceID": 33, "context": "been considered for instance in (El Ghaoui et al., 2012; Xiang & Ramadge, 2012; Xiang et al., 2014; Bonnefoy et al., 2014b).", "startOffset": 32, "endOffset": 123}, {"referenceID": 33, "context": "Due to its length though, the formula is deferred to the Appendix (see also (Xiang et al., 2014)[Lemma 3] for more details).", "startOffset": 76, "endOffset": 96}, {"referenceID": 8, "context": "Following El Ghaoui et al. (2012) (see also (Bonnefoy et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 18, "context": "sometimes referred to as finite identification of the support (Liang et al., 2014).", "startOffset": 62, "endOffset": 82}, {"referenceID": 28, "context": ", Forward-Backward (Beck & Teboulle, 2009), Primal Dual (Chambolle & Pock, 2011), coordinatedescent (Tseng, 2001; Friedman et al., 2007)) and relies only on the convergence of a sequence of safe regions.", "startOffset": 100, "endOffset": 136}, {"referenceID": 13, "context": ", Forward-Backward (Beck & Teboulle, 2009), Primal Dual (Chambolle & Pock, 2011), coordinatedescent (Tseng, 2001; Friedman et al., 2007)) and relies only on the convergence of a sequence of safe regions.", "startOffset": 100, "endOffset": 136}, {"referenceID": 17, "context": "A more general result is proved for a specific algorithm (Forward-Backward) in Liang et al. (2014). Interestingly, our scheme is independent of the algorithm considered (e.", "startOffset": 79, "endOffset": 99}, {"referenceID": 30, "context": "The basic sphere test of (Wang et al., 2013) requires the exact dual solution \u03b8 \u201c \u03b8\u0302p\u03bbt \u03011q for center, and has radius |1{\u03bbt \u03011{\u03bbt \u03011| \u2016y\u2016, which is strictly larger than ours.", "startOffset": 25, "endOffset": 44}, {"referenceID": 30, "context": "Note that contrarily to former sequential rules (Wang et al., 2013), our introduced GAP SAFE rules still work when one has only access to approximations of \u03b8\u0302p\u03bbt \u03011q.", "startOffset": 48, "endOffset": 67}, {"referenceID": 25, "context": "We did not compare our method against the strong rules of Tibshirani et al. (2012) because they are not safe and therefore need complex post-processing with parameters to tune.", "startOffset": 58, "endOffset": 83}, {"referenceID": 25, "context": "We did not compare our method against the strong rules of Tibshirani et al. (2012) because they are not safe and therefore need complex post-processing with parameters to tune. Also we did not compare against the sequential rule of Wang et al. (2013) (e.", "startOffset": 58, "endOffset": 251}, {"referenceID": 8, "context": "As the SAFE screening rule of El Ghaoui et al. (2012) is sequential but not dynamic, for a given \u03bb the proportion of screened variables does not depend on K.", "startOffset": 33, "endOffset": 54}, {"referenceID": 3, "context": "The rules of Bonnefoy et al. (2014a) are more efficient on this dataset but they do not benefit much from the dynamic framework.", "startOffset": 13, "endOffset": 37}, {"referenceID": 24, "context": "(Schmidt et al., 2013).", "startOffset": 0, "endOffset": 22}], "year": 2015, "abstractText": "Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the socalled safe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.", "creator": "LaTeX with hyperref package"}}}