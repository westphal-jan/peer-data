{"id": "1505.04073", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2015", "title": "Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices", "abstract": "Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule---that is based on the dual projection onto convex sets (DPC)---to quickly identify the inactive features---that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features---especially for high dimensional data---which leads to a speedup up to several orders of magnitude.", "histories": [["v1", "Fri, 15 May 2015 14:31:09 GMT  (77kb,D)", "http://arxiv.org/abs/1505.04073v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jie wang", "jieping ye"], "accepted": true, "id": "1505.04073"}, "pdf": {"name": "1505.04073.pdf", "metadata": {"source": "CRF", "title": "Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices", "authors": ["Jie Wang", "Jieping Ye"], "emails": [], "sections": [{"heading": null, "text": "Multi-Task Feature Learning (MTFL) is a powerful technique for increasing predictive power by simultaneously learning multiple related classification, regression and clustering tasks. However, solving the MTFL problem remains difficult when the feature dimension is extremely large. In this paper, we propose a novel screening rule - based on dual projection on convex sets (DPC) - to quickly identify the inactive features that have zero coefficients in the solution vectors across all tasks. One of DPC's attractive features is this: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. By removing the inactive features from the training phase, we can achieve significant savings in computing costs and memory usage without sacrificing accuracy. To the best of knowledge, it is the first screening rule that is applicable to multiple data saving models."}, {"heading": "1 Introduction", "text": "Empirical studies have shown that learning multiple related tasks (MTL) at the same time often provides superior predictive performance in terms of learning each task independently of each other (Ando and Zhang, 2005, Argyriou et al., 2008, Bakker and Heskes, 2003, Wang-David and Schuller, 2003, Wang et al., 2005, Zhang et al., 2006, Chen et al., 2013) This observation also has solid theoretical foundations (Ando and Zhang, 2005, Baxter, 2000, Wang-David and Schuller, 2003, Caruana et al, 1997, Zhang et al., especially when screening for each task is small. A popular MTL method specifically for high-dimensional data is multi-task feature learning (MTFL), which punishes the Lasso group to ensure that all tasks select a common set of characteristics (Argyriou et al, 2007). MTFL has found great successes in many real world applications (but not limited to classification of breast cancer)."}, {"heading": "2 Basics", "text": "In this section, we will briefly discuss some of the basics of a popular MTFL model and mention several equivalent formulas. Suppose we have T-learning exercises {(Xt, yt): t = 1,.., T}, where Xt-RNt \u00b7 d is the data matrix of the tth task with Nt samples and d characteristics, and yt-RNt is the corresponding response vector. A widely used MTFL model (Argyriou et al., 2007) takes the form of min W-Rd \u00b7 T-T-t = 1 2-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt-Yt."}, {"heading": "3 The Dual Problem", "text": "In this section, we show that we can formulate the double problem of the MTFL model in (1) as a projection problem by using the bilinearity of the internal product. (<) We first present a new set of variables: < < (2). (2) Then the MTFL model can be written in (1): < (2). (3) s.t. zt = yt \u2212 Xtwt \u2212 Xtwt, t = 1,. (2). (T.Let model in (1)."}, {"heading": "4 The DPC Rule", "text": "Inspired by the Karush-Kuhn-Tucker (KKT) conditions (Students, 2010), we first present the general guidelines in Section 4.1. The most difficult part consists of two folds: 1) We need to estimate the optimal dual solution as accurately as possible; 2) We need to solve a nonconvex optimization problem. In Section 4.2, we give an accurate assessment of the optimal dual solution based on the geometric properties of the projection operators. Then, in Section 4.3, we show that we can efficiently solve the global optimum of the nonconvex problem. In Section 4.4, we present the DPC rule for the MTFL model (1)."}, {"heading": "4.1 Guidelines for Developing DPC", "text": "We present the general guidelines for developing screening rules for the MTFL model (1) on KKT conditions. Let the KKT conditions look like this: yt = Xtw \u0432 1 (\u03bb),.., w \u0432 T (\u03bb))), be the optimal solution (1). According to Equation. (2), (5) and (9), the KKT conditions are: yt = Xtw \u0445 t (\u03bb) + \u03bb\u0432 t (\u03bb),.., T, (14) g '(\u03b8), if (\u03bb),. (1, if (w'). (6), [\u2212 1, 1], if (w '). < x (t) = 0, \".,., d. (15), where (w')."}, {"heading": "4.2 Estimation of the Dual Optimal Solution", "text": "Based on the geometric properties of the dual problem (12), which is a projection problem, we first derive the closed form of the primary and dual problems for specific values of \u03bb in Section 4.2.1 and then give an exact estimate of \u03b8 \u043c (\u03bb) for general cases in Section 4.2.2."}, {"heading": "4.2.1 Closed form solutions", "text": "The primary and dual optimal solutions W * (\u03bb) and \u03b8 * (\u03bb) are generally unknown. However, if the value of \u03bb is sufficiently large, we expect W * (\u03bb) = 0 and \u03b8 * (\u03bb) = y\u03bb to be equation (14). The following theorem confirms this. Theorem 1. For the MTFL model in (1), lettmax = max '= 1,..., d * x * T = 1 < x (t)', y > 2. (17) Then the following statements are equivalent: y * F * p * p * p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" \"p\" \"p\" p \"p\" p \"\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"\" p \"p\" p \"p\" p \"p\" p \"p\". \""}, {"heading": "4.2.2 The general cases", "text": "Theorem 1 gives a closed form of solving the problem. Theorem 1 gives a closed form of the problem. Theorem 1 gives a closed form of the problem. Theorem 1 gives a closed form of the problem. Theorem 2: (Ruszczyn, 2006) Let C be a non empty closed convex set. Then we will specify a closed convex for each point. To make this paper independent, we first check some geometric properties of the projection operator.Theorem 2: (Ruszczyn, 2006) Let C be a non empty closed convex set. Then, for each point we haveu = PC (u)."}, {"heading": "4.3 Solving the Nonconvex Problem", "text": "In this section we solve the optimization problem in (R) with the following problem. (Gay, 1981) Let H be a symmetric matrix and D be a positive definite matrix. (We be a symmetric matrix and D be a positive definite matrix.) We be a symmetric matrix and D be a positive definite matrix. (We be a symmetric matrix and D be a positive definite matrix.) We be a same solution to (25). (We be a symmetric matrix and D be a.) be a positive definite matrix and D be a positive definite matrix. (We be a symmetric matrix and D be a a positive definitive matrix.) be a positive definitive matrix."}, {"heading": "4.4 The Proposed DPC Rule", "text": "As indicated by R \u043c, we set the proposed screening rule, DPC, for the MTFL model (1) in the following theory.Theorem 8. For the MTFL model (1), we assume that \"s\" (1) is known by \"0\" (0, \"max\").Then we have \"(\u03bb,\" 0 \") < 1\" (w \").Frequently used approaches to determine a reasonable value of\" ig, \"such as cross-validation and stability selection, must resolve the MTFL model using a grid of tuning parameter values:\" 1 > \"2 >.. >\" K, \"which is very time consuming.\" Inspired by the ideas of the \"Strong Rule\" (Tibshirani et al., 2012) and \"SAFE\" (El Ghaoui et. \"k > k), we have assumed that\" K. \""}, {"heading": "5 Experiments", "text": "We evaluate DPC on both synthetic and real data sets. To measure the performance of DPC, we specify the reject ratio, i.e. the ratio of the number of inactive features identified by DPC to the actual number of inactive features. We also report on the acceleration, i.e. the ratio of the runtime of the solver without shielding to the runtime of the solver with DPC. The solver comes from the SLEP package (Liu et al., 2009c). For each data set, we solve the MTFL model in (1) along a sequence of 100 matching parameter values with equally large distances on the logarithmic scale from \u03bb / \u03bbmax from 1.0 to 0.01. We evaluate DPC only because no existing screening rule is applicable to the MTFL model in (1)."}, {"heading": "5.1 Synthetic Studies", "text": "We conduct experiments on two synthetic data sets, called Synthetics 1 and Synthetics 2, which are commonly used in literature (Tibshirani et al., 2012, Zou and Hastie, 2005). Both synthetic 1 and Synthetics 2 have 50 tasks. For t = 1,., 50, the true model isyt = Xtw, 1, 2, 3, 4, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7"}, {"heading": "5.2 Experiments on Real Data Sets", "text": "This year we have reached the point where we are in the first half of the year in the first half of the year in the second half of the year in the second half of the year in the second half of the year in the first half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half in the first half of the year in the second half of the second half of the year in the second half of the second half of the year in the second half of the year."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel screening method for the MTFL model in (1), called DPC. The DPC screening rule is based on an in-depth analysis of the geometric properties of the dual problem and the dual practicable set. To our knowledge, DPC is the first screening rule applicable to sparse models with multiple data matrices. DPC is safe in the sense that the identified features of DPC are guaranteed to have zero coefficients in the solution vectors for all tasks. Experiments with synthetic and real data sets show that DPC is very effective in identifying the inactive features, resulting in significant savings in computing costs and memory usage, without sacrificing accuracy. In addition, DPC is more effective as the feature dimension increases, making DPC a very competitive candidate for the application of very high-dimensional data. We plan to expand DPC to more general MFL models, such as MFL models with multiple MFL models."}, {"heading": "A Discussions regarding to the Dual Problem of (1)", "text": "Although Equation (9) implies that Equation (t) (t) (t) (t) (t) (t) (t) (t) (t) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n n) (n n n) (n) (n n (n) (n) n (n) (n n n n) (n (n n n) (n) (n) (n) (n n n n n (n) (n n n (n) (n) (n) (n n n (n) (n n n n (n n) (n n n) (n) (n n n (n (n n) (n) (n) (n) (n n n) (n n n n (n n n) (n n) (n n) (n"}, {"heading": "B Proof of Theorem 1", "text": "Proof. For reasons of notation convenience, let1. y\u03bb \u0109F; 2. \u03b8 \u043a (\u03bb) = y\u03bb; 3. W \u043a (\u03bb) = 0; 4. \u03bb \u2265 \u03bbmax.Eq. (13) means that 1 equals 2. (2 \u21d4 3) Let us assume that 2 is valid. Equation (14) implies that Xtw \u0445 t (\u03bb) = 0 is for t = 1...... T: Denote the objective function of the MTFL model (1) by f (W). However, we maintain that W \u0445 (\u03bb) must be equal to zero. To see this, let W \u0445 (\u03bb) 6 = 0 be another optimal solution of (1) and thus Xtw \u03b8 t (\u03bb) = 0 for t = 1,.... However, it is obvious that f (W \u0432) < f (W \u0445) 6 = 0 must be another optimal solution of (1). This leads to a contradiction. Thus, the optimal solution W (\u03bb) = 0 for t = 1."}, {"heading": "C Proof of Corollary 4", "text": "1. To show part 1, we just have to put u1 = u and u2 = 0 and then insert them into the inequality (18) [note that PC (0) = 0 since 0-C].2. Part 1 implies that \"PC (u)\" and \"u.\" Thus, we have \"u\" and \"PC (u) and\" u, \"which corresponds to the statement in part 2. The proof is complete."}, {"heading": "D Proof of Theorem 5", "text": "We first cite some useful features of projection operators.Lemma 10. (Ruszczyn) is the answer (Ruszczyn) to the question (max). (max). (max). (max). (max). (max). (max). (max). (max). (max). (max). (max). (max). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u. (u). (u). (u). (u). (u). (u). (u). (u). (u. (u). (u. (u). (u). (u). (u. (u). (u). (u. (u). (u). (u). (u). (u. (u). (u). (u. (u). (u). (u). (). (u. (u).). (u"}], "references": [{"title": "Uncovering shared structures in multiclass classification", "author": ["Y. Amit", "M. Fink", "N. Srebro", "S. Ullman"], "venue": "In Proceedings of the 24th Annual International Conference on Machine Learning,", "citeRegEx": "Amit et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amit et al\\.", "year": 2007}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R. Ando", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ando and Zhang.,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Task clustering and gating for bayesian multictask learning", "author": ["B. Bakker", "T. Heskes"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bakker and Heskes.,? \\Q2003\\E", "shortCiteRegEx": "Bakker and Heskes.", "year": 2003}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": null, "citeRegEx": "Bauschke and Combettes.,? \\Q2011\\E", "shortCiteRegEx": "Bauschke and Combettes.", "year": 2011}, {"title": "A model for inductive bias learning", "author": ["J. Baxter"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Baxter.,? \\Q2000\\E", "shortCiteRegEx": "Baxter.", "year": 2000}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["S. Ben-David", "R. Schuller"], "venue": "In Proceedings of Computational Learning Theory,", "citeRegEx": "Ben.David and Schuller.,? \\Q2003\\E", "shortCiteRegEx": "Ben.David and Schuller.", "year": 2003}, {"title": "Probabilistic dyadic data analysis with local and global consistency", "author": ["Deng Cai", "Xuanhui Wang", "Xiaofei He"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Cai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2009}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "A convex formulation for learning shared structures from multiple tasks", "author": ["J. Chen", "L. Tang", "J. Liu", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Safe feature elimination in sparse supervised learning", "author": ["L. El Ghaoui", "V. Viallon", "T. Rabbani"], "venue": "Pacific Journal of Optimization,", "citeRegEx": "Ghaoui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ghaoui et al\\.", "year": 2012}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Evgeniou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2005}, {"title": "Computing optimal locally constrained steps", "author": ["D. Gay"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Gay.,? \\Q1981\\E", "shortCiteRegEx": "Gay.", "year": 1981}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In Proceedings of the 28th Annual International Conference on Machine Learning,", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Tree-guided group lasso for multi-task regression with structured sparsity", "author": ["S. Kim", "E. Xing"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Kim and Xing.,? \\Q2009\\E", "shortCiteRegEx": "Kim and Xing.", "year": 2009}, {"title": "Learning to detect unseen object classes by betweenclass atttribute transfer", "author": ["C. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lampert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Bsparsity coordinate descent procedures for the multi-task with applications to neural semantic basis discovery", "author": ["H. Liu", "M. Palatucci", "J. Zhang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Multi-task feature learning with efficient `2,1-norm minimization", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "In The 25th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Computing a trust region step", "author": ["J. Mor\u00e9", "D. Sorensen"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Mor\u00e9 and Sorensen.,? \\Q1983\\E", "shortCiteRegEx": "Mor\u00e9 and Sorensen.", "year": 1983}, {"title": "Safe screening of non-support vectors in pathwise SVM computation", "author": ["K. Ogawa", "Y. Suzuki", "I. Takeuchi"], "venue": "In Proceedings of the 30th Annual International Conference on Machine Learning,", "citeRegEx": "Ogawa et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ogawa et al\\.", "year": 2013}, {"title": "Nonlinear Optimization", "author": ["A. Ruszczy\u0144ski"], "venue": null, "citeRegEx": "Ruszczy\u0144ski.,? \\Q2006\\E", "shortCiteRegEx": "Ruszczy\u0144ski.", "year": 2006}, {"title": "Fast projections onto mixed-norm balls with applications", "author": ["S. Sra"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Sra.,? \\Q2012\\E", "shortCiteRegEx": "Sra.", "year": 2012}, {"title": "Regression shringkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Tibshirani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2012}, {"title": "Two-layer feature reduction for sparse-group lasso via decomposition of convex sets", "author": ["J. Wang", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang and Ye.,? \\Q2014\\E", "shortCiteRegEx": "Wang and Ye.", "year": 2014}, {"title": "Efficient mixed-norm regularization: Algorithms and safe screening methods", "author": ["J. Wang", "J. Liu", "J. Ye"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Scaling SVM and least absolute deviations via exact data reduction", "author": ["J. Wang", "P. Wonka", "J. Ye"], "venue": "In Proceedings of the 31th Annual International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "A safe screening rule for sparse logistic regression", "author": ["J. Wang", "J. Zhou", "J. Liu", "P. Wonka", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Learning sparse representation of high dimensional data on large scale dictionaries", "author": ["Z.J. Xiang", "H. Xu", "P.J. Ramadge"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xiang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2011}, {"title": "Learning multiple related tasks using latent independent component analysis", "author": ["J. Zhang", "Z. Ghahramani", "Y. Yang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Probabilistic multi-task feature selection", "author": ["Y. Zhang", "D. Yeung", "Q. Xu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Modeling disease progression via fused sparse group lasso", "author": ["J. Zhou", "J. Liu", "V. Narayan", "J. Ye"], "venue": "In International Conference On Knowledge Discovery and Data Mining,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Zou and Hastie.,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "One popular MTL method especially for high-dimensional data is multi-task feature learning (MTFL), which uses the group Lasso penalty to ensure that all tasks select a common set of features (Argyriou et al., 2007).", "startOffset": 191, "endOffset": 214}, {"referenceID": 33, "context": "MTFL has found great success in many real-world applications including but not limited to: breast cancer classification (Zhang et al., 2010), disease progression prediction (Zhou et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 34, "context": ", 2010), disease progression prediction (Zhou et al., 2012), gene data analysis (Kim and Xing, 2009), and neural semantic basis discovery (Liu et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 15, "context": ", 2012), gene data analysis (Kim and Xing, 2009), and neural semantic basis discovery (Liu et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 26, "context": ", 2014b), sparse-group Lasso (Wang and Ye, 2014), support vector machine (SVM) (Ogawa et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 24, "context": "), for the standard Lasso problem (Tibshirani, 1996)\u2014that assumes a single data matrix\u2014to a popular MTFL model\u2014that involves multiple data matrices across different tasks.", "startOffset": 34, "endOffset": 52}, {"referenceID": 13, "context": "Then, by a carefully chosen parameterization of the constraint set, we transform the nonconvex problem to a quadratic programming problem over one quadratic constraint (QP1QC) (Gay, 1981), which can be solved for the global optimum efficiently.", "startOffset": 176, "endOffset": 187}, {"referenceID": 11, "context": ", Lasso (El Ghaoui et al., 2012, Wang et al., 2013b, Wang et al., Xiang et al., 2011, Tibshirani et al., 2012), nonnegative Lasso Wang and Ye (2014), group Lasso (Wang et al.", "startOffset": 12, "endOffset": 149}, {"referenceID": 2, "context": "A widely used MTFL model (Argyriou et al., 2007) takes the form of", "startOffset": 25, "endOffset": 48}, {"referenceID": 5, "context": "The subdifferential counterpart of the Fermat\u2019s rule (Bauschke and Combettes, 2011), i.", "startOffset": 53, "endOffset": 83}, {"referenceID": 22, "context": "(Ruszczy\u0144ski, 2006) Let C be a nonempty closed convex set.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "(Bauschke and Combettes, 2011) Let C be a nonempty closed convex subset of a Hilbert space H.", "startOffset": 0, "endOffset": 30}, {"referenceID": 13, "context": "(Gay, 1981) Let H be a symmetric matrix and D be a positive definite matrix.", "startOffset": 0, "endOffset": 11}, {"referenceID": 13, "context": "If this is the case, we apply Newton\u2019s method (Gay, 1981) to find \u03b1\u2217 as follows.", "startOffset": 46, "endOffset": 57}, {"referenceID": 20, "context": "As pointed out by Mor\u00e9 and Sorensen (1983), Newton\u2019s method is very efficient to find \u03b1\u2217 as \u03c6(\u03b1) is almost linear on (2\u03c1`,\u221e).", "startOffset": 18, "endOffset": 43}, {"referenceID": 25, "context": "Inspired by the ideas of Strong Rule (Tibshirani et al., 2012) and SAFE (El Ghaoui et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 8, "context": "2 Experiments on Real Data Sets We perform experiments on three real data sets: 1) the TDT2 text data set (Cai et al., 2009); 2) the animal data set (Lampert et al.", "startOffset": 106, "endOffset": 124}, {"referenceID": 16, "context": ", 2009); 2) the animal data set (Lampert et al., 2009); 3) the Alzheimers Disease Neuroimaging Initiative (ADNI) data set (http://adni.", "startOffset": 32, "endOffset": 54}, {"referenceID": 0, "context": "Similar to the Animal data set, we construct 30 tasks, each of which is a classification task of one category against all the others (Amit et al., 2007).", "startOffset": 133, "endOffset": 152}, {"referenceID": 13, "context": "By following the experiment settings in Kang et al. (2011), we choose 20 animal classes in the data set: antelope, grizzly-bear, killer-whale, beaver, Dalmatian, Persiancat, horse, german- shepherd, blue-whale, Siamese-cat, skunk, ox, tiger, hippopotamus, leopard, moose, spidermonkey, humpbackwhale, elephant, and gorilla.", "startOffset": 40, "endOffset": 59}, {"referenceID": 13, "context": "By following the experiment settings in Kang et al. (2011), we choose 20 animal classes in the data set: antelope, grizzly-bear, killer-whale, beaver, Dalmatian, Persiancat, horse, german- shepherd, blue-whale, Siamese-cat, skunk, ox, tiger, hippopotamus, leopard, moose, spidermonkey, humpbackwhale, elephant, and gorilla. We construct 20 tasks, where each of them is a classification task of one type of animal against all the others. For the tth task, we first randomly select 30 samples from the tth class as the positive samples; and then we randomly select 30 samples from all the other classes as the negative samples. We make use of all the seven sets of features kindly provided by Lampert et al. (2009): color histogram features, local self-similarity features, PyramidHOG (PHOG) features, SIFT features, colorSIFT features, SURF features, and DECAF features.", "startOffset": 40, "endOffset": 713}, {"referenceID": 22, "context": "Because g`\u2217(\u00b7) is convex, we have (Ruszczy\u0144ski, 2006) g`\u2217(\u03b8)\u2212 g`\u2217 ( y \u03bbmax ) \u2265 \u3008 \u2207g`\u2217 ( y \u03bbmax ) , \u03b8 \u2212 y \u03bbmax \u3009 .", "startOffset": 34, "endOffset": 53}], "year": 2015, "abstractText": "Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule\u2014that is based on the dual projection onto convex sets (DPC)\u2014to quickly identify the inactive features\u2014that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features\u2014especially for high dimensional data\u2014which leads to a speedup up to several orders of magnitude.", "creator": "LaTeX with hyperref package"}}}