{"id": "1505.04123", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2015", "title": "Margins, Kernels and Non-linear Smoothed Perceptrons", "abstract": "We focus on the problem of finding a non-linear classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of view (finding a perfect separator when one exists) and the dual point of view (giving a certificate of non-existence), with special focus on generalizations of two classical schemes - the Perceptron (primal) and Von-Neumann (dual) algorithms.", "histories": [["v1", "Fri, 15 May 2015 16:54:58 GMT  (21kb)", "http://arxiv.org/abs/1505.04123v1", "17 pages, published in the proceedings of the International Conference on Machine Learning, 2014"]], "COMMENTS": "17 pages, published in the proceedings of the International Conference on Machine Learning, 2014", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NA math.OC", "authors": ["aaditya ramdas", "javier pe\u00f1a"], "accepted": true, "id": "1505.04123"}, "pdf": {"name": "1505.04123.pdf", "metadata": {"source": "CRF", "title": "Margins, Kernels and Non-linear Smoothed Perceptrons", "authors": ["Aaditya Ramdas"], "emails": ["aramdas@cs.cmu.edu", "jfp@andrew.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 5.04 123v 1 [cs.L G] 15 MWe formulate our problem as one of maximizing the regularized normalized hard margin matrix (\u03c1) in an RKHS and formulating it in terms of a Mahalanobis dot product / semi-standard associated with the (normalized and signed) gram matrix of the core. We derive an accelerated smoothed algorithm with a convergence rate of \u221a log n\u03c1 givenn separable points strikingly similar to the classical kernelized perceptron algorithm, the rate of which is 1\u03c12. If there is no such classifier, we prove a version of Gordon's separation theorem for RKHS and give a reinterpretation of negative margins. This allows us to provide warranties for a primary-dual algorithm that is practically feasible in min-n-1-2 iterations with a separation set in the RHS, or if one is practically feasible."}, {"heading": "1 Introduction", "text": "We are interested in the problem of searching for a nonlinear delimiter for a given set of n points x1,..., xn Rd with labels y1,..., yn xi {\u00b1 1}. The search for a linear delimiter can be called the problem of searching for a unit vector w Rd (if there is one) so that for all iyi (w xi) = yi. (1) This is called the primal problem. In the more interesting nonlinear setting we will look for functions f in a reproducing kernel Hilbert Space (RKHS) FK that are associated with the kernel K (will be defined later), so that for all i yif (xi) \u2265 0. (2) We say that problems (1), (2) have an unnormalized margin f > 0 if there is a unit vector w, so that for all i, yi (w xi) or ytronamic (xi).to the 1999 HS unlinear concepts we present a normalized Y in the middle of the concepts."}, {"heading": "1.1 Related Work", "text": "In this paper, we focus on Rosenblatt's famous Perceptron algorithm (1958) and Dantzig's less famous VonNeumann algorithm (1992), which we present in later sections. As mentioned by Epelman & Freund (2000), Nesterov points out in a technical report of the same name that the latter is a special case of the now popular Frank Wolfe algorithm. Our work builds on Soheili & Pen (2012, 2013b) from the optimization field - we generalize the setting of learning functions in RKHSs, expand the algorithms, simplify the evidence, and at the same time bring new perspectives to it. There is extensive literature on the Perceptron algorithm in the learning community; we limit ourselves to discussing just a few directly related papers to illustrate the different differences."}, {"heading": "1.2 Paper Outline", "text": "Sec.2 will introduce the Perceptron and Normalized Perceptron algorithms, and their convergence guarantees a linear separation, with particular emphasis on the non-normalized and normalized margins. Sec.3 will then introduce RKHSs and the Normalized Kernel Perceptron algorithm, which we interpret as a subgradient algorithm for a regulated normalized hard-margin loss function. Sec.4 describes the Smoothed Normalized Kernel Perceptron algorithm, which operates with a gentle approximation to the original loss function, and outlines the argument for its faster convergence rate. Sec.5 discusses the non-separable case and the von Neumann algorithm, and we demonstrate a version of Gordon's theorem in RKHS.Finally, we will provide an algorithm in Sec.6 that ends with a separator if there is one, and a dual certificate for near-impossibility otherwise proportional margins."}, {"heading": "2 Linear Feasibility Problems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Perceptron", "text": "The classical perceptron algorithm can be specified in many ways, one is in the following form algorithm 1 > Perceptron Initialize w0 = 0 for k = 0, 1, 2, 3,... doif sign (w'k xi) 6 = yi for some i then wk + 1: = wk + yixi else Stop: Return wk as solutionend if end forIt comes with the following classical guarantee, as proved by Block (1962) and Novikoff (1962): If there is a unit vector u'Rd so that Y X'u \u2265 \u03c1 > 0, then a perfect delimiter will be found in maxi \u00b2 2 2\u03c12iterations / errors. The algorithm works if it is updated with any point that is incorrectly classified (xi, yi); it has the same guarantees if w is updated with the point that is incorrectly classified."}, {"heading": "2.2 Normalized Margins", "text": "If we normalize the data points according to the valid separator line for X2, the resulting error, which is limited by the perceptron algorithm, is slightly different. Let X2 represent the matrix with columns xi / xi + 2. Define the unnormalized and normalized border as\u03c1: = sup-w-2 = 1 inf-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "3 Kernels and RKHSs", "text": "The theory of kernel reproduction Hilbert Spaces (RKHSs) has a long history, and for a detailed introduction we refer to Scholkopf & Smola (2002). Let K: Rd \u00b7 Rd \u00b7 R be a symmetrical positive defined kernel that leads to a reproducing kernel Hilbert Space FK with an associated image at each point x K \u2212 K. < K: an associated inner product < K: an associated inner product < f > K = K (u, v)."}, {"heading": "3.1 Some Interesting and Useful Lemmas", "text": "The first Lemma justifies the exit condition of our algorithms. Lemma 1. L (\u03b1) < 0 implies G\u03b1 > 0 and there is a perfect classification iff G\u03b1 > 0. Proof. L (\u03b1) < 0 \u21d2 supp."}, {"heading": "4 Smoothed Normalized Kernel Perceptron", "text": "Define the breakdown of the worst classified points (f): = arg min = > * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "5 Infeasible Problems", "text": "s theorem (Chvatal 1983): Lemma 6 (Gordans Thm). Exactly one of the following statements can be accurate. Either there is a w-rd solution so that for all i, yi (w-xi) > 0.2. Or there is a p-rd solution so that the p-rd solution is so that for all i, yi (w-xi) > 0.2. Or there is a p-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-a-ra-ra-ra-ra-ra-a-ra-ra-ra-ra-a-ra-ra-ra-a-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-ra-a-ra-ra-a-ra-a-ra-ra-ra-ra-ra-a-ra-ra-ra-ra-ra-a-ra-ra-a-"}, {"heading": "5.1 A Separation Theorem for RKHSs", "text": "While finite dimensional Euclidean spaces are provided with strong guarantees of separation, which appear under different names such as the dividing hyper-plane theorem, Gordan's theorem, Farkas's lemmas, etc., the story is not always the same for finite dimensional functional spaces, which are often difficult to handle. We will prove here an appropriate version of such a theorem, which will be useful in our environment. What follows is an interesting version of the Hahn-Banach separation theorem, which looks similar to Gordon's theorem in end-dimensional spaces. The conditions to be observed here are that either G\u03b1 > 0 or vice versa G = 0.Theorem 2. Exactly one of the following conditions has a solution: 1. Either f FK so that for all i, yif (xi), K (xi) = short-term words that do not exist."}, {"heading": "5.2 The infeasible margin \u03c1K", "text": "& & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K # 160; K & # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K; K & # 160; K; K; 160; K; K; 160; K & # 160; K; K & # 160; K; K; 160; K & # 160; K; K & # 160; K; K; K & # 160; K; K; K & # 160; K; K; K; K; K & # 160; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; 160; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K; K;"}, {"heading": "6 Kernelized Primal-Dual Algorithms", "text": "The previous theorems allow us to write a variant of the normalized VonNeumann algorithm from the previous section, which is smoothed out and works for RKHSs. DefinieW: = {p-n-ipiiiiii-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-ii-i-ii-i-i-ii-i-ii-i-ii-i-ii-i-ii-ii-i-ii-ii-ii-ii-i-ii-ii-i-ii-ii-i-ii-ii-ii-ii-i-ii-ii-i-ii-i-ii-i-ii-ii-i-ii-ii-i-ii-i-ii-i-ii-ii-i-ii-i-ii-i-ii-i-ii-ii-i-ii-i-ii-i-ii-i-ii-ii-i-i-ii-ii-i-ii-ii-i-ii-i-ii-i-ii-i-ii-ii-i-i-ii-ii-i-ii-i-ii-ii-ii-i-i-ii-i-ii-ii-i-ii-i-ii-ii-i-ii-i-i-i-ii-ii-i-i-ii-ii-ii-ii-ii-ii-i-i-i-i-i-ii-i-i-ii-i-i-ii-i-ii-i-i-i-ii-ii-i-i-ii-i-ii-i-i-i-i-ii-ii-i-i-ii-i-ii-"}, {"heading": "As a consequence, \u2016p\u2016G = 0 iff p \u2208 W .", "text": "This is trivial for p = W and p = p = p = p > q = q = q = q = q = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p = p = p = p p p p = p = p = p p"}, {"heading": "7 Discussion", "text": "The SNK-Perceptron-Von-Neumann algorithm presented in this paper has a convergence rate of \u221a logn \u03c1K |} depending on the number of points. It is noteworthy that both are independent of the underlying dimensionality of the problem. We suspect that it is possible to reduce this dependence depending on the primary dual algorithm without paying a price in terms of dependence on margin 1 / 2 (or dependence on the underlying dimension of the problem). It is possible that a narrower dependence on n is possible if we try to use other smoothing functions instead of the 2 standard used in the last section. Specifically, it may be tempting to pay a price with dependence on margin 1 / 3 (or dependence on dependence on the underlying dimension of the problem)."}, {"heading": "Acknowledgements", "text": "We thank Negar Soheili and Avrim Blum for their insightful discussions."}, {"heading": "A Unified Proof By Induction of Lemma 5, 8: L\u00b5k(\u03b1k) \u2264 \u221212\u2016pk\u20162G", "text": "Allow d (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p (p), p (p), p (p), p (p (p), p (p), p (p (p), p (p (p), p (p), p (p (p), p (p (p), p (p (p), p (p (p), p (p (p), p (p (p), p (p), p (p (p), p (p), p (p (p), p (p (p), p (p), p (p (p), p (p (p), p (p), p (p (p), p (p), p (p (p), p (p (p), p (p (p), p (p), p (p (p), p (p (p), p (p), p (p (p), p (p (p), p (p), p (p), p (p (p), p (p (p"}], "references": [{"title": "The perceptron: A model for brain functioning", "author": ["H. Block"], "venue": "i\u2019, Reviews of Modern Physics", "citeRegEx": "Block,? \\Q1962\\E", "shortCiteRegEx": "Block", "year": 1962}, {"title": "Complexity and real computation, Springer", "author": ["L. Blum", "F. Cucker", "M. Shub", "S. Smale"], "venue": null, "citeRegEx": "Blum et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1998}, {"title": "A new condition number for linear programming", "author": ["D. Cheung", "F. Cucker"], "venue": "Mathematical programming", "citeRegEx": "Cheung and Cucker,? \\Q2001\\E", "shortCiteRegEx": "Cheung and Cucker", "year": 2001}, {"title": "Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm", "author": ["K.L. Clarkson"], "venue": "ACM Transactions on Algorithms", "citeRegEx": "Clarkson,? \\Q2010\\E", "shortCiteRegEx": "Clarkson", "year": 2010}, {"title": "A simple polynomial-time rescaling algorithm for solving linear programs", "author": ["J. Dunagan", "S. Vempala"], "venue": "Mathematical Programming", "citeRegEx": "Dunagan and Vempala,? \\Q2008\\E", "shortCiteRegEx": "Dunagan and Vempala", "year": 2008}, {"title": "Condition number complexity of an elementary algorithm for computing a reliable solution of a conic linear system", "author": ["M. Epelman", "R.M. Freund"], "venue": "Mathematical Programming", "citeRegEx": "Epelman and Freund,? \\Q2000\\E", "shortCiteRegEx": "Epelman and Freund", "year": 2000}, {"title": "Condition-based complexity of convex optimization in conic linear form via the ellipsoid algorithm", "author": ["R.M. Freund", "J.R. Vera"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Freund and Vera,? \\Q1999\\E", "shortCiteRegEx": "Freund and Vera", "year": 1999}, {"title": "From margin to sparsity\u2019, Advances in neural information processing systems", "author": ["T. Graepel", "R. Herbrich", "R.C. Williamson"], "venue": null, "citeRegEx": "Graepel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Graepel et al\\.", "year": 2001}, {"title": "Redundant noisy attributes, attribute errors, and linear-threshold learning using winnow, in \u2018Proceedings of the fourth annual workshop on Computational learning theory", "author": ["N. Littlestone"], "venue": null, "citeRegEx": "Littlestone,? \\Q1991\\E", "shortCiteRegEx": "Littlestone", "year": 1991}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["A. Nemirovski"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Nemirovski,? \\Q2004\\E", "shortCiteRegEx": "Nemirovski", "year": 2004}, {"title": "Excessive gap technique in nonsmooth convex minimization", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Nesterov,? \\Q2005\\E", "shortCiteRegEx": "Nesterov", "year": 2005}, {"title": "On convergence proofs for perceptrons, Technical report", "author": ["A.B. Novikoff"], "venue": null, "citeRegEx": "Novikoff,? \\Q1962\\E", "shortCiteRegEx": "Novikoff", "year": 1962}, {"title": "Incorporating condition measures into the complexity theory of linear programming", "author": ["J. Renegar"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Renegar,? \\Q1995\\E", "shortCiteRegEx": "Renegar", "year": 1995}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain.", "author": ["F. Rosenblatt"], "venue": "Psychological review", "citeRegEx": "Rosenblatt,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt", "year": 1958}, {"title": "New approximation algorithms for minimum enclosing convex shapes, in \u2018Proceedings of the Twenty-Second", "author": ["A. Saha", "S. Vishwanathan", "X. Zhang"], "venue": "Annual ACM-SIAM Symposium on Discrete Algorithms\u2019,", "citeRegEx": "Saha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saha et al\\.", "year": 2011}, {"title": "A generalized representer theorem, in \u2018Computational learning theory", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "Learning with kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "A smooth perceptron algorithm", "author": ["N. Soheili", "J. Pe\u00f1a"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Soheili and Pe\u00f1a,? \\Q2012\\E", "shortCiteRegEx": "Soheili and Pe\u00f1a", "year": 2012}, {"title": "2013a), \u2018A deterministic rescaled perceptron algorithm", "author": ["N. Soheili", "J. Pe\u00f1a"], "venue": null, "citeRegEx": "Soheili and Pe\u00f1a,? \\Q2013\\E", "shortCiteRegEx": "Soheili and Pe\u00f1a", "year": 2013}, {"title": "2013b), A primal\u2013dual smooth perceptron\u2013von Neumann algorithm, in \u2018Discrete Geometry and Optimization", "author": ["N. Soheili", "J. Pe\u00f1a"], "venue": null, "citeRegEx": "Soheili and Pe\u00f1a,? \\Q2013\\E", "shortCiteRegEx": "Soheili and Pe\u00f1a", "year": 2013}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Tseng,? \\Q2008\\E", "shortCiteRegEx": "Tseng", "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "It can be thought of as the width of the feasibility cone as in Freund & Vera (1999), a radius of well-posedness as in Cheung & Cucker (2001), and its inverse can be seen as a special case of a condition number defined by Renegar (1995) for these systems.", "startOffset": 222, "endOffset": 237}, {"referenceID": 8, "context": "It can be thought of as the width of the feasibility cone as in Freund & Vera (1999), a radius of well-posedness as in Cheung & Cucker (2001), and its inverse can be seen as a special case of a condition number defined by Renegar (1995) for these systems. 1.1 Related Work In this paper we focus on the famous Perceptron algorithm from Rosenblatt (1958) and the less-famous VonNeumann algorithm from Dantzig (1992) that we introduce in later sections.", "startOffset": 222, "endOffset": 354}, {"referenceID": 8, "context": "It can be thought of as the width of the feasibility cone as in Freund & Vera (1999), a radius of well-posedness as in Cheung & Cucker (2001), and its inverse can be seen as a special case of a condition number defined by Renegar (1995) for these systems. 1.1 Related Work In this paper we focus on the famous Perceptron algorithm from Rosenblatt (1958) and the less-famous VonNeumann algorithm from Dantzig (1992) that we introduce in later sections.", "startOffset": 222, "endOffset": 415}, {"referenceID": 8, "context": "It can be thought of as the width of the feasibility cone as in Freund & Vera (1999), a radius of well-posedness as in Cheung & Cucker (2001), and its inverse can be seen as a special case of a condition number defined by Renegar (1995) for these systems. 1.1 Related Work In this paper we focus on the famous Perceptron algorithm from Rosenblatt (1958) and the less-famous VonNeumann algorithm from Dantzig (1992) that we introduce in later sections. As mentioned by Epelman & Freund (2000), in a technical report by the same name, Nesterov pointed out in a note to the authors that the latter is a special case of the now-popular Frank-Wolfe algorithm.", "startOffset": 222, "endOffset": 492}, {"referenceID": 7, "context": "As mentioned by Epelman & Freund (2000), in a technical report by the same name, Nesterov pointed out in a note to the authors that the latter is a special case of the now-popular Frank-Wolfe algorithm. Our work builds on Soheili & Pe\u00f1a (2012, 2013b) from the field of optimization - we generalize the setting to learning functions in RKHSs, extend the algorithms, simplify proofs, and simultaneously bring new perspectives to it. There is extensive literature around the Perceptron algorithm in the learning community; we restrict ourselves to discussing only a few directly related papers, in order to point out the several differences from existing work. We provide a general unified proof in the Appendix which borrows ideas from accelerated smoothing methods developed by Nesterov (2005) - while this algorithm and others by Nemirovski (2004), Saha et al.", "startOffset": 81, "endOffset": 793}, {"referenceID": 7, "context": "We provide a general unified proof in the Appendix which borrows ideas from accelerated smoothing methods developed by Nesterov (2005) - while this algorithm and others by Nemirovski (2004), Saha et al.", "startOffset": 172, "endOffset": 190}, {"referenceID": 7, "context": "We provide a general unified proof in the Appendix which borrows ideas from accelerated smoothing methods developed by Nesterov (2005) - while this algorithm and others by Nemirovski (2004), Saha et al. (2011) can achieve similar rates for the same problem, those algorithms do not possess the simplicity of the Perceptron or Von-Neumann algorithms and our variants, and also don\u2019t look at the infeasible setting or primal-dual algorithms.", "startOffset": 172, "endOffset": 210}, {"referenceID": 7, "context": "We provide a general unified proof in the Appendix which borrows ideas from accelerated smoothing methods developed by Nesterov (2005) - while this algorithm and others by Nemirovski (2004), Saha et al. (2011) can achieve similar rates for the same problem, those algorithms do not possess the simplicity of the Perceptron or Von-Neumann algorithms and our variants, and also don\u2019t look at the infeasible setting or primal-dual algorithms. Accelerated smoothing techniques have also been seen in the learning literature like in Tseng (2008) and many others.", "startOffset": 172, "endOffset": 541}, {"referenceID": 3, "context": "This work is also connected to the idea of \u01eb-coresets by Clarkson (2010), though we will not explore that angle.", "startOffset": 57, "endOffset": 73}, {"referenceID": 3, "context": "This work is also connected to the idea of \u01eb-coresets by Clarkson (2010), though we will not explore that angle. A related algorithm is called the Winnow by Littlestone (1991) - this works on the l1 margin and is a saddle point problem over two simplices.", "startOffset": 57, "endOffset": 176}, {"referenceID": 0, "context": "do if sign(w\u22a4 k xi) 6= yi for some i then wk+1 := wk + yixi else Halt: Return wk as solution end if end for It comes with the following classic guarantee as proved by Block (1962) and Novikoff (1962): If there exists a unit vector u \u2208 R such that Y X\u22a4u \u2265 \u03c1 > 0, then a perfect separator will be found in maxi \u2016xi\u2016 2 2 \u03c12 iterations/mistakes.", "startOffset": 167, "endOffset": 180}, {"referenceID": 0, "context": "do if sign(w\u22a4 k xi) 6= yi for some i then wk+1 := wk + yixi else Halt: Return wk as solution end if end for It comes with the following classic guarantee as proved by Block (1962) and Novikoff (1962): If there exists a unit vector u \u2208 R such that Y X\u22a4u \u2265 \u03c1 > 0, then a perfect separator will be found in maxi \u2016xi\u2016 2 2 \u03c12 iterations/mistakes.", "startOffset": 167, "endOffset": 200}, {"referenceID": 7, "context": "Hence, it is always better to normalize the data as pointed out in Graepel et al. (2001). This idea extends to RKHSs, motivating the normalized Gram matrix considered later.", "startOffset": 67, "endOffset": 89}, {"referenceID": 7, "context": "Hence, it is always better to normalize the data as pointed out in Graepel et al. (2001). This idea extends to RKHSs, motivating the normalized Gram matrix considered later. Example Consider a simple example in R+. Assume that + points are located along the line 6x2 = 8x1, and the \u2212 points along 8x2 = 6x1, for 1/r \u2264 \u2016x\u20162 \u2264 r, where r > 1. The max-margin linear separator will be x1 = x2. If all the data were normalized to have unit Euclidean norm, then all the + points would all be at (0.6, 0.8) and all the \u2212 points at (0.8, 0.6), giving us a normalized margin of \u03c12 \u2248 0.14. Unnormalized, the margin is \u03c1 \u2248 0.14/r and maxi \u2016xi\u20162 = r. Hence, in terms of bounds, we get a discrepancy of r, which can be arbitrarily large. Winnow The question arises as to which norm we should normalize by. There is a now classic algorithm in machine learning, called Winnow by Littlestone (1991) or Multiplicate Weights.", "startOffset": 67, "endOffset": 883}, {"referenceID": 7, "context": "Hence, it is always better to normalize the data as pointed out in Graepel et al. (2001). This idea extends to RKHSs, motivating the normalized Gram matrix considered later. Example Consider a simple example in R+. Assume that + points are located along the line 6x2 = 8x1, and the \u2212 points along 8x2 = 6x1, for 1/r \u2264 \u2016x\u20162 \u2264 r, where r > 1. The max-margin linear separator will be x1 = x2. If all the data were normalized to have unit Euclidean norm, then all the + points would all be at (0.6, 0.8) and all the \u2212 points at (0.8, 0.6), giving us a normalized margin of \u03c12 \u2248 0.14. Unnormalized, the margin is \u03c1 \u2248 0.14/r and maxi \u2016xi\u20162 = r. Hence, in terms of bounds, we get a discrepancy of r, which can be arbitrarily large. Winnow The question arises as to which norm we should normalize by. There is a now classic algorithm in machine learning, called Winnow by Littlestone (1991) or Multiplicate Weights. It works on a slight transformation of the problem where we only need to search for u \u2208 R+. It comes with some very wellknown guarantees - If there exists a u \u2208 R+ such that Y X\u22a4u \u2265 \u03c1 > 0, then feasibility is guaranteed in \u2016u\u20161maxi \u2016ai\u2016\u221e logn/\u03c1 iterations. The appropriate notion of normalized margin here is \u03c11 := max w\u2208\u2206d min p\u2208\u2206n \u3008Y X\u22a4 \u221ew, p\u3009, where X\u221e is a matrix with columns xi/\u2016xi\u2016\u221e. Then, the appropriate iteration bound is logn/\u03c11. We will return to this l1-margin in the discussion section. In the next section, we will normalize by using the kernel appropriately. 3 Kernels and RKHSs The theory of Reproducing Kernel Hilbert Spaces (RKHSs) has a rich history, and for a detailed introduction, refer to Sch\u00f6lkopf & Smola (2002). Let K : R \u00d7 R \u2192 R be a symmetric positive definite kernel, giving rise to a Reproducing Kernel Hilbert Space FK with an associated feature mapping at each point x \u2208 R called \u03c6x : R \u2192 FK where \u03c6x(.", "startOffset": 67, "endOffset": 1646}, {"referenceID": 15, "context": "is some empirical loss function on the data and 12\u2016f\u2016K is an increasing function of \u2016f\u2016K , the Representer Theorem (Sch\u00f6lkopf et al. 2001) implies that the minimizer of the above function lies in the span of \u03c6xis (also the span of the yi\u03c6\u0303xis).", "startOffset": 115, "endOffset": 138}, {"referenceID": 10, "context": "i pi log pi + log n (12) is 1-strongly convex with respect to the l1-norm (Nesterov 2005).", "startOffset": 74, "endOffset": 89}, {"referenceID": 10, "context": "We provide a concise, self-contained and unified proof by induction in the Appendix for Lemma 5 and Lemma 8, borrowing ideas from Nesterov\u2019s excessive gap technique (Nesterov 2005) for smooth minimization of structured non-smooth functions.", "startOffset": 165, "endOffset": 180}], "year": 2015, "abstractText": "We focus on the problem of finding a non-linear classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of view (finding a perfect separator when one exists) and the dual point of view (giving a certificate of non-existence), with special focus on generalizations of two classical schemes the Perceptron (primal) and Von-Neumann (dual) algorithms. We cast our problem as one of maximizing the regularized normalized hard-margin (\u03c1) in an RKHS and rephrase it in terms of a Mahalanobis dot-product/semi-norm associated with the kernel\u2019s (normalized and signed) Gram matrix. We derive an accelerated smoothed algorithm with a convergence rate of \u221a log n \u03c1 given n separable points, which is strikingly similar to the classical kernelized Perceptron algorithm whose rate is 1 \u03c12 . When no such classifier exists, we prove a version of Gordan\u2019s separation theorem for RKHSs, and give a reinterpretation of negative margins. This allows us to give guarantees for a primal-dual algorithm that halts in min{ \u221a n |\u03c1| , \u221a n \u01eb } iterations with a perfect separator in the RKHS if the primal is feasible or a dual \u01eb-certificate of near-infeasibility.", "creator": "LaTeX with hyperref package"}}}