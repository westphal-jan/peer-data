{"id": "1505.04627", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2015", "title": "Simple regret for infinitely many armed bandits", "abstract": "We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter $\\beta$ characterizing the distribution of the near-optimal arms. We prove that depending on $\\beta$, our algorithm is minimax optimal either up to a multiplicative constant or up to a $\\log(n)$ factor. We also provide extensions to several important cases: when $\\beta$ is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.", "histories": [["v1", "Mon, 18 May 2015 13:16:42 GMT  (118kb,D)", "http://arxiv.org/abs/1505.04627v1", "in 32th International Conference on Machine Learning (ICML 2015)"]], "COMMENTS": "in 32th International Conference on Machine Learning (ICML 2015)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alexandra carpentier", "michal valko"], "accepted": true, "id": "1505.04627"}, "pdf": {"name": "1505.04627.pdf", "metadata": {"source": "META", "title": "Simple regret for infinitely many armed bandits", "authors": ["Alexandra Carpentier", "Michal Valko"], "emails": ["A.CARPENTIER@STATSLAB.CAM.AC.UK", "MICHAL.VALKO@INRIA.FR"], "sections": [{"heading": "1. Introduction", "text": "In many of these situations, the learner is confronted with a large number of possible actions under which he has to make a decision. In this context, the attitude, which we see as a direct extension of classical decision-making, in which we receive only feedback for the actions we select, is a selection from all actions (called the weapons) and receives a reward from the chosen action, which is typically a noisy characterization of the action. The learner then performs such rounds and their performance in relation to some criteria, for example, the cumulative regret or the simple regret of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W & CP Volume 37. Copyright 2015 by the author."}, {"heading": "2. Setting", "text": "We assume that in each case (in this case: Kt + 1 = Kt + 1 = At) the distribution of the distribution quantities (the distribution of the distribution quantity) has already been observed (in this case: Kt + 1 = Kt + 1 = At), or select a sample of a new arm generated according to L \u00b2 (in this case: Kt + 1 = Kt + 1 = Kt), or select a sample of a new arm generated according to L \u00b2 (in this case: Kt + 1 = Kt + 1 and in this case: Kt + 1 = Kt + 1}, which has already been observed (in this case: Kt + 1 = Kt + 1 = At), or select a sample of a new arm generated according to L \u00b2 (in this case: Kt + 1 = Kt + 1)."}, {"heading": "3. Main results", "text": "In this section we first present the information-theoretical lower limits for the infinite number of armed bandits, with a simple regret as a goal. Then we present our algorithm and its analysis, which proves the upper limits that match the lower limits - in some cases depending on \u03b2 up to a polylog-n factor. This makes our algorithm (almost) optimal."}, {"heading": "3.1. Lower bounds", "text": "The following theorem shows the information-theoretical complexity of our problem and is documented in Appendix \u03b2 \u03b2 C. Note that the rates depend decisively on \u03b2. Theorem 1 (lower limits). Let us write S\u03b2 for the composition of the distributions of weapons L, which fulfill the assumptions 1 and 2 for the parameters \u03b2, E, E, E, C. Let us assume that n is more than one constant, which depends on \u03b2, E, E, E, B, C. Depending on the value of \u03b2, we have the following results for each algorithm A, where v is a sufficiently small constant. \u2022 Case \u03b2 < 2: With a probability greater than 1 / 3, inf A sup L, E, B, C. Depending on the delimitation, we have the following results, for each algorithm A, where v is a constant."}, {"heading": "3.2. SiRI and its upper bounds", "text": "In this section, we present our algorithms, which we are able to hide ourselves in if we are able to show ourselves in a position to show what we are able to show ourselves in a position to be able. (n) Let us be able to put ourselves in a position to put ourselves in a position to put ourselves in a position to put ourselves in. (n) Let us put the logarithm in a position to put ourselves in a position to put ourselves in a position to put ourselves in a position that we are able to put ourselves in a position to put ourselves in a position. (n) Let us put the logarithm in a position that we are able to put ourselves in a position that we are able to put ourselves in a position that we are able to be in a position that we are able to be in a position to be able. (t) Let us be able to be in a position to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are in a position that we are able to be in a position that we are able to be able. (t) Let's put ourselves in a position to be in a position that we are in a position that we are able to be in a position that we are in a position that we are able to be in a position that we are able to be in a position that we are able to be in a position that we are able."}, {"heading": "4. Extensions of SiRI", "text": "We will now briefly discuss three extensions of the SiRI algorithm, which are either very relevant for practical or computational reasons, or for comparison with the previous results. In particular, we will consider cases 1) where there are no unambiguous results, 2) in a natural environment where the near-optimal arms exhibit a small variance, and 3) in the case of the unknown time horizon. These extensions are all in a sense derived from our results and from the existing literature, and we will therefore use them as corollaries.Algorithm 2 Amber-SiRI parameters: C, \u03b2, 3 redefined quantities: Set the number of arms asT \u03b2 = dmin (n), n / log."}, {"heading": "4.2. Dealing with unknown \u03b2", "text": "In practice, the convergence of these estimates is barely discernible, since the means of these estimates are not directly observed by us, but by ourselves. (...) In practice, the convergence of these estimates is barely discernible. (...) In practice, the convergence of these estimates is barely discernible. (...) In practice, the convergence of these estimates is evident. (...) In practice, the convergence of these estimates is barely discernible. (...) In practice, the convergence of these estimates is evident. (...) In practice, the convergence of these estimates is not evident. (...) In practice, the convergence of these estimates is evident. (...) In practice, the convergence of these estimates is evident. (...) In practice, the convergence of these estimates is evident. (...) In practice, the convergence of these estimates is evident. (...) In practice, the convergence of these estimates is evident."}, {"heading": "4.3. Anytime algorithm", "text": "Another interesting question is whether it is possible to produce SiRI at any time, a question that can be quickly answered in the affirmative. First, we can simply use a double trick to double the size of the sample in each period and throw away the preliminary samples used in the previous period. Second, Wang et al. (2008) suggest a more sophisticated way to deal with an unknown time horizon (UCB-AIR), which also applies directly to SiRI. Using these modifications, it is easy to convert SiRI into an anytime algorithm. Simple regret in this ever-present environment is only aggravated by a polylog n, where n is the unknown horizon. Specifically, in the current setting, SiRI's regret has been modified either by the duplication trick or by the construction of UCB-AIR, which is most likely satisfied (polylog (n) max (n \u2212 1 / 2, n \u2212 1 / \u03b2)))."}, {"heading": "5. Numerical simulations", "text": "In order to simulate different regimes of performance according to \u03b2-regularity, we look at different reservoir distributions of the arms. & \u03b2-distributions of the arms. In particular, we look at beta distributions B (\u03b2, y) as x = 1 and y = \u03b2. For B (1, \u03b2), the assumption 2 is exactly satisfied with regularity \u03b2. Since to the best of our knowledge SiRI is the first algorithm that optimizes simple regret in the infinitely many arms, there is no natural competitor for it. Nevertheless, we compare the algorithms used for linked settings.First, such a comparator is UCB-F (Wang et al., 2008), an algorithm that optimizes cumulative regret for this setting. UCB-F is designed for fixed horizon assessments and it is an extension of a version of UCB-V by Audibert et al. (2007). Second, we compare SiRI to lil'UCB et al."}, {"heading": "A. Additional notation", "text": "We write P1 for the probability in relation to the distribution of the arm reservoirs, P2 for the probability in relation to the distribution of the samples from the arms, and P1,2 for the probability in relation to both the distribution of the arm reservoirs and the distribution of the samples from the armies. F is the distribution function of the middle reservoir distribution L. F \u2212 1 is the pseudo-inverse of the middle reservoir distribution. To express the regularity assumption, we define G (\u00b7) = \u00b5; F \u2212 1 (1 \u2212 \u00b7). We assume that G has a certain regularity in its right end point, which is a standard assumption for an infinite number of armed bandits. In particular, we rewrite assumption 2 by modifying only the constants E, E, E, E and B. Assumption 3 (\u03b2 regularity in \u00b5, version 2). There is E, E \u2032, B \u2032 (0, 1) such an assumption that we assume that it is the same assumption (> B) or vice versa."}, {"heading": "B. Full proof of Theorem 2", "text": "The first layer consists of evidence about the empirical distributions of the arms, which are not too different from the actual means of the arms. In this part, the decisive object is event 2, more precisely, these two layers can be divided as follows: \u2022 We prove the appropriate high probability that the upper limits and lower limits of the arms are not too different from the number of arms drawn by the algorithms that have a given gap, depending on the gap considered. This is done in Lemma 4. Two important results can therefore be derived: (i) An upper layer on the number of suboptimal arms, depending on how suboptimal they are."}, {"heading": "C. Full proof of Theorem 1", "text": "Case \u03b2 < 2By Assumption 2 (equivalent to Assumption 3), we know thatE \u2032 u1 / \u03b2 \u2265 G (\u03b2) \u2265 Eu1 / \u03b2.Let's assume that if we pull an arm out of the reservoir, its distribution is simpler than the distribution associated with G and has variance 1. Since the budget is limited by n, an algorithm pulls at most n arms out of the reservoir. Let's define that I1 = [\u00b5) has its distribution 1 / \u03b2 1 \u00b0 n and variance 1 \u00b0 1. (c \u00b2) 1 / \u03b2 \u00b2 n mobility on most n arms out of the reservoir. (c \u00b2)"}, {"heading": "D. Proof of Lemma 1", "text": "With a composite bond, we know that with a probability greater than 1 \u2212 \u2212 \u2212 \u2212 n \u00b2, for all k \u2212 n \u00b2, for all k \u2212 n \u00b2, for all k \u2212 n \u00b2, for all k \u2212 n \u00b2, for all k \u2212 n \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p, for all p \u00b2, for all p \u00b2, for all p, for all p \u00b2, for all p \u2212 p, for all p, for all p, for all p, p, for all \u2212 n \u00b2, for all p, for all p, p, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p \u00b2, for all p, for all p \u00b2, for all p, for all p, for all p \u00b2, for all p, for all p, for all p, for all p \u00b2, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all p, for all"}], "references": [{"title": "Minimax Policies for Adversarial and Stochastic Bandits", "author": ["Audibert", "Jean-Yves", "Bubeck", "S\u00e9bastien"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Tuning Bandit Algorithms in Stochastic Environments", "author": ["Audibert", "Jean-Yves", "Munos", "R\u00e9mi", "Szepesv\u00e1ri", "Csaba"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Audibert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2007}, {"title": "Best arm identification in multi-armed bandits", "author": ["Audibert", "Jean-Yves", "Bubeck", "S\u00e9bastien", "Munos", "R\u00e9mi"], "venue": "Conference on Learning Theory,", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "Online Stochastic Optimization under Correlated Bandit Feedback", "author": ["Azar", "Mohammad Gheshlaghi", "Lazaric", "Alessandro", "Brunskill", "Emma"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Azar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2014}, {"title": "Bandit problems with infinitely many arms", "author": ["Berry", "Donald A", "Chen", "Robert W", "Zame", "Alan", "Heath", "David C", "Shepp", "Larry A"], "venue": "Annals of Statistics,", "citeRegEx": "Berry et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Berry et al\\.", "year": 1997}, {"title": "Two-Target Algorithms for Infinite-Armed Bandits with Bernoulli Rewards", "author": ["Bonald", "Thomas", "Prouti\u00e8re", "Alexandre"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Bonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bonald et al\\.", "year": 2013}, {"title": "Adaptive and minimax optimal estimation of the tail coefficient", "author": ["Carpentier", "Alexandra", "Kim", "Arlene K. H"], "venue": "Statistica Sinica,", "citeRegEx": "Carpentier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Carpentier et al\\.", "year": 2014}, {"title": "Simple regret for infinitely many armed bandits", "author": ["Carpentier", "Alexandra", "Valko", "Michal"], "venue": "ArXiv e-prints,", "citeRegEx": "Carpentier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Carpentier et al\\.", "year": 2015}, {"title": "Challenging the empirical mean and empirical variance: a deviation study", "author": ["Catoni", "Olivier"], "venue": "In Annales de l\u2019Institut Henri Poincare\u0301, Probabilite\u0301s et Statistiques,", "citeRegEx": "Catoni and Olivier.,? \\Q2012\\E", "shortCiteRegEx": "Catoni and Olivier.", "year": 2012}, {"title": "Stochastic Linear Optimization under Bandit Feedback", "author": ["Dani", "Varsha", "Hayes", "Thomas P", "Kakade", "Sham M"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Extreme Value Theory: An Introduction", "author": ["de Haan", "Laurens", "Ferreira", "Ana"], "venue": "Springer Series in Operations Research and Financial Engineering. Springer,", "citeRegEx": "Haan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Haan et al\\.", "year": 2006}, {"title": "Action elimination and stopping conditions for the multiarmed bandit and reinforcement learning problems", "author": ["Even-Dar", "Eyal", "Mannor", "Shie", "Mansour", "Yishay"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2006}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Lazaric", "Alessandro"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "Feature Selection and Dimensionality Reduction in Genomics and Proteomics. In Berrar, Dubitzky, and Granzow (eds.), Fundamentals of Data Mining in Genomics and Proteomics", "author": ["Hauskrecht", "Milos", "Pelikan", "Richard", "Valko", "Michal", "Lyons-Weiler", "James"], "venue": null, "citeRegEx": "Hauskrecht et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hauskrecht et al\\.", "year": 2006}, {"title": "A Simple General Approach to Inference About the Tail of a Distribution", "author": ["Hill", "Bruce M"], "venue": "The Annals of Statistics,", "citeRegEx": "Hill and M.,? \\Q1975\\E", "shortCiteRegEx": "Hill and M.", "year": 1975}, {"title": "lil\u2019UCB: An Optimal Exploration Algorithm for Multi-Armed Bandits", "author": ["Jamieson", "Kevin", "Malloy", "Matthew", "Nowak", "Robert", "Bubeck", "S\u00e9bastien"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "PAC subset selection in stochastic multi-armed bandits", "author": ["Kalyanakrishnan", "Shivaram", "Tewari", "Ambuj", "Auer", "Peter", "Stone"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Kalyanakrishnan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalyanakrishnan et al\\.", "year": 2012}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Karnin", "Zohar", "Koren", "Tomer", "Somekh", "Oren"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "Information complexity in bandit subset selection", "author": ["Kaufmann", "Emilie", "Kalyanakrishnan", "Shivaram"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Kaufmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2013}, {"title": "Multi-armed bandit problems in metric spaces", "author": ["Kleinberg", "Robert", "Slivkins", "Alexander", "Upfal", "Eli"], "venue": "In 40th ACM Symposium on Theory Of Computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning", "author": ["Munos", "R\u00e9mi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Munos and R\u00e9mi.,? \\Q2014\\E", "shortCiteRegEx": "Munos and R\u00e9mi.", "year": 2014}, {"title": "Statistical Inference Using Extreme Order Statistics", "author": ["Pickands", "James III"], "venue": "The Annals of Statistics,", "citeRegEx": "Pickands and III.,? \\Q1975\\E", "shortCiteRegEx": "Pickands and III.", "year": 1975}, {"title": "Algorithms for Infinitely Many-Armed Bandits", "author": ["Wang", "Yizao", "Audibert", "Jean-Yves", "Munos", "R\u00e9mi"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013).", "startOffset": 118, "endOffset": 183}, {"referenceID": 22, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013).", "startOffset": 118, "endOffset": 183}, {"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Inevitably, the sheer amount of possible actions makes it impossible to try each of them even once. Such a setting is practically relevant for cases where one faces a finite, but extremely large number of actions. This setting was first formalized by Berry et al. (1997) as follows.", "startOffset": 119, "endOffset": 456}, {"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Inevitably, the sheer amount of possible actions makes it impossible to try each of them even once. Such a setting is practically relevant for cases where one faces a finite, but extremely large number of actions. This setting was first formalized by Berry et al. (1997) as follows. At each time t, the learner can either sample an arm (a distribution) that has been already observed in the past, or sample a new arm, whose mean \u03bc is sampled from the mean reservoir distribution L. The additional challenges of the infinitely many armed bandits with respect to the multi-armed bandits come from two sources. First, we need to find a good arm among the sampled ones. Second, we need to sample (at least once) enough arms in order to have (at least once) a reasonably good one. These two difficulties ask for a while which we call the arm selection tradeoff. It is different from the known exploration/exploitation tradeoff and more linked to model selection principles: On one hand, we want to sample only from a small subsample of arms so that we can decide, with enough accuracy, which one is the best one among them. On the other hand, we want to sample as many arms as possible in order to have a higher chance to sample a good arm at least once. This tradeoff makes the problem of infinitely many armed bandits significantly different from the classical bandit problem. Berry et al. (1997) provide asymptotic, minimax-optimal (up to a log n factor) bounds for the average cumulative regret, defined as the difference between n times the highest possible value \u03bc\u0304\u2217 of the mean reservoir distribution and the mean of the sum of all samples that the learner collects.", "startOffset": 119, "endOffset": 1579}, {"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Inevitably, the sheer amount of possible actions makes it impossible to try each of them even once. Such a setting is practically relevant for cases where one faces a finite, but extremely large number of actions. This setting was first formalized by Berry et al. (1997) as follows. At each time t, the learner can either sample an arm (a distribution) that has been already observed in the past, or sample a new arm, whose mean \u03bc is sampled from the mean reservoir distribution L. The additional challenges of the infinitely many armed bandits with respect to the multi-armed bandits come from two sources. First, we need to find a good arm among the sampled ones. Second, we need to sample (at least once) enough arms in order to have (at least once) a reasonably good one. These two difficulties ask for a while which we call the arm selection tradeoff. It is different from the known exploration/exploitation tradeoff and more linked to model selection principles: On one hand, we want to sample only from a small subsample of arms so that we can decide, with enough accuracy, which one is the best one among them. On the other hand, we want to sample as many arms as possible in order to have a higher chance to sample a good arm at least once. This tradeoff makes the problem of infinitely many armed bandits significantly different from the classical bandit problem. Berry et al. (1997) provide asymptotic, minimax-optimal (up to a log n factor) bounds for the average cumulative regret, defined as the difference between n times the highest possible value \u03bc\u0304\u2217 of the mean reservoir distribution and the mean of the sum of all samples that the learner collects. A follow-up on this result was the work of Wang et al. (2008), providing algorithms with finite-time regret bounds and the work of Bonald & Prouti\u00e8re (2013), giving an algorithm that is optimal with exact constants in a strictly more specific setting.", "startOffset": 119, "endOffset": 1916}, {"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Inevitably, the sheer amount of possible actions makes it impossible to try each of them even once. Such a setting is practically relevant for cases where one faces a finite, but extremely large number of actions. This setting was first formalized by Berry et al. (1997) as follows. At each time t, the learner can either sample an arm (a distribution) that has been already observed in the past, or sample a new arm, whose mean \u03bc is sampled from the mean reservoir distribution L. The additional challenges of the infinitely many armed bandits with respect to the multi-armed bandits come from two sources. First, we need to find a good arm among the sampled ones. Second, we need to sample (at least once) enough arms in order to have (at least once) a reasonably good one. These two difficulties ask for a while which we call the arm selection tradeoff. It is different from the known exploration/exploitation tradeoff and more linked to model selection principles: On one hand, we want to sample only from a small subsample of arms so that we can decide, with enough accuracy, which one is the best one among them. On the other hand, we want to sample as many arms as possible in order to have a higher chance to sample a good arm at least once. This tradeoff makes the problem of infinitely many armed bandits significantly different from the classical bandit problem. Berry et al. (1997) provide asymptotic, minimax-optimal (up to a log n factor) bounds for the average cumulative regret, defined as the difference between n times the highest possible value \u03bc\u0304\u2217 of the mean reservoir distribution and the mean of the sum of all samples that the learner collects. A follow-up on this result was the work of Wang et al. (2008), providing algorithms with finite-time regret bounds and the work of Bonald & Prouti\u00e8re (2013), giving an algorithm that is optimal with exact constants in a strictly more specific setting.", "startOffset": 119, "endOffset": 2011}, {"referenceID": 4, "context": "Specifically, Berry et al. (1997) and Wang et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 4, "context": "Specifically, Berry et al. (1997) and Wang et al. (2008) assume that the mean reservoir distribution is such that, for a small \u03b5 > 0, locally around the best arm \u03bc\u0304\u2217, we have that P\u03bc\u223cL (\u03bc\u0304\u2217 \u2212 \u03bc \u2265 \u03b5) \u2248 \u03b5 , (1)", "startOffset": 14, "endOffset": 57}, {"referenceID": 11, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 2, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 16, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 17, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 12, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 15, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 13, "context": "An example where efficient strategies for minimizing the simple regret of an infinitely many armed bandit are relevant is the search of a good biomarker in biology, a single feature that performs best on average (Hauskrecht et al., 2006).", "startOffset": 212, "endOffset": 237}, {"referenceID": 4, "context": "This can be also applied to the prior work (Berry et al., 1997; Wang et al., 2008) where \u03b2 is also necessary for implementation and optimal bounds.", "startOffset": 43, "endOffset": 82}, {"referenceID": 22, "context": "This can be also applied to the prior work (Berry et al., 1997; Wang et al., 2008) where \u03b2 is also necessary for implementation and optimal bounds.", "startOffset": 43, "endOffset": 82}, {"referenceID": 9, "context": "One class of examples is fixed design such as linear bandits (Dani et al., 2008) other settings consider bandits in known or unknown metric space (Kleinberg et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 4, "context": "The main difference between our strategy and the cumulative strategies (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013) is in the number of arms sampled from the arm reservoir: For the simple regret, we need to sample more arms.", "startOffset": 71, "endOffset": 136}, {"referenceID": 22, "context": "The main difference between our strategy and the cumulative strategies (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013) is in the number of arms sampled from the arm reservoir: For the simple regret, we need to sample more arms.", "startOffset": 71, "endOffset": 136}, {"referenceID": 0, "context": "It is also interesting to compare SiRI with existing algorithms targeting the simple regret for finitely many arms, as the ones by Audibert et al. (2010). SiRI can be related to their UCB-E with a specific confidence term and a specific choice of the number of arms selected.", "startOffset": 131, "endOffset": 154}, {"referenceID": 4, "context": "Case of distributions on [0, 1] with \u03bc\u0304\u2217 = 1 The first extension concerns the specific setting, particularly highlighted by Bonald & Prouti\u00e8re (2013) but also presented by Berry et al. (1997) and Wang et al.", "startOffset": 172, "endOffset": 192}, {"referenceID": 4, "context": "Case of distributions on [0, 1] with \u03bc\u0304\u2217 = 1 The first extension concerns the specific setting, particularly highlighted by Bonald & Prouti\u00e8re (2013) but also presented by Berry et al. (1997) and Wang et al. (2008), where the domain of the distributions of the arms are included in [0, 1] and where \u03bc\u0304\u2217 = 1.", "startOffset": 172, "endOffset": 215}, {"referenceID": 4, "context": "Case of distributions on [0, 1] with \u03bc\u0304\u2217 = 1 The first extension concerns the specific setting, particularly highlighted by Bonald & Prouti\u00e8re (2013) but also presented by Berry et al. (1997) and Wang et al. (2008), where the domain of the distributions of the arms are included in [0, 1] and where \u03bc\u0304\u2217 = 1. In this case, the information theoretic complexity of the problem is smaller than the one of the general problem stated in Theorem 1. Specifically, the variance of the near-optimal arms is very small, i.e., in the order of \u03b5 for an \u03b5-optimal arm. This implies a better bound, in particular, that the parametric limitation of 1/ \u221a n can be circumvented. In order to prove it, the simplest way is to modify SiRI into Bernstein-SiRI, displayed in Algorithm 2. It is an Empirical Bernstein-modified SiRI algorithm that accommodates the situation of distributions of support included in [0, 1] with \u03bc\u0304\u2217 = 1. Note that in the general case, it would provide similar results as what is provided in Theorem 2. A similar idea was already introduced by Wang et al. (2008) in the infinitely many armed setting for cumulative regret.", "startOffset": 172, "endOffset": 1069}, {"referenceID": 4, "context": "Case of distributions on [0, 1] with \u03bc\u0304\u2217 = 1 The first extension concerns the specific setting, particularly highlighted by Bonald & Prouti\u00e8re (2013) but also presented by Berry et al. (1997) and Wang et al. (2008), where the domain of the distributions of the arms are included in [0, 1] and where \u03bc\u0304\u2217 = 1. In this case, the information theoretic complexity of the problem is smaller than the one of the general problem stated in Theorem 1. Specifically, the variance of the near-optimal arms is very small, i.e., in the order of \u03b5 for an \u03b5-optimal arm. This implies a better bound, in particular, that the parametric limitation of 1/ \u221a n can be circumvented. In order to prove it, the simplest way is to modify SiRI into Bernstein-SiRI, displayed in Algorithm 2. It is an Empirical Bernstein-modified SiRI algorithm that accommodates the situation of distributions of support included in [0, 1] with \u03bc\u0304\u2217 = 1. Note that in the general case, it would provide similar results as what is provided in Theorem 2. A similar idea was already introduced by Wang et al. (2008) in the infinitely many armed setting for cumulative regret. The idea is that the confidence term is more refined using the empirical variance and hence it will be very large for a near-optimal arm, thereby enhancing exploration. Plugging this term in the proof, conditioning on the event of high probability, such that \u03c3\u0302 k,t is close to the true variance, and using similar ideas as Wang et al. (2008), we can immediately deduce the following corollary.", "startOffset": 172, "endOffset": 1472}, {"referenceID": 22, "context": "The proof follows immediately from the proof of Theorem 2 using the empirical Bernstein bound as by Wang et al. (2008). Moreover, the lower bounds\u2019 rates follow directly from the two facts: 1) 1/n is clearly a lower bound, and therefore optimal for \u03b2 < 1, since it takes at least n samples of a Bernoulli arm that is constant times 1/n suboptimal, in order to discover that it is not optimal, and 2) n\u22121/\u03b2 can be trivially deduced from Theorem 11.", "startOffset": 100, "endOffset": 119}, {"referenceID": 4, "context": "Yet its knowledge is crucial for the implementation of SiRI, as well as for all the cumulative regret strategies described in (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013).", "startOffset": 126, "endOffset": 191}, {"referenceID": 22, "context": "Yet its knowledge is crucial for the implementation of SiRI, as well as for all the cumulative regret strategies described in (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013).", "startOffset": 126, "endOffset": 191}, {"referenceID": 4, "context": "Yet its knowledge is crucial for the implementation of SiRI, as well as for all the cumulative regret strategies described in (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Consequently, a very important question is whether it is possible to estimate it well enough to obtain good results, which we answer in the affirmative. An interesting remark is that Assumption 2 is actually related to assuming that the distribution function L is \u03b2 regularly varying in \u03bc\u0304\u2217. Therefore, \u03b2 is the tail index of the distribution function ofL and can be estimated with tools from extreme value theory (de Haan & Ferreira, 2006). Many estimators exist for estimating this tail index \u03b2, for instance, the popular Hill\u2019s estimate (Hill, 1975), but also Pickand\u2019s\u2019 estimate (Pickands, 1975) and others. However, our situation is slightly different from the one where the convergence of these estimators is proved, as the means of the arms are not directly observed. As a result, we propose another estimate, related to the estimate of Carpentier & Kim (2014), which accommodates our setting.", "startOffset": 127, "endOffset": 1061}, {"referenceID": 4, "context": "We would like to emphasize that \u03b2\u0304 estimate (6) of \u03b2 can be used to improve cumulative regret algorithms that need \u03b2, such as the ones by Berry et al. (1997) and Wang et al.", "startOffset": 138, "endOffset": 158}, {"referenceID": 4, "context": "We would like to emphasize that \u03b2\u0304 estimate (6) of \u03b2 can be used to improve cumulative regret algorithms that need \u03b2, such as the ones by Berry et al. (1997) and Wang et al. (2008). Similarly for these algorithms, one should spend a preliminary phase of N = \u221a n rounds to estimate \u03b2 and then run the algorithm of choice.", "startOffset": 138, "endOffset": 181}, {"referenceID": 22, "context": "For instance, consider the cumulative regret rate of UCB-F by Wang et al. (2008). If UCB-F uses our estimate of \u03b2 instead of the true \u03b2, it would still satisfy", "startOffset": 62, "endOffset": 81}, {"referenceID": 22, "context": "Second, Wang et al. (2008) propose a more refined way to deal with an unknown time horizon (UCB-AIR), that also directly applies to SiRI.", "startOffset": 8, "endOffset": 27}, {"referenceID": 22, "context": "First such comparator is UCB-F (Wang et al., 2008), an algorithm that optimizes cumulative regret for this setting.", "startOffset": 31, "endOffset": 50}, {"referenceID": 15, "context": "Second, we compare SiRI to lil\u2019UCB (Jamieson et al., 2014) designed for the best-arm identification in the fixed confidence setting.", "startOffset": 35, "endOffset": 58}, {"referenceID": 0, "context": "UCB-F is designed for fixed horizon of n evaluations and it is an extension of a version of UCB-V by Audibert et al. (2007). Second, we compare SiRI to lil\u2019UCB (Jamieson et al.", "startOffset": 101, "endOffset": 124}], "year": 2015, "abstractText": "We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter \u03b2 characterizing the distribution of the near-optimal arms. We prove that depending on \u03b2, our algorithm is minimax optimal either up to a multiplicative constant or up to a log(n) factor. We also provide extensions to several important cases: when \u03b2 is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.", "creator": "LaTeX with hyperref package"}}}