{"id": "1505.05114", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2015", "title": "Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems", "abstract": "We consider the fundamental problem of solving quadratic systems of equations in $n$ variables, where $y_i = |\\langle \\boldsymbol{a}_i, \\boldsymbol{x} \\rangle|^2$, $i = 1, \\ldots, m$ and $\\boldsymbol{x} \\in \\mathbb{R}^n$ is unknown. We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach. There are several key distinguishing features, most notably, a distinct objective functional and novel update rules, which operate in an adaptive fashion and drop terms bearing too much influence on the search direction. These careful selection rules provide a tighter initial guess, better descent directions, and thus enhanced practical performance. On the theoretical side, we prove that for certain unstructured models of quadratic systems, our algorithms return the correct solution in linear time, i.e. in time proportional to reading the data $\\{\\boldsymbol{a}_i\\}$ and $\\{y_i\\}$ as soon as the ratio $m/n$ between the number of equations and unknowns exceeds a fixed numerical constant. We extend the theory to deal with noisy systems in which we only have $y_i \\approx |\\langle \\boldsymbol{a}_i, \\boldsymbol{x} \\rangle|^2$ and prove that our algorithms achieve a statistical accuracy, which is nearly un-improvable. We complement our theoretical study with numerical examples showing that solving random quadratic systems is both computationally and statistically not much harder than solving linear systems of the same size---hence the title of this paper. For instance, we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size.", "histories": [["v1", "Tue, 19 May 2015 18:37:07 GMT  (3124kb,D)", "http://arxiv.org/abs/1505.05114v1", null], ["v2", "Tue, 22 Mar 2016 17:05:16 GMT  (3658kb,D)", "http://arxiv.org/abs/1505.05114v2", "accepted to Communications on Pure and Applied Mathematics (CPAM)"]], "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH", "authors": ["yuxin chen", "emmanuel j cand\u00e8s"], "accepted": true, "id": "1505.05114"}, "pdf": {"name": "1505.05114.pdf", "metadata": {"source": "CRF", "title": "Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems", "authors": ["Yuxin Chen", "Emmanuel J. Cand\u00e8s"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Problem formulation", "text": "Imagine if we had a set of m-quadratic equations containing the formyi = > FORD = > FORD = > FORD = > FORD = > FORD = > FORD = > FORD = > FORD = > FORD = > FORD = >, M, (1) where the data y = [yi] 1 \u2264 i \u2264 i \u2264 m and the design vectors ai \u00b7 Rn / Cn are known, whereas x Rn / Cn is unknown, the problem is that we hope to find a solution, if any, compatible with this non-linear system of equations, which is combinatorial in nature, as can alternatively be represented as the restoration of the missing signs of < ai, x > of magnitude-only observations."}, {"heading": "1.2 Nonconvex optimization", "text": "Within the framework of a stochastic noise model with independent samples, an initial impulse for solving the problem (3) is then the search for the maximum probability estimate (MLE), namely: minimizez \u2212 [2]. (5), where \"(z; yi) the log probability of a candidate solution z (6) is given. (5), where the logarithmic solution (2), whose logarithm is usually not identical, so that the problem of searching for an MLE NP is completely in general.To compensate for this computational intractability, several convex environments have been proposed that work particularly well when the design vectors are selected (8, 9, 12, 22, 23, 29, 38, 38), which is a basic idea for introducing such a strategy. (3), it is then the search for the maximum probability estimate (MLE)."}, {"heading": "1.3 This paper: Truncated Wirtinger Flow", "text": "In line with the spirit of WF, we propose a novel method called Truncated Wirtinger Flow (TWF), which records a more subtle gradient flow. Informal, TWF proceeds in two steps: 1. Initialization: Calculation of an initial conjecture z (0) using a spectral method applied to a subset of T0 of the observations. 2. Loop: for 0 \u2264 t < T, z (t + 1) = z (t) + \u00b5t m."}, {"heading": "1.4 Numerical surprises", "text": "To give readers a sense of the practical power of TWF, here are three illustrative numerical examples: \"It is impossible to restore the global sign - i.e., we cannot distinguish between x and x.\" \"There is no solution to the quadratic equations.\" (8) \"It is simply that in the real case there is a shift.\" (8) \"We will dist (x) / x x.\" (8) \"We will dist (x) / x.\" (9) \"We will be the relative orderer of an estimate x.\" (8) \"We assume that a Poisson log liquidity (6). (6)\" Standalone \"Matlab\" implementations of TWF are available at http: / / / web.stanford.edu / (see [40] for direct.WF liquidity)."}, {"heading": "1.5 Main results", "text": "The preceding numerical discoveries reveal promising features of the TWF in three aspects: (1) exponentially fast convergence; (2) exact recovery from the noise data with sample complexity O (n); (3) almost minimal square loss in the presence of noise. (11) To this end, we assume a tractable model in which the design vectors are usually independent of each other. (11) For concreteness, our results are based on the Poisson logarithon function'i (z): '(z). \"We begin with the performance guarantees of the TWF in the absence. (12), in which we will use\" i (z)."}, {"heading": "2 Algorithm: Truncated Wirtinger Flow", "text": "In this section, the two stages of the truncated Wirtinger flow are described in detail and in reverse order. At each stage, we begin with some algorithmic problems encountered by WF, which are then used to motivate and explain the basic principles of the TWF. Here and everywhere, we let A: Rn \u00b7 n 7 \u2192 Rm be the linear map, Rn \u00b7 n 7 \u2192 A (M): = {a > i May} 1 \u2264 i \u2264 and A be the design matrix A: = [a1, \u00b7, am] >."}, {"heading": "2.1 Truncated gradient stage", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,"}, {"heading": "2.2 Truncated spectral initialization", "text": "A natural alternative is the spectral method used in [11,32], which results in the calculation of the leading eigenvector of Y + 2xx >, whose leading eigenvector is exactly x with an eigenvalue of 3. Unfortunately, this spectral technique is a good starting point only if m & n log n, due to the fact that (a > i x) 2aia > i is heavily tailed, is a random quantity that does not have a generational moment 6 In the case of a complex evaluation, truncation is forced on the Host derivative, which refers to the Host derivative (z): = 2 yi \u2212 e.g."}, {"heading": "2.3 Choice of algorithmic parameters", "text": "There are two alternatives that work well in theory as well as in practice: 1. Fixed step variable. Let's take the step count for a constant \u00b5 > 0. As long as \u00b5 is not too large, our main results indicate that this strategy always works - even though the convergence rate depends on \u00b5. Under appropriate conditions, our theories hold for any given constant 0 < \u00b5 < 0.28.n: Signal dimension 1000 2000 3000 4000 5000 Re: Misconduct r0,60,70,80.91 Spectral method truncated: Signal dimension (105) 0.5 1 1.5 2 2.5 3 3.5 4Re: Misconduct r0,40,60,811,21.4 Spectral method truncated (a) (b) Another set of important algorithmic parameters for determination is the truncation threshold \u03b1z, & lbz, & lbz, slow misestimation and short-term misestimation (for traceability)."}, {"heading": "3 Why TWF works?", "text": "Before proceeding, it is best to develop an intuitive understanding of the TWF iterations. (> i >) We begin with a notation representing the (irretrievable) global phase. (> i) It is obvious that (\u2212 z) the (\u2212 z) -2 (n) -2 (n) -2 (n) -2 (n) -2 (n) n (\u2212 z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n (z) n n (n) n n n (z) n n n n (n) n (n) n (n) n (n) n (n) n (n) n) n (n) n (n) n n (n) n n (z) n (n) n n (n) n n (z) n (n) n (n) n n) n (z) n (n) n n (n) n) n (n) n n) n (n) n n n (z (n) n) n) n (n) n n n (z) n n n (n) n) n (n) n n n (n) n) n (n) n (n) n) n (z (n) z (n) z (n) z (n) z (n) z (n) z (n n) z (n) z (n) z (n) z (n n n) z (n n n n) z (n n) z (n (n) z (n) z (n n) z (n n n n) z (n n n) z (n n (n) z (n) z (n) z (n n n n) z (n n n n) z (n) z (n n n) z (n n n) z (n n n n n n n (n) z (n) z (n) z (n n n n n n n n) z (n n n n) z (z (z"}, {"heading": "4 Numerical experiments", "text": "In this section, we report on additional numerical results to verify the practicality of TWF. (42) This is a specific combination of parameters that satisfy our state (31). Unless otherwise stated, we perform 50 power iterations for initialization, assume a fixed step size of 0.2 when updating TWF iterations, and set the maximum number of iterations to T = 1000 for iterative refinement. The first series of experiments concerns the exact restoration of noise-free data, and generate a real signal x when updating TWF iterations, and set the maximum number of iterations that T = 1000 for iterative refinement stage.The first series of experiments concerns the exact restoration of noise-free data."}, {"heading": "5 Exact recovery from noiseless data", "text": "This section confirms the theoretical guarantees of the TWF in the absence of noise (i.e., theorem 1). We are separating the silent case mainly for educational reasons, since most of the steps move to the noisy case with minor modification. Analysis for the truncated spectral method follows arguments similar to those in [11, Section 7.8], which we move to Appendix C. In short, for each fixed constant it is sufficient to show that the TWF updating rule is locally contractive, as in the following Proposition 1 (Local error contradictions). Considering the noisless case (1), there are some universal constants 0 & lt. < < < < < < < <"}, {"heading": "5.2 Proof of the regularity condition", "text": "(2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3).).). (3). (3). (3). (3). (3). (3).). (3). (3). (3).). (3).). (3.).). (3.). (3.).). (3.). (3.). (3.).). (3.).). (3.)."}, {"heading": "6 Stability", "text": "s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s"}, {"heading": "7 Minimax lower bound", "text": "The aim of this section is to establish the Minimax constant, which is specified in Theorem 3. (...) We have the basic assumptions that are the general reduction of the schemes in [43, Section 2.2], which amounts to a finite collection of hypotheses that are minimally separated from each other. (...) We have the basic idea of adopting the general reduction scheme in [43, Section 2.2], which amounts to a finite collection of hypotheses. (...) We are collecting a useful result for the construction and analysis of such hypotheses. (...) We assume that the hypotheses are capable of creating and analyzing a sufficiently large, and m = a sufficiently large constant. (...) We assume that the hypotheses are capable of developing and analyzing such hypotheses. (...) We assume that the hypotheses are capable of (...)."}, {"heading": "8 Discussion", "text": "There are, however, a few enhancements that are worth highlighting. \u2022 General objective functions. For concreteness, we limit our analysis to the Poisson logarithm function, but the framework system we have designed is easily transferred to a broad class of (non-convex) objective functions. For example, all results remain if we replace the Poisson probability with the Gaussian probability; that is, the polynomial function we have designed is easily transferable to a broad class of (non-convex) 2."}, {"heading": "A Proofs for Section 5", "text": "First, we note that (a > > i > i > i > i > i > i > i > i > i (a > i x) 2 = (2a > i z) i (a > i) i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z z\" z \"z\" z \"z\" z \"z z\" z \"z\" z \"z z z\" z \"z z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z \"z\" z"}, {"heading": "B Proofs for Section 7", "text": "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "D Local error contraction with backtracking line search", "text": "In this section, we review the effectiveness of a traceability strategy by showing the local error contraction. To keep it short, we only sketch the evidence for the silent case, but the evidence extends to the silent case without much difficulty. Nor do we aim for an optimized constant. To concretize, we quote the following statement: The claim in Proposition 1 remains valid if it extends to case 6, and does so without much difficulty."}, {"heading": "Acknowledgements", "text": "E. C. is partially supported by NSF grant CCF-0963835 and by the Math + X Award of the Simons Foundation. Y. C. is supported by the same grant of NSF. We thank Carlos Sing-Long and Weijie Su for helpful comments on an early version of the manuscript. E. C. is grateful to Xiaodong Li and Mahdi Soltanolkotabi for many discussions about Wirtinger rivers."}], "references": [{"title": "Phase retrieval with polarization", "author": ["B. Alexeev", "A.S. Bandeira", "M. Fickus", "D.G. Mixon"], "venue": "SIAM Journal on Imaging Sciences, 7(1):35\u201366", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple", "author": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"], "venue": "efficient, and neural algorithms for sparse coding. arXiv:1503.00778", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["S. Balakrishnan", "M.J. Wainwright", "B. Yu"], "venue": "arXiv preprint arXiv:1408.2156", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Painless reconstruction from magnitudes of frame coefficients", "author": ["R. Balan", "B. Bodmann", "P. Casazza", "D. Edidin"], "venue": "Journal of Fourier Analysis and Applications, 15(4):488\u2013501", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Saving phase: Injectivity and stability for phase retrieval", "author": ["A.S. Bandeira", "J. Cahill", "D.G. Mixon", "A.A. Nelson"], "venue": "Applied and Computational Harmonic Analysis, 37(1):106\u2013125", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Multireference alignment using semidefinite programming", "author": ["A.S. Bandeira", "M. Charikar", "A. Singer", "A. Zhu"], "venue": "Conference on Innovations in theoretical computer science, pages 459\u2013470", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Lectures on modern convex optimization", "author": ["A. Ben-Tal", "A. Nemirovski"], "venue": "volume 2. SIAM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "ROP: Matrix recovery via rank-one projections", "author": ["T. Cai", "A. Zhang"], "venue": "The Annals of Statistics, 43(1):102\u2013138", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Solving quadratic equations via PhaseLift when there are about as many equations as unknowns", "author": ["E.J. Cand\u00e8s", "X. Li"], "venue": "Foundations of Computational Mathematics, 14(5):1017\u20131026", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval from coded diffraction patterns", "author": ["E.J. Cand\u00e8s", "X. Li", "M. Soltanolkotabi"], "venue": "to appear in Applied and Computational Harmonic Analysis", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval via Wirtinger flow: Theory and algorithms", "author": ["E.J. Cand\u00e8s", "X. Li", "M. Soltanolkotabi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming", "author": ["E.J. Cand\u00e8s", "T. Strohmer", "V. Voroninski"], "venue": "Communications on Pure and Applied Mathematics, 66(8):1017\u20131026", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Supplemental materials for: \u201csolving random quadratic systems of equations is nearly as easy as solving linear systems", "author": ["Y. Chen", "E.J. Cand\u00e8s"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Exact and stable covariance estimation from quadratic sampling via convex programming", "author": ["Y. Chen", "Y. Chi", "A.J. Goldsmith"], "venue": "to appear, IEEE Transactions on Information Theory", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Near-optimal joint optimal matching via convex relaxation", "author": ["Y. Chen", "L.J. Guibas", "Q. Huang"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "A convex formulation for mixed regression with two components: Minimax optimal rates", "author": ["Yudong Chen", "Xinyang Yi", "Constantine Caramanis"], "venue": "In Conf. on Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Stable optimizationless recovery from phaseless linear measurements", "author": ["L. Demanet", "P. Hand"], "venue": "Journal of Fourier Analysis and Applications, 20(1):199\u2013221", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval: Stability and recovery guarantees", "author": ["Y.C. Eldar", "S. Mendelson"], "venue": "Applied and Computational Harmonic Analysis, 36(3):473\u2013494", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval by iterated projections", "author": ["V. Elser"], "venue": "JOSA. A, 20(1):40\u201355", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Phase retrieval algorithms: a comparison", "author": ["J.R. Fienup"], "venue": "Applied optics, 21:2758\u20132769", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1982}, {"title": "A practical algorithm for the determination of phase from image and diffraction plane pictures", "author": ["R.W. Gerchberg"], "venue": "Optik, 35:237", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1972}, {"title": "Improved recovery guarantees for phase retrieval from coded diffraction patterns", "author": ["D. Gross", "F. Krahmer", "R. Kueng"], "venue": "arXiv preprint arXiv:1402.6286", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "A partial derandomization of phaselift using spherical designs", "author": ["D. Gross", "F. Krahmer", "R. Kueng"], "venue": "Journal of Fourier Analysis and Applications, 21(2):229\u2013266", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast matrix completion without the condition number", "author": ["Moritz Hardt", "Mary Wootters"], "venue": "Conference on Learning Theory, pages", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Consistent shape maps via semidefinite programming", "author": ["Q. Huang", "L. Guibas"], "venue": "Computer Graphics Forum, 32(5):177\u2013186", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Recovery of sparse 1-D signals from the magnitudes of their Fourier transform", "author": ["K. Jaganathan", "S. Oymak", "B. Hassibi"], "venue": "IEEE ISIT, pages 1473\u20131477", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "ACM symposium on Theory of computing, pages 665\u2013674", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Sparse signal recovery from quadratic measurements via convex programming", "author": ["X. Li", "V. Voroninski"], "venue": "SIAM Journal on Mathematical Analysis, 45(5):3019\u20133033", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Alternating projection", "author": ["S. Marchesin", "Y. Tu", "H. Wu"], "venue": "ptychographic imaging and phase synchronization. arXiv:1402.0550", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Probability and computing", "author": ["M. Mitzenmacher", "E. Upfal"], "venue": "Cambridge University Press", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Phase retrieval using alternating minimization", "author": ["P. Netrapalli", "P. Jain", "S. Sanghavi"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Non-convex robust PCA", "author": ["P. Netrapalli", "U. Niranjan", "S. Sanghavi", "A. Anandkumar", "P. Jain"], "venue": "Advances in Neural Information Processing Systems, pages 1107\u20131115", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Numerical Optimization (2nd edition)", "author": ["J. Nocedal", "S.J. Wright"], "venue": "Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressive phase retrieval via generalized approximate message passing", "author": ["P. Schniter", "S. Rangan"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "GESPAR: Efficient phase retrieval of sparse signals", "author": ["Y. Shechtman", "A. Beck", "Y.C. Eldar"], "venue": "IEEE Transactions on Signal Processing, 62(4):928\u2013938", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval with application to optical imaging", "author": ["Y. Shechtman", "Y.C. Eldar", "O. Cohen", "H.N. Chapman", "J. Miao", "M. Segev"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Sparsity based sub-wavelength imaging with partially incoherent light via quadratic compressed sensing", "author": ["Y. Shechtman", "Y.C. Eldar", "A. Szameit", "M. Segev"], "venue": "Optics express, 19(16)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Algorithms and Theory for Clustering and Nonconvex Quadratic Programming", "author": ["M. Soltanolkotabi"], "venue": "PhD thesis, Stanford University", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Guaranteed matrix completion via non-convex factorization", "author": ["R. Sun", "Z. Luo"], "venue": "arXiv:1411.8003", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Numerical linear algebra", "author": ["L.N. Trefethen", "D. Bau III"], "venue": "volume 50. SIAM", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1997}, {"title": "Introduction to nonparametric estimation", "author": ["A.B. Tsybakov", "V. Zaiats"], "venue": "volume 11. Springer", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Compressed Sensing, Theory and Applications, pages 210 \u2013 268", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["I. Waldspurger"], "venue": "d\u2019Aspremont, and S. Mallat. Phase recovery, maxcut and complex semidefinite programming. Mathematical Programming, 149(1-2):47\u201381", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach [11].", "startOffset": 184, "endOffset": 188}, {"referenceID": 6, "context": "However simple this formulation may seem, even checking whether a solution to (2) exists or not is known to be NP complete [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 19, "context": "Moving from combinatorial optimization to the physical sciences, one application of paramount importance is the phase retrieval [20, 21] problem, which permeates through a wide spectrum of techniques including X-ray crystallography, diffraction imaging, and microscopy.", "startOffset": 128, "endOffset": 136}, {"referenceID": 20, "context": "Moving from combinatorial optimization to the physical sciences, one application of paramount importance is the phase retrieval [20, 21] problem, which permeates through a wide spectrum of techniques including X-ray crystallography, diffraction imaging, and microscopy.", "startOffset": 128, "endOffset": 136}, {"referenceID": 36, "context": "We refer to [37] for in-depth reviews of this subject.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 8, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 11, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 13, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 15, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 16, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 21, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 22, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 25, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 28, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 37, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 43, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 10, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 18, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 20, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 29, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 31, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 34, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 35, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "One promising approach along this line is the recently proposed two-stage algorithm called Wirtinger Flow (WF) [11].", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "The main results of [11] demonstrate that WF is surprisingly accurate for both real-valued and complexvalued Gaussian sampling models.", "startOffset": 20, "endOffset": 24}, {"referenceID": 38, "context": "In the presence of Gaussian noise, WF is stable and converges to the MLE as shown in [39].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "we cannot distinguish x from \u2212x\u2014we will evaluate our solutions to the quadratic equations through the distance measure put forth in [11] representing the Euclidean distance modulo a global sign: for complex signals, dist (z,x) := min\u03c6:\u2208[0,2\u03c0) \u2016e\u2212j\u03c6z \u2212 x\u2016.", "startOffset": 132, "endOffset": 136}, {"referenceID": 33, "context": "Arguably the most popular method for solving large-scale least squares problems is the conjugate gradient (CG) method [34] applied to the normal equations.", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "We consider a type of measurements that falls under the category of coded diffraction patterns (CDP) [10] and set y = |FDx|, 1 \u2264 l \u2264 L.", "startOffset": 101, "endOffset": 105}, {"referenceID": 38, "context": "This phenomenon arises regardless of the SNR! For experiments with noisy complex-valued data and (untruncated) WF, please see [39].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "These outperform the provable guarantees of WF [11], which requires O(n log n) sample complexity and runs in O(mn2 log 1/ ) time.", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "With these in place, we take the step size in a far more liberal fashion\u2014which is bounded away from 0\u2014compared to a step size which is inversely propotional to n as explained in [11].", "startOffset": 178, "endOffset": 182}, {"referenceID": 12, "context": "Interested readers are referred to the supplemental materials [13] for the proof of the universal theory (i.", "startOffset": 62, "endOffset": 66}, {"referenceID": 38, "context": "[39] proves similar stability estimates using the WF approach under Gaussian noise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 203, "endOffset": 210}, {"referenceID": 20, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 203, "endOffset": 210}, {"referenceID": 18, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 233, "endOffset": 237}, {"referenceID": 31, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 264, "endOffset": 268}, {"referenceID": 34, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 310, "endOffset": 314}, {"referenceID": 35, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 379, "endOffset": 383}, {"referenceID": 31, "context": "While these paradigms enjoy favorable empirical behavior, most of them fall short of theoretical support, except for a version of alternating minimization (called AltMinPhase) [32] that requires fresh samples for each iteration.", "startOffset": 176, "endOffset": 180}, {"referenceID": 10, "context": "Interesting readers are referred to [11] for a comparison of these non-convex schemes, and [10] for a discussion of other alternative approaches (e.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Interesting readers are referred to [11] for a comparison of these non-convex schemes, and [10] for a discussion of other alternative approaches (e.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "[1, 4]) and performance lower bounds (e.", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[1, 4]) and performance lower bounds (e.", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[5, 18]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[5, 18]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "On the other hand, the family of two-stage nonconvex procedures\u2014spectral initialization followed by iterative refinement\u2014has proved efficient for other problems that involve latent variables, which leads to theoretical guarantees for general EM algorithms [3] and sparse coding schemes [2].", "startOffset": 256, "endOffset": 259}, {"referenceID": 1, "context": "On the other hand, the family of two-stage nonconvex procedures\u2014spectral initialization followed by iterative refinement\u2014has proved efficient for other problems that involve latent variables, which leads to theoretical guarantees for general EM algorithms [3] and sparse coding schemes [2].", "startOffset": 286, "endOffset": 289}, {"referenceID": 10, "context": "One natural alternative is the spectral method adopted in [11,32], which amounts to computing the leading eigenvector of \u1ef8 := 1 m \u2211m i=1 yiaia > i .", "startOffset": 58, "endOffset": 65}, {"referenceID": 31, "context": "One natural alternative is the spectral method adopted in [11,32], which amounts to computing the leading eigenvector of \u1ef8 := 1 m \u2211m i=1 yiaia > i .", "startOffset": 58, "endOffset": 65}, {"referenceID": 10, "context": "We start with a notation representing the (unrecoverable) global phase [11] for real-valued data", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "The observations (38) and (39) are reminiscent of a (local) regularity condition given in [11], which has been shown to be a fundamental criterion that dictates rapid convergence of iterative procedures (including WF and other gradient descent schemes).", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "For the sake of comparison, we also report the empirical performance of WF in all the above settings, where the step size is set to be the default choice of [11], that is, \u03bct = min{1\u2212 e\u2212t/330, 0.", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "When t \u2208 [0, 1], it has been shown in the proof of [12, Lemma 3.", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": "9 for all t \u2208 [0, 1], as illustrated in Fig.", "startOffset": 14, "endOffset": 20}, {"referenceID": 10, "context": "For instance, all results continue to hold if we replace the Poisson likelihood by the Gaussian likelihood; that is, the polynomial function \u2212\u2211mi=1(yi \u2212 |ai z|2)2 studied in [11].", "startOffset": 174, "endOffset": 178}, {"referenceID": 7, "context": "It is known that this problem can be efficiently solved by using more computational-intensive semidefinite programs [8,14].", "startOffset": 116, "endOffset": 122}, {"referenceID": 13, "context": "It is known that this problem can be efficiently solved by using more computational-intensive semidefinite programs [8,14].", "startOffset": 116, "endOffset": 122}, {"referenceID": 23, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 129, "endOffset": 142}, {"referenceID": 26, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 129, "endOffset": 142}, {"referenceID": 27, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 129, "endOffset": 142}, {"referenceID": 39, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 129, "endOffset": 142}, {"referenceID": 32, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 162, "endOffset": 166}, {"referenceID": 5, "context": "A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25].", "startOffset": 101, "endOffset": 112}, {"referenceID": 14, "context": "A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25].", "startOffset": 101, "endOffset": 112}, {"referenceID": 24, "context": "A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25].", "startOffset": 101, "endOffset": 112}, {"referenceID": 42, "context": "[44]) such that for any (h, z) with \u2016h\u2016 = \u2016z\u2016 = 1, there exists a pair h0, z0 \u2208 N satisfying \u2016h\u2212 h0\u2016 \u2264 and \u2016z \u2212 z0\u2016 \u2264 .", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We consider the fundamental problem of solving quadratic systems of equations in n variables, where yi = |\u3008ai,x\u3009|, i = 1, . . . ,m and x \u2208 R is unknown. We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach [11]. There are several key distinguishing features, most notably, a distinct objective functional and novel update rules, which operate in an adaptive fashion and drop terms bearing too much influence on the search direction. These careful selection rules provide a tighter initial guess, better descent directions, and thus enhanced practical performance. On the theoretical side, we prove that for certain unstructured models of quadratic systems, our algorithms return the correct solution in linear time, i.e. in time proportional to reading the data {ai} and {yi} as soon as the ratio m/n between the number of equations and unknowns exceeds a fixed numerical constant. We extend the theory to deal with noisy systems in which we only have yi \u2248 |\u3008ai,x\u3009| and prove that our algorithms achieve a statistical accuracy, which is nearly un-improvable. We complement our theoretical study with numerical examples showing that solving random quadratic systems is both computationally and statistically not much harder than solving linear systems of the same size\u2014hence the title of this paper. For instance, we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size.", "creator": "LaTeX with hyperref package"}}}