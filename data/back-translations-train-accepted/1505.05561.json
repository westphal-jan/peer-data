{"id": "1505.05561", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "Why Regularized Auto-Encoders learn Sparse Representation?", "abstract": "Although a number of auto-encoder models enforce sparsity explicitly in their learned representation while others don't, there has been little formal analysis on what encourages sparsity in these models in general. Therefore, our objective here is to formally study this general problem for regularized auto-encoders. We show that both regularization and activation function play an important role in encouraging sparsity. We provide sufficient conditions on both these criteria and show that multiple popular models-- like De-noising and Contractive auto-encoder-- and activations-- like Rectified Linear and Sigmoid-- satisfy these conditions; thus explaining sparsity in their learned representation. Our theoretical and empirical analysis together, throws light on the properties of regularization/activation that are conducive to sparsity. As a by-product of the insights gained from our analysis, we also propose a new activation function that overcomes the individual drawbacks of multiple existing activations (in terms of sparsity) and hence produces performance at par (or better) with the best performing activation for all auto-encoder models discussed.", "histories": [["v1", "Thu, 21 May 2015 00:10:46 GMT  (966kb,D)", "http://arxiv.org/abs/1505.05561v1", null], ["v2", "Fri, 29 May 2015 19:22:37 GMT  (967kb,D)", "http://arxiv.org/abs/1505.05561v2", "10 pages of content, 1 page of reference, 3 pages of supplementary. Minor changes to theorem 1 and more empirical results added"], ["v3", "Wed, 2 Mar 2016 15:29:29 GMT  (379kb,D)", "http://arxiv.org/abs/1505.05561v3", "8 pages of content, 1 page of reference, 4 pages of supplementary"], ["v4", "Mon, 23 May 2016 23:04:21 GMT  (1246kb,D)", "http://arxiv.org/abs/1505.05561v4", "8 pages of content, 1 page of reference, 4 pages of supplementary. ICML 2016"], ["v5", "Fri, 17 Jun 2016 23:01:20 GMT  (1835kb,D)", "http://arxiv.org/abs/1505.05561v5", "8 pages of content, 1 page of reference, 4 pages of supplementary. ICML 2016; bug fix in lemma 1"]], "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["devansh arpit", "yingbo zhou", "hung q ngo 0001", "venu govindaraju"], "accepted": true, "id": "1505.05561"}, "pdf": {"name": "1505.05561.pdf", "metadata": {"source": "CRF", "title": "Why Regularized Auto-Encoders learn Sparse Representation?", "authors": ["Devansh Arpit", "Yingbo Zhou", "Hung Ngo", "Venu Govindaraju"], "emails": ["devansha@buffalo.edu", "yingbozh@buffalo.edu", "hungngo@buffalo.edu", "govind@buffalo.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that the solution to problems that have arisen in the past is not a solution, but a solution that is capable of bringing about a solution."}, {"heading": "2 Auto-Encoders and Sparse Representation", "text": "It's not just the way in which the data is encrypted, but also the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted. It's the way in which the data is encrypted and the way in which the data is encrypted."}, {"heading": "2.2 Part II: Do existing Auto-Encoders learn Sparse Representation?", "text": "To complete the loop, we show that most of the popular AE targets have a regularization term similar to what we have proposed in episodes 1 and 2, and thus, in fact, learn sparse representation.2In other words: \u2202 2hj \u2202 a2j = \u03b4 (Wjx + bej), where \u03b4 (.) is the Dirac delta function. Although \u2202 2hj \u2202 a2j strictly speaking is not always negative, this value is zero everywhere except if the argument is exactly 0, in this case it is + \u221e."}, {"heading": "2.2.1 De-noising Auto-Encoder (DAE)", "text": "DAE [31] aims to minimize the reconstruction error between each sample x and the reconstructed vector using the corresponding corrupt version x. The corrupt version x is based on a conditional distribution p (x, fd (x, fe (x))))] (5), where p (x, i | x) indicates the conditional distribution of the x given. Since the above target is analytically intractable due to the corruption process, we take a second order Taylor's approximation of the DAE objective3 around the distribution means that \u00b5x = Ep (x, x) [x] [x] indicates the conditional distribution of the x given. Since the above target is analytically intractable, it is a second order Taylor's objective 3 around the distribution meaning that \u00b5x = Ep (x, x) [x] [x] [x] [x] to overcome this difficulty, theorem 2. Lass {W} be the parameter of a parameter not equal to an objective E = 3)."}, {"heading": "2.2.2 Contractive Auto-Encoder (CAE)", "text": "CAE [28] objectively is given by JCAE = JAE + \u03bbEx (7), where J (x) = \u2202 h \u2202 x denotes the Jacobic matrix and the goal is to minimize the sensitivity of the hidden representation to slight input alterations.Note 1. Let the parameters of a contractive auto encoder (CAE) be displayed with se (.) activation function, square loss or square entropy loss, and regulation coefficient \u03bb at any point in the training on data sampled from any distribution D. Let aj: = Wjx + bej so that hj = se (aj) corresponds to the sample x. Then JCAE = JAE + ecuEx m \u0445j = 1 (((KE) 2; Wj \u044522) (8) Thus, the CAE regulation also has a form identical to the present form (KE)."}, {"heading": "2.2.3 Marginalized De-noising Auto-Encoder (mDAE)", "text": "mDAE [4] objectively is indicated by: JmDAE = JAE + 12 Ex n \u2211 i = 1 \u03c32xi m \u2211 j = 1 \u2202 2 '\u2202 hj 2 (\u2202 hj \u2202 x \u0109i) 2 (9), where \u03c32xi denotes the corruption variance intended for the ith input dimension. mDAE authors proposed this algorithm with the primary aim of speeding up the formation of DAE by deriving an approximate form that eliminates the need to iterate over a large number of explicitly corrupted instances of each training program. Comment 2. Let the parameters of a marginalized de-noising auto-encoder (mDAE) with se (.) activation function, linear decryption, square loss and \u03c32xi = \u00b2 i-Rm-n represent how the parameters of a marginalized de-noising auto-encoder (mDAE) with se (key, key, key, decryption, and quantitative decryption {and adratic), quantitative decryption and loss."}, {"heading": "2.2.4 Sparse Auto-Encoder (SAE)", "text": "Scanty AEs are given by: JSAE = JAE + \u03bb m \u2211 j = 1 (\u03c1 / \u03c1j) + (1 \u2212 \u03c1) log (((1 \u2212 \u03c1) / (1 \u2212 \u03c1j))) (11), where \u03c1j = Ex [hj] and \u03c1 is the desired average activation (typically close to 0). Therefore, SAE requires an additional parameter (\u03c1) that must be specified in advance. In order for SAE to follow our paradigm, we set \u03c1 = 0 and thus adjust the value of \u03bb, an equilibrium between the final level of average sparseness and the reconstruction error would automatically be imposed. Thus, the SAE target JSAE = JAE \u2212 \u03bb m \u2211 j = 1 log (1 \u2212 \u03c1j) (if \u03c1 = 0) (12) Note for small values of \u0421j, log (1 \u2212 \u03c1j). Thus, the aforementioned goal bears a very close similargement to the sparse encoding (2 equilibrium, except that SC has a codification that is generally valid)."}, {"heading": "3 Improving Bias Gradient of ReLU with Rectified Softplus (ReS)", "text": "True disparity (large number of hard zeros) is generally desired in the feature representation for purposes such as robustness, separability, compactness and performance, as discussed in Section 2.1.1, while Maxout and Tanh do not meet the negative saturation characteristics (and therefore do not guarantee thrift), ReLU, Softplus and Sigmoid meet the required characteristics (Theorem 1) in general, thus promoting thrift. However, the latter three activations each have some individual disadvantages. While ReLU does not have a gradient that can lead to poor thrift for regulation in episode 2 (e.g. DAE, CAE, mDAE, mDAE), Softplus and Sigmoid do not have hard zeros.0 2 4 8 10Iteration 020406080Bi asG radi ent M eanReS (MNIST) ReLu (MNIST) ReLu (MNIST) ReS (CIFAR-10) Reu (C10-IFIF0) 0.0 MISE 1.0 / 0.0 MISE 1.0 / 0.0"}, {"heading": "4 Empirical Analysis and Observations", "text": "We use the following two sets of data for our experiments: 1. MNIST [17]: This is a set of 10 classes of handwritten digital images (binary rated), of which 50 000 images are provided as train images, 10 000 validation images and 10 000 test images. 2. CIFAR-10 [16]: It consists of 60000 32 x 32 real (continuously rated) colour images in 10 classes with 6000 images per class. There are 50 000 train images and 10 000 test images. For CIFAR-10 we randomly cut 50 000 fields size 8 x 8 (since CIFAR-10 has images from the real world) to train the auto encoders. Experimental Protocols: For all experiments we use RMS Prop [30] for objective optimisation, learning rate 0.001, pre-training epochs 100, stack size 50 and hidden units 1000 (for MNIST) and 500 (for CIFAR-10) 0.0."}, {"heading": "4.1 Behaviour of Sigmoid Activation", "text": "We say that a sigmoid unit should not be negative, which would mean that a unit would always lead to an increase in the unit if it is saturated, if hj \u2265 0.9. So a unit in the linear region, if 0.1 < hj \u2264 0.9. We are conducting two types of experiments, both of which confirm our argument in Section 2.1.1 that AEs only use the linear range of sigmoid activation, and thus verify SAE values, and thus make full use of the verification of SAE values. (We remember that corollarity 1 always applies to sigmoid; justify the thrift for SAE-2) Ex [1 \u2212 2hj] against iterations: To apply corollarity 2, we must apply to sigmoid."}, {"heading": "4.3 Comparison of Auto-Encoder objectives", "text": "At this point, we are interested in analyzing how sensitive different AE targets are across different values of the regulation coefficient (\u03c32), activation functions and datasets, in the light of our analysis of the equivalent forms of AE targets, which we derive in Section 2.2. So we train all AE models without additional limitations (compared to Section 4.2) and chart sparseness vs. \u03c32. The diagrams are in Figure 4 for MNIST and CIFAR-10. We find that the sparseness trend for both CAE and mDAE is sensitive to the value of the regulation coefficient, while the value for DAE and SAE is stable and has a slightly decreasing trend. As already mentioned in Section 4.2, a plausible explanation for this observation from the perspective of the AE objective function is that the coefficient is effectively regulated and thus has a non-linear effect on the gradient."}, {"heading": "4.3.1 Why is DAE less sensitive compared to CAE/mDAE?", "text": "The surprising part of the above experiments is that DAE exhibits a stable saving trend (across different values of \u03c32) for ReLU, although DAE (similar to CAE, mDAE) exhibits a regularization trend that is given in episode 2. The fact that ReLU generates virtually no bias gradients from this form of regularization draws our attention to an interesting possibility: ReLU generates the positive bias gradient based on the first order bias in DAE. Remember that we marginalize the first term of order in DAE (during Taylor's expansion, see evidence in Section 2.2.1) while taking the expectation beyond all corrupted versions of a training sample. However, the mathematically equivalent goal of DAE achieved through this analytical marginalization is not what we optimize in practice. While optimizing with explicit corruption in a batch way and we do not actually get a first-order XE, we do not get a zero-order of the first-order."}, {"heading": "4.4 Effect of True Sparsity on Supervised Performance", "text": "As mentioned in Section 3, true sparse representation generally leads to better performance. On the same note, ReS overcomes the individual disadvantages of all three activations- ReLU, Softplus and Sigmoid, thereby promoting true sparseness. Although the focus of this paper is not on the supervised evaluation of AEs to test the effectiveness of true sparseness in the learned representations of AEs, we quantitatively analyze the robustness of the proposed activation function ReS against others. As our paper focuses on the analysis of unattended representation of AEs, we train various AE models and use them as feature extractors, i.e. we do not fine tune the weight vectors of AEs. Hyper parameters for training all AEs are selected based on the validation set, including the pre-training learning rate (candidate set {0.0001, 0.0005, 0.0005, 0.005, 0.01}) and the regulation efficiency."}, {"heading": "5 Conclusion", "text": "Inspired by the fact that neurons in the brain exhibit sparsely distributed behavior, we establish a link between auto-encoders and sparse representation. Our contribution is multi-layered, showing: a) AE regulations with positive encoding bias gradients promote low preactivation values (proposal 1); b) monotonous increasing convex activation functions with negative saturation at zero promote the rarity of such regulations (theorem 1) and that multiple existing activations satisfy them; c) existing AEs have regulations of the form proposed in episodes 1 and 2, justifying why they learn sparse representation. d) Based on these findings, we propose a new activation function (ReS in section 3) that overcomes the individual disadvantages of existing activations in terms of rarity, leading to better empirical performance."}, {"heading": "A1 Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A1 .1 Supplementary Proofs", "text": "Proxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"}], "references": [{"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Better mixing via deep representations", "author": ["Yoshua Bengio", "Gr\u00e9goire Mesnil", "Yann Dauphin", "Salah Rifai"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological Cybernetics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["Minmin Chen", "Kilian Q. Weinberger", "Fei Sha", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Adam Coates", "Andrew Y. Ng"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Theory of the backpropagation neural network", "author": ["Robert Hecht-Nielsen"], "venue": "In Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Receptive fields of single neurones in the cat\u2019s striate cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "The Journal of physiology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1959}, {"title": "Distributed representation of objects in the human ventral visual pathway", "author": ["Alumit Ishai", "Leslie Ungerleider", "Alex Martin", "Jennifer Schouten", "James Haxby"], "venue": "Proc Natl Acad Sci,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["Koray Kavukcuoglu", "Yann Lecun"], "venue": "Technical report, Courant Institute,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["Koray Kavukcuoglu", "Marc\u2019Aurelio Ranzato", "Yann LeCun"], "venue": "arXiv preprint arXiv:1010.3467,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Sparse deep belief net model for visual area v2", "author": ["Honglak Lee", "Chaitanya Ekanadham", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Why does the unsupervised pretraining encourage moderate-sparseness", "author": ["Jun Li", "Wei Luo", "Jian Yang", "Xiaotong Yuan"], "venue": "CoRR, abs/1312.5813,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "An introduction to computing with neural nets", "author": ["Richard P Lippmann"], "venue": "ASSP, IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1987}, {"title": "Zero-bias autoencoders and the benefits of co-adapting features", "author": ["Roland Memisevic", "Kishore Reddy Konda", "David Krueger"], "venue": "In ICLR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Sparse autoencoder", "author": ["Andrew Ng"], "venue": "Lecture notes,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by v1", "author": ["Bruno A. Olshausen", "David J. Fieldt"], "venue": "Vision Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Where do you know what you know? the representation of semantic knowledge in the human brain", "author": ["Karalyn Patterson", "Peter Nestor", "Timothy Rogers"], "venue": "Nature Rev. Neuroscience,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Higher order contractive auto-encoder", "author": ["Salah Rifai", "Gr\u00e9goire Mesnil", "Pascal Vincent", "Xavier Muller", "Yoshua Bengio", "Yann Dauphin", "Xavier Glorot"], "venue": "In ECML/PKDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Salah Rifai", "Pascal Vincent", "Xavier Muller", "Xavier Glorot", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Learning representations by back-propagating errors", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "venue": "Nature, pages 533\u2013536,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1986}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T Tieleman", "G Hinton"], "venue": "In COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Yi Ma"], "venue": "IEEEE TPAMI,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 82, "endOffset": 97}, {"referenceID": 20, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 82, "endOffset": 97}, {"referenceID": 21, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 82, "endOffset": 97}, {"referenceID": 7, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 82, "endOffset": 97}, {"referenceID": 16, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 138, "endOffset": 149}, {"referenceID": 12, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 138, "endOffset": 149}, {"referenceID": 4, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 138, "endOffset": 149}, {"referenceID": 10, "context": "A key commonality between these models is finding distributed representation [13, 11] for observed data although the former focuses on finding sparse distributed representation while the latter focuses on learning complex functions.", "startOffset": 77, "endOffset": 85}, {"referenceID": 0, "context": "When combined together, some of the main advantages of sparse distributed representation in the context of deep neural networks [1] has been shown to be information disentangling and manifold flattening [2], and better linear separability and representational power [7].", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "When combined together, some of the main advantages of sparse distributed representation in the context of deep neural networks [1] has been shown to be information disentangling and manifold flattening [2], and better linear separability and representational power [7].", "startOffset": 203, "endOffset": 206}, {"referenceID": 6, "context": "When combined together, some of the main advantages of sparse distributed representation in the context of deep neural networks [1] has been shown to be information disentangling and manifold flattening [2], and better linear separability and representational power [7].", "startOffset": 266, "endOffset": 269}, {"referenceID": 14, "context": "Due to the aforementioned biological connection between SR and NNs, a natural follow-up pursued by a number of researchers was to propose AE variants that encouraged sparsity in their learned representation [18, 14, 24, 21].", "startOffset": 207, "endOffset": 223}, {"referenceID": 11, "context": "Due to the aforementioned biological connection between SR and NNs, a natural follow-up pursued by a number of researchers was to propose AE variants that encouraged sparsity in their learned representation [18, 14, 24, 21].", "startOffset": 207, "endOffset": 223}, {"referenceID": 19, "context": "Due to the aforementioned biological connection between SR and NNs, a natural follow-up pursued by a number of researchers was to propose AE variants that encouraged sparsity in their learned representation [18, 14, 24, 21].", "startOffset": 207, "endOffset": 223}, {"referenceID": 17, "context": "On the other hand, there has also been work on empirically analyzing/suggesting the sparseness of hidden representations learned after pre-training with unsupervised models [22, 19, 23].", "startOffset": 173, "endOffset": 185}, {"referenceID": 15, "context": "On the other hand, there has also been work on empirically analyzing/suggesting the sparseness of hidden representations learned after pre-training with unsupervised models [22, 19, 23].", "startOffset": 173, "endOffset": 185}, {"referenceID": 18, "context": "On the other hand, there has also been work on empirically analyzing/suggesting the sparseness of hidden representations learned after pre-training with unsupervised models [22, 19, 23].", "startOffset": 173, "endOffset": 185}, {"referenceID": 26, "context": "Finally the second part shows multiple popular AE objectives including De-noising auto-encoder (DAE) [31] and Contractive auto-encoder (CAE) [28] indeed have the suggested form of regularization; thus explaining why existing AEs encourage sparsity in their latent representation.", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "Finally the second part shows multiple popular AE objectives including De-noising auto-encoder (DAE) [31] and Contractive auto-encoder (CAE) [28] indeed have the suggested form of regularization; thus explaining why existing AEs encourage sparsity in their latent representation.", "startOffset": 141, "endOffset": 145}, {"referenceID": 24, "context": "Auto-Encoders (AE) [29, 3] are a class of single hidden layer neural networks that minimize the data reconstruction error while learning an intermediate mapping between the input and output space.", "startOffset": 19, "endOffset": 26}, {"referenceID": 2, "context": "Auto-Encoders (AE) [29, 3] are a class of single hidden layer neural networks that minimize the data reconstruction error while learning an intermediate mapping between the input and output space.", "startOffset": 19, "endOffset": 26}, {"referenceID": 20, "context": "Learning a dictionary adapted to a set of training data such that the latent code is sparse is generally formulated as the following optimization problem [25]", "startOffset": 154, "endOffset": 158}, {"referenceID": 8, "context": "Finally, the upper bound on weight vectors\u2019 length can easily be guaranteed1 using Maxnorm Regularization or Weight Decay which are widely used tricks while training deep networks [10].", "startOffset": 180, "endOffset": 184}, {"referenceID": 6, "context": ", their value is less than a certain threshold (\u03b4min), in practice, a representation that is truly sparse (large number of hard zeros) usually yields better performance [7, 32, 33].", "startOffset": 169, "endOffset": 180}, {"referenceID": 27, "context": ", their value is less than a certain threshold (\u03b4min), in practice, a representation that is truly sparse (large number of hard zeros) usually yields better performance [7, 32, 33].", "startOffset": 169, "endOffset": 180}, {"referenceID": 26, "context": "DAE [31] aims at minimizing the reconstruction error between every sample x and the reconstructed vector using its corresponding corrupted version x\u0303.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "CAE [28] objective is given by", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "In addition, since the first order regularization term in Higher order CAE (CAE+H) [27] is same as CAE, this suggests that CAE+H objective should have similar properties in term of sparsity.", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "mDAE [4] objective is given by:", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": "CIFAR-10 [16]: It consists of 60000 32\u00d7 32 real world (continuous valued) color images in 10 classes, with 6000 images per class.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "Experimental Protocols: For all experiments, we use RMS Prop [30] for objective optimization, learning rate 0.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "We initialize the bias to zeros and use normalized initialization [6] for weight vectors.", "startOffset": 66, "endOffset": 69}], "year": 2015, "abstractText": "Although a number of auto-encoder models enforce sparsity explicitly in their learned representation while others don\u2019t, there has been little formal analysis on what encourages sparsity in these models in general. Therefore, our objective here is to formally study this general problem for regularized auto-encoders. We show that both regularization and activation function play an important role in encouraging sparsity. We provide sufficient conditions on both these criteria and show that multiple popular models\u2013 like De-noising and Contractive auto-encoder\u2013 and activations\u2013 like Rectified Linear and Sigmoid\u2013 satisfy these conditions; thus explaining sparsity in their learned representation. Our theoretical and empirical analysis together, throws light on the properties of regularization/activation that are conducive to sparsity. As a by-product of the insights gained from our analysis, we also propose a new activation function that overcomes the individual drawbacks of multiple existing activations (in terms of sparsity) and hence produces performance at par (or better) with the best performing activation for all auto-encoder models discussed.", "creator": "LaTeX with hyperref package"}}}