{"id": "1505.05612", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering", "abstract": "In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long-Short Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, a LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 120,000 images and 250,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset are evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human).", "histories": [["v1", "Thu, 21 May 2015 06:09:36 GMT  (1971kb,D)", "http://arxiv.org/abs/1505.05612v1", null], ["v2", "Fri, 30 Oct 2015 07:45:46 GMT  (1800kb,D)", "http://arxiv.org/abs/1505.05612v2", "Dataset released on the project page, seethis http URL; NIPS 2015 camera ready version"], ["v3", "Mon, 2 Nov 2015 21:12:15 GMT  (1800kb,D)", "http://arxiv.org/abs/1505.05612v3", "Dataset released on the project page, seethis http URL; NIPS 2015 camera ready version"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["haoyuan gao", "junhua mao", "jie zhou", "zhiheng huang", "lei wang", "wei xu"], "accepted": true, "id": "1505.05612"}, "pdf": {"name": "1505.05612.pdf", "metadata": {"source": "CRF", "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering", "authors": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "emails": ["gaohaoyuan@baidu.com,", "mjhustc@ucla.edu,", "zhoujie01@baidu.com", "huangzhiheng@baidu.com", "wanglei22@baidu.com", "wei.xu@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Related Work", "text": "In recent years, we have made significant progress by using deep neural network models in both computer vision and natural language. Convolutional Neural Network (CNN [20]) based methods are the most advanced in computer vision in various tasks such as object classification [17, 33, 17], recognition [10, 43] and segmentation [3]. For natural language, the recursive neural network (RNN [7, 27]) and the long-short term memory (LSTM [12]) are also widely used in machine translation [13, 34] and speech recognition [28]. The structure of our mQA model is inspired by the m-RNN model."}, {"heading": "3 The Multimodal QA (mQA) Model", "text": "We show the architecture of our mQA model in Figure 2. The model has four components: (I). a Long-Short Term Memory (LSTM [12]) to extract the semantic representation of a question, (II). a deep Convolutionary Neural Network (CNN) to extract the image representation, (III). an LSTM to extract the representation of the current word in the answer and its linguistic context, and (IV). a merging component that incorporates the information from the first three parts and generates the next word in the answer. These four components can be trained jointly 1. The details of the four model components are described in Section 3.1. The inputs of the model are a question and the reference image. The model is trained to generate the answer. The words in the question and answer are represented by a hot vector model (i.e. binary vectors with the length of the dictionary and the N sign, and we have the word that gives a very long one, not a zero, in the N word)."}, {"heading": "3.1 The Four Components of the mQA Model", "text": "The semantic meaning of the question is underlined by the first component of the \"Q\" model, which contains a 512-dimensional word that encompasses layer by layer and an LSTM layer with 400 memory cells. \"Embedded\" is used to capture the meaning of the word \"embedded\" in a dense semantic space; the meaning of the word \"embedded\" in another context is underlined by the word \"embedded\"; the meaning of the word \"embedded\" in another context is underlined by the fact that it is a bridge between the words and the words. \"LSTM\" and \"it is a recurrent dependence on the data.\""}, {"heading": "3.2 The Weight Sharing Strategy", "text": "As mentioned in section 2, our model assumes different LSTMs for the question and answer due to the different grammatical properties of the question and answer. However, the meaning of individual words should be the same for both question and answer. Therefore, we share the weight matrix between the text embedding layers of the first component and the third component. Furthermore, this weight matrix for the text embedding layers is transposed with the weight matrix in the fully connected Softmax layer. Intuitively, the function of the weight matrix in the text embedding layer is to encode a hot word into a dense word representation. The function of the weight matrix in the Softmax layer is to decode the dense word representation into a pseudo-one-word representation, which is the reverse operation of word embedding. This strategy will reduce almost half of the parameters in the model and show that they perform better in the visual caption and concept [25]."}, {"heading": "3.3 Training Details", "text": "The CNN we use is prepared for the ImageNet classification task [32]. This component is fixed during the QA training. We use a log probability loss defined on the word order of the answer. Minimizing this loss function corresponds to maximizing the likelihood of the model generating the basic truth answers in the training set. We jointly train the first, second and fourth components using a stochastic, decent method. The initial learning rate is 1 and we reduce it by a factor of 10 for each epoch of the data. We stop the training if the loss does not decrease within three epochs. To answer the Chinese question, we segment the sentences into several phrases. These phrases can be treated on an equal footing with the English words."}, {"heading": "4 The Freestyle Multilingual Image Question Answering (FM-IQA) Dataset", "text": "In Section 4.1 we describe the process of collecting the data and the method of monitoring the quality of the annotations. Some statistics and examples of the data set are given in Section 4.2. The data set will be published after publication of this paper."}, {"heading": "4.1 The Data Collection", "text": "Starting with the 123,287 images from the newly released MS COCO [21] Training and Validation, which were selected as the first set of images, the annotations are collected using Baidu's online annotation server 2, which is very similar to the Amazon Mechanical Turk System. To make the labeled question-and-answer pairs more diversified, annotators are free to ask all kinds of questions as long as these questions are related to the content of the image. The question should be answered through visual content and common sense (e.g., we do not expect to receive questions like \"What is the name of the person in the image?\"). Annotators must provide an answer to the question itself. On the one hand, the freedom we give the annotators is beneficial in order to obtain a freestyle, interesting and diversified question. On the other hand, annotators make it more difficult to control the quality of the annotations compared to a more detailed guide."}, {"heading": "4.2 The Statistics of the Dataset", "text": "Currently, there are 120,360 images with 250,569 question-answer pairs 3. Each image has at least two question-answer pairs as annotations. The average lengths of questions and answers are 7.38 and 3.82, respectively, according to Chinese words. Some sample images are shown in Figure 3. We randomly selected 1,000 question-answer pairs and associated images as a test set. The questions in this data set are diversified, which requires a variety of AI skills to answer them. They contain some relatively simple questions that can be understood by the image, e.g. the actions of objects (e.g. \"What does the boy with the green cap do?\"), the object class (e.g. \"Is there any person in the picture?\"), the relative positions and interactions between objects (e.g. \"Is the computer on the right or left side of the Lord?\"), and the attributes of the objects (e.g. \"What is the color of the frisbee?\"), the relative positions and interactions between the objects (e.g. \"Are we on the right side of the bus, the right side of the attributes that are the attributes of the right side of the questions?\"), the relative positions and interactions between the objects (e.g. \"Are we on the right side of the right side of the questions\" what are the attributes of the right side of the questions \")."}, {"heading": "5 Experiments", "text": "In recent work on visual answers to questions ([31, 23]), they test their method against data sets in which the answer to the question is a single word or a short phrase. In this setting, we have completed all annotations to Chinese questions and answers to the images. We are working to provide appropriate English question-and-answer pairs. Once completed, this data set can be used for some new and interesting tasks, such as visual machine translation. It is plausible to use automatic evaluation metrics that measure the similarity of each word, such as the Wu-Palmer similarity measure (WUPS) [40]. However, for our newly proposed data set, the answers in the data set are freestyle and can be complete sentences. In most cases, there are numerous choices of answers, all of which are correct. Possible alternatives are BLEU score [30], METEOR [18], CIDEr [38] or other metrics commonly used in the mapping."}, {"heading": "5.1 The Visual Turing Test", "text": "In this Visual Turing Test, a human judge is presented with an image, a question, and the answer to the question generated by the test model or by human commentators. He or she must determine from the answer whether the answer is given by a human (i.e., pass the test) or by a machine (i.e., fail the test).In practice, we use the images and questions from the test set of our FM-IQA dataset. We use our mQA model to generate the answer for each question. We also implement a basic model for answering the question without visual information. The structure of this basic model is similar to mQA except that we do not feed the image information extracted from CNN into the fixer layer. We call it blind QA. The answers generated by our mQA model, the blind QA model and the truth answer are mixed together, resulting in 3000 pairs of questions that are answered with the corresponding images."}, {"heading": "5.2 The Score of the Generated Answer", "text": "The Visual Turing Test gives only a rough evaluation of the answers generated. We also make a fine-grained evaluation with values of \"0,\" \"1\" or \"2.\" \"0\" and \"2\" mean that the answer is completely wrong or perfectly correct. \"1\" means that the answer is only partially correct (e.g. the general categories are correct, but the subcategories are wrong) and makes sense to the human judges. \"Human judges also rate an answer with\" 1, \"if the question is very difficult to answer, so even a person can make mistakes without carefully looking at the image. We show randomly selected images whose values are in Figure 4\" 1. \"Note that the human judges for this task are not necessarily the same people for the Visual Turing Test. The results are shown in Table 1. We show that among the answers that are not perfectly correct (i.e. the values are not 2), more than half of them are partially correct."}, {"heading": "6 Discussion", "text": "In this paper, we present the mQA model, which is able to give a sentence or phrase in response to a freestyle question for an image. To confirm the effectiveness of the method, we construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset with over 250,000 question-answer pairs. We evaluate our method using human judges using a real Turing test. It shows that 64.7% of the answers given by our mQA model are treated as answers provided by a human. The FM-IQA dataset can be used for other tasks, such as visual machine translation, where the visual information can serve as context information that helps eliminate the ambiguity of words in a sentence."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "arXiv preprint arXiv:1505.00468,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Vizwiz: nearly real-time answers to visual questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White"], "venue": "In ACM symposium on User interface software and technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4389,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt"], "venue": "arXiv preprint arXiv:1411.4952,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "PNAS, 112(12):3618\u20133623,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "The iapr tc-12 benchmark: A new evaluation resource for visual information systems", "author": ["M. Grubinger", "P. Clough", "H. M\u00fcller", "T. Deselaers"], "venue": "In International Workshop OntoImage,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In EMNLP, pages 1700\u20131709,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Fisher vectors derived from hybrid gaussian-laplacian mixture models for image annotation", "author": ["B. Klein", "G. Lev", "G. Sadeh", "L. Wolf"], "venue": "arXiv preprint arXiv:1411.7399,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgements", "author": ["A. Lavie", "A. Agarwal"], "venue": "In Workshop on Statistical Machine Translation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Simple image description generator via a linear phrase-based approach", "author": ["R. Lebret", "P.O. Pinheiro", "R. Collobert"], "venue": "arXiv preprint arXiv:1412.8419,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1405.0312,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "arXiv preprint arXiv:1505.01121,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "In ICLR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "arXiv preprint arXiv:1504.06692,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "NIPS DeepLearning Workshop,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Learning longer memory in recurrent neural networks", "author": ["T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In ACL,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "arXiv preprint arXiv:1505.02074,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Joint video and text parsing for understanding events and answering", "author": ["K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "S.-C. Zhu"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind, pages 433\u2013460,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1950}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "arXiv preprint arXiv:1411.5726,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "In ACL, pages 133\u2013138,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1994}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "In ACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Learning from weakly supervised data by the expectation loss svm (e-svm) algorithm", "author": ["J. Zhu", "J. Mao", "A.L. Yuille"], "venue": "In NIPS,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 14, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 13, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 37, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 5, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 7, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 3, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 18, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 15, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 39, "context": "In particular, many studies have made rapid progress on the task of image captioning [26, 15, 14, 39, 6, 8, 4, 19, 16, 41].", "startOffset": 85, "endOffset": 122}, {"referenceID": 16, "context": "deep Convolutional Neural Networks (CNN [17]), Recurrent Neural Network (RNN [7]) or LongShort Term Memory (LSTM [12])).", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "deep Convolutional Neural Networks (CNN [17]), Recurrent Neural Network (RNN [7]) or LongShort Term Memory (LSTM [12])).", "startOffset": 77, "endOffset": 80}, {"referenceID": 11, "context": "deep Convolutional Neural Networks (CNN [17]), Recurrent Neural Network (RNN [7]) or LongShort Term Memory (LSTM [12])).", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": ", [21, 42, 11]) play a crucial rule for this progress.", "startOffset": 2, "endOffset": 14}, {"referenceID": 40, "context": ", [21, 42, 11]) play a crucial rule for this progress.", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": ", [21, 42, 11]) play a crucial rule for this progress.", "startOffset": 2, "endOffset": 14}, {"referenceID": 33, "context": "The second component is a deep Convolutional Neural Network [35] that extracted the image representation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "We also adopt the transposed weight sharing scheme as proposed in [25], which allows the weight sharing between word embedding layer and the fully connected Softmax layer.", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "To train our method, we construct a large-scale Freestyle Multilingual Image Question Answering dataset (FM-IQA, see details in Section 4) based on the MS COCO dataset [21].", "startOffset": 168, "endOffset": 172}, {"referenceID": 35, "context": "We conduct a Visual Turing Test [37] using human judges.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "In the discussion, we analyze the failure cases of our model and show that combined with the m-RNN [24] model, our model can automatically ask a question about an image and answer that question.", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "For computer vision, methods based on Convolutional Neural Network (CNN [20]) achieve the state-of-the-art performance in various tasks, such as object classification [17, 33, 17], detection [10, 43] and segmentation [3].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "For computer vision, methods based on Convolutional Neural Network (CNN [20]) achieve the state-of-the-art performance in various tasks, such as object classification [17, 33, 17], detection [10, 43] and segmentation [3].", "startOffset": 167, "endOffset": 179}, {"referenceID": 31, "context": "For computer vision, methods based on Convolutional Neural Network (CNN [20]) achieve the state-of-the-art performance in various tasks, such as object classification [17, 33, 17], detection [10, 43] and segmentation [3].", "startOffset": 167, "endOffset": 179}, {"referenceID": 16, "context": "For computer vision, methods based on Convolutional Neural Network (CNN [20]) achieve the state-of-the-art performance in various tasks, such as object classification [17, 33, 17], detection [10, 43] and segmentation [3].", "startOffset": 167, "endOffset": 179}, {"referenceID": 9, "context": "For computer vision, methods based on Convolutional Neural Network (CNN [20]) achieve the state-of-the-art performance in various tasks, such as object classification [17, 33, 17], detection [10, 43] and segmentation [3].", "startOffset": 191, "endOffset": 199}, {"referenceID": 41, "context": "For computer vision, methods based on Convolutional Neural Network (CNN [20]) achieve the state-of-the-art performance in various tasks, such as object classification [17, 33, 17], detection [10, 43] and segmentation [3].", "startOffset": 191, "endOffset": 199}, {"referenceID": 2, "context": "For computer vision, methods based on Convolutional Neural Network (CNN [20]) achieve the state-of-the-art performance in various tasks, such as object classification [17, 33, 17], detection [10, 43] and segmentation [3].", "startOffset": 217, "endOffset": 220}, {"referenceID": 6, "context": "For natural language, the Recurrent Neural Network (RNN [7, 27]) and the Long-Short Term Memory (LSTM [12]) are also widely used in machine translation [13, 5, 34] and speech recognition [28].", "startOffset": 56, "endOffset": 63}, {"referenceID": 26, "context": "For natural language, the Recurrent Neural Network (RNN [7, 27]) and the Long-Short Term Memory (LSTM [12]) are also widely used in machine translation [13, 5, 34] and speech recognition [28].", "startOffset": 56, "endOffset": 63}, {"referenceID": 11, "context": "For natural language, the Recurrent Neural Network (RNN [7, 27]) and the Long-Short Term Memory (LSTM [12]) are also widely used in machine translation [13, 5, 34] and speech recognition [28].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "For natural language, the Recurrent Neural Network (RNN [7, 27]) and the Long-Short Term Memory (LSTM [12]) are also widely used in machine translation [13, 5, 34] and speech recognition [28].", "startOffset": 152, "endOffset": 163}, {"referenceID": 4, "context": "For natural language, the Recurrent Neural Network (RNN [7, 27]) and the Long-Short Term Memory (LSTM [12]) are also widely used in machine translation [13, 5, 34] and speech recognition [28].", "startOffset": 152, "endOffset": 163}, {"referenceID": 32, "context": "For natural language, the Recurrent Neural Network (RNN [7, 27]) and the Long-Short Term Memory (LSTM [12]) are also widely used in machine translation [13, 5, 34] and speech recognition [28].", "startOffset": 152, "endOffset": 163}, {"referenceID": 27, "context": "For natural language, the Recurrent Neural Network (RNN [7, 27]) and the Long-Short Term Memory (LSTM [12]) are also widely used in machine translation [13, 5, 34] and speech recognition [28].", "startOffset": 187, "endOffset": 191}, {"referenceID": 23, "context": "The structure of our mQA model is inspired by the m-RNN model [24] for the image captioning and image-sentence retrieval tasks.", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "There has been recent effort on the visual question answering task [9, 2, 22, 36].", "startOffset": 67, "endOffset": 81}, {"referenceID": 1, "context": "There has been recent effort on the visual question answering task [9, 2, 22, 36].", "startOffset": 67, "endOffset": 81}, {"referenceID": 21, "context": "There has been recent effort on the visual question answering task [9, 2, 22, 36].", "startOffset": 67, "endOffset": 81}, {"referenceID": 34, "context": "There has been recent effort on the visual question answering task [9, 2, 22, 36].", "startOffset": 67, "endOffset": 81}, {"referenceID": 8, "context": ", there are only 2591 and 1449 images for [9] and [22] respectively).", "startOffset": 42, "endOffset": 45}, {"referenceID": 21, "context": ", there are only 2591 and 1449 images for [9] and [22] respectively).", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "In addition, as in [25], this weight matrix is also shared, in a transposed manner, with the weight matrix in the Softmax layer.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "There are the concurrent and independent works on this topic: [1, 23, 31].", "startOffset": 62, "endOffset": 73}, {"referenceID": 22, "context": "There are the concurrent and independent works on this topic: [1, 23, 31].", "startOffset": 62, "endOffset": 73}, {"referenceID": 30, "context": "There are the concurrent and independent works on this topic: [1, 23, 31].", "startOffset": 62, "endOffset": 73}, {"referenceID": 0, "context": "[1] propose a large-scale dataset also based on MS COCO.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Because we use a different set of annotators and different requirements of the annotation, our dataset and the [1] can be complementary to each other, and lead to some interesting topics, such as dataset transferring for visual question answering.", "startOffset": 111, "endOffset": 114}, {"referenceID": 22, "context": "Both [23] and [31] use a model containing a single LSTM and a CNN.", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "Both [23] and [31] use a model containing a single LSTM and a CNN.", "startOffset": 14, "endOffset": 18}, {"referenceID": 30, "context": "They concatenate the question and answer (for [31], the answer is a single word.", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "[23] also prefer a single word as the answer), and then feed them to the LSTM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "For the dataset, [23] adopt the dataset proposed in [22], which is much smaller than our FM-IQA dataset.", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "For the dataset, [23] adopt the dataset proposed in [22], which is much smaller than our FM-IQA dataset.", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "[31] utilize the annotations in MS COCO and synthesize a dataset with four pre-defined types of questions (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "an Long-Short Term Memory (LSTM [12]) for extracting semantic representation of a question, (II).", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "LSTM [12] is a Recurrent Neural Network [7] that is designed for solving the gradient explosion or vanishing problem.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "LSTM [12] is a Recurrent Neural Network [7] that is designed for solving the gradient explosion or vanishing problem.", "startOffset": 40, "endOffset": 43}, {"referenceID": 22, "context": "Different from [23, 31], the image representation does not feed into this LSTM.", "startOffset": 15, "endOffset": 23}, {"referenceID": 30, "context": "Different from [23, 31], the image representation does not feed into this LSTM.", "startOffset": 15, "endOffset": 23}, {"referenceID": 33, "context": "In this paper, we use the GoogleNet [35].", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "Note that other CNN models, such as AlexNet [17] and VggNet [33], can also be used as the component in our model.", "startOffset": 44, "endOffset": 48}, {"referenceID": 31, "context": "Note that other CNN models, such as AlexNet [17] and VggNet [33], can also be used as the component in our model.", "startOffset": 60, "endOffset": 64}, {"referenceID": 22, "context": "In [23, 31], they concatenate the training question and answer, and use a single LSTM.", "startOffset": 3, "endOffset": 11}, {"referenceID": 30, "context": "In [23, 31], they concatenate the training question and answer, and use a single LSTM.", "startOffset": 3, "endOffset": 11}, {"referenceID": 24, "context": "This strategy allows the weight sharing between word embedding layer and the fully connected Softmax layer as introduced in [25] (see details in Section 3.", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "Similar to [25], we use the sigmoid function as the activation function of the three gates and adopt ReLU [29] as the non-linear function for the LSTM memory cells.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "Similar to [25], we use the sigmoid function as the activation function of the three gates and adopt ReLU [29] as the non-linear function for the LSTM memory cells.", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "for the word embedding layer, the fusing layer and the intermediate layer is the scaled hyperbolic tangent function [20]: g(x) = 1.", "startOffset": 116, "endOffset": 120}, {"referenceID": 24, "context": "This strategy will reduce nearly half of the parameters in the model and is shown to have better performance in image captioning and novel visual concept learning tasks [25].", "startOffset": 169, "endOffset": 173}, {"referenceID": 20, "context": "We start with the 123,287 images from the newly released MS COCO [21] training and validation set as the initial image set.", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "For the very recent works for visual question answering ([31, 23]), they test their method on the datasets where the answer of the question is a single word or a short phrase.", "startOffset": 57, "endOffset": 65}, {"referenceID": 22, "context": "For the very recent works for visual question answering ([31, 23]), they test their method on the datasets where the answer of the question is a single word or a short phrase.", "startOffset": 57, "endOffset": 65}, {"referenceID": 38, "context": "it is plausible to use automatic evaluation metrics that measure the single word similarity, such as Wu-Palmer similarity measure (WUPS) [40].", "startOffset": 137, "endOffset": 141}, {"referenceID": 29, "context": "The possible alternatives are BLEU score [30], METEOR [18], CIDEr [38] or other metrics that are widely used in the image captioning task [24].", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "The possible alternatives are BLEU score [30], METEOR [18], CIDEr [38] or other metrics that are widely used in the image captioning task [24].", "startOffset": 54, "endOffset": 58}, {"referenceID": 36, "context": "The possible alternatives are BLEU score [30], METEOR [18], CIDEr [38] or other metrics that are widely used in the image captioning task [24].", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "The possible alternatives are BLEU score [30], METEOR [18], CIDEr [38] or other metrics that are widely used in the image captioning task [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 24, "context": "We also modified the LSTM in the first component to the multimodal LSTM shown in [25].", "startOffset": 81, "endOffset": 85}], "year": 2015, "abstractText": "In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long-Short Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, a LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 120,000 images and 250,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset are evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human).", "creator": "LaTeX with hyperref package"}}}