{"id": "1505.05667", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network", "abstract": "In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.", "histories": [["v1", "Thu, 21 May 2015 10:23:10 GMT  (199kb,D)", "http://arxiv.org/abs/1505.05667v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["chenxi zhu", "xipeng qiu", "xinchi chen", "xuanjing huang"], "accepted": true, "id": "1505.05667"}, "pdf": {"name": "1505.05667.pdf", "metadata": {"source": "CRF", "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network", "authors": ["Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang"], "emails": ["czhu13@fudan.edu.cn", "xpqiu@fudan.edu.cn", "xinchichen13@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In practice, however, the number of features is so large that they are too complex for practical use and susceptibility to higher-level tasks. However, recently, many methods have been proposed to learn different distributed representations at both levels of syntax and semantics, and these distributed representations have been widely applied to many different authorities. (NLP) Tasks such as syntax and syntax have recently been proposed in many ways to capture both syntax and semantic levels, and these distributed representations have been widely applied to the respective authority. (Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al.)."}, {"heading": "2 Recursive Neural Network", "text": "In this section, we briefly describe the recursive neural network architecture of (Socher et al., 2013a).The idea of recursive neural networks (RNN) for processing natural language (NLP) is to learn a deep learning model that can be applied to phrases and sentences that have a grammatical structure (Pollack, 1990; Socher et al., 2013c).RNN can also be considered a general structure to the model theorem. At each node in the tree, the contexts on the left and right sides of the node are combined by a classical layer.The weights of the layer are divided across all nodes in the tree, and the layer calculated at the top node gives a representation for the entire theorem. Following the binary tree structure, RNN can assign a fixed vector to each word that assigns a fixed vector on the leaves of the tree, and generate combined word and phrase pairs to nodes."}, {"heading": "3 Recursive Convolutional Neural Network", "text": "The grammar of dependence is a widely used syntactic structure that directly reflects the relationships between the words in a sentence. In a dependency tree, all nodes are terminals (words) and each node can have more than two children. Therefore, the standard RNN architecture is not suitable for the grammar of dependence, as it is based on the binary tree. In this section, we propose a more general architecture, the so-called recursive Convolutionary Neural Network (RCNN), which takes up the idea of the Convolutionary Neural Network (CNN) and can deal with the k tree."}, {"heading": "3.1 RCNN Unit", "text": "\"We have to deal with the question of what we should do.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"We..\" We. \"\" We. \"We.\" We. \"We.\" We.. \"We..\" We.. \"We.\" \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"\" We. \"\" We. \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"We.\""}, {"heading": "4 Parsing", "text": "To measure the plausibility of a subtree rooted in h in the dependency tree, we use a linear layer neural network to calculate the value of its RCNN unit. For the constituent analysis, the representation of a non-terminal node depends only on its two children. The combination is relatively simple and its correctness can be measured with the final representation of the non-terminal node (Socher et al., 2013a). However, for the analysis of the dependence, all combinations of the head h and its children ci (0 < i \u2264 K) are important to measure the correctness of the subtree. Therefore, our score function s (h) is calculated on all hidden layers zi (0 < i \u2264 K): s (h) = K \u0445 i = 1 v (h, ci) \u00b7 zi, (6) where v (h, ci), i \u00d7 1 is the score vector, which is also dependent on the POS tags and ci."}, {"heading": "5 Training", "text": "For a given training instance (xi, yi), we use the maximum margin criterion to train our model. First, we forecast the dependence tree y-margin with the highest score for each xi and define a structured margin loss between the predicted tree y-i and the given right tree yi. (yi, y-i) is measured by counting the number of nodes yi with an incorrect span (or label) in the proposed tree (Goodman, 1998). (yi, y-i) = a given tree y-i (9), which is a discount parameter and d represents the nodes in the trees.Given a number of training dependency parameters D, the final training goal is to minimize the loss function J, plus an l2 regulation parameter: J (xi, yi margin) and the initial parameters of the initial parameters of the training dependence parameters (yi)."}, {"heading": "6 Re-rankers", "text": "Parser-best lists revaluation was introduced by Collins and Koo (2005), as well as Charniak and Johnson (2005), who used discriminatory methods to reevaluate constituent parsers. In dependency parsing, Sangati et al. (2009) used a generative third-order model to reevaluate the best lists of base parsers. Hayashi et al. (2013) used a discriminatory forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a significant increase in parser performance. Given T (x), the amount of best trees in a set x from a base parser, we use the popular mix of re-ranking strategies (Hayashi et al., 2013; Le and Mikolov, 2014), which represents a combination of our model and the base parser. Given T (x), the amount of best trees of a base parser, we use the base re-ranking strategy (Parser et al., 2013; Le and Mikolov, 2014), which represents a parser combination of our model and base parser. Parser-parser-parser-parser-parser-parser-parser-parser-. y = arg max y-T (xi) of the best trees of a base parser, we set the base parser-parser-parser-xi-parser-parser-xi-parser-parser (Parser-xi-xi-model), Parser-xi-xi-xi-xi-parser-parser-we use the parser-xi-xi-we-parser-xi-parser-parser (Parser-xi-xi), parser-xi-xi-xi-we-xi-parser-xi-parser-parser-x."}, {"heading": "7 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Datasets", "text": "To demonstrate empirically the effectiveness of our approach, we use two sets of data in different languages (English and Chinese) in our experimental evaluation and compare our model with other state-of-the-art methods using the unlabeled attachment score (UAS), ignoring the punctuation; for English records, we follow the standard splits of Penn Treebank (PTB), using Sections 2-21 for training, Section 22 for development and Section 23 for testing; we mark the development and testing sets using an automatic POS tagger (with an accuracy of 97.2%) and mark the training set using four-way jackknives similarly (Collins and Koo, 2005); for Chinese records, we follow the same breakdown of Penn Chinese Treeban (CTB5) as described in (Zhang and Clark, 2008) and use Sections 001-815, 1001-1136 as training set, Sections 886-931."}, {"heading": "7.2 English Dataset", "text": "We first evaluate the performance of the RCNN and Re-Ranker (Eq. (14)) on the basis of advantages. Figure 5 shows UASs of different models with different k. The base parser reaches 92.45%. If k = 64, the oracle best of base parser reaches 97.34%, while the oracle worst reaches 73.30% (-19.15%). The reason for this is that RCNN achieves the maximum improvement of 93.00% (+ 0.55%) when k = 6. If k > 6, the performance of RCNN decreases with the increase of k, but is still higher than the baseline (92.45%). The reason for this is that RCNN might require more negative samples if k is large. Since the negative samples in the k-best results of a base parser are limited, the learned parameters could easily be overtaken."}, {"heading": "7.3 Chinese Dataset", "text": "We are also experimenting at the Penn Chinese Treebank (CTB5). Hyperparameters are the same as in the previous experiment in English, except \u03b1 is optimized by searching with the step size 0.005. The final experimental results of the test set are presented in Table 3. Our re-Ranker achieves the performance of 85.71% (+ 0.25%) on the test set, which also exceeds the previous state-of-the-art methods. By adding the oracle, the re-Ranker can reach 87.43% on the UAS, which is shown in the last line (\"our re-Ranker (with oracle)\") of Table 3. Compared to the re-ranking model by Hayashi et al. (2013), which uses a large number of handmade features, our model can achieve a competitive performance with minimal feature engineering."}, {"heading": "7.4 Discussions", "text": "The low divergence of the dependency trees in the output list also leads to overadjustment in the training phase. Although our reanchor outperforms the most modern methods, it can also benefit from improving the quality of candidate results. It has also been reported in other reanking work that a larger k (e.g. k > 64) performs worse. We think the reason for this is that the oracle increases best when k is larger, but the oracle decreases worst with greater degrees. Error types increase sharply. The reanking model requires more negative samples to avoid overadjustment. If k is larger, the number of negative samples for training must also be increased many times over. However, we can obtain at most negative samples from the k-best results of the base saver. The experiments also show that our model can achieve significant improvements by adding the oracle into the output method of the base saver, indicating that better coding results can be implemented by the candidate."}, {"heading": "8 Related Work", "text": "There have been several papers on the use of neural networks and distributed representation for dependency sparsing.Stenetorp (2013) attempted to build recursive neural networks for transition-based dependency analysis, but the empirical performance of its model is still unsatisfactory. Chen and Manning (2014) improved the transition-based dependency analysis by presenting all words, POS tags, and arc labels as dense vectors, and modeled their interactions with neural networks to make predictions of actions. Their methods target transition-based parsing and cannot model the sentence in the semantic vector space for other NLP tasks. Socher et al. (2013b) proposed a compositional vector computed by the dependency tree RNN (DT-RNN) to model sentences and images in a common embedding space. However, there are two major differences as follows."}, {"heading": "9 Conclusion", "text": "We propose a recursive architecture of the Convolutionary Neural Network (RCNN) to capture the syntactic and compositional-semantic representations of phrases and words. RCNN is a general architecture and can handle the k-ar parsing tree, so RCNN is very well suited for many NLP tasks to minimize the effort of feature engineering with an external dependency saver. Although RCNN is used in this paper only for the re-evaluation of the dependency saver, it can be considered a semantic modeling of text sequences and process the input sequences of varying lengths into a fixed-length vector. We believe that the integrated parameter in RCNN can be learned together with some other NLP tasks, such as text classification. For future research, we will develop an integrated parser to combine RCNN with a decoding algorithm."}, {"heading": "Acknowledgments", "text": "We thank the anonymous critics for their valuable comments. This work was partly funded by the National Natural Science Foundation of China (61472088, 61473092), the National High Technology Research and Development Program of China (2015AA015408), the Shanghai Science and Technology Development Fund (14ZR1403200) and the Shanghai Leading Academic Discipline Project (B114)."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Coarse-to-fine n-best parsing", "author": ["Charniak", "Johnson2005] Eugene Charniak", "Mark Johnson"], "venue": null, "citeRegEx": "Charniak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2005}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Feature embedding for dependency parsing", "author": ["Chen et al.2014] Wenliang Chen", "Yue Zhang", "Min Zhang"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Discriminative reranking for natural language parsing", "author": ["Collins", "Koo2005] Michael Collins", "Terry Koo"], "venue": "Computational Linguistics,", "citeRegEx": "Collins et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins"], "venue": "Computational linguistics,", "citeRegEx": "Collins.,? \\Q2003\\E", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Towards incremental parsing of natural language using recursive neural networks", "author": ["Costa et al.2003] Fabrizio Costa", "Paolo Frasconi", "Vincenzo Lombardo", "Giovanni Soda"], "venue": "Applied Intelligence,", "citeRegEx": "Costa et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Costa et al\\.", "year": 2003}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Parsing inside-out. arXiv preprint cmp-lg/9805007", "author": ["Joshua Goodman"], "venue": null, "citeRegEx": "Goodman.,? \\Q1998\\E", "shortCiteRegEx": "Goodman.", "year": 1998}, {"title": "Efficient stacked dependency parsing by forest reranking", "author": ["Shuhei Kondo", "Yuji Matsumoto"], "venue": null, "citeRegEx": "Hayashi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hayashi et al\\.", "year": 2013}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Huang", "Sagae2010] Liang Huang", "Kenji Sagae"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of ICML", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "The inside-outside recursive neural network model for dependency parsing", "author": ["Le", "Zuidema2014] Phong Le", "Willem Zuidema"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Online largemargin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Wide coverage natural language processing using kernel methods and neural networks for structured data", "author": ["Fabrizio Costa", "Paolo Frasconi", "Massimiliano Pontil"], "venue": null, "citeRegEx": "Menchetti et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Menchetti et al\\.", "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50\u201357", "author": ["Joakim Nivre"], "venue": null, "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Recursive distributed representations", "author": ["Jordan B Pollack"], "venue": "Artificial Intelligence,", "citeRegEx": "Pollack.,? \\Q1990\\E", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "online) subgradient methods for structured prediction", "author": ["J Andrew Bagnell", "Martin A Zinkevich"], "venue": "In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats)", "citeRegEx": "Ratliff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "A generative re-ranking model for dependency parsing", "author": ["Willem Zuidema", "Rens Bod"], "venue": "In Proceedings of the 11th International Conference on Parsing Technologies,", "citeRegEx": "Sangati et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sangati et al\\.", "year": 2009}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Q Le", "C Manning", "A Ng"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Transitionbased dependency parsing using recursive neural networks", "author": ["Pontus Stenetorp"], "venue": "In NIPS Workshop on Deep Learning", "citeRegEx": "Stenetorp.,? \\Q2013\\E", "shortCiteRegEx": "Stenetorp.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Yamada", "Matsumoto2003] H. Yamada", "Y. Matsumoto"], "venue": "In Proceedings of the International Workshop on Parsing Technologies (IWPT),", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beam-search", "author": ["Zhang", "Clark2008] Yue Zhang", "Stephen Clark"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies:", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 20, "context": "Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data.", "startOffset": 97, "endOffset": 161}, {"referenceID": 16, "context": "Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data.", "startOffset": 97, "endOffset": 161}, {"referenceID": 29, "context": "natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al.", "startOffset": 56, "endOffset": 147}, {"referenceID": 18, "context": "natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al.", "startOffset": 56, "endOffset": 147}, {"referenceID": 6, "context": "natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al.", "startOffset": 56, "endOffset": 147}, {"referenceID": 13, "context": ", 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013).", "startOffset": 46, "endOffset": 88}, {"referenceID": 19, "context": ", 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013).", "startOffset": 46, "endOffset": 88}, {"referenceID": 1, "context": "For dependency parsing, Chen et al. (2014) and Bansal et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 0, "context": "(2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation.", "startOffset": 11, "endOffset": 32}, {"referenceID": 21, "context": "The idea of recursive neural networks (RNN) for natural language processing (NLP) is to train a deep learning model that can be applied to phrases and sentences, which have a grammatical structure (Pollack, 1990; Socher et al., 2013c).", "startOffset": 197, "endOffset": 234}, {"referenceID": 24, "context": "Based on RNN, Socher et al. (2013a) introduced a compositional vector grammar, which uses the syntactically untied weights W to learn the syntactic-semantic, compositional vector representations.", "startOffset": 14, "endOffset": 36}, {"referenceID": 24, "context": "For more details on how standard RNN can be used for parsing, see (Socher et al., 2011).", "startOffset": 66, "endOffset": 87}, {"referenceID": 5, "context": "For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser (Collins, 2003).", "startOffset": 105, "endOffset": 120}, {"referenceID": 10, "context": "\u2206(yi, \u0177i) is measured by counting the number of nodes yi with an incorrect span (or label) in the proposed tree (Goodman, 1998).", "startOffset": 112, "endOffset": 127}, {"referenceID": 22, "context": "We use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradient-like direction.", "startOffset": 70, "endOffset": 92}, {"referenceID": 8, "context": "To minimize the objective, we use the diagonal variant of AdaGrad (Duchi et al., 2011).", "startOffset": 66, "endOffset": 86}, {"referenceID": 11, "context": "Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser.", "startOffset": 120, "endOffset": 164}, {"referenceID": 5, "context": "Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005).", "startOffset": 42, "endOffset": 65}, {"referenceID": 5, "context": "Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing.", "startOffset": 42, "endOffset": 97}, {"referenceID": 5, "context": "Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser.", "startOffset": 42, "endOffset": 216}, {"referenceID": 5, "context": "Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing.", "startOffset": 42, "endOffset": 318}, {"referenceID": 19, "context": "For initialization of parameters, we train word2vec embeddings (Mikolov et al., 2013) on Wikipedia corpus for English and Chinese respectively.", "startOffset": 63, "endOffset": 85}, {"referenceID": 11, "context": "It outperforms Hayashi et al. (2013) and Le and Zuidema (2014), which also use the mixture reranking strategy.", "startOffset": 15, "endOffset": 37}, {"referenceID": 11, "context": "It outperforms Hayashi et al. (2013) and Le and Zuidema (2014), which also use the mixture reranking strategy.", "startOffset": 15, "endOffset": 63}, {"referenceID": 9, "context": "Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted UAS Traditional Methods Zhang and Clark (2008) 91.", "startOffset": 38, "endOffset": 60}, {"referenceID": 9, "context": "Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted UAS Traditional Methods Zhang and Clark (2008) 91.", "startOffset": 38, "endOffset": 147}, {"referenceID": 9, "context": "Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.", "startOffset": 38, "endOffset": 175}, {"referenceID": 9, "context": "Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.1 Distributed Representations Stenetorp (2013) 86.", "startOffset": 38, "endOffset": 225}, {"referenceID": 2, "context": "25 Chen et al. (2014) 93.", "startOffset": 3, "endOffset": 22}, {"referenceID": 2, "context": "25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.", "startOffset": 3, "endOffset": 52}, {"referenceID": 2, "context": "25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.", "startOffset": 3, "endOffset": 90}, {"referenceID": 2, "context": "25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.", "startOffset": 3, "endOffset": 118}, {"referenceID": 2, "context": "20 Distributed Representations Chen et al. (2014) 82.", "startOffset": 31, "endOffset": 50}, {"referenceID": 2, "context": "20 Distributed Representations Chen et al. (2014) 82.94 Chen and Manning (2014) 83.", "startOffset": 31, "endOffset": 80}, {"referenceID": 2, "context": "20 Distributed Representations Chen et al. (2014) 82.94 Chen and Manning (2014) 83.9 Re-rankers Hayashi et al. (2013) 85.", "startOffset": 31, "endOffset": 118}, {"referenceID": 9, "context": "Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990).", "startOffset": 137, "endOffset": 150}], "year": 2015, "abstractText": "In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.", "creator": "LaTeX with hyperref package"}}}