{"id": "1505.05770", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "Variational Inference with Normalizing Flows", "abstract": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.", "histories": [["v1", "Thu, 21 May 2015 15:36:37 GMT  (5463kb,D)", "http://arxiv.org/abs/1505.05770v1", null], ["v2", "Fri, 22 May 2015 09:13:28 GMT  (5463kb,D)", "http://arxiv.org/abs/1505.05770v2", "Proceedings of the 32nd International Conference on Machine Learning"], ["v3", "Tue, 26 May 2015 15:46:33 GMT  (5485kb,D)", "http://arxiv.org/abs/1505.05770v3", "Proceedings of the 32nd International Conference on Machine Learning"], ["v4", "Mon, 22 Jun 2015 18:36:32 GMT  (5485kb,D)", "http://arxiv.org/abs/1505.05770v4", "Proceedings of the 32nd International Conference on Machine Learning"], ["v5", "Mon, 13 Jun 2016 08:46:44 GMT  (5485kb,D)", "http://arxiv.org/abs/1505.05770v5", "Proceedings of the 32nd International Conference on Machine Learning"], ["v6", "Tue, 14 Jun 2016 09:01:36 GMT  (5485kb,D)", "http://arxiv.org/abs/1505.05770v6", "Proceedings of the 32nd International Conference on Machine Learning"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG stat.CO stat.ME", "authors": ["danilo jimenez rezende", "shakir mohamed"], "accepted": true, "id": "1505.05770"}, "pdf": {"name": "1505.05770.pdf", "metadata": {"source": "META", "title": "Variational Inference with Normalizing Flows", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed"], "emails": ["DANILOR@GOOGLE.COM", "SHAKIR@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Amortized Variational Inference", "text": "To draw conclusions, it is sufficient to use the marginal probability of a probability model, and requires the marginalization of all missing or latent variables in the model. This integration is typically insoluble, and instead we optimize a lower limit for the marginal probability. Consider a general probability model with observations x, latent variables z, over which we must integrate, and model parameters. We introduce an approximate posterior distribution for the latent variables q\u03c6 (z | x) and follow the principle of variation (Jordan et al., 1999) to obtain a limit for the marginal probability: log p\u03b8 (x) = logatory distribution p (z) dz (1) = log q\u03c6 (z | x) q\u03c6 (z) q\u03c6 (z | x) qvident (z) p (z) dz (z).IDKL (z) p (z) + Eq) qvident (z), we (quz) function (asiz)."}, {"heading": "2.1. Stochastic Backpropagation", "text": "Much of the research in the field of variable inference over the years has focused on methods for calculating the gradient of the expected log probability variability. Whereas in the past we would have relied on local variation methods (Bishop, 2006), today we always calculate such expectations using Monte Carlo approximations (including the KL term in the boundary if it is not analytically known), which forms what is rightly called a doublystochastic estimate (Titsias & Lazaro-Gredilla, 2014), since we have one source of stochasticity from the minibatch and another from the Monte Carlo approximation of expectation. We focus on models with continuous latent variables, and the approach we choose is the required gradients using an uncentered repair variability of expectation (Papaspiliopoulos et al, 2003; Williams, 1992)."}, {"heading": "2.2. Inference Networks", "text": "A second important practice is that the approximate posterior distribution q\u03c6 (\u00b7) is represented by a detection model or an inference network (Stuhlmu Mueller et al., 2013; Rezende et al., 2014; Dayan, 2000; Gershman & Goodman, 2014; Kingma & Welling, 2014). An inference network is a model that learns an inverse map from observations of latent variables. By using an inference network, we avoid the need to calculate variation parameters per data point, but can instead calculate a set of global variation parameters that are valid for both training and testing time. This allows us to amortize the cost of inference by generalizing between the rear estimates for all latent variables through the parameters of the inference network. The simplest inference models we can use are diagonal Gaussian densities, q\u03c6 (z | x) = N (z | \u00b5Phillips (x), diag, which are equal to 2.0 in mean function."}, {"heading": "2.3. Deep Latent Gaussian Models", "text": "In this paper, we examine deep latent Gaussian models (DLGM), which represent a general class of deep-set graphical models derived from a hierarchy of L-layers of latent Gaussian variables zl by layer l. Each layer of latent variables is nonlinear dependent on the layer above, and for DLGMs this nonlinear dependence is specified by deep neural networks. Common probability model is: p (x, z1,.., zL) = p (x | f0 (z1)) L-l = 1 p (zl | fl (zl + 1))) (4), where the Lth-Gaussian distribution is not dependent on other random variables. The previous latent variables are a unit of Gaussian p (zl) = N (0, I), and the observation probability of variable cellular (x | z) is any appropriate distribution conditioned on Kingz1 and also paramrized by a deep neonal problem."}, {"heading": "3. Normalizing Flows", "text": "If we examine the boundary (3), we can see that the optimal distribution of variations that IDKL [q \u0109p] = 0 allows is one for which q\u03c6 (z | x) = p\u03b8 (z | x), i.e. q, corresponds to the true posterior distribution. Obviously, this possibility is not feasible given the typically used q (\u00b7) distributions, such as independent Gaussians or other central field approximations. Indeed, a limitation of the method of variation due to the available choices of approximation families is that even in an asymptotic regime we cannot obtain the true posterior. Thus, an ideal family of variation distributions q\u03c6 (z | x) is a highly flexible, preferably flexible enough to include the true posterior as a solution. One way to achieve this ideal is based on the principle of normalization of streams (Tabak & Turner, 2013; Tabak & Vandjenjnden, 2010)."}, {"heading": "3.1. Finite Flows", "text": "The basic rule for the transformation of densities considers an invertible, smooth mapping f: IRd \u2192 IRd with inversef \u2212 1 = g, i.e. the composition g \u2212 f (z) = z. If we convert this random variable z with distribution q (z), the resulting random variable z (z) has a distribution: q (z) = q (z).n The chain rule (inverse function theorem) and is a property of the Jacobians of inverted functions. We can arbitrarily define complex random variable z (z) q.n The last equality can be achieved by applying the chain rule (z \u2212 floorem) and the property of the Jacobians of inversible functions."}, {"heading": "3.2. Infinitesimal Flows", "text": "It is natural to consider the case where the length of the normalizing flow tends toward infinity. In this case, we get an infinitesimal flow that is not described in terms of a finite sequence of transformations - a finite flow, but as a partial differential equation that describes how the initial density q0 (z) develops over \"time.\" An important family of flows is given by the stochastic differential equation of Langevin (SDE). (SDE): dz (t) = F (t), focerice (t), t + G (t), d), d), d), d (t), d), d (t), d), x, the initial equation of Langevin (z)."}, {"heading": "4. Inference with Normalizing Flows", "text": "To enable a scalable conclusion using finite normalizing flows, we must specify a class of invertable transformations that can be used, and an efficient mechanism for calculating the determinant of the Jacobian. Although it is easy to build invertable parametric functions for use in Equation (5), such as inverted neural networks (Baird et al., 2005; Rippel & Adams, 2013), such approaches typically have a complexity for calculating the Jacobic determinant, which is scaled as O (LD3), with D being the dimension of the hidden layers and L the number of hidden layers. Furthermore, calculating the gradients of the Jacobic determinant requires several additional operations, which are also O (LD3) and include matrix inverses that may be numerically unstable."}, {"heading": "4.1. Invertible Linear-time Transformations", "text": "We consider a family of transformations of the form: f (z) = z + uh (w > z + b), (10) where \u03bb = [w] IRD, u \u2212 IRD, b \u2212 IR} are free parameters and h (\u00b7) is a smooth elemental nonlinearity, with the derivative h \"(\u00b7). For this figure we can calculate the logdet \u2212 jacobic term in O (D) time (using the matrix determinant terminal terminal):. (12) From (7) we conclude that the density qK (z) is implicit by the transformation of an arbitrary initial density q0 (z) by the transformation of radiality q0 (z) by the sequence of maps fk of the form (10)."}, {"heading": "4.2. Flow-Based Free Energy Bound", "text": "If we parameterise the approximate rear distribution with a linear flux K, q\u03c6 (z | x): = qK (zK), the free energy (3) can be written as an expectation of the initial distribution q0 (z): F (x) = Eq\u03c6 (z | x) [log q\u03c6 (z | x) \u2212 log p (x, z)] = Eq0 (z0) [ln qK (zK) \u2212 log p (x, zK)] = Eq0 (z0) [ln q0 (z0)] \u2212 Eq0 (z0) [log p (x, zK)] \u2212 Eq0 (z0) [K \u00b2 k = 1 ln | 1 + u > k \u00b2 k (zk) |]. (15) The normalization of currents and this free energy limit can be used with any variation optimization scheme, including generalized variation parameters EM."}, {"heading": "4.3. Algorithm Summary and Complexity", "text": "The resulting algorithm is a simple modification of the amortized consequence algorithm for DLGMs, which is described by (Kingma and Welling, 2014; Rezende et al., 2014), that we summarize in algorithm 1. Using a follow-up net algorithm 1 Variational Inf. with Normalizing Flows parameters:? Variational, generative while not converged dox???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "5. Alternative Flow-based Posteriors", "text": "Using the framework of flow normalization, we can provide a unified view of recent proposals for designing more flexible posterior transition processes. At the outset, we distinguish between two types of flow mechanisms that differ in the way the Jacobian is handled. The work in this paper looks at the general normalization of flows and presents a method for calculating the linear time of the Jacobian. In contrast, volume reservation processes are able to shape the flow in such a way that its Jacobian determinant is equal while still allowing rich posterior distributions. Both categories allow currents that are finite or infinitesimal. The non-linear independent components of stimulation (NICE) are developed by Dinh et al. (2014) is an instance of a finite volume-maintaining flux. The transformations used are neural networks f () with easy-to-calculate inverses (z) of the form: f + zB + h.B."}, {"heading": "6. Results", "text": "During this section, we evaluate the effects of the use of normalization currents based on posterior approximations for conclusions in deep latent Gaussian models (DLGMs). Training was conducted by following a Monte Carlo estimate of the gradient of a random version of free energy (20) with respect to model parameters \u03b8 and variation parameters \u03c6 using stochastic repropoagation. The resulting Q matrix will be a random orthogonal matrix (Genz, 1998).Table 1. Test energy functions U (z) 1: 1 2 (z) -20,4) 2 (e-factorization).The resulting Q matrix will be a random orthogonal matrix."}, {"heading": "6.1. Representative Power of Normalizing Flows", "text": "To give an insight into the representative power of density approximations based on the normalization of flows, we parameterize a series of unnormalized 2D densities p (z), p (z), and exp [\u2212 U (z)] that are shown in Table 1.In Figure 3 (a), we show the true distribution for four cases that have distributions that exhibit characteristics such as multimodality and periodicity that cannot be captured with typical posterior approximations.Figure 3 (b) shows the performance of normalization flows for these densities based on flux lengths of 2, 8, and 32 transformations. We see a significant improvement in matricity quality when we find the flow length is increased in Equation (10), and the output distribution was a diagonal Gaussian, q0 (z) = N (z | \u00b5, \u03c32I). We see a significant improvement in matricity quality when we find the flow length is increased with the approximation of the ICE, \u00b5 = (with the help of ICE)."}, {"heading": "6.2. MNIST and CIFAR-10 Images", "text": "The MNIST digit dataset (LeCun & Cortes, 1998) contains 60,000 training images and 10,000 test images of ten handwritten digits (0 to 9) measuring 28 x 28 pixels. We used the binary dataset as in (Uria et al., 2014). We trained various DLGMs with 40 latent variables for 500,000 parameter updates. The performance of a DLGM using the (planar) normalizing flow (DLGM + NF) is compared to the volume-maintaining approaches that NICE (DLGM + NICE) uses on exactly the same model for different flow lengths K, and we summarize the performance in Figure 4. This graph shows that an increase in flow length systematically improves the bound F as shown in Figure 4 (a), and reduces the KL divergence (z | x) values that we use as natural F values."}, {"heading": "7. Conclusion and Discussion", "text": "In this paper, we have developed a simple approach to learning highly non-Gaussian posterior densities by learning transformations of simple densities into more complex ones through a normalizing flux.Combined with an amortized approach to variable conclusions using inference networks and efficient Monte Carlo gradient estimation, we are able to show clear improvements over simple approaches to various problems.With this view of normalization of flows, we are able to provide a unified perspective on other closely related methods for flexible posterior estimates that point to a wide range of approaches for developing more powerful posterior approximations with different statistical and computational compromises. An important conclusion from the discussion in Section 3 is that there are classes of normalizing flows that allow us to create extremely rich posterior approximations for variational consequences.With normalizing flows, we are able to show that in the symptom-rich solution space the posterior regime is sufficiently soluble."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "In ICML,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "One-step neural network inversion with PDF learning and emulation", "author": ["L. Baird", "D. Smalenberger", "S. Ingkiriwang"], "venue": "In IJCNN,", "citeRegEx": "Baird et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Baird et al\\.", "year": 2005}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "springer New York,", "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Affine independent variational inference", "author": ["E. Challis", "D. Barber"], "venue": "In NIPS,", "citeRegEx": "Challis and Barber,? \\Q2012\\E", "shortCiteRegEx": "Challis and Barber", "year": 2012}, {"title": "Helmholtz machines and wake-sleep learning. Handbook of Brain Theory and Neural Network", "author": ["P. Dayan"], "venue": null, "citeRegEx": "Dayan,? \\Q2000\\E", "shortCiteRegEx": "Dayan", "year": 2000}, {"title": "NICE: Non-linear independent components estimation", "author": ["L. Dinh", "D. Krueger", "Y. Bengio"], "venue": "arXiv preprint arXiv:1410.8516,", "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Methods for generating random orthogonal matrices", "author": ["A. Genz"], "venue": "Monte Carlo and Quasi-Monte Carlo Methods,", "citeRegEx": "Genz,? \\Q1998\\E", "shortCiteRegEx": "Genz", "year": 1998}, {"title": "Nonparametric variational inference", "author": ["S. Gershman", "M. Hoffman", "D. Blei"], "venue": "In ICML,", "citeRegEx": "Gershman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2012}, {"title": "Amortized inference in probabilistic reasoning", "author": ["S.J. Gershman", "N.D. Goodman"], "venue": "In Annual Conference of the Cognitive Science Society,", "citeRegEx": "Gershman and Goodman,? \\Q2014\\E", "shortCiteRegEx": "Gershman and Goodman", "year": 2014}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "A. Mnih", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Stochastic variational inference", "author": ["M.D. Hoffman", "Blei", "D. M", "C. Wang", "J. Paisley"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Improving the mean field approximation via the use of mixture distributions", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "In Learning in graphical models,", "citeRegEx": "Jaakkola and Jordan,? \\Q1998\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 1998}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Convolutional deep belief networks on CIFAR-10", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Unpublished manuscript,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2010}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In ICML,", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "MCMC using hamiltonian dynamics", "author": ["R.M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "Neal,? \\Q2011\\E", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Noncentered parameterisations for hierarchical models and data augmentation", "author": ["O. Papaspiliopoulos", "G.O. Roberts", "M. Sk\u00f6ld"], "venue": "In Bayesian Statistics", "citeRegEx": "Papaspiliopoulos et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Papaspiliopoulos et al\\.", "year": 2003}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2013}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "High-dimensional probability estimation with deep density models", "author": ["O. Rippel", "R.P. Adams"], "venue": "arXiv preprint arXiv:1302.5125,", "citeRegEx": "Rippel and Adams,? \\Q2013\\E", "shortCiteRegEx": "Rippel and Adams", "year": 2013}, {"title": "Markov chain Monte Carlo and variational inference: Bridging the gap", "author": ["T. Salimans", "D.P. Kingma", "M. Welling"], "venue": "In ICML,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Learning stochastic inverses", "author": ["A. Stuhlm\u00fcller", "J. Taylor", "N. Goodman"], "venue": "In NIPS, pp. 3048\u20133056,", "citeRegEx": "Stuhlm\u00fcller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Stuhlm\u00fcller et al\\.", "year": 2013}, {"title": "Online learning Fokker-Planck machine", "author": ["J.A.K. Suykens", "H. Verrelst", "J. Vandewalle"], "venue": "Neural processing letters,", "citeRegEx": "Suykens et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Suykens et al\\.", "year": 1998}, {"title": "A family of nonparametric density estimation algorithms", "author": ["E.G. Tabak", "C.V. Turner"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Tabak and Turner,? \\Q2013\\E", "shortCiteRegEx": "Tabak and Turner", "year": 2013}, {"title": "Density estimation by dual ascent of the log-likelihood", "author": ["Tabak", "E. G", "E. Vanden-Eijnden"], "venue": "Communications in Mathematical Sciences,", "citeRegEx": "Tabak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tabak et al\\.", "year": 2010}, {"title": "Doubly stochastic variational Bayes for non-conjugate inference", "author": ["M. Titsias", "M. Lazaro-Gredilla"], "venue": "In ICML,", "citeRegEx": "Titsias and Lazaro.Gredilla,? \\Q2014\\E", "shortCiteRegEx": "Titsias and Lazaro.Gredilla", "year": 2014}, {"title": "Two problems with variational expectation maximisation for time-series models", "author": ["R.E. Turner", "M. Sahani"], "venue": "Bayesian Time series models,", "citeRegEx": "Turner and Sahani,? \\Q2011\\E", "shortCiteRegEx": "Turner and Sahani", "year": 2011}, {"title": "A deep and tractable density estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": "In ICML,", "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "Convergence and asymptotic normality of variational Bayesian approximations for exponential family models with missing values", "author": ["B. Wang", "D.M. Titterington"], "venue": "In UAI,", "citeRegEx": "Wang and Titterington,? \\Q2004\\E", "shortCiteRegEx": "Wang and Titterington", "year": 2004}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "In ICML,", "citeRegEx": "Welling and Teh,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Automated variational inference in probabilistic programming", "author": ["D. Wingate", "T. Weber"], "venue": "In NIPS Workshop on Probabilistic Programming,", "citeRegEx": "Wingate and Weber,? \\Q2013\\E", "shortCiteRegEx": "Wingate and Weber", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Variational inference now lies at the core of large-scale topic models of text (Hoffman et al., 2013), provides the state-of-the-art in semi-supervised classification (Kingma et al.", "startOffset": 79, "endOffset": 101}, {"referenceID": 15, "context": ", 2013), provides the state-of-the-art in semi-supervised classification (Kingma et al., 2014), drives the models that currently produce the most realistic generative models of images (Gregor et al.", "startOffset": 73, "endOffset": 94}, {"referenceID": 10, "context": ", 2014), drives the models that currently produce the most realistic generative models of images (Gregor et al., 2014; Rezende et al., 2014), and are a default", "startOffset": 97, "endOffset": 140}, {"referenceID": 21, "context": ", 2014), drives the models that currently produce the most realistic generative models of images (Gregor et al., 2014; Rezende et al., 2014), and are a default", "startOffset": 97, "endOffset": 140}, {"referenceID": 13, "context": "Another potentially powerful alternative would be to specify the approximate posterior as a mixture model, such as those developed by Jaakkola & Jordan (1998); Jordan et al. (1999); ar X iv :1 50 5.", "startOffset": 160, "endOffset": 181}, {"referenceID": 13, "context": "We introduce an approximate posterior distribution for the latent variables q\u03c6(z|x) and follow the variational principle (Jordan et al., 1999) to obtain a bound on the marginal likelihood:", "startOffset": 121, "endOffset": 142}, {"referenceID": 2, "context": "Whereas we would have previously resorted to local variational methods (Bishop, 2006), in general we now always compute such expectations using Monte Carlo approximations (including the KL term in the bound, if it is not analytically known).", "startOffset": 71, "endOffset": 85}, {"referenceID": 19, "context": "We focus on models with continuous latent variables, and the approach we take computes the required gradients using a non-centered reparameterization of the expectation (Papaspiliopoulos et al., 2003; Williams, 1992), combined with Monte Carlo approximation \u2014 referred to as stochastic backpropagation (Rezende et al.", "startOffset": 169, "endOffset": 216}, {"referenceID": 21, "context": ", 2003; Williams, 1992), combined with Monte Carlo approximation \u2014 referred to as stochastic backpropagation (Rezende et al., 2014).", "startOffset": 109, "endOffset": 131}, {"referenceID": 20, "context": "A number of general purpose approaches based on Monte Carlo control variate (MCCV) estimators exist as an alternative to stochastic backpropagation, and allow for gradient computation with latent variables that may be continuous or discrete (Williams, 1992; Mnih & Gregor, 2014; Ranganath et al., 2013; Wingate & Weber, 2013).", "startOffset": 241, "endOffset": 325}, {"referenceID": 24, "context": "A second important practice is that the approximate posterior distribution q\u03c6(\u00b7) is represented using a recognition model or inference network (Stuhlm\u00fcller et al., 2013; Rezende et al., 2014; Dayan, 2000; Gershman & Goodman, 2014; Kingma & Welling, 2014).", "startOffset": 143, "endOffset": 254}, {"referenceID": 21, "context": "A second important practice is that the approximate posterior distribution q\u03c6(\u00b7) is represented using a recognition model or inference network (Stuhlm\u00fcller et al., 2013; Rezende et al., 2014; Dayan, 2000; Gershman & Goodman, 2014; Kingma & Welling, 2014).", "startOffset": 143, "endOffset": 254}, {"referenceID": 4, "context": "A second important practice is that the approximate posterior distribution q\u03c6(\u00b7) is represented using a recognition model or inference network (Stuhlm\u00fcller et al., 2013; Rezende et al., 2014; Dayan, 2000; Gershman & Goodman, 2014; Kingma & Welling, 2014).", "startOffset": 143, "endOffset": 254}, {"referenceID": 21, "context": "This model class is very general and includes other models such as factor analysis and PCA, non-linear factor analysis, and non-linear Gaussian belief networks as special cases (Rezende et al., 2014).", "startOffset": 177, "endOffset": 199}, {"referenceID": 21, "context": "The inference networks used in Kingma & Welling (2014); Rezende et al. (2014) are simple diagonal or diagonal-plus-low rank Gaussian distributions.", "startOffset": 56, "endOffset": 78}, {"referenceID": 0, "context": "This approach has been explored for sampling from complex densities by Welling & Teh (2011); Ahn et al. (2012); Suykens et al.", "startOffset": 93, "endOffset": 111}, {"referenceID": 0, "context": "This approach has been explored for sampling from complex densities by Welling & Teh (2011); Ahn et al. (2012); Suykens et al. (1998).", "startOffset": 93, "endOffset": 134}, {"referenceID": 18, "context": ", Neal (2011). We will use the Hamiltonian flow to make a connection to the recently introduced Hamiltonian variational approach from Salimans et al.", "startOffset": 2, "endOffset": 14}, {"referenceID": 18, "context": ", Neal (2011). We will use the Hamiltonian flow to make a connection to the recently introduced Hamiltonian variational approach from Salimans et al. (2015) in section 5.", "startOffset": 2, "endOffset": 157}, {"referenceID": 1, "context": ", invertible neural networks (Baird et al., 2005; Rippel & Adams, 2013), such approaches typically have a complexity for computing the Jacobian determinant that scales as O(LD), where D is the dimension of the hidden layers and L is the number of hidden layers used.", "startOffset": 29, "endOffset": 71}, {"referenceID": 21, "context": "The resulting algorithm is a simple modification of the amortized inference algorithm for DLGMs described by (Kingma & Welling, 2014; Rezende et al., 2014), which we summarize in algorithm 1.", "startOffset": 109, "endOffset": 155}, {"referenceID": 6, "context": "The estimated gradients are used in conjunction with preconditioned stochastic gradient-based optimization methods such as RMSprop or AdaGrad (Duchi et al., 2010), where we use parameter updates of the form: (\u03b8,\u03c6) \u2190 (\u03b8,\u03c6) + \u0393t(g\u03b8,g\u03c6), with \u0393 is a diagonal preconditioning matrix that adaptively scales the gradients for faster minimization.", "startOffset": 142, "endOffset": 162}, {"referenceID": 5, "context": "The Non-linear Independent Components Estimation (NICE) developed by Dinh et al. (2014) is an instance of a finite volume-preserving flow.", "startOffset": 69, "endOffset": 88}, {"referenceID": 5, "context": "Dinh et al. (2014) assume the partitioning is of the form z = [zA = z1:d, zB = zd+1:D].", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "The Hamiltonian variational approximation (HVI) developed by Salimans et al. (2015) is an instance of an infinitesimal volume-preserving flow.", "startOffset": 61, "endOffset": 84}, {"referenceID": 23, "context": "The Hamiltonian variational approximation (HVI) developed by Salimans et al. (2015) is an instance of an infinitesimal volume-preserving flow. For HVI, we consider posterior approximations q(z,\u03c9|x) that make use of additional auxiliary variables\u03c9. The latent variables z are independent of the auxiliary variables \u03c9 and using the change of variables rule, the resulting distribution is: q(z\u2032,\u03c9\u2032) = |J|q(z)q(\u03c9), where z\u2032,\u03c9\u2032 = f(z,\u03c9) using a transformation f . Salimans et al. (2015) obtain a volume-preserving invertible transformation by exploiting the use of such transition operators in the MCMC literature, in particular the methods of Langevin and Hybrid Monte Carlo.", "startOffset": 61, "endOffset": 482}, {"referenceID": 7, "context": "The resulting Q-matrix will be a random orthogonal matrix (Genz, 1998).", "startOffset": 58, "endOffset": 70}, {"referenceID": 21, "context": "9) (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 3, "endOffset": 49}, {"referenceID": 5, "context": "Figure 3(c) shows the same approximation using the volumepreserving transformation used in NICE (Dinh et al., 2014) for the same number of transformations.", "startOffset": 96, "endOffset": 115}, {"referenceID": 23, "context": "Results below from (Salimans et al., 2015) DLGM + HVI (1 leapfrog step) 88.", "startOffset": 19, "endOffset": 42}, {"referenceID": 10, "context": "Results below from (Gregor et al., 2014) DARN nh = 500 84.", "startOffset": 19, "endOffset": 40}, {"referenceID": 30, "context": "We used the binarized dataset as in (Uria et al., 2014).", "startOffset": 36, "endOffset": 55}], "year": 2015, "abstractText": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.", "creator": "LaTeX with hyperref package"}}}