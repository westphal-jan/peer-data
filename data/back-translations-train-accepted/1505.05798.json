{"id": "1505.05798", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret", "abstract": "Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial set- ting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.", "histories": [["v1", "Thu, 21 May 2015 17:24:57 GMT  (283kb,D)", "http://arxiv.org/abs/1505.05798v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haitham bou-ammar", "rasul tutunov", "eric eaton"], "accepted": true, "id": "1505.05798"}, "pdf": {"name": "1505.05798.pdf", "metadata": {"source": "META", "title": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret", "authors": ["Haitham Bou Ammar", "Rasul Tutunov", "Eric Eaton"], "emails": ["HAITHAMB@SEAS.UPENN.EDU", "TUTUNOV@SEAS.UPENN.EDU", "EEATON@CIS.UPENN.EDU"], "sections": [{"heading": null, "text": "1. Introduction Reinforcement learning (RL) (RL) (Busoniu et al., 2010; Sutton & Barto, 1998) often requires substantial experience before it achieves acceptable performance on individual control problems. An essential contribution to this problem is the tabula rasa adoption of typical RL methods that learn from scratch from the application of RL to each new task. In these settings, learning performance is directly correlated with the quality of the tasks acquired. Unfortunately, the amount of experience required for high-quality performance can be exponentially inhibited by the degrees of freedom that inhibit the application of RL to high-dimensional control tasks. If the data is limited, transfer learning can significantly improve model performance on new tasks by using earlier learned knowledge during training (Taylor & Stone, 2009; Gheshlaghi Azar et al, 2013; Lazaric, 2011; Ferrante et al, 2008; Bou Ammar et al, 2012)."}], "references": [{"title": "Online learning in Markov decision processes with adversarially chosen transition probability distributions", "author": ["Yasin Abbasi-Yadkori", "Peter Bartlett", "Varun Kanade", "Yevgeny Seldin", "Csaba Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2013}, {"title": "Reinforcement learning transfer via sparse coding", "author": ["Haitham Bou Ammar", "Karl Tuyls", "Matthew E. Taylor", "Kurt Driessen", "Gerhard Weiss"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Ammar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2012}, {"title": "Online multi-task learning for policy gradient methods", "author": ["Haitham Bou Ammar", "Eric Eaton", "Paul Ruvolo", "Matthew Taylor"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Design and Control of Quadrotors with Application to Autonomous Flying", "author": ["Samir Bouabdallah"], "venue": "PhD Thesis, E\u0301cole polytechnique fe\u0301de\u0301rale de Lausanne,", "citeRegEx": "Bouabdallah.,? \\Q2007\\E", "shortCiteRegEx": "Bouabdallah.", "year": 2007}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Reinforcement Learning and Dynamic Programming Using Function Approximators", "author": ["Lucian Busoniu", "Robert Babuska", "Bart De Schutter", "Damien Ernst"], "venue": null, "citeRegEx": "Busoniu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2010}, {"title": "Sequential transfer in multi-armed bandit with finite set of models", "author": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Azar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2013}, {"title": "Cauchy-Schwarz inequalities associated with positive semidefinite matrices", "author": ["Roger A. Horn", "Roy Mathias"], "venue": "Linear Algebra and its Applications 142:63\u201382,", "citeRegEx": "Horn and Mathias.,? \\Q1990\\E", "shortCiteRegEx": "Horn and Mathias.", "year": 1990}, {"title": "Policy search for motor primitives in robotics", "author": ["Jens Kober", "Jan Peters"], "venue": "Machine Learning,", "citeRegEx": "Kober and Peters.,? \\Q2011\\E", "shortCiteRegEx": "Kober and Peters.", "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Abhishek Kumar", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Kumar and III.,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III.", "year": 2012}, {"title": "Transfer in reinforcement learning: a framework and a survey", "author": ["Alessandro Lazaric"], "venue": "Reinforcement Learning: State of the Art. Springer,", "citeRegEx": "Lazaric.,? \\Q2011\\E", "shortCiteRegEx": "Lazaric.", "year": 2011}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Jan Peters", "Stefan Schaal"], "venue": "Neural Networks,", "citeRegEx": "Peters and Schaal.,? \\Q2008\\E", "shortCiteRegEx": "Peters and Schaal.", "year": 2008}, {"title": "ELLA: An Efficient Lifelong Learning Algorithm", "author": ["Paul Ruvolo", "Eric Eaton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Ruvolo and Eaton.,? \\Q2013\\E", "shortCiteRegEx": "Ruvolo and Eaton.", "year": 2013}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David Mcallester", "Satinder Singh", "Yishay Mansour"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Transfer learning for reinforcement learning domains: a survey", "author": ["Matthew E. Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor and Stone.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone.", "year": 2009}, {"title": "Discovering structure in multiple learning tasks: the TC algorithm", "author": ["Sebastian Thrun", "Joseph O\u2019Sullivan"], "venue": "In Proceedings of the 13th International Conference on Machine Learning (ICML),", "citeRegEx": "Thrun and O.Sullivan.,? \\Q1996\\E", "shortCiteRegEx": "Thrun and O.Sullivan.", "year": 1996}, {"title": "Learning more from less data: experiments in lifelong learning", "author": ["Sebastian Thrun", "Joseph O\u2019Sullivan"], "venue": "Seminar Digest,", "citeRegEx": "Thrun and O.Sullivan.,? \\Q1996\\E", "shortCiteRegEx": "Thrun and O.Sullivan.", "year": 1996}, {"title": "Nonlinear tracking and landing controller for quadrotor aerial robots", "author": ["Holger Voos", "Haitham Bou Ammar"], "venue": "In Proceedings of the IEEE Multi-Conference on Systems and Control,", "citeRegEx": "Voos and Ammar.,? \\Q2010\\E", "shortCiteRegEx": "Voos and Ammar.", "year": 2010}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Multi-task reinforcement learning: a hierarchical Bayesian approach", "author": ["Aaron Wilson", "Alan Fern", "Soumya Ray", "Prasad Tadepalli"], "venue": "In Proceedings of the 24th International Conference on Machine Learning (ICML),", "citeRegEx": "Wilson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2007}, {"title": "Flexible latent variable models for multi-task learning", "author": ["Jian Zhang", "Zoubin Ghahramani", "Yiming Yang"], "venue": "Machine Learning,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "Introduction Reinforcement learning (RL) (Busoniu et al., 2010; Sutton & Barto, 1998) often requires substantial experience before achieving acceptable performance on individual control problems.", "startOffset": 41, "endOffset": 85}, {"referenceID": 10, "context": "When data is in limited supply, transfer learning can significantly improve model performance on new tasks by reusing previous learned knowledge during training (Taylor & Stone, 2009; Gheshlaghi Azar et al., 2013; Lazaric, 2011; Ferrante et al., 2008; Bou Ammar et al., 2012).", "startOffset": 161, "endOffset": 275}, {"referenceID": 20, "context": "ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008).", "startOffset": 60, "endOffset": 101}, {"referenceID": 21, "context": "ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008).", "startOffset": 60, "endOffset": 101}, {"referenceID": 1, "context": ", 2008; Bou Ammar et al., 2012). Multitask learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultaneProceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008). In the lifelong learning setting (Thrun & O\u2019Sullivan, 1996a;b), which can be framed as an online MTL problem, agents acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime. Recently, based on the work of Ruvolo & Eaton (2013) on supervised lifelong learning, Bou Ammar et al.", "startOffset": 12, "endOffset": 658}, {"referenceID": 1, "context": ", 2008; Bou Ammar et al., 2012). Multitask learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultaneProceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008). In the lifelong learning setting (Thrun & O\u2019Sullivan, 1996a;b), which can be framed as an online MTL problem, agents acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime. Recently, based on the work of Ruvolo & Eaton (2013) on supervised lifelong learning, Bou Ammar et al. (2014) developed a lifelong learner for policy gradient RL.", "startOffset": 12, "endOffset": 715}, {"referenceID": 14, "context": "Policy search methods have shown success in solving high-dimensional problems, such as robotic control (Kober & Peters, 2011; Peters & Schaal, 2008a; Sutton et al., 2000).", "startOffset": 103, "endOffset": 170}, {"referenceID": 0, "context": "Given \u03b8\u0303, the constrained solution is then determined by learning a projection into the constraint set via Bregman projections (see Abbasi-Yadkori et al. (2013)).", "startOffset": 132, "endOffset": 161}, {"referenceID": 14, "context": "Typical policy gradient methods (Kober & Peters, 2011; Sutton et al., 2000) maximize a lower bound of the expected cost ltj ( \u03b1tj ) , which can be derived by taking the logarithm and applying Jensen\u2019s inequality:", "startOffset": 32, "endOffset": 75}, {"referenceID": 19, "context": "We detail this process for two popular PG learners, eREINFORCE (Williams, 1992) and eNAC (Peters & Schaal, 2008b).", "startOffset": 63, "endOffset": 79}, {"referenceID": 19, "context": "We detail this process for two popular PG learners, eREINFORCE (Williams, 1992) and eNAC (Peters & Schaal, 2008b). The derivations of the update rules below can be found in Appendix A. These updates are governed by learning rates \u03b2 and \u03bb that decay over time; \u03b2 and \u03bb can be chosen using line-search methods as discussed by Boyd & Vandenberghe (2004). In our experiments, we adopt a simple yet effective strategy, where \u03b2 = cj\u22121 and \u03bb = cj\u22121, with 0 < c < 1.", "startOffset": 64, "endOffset": 351}, {"referenceID": 0, "context": "Using results developed by Abbasi-Yadkori et al. (2013), it is easy to see that \u2207\u03b8\u03a90 ( \u03b8\u0303j ) \u2212\u2207\u03b8\u03a90 ( \u03b8\u0303j+1 ) = \u03b7tj f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j .", "startOffset": 27, "endOffset": 56}, {"referenceID": 5, "context": "Cart Pole: The cart-pole (CP) has been used extensively as a benchmark for evaluating RL methods (Busoniu et al., 2010).", "startOffset": 97, "endOffset": 119}, {"referenceID": 1, "context": ", eNAC) and PG-ELLA (Bou Ammar et al., 2014), examining both the constrained and unconstrained variants of our algorithm. We also varied the number of iterations in our alternating optimization from 10 to 100 to evaluate the effect of these inner iterations on the performance, as shown in Figures 2 and 3. For the two MTL algorithms (our approach and PG-ELLA), the policy parameters for each task tj were initialized using the learned basis (i.e., \u03b1tj = Lstj ). We configured PG-ELLA as described by Bou Ammar et al. (2014), ensuring a fair comparison.", "startOffset": 25, "endOffset": 525}, {"referenceID": 3, "context": "To ensure realistic dynamics, we used the simulated model described by (Bouabdallah, 2007; Voos & Bou Ammar, 2010), which has been verified and used in the control of physical quadrotors.", "startOffset": 71, "endOffset": 114}, {"referenceID": 3, "context": "To ensure realistic dynamics, we used the simulated model described by (Bouabdallah, 2007; Voos & Bou Ammar, 2010), which has been verified and used in the control of physical quadrotors. We generated 10 different quadrotor systems by varying the inertia around the x, y and z-axes. We used a linear quadratic regulator, as described by Bouabdallah (2007), to initialize the policies in both the learning and testing phases.", "startOffset": 72, "endOffset": 356}], "year": 2015, "abstractText": "Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.", "creator": "LaTeX with hyperref package"}}}