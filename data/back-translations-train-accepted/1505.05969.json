{"id": "1505.05969", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2015", "title": "Learning Program Embeddings to Propagate Feedback on Student Code", "abstract": "Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University's CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.", "histories": [["v1", "Fri, 22 May 2015 07:03:45 GMT  (2433kb,D)", "http://arxiv.org/abs/1505.05969v1", "Accepted to International Conference on Machine Learning (ICML 2015)"]], "COMMENTS": "Accepted to International Conference on Machine Learning (ICML 2015)", "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.SE", "authors": ["chris piech", "jonathan huang", "andy nguyen", "mike phulsuksombati", "mehran sahami", "leonidas j guibas"], "accepted": true, "id": "1505.05969"}, "pdf": {"name": "1505.05969.pdf", "metadata": {"source": "META", "title": "Learning Program Embeddings to Propagate Feedback on Student Code", "authors": ["Chris Piech", "Jonathan Huang", "Andy Nguyen", "Mike Phulsuksombati", "Mehran Sahami"], "emails": ["PIECH@CS.STANFORD.EDU", "JONATHANHUANG@GOOGLE.COM", "TANONEV@CS.STANFORD.EDU", "MIKEP15@CS.STANFORD.EDU", "SAHAMI@CS.STANFORD.EDU", "GUIBAS@CS.STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, we will be able to establish ourselves in the region to pave the way for the future, \"he said."}, {"heading": "2. Related Work", "text": "The emergence of massive online computer courses has made the problem of automated thinking with large collections of code a major problem. There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on the use of large homework submission datasets to improve student feedback. The volume of work speaks to the importance of this problem. However, despite research efforts, the provision of qualitative feedback on a scale remains an open problem. A key challenge that a number of papers deal with measuring similarity between source code is the measurement of source code maps. Some authors have done this without explicitly characterizing the code edit distance - for example, a popular choice was Huang et al., 2013; Rogers et al., 2014). (Mokbel et al al al al al, al, al, al, 2013."}, {"heading": "3. Embedding Hoare Triples", "text": "In fact, it is a matter of a pure structure that is able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "3.1. Neural network encoding and decoding of states", "text": "We assume that preconditions have a certain basic encoding as a d-dimensional vector, which we call P. For example, in image processing courses, the state space could simply be the pixel encoding of an image, while in the discrete grid world-typical programming problems that we use in our experiments, we could choose to encode the (x, y) coordinate and discretized header of a robot by means of a linkage of one-hot encodings. Likewise, we assume that there is a basic encoding Q of the postcondition. We will focus our exposure in the rest of our paper on the case where the precondition space and the postcondition spaces share a common basic encoding. This is particularly suitable for our experimental setting, in which both the preconditions and the postconditions are representations of a grid world. In this case, we can decode the same parameters (i.e. W dec, W dec, W conditional dec, W dec and postcode spaces)."}, {"heading": "3.2. Nonparametric model of program embedding", "text": "To reconstruct the program, we must propose a simple, non-parametric model in which each program is associated with its own non-parametric matrix. (1) The total parameter substitution for our non-parametric matrix model (1) is as follows: (1) a predictive loss that quantifies how well we can predict a prediction, how well we can predict a predictive condition of a program, (2) an automatic encoding, (2) an automatic encoding, (2) an automatic encoding, (3) an automatic encoding, (3) an automatic encoding, (2) an automatic encoding, (2) an automatic encoding, (3) an automatic encoding."}, {"heading": "3.3. Triple Extraction", "text": "These tests include a variety of reasonable starting conditions. We instrumentalize the execution of the program in such a way that each time a sub-tree A-S is executed, we record the value P of all variables before execution and the value Q of all variables after execution and store the triple (P, A, Q). We run all programs on unit tests and collect triples for all sub-trees. This results in a large dataset {(Pi, Ai, Qi)} ni = 1, from which we cause equivalent triples to collapse. In practice, some sub-trees, especially the corpus of loops, generate a large (potentially infinite) number of triples. To prevent any sub-tree from having an undue influence on our model, we limit the number of triples for any sub-tree. Collecting triples on sub-trees, as opposed to merely collecting triples on complete programs, is also crucial, as it allows us to keep the overall data from ASB-only constitutionality for the program."}, {"heading": "4. Feedback Propagation", "text": "The result of collaborative learning to embed states and a corpus of programs is a fixed dimensional, real-rated matrix MA for each subtreeA of any program in our corpus. These matrices can be cooperative with machine learning algorithms that can perform tasks that go beyond predicting what a program is doing. However, the central application in this work is the force multiplication of the feedback provided by the teachers, in which an active learning algorithm interacts with human gradators, giving feedback to many more tasks than the grader comments on. However, we propose a two-phase interaction. In the first phase, the algorithm selects a subset of sample programs to apply a finite set of comments. In the second phase, the algorithm uses the annotations provided by humans as verified labels with which it can learn to predict feedback for unlabeled submissions."}, {"heading": "4.1. Incorporating structure via recursive embedding", "text": "In order to recover the elements of the program structure and style that are critical to student feedback, we use the embedding matrices we learned for the NPM model, but include all the constituent subtrees of a particular AST model. In particular, if we use the embedding matrices learned in the NPM model (which we henceforth call MNPMA for a subtree), we propose a new model based on recursive neural networks (called the NPM RNN model), in which we parameterise a matrix MA in this new model whose architecture follows the abstract syntax tree (similar to the way in which the RNN architectures might take the form of a parse tree (Socher et al)."}, {"heading": "5. Datasets", "text": "The aforementioned lcihsrc\u00fcehncS nvo edn nlrerueaeFnln uaf edn nlrcehnc\u00fcehnn nlrsAe\u00fcgn ufa nde nlrsrteeaeaFnln ufa ende nlrsrteeaeFngn uaf nde nlrsdteeaeaFnln nvo nlrf\u00fc the nlrfhdcehnc\u00fceSe, nvo the nvo nlrf\u00fc ide nlrf\u00fc ide nlrfteeaeaeaeaeFnn nlrf\u00fc nvo nlrf\u00fc the nlrrrf\u00fc the nlrf\u00fc the nlrrrf\u00fc the nlrrrrf\u00fc the nlrrrf\u00fc the nlrrrf\u00fc the nlrrf\u00fc the nlrrf\u00fc the nlrf\u00fc the nlrf\u00fc the nlllrf\u00fc the nlrf\u00fc the nllllrf\u00fc-eaeaeaeaef\u00fc nvo nlf\u00fc the nlrf\u00fc the nlrlrf\u00fc nlrf\u00fc the nlrlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc the ecrf\u00fc the nlrlrf\u00fc the nlrlrrrf\u00fc the nlrrrf\u00fc the nlrrrrrrf\u00fc the nlrrf\u00fc the nlrrrrrrf\u00fc the nlrrf\u00fc the nlrlrrf\u00fc the nlrrrrrf\u00fc the efu-crrf\u00fc the nlrlrrrf\u00fc the nlrlrrrrrrrf\u00fc the nlrlrrf\u00fc the nlrlrrrrrf\u00fc the nlrlrrfu nlrfu nlrfu nlrfu nlrfu nlrfu nl"}, {"heading": "6. Results", "text": "We are relying on a few baselines to evaluate our methods, but the most important baseline we are comparing with is a simplification of the NPM-RNN model (which we will simplify to call RNN), in which we drop the program that embeds the terms Mj from each node (cf. Equation 7).The RNN model can be trained to predict postal conditions and disseminate feedback. It has much fewer parameters than the NPM model (and thus NPM-RNN), which is a strictly parametric model and therefore should have an advantage with smaller training set systems. On the other hand, it is also a strictly less meaningful model, and therefore the question is: How much does the meaningfulness of the NPM and NPM-RNN models actually help in practice? We are pursuing this question with two tasks, among others: predicting postal conditions and propagating feedback."}, {"heading": "6.1. Prediction of postcondition", "text": "To understand how much functionality a program covers in our embedding, we evaluate the accuracy with which we can use the program to embed matrices learned through the NPM model to predict postal conditions - but note that we do not propose to use the embedding to predict postal conditions in practice. We divide our observed hoare triples into training and test sets and learn our NPM model from the training set. Then, for each tripel (P, A, Q) in the test set, we measure how well we can predict postal condition Q in the light of the corresponding program A and the precondition P. We evaluate the accuracy as the average number of state variables (e.g. row, column, orientation, and location of the beeps) predicted correctly per triple, and compare the basic method \"Common\" in addition to the RNN model, where we select the most common postal condition for a given 98 precondition with the best PM%, as the 94% problem is accurately observed in the training set of 94%."}, {"heading": "6.2. Composability of program embeddings", "text": "If we are to present programs as matrices acting on a feature space, then a natural desiderate is that they \"compose well.\" That is, if program C is functionally equivalent to executing program B followed by program A, then it should be the case that MC \u2248 MB \u00b7 MA. To assess the extent to which our program matrices can be composed, we use a corpus of 5000 programs consisting of a subroutine A followed by another subroutine B (Compose-2). We then compare the accuracy of the afterlife prognosis using the embedding of an entire program MC against the product of the embedding MB \u00b7 MA. As Table 3 shows, the accuracy when using the NPM model to predict afterlife is 94% when using the matrix for root embedding. If we use the product of two embedding matrices, we see that the accuracy does not drop dramatically, with a decoding accuracy of 94% when we are using the embedding matrix for root embedding. If we use the product of two embedding matrices, we see that the accuracy is followed by a decoding accuracy of only 92% when we are composing B, followed by a subcomposing of B, then we are followed by 92% when we are composing B."}, {"heading": "6.3. Prediction of Feedback", "text": "nI \"r\" eeiD nlrf\u00fc \"eeiD nlrf\u00fc\" eeiS, \"nlrf\u00fc ide nlrf\u00fc\" eeiD \"nlrf\u00fc\" eeiS, \"nlrf\u00fc\" rf\u00fc \"eeiD,\" nlrf\u00fc \"rf\u00fc\" eeiD, \"nlrf\u00fc\" rf\u00fc \"eeiD,\" nlrf\u00fc \"rf\u00fc\" eeiD, \"nlrf\u00fc\" rf\u00fc \"eeiD,\" rf\u00fc \"rf\u00fc\" eeiD, \"rf\u00fc\" rf\u00fc, \"nlrf\u00fc,\" nlrf\u00fc, \"nlf\u00fc,\" nlrf\u00fc, \"nlf\u00fc,\" nef\u00fc, \"nef\u00fc,\" nef\u00fc, \"nef\u00fc,\" rf\u00fc, \"D,\" rf\u00fc, \"D,\" raf\u00fc, \"rf\u00fc,\" rf\u00fc, \"D,\" rf\u00fc, \"rf\u00fc,\" naf\u00fc, \",\" rf\u00fc, \"nlf\u00fc,\", \"nlf\u00fc,\", \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\", \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \",\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \"rf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\", \"nlf\u00fc,\" nlf\u00fc, \"nlf\u00fc,\" nlf\u00fc, \""}, {"heading": "6.4. Code complexity and performance", "text": "The results of the above experiments suggest that non-parametric models perform better on more complex code, while the parametric (RNN) model performs better on simpler code. To go deeper, let's look specifically at how our performance depends on the complexity of the programs in our corpus - a question that is also central to understanding the application of our models to other tasks. We will focus on submissions to the code structure (McCabe, 1976) that cover a wide range of complexities, from simple programs to submissions with over 50 decision points (loops and if-statements).The distribution of cyclomatic complexity (McCabe, 1976), a measure of code structure, reflects this wide range (shown in grey in Figures 4 (a), (b))).First, we sort and sort all submissions to the group 3 by cyclomatic complexity in ten groups of equal size."}, {"heading": "7. Discussion", "text": "In this paper, we have presented a method for simultaneously embedding prerequisites and afterconditions in points in the common Euclidean space in which a program can be considered as a linear mapping between these points. These embeddings are predictable for the function of a program and, as we have shown, can be applied to the tasks of disseminating teacher feedback. The courses on which we evaluate our model are compelling case studies for various reasons. It is expected that tens of millions of students will use Code.org next year, which means that the ability to give autonomous feedback could affect an enormous number of people. Stanford's course, though much smaller, highlights the complexity of the code our methodology handles.Much work remains to make these embedding more generally applicable, especially for domains where we do not have tens of thousands of submissions per problem or the programs are more complex. For settings in which users can define their own variables, it would be necessary to find a novel method of storing the vector in a program space."}, {"heading": "Acknowledgments", "text": "We thank Kevin Murphy, John Mitchell, Vova Kim, Roland Angst, Steve Cooper and Justin Solomon for their critical feedback and useful discussions. We appreciate the generosity of the Code.Org team, especially Nan Li and Ellen Spertus, who provided us with data and support. Chris is supported by NSF-GRFP grant number DGE-114747."}], "references": [{"title": "Powergrading: a clustering approach to amplify human effort for short answer grading", "author": ["Basu", "Sumit", "Jacobs", "Chuck", "Vanderwende", "Lucy"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Basu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2013}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "James", "Bengio", "Yoshua"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Can recursive neural tensor networks learn logical reasoning", "author": ["Bowman", "Samuel R"], "venue": "arXiv preprint arXiv:1312.6192,", "citeRegEx": "Bowman and R.,? \\Q2013\\E", "shortCiteRegEx": "Bowman and R.", "year": 2013}, {"title": "Divide and correct: Using clusters to grade short answers at scale", "author": ["Brooks", "Michael", "Basu", "Sumit", "Jacobs", "Charles", "Vanderwende", "Lucy"], "venue": "In Proceedings of the first ACM conference on Learning@ scale conference,", "citeRegEx": "Brooks et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brooks et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Learning taskdependent distributed representations by backpropagation through structure", "author": ["Goller", "Christoph", "Kuchler", "Andreas"], "venue": "In Neural Networks,", "citeRegEx": "Goller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Goller et al\\.", "year": 1996}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "An axiomatic basis for computer programming", "author": ["Hoare", "Charles Antony Richard"], "venue": "Communications of the ACM,", "citeRegEx": "Hoare and Richard.,? \\Q1969\\E", "shortCiteRegEx": "Hoare and Richard.", "year": 1969}, {"title": "Syntactic and functional variability of a million code submissions in a machine learning mooc", "author": ["Huang", "Jonathan", "Piech", "Chris", "Nguyen", "Andy", "Guibas", "Leonidas J"], "venue": "In The 16th International Conference on Artificial Intelligence in Education (AIED", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "A complexity measure", "author": ["McCabe", "Thomas J"], "venue": "Software Engineering, IEEE Transactions on,", "citeRegEx": "McCabe and J.,? \\Q1976\\E", "shortCiteRegEx": "McCabe and J.", "year": 1976}, {"title": "Domainindependent proximity measures in intelligent tutoring systems", "author": ["Mokbel", "Bassam", "Gross", "Sebastian", "Paassen", "Benjamin", "Pinkwart", "Niels", "Hammer", "Barbara"], "venue": "In Proceedings of the 6th International Conference on Educational Data Mining (EDM),", "citeRegEx": "Mokbel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mokbel et al\\.", "year": 2013}, {"title": "Codewebs: Scalable homework search for massive open online programming courses", "author": ["Nguyen", "Andy", "Piech", "Christopher", "Huang", "Jonathan", "Guibas", "Leonidas"], "venue": "In Proceedings of the 23rd International World Wide Web Conference (WWW", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Functional maps: a flexible representation of maps between shapes", "author": ["Ovsjanikov", "Maks", "Ben-Chen", "Mirela", "Solomon", "Justin", "Butscher", "Adrian", "Guibas", "Leonidas"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "Ovsjanikov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ovsjanikov et al\\.", "year": 2012}, {"title": "Analysis and visualization of maps between shapes", "author": ["Ovsjanikov", "Maks", "Ben-Chen", "Mirela", "Chazal", "Frederic", "Guibas", "Leonidas"], "venue": "In Computer Graphics Forum,", "citeRegEx": "Ovsjanikov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ovsjanikov et al\\.", "year": 2013}, {"title": "Autonomously generating hints by inferring problem solving policies", "author": ["Piech", "Chris", "Sahami", "Mehran", "Huang", "Jonathan", "Guibas", "Leonidas"], "venue": "In Proceedings of the Second", "citeRegEx": "Piech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Piech et al\\.", "year": 2015}, {"title": "ACES: Automatic evaluation of coding style", "author": ["Rogers", "Stephanie", "Garcia", "Dan", "Canny", "John F", "Tang", "Steven", "Kang", "Daniel"], "venue": "PhD thesis,", "citeRegEx": "Rogers et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rogers et al\\.", "year": 2014}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["Song", "Le", "Huang", "Jonathan", "Smola", "Alex", "Fukumizu", "Kenji"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Kernel embeddings of conditional distributions: A unified kernel framework for nonparametric inference in graphical models", "author": ["Song", "Le", "Fukumizu", "Kenji", "Gretton", "Arthur"], "venue": "Signal Processing Magazine,", "citeRegEx": "Song et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Song et al\\.", "year": 2013}, {"title": "Learning to discover efficient mathematical identities", "author": ["Zaremba", "Wojciech", "Kurach", "Karol", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 0, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 11, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 3, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 14, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 8, "context": "Some authors have done this without an explicit featurization of the code \u2014 for example, the AST edit distance has been a popular choice (Huang et al., 2013; Rogers et al., 2014).", "startOffset": 137, "endOffset": 178}, {"referenceID": 15, "context": "Some authors have done this without an explicit featurization of the code \u2014 for example, the AST edit distance has been a popular choice (Huang et al., 2013; Rogers et al., 2014).", "startOffset": 137, "endOffset": 178}, {"referenceID": 10, "context": "(Mokbel et al., 2013) explicitly hand engineered a small collection of features on ASTs that are meant to be domainindependent.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "To incorporate functionality, (Nguyen et al., 2014) proposed a method that discovers program modifications that do not appear to change the semantic meaning of code.", "startOffset": 30, "endOffset": 51}, {"referenceID": 18, "context": "Our models are related to recent work from the NLP and deep learning communities on recursive neural networks, particularly for modeling semantics in sentences or symbolic expressions (Socher et al., 2013; 2011; Zaremba et al., 2014; Bowman, 2013).", "startOffset": 184, "endOffset": 247}, {"referenceID": 12, "context": "The computer graphics community have represented pairings of nonlinear geometric shapes as linear maps between shape features, called functional maps (Ovsjanikov et al., 2012; 2013).", "startOffset": 150, "endOffset": 181}, {"referenceID": 17, "context": "From the kernel methods literature, there has also been recent work on representations of conditional probability distributions as operators on a Hilbert space (Song et al., 2013; 2009).", "startOffset": 160, "endOffset": 185}, {"referenceID": 4, "context": "Learning rates are set using Adagrad (Duchi et al., 2011).", "startOffset": 37, "endOffset": 57}, {"referenceID": 11, "context": "Moreover, we extended this baseline by amalgamating functionally equivalent code (Nguyen et al., 2014).", "startOffset": 81, "endOffset": 102}, {"referenceID": 6, "context": "However, there has been recent progress in the deep learning community towards models capable of simulating Turing machines (Graves et al., 2014).", "startOffset": 124, "endOffset": 145}], "year": 2015, "abstractText": "Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University\u2019s CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.", "creator": "LaTeX with hyperref package"}}}