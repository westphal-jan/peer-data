{"id": "1505.06169", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2015", "title": "Learning Dynamic Feature Selection for Fast Sequential Prediction", "abstract": "We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and transition-based dependency parsing. On the typical benchmarking datasets we can preserve POS tagging accuracy above 97% and parsing LAS above 88.5% both with over a five-fold reduction in run-time, and NER F1 above 88 with more than 2x increase in speed.", "histories": [["v1", "Fri, 22 May 2015 18:28:21 GMT  (47kb,D)", "http://arxiv.org/abs/1505.06169v1", "Appears in The 53rd Annual Meeting of the Association for Computational Linguistics, Beijing, China, July 2015"]], "COMMENTS": "Appears in The 53rd Annual Meeting of the Association for Computational Linguistics, Beijing, China, July 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["emma strubell", "luke vilnis", "kate silverstein", "andrew mccallum"], "accepted": true, "id": "1505.06169"}, "pdf": {"name": "1505.06169.pdf", "metadata": {"source": "CRF", "title": "Learning Dynamic Feature Selection for Fast Sequential Prediction", "authors": ["Emma Strubell", "Luke Vilnis", "Kate Silverstein", "Andrew McCallum"], "emails": ["mccallum}@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who stand up for the rights of women and men have a hard time enforcing their rights. Indeed, most people who stand up for the rights of men and women have a hard time respecting their rights and duties. Most people who stand up for the rights of women have a hard time violating their rights and duties. Indeed, most people who stand up for the rights of men and women have a hard time exercising their rights and duties. Most people who stand up for the rights of women and men have a hard time violating their rights and duties."}, {"heading": "2 Classification and Structured Prediction", "text": "Our algorithm accelerates prediction for multi-class classification problems where the label set can be traceably listed and evaluated, and the class-specific values of the input characteristics decompose as the sum of multiple label templates. Often, classification problems in NLP are solved by using linear classifiers that calculate results for label pairs using a dot product, which meet our additive scoring criteria, and our acceleration methods are directly applicable.However, in this work we are interested in speeding up structured prediction problems, especially parts of speech (POS) tagging and dependency saving. We apply our classification algorithms to these problems by reducing them to sequential prediction (thumb \u0301 III et al., 2009). For labeling a sentence, we describe the part of speech annotation (POS) by analyzing the labeling by right-label sequencing."}, {"heading": "3 Linear models", "text": "Our basic classifier for sequential prediction tasks will be a linear model. In view of an input x-X, a series of labels Y, a characteristic map \u03a6 (x, y) and a weight vector w, a linear model predicts the highest score labeled \u043a = arg max y-Y-w \u00b7 \u03a6 (x, y). (1) The parameter w is usually learned by minimizing a regulated (R) sum of loss functions (') via the training examples indexed by iw \u0445 = arg min w-i (xi, yi, w) + R (w). In this essay, we subdivide the characteristics into a series of characteristic templates so that the weights, characteristic function and point product factor asw \u00b7 \u0435 (x, y) = \u2211 j wj \u00b7 \u03a6j (x, y) (2) can be used for a series of characteristic templates {\u03a6j (x, y)}. Our goal is to use the precursor as little as possible in Dot2."}, {"heading": "4 Method", "text": "We achieve this goal by developing paired learning and follow-up procedures for prefabricated classifiers that optimize both the accuracy and speed of conclusions by using a process of dynamic selection of characteristics. Since many decisions are easy to make on highly predictable characteristics, we want our model to use fewer templates when it is safer. For a fixed, learned sequence of feature templates, we gradually build up a vector of class values over each preset of the template sequence we call prefix values. As soon as we reach a hold criterion based on class confidence (margin), we stop calculating prefix values and predict the current highest rating class. Our goal is to classify each preset as well as possible without using the following templates and minimize the number of templates required for accurate prediction.Given this method, in order to quickly draw conclusions on an ordered set of feature templates, we leave it to us to choose the group of Yvi, in 2006, to choose the order of several ideas from Yucz, in the order of 4.In the section of Yucz)."}, {"heading": "4.1 Definitions", "text": "In view of a model that additively calculates values using template-specific scoring functions such as in (2), parameter w = and an observation x \u00b2 X, we can define the i'th prefix score for the label y \u00b2 Y as follows: Pi, y (x, w) = i \u2211 j = 1wj \u00b7 \u03a6j (x, y), or Pi, y, if the selection of observations and weights from the context is clear. If we abuse the notation, we also refer to the vector that contains all i'th prefix scores for observation x that are associated with each label in Y as Pi (x, w) or Pi, if this is unique. If we misuse a parameter m > 0, called margin, we define a function h on prefix scores: h (Pi, y) = max {0, max y \u00b2 6 = y Pi, y \u2032 \u2212 Pi, y \u00b2 \u2212 Pi (Pi, y \u00b2 \u2212 Pi, + m, alcemus} {1} Intempi = 1 marginesi \u00b2 (i.wi = 1) and input \u00b2 ii = m \u00b2."}, {"heading": "4.2 Inference", "text": "As described in Algorithm 1, we calculate prefixes at test time until a label moves ahead of all other markups with a margin m, and then predict with this label. At pull time, we predict until the correct label moves with margin m, and return the entire set of prefixes used by the learning algorithm. If no prefix values have a margin, then we predict with the final prefix value that includes all feature templates."}, {"heading": "4.3 Learning", "text": "We divide learning into two sub-problems: firstly, given an orderly sequence of feature templates and our inference procedure, we want to learn parameters that optimize accuracy while using as few of these templates as possible; secondly, given a method for training feature template classifiers, we want to learn a sequence of templates that optimizes accuracy. We want to optimize several different goals during learning: template parameters alone should have a strong predictive power, but also work well in combination with the values from later templates; and, we want to promote well calibrated confidence scores that allow us to stop predictions early without significantly reducing generating capability."}, {"heading": "4.4 Learning the parameters", "text": "To learn parameters that encourage the use of a few feature templates, we consider the model as the output not of a single prediction, but of a sequence of prefix predictions {Pi}. For each training example, each feature template receives a number of hinge loss gradients corresponding to its removal from the index, in which the margin requirement is eventually met. This corresponds to treating each prefix as a proprietary model for which we have a hinge loss function, and learning all models at the same time. Our high-level approach is used in algorithm 2.Concretely for k feature templates, we optimize the following structured margin target (with the dependence of P's on w, where explicitly helpfully written): w's = arg min w (x, y, w) '(x, y, w)' (x, y, w) = i-Margin-Margin-Margin-Margin-Margin-Margin-Margin-Margin-Margin-Margin-Margin-Margin-Margin-Target (Pi = 1 h (Pi), w, target-Margin-where-y-y-Margin-i) Margin-where-Margin-y-i (Pi) Margin-where-i-Margin-Margin-where-i-Margin-Margin-is-helpful):"}, {"heading": "4.5 Learning the template ordering", "text": "We examine three approaches to learn ordering the template."}, {"heading": "4.5.1 Group Lasso and Group Orthogonal Matching Pursuit", "text": "The Lasso Regularization Group (Yuan and Lin, 2006) penalizes the sum of the \"2 norms of weights of characteristic patterns (unlike what is commonly referred to as\" '2 \"regularization, which penalizes square' 2 norms).By varying the strength of the regularizer, we can learn a sequence of the meaning of each template for a particular model.This regularizer encourages entire groups of weights to be set to 0, whose templates can then be discarded from the model.By varying the strength of the regularizer, we can learn a sequence of the meaning of each template for a particular model.The groups contained for a given regularization strength are almost always subsets of each other (the technical conditions for this are in Hastie et al. (2007).The sequence of solutions for the varied regularization strength is referred to as the regularization pathway, and by slight misuse of terminology we use this to refer to the temparization."}, {"heading": "4.5.2 Wrapper Method", "text": "The wrapper method (Kohavi and John, 1997) is a meta-algorithm for feature selection that is normally based on a validation set. We use it in a step-by-step approach to learn a sequence of templates. Considering the sequence of the initial sub-sequence and a learning process, we add each remaining template to our ordering and estimation parameters, and select the next template that most improves the performance of the development sets. We begin the process without templates and repeat the process until we have a complete sequence across the set of feature templates. In learning the sequence, we use the same hyperparameters that are used during the final training. Although this approach is simpler than the Lasso and Matching Pursuit approaches, we have empirically found that it outperforms the others because we need to use a development set to select features for our high-dimensional application areas."}, {"heading": "5 Related Work", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "6 Experimental Results", "text": "We present experiments on three NLP tasks for which greedy sequence marking was a successful solution: part-of-speech tagging, transition-based dependency sparsing, and entity detection. In all cases, our method achieves multiplicative acceleration at test time without great loss of accuracy."}, {"heading": "6.1 Part-of-speech tagging", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a"}, {"heading": "6.1.1 Learning the template ordering", "text": "As described in Section 4.5, we experimented with part-of-speech tagging with three different algorithms to learn a sequence of feature templates: Group Lasso, Group Orthogonal Matching Pursuit (GOMP), and the wrapper method. In the case of Group Lasso, this corresponds to the experimental setup used in the evaluation of Group Lasso for NLP in Martins et al. (2011). As described in the part-of-speech tagging experiments of Appendix A, we found the wrapper method that works best in our dynamic prediction setting, so we use it in our remaining experiments to parse and name entities. Essentially, the group selects small templates too early in the order by punishing template standards, and GOMP selects large templates too early by overtaking the traction error."}, {"heading": "6.2 Transition-based dependency parsing", "text": "Our model uses a total of 60 feature templates, based mainly on the word form, the POS tag, the lemmas, and the assigned head label of the current and previous input and stack tokens, and analyzes about 300 sentences / second on a modest 2.1 GHz AMD Opteron Machine.We train our parser on Penn TreeBank, learn the parameters with AdaGrad and the parsing split, train on sections 2-21, test on section 23, and use section 22 for development and the Stanford dependency framework (deMarneffe and Manning, 2008). POS tags were automatically generated via 10-way jackknifing using the POS model described in the previous section, training on sections 2-21, testing on section 23, and using section 22 for development and the Stanford dependency framework (deMarneffe and Manning, 2008)."}, {"heading": "6.3 Named entity recognition", "text": "We implement a greedy left-to-right entity recognition based on Ratinov and Roth (2009) with a total of 46 feature templates, including surface features such as lemmas and capitalization, gazetteer look-ups, and the advanced predictive history of each token, as described in Ratinov and Roth (2009). Training, tuning, and evaluation are based on the English CoNLL 2003 dataset with the BILOU encoding for label spans. Our base model achieves F1 values of 88.35 and 93.37 in test and development sets and tags at a rate of approximately 5300 tokens per second on the hardware described above. We achieve 2.3-fold acceleration while maintaining F1 values above 88 in the test set."}, {"heading": "7 Conclusions and Future Work", "text": "By learning to dynamically select the most predictive features at test time, our algorithm offers significant speed improvements over classifier-based structured prediction algorithms, which already include even the fastest methods in NLP. In addition, these speed gains come with very low additional implementation costs and can easily be combined with existing state-of-the-art systems. Future work will eliminate the fixed order for feature templates and dynamically add additional features based on the current results of different labels."}, {"heading": "8 Acknowledgements", "text": "This work has been supported in part by the Center for Intelligent Information Retrieval, in part by DARPA under contract number FA8750-13-20020, and in part by NSF grant number CNS-0958392. The U.S. government is authorized to reproduce and distribute the reprint for government purposes, regardless of the copyright comments contained therein. All opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}, {"heading": "A Experiments: Learning Template Ordering", "text": "This year, it's gotten to the point that it will be able to retaliate until it's able to retaliate, \"he said.\" It's the way it is, \"he said.\" It's the way it is, \"he said.\" It's the way it is. \""}, {"heading": "B Sparse Regularized Group Orthogonal Matching Pursuit", "text": "At each stage, GOMP effectively uses each feature template to perform a linear regression adapted to the course of the loss function. In this experiment, we attempt to find the correlation of each feature subset with the residual characteristics of the model. We then add the feature template that best fits this course, and retrain the model. We adapt this algorithm to the setting of high-dimensional NLP problems by efficiently inverting the covariance matrices of the feature templates and regulating the calculation of the residual correlation, resulting in a scalable feature selection technique for our problem definition, detailed below. For the purposes of exposure, we break from the notation of Section 3, where the features of x and y are combined, as the algorithm is designed from a linear-algebraic regression point that considers the design matrix and the label of Y as a separate group."}], "references": [{"title": "Understanding the value of features for coreference resolution", "author": ["Eric Bengtson", "Dan Roth."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 294\u2013303. Association for Computational Linguistics.", "citeRegEx": "Bengtson and Roth.,? 2008", "shortCiteRegEx": "Bengtson and Roth.", "year": 2008}, {"title": "Classifier cascade for minimizing feature evaluation cost", "author": ["Minmin Chen", "Zhixiang \u201cEddie\u201d Xu", "Kilian Q Weinberger", "Olivier Chappele", "Dor Kedem"], "venue": "In AISTATS", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Getting the Most out of Transition-based Dependency Parsing", "author": ["Jinho Choi", "Martha Palmer."], "venue": "Association for Computational Linguistics, pages 687\u2013 692.", "citeRegEx": "Choi and Palmer.,? 2011", "shortCiteRegEx": "Choi and Palmer.", "year": 2011}, {"title": "Fast and robust part-of-speech tagging using dynamic model selection", "author": ["Jinho Choi", "Martha Palmer."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Choi and Palmer.,? 2012", "shortCiteRegEx": "Choi and Palmer.", "year": 2012}, {"title": "Search-based structured prediction", "author": ["Hal Daum\u00e9 III", "John Langford", "Daniel Marcu."], "venue": "Machine Learning, 75(3):297\u2013325.", "citeRegEx": "III et al\\.,? 2009", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "The stanford typed dependencies representation", "author": ["Marie-Catherine de Marneffe", "Christopher D. Manning."], "venue": "COLING 2008 Workshop on Crossframework and Cross-domain Parser Evaluation.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "JMLR, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "The Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Svmtool: A general pos tagger generator based on support vector machines", "author": ["Jes\u00fas Gim\u00e9nez", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "In Proceedings of the 4th LREC,", "citeRegEx": "Gim\u00e9nez and M\u00e0rquez.,? \\Q2004\\E", "shortCiteRegEx": "Gim\u00e9nez and M\u00e0rquez.", "year": 2004}, {"title": "SpeedBoost: Anytime Prediction with Uniform Near-Optimality", "author": ["Alexander Grubb", "J. Andrew Bagnell."], "venue": "AISTATS.", "citeRegEx": "Grubb and Bagnell.,? 2012", "shortCiteRegEx": "Grubb and Bagnell.", "year": 2012}, {"title": "Forward stagewise regression and the monotone lasso", "author": ["Trevor Hastie", "Jonathan Taylor", "Robert Tibshirani", "Guenther Walther."], "venue": "Electronic Journal of Statistics, 1:1\u201329.", "citeRegEx": "Hastie et al\\.,? 2007", "shortCiteRegEx": "Hastie et al\\.", "year": 2007}, {"title": "Cost-sensitive dynamic feature selection", "author": ["He He", "Jason Eisner."], "venue": "ICML Workshop on Inferning: Interactions between Inference and Learning.", "citeRegEx": "He and Eisner.,? 2012", "shortCiteRegEx": "He and Eisner.", "year": 2012}, {"title": "Dynamic feature selection for dependency parsing", "author": ["He He", "Hal Daum\u00e9 III", "Jason Eisner."], "venue": "EMNLP.", "citeRegEx": "He et al\\.,? 2013", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "A Non-Monotonic Arc-Eager Transition System for Dependency Parsing", "author": ["M Honnibal", "Y Goldberg."], "venue": "CoNLL.", "citeRegEx": "Honnibal and Goldberg.,? 2013", "shortCiteRegEx": "Honnibal and Goldberg.", "year": 2013}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H John."], "venue": "Artificial Intelligence, 97(1):273\u2013324.", "citeRegEx": "Kohavi and John.,? 1997", "shortCiteRegEx": "Kohavi and John.", "year": 1997}, {"title": "Group orthogonal matching pursuit for logistic regression", "author": ["Aur\u00e9lie C Lozano", "Grzegorz Swirszcz", "Naoki Abe."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 452\u2013460.", "citeRegEx": "Lozano et al\\.,? 2011", "shortCiteRegEx": "Lozano et al\\.", "year": 2011}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Structured sparsity in structured prediction", "author": ["Andr\u00e9 Martins", "Noah Smith", "Pedro Aguiar", "M\u00e1rio Figueiredo."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1500\u20131511. Association for Computa-", "citeRegEx": "Martins et al\\.,? 2011", "shortCiteRegEx": "Martins et al\\.", "year": 2011}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, vol-", "citeRegEx": "Nivre.,? 2009", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 147\u2013 155. Association for Computational Linguistics.", "citeRegEx": "Ratinov and Roth.,? 2009", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J. Gordon", "Drew Bagnell."], "venue": "Geoffrey J. Gordon, David B. Dunson, and Miroslav Dud\u0131\u0301k, editors, AISTATS, volume 15 of JMLR Pro-", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Vine pruning for efficient multi-pass dependency parsing", "author": ["Alexander M Rush", "Slav Petrov."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Rush and Petrov.,? 2012", "shortCiteRegEx": "Rush and Petrov.", "year": 2012}, {"title": "Grouped orthogonal matching pursuit for variable selection and prediction", "author": ["Grzegorz Swirszcz", "Naoki Abe", "Aurelie C Lozano."], "venue": "Advances in Neural Information Processing Systems, pages 1150\u20131158.", "citeRegEx": "Swirszcz et al\\.,? 2009", "shortCiteRegEx": "Swirszcz et al\\.", "year": 2009}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer."], "venue": "HLT-NAACL.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Supervised sequential classification under budget constraints", "author": ["Kirill Trapeznikov", "Venkatesh Saligrama."], "venue": "AISTATS.", "citeRegEx": "Trapeznikov and Saligrama.,? 2013", "shortCiteRegEx": "Trapeznikov and Saligrama.", "year": 2013}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Ioannis Tsochantaridis", "Thomas Hofmann", "Thorsten Joachims", "Yasemin Altun."], "venue": "Proceedings of the Twenty-first International Conference on Machine Learning, page", "citeRegEx": "Tsochantaridis et al\\.,? 2004", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["Paul Viola", "Michael Jones."], "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 1, pages I\u2013511. IEEE.", "citeRegEx": "Viola and Jones.,? 2001", "shortCiteRegEx": "Viola and Jones.", "year": 2001}, {"title": "Structured prediction cascades", "author": ["David Weiss", "Ben Taskar."], "venue": "AISTATS.", "citeRegEx": "Weiss and Taskar.,? 2010", "shortCiteRegEx": "Weiss and Taskar.", "year": 2010}, {"title": "Learning adaptive value of information for structured prediction", "author": ["David Weiss", "Ben Taskar."], "venue": "NIPS.", "citeRegEx": "Weiss and Taskar.,? 2013", "shortCiteRegEx": "Weiss and Taskar.", "year": 2013}, {"title": "Dual Averaging Method for Regularized Stochastic Learning and Online Optimization", "author": ["Lin Xiao."], "venue": "NIPS.", "citeRegEx": "Xiao.,? 2009", "shortCiteRegEx": "Xiao.", "year": 2009}, {"title": "Cost-sensitive tree of classifiers", "author": ["Zhixiang \u201cEddie\u201d Xu", "Matt J Kusner", "Kilian Q Weinberger", "Minmin Chen"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["Ming Yuan", "Yi Lin."], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49\u201367.", "citeRegEx": "Yuan and Lin.,? 2006", "shortCiteRegEx": "Yuan and Lin.", "year": 2006}, {"title": "wrapper). We use the methods of setting regularization parameters for Lasso and GOMP discussed in Section 4.5. Note that for the case of Group Lasso, this corresponds to the experimental setup used when evaluating Group Lasso for NLP", "author": ["Martins"], "venue": null, "citeRegEx": "Martins,? \\Q2011\\E", "shortCiteRegEx": "Martins", "year": 2011}], "referenceMentions": [{"referenceID": 26, "context": "Viola and Jones (2001) use a cascade of boosted models to perform face detection.", "startOffset": 0, "endOffset": 23}, {"referenceID": 26, "context": "Viola and Jones (2001) use a cascade of boosted models to perform face detection. Weiss and Taskar (2010) add increasingly higher-order dependencies to a graphical model while filtering the outar X iv :1 50 5.", "startOffset": 0, "endOffset": 106}, {"referenceID": 27, "context": "Most similarly to our work, Weiss and Taskar (2013) improve performance for several structured vision tasks by dynamically selecting features at runtime.", "startOffset": 28, "endOffset": 52}, {"referenceID": 8, "context": "For POS tagging, we describe a sentence\u2019s part of speech annotation by the left-to-right sequence of tagging decisions for individual tokens (Gim\u00e9nez and M\u00e0rquez, 2004).", "startOffset": 141, "endOffset": 168}, {"referenceID": 18, "context": "Similarly, we implement our parser with a classifier that generates a sequence of shift-reduce parsing transitions (Nivre, 2009).", "startOffset": 115, "endOffset": 128}, {"referenceID": 20, "context": ", 2009) and DAgger (Ross et al., 2011) are two popular principled frameworks for reducing sequential prediction to classification by learning a classifier on additional synthetic training data.", "startOffset": 19, "endOffset": 38}, {"referenceID": 0, "context": "However, as we do in our experiments, practitioners often see good results by training on the gold standard labels with an off-the-shelf classification algorithm, as though classifying IID data (Bengtson and Roth, 2008; Choi and Palmer, 2012).", "startOffset": 194, "endOffset": 242}, {"referenceID": 3, "context": "However, as we do in our experiments, practitioners often see good results by training on the gold standard labels with an off-the-shelf classification algorithm, as though classifying IID data (Bengtson and Roth, 2008; Choi and Palmer, 2012).", "startOffset": 194, "endOffset": 242}, {"referenceID": 31, "context": "5, we develop several methods for picking template orderings, based on ideas from group sparsity (Yuan and Lin, 2006; Swirszcz et al., 2009), and other techniques for feature subset-selection (Kohavi and John, 1997).", "startOffset": 97, "endOffset": 140}, {"referenceID": 22, "context": "5, we develop several methods for picking template orderings, based on ideas from group sparsity (Yuan and Lin, 2006; Swirszcz et al., 2009), and other techniques for feature subset-selection (Kohavi and John, 1997).", "startOffset": 97, "endOffset": 140}, {"referenceID": 14, "context": ", 2009), and other techniques for feature subset-selection (Kohavi and John, 1997).", "startOffset": 59, "endOffset": 82}, {"referenceID": 25, "context": "This is the familiar structured hinge loss function as in structured support vector machines (Tsochantaridis et al., 2004), which has a minimum at 0 if and only if class y is ranked ahead of all other classes by at least m.", "startOffset": 93, "endOffset": 122}, {"referenceID": 31, "context": "The Group Lasso regularizer (Yuan and Lin, 2006) penalizes the sum of `2-norms of weights of feature templates (different from what is commonly called \u201c`2\u201d regularization, penalizing squared `2 norms), \u2211 i ci\u2016wi\u20162, where ci is a weight for each template.", "startOffset": 28, "endOffset": 48}, {"referenceID": 10, "context": "The included groups for a given regularization strength are nearly always subsets of one another (technical conditions for this to be true are given in Hastie et al. (2007)).", "startOffset": 152, "endOffset": 173}, {"referenceID": 22, "context": "An alternative and related approach to learning template orderings is based on the Group Orthogonal Matching Pursuit (GOMP) algorithm for generalized linear models (Swirszcz et al., 2009; Lozano et al., 2011), with a few modifications for the setting of high-dimensional, sparse NLP data (described in Appendix B).", "startOffset": 164, "endOffset": 208}, {"referenceID": 15, "context": "An alternative and related approach to learning template orderings is based on the Group Orthogonal Matching Pursuit (GOMP) algorithm for generalized linear models (Swirszcz et al., 2009; Lozano et al., 2011), with a few modifications for the setting of high-dimensional, sparse NLP data (described in Appendix B).", "startOffset": 164, "endOffset": 208}, {"referenceID": 10, "context": "Orthogonal matching pursuit algorithms are a set of stagewise feature selection techniques similar to forward stagewise regression (Hastie et al., 2007) and LARS (Efron et al.", "startOffset": 131, "endOffset": 152}, {"referenceID": 7, "context": ", 2007) and LARS (Efron et al., 2004).", "startOffset": 17, "endOffset": 37}, {"referenceID": 14, "context": "The wrapper method (Kohavi and John, 1997) is a meta-algorithm for feature selection, usually based on a validation set.", "startOffset": 19, "endOffset": 42}, {"referenceID": 28, "context": ") In contrast, other work (Weiss and Taskar, 2013; He et al., 2013) learns a separate classifier to determine when to add features.", "startOffset": 26, "endOffset": 67}, {"referenceID": 12, "context": ") In contrast, other work (Weiss and Taskar, 2013; He et al., 2013) learns a separate classifier to determine when to add features.", "startOffset": 26, "endOffset": 67}, {"referenceID": 12, "context": ") In contrast, other work (Weiss and Taskar, 2013; He et al., 2013) learns a separate classifier to determine when to add features. Such heavier-weight approaches are unsuitable for our setting, where the core classifier\u2019s features and scoring are already so cheap that adding complex decision-making would cause too much computational overhead. Other previous work on cascades uses a series of increasingly complex models, such as the Viola-Jones face detection cascade of classifiers (2001), which applies boosted trees trained on subsets of features in increasing order of complexity as needed, aiming to reject many sub-image windows early in processing.", "startOffset": 51, "endOffset": 493}, {"referenceID": 9, "context": "Our work is also related to the field of learning and inference under test-time budget constraints (Grubb and Bagnell, 2012; Trapeznikov and Saligrama, 2013).", "startOffset": 99, "endOffset": 157}, {"referenceID": 24, "context": "Our work is also related to the field of learning and inference under test-time budget constraints (Grubb and Bagnell, 2012; Trapeznikov and Saligrama, 2013).", "startOffset": 99, "endOffset": 157}, {"referenceID": 28, "context": "g vision) and the extra computation of an auxiliary pruning-decision model is offset by substantial reduction in feature computations (Weiss and Taskar, 2013).", "startOffset": 134, "endOffset": 158}, {"referenceID": 26, "context": "For example, Xu et al. (2013) learn a tree of classifiers that subdivides the set of classes to minimize average testtime cost.", "startOffset": 13, "endOffset": 30}, {"referenceID": 1, "context": "Chen et al. (2012) similarly use a linear cascade instead of a tree.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Chen et al. (2012) similarly use a linear cascade instead of a tree. Weiss and Taskar (2010) prune output labels in the context of structured prediction through a cascade of increasingly complex models, and Rush and Petrov (2012) successfully apply these structured prediction cascades to the task of graph-based dependency parsing.", "startOffset": 0, "endOffset": 93}, {"referenceID": 1, "context": "Chen et al. (2012) similarly use a linear cascade instead of a tree. Weiss and Taskar (2010) prune output labels in the context of structured prediction through a cascade of increasingly complex models, and Rush and Petrov (2012) successfully apply these structured prediction cascades to the task of graph-based dependency parsing.", "startOffset": 0, "endOffset": 230}, {"referenceID": 12, "context": "In the context of NLP, He et al. (2013) describe a method for dynamic feature template selection at test time in graph-based dependency parsing.", "startOffset": 23, "endOffset": 40}, {"referenceID": 31, "context": "The Group Lasso regularizer (Yuan and Lin, 2006) sparsifies groups of feature weights (e.", "startOffset": 28, "endOffset": 48}, {"referenceID": 17, "context": "from the applications of group sparsity, such as the work of Martins et al. (2011) in Group Lasso for NLP problems.", "startOffset": 61, "endOffset": 83}, {"referenceID": 16, "context": "We evaluate our models on the Penn Treebank WSJ corpus (Marcus et al., 1993), employing the typical split of sections used for part-of-speech tagging: 0-18 train, 19-21 development, 22-24 test.", "startOffset": 55, "endOffset": 76}, {"referenceID": 6, "context": "The parameters of our models are learned using AdaGrad (Duchi et al., 2011) with `2 regularization via regularized dual averaging (Xiao, 2009), and we used random search on the development set to select hyperparameters.", "startOffset": 55, "endOffset": 75}, {"referenceID": 29, "context": ", 2011) with `2 regularization via regularized dual averaging (Xiao, 2009), and we used random search on the development set to select hyperparameters.", "startOffset": 62, "endOffset": 74}, {"referenceID": 2, "context": "Our baseline tagger uses the same features described in Choi and Palmer (2012). We evaluate our models on the Penn Treebank WSJ corpus (Marcus et al.", "startOffset": 56, "endOffset": 79}, {"referenceID": 8, "context": "1GHz AMD Opteron machine with accuracy comparable to similar taggers (Gim\u00e9nez and M\u00e0rquez, 2004; Choi and Palmer, 2012; Toutanova et al., 2003).", "startOffset": 69, "endOffset": 143}, {"referenceID": 3, "context": "1GHz AMD Opteron machine with accuracy comparable to similar taggers (Gim\u00e9nez and M\u00e0rquez, 2004; Choi and Palmer, 2012; Toutanova et al., 2003).", "startOffset": 69, "endOffset": 143}, {"referenceID": 23, "context": "1GHz AMD Opteron machine with accuracy comparable to similar taggers (Gim\u00e9nez and M\u00e0rquez, 2004; Choi and Palmer, 2012; Toutanova et al., 2003).", "startOffset": 69, "endOffset": 143}, {"referenceID": 17, "context": "For the case of Group Lasso, this corresponds to the experimental setup used when evaluating Group Lasso for NLP in Martins et al. (2011). As detailed in the part-of-speech tagging experiments of Appendix A, we found the wrapper method to work best in our dynamic prediction setting.", "startOffset": 116, "endOffset": 138}, {"referenceID": 2, "context": "We base our parsing experiments on the greedy, non-projective transition-based dependency parser described in Choi and Palmer (2011). Our model uses a total of 60 feature templates based mainly on the word form, POS tag, lemma and assigned head label of current and previous input and stack tokens, and parses about 300 sentences/second on a modest 2.", "startOffset": 110, "endOffset": 133}, {"referenceID": 2, "context": "83, which are comparable to similar greedy parsers (Choi and Palmer, 2011; Honnibal and Goldberg, 2013).", "startOffset": 51, "endOffset": 103}, {"referenceID": 13, "context": "83, which are comparable to similar greedy parsers (Choi and Palmer, 2011; Honnibal and Goldberg, 2013).", "startOffset": 51, "endOffset": 103}, {"referenceID": 19, "context": "We implement a greedy left-to-right named entity recognizer based on Ratinov and Roth (2009) using a total of 46 feature templates, including surface features such as lemma and capitalization, gazetteer look-ups, and each token\u2019s extended prediction history, as described in (Ratinov and Roth, 2009).", "startOffset": 275, "endOffset": 299}, {"referenceID": 19, "context": "We implement a greedy left-to-right named entity recognizer based on Ratinov and Roth (2009) using a total of 46 feature templates, including surface features such as lemma and capitalization, gazetteer look-ups, and each token\u2019s extended prediction history, as described in (Ratinov and Roth, 2009).", "startOffset": 69, "endOffset": 93}], "year": 2015, "abstractText": "We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and transition-based dependency parsing. On the typical benchmarking datasets we can preserve POS tagging accuracy above 97% and parsing LAS above 88.5% both with over a five-fold reduction in run-time, and NER F1 above 88 with more than 2x increase in speed.", "creator": "TeX"}}}