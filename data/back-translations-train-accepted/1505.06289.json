{"id": "1505.06289", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2015", "title": "Text to 3D Scene Generation with Rich Lexical Grounding", "abstract": "The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.", "histories": [["v1", "Sat, 23 May 2015 08:32:11 GMT  (3282kb,D)", "https://arxiv.org/abs/1505.06289v1", "10 pages, 7 figures, 3 tables. To appear in ACL-IJCNLP 2015 (pre-camera-ready draft)"], ["v2", "Fri, 5 Jun 2015 01:13:17 GMT  (3276kb,D)", "http://arxiv.org/abs/1505.06289v2", "10 pages, 7 figures, 3 tables. To appear in ACL-IJCNLP 2015"]], "COMMENTS": "10 pages, 7 figures, 3 tables. To appear in ACL-IJCNLP 2015 (pre-camera-ready draft)", "reviews": [], "SUBJECTS": "cs.CL cs.GR", "authors": ["angel x chang", "will monroe", "manolis savva", "christopher potts", "christopher d manning"], "accepted": true, "id": "1505.06289"}, "pdf": {"name": "1505.06289.pdf", "metadata": {"source": "CRF", "title": "Text to 3D Scene Generation with Rich Lexical Grounding", "authors": ["Angel Chang", "Will Monroe", "Manolis Savva", "Christopher Potts", "Christopher D. Manning"], "emails": ["angelx@cs.stanford.edu,", "wmonroe4@cs.stanford.edu,", "msavva@cs.stanford.edu,", "cgpotts@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The ability to create descriptions of scenes to 3D geometric representations has a variety of applications; many creative industries use 3D scenes. Robotics applications must interpret commands that refer to real environments, and the ability to visualize scenarios is of great practical use in educational tools. Unfortunately, 3D scene design user interfaces are immeasurably complex for users. Before work, the task remains difficult and time-consuming for non-experts, even with simplified interfaces (Savva et al., 2014)."}, {"heading": "2 Task Description", "text": "In the 3D scene creation task text, the input is a natural language description, and the output is a 3D representation of a plausible scene that matches the description and can be viewed and rendered from multiple perspectives. Specifically, the output is a scene y: an array of 3D models that represent objects at specific positions and orientations in space. In this essay, we will focus on the subproblem of lexical grounding of text terms to 3D model referents (i.e., the selection of 3D models that represent objects indicated by terms in the input utterance x).We will use a clipboard representation that is evaluated from the input text to capture the physical objects present in a scene and the constraints between them. This representation is then used to create a 3D scene (Figure 2).A naive approach to scene creation of 3D objects could use the key word search to retrieve 3D models."}, {"heading": "3 Related Work", "text": "The task of image recovery is somewhat similar to our task in that 3D scene restoration is an approach that can allow an approximation of the creation of 3D scenes. However, there are fundamental differences between 2D images and 3D scenes. Creation in the image space focuses mainly on the composition of simple 2D clip art elements, as was recently illustrated by Zitnick et al. (2013). The task of composing 3D scenes represents a much higher-dimensional search space of scene configurations, where it is difficult to find plausible and desirable configurations. Unlike previous work in clip art creation, where a small predetermined group of objects is used, we have created a large database of objects that can occur in different interiors: 12490 3D models from about 270 categories."}, {"heading": "3.1 Text to Scene Systems", "text": "Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated the linguistic manipulation of objects in 3D scenes. However, the range of discourse was limited to a micro-world with simple geometric shapes to facilitate parsing and grounding natural speech input. More recently, prototype texts were developed for 3D scene generation systems for wider areas, especially for the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006). Chang et al. (2014) showed that it is possible to learn spatial priors for objects and relationships directly from 3D scene data. These systems use manually defined mappings between language and its representation of the physical world, preventing generalization to more complex object descriptions, variations in word choice and spelling, and other languages. It also forces users to use unnatural language to express their intention (e.g. the table is two meters south of the window)."}, {"heading": "3.2 Related Tasks", "text": "Previous work has produced sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and refer to specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, the generation of scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate layer of structure between raw image pixels and simpler microcosms (e.g. grid and block worlds).This level of structure is suitable for the generational task, but still realistic enough to represent a variety of challenges associated with natural scenarios. A related line of work focuses on grounding reference expressions for references in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005)."}, {"heading": "4 Dataset", "text": "We are introducing a new dataset of 1128 scenes and 4284 freeform descriptions of these scenes in natural language (each model has a category label and has a unique ID).To create this training set, we used a simple online scene design interface that allows users to assemble scenes based on available 3D models of common household items (each model has a category label and has a unique ID).To get more diverse descriptions for each scene, we asked other workers to describe each scene. Figure 3 shows examples of seed description sets, 3D scenes created by humans in response to these descriptions, and new descriptions created by others when viewing the scenes.We manually examined a random subset of descriptions (about 10%) to eliminate spam and unacceptably bad descriptions."}, {"heading": "5 Model", "text": "To create a model for creating scene templates from text, we train a classifier to lexical1Available at http: / / nlp.stanford.edu / data / text2scene.shtml.2Mean 26.2 words, SD 17.4; versus averages 16.6, SD 7.2 for seed sets. Looking at seed sets as a \"reference,\" the macro-averaged BLEU score (Papineni et al., 2002) of Turker descriptions is 12.0.groundings. We then combine our learned lexical basics with a rules-based scenario creation model. The learned basics allow us to select better models, while the rules-based model provides easy compositionality for dealing with correlation and relationships between objects."}, {"heading": "5.1 Learning lexical groundings", "text": "To learn lexical assignments using examples, we train a classifier for a related grounding task and extract the weights of lexical characteristics for scene creation. This classifier learns from a \"discriminatory version\" of our scene dataset in which the scene is hidden in each scene description pair between four other randomly sampled distraction scenes. The goal of the training is to maximize the L2-regulated log probability of this scene dataset under a single dataset. All logistic regression models use each true scene and each distraction scene as an example (with True / Distractor as the output marker).The learned model uses binary rated features that specify the simultaneous occurrence of a Unigram or Bigram dataset and an object category or model ID. Thus, for example, features derived from the scene description and each distraction scene extraction category are presented as Notepad (Notepad), Notepad (the Notepad):"}, {"heading": "5.2 Rule-based Model", "text": "We use the rule-based parsing component described in Chang et al. (2014). This system contains knowledge that is important for the creation of scenes and is not taken into account by our learned model (e.g. spatial relations and co-references). In Section 5.3 we describe how we use our learned model to extend this model.This rule-based approach is a three-step process using established NLP systems: 1) The input text is split into several sentences and analyzed using the Stanford CoreNLP pipeline (Manning etal., 2014). Headers of noun phrases are identified as object categories, filtered using WordNet (Miller, 1995) to include only physical objects. 2) References to the same object are broken down using the Stanford CoreNLP pipeline. 3) Properties are appended to each object by extracting other adjectives and nouns from the noun phrase. These properties are later used to query the 3D database."}, {"heading": "5.3 Combined Model", "text": "We integrate our learned lexical basics into this model to provide an improved scene generation system.Identification of object categories Using the rules-based model, we extract all noun phrases as potential objects. For each noun phrase p, we extract properties {\u03c6i} and calculate the score of a category c described by the noun phrase as the sum of the property weights from the learned model in Section 5.1: Score (c | p) = Looking score for the noun phrase p (i, c), where \u03b8 (i, c) is the weight for attributing attributes to category c. From categories with a score higher than Tc = 0.5, we select the best rated category as representative for the noun phrase. If no category exceeds the score as Tc, we use the header of the noun for the object category p.3D model selection For each object that is recognized in the description, we use the noun phrase as the best representative for the description."}, {"heading": "6 Learned lexical groundings", "text": "By extracting high-weight features from our learned model, we can visualize specific models on which lexical terms are based (see Figure 4). These features correspond to high-frequency text and 3D model pairs within the scene corpus. Table 1 shows some of the best-learned lexical basics for model database categories. We are able to restore many simple identity mappings without using lexical similarity features, and we capture several lexical variants (e.g. sofa for couch). Some erroneous mappings reflect frequent coincidences; for example, fruits are mapped into bowls based on fruits typically observed in bowls in our dataset."}, {"heading": "7 Experimental Results", "text": "We perform a human judgement experiment in order to compare the quality of the generated scenes with the approaches and basic methods presented by us. To assess whether lexical grounding improves scene generation, we need a method to arrange the selected models in 3D scenes. Since 3D scene layout is not a focal point of our work, we use an approach based on previous work in 3D scene synthesis and text generation (Fisher et al., 2012; Chang et al., 2014), simplified by sampling and not by a mountaineering strategy. Conditions We compare five conditions: {random, learned, rule, combination, human}. The random condition represents a baseline that synthesizes a scene with randomly selected models, while the human represents scenes created by humans. The learned condition takes our learned lexicalic bases, selects the four most likely objects from, and generates a scene based on them, and the rules and conditions are created by them, the composited, the rules and conditions are modified."}, {"heading": "7.1 Qualitative Evaluation", "text": "Figure 5 shows a qualitative comparison of 3D scenes generated from sample input descriptions using each of the four methods. In the top row, the rules-based approach selects a CPU chassis for the computer, while combo and learner select a more iconic monitor. In the bottom row, the rules-based approach selects two newspapers and places them on the floor, while the combined approach correctly selects a coffee table with two newspapers on it. The learned model is limited to four objects and has no idea of object identity, so it often duplicates objects."}, {"heading": "7.2 Human Evaluation", "text": "We conducted an experiment in which people rated the degree to which the scenes matched the text descriptions from which they were generated. Such ratings are a natural way to assess how well our approach can generate scenes from the text: in practice, a person would provide an input description and then randomly assess the suitability of the resulting scenes. For the MTurk descriptions, we randomly sampled 100 descriptions from the development of our data sets. Procedures During the experiment, each participant was shown 30 pairs of scene descriptions randomly drawn from all five conditions. All participants provided 30 responses to a total of 5040 scene descriptions. Participants were asked to rate how well the generated scene was matched on a 7-point scale by Likert, with a poor match and a very good match (see Figure 6)."}, {"heading": "7.3 Error Analysis", "text": "Figure 7 shows some common errors in our system. The upper left scene was created using the rules-based method, the upper right scene with the learned method, and the lower two with the combined approach. At the top left is an erroneous selection of specific object categories (logs) for the four wooden chairs in the input description due to incorrect head identification. At the top right, the learned model identifies the lme4 R package and optimizes the fit with maximum log probability (Baayen et al., 2008). We report significance results based on probability ratio (LR) tests. The presence of a brown desk and a brown lamp, but our system mistakenly selects two desks and two lamps (since we always select the top four objects). The lower right scene does not adhere to the expressed spatial constraints (in the corner of the room) because our system does not understand the grounding of the room corner and the upper right side of the door does not fit well due to the lower resolution."}, {"heading": "7.4 Scene Similarity Metric", "text": "We introduce an automated measure of the evaluation of scenes represented by a scene template, the aligned similarity of the scene template (ASTS). Given a 1: 1 alignment A between the nodes of a scene template and the objects in a scene, we leave the alignment penalty J (A) to be the sum of the number of unaligned nodes in the scene template and the number of unaligned objects in the scene. For the aligned nodes, we calculate a similarity value S per node pair (n, n \u2032), where S (n, n \u2032) = 1 if the model identifier matches, S (n, n \u2032) = 0.5 if only the category matches and 0 otherwise. We define the ASTS of a scene in relation to a scene template as the maximum alignment value across all such alignments: ASTS (s, z) = max. A < S (n, n \u2032) J (A)."}, {"heading": "8 Future Work", "text": "One obvious improvement would be to expand our learned lexical grounding approach to include spatial relationships. One approach would be to add spatial constraints to the definition of our scene similarity and use this improved metric in the formation of a semantic parser to generate scene templates. To select objects, our current system uses information derived from language object matches and sparsely annotated category labels. Another promising way to achieve better lexical grounding is to distribute category labels using geometric and image functions to learn the categories of unlabeled objects. Novel categories can also be extracted from Turkish descriptions to improve the annotations in our 3D model database."}, {"heading": "9 Conclusion", "text": "We present a dataset of 3D scenes with natural language descriptions that we believe are of great interest to the research community. Using this corpus of scenes and descriptions, we present an approach that uses data to learn how to associate text descriptions with objects. To evaluate how our grounding approach impacts the generated scene quality, we collect human judgments about generated scenes. In addition, we present a metric for automatically comparing generated scene templates with scenes and show that it strongly correlates with human judgments. We show that rich lexical creation can be learned directly from an unaligned corpus of 3D scenes and natural language descriptions, and that our model successfully compares lexical terms with concrete references, improving scene creation compared to previous work."}, {"heading": "Acknowledgments", "text": "We thank Katherine Breeden for valuable feedback. The authors are grateful for the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA875013-2-0040, the National Science Foundation under grant no. IIS 1159679, the Department of the Navy, Office of Naval Research, under grant no. N00014-10-1-0109, and the Stanford Graduate Fellowship Fund. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation, the Office of Naval Research, DARPA, AFRL or the U.S. Government."}], "references": [{"title": "Mixed-effects modeling with crossed random effects for subjects and items", "author": ["R.H. Baayen", "D.J. Davidson", "D.M. Bates."], "venue": "Journal of Memory and Language, 59(4):390\u2013412.", "citeRegEx": "Baayen et al\\.,? 2008", "shortCiteRegEx": "Baayen et al\\.", "year": 2008}, {"title": "Learning spatial knowledge for text to 3D scene generation", "author": ["Angel X. Chang", "Manolis Savva", "Christopher D. Manning."], "venue": "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chang et al\\.,? 2014", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "WordsEye: an automatic text-to-scene conversion system", "author": ["Bob Coyne", "Richard Sproat."], "venue": "Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques.", "citeRegEx": "Coyne and Sproat.,? 2001", "shortCiteRegEx": "Coyne and Sproat.", "year": 2001}, {"title": "Annotation tools and knowledge representation for a text-to-scene system", "author": ["Bob Coyne", "Alexander Klapheke", "Masoud Rouhizadeh", "Richard Sproat", "Daniel Bauer."], "venue": "Proceedings of COLING 2012: Technical Papers.", "citeRegEx": "Coyne et al\\.,? 2012", "shortCiteRegEx": "Coyne et al\\.", "year": 2012}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth."], "venue": "Computer Vision\u2013ECCV 2010.", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Example-based synthesis of 3D object arrangements", "author": ["Matthew Fisher", "Daniel Ritchie", "Manolis Savva", "Thomas Funkhouser", "Pat Hanrahan."], "venue": "ACM Transactions on Graphics (TOG), 31(6):135.", "citeRegEx": "Fisher et al\\.,? 2012", "shortCiteRegEx": "Fisher et al\\.", "year": 2012}, {"title": "Learning distributions over logical forms for referring expression generation", "author": ["Nicholas FitzGerald", "Yoav Artzi", "Luke Zettlemoyer."], "venue": "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "FitzGerald et al\\.,? 2013", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2013}, {"title": "A game-theoretic approach to generating spatial descriptions", "author": ["Dave Golland", "Percy Liang", "Dan Klein."], "venue": "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Golland et al\\.,? 2010", "shortCiteRegEx": "Golland et al\\.", "year": 2010}, {"title": "Grounded semantic composition for visual scenes", "author": ["Peter Gorniak", "Deb Roy."], "venue": "Journal of Artificial Intelligence Research (JAIR), 21(1):429\u2013470.", "citeRegEx": "Gorniak and Roy.,? 2004", "shortCiteRegEx": "Gorniak and Roy.", "year": 2004}, {"title": "Probabilistic grounding of situated speech using plan recognition and reference resolution", "author": ["Peter Gorniak", "Deb Roy."], "venue": "Proceedings of the 7th International Conference on Multimodal Interfaces.", "citeRegEx": "Gorniak and Roy.,? 2005", "shortCiteRegEx": "Gorniak and Roy.", "year": 2005}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Li Fei-Fei."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Karpathy et al\\.,? 2014", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "ReferItGame: Referring to objects in photographs of natural scenes", "author": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L. Berg."], "venue": "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Kazemzadeh et al\\.,? 2014", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["Jayant Krishnamurthy", "Thomas Kollar."], "venue": "Transactions of the Association for Computational Linguistics, 1:193\u2013206.", "citeRegEx": "Krishnamurthy and Kollar.,? 2013", "shortCiteRegEx": "Krishnamurthy and Kollar.", "year": 2013}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C. Berg", "Tamara L. Berg."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Kulkarni et al\\.,? 2011", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Com-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Cynthia Matuszek", "Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox."], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "WordNet: A lexical database for English", "author": ["George A. Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Collecting semantic data by Mechanical Turk for the lexical knowledge resource of a text-to-picture generating system", "author": ["Masoud Rouhizadeh", "Margit Bowler", "Richard Sproat", "Bob Coyne."], "venue": "Proceedings of the Ninth International Conference on Com-", "citeRegEx": "Rouhizadeh et al\\.,? 2011", "shortCiteRegEx": "Rouhizadeh et al\\.", "year": 2011}, {"title": "On being the right scale: Sizing large collections of 3D models", "author": ["Manolis Savva", "Angel X. Chang", "Gilbert Bernstein", "Christopher D. Manning", "Pat Hanrahan."], "venue": "SIGGRAPH Asia 2014 Workshop on Indoor Scene Understanding: Where Graphics", "citeRegEx": "Savva et al\\.,? 2014", "shortCiteRegEx": "Savva et al\\.", "year": 2014}, {"title": "Real-time automatic 3D scene generation from natural language voice and text descriptions", "author": ["Lee M. Seversky", "Lijun Yin."], "venue": "Proceedings of the 14th Annual ACM International Conference on Multimedia.", "citeRegEx": "Seversky and Yin.,? 2006", "shortCiteRegEx": "Seversky and Yin.", "year": 2006}, {"title": "Image ranking and retrieval based on multi-attribute queries", "author": ["Behjat Siddiquie", "Rog\u00e9rio Schmidt Feris", "Larry S. Davis."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Siddiquie et al\\.,? 2011", "shortCiteRegEx": "Siddiquie et al\\.", "year": 2011}, {"title": "Understanding natural language", "author": ["Terry Winograd."], "venue": "Cognitive Psychology, 3(1):1\u2013191.", "citeRegEx": "Winograd.,? 1972", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "Learning the visual interpretation of sentences", "author": ["C. Lawrence Zitnick", "Devi Parikh", "Lucy Vanderwende."], "venue": "IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Zitnick et al\\.,? 2013", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "Prior work has shown the task remains challenging and time intensive for non-experts, even with simplified interfaces (Savva et al., 2014).", "startOffset": 118, "endOffset": 138}, {"referenceID": 2, "context": "Systems that can interpret natural descriptions to build a visual representation allow non-experts to visually express their thoughts with language, as was demonstrated by WordsEye, a pioneering work in text to 3D scene generation (Coyne and Sproat, 2001).", "startOffset": 231, "endOffset": 255}, {"referenceID": 20, "context": "WordsEye and other prior work in this area (Seversky and Yin, 2006; Chang et al., 2014) used manually chosen mappings between language and objects in scenes.", "startOffset": 43, "endOffset": 87}, {"referenceID": 1, "context": "WordsEye and other prior work in this area (Seversky and Yin, 2006; Chang et al., 2014) used manually chosen mappings between language and objects in scenes.", "startOffset": 43, "endOffset": 87}, {"referenceID": 18, "context": "Prior work in text to 3D scene generation focused on collecting manual annotations of object properties and relations (Rouhizadeh et al., 2011; Coyne et al., 2012), which are used to drive rule-based generation systems.", "startOffset": 118, "endOffset": 163}, {"referenceID": 3, "context": "Prior work in text to 3D scene generation focused on collecting manual annotations of object properties and relations (Rouhizadeh et al., 2011; Coyne et al., 2012), which are used to drive rule-based generation systems.", "startOffset": 118, "endOffset": 163}, {"referenceID": 21, "context": "There is much prior work in image retrieval given textual queries; a recent overview is provided by Siddiquie et al. (2011). The image retrieval task bears some similarity to our task insofar as 3D scene retrieval is an approach that can approximate 3D scene generation.", "startOffset": 100, "endOffset": 124}, {"referenceID": 23, "context": "Generation in image space has predominantly focused on composition of simple 2D clip art elements, as exemplified recently by Zitnick et al. (2013). The task of composing 3D scenes presents a much higherdimensional search space of scene configurations where finding plausible and desirable configurations is difficult.", "startOffset": 126, "endOffset": 148}, {"referenceID": 22, "context": "Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguistic manipulation of objects in 3D scenes.", "startOffset": 37, "endOffset": 53}, {"referenceID": 2, "context": "More recently, prototype text to 3D scene generation systems have been built for broader domains, most notably the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006).", "startOffset": 131, "endOffset": 155}, {"referenceID": 1, "context": "More recently, prototype text to 3D scene generation systems have been built for broader domains, most notably the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006). Chang et al.", "startOffset": 132, "endOffset": 198}, {"referenceID": 1, "context": "Chang et al. (2014) showed it is possible to learn spatial priors for objects and relations directly from 3D scene data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al.", "startOffset": 59, "endOffset": 127}, {"referenceID": 13, "context": "Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al.", "startOffset": 59, "endOffset": 127}, {"referenceID": 10, "context": "Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al.", "startOffset": 59, "endOffset": 127}, {"referenceID": 6, "context": ", 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014).", "startOffset": 65, "endOffset": 115}, {"referenceID": 11, "context": ", 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014).", "startOffset": 65, "endOffset": 115}, {"referenceID": 8, "context": "A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005).", "startOffset": 129, "endOffset": 175}, {"referenceID": 9, "context": "A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005).", "startOffset": 129, "endOffset": 175}, {"referenceID": 15, "context": "More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013).", "startOffset": 85, "endOffset": 140}, {"referenceID": 12, "context": "More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013).", "startOffset": 85, "endOffset": 140}, {"referenceID": 7, "context": "Golland et al. (2010) ground spatial relationship language in 3D scenes (e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 17, "context": "If one considers seed sentences to be the \u201creference,\u201d the macro-averaged BLEU score (Papineni et al., 2002) of the Turker descriptions is 12.", "startOffset": 85, "endOffset": 108}, {"referenceID": 1, "context": "We use the rule-based parsing component described in Chang et al. (2014). This system incorporates knowledge that is important for scene generation and not addressed by our learned model (e.", "startOffset": 53, "endOffset": 73}, {"referenceID": 16, "context": "Head words of noun phrases are identified as candidate object categories, filtered using WordNet (Miller, 1995) to only include physical objects.", "startOffset": 97, "endOffset": 111}, {"referenceID": 1, "context": "We use the same model database as Chang et al. (2014) and also extract spatial relations between objects using the same set of dependency patterns.", "startOffset": 34, "endOffset": 54}, {"referenceID": 5, "context": "Since 3D scene layout is not a focus of our work, we use an approach based on prior work in 3D scene synthesis and text to scene generation (Fisher et al., 2012; Chang et al., 2014), simplified by using sampling rather than a hill climbing strategy.", "startOffset": 140, "endOffset": 181}, {"referenceID": 1, "context": "Since 3D scene layout is not a focus of our work, we use an approach based on prior work in 3D scene synthesis and text to scene generation (Fisher et al., 2012; Chang et al., 2014), simplified by using sampling rather than a hill climbing strategy.", "startOffset": 140, "endOffset": 181}, {"referenceID": 0, "context": "We used the lme4 R package and optimized fit with maximum log-likelihood (Baayen et al., 2008).", "startOffset": 73, "endOffset": 94}], "year": 2015, "abstractText": "The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.", "creator": "TeX"}}}