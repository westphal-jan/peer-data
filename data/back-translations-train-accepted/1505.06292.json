{"id": "1505.06292", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2015", "title": "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements", "abstract": "We propose and study a row-and-column affine measurement scheme for low-rank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix $X$. This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition ($SVD$) and least-squares ($LS$), which we term \\alg. We prove that (a simplified version of) our algorithm can recover $X$ exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-and-column design and \\alg algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed row-and-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction.", "histories": [["v1", "Sat, 23 May 2015 08:45:20 GMT  (463kb)", "http://arxiv.org/abs/1505.06292v1", "ICML 2015"]], "COMMENTS": "ICML 2015", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "authors": ["or zuk", "avishai wagner"], "accepted": true, "id": "1505.06292"}, "pdf": {"name": "1505.06292.pdf", "metadata": {"source": "CRF", "title": "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements", "authors": ["Avishai Wagner"], "emails": ["avishai.wagner@mail.huji.ac.il", "or.zuk@mail.huji.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 150 5.06 292v 1 [csWe propose and examine a series and column-based measurement scheme for restoring the low matrix. Each measurement is a linear combination of elements in a row or column of a Matrix X. This setting naturally results in applications in different areas. However, the current algorithms developed for standard matrix recovery problems do not work well in our case, hence the need to develop new algorithms and theories for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SV D) and Least-Squares (LS), which we call SVLS. We prove that (a simplified version) our algorithm X can restore exactly with the minimum number of measurements in the noiseless case. In the generally loud case, we prove performance guarantees for restoration accuracy under the Frobenius standard."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "1.1 Prior Work", "text": "Before giving a detailed derivation and analysis of our design and our algorithms, we will give an overview of existing designs and their properties. We will focus on two properties: (i) storage required to represent the measurement operator, and (ii) measurement spareness, defined as the sum of all measurements of the number of matrix entries involved in each measurement, i.e., S (A) = | 0. The latter property can be related to measurement costs, as well as computing time. In the Gaussian ensemble model, the entries of matrix A in the matrix representation A (X) = Avec (X) are generally Gaussian random variables, Aij-N (0, 1). For this ensemble, a unique low matrix X with O (r + n2) noiseless measurements with nuclear standard minimization [5, 22] or other noise reduction methods can be used."}, {"heading": "2 Preliminaries and Notations", "text": "We designate the space of matrices of size n1 \u00b7 n2, by On1 \u00b7 n2 the space of matrices of size n1 \u00b7 n2 with orthonormal columns, and by M (r) n1 \u00b7 n2 the space of matrices of size n1 \u00b7 n2 and rank 6 r. We designate n = max (n1, n2).We designate the space of matrix Frobenius norm, by space of size n1 \u00b7 | | 2 the spectral line norm. For a vector that normalizes the standard l2. For a matrix Rn1 \u00b7 n2we denote by chip (X) the subspace of Rn1 through the columns of X and we define the PX as the orthogonal projection in span (X).For a matrix X denote X, which we define by Xi \u00b7 the series, by the J column of Rn1 and the column of two, we define the PX as the orthogonal projection in X (span)."}, {"heading": "2.1 Comparison to Standard Designs", "text": "Our proposed row and column design differs from standard designs that appear in the literature. It is instructive to compare our GRC ensemble with the Gaussian ensemble (GE) [5], with the matrix representation A (X) = 0.5 \u00b0 C, where A-Rd \u00b7 n1n2 and Ai.i.d. \u0445 N (0, 1). For the latter, the following r-restricted isometry property (RIP) can be used: Definition 1. (r-RIP) Let A: Rn1 \u00b7 n2 \u2192 Rd be a linear map. For each integer R with 1 \u2264 r \u2264 r \u2264 min (n2), define the r-restricted isometry constant to be the smallest number of such (1 \u2212 r)."}, {"heading": "3 Algorithms for Recovery of X", "text": "In this section we present an efficient algorithm that we call SVLS (Singular Value Least Squares). SVLS is very easy to implement - for simplicity we start with algorithm 1 for the silent case and then present algorithm 2 (SVLS) that is applicable to the general (loud) case."}, {"heading": "3.1 Noiseless Case", "text": "In the silent case, we reduce the optimization problem (3) to solving a system of linear equations [6] and provide algorithm 1, which often leads to a closed-form estimator. We then specify conditions under which the closed-form solution is highly likely to be unique and equal to the true matrix. If rank (A (R) U (6) = r is occupied, the resulting estimator X (R) can be written in closed form as follows: X (U) TA (R) T A (R) U (1) TA (R) T) T (R) (6) Algorithm 1 does not treat the measurements in series and column symmetrically. We can apply the same algorithm, but change the role of rows and columns. The resulting algorithm 1 Input: A (R) T (R), A (R), B (C) and rank r1."}, {"heading": "3.2 General (Noisy) Case", "text": "The minimization problem is not convex and there is no known algorithm with optimum guarantees. We propose algorithm 2 (SVLS), which empirically returns a matrix estimator X with a low value of loss F. Furthermore, in section 4 we prove recovery guarantees for the performance of the SVLS algorithm 2 SVLS input: A (R), A (C), B (R), B (C) and rank r1. Calculate U, the largest left singular vectors of B (C), (U) is a basis for column spacing of B (C) (r)).2. Find the solution with the fewest squares Y (R) and rank r1. Calculate the solution Y (R) - A (R) - A (R) - Y | | F (8), if rank (A (R).R) - R = Y (R), before finding the solution of Y (U) - (U -) (U -) (-) (U -)."}, {"heading": "3.2.1 Gradient Descent", "text": "The estimator X returned by SVLS could not minimize the loss function F in equivalent (2). Therefore, we perform an additional gradient descend step from X to achieve an estimator with a lower loss (which may only be a local minimum as the problem is not convex). SVLS can therefore be considered a quick method of providing a desirable starting point for local search algorithms. Details of the gradient descend are listed in the appendix, Section 7.2."}, {"heading": "3.3 Estimation of Unknown Rank", "text": "In real life problems, one does not know the true rank of a matrix and should estimate it on the basis of data. Our row and column sample design is particularly suitable for ranking because rank (B (C, 0) = rank (B (R, 0))) = rank (X) with a high probability, if enough rows and columns are scanned. In the silent case, we can estimate rank (X) by r = rank (B (C, 0) or rank (B (R, 0)). In the loud case, we estimate rank (X) by B (C), B (R). We use the popular arc method to estimate rank (B (C) the following year (C) = Argmaxi (K) \u2212 1] (B (B (C))) + 1 (B) + 1 (B (C)). We calculate similarly r (R) by B (R) and take the average as our ranking estimator, r = round (C) + 1 (C) we can prove the values for simulation (C) (S) -2."}, {"heading": "3.4 Low Rank Approximation", "text": "In the lower rank matrix approach problem, the goal is to approximate a (possibly complete rank) matrix X by the nearest (in the Frobenius standard) rank-r matrix X (r). According to the Eckart-Young Theorem [12], this problem has a closed solution, which is the truncated SV D of X. SV D is a powerful tool in affinity matrix recovery and various algorithms such as SVT, OptSpace, SVP, and other SVD application. However, in [15] the authors attempt to find a low-grade approach to X by using measurements XA (C) = B (C) and A (R) X = B (R). For large n1, n2, they specify a single-pass algorithm that calculates X (r) using only B (C) and B (R)."}, {"heading": "4 Performance Guarantees", "text": "In this section we provide guarantees for the accuracy of the estimator X returned by SVLS. Our guarantees are probabilistic with regard to the randomization of the design matrices A (R), A (C). In the silent case, we specify conditions that are almost optimal for an exact restoration."}, {"heading": "4.1 Noiseless Case", "text": "A rank r matrix of size n1 \u00b7 n2 has r (n1 + n2 \u2212 r) degrees of freedom and therefore cannot be clearly restored by less measurements. Let's set k (R) = k (C) = r gives exactly this minimum number of measurements. Next, we show that this number with probability 1 is sufficient to guarantee a precise recovery of X in the GRC model. In the RCMC model, the number of measurements is increased by a logarithmic factor in n and we need an additional incoherence assumption to X to guarantee a high probability of a precise recovery. First, we present two lemms that will be useful. Your evidence is in the Appendix, Section 7.1.Lemma 1. Let X1, X2, XM (r) n1 \u00d7 n2 and A (R), Rk (R), Rk (R), Rn2 \u00b7 k (C), such a rank (A), X1 \u00b7 C (R) and then (R) A (R) (R) (R)."}, {"heading": "4.1.1 Exact Recovery for GRC", "text": "For the silent case, we can restore X with the minimum number of measurements, as shown in theorem 1 (proof in the appendix, Section 7.1): Theorem 1. Let X be the output of algorithm 1 in the GRC model with Z (C), Z (R) = 0 and k (R), k (C) \u2265 r. Then P (X = X) = 1."}, {"heading": "4.1.2 Exact Recovery for RCMC", "text": "In the RCMC model, rows and columns of X are scanned in place. As the same line can be scanned again and again, we cannot guarantee the uniqueness of the solution, as was the case in the case of the GRC model, but rather want to prove that an exact restoration of X is possible with a high probability. We start from the Bernoulli row and column model as in Section 2 and, for simplicity, assume that k (R) = k (C) = k (k) = k. Theorem 2. Let X = U V T be the SV D of X-Rn1 \u00d7 n2 and max (\u00b5 (U), \u00b5 (V) < \u00b5. Let us take A (R) and A (C) as in the RCMC model without noise and probabilities p (R) = k n1 and p (C) = kn2. Let us leave \u03b2 > 1 so that CR-log (n) r\u00b5 k < 1, where the arc can be uniform and X-shaped."}, {"heading": "4.2 General (Noisy) Case", "text": "In this loud case, we cannot guarantee an exact recovery of X, and our goal is to minimize the error | | X \u2212 X \u2212 X | | | F for the output of SVLS. At this point, we limit the error of the GRC model. For simplicity, we show the result for k (R) = k (C) = k. We focus on the high-dimensional case k \u2264 n, in which the number of measurements is small. In this case, our limit is similar to the limit of the Gaussian ensemble (GE). In [5], it is shown for GE that | X \u2212 X \u2212 X | F < CG 270 \u2212 n is most likely to apply to the constant K. Next, we give an analog result for our GRC model (proof in the appendix, section 7.4).Theorem 3. Let us leave A (R) and A (C) for the constant K."}, {"heading": "5 Simulations Results", "text": "We investigated the performance of our algorithm using simulations. We measured reconstruction accuracy using the Relative Root Mean Square Error (RRMSE), defined as RRMSE \u2261 RRMSE (X, X, X, X, F / | X, F). (15) For simplicity, we focused on square matrices with n1 = n2 = n and used an equal number of row and column measurements, k (R) = k (C) = k. In all simulations, we stitched a random rank r matrix X = UV T with U, V, Rn \u00b7 r, U, Vi.i.d. \u0445 N (0, \u03c32). In all simulations, we assumed that rank (X) is unknown and estimated it in the equivalent using the elbow method (10)."}, {"heading": "5.1 Row-Column Matrix Completion (RCMC)", "text": "We compared the reconstruction rate (probability of the exact recovery of X as a function of the number of measurements d) for the RCMC construction with SVLS with the reconstruction rate for the standard MC construction with the OptSpace [18] and SVT [2] algorithms. To allow numerical errors, we point out for each simulation that the recovery of the MC construction is successful if its RRMSE value is lower than 10 \u2212 3, and for each value that d has recorded the percentage of simulations, the recovery was successful. In Figure 1 we show results for n = 150, r = 3 and x = 1, SVLS puts X with the probability 1 (2n \u2212 r) = 894 gives dn2: 0.04, while MC with OptSpace and SVT roughly gives 3: 1 and 8: 3: 3: 3 measurements for the RCS measurement."}, {"heading": "5.2 Gaussian Rows and Columns (GRC)", "text": "We tested the performance of the GRC model with A (R), A (C) i.i.d. \u0445 N (0, 1n) and X = UV T, where U, V i.i.d. \u0445 N (0, 1 \u221a r). We compared our results with the Gaussian Ensemble Model (GE), where for each n, A (X) was normalized to allow a fair comparison. In Figure 3, we take n = 100 and r = 2 and change the number of measurements d = 2nk (where A (R), Rk \u00d7 n and A (C) \u0445Rn \u00d7 k). We added Gaussian noise Z (R), Z (C) with different noise levels. At all noise levels, the performance of the GRC was better than the performance of GE. The RMSE error decays at a rate of T \u00b7 k. For GE, we used the APGL algorithm [26] for the core standard minimization. In the next tests, we ran with different values for SVSE and RSE."}, {"heading": "6 Discussion", "text": "We have focused on two models: matrix completion from single columns and rows (RCMC) and matrix recovery from Gaussian combination of columns and rows (GRC). For the RCMC model, we have proven that our method is highly likely to restore X in silent case, and simulation results show that the RCMC model exceeds the standard approach to matrix completion in both speed and accuracy for low noise models. For the GRC model, we have proven that our method restores recoversX with the optimal number of measurements in silent case and sets an upper limit on the error in loud case. For the RCMC model, our simulations show that the RCMC model can achieve comparable or favorable results, especially at low noise level."}, {"heading": "7 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Proofs for Noiseless GRC Case", "text": "(A)..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "7.2 Gradient Descent", "text": "The gradient descent stage is performed directly in the space of the rank r matrices, calculating the gradient of loss as a function of W and S, L (W, S) = F (WS) = | | A (R) WS \u2212 B (R) | | 2F + | WSA (C) \u2212 B (C) | | 2F. (20) We want to minimize equivalence (20), but the loss L is not convex and therefore the gradient descent could not converge to a global optimum. We propose X (the output of SVLS) as a starting point that could be close enough to allow the gradient descent to converge with the global optimum and could also accelerate convergence. The gradient of L is (according to the chain rule): L = 2 [A (R) T (R) WS \u2212 B (WS \u2212 R) (WS) (R) (WS) and S \u00b7 S \u00b7 S (S) (S) (S \u00b7 S) (S) (S)."}, {"heading": "7.3 Proofs for Noiseless RCMC Case", "text": "We prove that there is a numerical constant that applies to all other countries, if they have a high probability, then we have p \u2212 1 (R) -1 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2 (R) -2). We generalize theorem4.1 (R) -4 (R).Lemma 4 (R) as in the RCMC model with the inclusion probability p, and U (U) -1 \u00b7 r (U) = n1r maxi (PU)."}, {"heading": "7.4 Proofs for Noisy GRC Case", "text": "The proof of the theoretical 3 is the high concentration results of the Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "7.5 Simulations for Large Values of n", "text": "We varied n between 10 and 1000, with the results averaging over 100 different matrices from rank 3 at each point, and tried to restore them using k = 20 row and column measurements. Measuring matrices were A (R), A (C) i.i.d. \u0445 1n, to similar standards for each measurement vector for different values from n. Recovery performance was insensitive to n. Let's take A (R), A (C) i.i.d. \u0445 N (0, 1) instead of N (0, 1n), the scaling of our results is correct with Theorem 3. Next, we take n, k, r \u2192 \u00b2, while the ratios nk = 5 and kr = 4 remain constant and calculate the relative error for different noise levels. Again, the relative error converges quickly to constant, regardless of n, k, r."}, {"heading": "7.6 Low Rank matrix Approximation", "text": "The output of this algorithm is not small if k > r. This algorithm differs from SV LSP and its purpose is to approximate a (possibly complete) matrix by a low-rank matrix. We have adapted algorithm 3 with some changes to our purpose. Firstly, we estimate the rank of X using the arc method from Section 3.3 and instead of calculating the QR decomposition of B (C) and B (R) T, we find their largest single vectors. Furthermore, we repeat part two of algorithm 3 while replacing the rolls of columns and rows as in SVLS. This variation gives our modified algorithm SV LSP, as in Section 3.4.We have compared our SVLS with SV LSP, as shown in Section 3.4. We took X-M (10) 1000 x 1000 and = 1. We have tried to restore X in the GRC model."}, {"heading": "7.7 Rank Estimation", "text": "We are testing the elbow method for estimating rank X (see equivalent (10)). We are taking a matrix X of size 400 x 400 and different ranks. We are adding Gaussian noise with \u03c3 = 0.25, while the measurements are sampled as in the RCMC model. For each number of measurements we have sampled 100 matrices and taken the average estimated rank. We are calculating the estimator r for different values of d, the number of measurements. We are comparing our method with the ranking that appears in the OptSpace [17] for the standard MC problem. Our simulation results, shown in Figure 8, suggest that the RCMC model with the elbow method is a much better design for estimating rank X."}, {"heading": "7.8 Test Error", "text": "In the matrix completion with MC and RCMC, the RRMSE loss function is similar to the loss of both the observed and the unobserved entries. This loss can be too optimistic if we consider our prediction error only for unobserved entries. So, instead of including all measurements in the calculation of the RRMSE, we calculate a different measure of the prediction error that the RRMSE indicates only for the unobserved entries. For each individual entry measurement, operator A E (A) defines the set of measured entries and E (E) its supplement, i.e. the set of unobserved entries (i, j). We define XE as a matrix, so that XE (i, j), if (i, j), E and 0 otherwise. Instead of RRMSE (X, X), we now calculate RMSE (XE, E, E). This quantity, therefore, we do not just reconstruct our results on the SVS and MSE."}], "references": [{"title": "Lambertian reflectance and linear subspaces", "author": ["Ronen Basri", "David W Jacobs"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1956}, {"title": "ROP: Matrix recovery via rank-one projections", "author": ["Tony T Cai", "Anru Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Sparsity and incoherence in compressive sampling", "author": ["Emmanuel J Cand\u00e8s", "Justin Romberg"], "venue": "Inverse Problems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Genotype imputation via matrix completion", "author": ["Eric C Chi", "Hua Zhou", "Gary K Chen", "Diego Ortega Del Vecchyo", "Kenneth Lange"], "venue": "Genome Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Genomic maps of long noncoding RNA occupancy reveal principles of RNAchromatin interactions", "author": ["Ci Chu", "Kun Qu", "Franklin L Zhong", "Steven E Artandi", "Howard Y Chang"], "venue": "Molecular cell,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "The approximation of one matrix by another of lower rank", "author": ["Carl Eckart", "Gale Young"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1936}, {"title": "The optimal hard threshold for singular values", "author": ["Matan Gavish", "David L Donoho"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Quantum state tomography via compressed sensing", "author": ["David Gross", "Yi-Kai Liu", "Steven T Flammia", "Stephen Becker", "Jens Eisert"], "venue": "Physical Review Letters,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp"], "venue": "SIAM Review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Guaranteed rank minimization via singular value projection", "author": ["Prateek Jain", "Raghu Meka", "Inderjit S Dhillon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Matrix completion from noisy entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Fixed point and Bregman iterative methods for matrix rank minimization", "author": ["Shiqian Ma", "Donald Goldfarb", "Lifeng Chen"], "venue": "Mathematical Programming,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization", "author": ["Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo"], "venue": "SIAM Review,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Condition numbers of random matrices", "author": ["Stanislaw J Szarek"], "venue": "Journal of Complexity,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1991}, {"title": "New concentration inequalities in product spaces", "author": ["Michel Talagrand"], "venue": "Inventiones Mathematicae,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["Kim-Chuan Toh", "Sangwoon Yun"], "venue": "Pacific Journal of Optimization,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}], "referenceMentions": [{"referenceID": 18, "context": "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].", "startOffset": 155, "endOffset": 158}, {"referenceID": 13, "context": "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].", "startOffset": 185, "endOffset": 189}, {"referenceID": 8, "context": "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 3, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 5, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 7, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 16, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 17, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 20, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 4, "context": "Gaussian weights [5, 22].", "startOffset": 17, "endOffset": 24}, {"referenceID": 21, "context": "Gaussian weights [5, 22].", "startOffset": 17, "endOffset": 24}, {"referenceID": 5, "context": "Remarkably, although the recovery problem is in general NP-hard, when r \u226a min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d \u226a n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].", "startOffset": 260, "endOffset": 274}, {"referenceID": 7, "context": "Remarkably, although the recovery problem is in general NP-hard, when r \u226a min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d \u226a n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].", "startOffset": 260, "endOffset": 274}, {"referenceID": 20, "context": "Remarkably, although the recovery problem is in general NP-hard, when r \u226a min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d \u226a n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].", "startOffset": 260, "endOffset": 274}, {"referenceID": 21, "context": "Remarkably, although the recovery problem is in general NP-hard, when r \u226a min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d \u226a n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].", "startOffset": 260, "endOffset": 274}, {"referenceID": 9, "context": "For example, (i) In collaborative filtering, we may wish to recover a users-items preference matrix and have access to only a subset of the users, but can observe their preference scores for all items (ii) When recovering a protein-RNA interactions matrix in molecular biology, a single experiment may simultaneously measure the interactions of a specific protein with all RNA molecules [10].", "startOffset": 387, "endOffset": 391}, {"referenceID": 5, "context": "It is thus not surprising that algorithms such as nuclear norm minimization [6, 22], which succeed for the GE and MC models, fail in our case, and different algorithms and theory are required.", "startOffset": 76, "endOffset": 83}, {"referenceID": 21, "context": "It is thus not surprising that algorithms such as nuclear norm minimization [6, 22], which succeed for the GE and MC models, fail in our case, and different algorithms and theory are required.", "startOffset": 76, "endOffset": 83}, {"referenceID": 4, "context": "For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants.", "startOffset": 136, "endOffset": 143}, {"referenceID": 21, "context": "For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants.", "startOffset": 136, "endOffset": 143}, {"referenceID": 15, "context": "For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants.", "startOffset": 201, "endOffset": 205}, {"referenceID": 5, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 7, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 19, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 20, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 25, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 16, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 239, "endOffset": 247}, {"referenceID": 17, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 239, "endOffset": 247}, {"referenceID": 7, "context": "However, recovery guarantees for this model are weaker: setting n = max(n1, n2), it is shown that \u0398(nrlog(n)) measurements are required to recover a rank r matrix of size n1 \u00d7 n2 [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 2, "context": "Recently a new design of rank one projections was proposed [3], where each measurement is of the form \u03b1X\u03b2 and such that \u03b1 \u2208 R1 , \u03b2 \u2208 R2 have i.", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "Our problem is a specialization of the general affine matrix recovery problem [22], in which a matrix is measured using a general affine transformation A with b = A(X) + z.", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "It is instructive to compare our GRC ensemble to the Gaussian Ensemble (GE) [5], with the matrix representation A(X) = Avec(X) where A \u2208 Rd\u00d7n1n2 and A i.", "startOffset": 76, "endOffset": 79}, {"referenceID": 21, "context": "The GE model satisfies the r-RIP condition for d = O(rn) with high probability [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability.", "startOffset": 66, "endOffset": 73}, {"referenceID": 21, "context": "Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability.", "startOffset": 66, "endOffset": 73}, {"referenceID": 15, "context": "Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "We next compare RCMC to the standard Matrix Completion model [6], in which single entries are chosen at random to be observed.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "Unlike GE, for MC incoherence conditions on X are required in order to guarantee unique recovery of X [6] : Definition 2.", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "When X is \u03bc-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability.", "startOffset": 107, "endOffset": 118}, {"referenceID": 15, "context": "When X is \u03bc-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability.", "startOffset": 107, "endOffset": 118}, {"referenceID": 16, "context": "When X is \u03bc-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability.", "startOffset": 107, "endOffset": 118}, {"referenceID": 5, "context": "1 Noiseless Case In the noiseless case we reduce the optimization problem (3) to solving a system of linear equations [6], and provide Algorithm 1, which often leads to a closed-form estimator.", "startOffset": 118, "endOffset": 121}, {"referenceID": 12, "context": "Modern methods for rank estimation from singular values [13] can be similarly applied to B, B and may yield more accurate rank estimates.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "By the Eckart-Young Theorem [12], this problem has a closed-form solution which is the truncated SV D of X .", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "In [15] the authors try to find a low rank approximation to X using measurements XA = B and AX = B.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In [15] it is assumed that k = k = k and one estimates X(k) instead of a rank-r matrix which can lead to poor performance if r \u226a k.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "We adjusted the algorithm presented in [15] to our problem and give a new estimator which is a combination of SVLS and [15]\u2019s method, replacing X\u0302 and X\u0302 in steps 3,4 of SVLS by:", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "We adjusted the algorithm presented in [15] to our problem and give a new estimator which is a combination of SVLS and [15]\u2019s method, replacing X\u0302 and X\u0302 in steps 3,4 of SVLS by:", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "In [5] it is shown for GE that ||X \u2212 X\u0302||F < CG \u221a", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "We compared the reconstruction rate (probability of exact recovery of X as function of the number of measurements d) for the RCMC design with SVLS to the reconstruction rate for the standard MC design with the OptSpace [18] and SVT[2] algorithms.", "startOffset": 219, "endOffset": 223}, {"referenceID": 1, "context": "We compared the reconstruction rate (probability of exact recovery of X as function of the number of measurements d) for the RCMC design with SVLS to the reconstruction rate for the standard MC design with the OptSpace [18] and SVT[2] algorithms.", "startOffset": 231, "endOffset": 234}, {"referenceID": 25, "context": "For GE we used the APGL algorithm [26] for nuclear norm minimization.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "[1] Ronen Basri and David W Jacobs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Tony T Cai and Anru Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Emmanuel J Cand\u00e8s and Yaniv Plan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Emmanuel J Cand\u00e8s and Yaniv Plan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Emmanuel J Cand\u00e8s and Benjamin Recht.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Emmanuel J Cand\u00e8s and Justin Romberg.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Emmanuel J Cand\u00e8s and Terence Tao.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Eric C Chi, Hua Zhou, Gary K Chen, Diego Ortega Del Vecchyo, and Kenneth Lange.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Ci Chu, Kun Qu, Franklin L Zhong, Steven E Artandi, and Howard Y Chang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Sanjoy Dasgupta and Anupam Gupta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Carl Eckart and Gale Young.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Matan Gavish and David L Donoho.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] David Gross, Yi-Kai Liu, Steven T Flammia, Stephen Becker, and Jens Eisert.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Prateek Jain, Raghu Meka, and Inderjit S Dhillon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Yehuda Koren, Robert Bell, and Chris Volinsky.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Shiqian Ma, Donald Goldfarb, and Lifeng Chen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Shai Shalev-Shwartz and Shai Ben-David.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Stanislaw J Szarek.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Michel Talagrand.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Kim-Chuan Toh and Sangwoon Yun.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "1 from [6].", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "1 from [6].", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "We start with a lemma from [7].", "startOffset": 27, "endOffset": 30}, {"referenceID": 24, "context": "We next use a result from large deviations theory [25]:", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "2 in [6].", "startOffset": 5, "endOffset": 8}, {"referenceID": 23, "context": "[24] Let A \u2208 Rn\u00d7k be a random matrix A i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "We also use the following lemma from [23]:", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Lemma 7 is a direct result of the Johnson-Lindenstrauss lemma [11] applied to each vector in Q and using the union bound.", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "6 Low Rank matrix Approximation We bring here the one pass algorithm to approximate X from [15] for the convenience of the reader.", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "We compare our method to the rank estimation which appears in OptSpace [17] for the standard MC problem.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "(10) in the main text, and for the MC model we used the method described in [17].", "startOffset": 76, "endOffset": 80}], "year": 2015, "abstractText": "We propose and study a row-and-column affine measurement scheme for lowrank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix X . This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SV D) and least-squares (LS), which we term SVLS . We prove that (a simplified version of) our algorithm can recover X exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-andcolumn design and SVLS algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed rowand-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction.", "creator": "LaTeX with hyperref package"}}}