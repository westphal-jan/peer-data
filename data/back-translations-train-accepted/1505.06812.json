{"id": "1505.06812", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2015", "title": "Optimizing Non-decomposable Performance Measures: A Tale of Two Classes", "abstract": "Modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics, and require optimizing performance measures that are non-decomposable over the dataset, such as F-measure. Such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent.", "histories": [["v1", "Tue, 26 May 2015 05:59:33 GMT  (86kb,D)", "http://arxiv.org/abs/1505.06812v1", "To appear in proceedings of the 32nd International Conference on Machine Learning (ICML 2015)"]], "COMMENTS": "To appear in proceedings of the 32nd International Conference on Machine Learning (ICML 2015)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["harikrishna narasimhan", "purushottam kar", "prateek jain 0002"], "accepted": true, "id": "1505.06812"}, "pdf": {"name": "1505.06812.pdf", "metadata": {"source": "CRF", "title": "Optimizing Non-decomposable Performance Measures: A Tale of Two Classes", "authors": ["Harikrishna Narasimhan", "Purushottam Kar"], "emails": ["harikrishna@csa.iisc.ernet.in", "t-purkar@microsoft.com", "prajain@microsoft.com"], "sections": [{"heading": null, "text": "In this paper, we show that it is actually possible to perform point stochastic updates for two large families of performance metrics that can be expressed as functions of true positive / negative rates. Our core contribution is an adaptive linearization scheme for these families, which we use to develop optimization techniques that enable truly point-based herchastic updates. For concave performance metrics, we propose SPADE, a herchastic primordial dual solver; for pseudo-linear metrics, we propose STAMP, a stochastic alternative maximization method. Both methods have crisp convergence guarantees, show significant accelerations over existing methods - often by an order of magnitude or more - and provide similar or more accurate predictions of test data."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Related Works", "text": "As mentioned in Section 1, existing methods for optimizing performance measures that we examine may be divided into surrogate-based approaches and indirect approaches based on cost-sensitive classification or plug-in methods. A third approach, applicable to certain performance measures, is the decision theory method that learns a class probability estimate and calculates predictions that maximize the expected value of the performance metric on a test set Lewis [1995], Ye et al. [2012]. In addition, there are methods that are dedicated to specific performance metrics. However, Parambath et al. [2014] focus on optimizing F metrics by exploiting the pseudo-linearity of the function along with a cross-validation-based strategy. Our STAMP method uses an alternating maximization strategy that does not require cross-validation, which significantly improves the training time of settings settings (see Settings that are also important to note in Settings below)."}, {"heading": "3 Problem Setting", "text": "Let us use the instance space and Y = {\u2212 1, + 1} the label space with a certain distribution D over X \u00b7 y. Let p: = Pr (x, y) \u0445 D [y = + 1] specify the percentage of positives in the population. Let T = {(x1, y1), Jr., (xT, yT) specify a sample of training points that were i.e. sampled by D. We will consider performance measures that can be expressed in relation to the true positive and negative rates of a classifier. To represent these yardsticks, we will use the term of a reward function r that assigns a reward r (y, y)."}, {"heading": "4 Concave Performance Measures", "text": "The first class of performance yardsticks we analyze are concave performance yardsticks. These yardsticks can be written as concave functions of TPR and TNR i.e.P (w) = concave performance yardsticks (P (w), N (w)) for some concave link functions. Table 1 contains a list of such performance yardsticks along with some of their relevant properties and references to works using these performance yardsticks. We will find it convenient to define the (concave) fennel conjugation of the link functions for our performance yardsticks. Table 1 contains a list of such performance yardsticks along with some of their relevant properties and references to works using these performance yardsticks. (Table 1): Conjugation of the link functions for our performance yardsticks. For all concave function and regulation yardsticks, \u03b2 (R), defined performance yardsticks (\u03b1, \u03b2) = definition yardsticks for the work applying these performance yardsticks."}, {"heading": "4.1 A Stochastic Primal-dual Method for Optimizing Concave Performance Measures", "text": "We now present a novel stochastic method to optimize the class of concave power measures. The application of stochastic gradient methods to these measures poses specific challenges due to the non-decomposable nature of these measures, which make it difficult to obtain cost-effective, unbiased estimates of the gradient using a single point. Recent work by Kar et al. [2013, 2014] has attempted to solve this problem by looking at mini-batch methods or using a buffer to maintain a convergence rate of the flow. However, such techniques entail a distortion of the learning algorithm in the form of buffer size or mini-batch length, which leads to slower convergence. In fact, the 1PMB method by Kar et al. [2014] is only able to guarantee a convergence rate of \u2212 4 \u221a T, whereas SGD techniques are usually able to show a 2-subset of power measures (see below for the SP3)."}, {"heading": "4.2 Convergence Analysis for SPADE", "text": "That is, we will work with power metrics that are considered monotonous (see Table 1).We will now present two useful concepts (see Table 1).The power metrics are described as stable if we are considered sufficient for some functionality. (See Table 1) The power metrics are described as sufficient if we are considered sufficient for some functionality. (See Table 2) The power metrics are described as sufficient. (See Table 2) The power metrics are described as sufficient if we are named for some functionality. (R) We have sufficient assumptions for all functionality and are satisfied with all the performance metrics that are considered here. (See Table 2) The power metrics are considered sufficient. (See Table 1) The power metrics are described as sufficient if we are named for some functionality."}, {"heading": "4.3 The Case of non-Lipschitz Link Functions", "text": "However, non-Lipschitz linkage functions, as used in the G mean performance scale, pose a particular challenge to the previous analysis. Due to their non-Lipschitz function, their sufficient dual region is unlimited. In fact, Table 1 indicates that the sufficient region for the G mean extends indefinitely along both coordinate axes. Specifically, what happens is that the gradients of the G mean function differ between either u \u2192 0, or v \u2192 0. This represents a stumbling block for the proof of Theorem 4, since regret and the online-to-batch conversion results use therein error. A natural way to solve this problem is to ensure that the reward functions r +, r \u2212 always assign rewards that are delimited from zero. More specifically, we have r + (w; y), r \u2212 (w; y; y) for all W and X ranges, and that the reward region can be limited to 3."}, {"heading": "5 Pseudo-linear Performance Measures", "text": "The second class of power metrics we analyze are pseudo-linear power metrics. These metrics have a broken linear function as a connection function and can be written as follows: P (a, b) (w) = a0 + a1 \u00b7 P (w) + a2 \u00b7 N (w) b0 + b1 \u00b7 P (w) + b2 \u00b7 N (w), Algorithm 2 AMP: Alternative maximization method Input: power measurement P (a, b), practicable setW, tolerance output: An -optimal classifier w \u00b2 W1: Construct the weighting function V (a, b) 2: w0 \u2190 0, t \u00b2 1 3: while vt > vt \u2212 1 + do 4: wt + 1 arg maxw \u00b2 W V (a, b) (w) (w, vt) 5: Positive classifier w \u00b2 W1: Construct the weighting function V (a, b) so that V (a \u00b2, the literature corresponds to)."}, {"heading": "5.1 Alternate-maximization for Optimizing Pseudo-linear Performance Measures", "text": "The pseudo-linear functions are so called because their planes can be defined with linear hemispheres."}, {"heading": "6 Experimental Results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Acknowledgements", "text": "HN thanks the support of a Google India PhD Fellowship."}, {"heading": "A Proof of Lemma 3", "text": "Lemma. 3. The stability parameter of a performance measure can be described as follows: \"This is a sufficient dual region.\" The proof results from the fact that any arbitrary value of such a value, for which such a value has been set, can be safely excluded from the sufficient dual region. \"To prove the result in one direction, we assume that this value is stable.\" () \"L\" for some L > 0. Let us now consider some \"R2.\" (\"We.\"). (\"We.\" (\"We.\"). \"(\" We. \").\" (. \"We.\"). \".\" (. \"We.\"). \"(.\" We. \").\". \"(.\" We. \").\" (. \"We.\"). \"(.\" We. \"). (.\" (.). \"(.\" We. \").\" (. \"). (.\" (. \"We.\"). \"(.\"). (. \"(.).\" (. \").\" (. \").\" (. \"(.\"). \"(.\"). \"(.\"). \"(.\" (.). \"(.).\" (. \").\" (. \").\" (. \").\" (. \"(.).\" (. \"(.).\"). \"(.\"). \"(.\"). \"(.\"). \"(.\" (.). \"(.\"). \"(.\" (.). \"(.).\"). \"(.\"). \"(.\"). \"(.).\" (. \"). (.\" (.). \"). (.\" (. \"(.).\"). (. \").\" (. \"(.\"). (.). (. \"(.).\"). \"(.\"). (. \"(.\"). \"). (.\"). \"(.). (. (.\"). \"). (.\" (.). (. \").\"). (. (. \"). (.\"). (. \")."}, {"heading": "B Proof of Theorem 4", "text": "Theorem 4: Suppose that we obtain a stream of random samples (x1, y1) so that its double application is possible (\u03b2, yT). (xT, yT) results from a distribution D over X \u00b7 y. Let us (xT, yT) perform a concave, Lipschitz link function. Let algorithm 1 be executed with a probability of at least 1 \u2212 \u00b2. (xT, yT). (xT, yT). (xT). (xT). (xT). (xT). (xT). (xT). (xT). (xT). (xT). (xT). (xT). (xT)."}, {"heading": "C Proof of Theorem 5", "text": "Theorem 5: Suppose we have the problem position in Theorem 4: (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (4): (4): (4): (4): (4): (4): (4): (4): (4) (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4): (4: (4): 4: 4: (4): (4): (4): (4): (4): (4): (4: (4): (4): (4): (4): (4: 4): (4): (4): (4): (4: (4): (4): (4): (4): (4): (4): (4): (4): (4): (4: (4): (4): (4): (4): (4): (4): (4): (4): (4): 4):"}, {"heading": "D Proof of Theorem 8", "text": "Theorem 8: Let Algorithm 2 be executed with a success measurement P (a > b) and reward functions that provide values in the range [0, m). Let's let P (p): = \u2212 \u2212 \u2212 P (a, b) (w). Let's leave \u00b7 t = P (a, b) (wt) the excess error for the model generated at that time wt. Then there is a value p (m) < 1 so that for all w) t \u2264 0 \u00b7 p (p) p (p) p (p) p (m) p (m) p (m) p (m) p (m) p (p).Proof. To be general in its treatment, the proof requires the following regularity conditions on the performance measurement. b0 6 = 02. P (w) \u2212 p (w) \u00b7 p (w) p (w) p (w) p) \u00b7 p (p) \u00b7 p) \u00b7 p (p) \u00b7 p (w) p (p) p) p (p) p (w) p) p) p (p) p)."}, {"heading": "E An analysis of the AMP Algorithm under Inexact Maximizations", "text": "For this and the next section, for the sake of simplicity, we also assume that the level is only approximately the same. < b = > b = p = 1 / 2, so that the F measurement looks like this: F (P, N) = 2P2 + P \u2212 N, and the evaluation function then looks like: V (w, v) = (1 \u2212 v / 2) \u00b7 P (w) + v / 2 \u00b7 N (w). We assume that the reward functions give limited rewards in the range [0, m). So far, we assumed that step 4 in the AMP algorithm would give us wt + 1 such atV (wt + 1, vt) = max w-W V (w, vt) = max W V (v), we only assume that wt + 1 satisfiesV (wt + 1, vt) = max."}, {"heading": "F Proof of Theorem 9", "text": "Theorem 9: \"Shall we execute the algorithm 3 with a performance metric P (a, b) and reward functions with a range [0, m)?\" \"Shall we guarantee the convergence rate for P (a, b) through the AMP algorithm?\" Let us set the epoch lengths to se, s \"e = O\" (1 \u03b72e). After that, we can most likely ensure that P \"\u2212 P (a, b) (we) \u2264.\" In addition, we can set the number of samples consumed to a maximum of O \"(1 2). Using Hoeffdings inequality, standard regret and online-to-batch, Cesa-Bianchi et al. [2001], Zinkevich [2003], we can ensure that when the stream lengths for the model optimization stage and challenge-level estimation procedures are set on Se, then for some fixed c > 0, that we have the logbook for this logbook on 1."}], "references": [{"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "On the Generalization Ability of On-Line Learning Algorithms", "author": ["Nicol\u00f3 Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "In 15th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2001}, {"title": "Evaluation of Classifiers for an Uneven Class Distribution Problem", "author": ["Sophia Daskalaki", "Ioannis Kopanas", "Nikolaos Avouris"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "Daskalaki et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Daskalaki et al\\.", "year": 2006}, {"title": "Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization", "author": ["Krzysztof Dembczy\u0144ski", "Arkadiusz Jachnik", "Wojciech Kotlowski", "Willem Waegeman", "Eyke H\u00fcllermeier"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Dembczy\u0144ski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dembczy\u0144ski et al\\.", "year": 2013}, {"title": "On Nonlinear Fractional Programming", "author": ["Werner Dinkelbach"], "venue": "Management Science,", "citeRegEx": "Dinkelbach.,? \\Q1967\\E", "shortCiteRegEx": "Dinkelbach.", "year": 1967}, {"title": "Multi-Label Prediction via Compressed Sensing", "author": ["Daniel Hsu", "Sham Kakade", "John Langford", "Tong Zhang"], "venue": "In 23rd Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "On Some Properties of Programming Problems in Parametric Form Pertaining to Fractional Programming", "author": ["R. Jagannathan"], "venue": "Management Science,", "citeRegEx": "Jagannathan.,? \\Q1966\\E", "shortCiteRegEx": "Jagannathan.", "year": 1966}, {"title": "Communication-Efficient Distributed Dual Coordinate Ascent", "author": ["Martin Jaggi", "Virginia Smith", "Martin Tak\u00e1c", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I. Jordan"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Cutting-plane training of structural SVMs", "author": ["Thorsten Joachims", "Thomas Finley", "Chun-Nam John Yu"], "venue": "Machine Learning,", "citeRegEx": "Joachims et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2009}, {"title": "On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions", "author": ["Purushottam Kar", "Bharath K Sriperumbudur", "Prateek Jain", "Harish Karnick"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Kar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2013}, {"title": "Online and Stochastic Gradient Methods for Nondecomposable Loss Functions", "author": ["Purushottam Kar", "Harikrishna Narasimhan", "Prateek Jain"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Kar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2014}, {"title": "Learning without default: a study of one-class classification and the low-default portfolio problem", "author": ["Kenneth Kennedy", "Brian Mac Namee", "Sarah Jane Delany"], "venue": "In International Conference on Artificial Intelligence and Cognitive Science (ICAICS),", "citeRegEx": "Kennedy et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kennedy et al\\.", "year": 2010}, {"title": "Consistent Binary Classification with Generalized Performance Metrics", "author": ["Oluwasanmi O. Koyejo", "Nagarajan Natarajan", "Pradeep K. Ravikumar", "Inderjit S. Dhillon"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Koyejo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koyejo et al\\.", "year": 2014}, {"title": "Evaluating and optimizing autonomous text classification systems", "author": ["D.D. Lewis"], "venue": "In 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Lewis.,? \\Q1995\\E", "shortCiteRegEx": "Lewis.", "year": 1995}, {"title": "A Quadratic Mean based Supervised Learning Model for Managing Data Skewness", "author": ["W. Liu", "S. Chawla"], "venue": "In 11th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "Liu and Chawla.,? \\Q2011\\E", "shortCiteRegEx": "Liu and Chawla.", "year": 2011}, {"title": "Stochastic Convex Optimization with Multiple Objectives", "author": ["Mehrdad Mahdavi", "Tianbao Yang", "Rong Jin"], "venue": "In 27th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Mahdavi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2013}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "On the Statistical Consistency of Plug-in Classifiers for Non-decomposable Performance Measures", "author": ["Harikrishna Narasimhan", "Rohit Vaish", "Shivani Agarwal"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Narasimhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2014}, {"title": "Optimizing F-Measures by Cost-Sensitive Classification", "author": ["Shameem Puthiya Parambath", "Nicolas Usunier", "Yves Grandvalet"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Parambath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Parambath et al\\.", "year": 2014}, {"title": "Online Learning: Beyond Regret", "author": ["Alexander Rakhlin", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In 24th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Rakhlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2011}, {"title": "Fractional Programming. II, on Dinkelbach\u2019s Algorithm", "author": ["Siegfried Schaible"], "venue": "Management Science,", "citeRegEx": "Schaible.,? \\Q1976\\E", "shortCiteRegEx": "Schaible.", "year": 1976}, {"title": "Pegasos: primal estimated sub-gradient solver for SVM", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Math. Program.,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["Marina Sokolova", "Guy Lapalme"], "venue": "Information Processing & Management,", "citeRegEx": "Sokolova and Lapalme.,? \\Q2009\\E", "shortCiteRegEx": "Sokolova and Lapalme.", "year": 2009}, {"title": "An Introduction to Signal Detection and Estimation", "author": ["P.H. Vincent"], "venue": null, "citeRegEx": "Vincent.,? \\Q1994\\E", "shortCiteRegEx": "Vincent.", "year": 1994}, {"title": "Optimizing F-Measures: A Tale of Two Approaches", "author": ["Nan Ye", "Kian Ming A. Chai", "Wee Sun Lee", "Hai Leong Chieu"], "venue": "In 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Ye et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2012}, {"title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent", "author": ["Martin Zinkevich"], "venue": "In 20th International Conference on Machine Learning (ICML),", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Class imbalance is also often introduced as a result of the reduction of a problem to binary classification, such as in multi-class problems Bishop [2006] and multi-label problems due to extreme label sparsity Hsu et al.", "startOffset": 141, "endOffset": 155}, {"referenceID": 0, "context": "Class imbalance is also often introduced as a result of the reduction of a problem to binary classification, such as in multi-class problems Bishop [2006] and multi-label problems due to extreme label sparsity Hsu et al. [2009]. Traditional performance measures such as misclassification rate are ill-suited in such situations as it is usually trivial to optimize them by constantly predicting the majority class.", "startOffset": 141, "endOffset": 228}, {"referenceID": 0, "context": "Class imbalance is also often introduced as a result of the reduction of a problem to binary classification, such as in multi-class problems Bishop [2006] and multi-label problems due to extreme label sparsity Hsu et al. [2009]. Traditional performance measures such as misclassification rate are ill-suited in such situations as it is usually trivial to optimize them by constantly predicting the majority class. Instead, the performance measures of choice in such cases are those that perform a more holistic evaluation over the entire data. Naturally, these performance measures are non-decomposable over the dataset and cannot be cannot be expressed as a sum of errors on individual data points. Popular examples include F-measure, G-mean, H-mean etc. A consistent effort directed at optimizing these performance measures has, over the years, resulted in the development of two broad approaches - 1) surrogate based approaches (e.g. SVMPerf Joachims et al. [2009]) that design convex surrogates for these performance measures, and 2) indirect approaches which include cost-sensitive classification-based approaches Parambath et al.", "startOffset": 141, "endOffset": 968}, {"referenceID": 0, "context": "Class imbalance is also often introduced as a result of the reduction of a problem to binary classification, such as in multi-class problems Bishop [2006] and multi-label problems due to extreme label sparsity Hsu et al. [2009]. Traditional performance measures such as misclassification rate are ill-suited in such situations as it is usually trivial to optimize them by constantly predicting the majority class. Instead, the performance measures of choice in such cases are those that perform a more holistic evaluation over the entire data. Naturally, these performance measures are non-decomposable over the dataset and cannot be cannot be expressed as a sum of errors on individual data points. Popular examples include F-measure, G-mean, H-mean etc. A consistent effort directed at optimizing these performance measures has, over the years, resulted in the development of two broad approaches - 1) surrogate based approaches (e.g. SVMPerf Joachims et al. [2009]) that design convex surrogates for these performance measures, and 2) indirect approaches which include cost-sensitive classification-based approaches Parambath et al. [2014] which solve weighted classification problems, and plug-in approaches Koyejo et al.", "startOffset": 141, "endOffset": 1143}, {"referenceID": 0, "context": "Class imbalance is also often introduced as a result of the reduction of a problem to binary classification, such as in multi-class problems Bishop [2006] and multi-label problems due to extreme label sparsity Hsu et al. [2009]. Traditional performance measures such as misclassification rate are ill-suited in such situations as it is usually trivial to optimize them by constantly predicting the majority class. Instead, the performance measures of choice in such cases are those that perform a more holistic evaluation over the entire data. Naturally, these performance measures are non-decomposable over the dataset and cannot be cannot be expressed as a sum of errors on individual data points. Popular examples include F-measure, G-mean, H-mean etc. A consistent effort directed at optimizing these performance measures has, over the years, resulted in the development of two broad approaches - 1) surrogate based approaches (e.g. SVMPerf Joachims et al. [2009]) that design convex surrogates for these performance measures, and 2) indirect approaches which include cost-sensitive classification-based approaches Parambath et al. [2014] which solve weighted classification problems, and plug-in approaches Koyejo et al. [2014], Narasimhan et al.", "startOffset": 141, "endOffset": 1233}, {"referenceID": 0, "context": "Class imbalance is also often introduced as a result of the reduction of a problem to binary classification, such as in multi-class problems Bishop [2006] and multi-label problems due to extreme label sparsity Hsu et al. [2009]. Traditional performance measures such as misclassification rate are ill-suited in such situations as it is usually trivial to optimize them by constantly predicting the majority class. Instead, the performance measures of choice in such cases are those that perform a more holistic evaluation over the entire data. Naturally, these performance measures are non-decomposable over the dataset and cannot be cannot be expressed as a sum of errors on individual data points. Popular examples include F-measure, G-mean, H-mean etc. A consistent effort directed at optimizing these performance measures has, over the years, resulted in the development of two broad approaches - 1) surrogate based approaches (e.g. SVMPerf Joachims et al. [2009]) that design convex surrogates for these performance measures, and 2) indirect approaches which include cost-sensitive classification-based approaches Parambath et al. [2014] which solve weighted classification problems, and plug-in approaches Koyejo et al. [2014], Narasimhan et al. [2014] which rely on consistent estimates of class probabilities.", "startOffset": 141, "endOffset": 1259}, {"referenceID": 19, "context": "For large datasets, streaming methods such as stochastic gradient descent Shalev-Shwartz et al. [2011] that take only a few passes over the entire data are preferable.", "startOffset": 74, "endOffset": 103}, {"referenceID": 9, "context": "Recently, Kar et al. [2014] proposed optimizing SVMPerf-style surrogates using SGD techniques.", "startOffset": 10, "endOffset": 28}, {"referenceID": 9, "context": "A third approach applicable to certain performance measures is the decision-theoretic method that learns a class probability estimate and computes predictions that maximize the expected value of the performance measure on a test set Lewis [1995], Ye et al.", "startOffset": 233, "endOffset": 246}, {"referenceID": 9, "context": "A third approach applicable to certain performance measures is the decision-theoretic method that learns a class probability estimate and computes predictions that maximize the expected value of the performance measure on a test set Lewis [1995], Ye et al. [2012]. In addition to these there exist methods dedicated to specific performance measures.", "startOffset": 233, "endOffset": 264}, {"referenceID": 9, "context": "A third approach applicable to certain performance measures is the decision-theoretic method that learns a class probability estimate and computes predictions that maximize the expected value of the performance measure on a test set Lewis [1995], Ye et al. [2012]. In addition to these there exist methods dedicated to specific performance measures. For instance Parambath et al. [2014] focus on optimizing F-measure by exploiting the pseudo-linearity of the function along with a cross validation-based strategy.", "startOffset": 233, "endOffset": 387}, {"referenceID": 3, "context": "For instance, Dembczy\u0144ski et al. [2013] study plug-in style methods for maximizing F-measure in multi-label settings whereas works such as Koyejo et al.", "startOffset": 14, "endOffset": 40}, {"referenceID": 3, "context": "For instance, Dembczy\u0144ski et al. [2013] study plug-in style methods for maximizing F-measure in multi-label settings whereas works such as Koyejo et al. [2014], Narasimhan et al.", "startOffset": 14, "endOffset": 160}, {"referenceID": 3, "context": "For instance, Dembczy\u0144ski et al. [2013] study plug-in style methods for maximizing F-measure in multi-label settings whereas works such as Koyejo et al. [2014], Narasimhan et al. [2014], Ye et al.", "startOffset": 14, "endOffset": 186}, {"referenceID": 3, "context": "For instance, Dembczy\u0144ski et al. [2013] study plug-in style methods for maximizing F-measure in multi-label settings whereas works such as Koyejo et al. [2014], Narasimhan et al. [2014], Ye et al. [2012] study plug-in approaches for the same problem in the more challenging binary classification setting.", "startOffset": 14, "endOffset": 204}, {"referenceID": 3, "context": "For instance, Dembczy\u0144ski et al. [2013] study plug-in style methods for maximizing F-measure in multi-label settings whereas works such as Koyejo et al. [2014], Narasimhan et al. [2014], Ye et al. [2012] study plug-in approaches for the same problem in the more challenging binary classification setting. Historically, online learning algorithms have played a key role in designing solvers for large-scale batch problems. However, for non-decomposable loss functions, defining an online learning framework and providing efficient algorithms with small regret itself is challenging. Rakhlin et al. [2011] propose a generic method for such loss functions; however the algorithms proposed therein run in exponential time.", "startOffset": 14, "endOffset": 604}, {"referenceID": 3, "context": "For instance, Dembczy\u0144ski et al. [2013] study plug-in style methods for maximizing F-measure in multi-label settings whereas works such as Koyejo et al. [2014], Narasimhan et al. [2014], Ye et al. [2012] study plug-in approaches for the same problem in the more challenging binary classification setting. Historically, online learning algorithms have played a key role in designing solvers for large-scale batch problems. However, for non-decomposable loss functions, defining an online learning framework and providing efficient algorithms with small regret itself is challenging. Rakhlin et al. [2011] propose a generic method for such loss functions; however the algorithms proposed therein run in exponential time. Kar et al. [2014] also study such measures with the", "startOffset": 14, "endOffset": 737}, {"referenceID": 25, "context": "Our methods make use of standard online convex optimization primitives Zinkevich [2003]. However, their application requires special care in order to avoid divergent behavior.", "startOffset": 71, "endOffset": 88}, {"referenceID": 23, "context": "A large number of popular performance measures fall in this family since these measures are relevant in situations with severe label imbalance or in situations where cost-sensitive classification is required such as detection theory Vincent [1994]. Table 1 gives a list of such performance measures along with some of their relevant properties and references to works that utilize these performance measures.", "startOffset": 233, "endOffset": 248}, {"referenceID": 20, "context": "Min (Vincent [1994]) min{P,N} X X {\u03b1+ \u03b2 = 1} \u2229 R+ 0 H-mean (Kennedy et al.", "startOffset": 5, "endOffset": 20}, {"referenceID": 10, "context": "Min (Vincent [1994]) min{P,N} X X {\u03b1+ \u03b2 = 1} \u2229 R+ 0 H-mean (Kennedy et al. [2010]) 2PN P+N X X 4 {\u221a \u03b1+ \u221a \u03b2 \u2265 \u221a 2 } \u2229 B(0, 2) 0 Q-mean (Liu and Chawla [2011]) 1\u2212 \u221a (1\u2212P )2+(1\u2212N)2 2 X X { \u03b1 + \u03b2 \u2264 1/2 } \u2229 R+ 1 G-mean (Daskalaki et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 10, "context": "Min (Vincent [1994]) min{P,N} X X {\u03b1+ \u03b2 = 1} \u2229 R+ 0 H-mean (Kennedy et al. [2010]) 2PN P+N X X 4 {\u221a \u03b1+ \u221a \u03b2 \u2265 \u221a 2 } \u2229 B(0, 2) 0 Q-mean (Liu and Chawla [2011]) 1\u2212 \u221a (1\u2212P )2+(1\u2212N)2 2 X X { \u03b1 + \u03b2 \u2264 1/2 } \u2229 R+ 1 G-mean (Daskalaki et al.", "startOffset": 60, "endOffset": 157}, {"referenceID": 2, "context": "[2010]) 2PN P+N X X 4 {\u221a \u03b1+ \u221a \u03b2 \u2265 \u221a 2 } \u2229 B(0, 2) 0 Q-mean (Liu and Chawla [2011]) 1\u2212 \u221a (1\u2212P )2+(1\u2212N)2 2 X X { \u03b1 + \u03b2 \u2264 1/2 } \u2229 R+ 1 G-mean (Daskalaki et al. [2006]) \u221a PN X 7 3 \u221a {\u03b1\u03b2 \u2265 1/4} \u2229 R+ 0", "startOffset": 140, "endOffset": 164}, {"referenceID": 9, "context": "Recent works Kar et al. [2013, 2014] have tried to resolve this issue by looking at mini-batch methods or by using a buffer to maintain a sketch of the stream. However, such techniques bring in a bias into the learning algorithm in the form of buffer size or mini batch length which results in slower convergence. Indeed, the 1PMB method of Kar et al. [2014] is only able to guarantee a \u22124 \u221a T rate of convergence, whereas SGD techniques are usually able to guarantee \u22122 \u221a T rates.", "startOffset": 13, "endOffset": 359}, {"referenceID": 7, "context": "We note here that primal dual frameworks have been utilized before in diverse areas such as distributed optimization Jaggi et al. [2014] and multi-objective optimization Mahdavi et al.", "startOffset": 117, "endOffset": 137}, {"referenceID": 7, "context": "We note here that primal dual frameworks have been utilized before in diverse areas such as distributed optimization Jaggi et al. [2014] and multi-objective optimization Mahdavi et al. [2013]. However, these works simply assume the functions involved therein to be Lipschitz and/or smooth and do not address cases where they fail to be so.", "startOffset": 117, "endOffset": 192}, {"referenceID": 18, "context": "We refer the reader to Parambath et al. [2014] for a more relaxed introduction to these functions and their properties.", "startOffset": 23, "endOffset": 47}, {"referenceID": 18, "context": "We refer the reader to Parambath et al. [2014] for a more relaxed introduction to these functions and their properties. For our purposes, however, it suffices to notice that this property immediately points toward a cost-sensitive method to optimize these performance measures. This fact was noticed by Parambath et al. [2014] who exploited this to develop a cost-sensitive classification method for optimizing the F-measure by simply searching for the best weights with which to perform cost-sensitive classification.", "startOffset": 23, "endOffset": 327}, {"referenceID": 18, "context": "Pseudo-linear functions have enjoyed a fair amount of interest in the optimization community Schaible [1976], Dinkelbach [1967], Jagannathan [1966] within the sub-field of fractional programming.", "startOffset": 93, "endOffset": 109}, {"referenceID": 4, "context": "Pseudo-linear functions have enjoyed a fair amount of interest in the optimization community Schaible [1976], Dinkelbach [1967], Jagannathan [1966] within the sub-field of fractional programming.", "startOffset": 110, "endOffset": 128}, {"referenceID": 4, "context": "Pseudo-linear functions have enjoyed a fair amount of interest in the optimization community Schaible [1976], Dinkelbach [1967], Jagannathan [1966] within the sub-field of fractional programming.", "startOffset": 110, "endOffset": 148}, {"referenceID": 4, "context": "Pseudo-linear functions have enjoyed a fair amount of interest in the optimization community Schaible [1976], Dinkelbach [1967], Jagannathan [1966] within the sub-field of fractional programming. Of the many methods that have been developed to optimize these functions, the DinkelbachJagannathan (DJ) procedure Dinkelbach [1967], Jagannathan [1966] is of specific interest to us.", "startOffset": 110, "endOffset": 329}, {"referenceID": 4, "context": "Pseudo-linear functions have enjoyed a fair amount of interest in the optimization community Schaible [1976], Dinkelbach [1967], Jagannathan [1966] within the sub-field of fractional programming. Of the many methods that have been developed to optimize these functions, the DinkelbachJagannathan (DJ) procedure Dinkelbach [1967], Jagannathan [1966] is of specific interest to us.", "startOffset": 110, "endOffset": 349}, {"referenceID": 4, "context": "Pseudo-linear functions have enjoyed a fair amount of interest in the optimization community Schaible [1976], Dinkelbach [1967], Jagannathan [1966] within the sub-field of fractional programming. Of the many methods that have been developed to optimize these functions, the DinkelbachJagannathan (DJ) procedure Dinkelbach [1967], Jagannathan [1966] is of specific interest to us. It turns out that the AMP method can be seen as performing DJ-style updates over parameterized spaces (the parameter being the model w). It is known (for instance see Schaible [1976]) that the DJ process is able to offer a linear convergence rates.", "startOffset": 110, "endOffset": 563}, {"referenceID": 4, "context": "Pseudo-linear functions have enjoyed a fair amount of interest in the optimization community Schaible [1976], Dinkelbach [1967], Jagannathan [1966] within the sub-field of fractional programming. Of the many methods that have been developed to optimize these functions, the DinkelbachJagannathan (DJ) procedure Dinkelbach [1967], Jagannathan [1966] is of specific interest to us. It turns out that the AMP method can be seen as performing DJ-style updates over parameterized spaces (the parameter being the model w). It is known (for instance see Schaible [1976]) that the DJ process is able to offer a linear convergence rates. Our proof of Theorem 8, which was obtained independently, can then be seen as giving a similar result in the parameterized setting. However, we wish to move one step further and optimize these performance measures in an online stochastic manner. To this end, we observe that the AMP algorithm can be executed in an online fashion by using stochastic updates to train the intermediate models. The resulting algorithm STAMP, is presented in Algorithm 3. However, this algorithm is much harder to analyze because unlike AMP which has the luxury of offering exact updates, STAMP offers inexact, even noisy updates. Indeed, even existing works in the optimization community (for example Schaible [1976]) do not seem to have analyzed DJ-style methods with noisy updates.", "startOffset": 110, "endOffset": 1327}, {"referenceID": 8, "context": "cases we compared to the SVMPerf method Joachims et al. [2009] and plug-in method Koyejo et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 8, "context": "cases we compared to the SVMPerf method Joachims et al. [2009] and plug-in method Koyejo et al. [2014] specialized to these measures.", "startOffset": 40, "endOffset": 103}, {"referenceID": 8, "context": "cases we compared to the SVMPerf method Joachims et al. [2009] and plug-in method Koyejo et al. [2014] specialized to these measures. For the sake of reference, we also compared to the standard logistic regression method for (unweighted) binary classification. Additionally for F1-measure, we also compared to the 1PMB stochastic gradient descent method proposed recently by Kar et al. [2014]. All methods were implemented in C.", "startOffset": 40, "endOffset": 393}, {"referenceID": 9, "context": "Secondly, we note that both SVMPerf and 1PMB optimize the same \u201cstruct-SVM\u201d style surrogate for the F-measure Kar et al. [2014]. This surrogate has been observed to give poor accuracies when compared to plug-in methods in several previous works Koyejo et al.", "startOffset": 110, "endOffset": 128}, {"referenceID": 9, "context": "Secondly, we note that both SVMPerf and 1PMB optimize the same \u201cstruct-SVM\u201d style surrogate for the F-measure Kar et al. [2014]. This surrogate has been observed to give poor accuracies when compared to plug-in methods in several previous works Koyejo et al. [2014], Narasimhan et al.", "startOffset": 110, "endOffset": 266}, {"referenceID": 9, "context": "Secondly, we note that both SVMPerf and 1PMB optimize the same \u201cstruct-SVM\u201d style surrogate for the F-measure Kar et al. [2014]. This surrogate has been observed to give poor accuracies when compared to plug-in methods in several previous works Koyejo et al. [2014], Narasimhan et al. [2014]. STAMP on the other hand, works directly with F-measure in a manner similar to, but faster than, the plug-in methods which might explain its better performance.", "startOffset": 110, "endOffset": 292}], "year": 2015, "abstractText": "Modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics, and require optimizing performance measures that are non-decomposable over the dataset, such as F-measure. Such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent. In this paper we reveal that for two large families of performance measures that can be expressed as functions of true positive/negative rates, it is indeed possible to implement point stochastic updates. The families we consider are concave and pseudo-linear functions of TPR, TNR which cover several popularly used performance measures such as F-measure, G-mean and H-mean. Our core contribution is an adaptive linearization scheme for these families, using which we develop optimization techniques that enable truly point-based stochastic updates. For concave performance measures we propose SPADE, a stochastic primal dual solver; for pseudo-linear measures we propose STAMP, a stochastic alternate maximization procedure. Both methods have crisp convergence guarantees, demonstrate significant speedups over existing methods often by an order of magnitude or more, and give similar or more accurate predictions on test data.", "creator": "LaTeX with hyperref package"}}}