{"id": "1505.06813", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2015", "title": "Surrogate Functions for Maximizing Precision at the Top", "abstract": "The problem of maximizing precision at the top of a ranked list, often dubbed Precision@k (prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure.", "histories": [["v1", "Tue, 26 May 2015 06:01:24 GMT  (95kb,D)", "http://arxiv.org/abs/1505.06813v1", "To appear in the the proceedings of the 32nd International Conference on Machine Learning (ICML 2015)"]], "COMMENTS": "To appear in the the proceedings of the 32nd International Conference on Machine Learning (ICML 2015)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["purushottam kar", "harikrishna narasimhan", "prateek jain 0002"], "accepted": true, "id": "1505.06813"}, "pdf": {"name": "1505.06813.pdf", "metadata": {"source": "CRF", "title": "Surrogate Functions for Maximizing Precision at the Top", "authors": ["Purushottam Kar", "Harikrishna Narasimhan", "Prateek Jain"], "emails": ["t-purkar@microsoft.com", "harikrishna@csa.iisc.ernet.in", "prajain@microsoft.com", "Precision@k", "(prec@k),", "prec@k.", "prec@k.", "prec@k", "prec@k", "prec@k", "prec@k.", "Precision@k,", "Precision@k", "(prec@k)", "prec@k", "prec@k", "prec@k,", "prec@k"], "sections": [{"heading": null, "text": "The most notable of these is the lack of a convex upper limit for Prec @ k. At the heart of our results is a family of truly superficial surrogates for Prec @ k. These surrogates are motivated in principle and have attractive properties such as consistency to Prec @ k under various natural boundary / noise conditions. These surrogates are then used to design a class of novel Perceptron algorithms to optimize Prec @ k with detectable error limits. In addition, we develop scalable stochastic methods for descending gradients with detectable convergence boundaries. Our evidence is based on novel uniform convergence boundaries that require in-depth analysis of the structural properties of Prec @ k and its environment."}, {"heading": "1 Introduction", "text": "The ranking of particular items or labels according to their relevance is at the core of several real learning systems. For example, in the case of classification problems with a rare class, as is the case with spam / anomalies detection, the aim is to classify the given e-mails / events according to their probability to come from the rare class (spam / anomaly). Similarly, in the case of problems with multiple labels, the aim is to classify the labels according to their probability to be relevant to a data point, Tsoumakas and Katakis [2007]. Ranking items at the top in these applications is of utmost importance and several performance measures, such as Precision @ k, Average Precision and NDCG have been designed to promote accuracy at the top of the rankings, of which Precision @ k (prec @ k) is a measure that is particularly popular in a variety of domains."}, {"heading": "1.1 Our Contributions", "text": "This year, we will be able to live in the United States for the next two years, in which we will be able to live in Europe, \"he said."}, {"heading": "1.2 Related Work", "text": "Whereas previous methods for this problem, such as RankSVM, focused on optimizing pairwise ranking accuracy Herbrich et al. [2000], Joachims [2002], Freund et al. [2003], Burges et al. [2005], there has recently been enormous interest in performance measures that promote good ranking performance at the top of the ranking list, and in methods that directly optimize these measures Cle \u0301 menc and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a], Li et al. [2014] In this work, we focus on one such assessment measure - Precision @ k, which is widely used in practice."}, {"heading": "2 Problem Formulation and Notation", "text": "We get a set of dots (xi, yi),.., (xn, yn), where xi, X and yi, {0, 1}. We will use X to denote the entire dataset, X + and X \u2212 to denote the set of positive and negative (zero) points, and y, {0, 1} n to denote the label vector. z = (x, y) is meant to denote a designated data point. Our results willingly extend to multiple mark and ranking settings, but for simplicity's sake, we focus only on two-part ranking problems, where the goal is to rank (a subset) of positive examples above the negative points."}, {"heading": "3 A Family of Novel Surrogates for prec@k", "text": "Since prec @ k is a non-convex loss function that is difficult to optimize directly, it is natural to look for replacement functions that act as a good substitute for prec @ k. There will be two characteristics that we will want from such a surrogate: 1. Upper boundary property: The surrogate should limit the prec @ k loss function upwards so that the minimization of the surrogate promotes small prec @ k losses. 2. Conditional consistency: Under certain regularity assumptions, the optimization of the surrogate should also provide an optimal solution for prec @ k. Motivated by the above requirements, we are developing a family of surrogates that limits the prec @ k loss function upwards and is consistent with it under certain marginal / noise conditions. We note that the results of Calauze, nes et al. [2012], which include the possibility of consistent convex surrogates for the ranking of 2005 performance measures, are not applicable to this work, nor are they necessary for a prec @."}, {"heading": "3.1 The Curious Case of `structprec@k(\u00b7)", "text": "The \"structprec @ k (\u00b7) surrogate is part of a broad class of surrogates called struct-SVM surrogates designed for structured output prediction problems that can have exponentially large output spaces.\" structprec @ k (\u00b7) is defined as maximum output prediction problems that can have exponentially large output spaces. (3) The above surrogate penalizes a scoring function if there are a set of k points with large scores (i.e. the second term is large) that are actually negative (i.e. the first term is large), but since the candidate limits himself to calling only k points positive, whereas the true label vector y has n + positives, in cases where n + > k, a non-optimal candidate label can exploit the remaining n + \u2212 k hierarchy to hide those gates that do not represent high scores for the surrogate @."}, {"heading": "3.2 The Ramp Surrogate `rampprec@k(\u00b7)", "text": "The key to maximizing prec @ k in a split leaderboard is to select a subset of k-relevant points and rank them at the top k positions. (This can happen if the top k-relevant points are not topped by any irrelevant point.) Therefore, a surrogate must show a margin that leads to irrelevant points higher than the top k-relevant points. (This is not the reason why we show this margin. (4) The term (P) contains the sum of the points of the highest score. (Note: \"rampprec @ k @ k\" is similar to the \"ramp-relevant\" losses for the \"rampprec.\" (4) We have the sum of the points of the highest score of the. \"we\" rampprec @ k \"is similar to the\" rampprec @ k. \""}, {"heading": "3.3 The Max Surrogate `maxprec@k(\u00b7)", "text": "An immediate convex upper limit for (Q) is achieved by replacing the sum of the values of the n + \u2212 k lowest weighted positive values with those of the highest weighted values as follows: (Q) \u2264 max y \u00b2 (1 \u2212 y \u00b2) \u00b7 y \u00b2 1 = n + \u2212 k \u00b2 n = 1 y \u00b2 isi, which gives us the \"maxprec @ k (s) substitute size defined below: maxprec @ k \u00b2 1 = k \u00b2 (y, y \u00b2) + n \u00b2 i = 1 (y \u00b2 i \u2212 yi) si + max y \u00b2 (1 \u2212 y \u00b2) \u00b7 y \u00b2 y \u00b2 1 = n + \u2212 kn \u00b2 i = 1 y \u00b2 isi. (6) The upper limit given above, which represents a point maximum of convex functions, is convex-shaped, as well as an upper limit for prec @ k (s) si (s) because it is the upper limit of the condition\" rampprec @ k (s) consistent."}, {"heading": "3.4 The Avg Surrogate `avgprec@k(\u00b7)", "text": "iii. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i."}, {"heading": "4 Perceptron & SGD Algorithms for prec@k", "text": "It is not only the way in which we can apply the ranking, but also the way in which we can apply the ranking of individual data points (e.g. individual queries in the ranking settings).It is useful to note that the requirement for mini-batches in ranking and multi-label settings can be applied to individual data points for our algorithms (e.g. individual queries in the ranking settings).At any time, our algorithms receive a batch of B-points Xt = [x1t]., x b] and rank these points using the existing model. Let us explain the prec @ k losses (Eq. 1) at time."}, {"heading": "5 Generalization Bounds", "text": "We will use these UC limits, together with the error limits in theories 7 and 9, to prove two key results - 1) to establish our generalization and convergence limits for the PERCEPTRON @ K-AVG and the PERCEPTRON @ K-MAX algorithms and, 2) to establish a convergence guarantee for the SGD @ K-AVG algorithms. To better represent our generalization and convergence limits, we will use standardized versions of Prec @ k and the environments. To do this, we will write a convergence method for the SGD @ K-AVG algorithms."}, {"heading": "6 Experiments", "text": "It is indeed the case that we will be able to go in search of a solution that meets the needs of the people."}, {"heading": "Acknowledgments", "text": "HN thanks the support of a Google India PhD Fellowship."}, {"heading": "A Structural SVM Surrogate for prec@k", "text": "The structural SVM surrogate for Prec @ 1 on these points has a clear sign. < K-model for a series of n points {(x1, y1),. < W-model (xn, yn),.. W-model (xn, yn),.. W-model (xn, yn),. W-model (xn, yn),. W-model (w-model),. W-model (w-model),. W-model (w-model),. W-model (w-model),. W-model (w-model),. W-model (w-model),. W-model (w-model),. W-model (w-model),. W-model (w-model),. W-model."}, {"heading": "B Proofs of Claims from Section 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Claim 1", "text": "Claim 1: For each k \u2264 n + and scoring function s we have \"Rampprec @ k (s) \u2265 prec @ k (s). Furthermore, if for any scoring function we have s\" Rampprec @ k (s) \u2264, \"then there is necessarily a sentence S = [n] of size at most k, so that for all k = k we have a row of\" i \"S si = n\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i. \"i\" i \"i\" i. \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i. \"i\" i \"i\" i. \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i"}, {"heading": "B.2 Proof of Claim 3", "text": "Claim 3 for each scoring function s, which realizes the weak k margin over a data set we have, \"Rampprec @ k (s) = Prec @ k (s) = Proof.Consider a scoring function s, which fulfills the weak k margin condition, and any y margin condition, which contains a positive y margin condition. (Based on the prec @ k accuracy of the y margin, we have the following two cases Case 1 (K (y, y margin) = k): In this case, we have a positive K margin (y, y margin) + positive K margin (y margin) + positive K margin condition i = k margin) = k margin (y margin) = k margin (y margin)."}, {"heading": "B.3 A Useful Supplementary Lemma", "text": "16. In view of a series of n real numbers x1.. xn and any two integers k \u2264 k \u2032 \u2264 n, we assume that k \"> k.\" Without loss of generality, we assume that the set is ordered in ascending order, i.e. x1 \u2264 x2 \u2264... \u2264 xn. Thus, the above statement is equivalent to the representation that 1k \"i = 1 xi \u2264 1 k,\" \"j = 1 xj\" (1 k \u2212 1 k \") k\" i = 1 k \"k\" k \"i = 1 xi \u2264 1 k\" k, \"with the last inequality since k \u2212 k\" > 0 being true and the left side being the average of the numbers, which are all smaller than the middle side, the right side of which forms the lemm."}, {"heading": "B.4 Proof of the Upper-bounding Property for the `avgprec@k(\u00b7) Surrogate", "text": "Claim 17: For each k + \u2212 n \u2212 si = k = i = i = i = i = i = i = i (s) y (s) y (s) y (s) y (s) y (s) y (s) y (s) i (s) i (s) i (s) i (s) i (s) i (k) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i i (s) i i i (s) i i i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s) i (s)"}, {"heading": "B.5 Proof of Claim 6", "text": "si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si-si si si si-si si si si"}, {"heading": "C Proofs from Section 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Proof of Theorem 7", "text": "We will prove the theorems with two lemmats that we below.Lemma 18. For each time in which we are satisfied with two lemmats, we have an answer to the question of whether we justify the theorems with two lemmats that we below.Lemma 18. For each time in which we are satisfied with two lemmats, we have an answer to the question of whether we define the theorems with two lemmats that we below.Lemma 18. That time in which we define the theorem with two lemmats that we below.Lemma, we define the theorem with two lemmats. That time in which we define the theorem with two lemmats, that time in which we define the theorem with two lemmats, that time in which we define the theorem with two lemmats, that time in which we define the theorem with two lemmats."}, {"heading": "C.2 Proof of Theorem 9", "text": "Theorem 9, we define Pt: = < p > p > p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p) p (1) p) p (1) p (1) p (1) p (1) p) p (1) p (1) p (1) p (1) p (p) p (1) p) p (p) p (1) p) p (p) p (1) p) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p) p (p) p (p) p (p) p) p (\"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p i \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p i \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" i \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "D Proof of Theorem 12", "text": "Our evidence for Theorem 12 relies crucially on the following two quotations, which help us in using the structure of our replacement functions. The first basic problem is that the meaningful superiority of a number of Lipschitz functions also includes Lipschitz.Lemma 21. Let f1,.., fm be m real rated functions fi: Rn \u2192 R so that every fi is 1-Lipschitz in relation to the top of the rankings. Then, the functionality (v) = max i [m] fi (v) is 1-Lipschitz in relation to the selected total population. The second problem establishes the convergence of additive estimates over the top of the rankings. The abstract nature of the result would allow us to apply it to a variety of situations and would be crucial for our analyses. Lemma 22. Let V be a universe with an overall order."}, {"heading": "D.1 A Uniform Convergence Bound for the prec@\u03ba(\u00b7) Performance Measure", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 9, 9, 8, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "D.2 A Uniform Convergence Bound for the `rampprec@\u03ba(\u00b7) Surrogate", "text": "Let us first remember the form of the (normalized) surrogate below - note that it is a non-convex surrogate. Let us also remember that k = \u03ba \u00b7 n + (y). \"rampprec @ (w; z1,..., zn) = max.: y = k (y, y) k + 1k n: i = 1 y: iw > xi} 1 (w; z1,..., zn) \u2212 max: y: 1 = k K (y, y) = k1k n: i = 1 y: iw > xi 2 (w; z1,..., zn) We will now show that both the functions \u03c61 (\u00b7) and \u0432 2 (\u00b7) have a uniform convergence, which should be sufficient to prove that\" rampprec @ (\u00b7) has a uniform convergence."}, {"heading": "D.2.1 A Uniform Convergence Result for \u03a81(\u00b7)", "text": "We have 1 (w; z1,., zn) = max. (b) = max. (b) = max. (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b). (b).....c). (b). (b).....c). (b). (b). (b). (b).....c). (b). (b). (b). (b). (b). (b). (c). (c). (c). c). c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c. c"}, {"heading": "D.2.2 A Uniform Convergence Result for \u03a82(\u00b7)", "text": "The proof follows here in a similar manner with a direct application of sequence 29, which shows us that the number 2 (\u00b7) is Lipschitz, and an application of Lemma 22 together with the observation that the number 1 2b log 2 \u03b4 is similar to the discussion used above, which concludes the point-by-point convergence proof. The above two-part argument establishes the following uniform convergence result for the \"Rampprec @ E (\u00b7) measurement of power Theorem 25. We have, with a probability of at least 1 \u2212 3 above the selection of the b samples, sup w-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-"}, {"heading": "D.3 A Uniform Convergence Bound for the `avgprec@\u03ba(\u00b7) Surrogate", "text": "We will prove this result on the basis of a series of partial results, which we indicate below. As before, we will apply for each W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W W-W-W-W-W-W-W"}, {"heading": "D.4 Proof of Lemma 21", "text": "Lemma 21. Let f1,. \u2212 \u2212 \u2212 fm are m real value functions fi: Rn \u2192 R in such a way that each fi is a 1-Lipschitz norm in relation to the 1-Lipschitz norm. \u2212 fm (v) = max i-fi (v) is 1-Lipschitz in relation to the 1-Lipschitz norm too.Proof. Let g (v) = fi (v) and g (v) = fj (v). \u2212 g (v) \u2212 fi (v) \u2212 v (v), v \u2212 v \"v.\" The premise guarantees us that for all i-fi (v) \u2212 fi (v) \u2212 fi (v) \u2212 fi (v \"v\") \u2212 v. \""}, {"heading": "D.5 Proof of Lemma 22", "text": "Lemma 22: Let V be a universe with an overall order built upon it, and let v1,.., vn be a population of n items ordered in decreasing order. Let v 1,.., v. \u2212 b be a sample selected from the population and also ordered in decreasing order. Then, for all specified h: V, 1, and 1, we have a sample (0, 1) with a probability of at least 1 \u2212 g over the selection of samples at least 1 \u2212 g. For simplicity's sake, we assume that we are both integers, so that there are no rounding."}, {"heading": "D.6 A Uniform Convergence Bound for the `maxprec@\u03ba(\u00b7) Surrogate", "text": "Having demonstrated a generalization limit for the \"avgprec @ \u0443 (\u00b7) surrogate,\" we find that similar techniques that involve dividing the candidate label space into labels with a fixed true positive rate \u03b2, and arguing a uniform convergence for each partition, can also be used to prove a generalization limit for the \"maxprec @ \u0445 (\u00b7) surrogate.\" We will postpone the details of the argument to a later version of the essay."}, {"heading": "E Proof of Theorem 15", "text": "Theorem 15. Let's be the model returned by algorithm 3 when running on a current with T-lots of length b. Then we have a probability of at least 1 \u2212 \u03b4 for each W-lot. The proof for this theorem closely follows that of theorems 7 and 8 in Kar et al. [2014]. More specifically, Theorem 6 of Kar et al. [2014] ensures that any convex loss function demonstrating uniform convergence would ensure a result as we are trying to prove it. Since Theorem 12 confirms that \"avgprec @\" (\u00b7) has uniform convergence, the proof follows. F Additional empirical results"}], "references": [{"title": "The Infinite Push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list", "author": ["S. Agarwal"], "venue": "In 11th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "Agarwal.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal.", "year": 2011}, {"title": "Concentration inequalities", "author": ["Stphane Boucheron", "Gbor Lugosi", "Olivier Bousquet"], "venue": "In Advanced Lectures in Machine Learning,", "citeRegEx": "Boucheron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2004}, {"title": "Accuracy at the top", "author": ["Stephen Boyd", "Corinna Cortes", "Mehryar Mohri", "Ana Radovanovic"], "venue": "In 26th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Boyd et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2012}, {"title": "Learning to rank using gradient descent", "author": ["C. Burges", "T. Shaked", "E. Renshaw", "A. Lazier", "M. Deeds", "N. Hamilton", "G. Hullender"], "venue": "In 22nd International Conference on Machine Learning (ICML),", "citeRegEx": "Burges et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Burges et al\\.", "year": 2005}, {"title": "On the (Non-)existence of Convex, Calibrated Surrogate Losses for Ranking", "author": ["Cl\u00e9ment Calauz\u00e8nes", "Nicolas Usunier", "Patrick Gallinari"], "venue": "In 26th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Calauz\u00e8nes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Calauz\u00e8nes et al\\.", "year": 2012}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li"], "venue": "In 24th International Conference on Machine learning (ICML),", "citeRegEx": "Cao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "Structured Learning for Non-Smooth Ranking Losses", "author": ["Soumen Chakrabarti", "Rajiv Khanna", "Uma Sawant", "Chiru Bhattacharyya"], "venue": "In 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Chakrabarti et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chakrabarti et al\\.", "year": 2008}, {"title": "Perceptron-like algorithms and generalization bounds for learning to rank", "author": ["Sougata Chaudhuri", "Ambuj Tewari"], "venue": "CoRR, abs/1405.0591,", "citeRegEx": "Chaudhuri and Tewari.,? \\Q2014\\E", "shortCiteRegEx": "Chaudhuri and Tewari.", "year": 2014}, {"title": "Online ranking with top-1 feedback", "author": ["Sougata Chaudhuri", "Ambuj Tewari"], "venue": "In 18th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Chaudhuri and Tewari.,? \\Q2015\\E", "shortCiteRegEx": "Chaudhuri and Tewari.", "year": 2015}, {"title": "Ranking the best instances", "author": ["St\u00e9phan Cl\u00e9men\u00e7on", "Nicolas Vayatis"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Cl\u00e9men\u00e7on and Vayatis.,? \\Q2007\\E", "shortCiteRegEx": "Cl\u00e9men\u00e7on and Vayatis.", "year": 2007}, {"title": "Tighter Bounds for Structured Estimation", "author": ["Chuong B. Do", "Quoc Le", "Choon Hui Teo", "Olivier Chapelle", "Alex Smola"], "venue": "In 22nd Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Do et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Do et al\\.", "year": 2008}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Freund et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2003}, {"title": "Hardness of learning halfspaces with noise", "author": ["Venkatesan Guruswami", "Prasad Raghavendra"], "venue": "SIAM J. Comput.,", "citeRegEx": "Guruswami and Raghavendra.,? \\Q2009\\E", "shortCiteRegEx": "Guruswami and Raghavendra.", "year": 2009}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Herbrich et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2000}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Joachims.,? \\Q2002\\E", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "A Support Vector Method for Multivariate Performance Measures", "author": ["Thorsten Joachims"], "venue": "In 22nd International Conference on Machine Learning (ICML),", "citeRegEx": "Joachims.,? \\Q2005\\E", "shortCiteRegEx": "Joachims.", "year": 2005}, {"title": "Online and stochastic gradient methods for nondecomposable loss functions", "author": ["Purushottam Kar", "Harikrishna Narasimhan", "Prateek Jain"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Kar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2014}, {"title": "Direct optimization of ranking measures", "author": ["Quoc V. Le", "Alexander J. Smola"], "venue": "arXiv preprint arXiv:0704.3359,", "citeRegEx": "Le and Smola.,? \\Q2007\\E", "shortCiteRegEx": "Le and Smola.", "year": 2007}, {"title": "Top rank optimization in linear time", "author": ["Nan Li", "Rong Jin", "Zhi-Hua Zhou"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Perceptrons: An Introduction to Computational Geometry", "author": ["Marvin Lee Minsky", "Seymour Papert"], "venue": null, "citeRegEx": "Minsky and Papert.,? \\Q1988\\E", "shortCiteRegEx": "Minsky and Papert.", "year": 1988}, {"title": "A Structural SVM Based Approach for Optimizing Partial AUC", "author": ["Harikrishna Narasimhan", "Shivani Agarwal"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Narasimhan and Agarwal.,? \\Q2013\\E", "shortCiteRegEx": "Narasimhan and Agarwal.", "year": 2013}, {"title": "Optimizing Non-decomposable Performance Measures: A Tale of Two Classes", "author": ["Harikrishna Narasimhan", "Purushottam Kar", "Prateek Jain"], "venue": "In 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "On convergence proofs on perceptrons", "author": ["A.B.J. Novikoff"], "venue": "In Proceedings of the Symposium on the Mathematical Theory of Automata,", "citeRegEx": "Novikoff.,? \\Q1962\\E", "shortCiteRegEx": "Novikoff.", "year": 1962}, {"title": "Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Yashoteja Prabhu", "Manik Varma"], "venue": "In 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Prabhu and Varma.,? \\Q2014\\E", "shortCiteRegEx": "Prabhu and Varma.", "year": 2014}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["Frank Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "Rosenblatt.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1958}, {"title": "The p-norm push: A simple convex ranking algorithm that concentrates at the top of the list", "author": ["C. Rudin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rudin.,? \\Q2009\\E", "shortCiteRegEx": "Rudin.", "year": 2009}, {"title": "Multi-Label Classification: An Overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "International Journal of Data Warehousing and Mining,", "citeRegEx": "Tsoumakas and Katakis.,? \\Q2007\\E", "shortCiteRegEx": "Tsoumakas and Katakis.", "year": 2007}, {"title": "Learning to rank by optimizing NDCG measure", "author": ["Hamed Valizadegan", "Rong Jin", "Ruofei Zhang", "Jianchang Mao"], "venue": "In 26th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Valizadegan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Valizadegan et al\\.", "year": 2009}, {"title": "A support vector method for optimizing average precision", "author": ["Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "In 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Yue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2007}, {"title": "Ranking via robust binary classification", "author": ["Hyokun Yun", "Parameswaran Raman", "S Vishwanathan"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Yun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yun et al\\.", "year": 2014}, {"title": "Covering Number Bounds of Certain Regularized Linear Function Classes", "author": ["Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang.,? \\Q2002\\E", "shortCiteRegEx": "Zhang.", "year": 2002}], "referenceMentions": [{"referenceID": 21, "context": "Similarly, in multilabel classification problems, the goal is to rank the labels according to their likelihood of being relevant to a data point Tsoumakas and Katakis [2007]. The ranking of items at the top is of utmost importance in these applications and several performance measures, such as Precision@k, Average Precision and NDCG have been designed to promote accuracy at top of ranked lists.", "startOffset": 145, "endOffset": 174}, {"referenceID": 13, "context": "Informally, prec@k counts the number of relevant items in the top-k positions of a ranked list and is widely used in domains such as binary classification Joachims [2005], multi-label classification Prabhu and Varma [2014] and ranking Le and Smola [2007].", "startOffset": 155, "endOffset": 171}, {"referenceID": 13, "context": "Informally, prec@k counts the number of relevant items in the top-k positions of a ranked list and is widely used in domains such as binary classification Joachims [2005], multi-label classification Prabhu and Varma [2014] and ranking Le and Smola [2007].", "startOffset": 155, "endOffset": 223}, {"referenceID": 13, "context": "Informally, prec@k counts the number of relevant items in the top-k positions of a ranked list and is widely used in domains such as binary classification Joachims [2005], multi-label classification Prabhu and Varma [2014] and ranking Le and Smola [2007]. Given its popularity, prec@k has received attention from algorithmic, as well as learning theoretic perspectives.", "startOffset": 155, "endOffset": 255}, {"referenceID": 13, "context": "Informally, prec@k counts the number of relevant items in the top-k positions of a ranked list and is widely used in domains such as binary classification Joachims [2005], multi-label classification Prabhu and Varma [2014] and ranking Le and Smola [2007]. Given its popularity, prec@k has received attention from algorithmic, as well as learning theoretic perspectives. However, there remain specific deficiencies in our understanding of this performance measure. In fact, to the best of our knowledge, there is only one known convex surrogate function for prec@k, namely, the struct-SVM surrogate due to Joachims [2005] which, as we reveal in this work, is not an upper bound on prec@k in general, and need not recover an optimal ranking even in strictly separable settings.", "startOffset": 155, "endOffset": 621}, {"referenceID": 12, "context": "Since the intractability of binary classification in the agnostic setting Guruswami and Raghavendra [2009] extends to prec@k, our goal would be to exploit natural notions of benign-ness usually observed in natural distributions to overcome such intractability results.", "startOffset": 74, "endOffset": 107}, {"referenceID": 10, "context": "Our algorithms can be shown to be a natural extension of the classical perceptron algorithm for binary classification Rosenblatt [1958]. Indeed, akin to the classical perceptron, both our algorithms enjoy mistake bounds that reduce to crisp convergence bounds under the margin conditions mentioned earlier.", "startOffset": 118, "endOffset": 136}, {"referenceID": 6, "context": "While the earlier methods for this problem, such as RankSVM, focused on optimizing pair-wise ranking accuracy Herbrich et al. [2000], Joachims [2002], Freund et al.", "startOffset": 110, "endOffset": 133}, {"referenceID": 6, "context": "While the earlier methods for this problem, such as RankSVM, focused on optimizing pair-wise ranking accuracy Herbrich et al. [2000], Joachims [2002], Freund et al.", "startOffset": 110, "endOffset": 150}, {"referenceID": 5, "context": "[2000], Joachims [2002], Freund et al. [2003], Burges et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 1, "context": "[2003], Burges et al. [2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 1, "context": "[2003], Burges et al. [2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al.", "startOffset": 8, "endOffset": 263}, {"referenceID": 1, "context": "[2003], Burges et al. [2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al.", "startOffset": 8, "endOffset": 277}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al.", "startOffset": 256, "endOffset": 271}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a,b], Li et al.", "startOffset": 256, "endOffset": 291}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a,b], Li et al. [2014]. In this work, we focus on one such evaluation measure \u2013 Precision@k, which is widely used in practice.", "startOffset": 256, "endOffset": 343}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a,b], Li et al. [2014]. In this work, we focus on one such evaluation measure \u2013 Precision@k, which is widely used in practice. The only prior algorithms that we are aware of that directly optimize this performance measure are a structural SVM based cutting plane method due to Joachims [2005], and an efficient stochastic implementation of the same due to Kar et al.", "startOffset": 256, "endOffset": 613}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a,b], Li et al. [2014]. In this work, we focus on one such evaluation measure \u2013 Precision@k, which is widely used in practice. The only prior algorithms that we are aware of that directly optimize this performance measure are a structural SVM based cutting plane method due to Joachims [2005], and an efficient stochastic implementation of the same due to Kar et al. [2014]. However, as pointed out earlier, the convex surrogate used in these methods is not well-suited for prec@k.", "startOffset": 256, "endOffset": 694}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a,b], Li et al. [2014]. In this work, we focus on one such evaluation measure \u2013 Precision@k, which is widely used in practice. The only prior algorithms that we are aware of that directly optimize this performance measure are a structural SVM based cutting plane method due to Joachims [2005], and an efficient stochastic implementation of the same due to Kar et al. [2014]. However, as pointed out earlier, the convex surrogate used in these methods is not well-suited for prec@k. It is also important to note that the bipartite ranking setting considered in this work is different from other popular forms of ranking such as subset or list-wise ranking settings, which arise in several information retrieval applications, where again there has been much work in optimizing performance measures that emphasize on accuracy at the top (e.g. NDCG) Valizadegan et al. [2009], Cao et al.", "startOffset": 256, "endOffset": 1192}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a,b], Li et al. [2014]. In this work, we focus on one such evaluation measure \u2013 Precision@k, which is widely used in practice. The only prior algorithms that we are aware of that directly optimize this performance measure are a structural SVM based cutting plane method due to Joachims [2005], and an efficient stochastic implementation of the same due to Kar et al. [2014]. However, as pointed out earlier, the convex surrogate used in these methods is not well-suited for prec@k. It is also important to note that the bipartite ranking setting considered in this work is different from other popular forms of ranking such as subset or list-wise ranking settings, which arise in several information retrieval applications, where again there has been much work in optimizing performance measures that emphasize on accuracy at the top (e.g. NDCG) Valizadegan et al. [2009], Cao et al. [2007], Yue et al.", "startOffset": 256, "endOffset": 1211}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a,b], Li et al. [2014]. In this work, we focus on one such evaluation measure \u2013 Precision@k, which is widely used in practice. The only prior algorithms that we are aware of that directly optimize this performance measure are a structural SVM based cutting plane method due to Joachims [2005], and an efficient stochastic implementation of the same due to Kar et al. [2014]. However, as pointed out earlier, the convex surrogate used in these methods is not well-suited for prec@k. It is also important to note that the bipartite ranking setting considered in this work is different from other popular forms of ranking such as subset or list-wise ranking settings, which arise in several information retrieval applications, where again there has been much work in optimizing performance measures that emphasize on accuracy at the top (e.g. NDCG) Valizadegan et al. [2009], Cao et al. [2007], Yue et al. [2007], Le and Smola [2007], Chakrabarti et al.", "startOffset": 256, "endOffset": 1230}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a,b], Li et al. [2014]. In this work, we focus on one such evaluation measure \u2013 Precision@k, which is widely used in practice. The only prior algorithms that we are aware of that directly optimize this performance measure are a structural SVM based cutting plane method due to Joachims [2005], and an efficient stochastic implementation of the same due to Kar et al. [2014]. However, as pointed out earlier, the convex surrogate used in these methods is not well-suited for prec@k. It is also important to note that the bipartite ranking setting considered in this work is different from other popular forms of ranking such as subset or list-wise ranking settings, which arise in several information retrieval applications, where again there has been much work in optimizing performance measures that emphasize on accuracy at the top (e.g. NDCG) Valizadegan et al. [2009], Cao et al. [2007], Yue et al. [2007], Le and Smola [2007], Chakrabarti et al.", "startOffset": 256, "endOffset": 1251}, {"referenceID": 0, "context": "[2005], of late, there has been enormous interest in performance measures that promote good ranking performance at the top portion of the ranked list, and in ranking methods that directly optimize these measures Cl\u00e9men\u00e7on and Vayatis [2007], Rudin [2009], Agarwal [2011], Boyd et al. [2012], Narasimhan and Agarwal [2013a,b], Li et al. [2014]. In this work, we focus on one such evaluation measure \u2013 Precision@k, which is widely used in practice. The only prior algorithms that we are aware of that directly optimize this performance measure are a structural SVM based cutting plane method due to Joachims [2005], and an efficient stochastic implementation of the same due to Kar et al. [2014]. However, as pointed out earlier, the convex surrogate used in these methods is not well-suited for prec@k. It is also important to note that the bipartite ranking setting considered in this work is different from other popular forms of ranking such as subset or list-wise ranking settings, which arise in several information retrieval applications, where again there has been much work in optimizing performance measures that emphasize on accuracy at the top (e.g. NDCG) Valizadegan et al. [2009], Cao et al. [2007], Yue et al. [2007], Le and Smola [2007], Chakrabarti et al. [2008],", "startOffset": 256, "endOffset": 1278}, {"referenceID": 7, "context": "There has also been some recent work on perceptron style ranking methods for list-wise ranking problems Chaudhuri and Tewari [2014], but these methods are tailored to optimize the NDCG and MAP measures, which are different from the prec@k measure that we consider here.", "startOffset": 104, "endOffset": 132}, {"referenceID": 7, "context": "There has also been some recent work on perceptron style ranking methods for list-wise ranking problems Chaudhuri and Tewari [2014], but these methods are tailored to optimize the NDCG and MAP measures, which are different from the prec@k measure that we consider here. Other less related works include online ranking algorithms for optimizing ranking measures in an adversarial setting with limited feedback Chaudhuri and Tewari [2015].", "startOffset": 104, "endOffset": 437}, {"referenceID": 4, "context": "We note that the results of Calauz\u00e8nes et al. [2012] that negate the possibility of consistent convex surrogates for ranking performance measures do not apply to our results since they are neither stated for prec@k, nor do they negate the possibility of conditional consistency.", "startOffset": 28, "endOffset": 53}, {"referenceID": 4, "context": "We note that the results of Calauz\u00e8nes et al. [2012] that negate the possibility of consistent convex surrogates for ranking performance measures do not apply to our results since they are neither stated for prec@k, nor do they negate the possibility of conditional consistency. It is notable that the seminal work of Joachims [2005] did propose a convex surrogate for prec@k, that we refer to as `struct prec@k(\u00b7).", "startOffset": 28, "endOffset": 334}, {"referenceID": 14, "context": "1 The Curious Case of `struct prec@k(\u00b7) The `struct prec@k(\u00b7) surrogate is a part of a broad class of surrogates called struct-SVM surrogates that are designed for structured output prediction problems that can have exponentially large output spaces Joachims [2005]. Given a set of n labeled data points, `struct prec@k(\u00b7) is defined as", "startOffset": 250, "endOffset": 266}, {"referenceID": 10, "context": "Note that ` prec@k(\u00b7) is similar to the \u201cramp\u201d losses for binary classification Do et al. [2008]. We now show that ` prec@k(\u00b7) is indeed an upper bounding surrogate for prec@k.", "startOffset": 80, "endOffset": 97}, {"referenceID": 16, "context": "Mini-batch methods have recently gained popularity and have been used to optimize ranking loss functions such as `struct prec@k(\u00b7) as well Kar et al. [2014]. It is useful to note that the requirement for mini-batches goes away in ranking and multi-label classification settings, for our algorithms can be applied to individual data points in those settings (e.", "startOffset": 139, "endOffset": 157}, {"referenceID": 16, "context": "Mini-batch methods have recently gained popularity and have been used to optimize ranking loss functions such as `struct prec@k(\u00b7) as well Kar et al. [2014]. It is useful to note that the requirement for mini-batches goes away in ranking and multi-label classification settings, for our algorithms can be applied to individual data points in those settings (e.g. individual queries in ranking settings). At every time instant t, our algorithms receive a batch of b points Xt = [ xt , . . . ,x b t ] and rank these points using the existing model. Let \u2206t denote the prec@k loss (equation 1) at time t. If \u2206t = 0 i.e. all top k ranks are occupied by positive points, then the model is not updated. Otherwise, the model is updated using the false positives and negatives. For sake of simplicity, we will only look at linear models in this paper. Depending on the kind of updates we make, we get two variants of the perceptron rule for prec@k. Our first algorithm, PERCEPTRON@K-AVG, updates the model using a combination of all the false positives and negatives (see Algorithm 1). The effect of the update is a very natural one \u2013 it explicitly boosts the scores of the positive points that failed to reach the top ranks, and attenuates the scores of the negative points that got very high scores. It is interesting to note that in the limiting case of k = 1 and unit batch length (i.e. b = 1), the PERCEPTRON@K-AVG update reduces to that of the standard perceptron algorithm Rosenblatt [1958], Minsky and Papert [1988] for the choice \u0177t = sign(st).", "startOffset": 139, "endOffset": 1489}, {"referenceID": 16, "context": "Mini-batch methods have recently gained popularity and have been used to optimize ranking loss functions such as `struct prec@k(\u00b7) as well Kar et al. [2014]. It is useful to note that the requirement for mini-batches goes away in ranking and multi-label classification settings, for our algorithms can be applied to individual data points in those settings (e.g. individual queries in ranking settings). At every time instant t, our algorithms receive a batch of b points Xt = [ xt , . . . ,x b t ] and rank these points using the existing model. Let \u2206t denote the prec@k loss (equation 1) at time t. If \u2206t = 0 i.e. all top k ranks are occupied by positive points, then the model is not updated. Otherwise, the model is updated using the false positives and negatives. For sake of simplicity, we will only look at linear models in this paper. Depending on the kind of updates we make, we get two variants of the perceptron rule for prec@k. Our first algorithm, PERCEPTRON@K-AVG, updates the model using a combination of all the false positives and negatives (see Algorithm 1). The effect of the update is a very natural one \u2013 it explicitly boosts the scores of the positive points that failed to reach the top ranks, and attenuates the scores of the negative points that got very high scores. It is interesting to note that in the limiting case of k = 1 and unit batch length (i.e. b = 1), the PERCEPTRON@K-AVG update reduces to that of the standard perceptron algorithm Rosenblatt [1958], Minsky and Papert [1988] for the choice \u0177t = sign(st).", "startOffset": 139, "endOffset": 1515}, {"referenceID": 22, "context": "The next lemma establishes that, similar to the classical perceptron Novikoff [1962], PERCEPTRON@K-AVG also enjoys a mistake bound.", "startOffset": 69, "endOffset": 85}, {"referenceID": 21, "context": "Similar to the classical perceptron mistake bound Novikoff [1962], the above bound can also be reduced to a simpler convergence bound in separable settings.", "startOffset": 50, "endOffset": 66}, {"referenceID": 12, "context": "Hence for several datasets, PERCEPTRON@K-AVG might be able to find a perfect ranking while at the same time, it might be impossible for standard binary classification techniques to find any reasonable classifier in poly-time Guruswami and Raghavendra [2009]. We note that PERCEPTRON@K-AVG performs updates with all the false negatives in the mini-batches.", "startOffset": 225, "endOffset": 258}, {"referenceID": 20, "context": "[2014], Narasimhan et al. [2015] who propose to use mini-batch methods to overcome this problem Kar et al.", "startOffset": 8, "endOffset": 33}, {"referenceID": 16, "context": "[2015] who propose to use mini-batch methods to overcome this problem Kar et al. [2014]. By combining the ` prec@k(\u00b7) surrogate with mini-batch-style processing, we design SGD@K-AVG (Algorithm 3), a scalable SGD algorithm for optimizing prec@k.", "startOffset": 70, "endOffset": 88}, {"referenceID": 16, "context": "Recently, Kar et al. [2014] also established a similar result for the `struct prec@k(\u00b7) surrogate.", "startOffset": 10, "endOffset": 28}, {"referenceID": 14, "context": "Methods: We compared both perceptron algorithms, SGD@K-AVG, as well as an SGD solver for the `max prec@k(\u00b7) surrogate, with the cutting plane-based SVMPerf solver of Joachims [2005]. We also compare against stochastic", "startOffset": 166, "endOffset": 182}, {"referenceID": 16, "context": "1PMB solver of Kar et al. [2014]. The perceptron and SGD methods were given a maximum of 25 passes over the data with a batch length of 500.", "startOffset": 15, "endOffset": 33}, {"referenceID": 30, "context": "This will allow a standard L\u221e covering number argument Zhang [2002] to give us the required uniform convergence results.", "startOffset": 55, "endOffset": 68}, {"referenceID": 1, "context": "where the third step follows from Bernstein\u2019s inequality (which holds in situations with sampling without replacement as well Boucheron et al. [2004]) since |T(v) \u00b7 h(v)| \u2264 1 for all v and we have assumed b \u2265 1 \u03ba log 2 \u03b4 .", "startOffset": 126, "endOffset": 150}, {"referenceID": 16, "context": "The proof of this theorem closely follows that of Theorems 7 and 8 in Kar et al. [2014]. More specifically, Theorem 6 from Kar et al.", "startOffset": 70, "endOffset": 88}, {"referenceID": 16, "context": "The proof of this theorem closely follows that of Theorems 7 and 8 in Kar et al. [2014]. More specifically, Theorem 6 from Kar et al. [2014] ensures that any convex loss function demonstrating uniform convergence would ensure a result of the kind we are trying to prove.", "startOffset": 70, "endOffset": 141}], "year": 2015, "abstractText": "The problem of maximizing precision at the top of a ranked list, often dubbed Precision@k (prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure. The most notable of these is the lack of a convex upper bounding surrogate for prec@k. We also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key contributions in these directions. At the heart of our results is a family of truly upper bounding surrogates for prec@k. These surrogates are motivated in a principled manner and enjoy attractive properties such as consistency to prec@k under various natural margin/noise conditions. These surrogates are then used to design a class of novel perceptron algorithms for optimizing prec@k with provable mistake bounds. We also devise scalable stochastic gradient descent style methods for this problem with provable convergence bounds. Our proofs rely on novel uniform convergence bounds which require an in-depth analysis of the structural properties of prec@k and its surrogates. We conclude with experimental results comparing our algorithms with state-of-the-art cutting plane and stochastic gradient algorithms for maximizing prec@k.", "creator": "LaTeX with hyperref package"}}}