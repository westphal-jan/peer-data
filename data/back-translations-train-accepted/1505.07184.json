{"id": "1505.07184", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2015", "title": "Unsupervised Cross-Domain Word Representation Learning", "abstract": "Meaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. Given a pair of \\emph{source}-\\emph{target} domains, we propose an unsupervised method for learning domain-specific word representations that accurately capture the domain-specific aspects of word semantics. First, we select a subset of frequent words that occur in both domains as \\emph{pivots}. Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, pivots that appear in a document must accurately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-the-art domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset.", "histories": [["v1", "Wed, 27 May 2015 04:02:56 GMT  (280kb,D)", "http://arxiv.org/abs/1505.07184v1", "53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conferences on Natural Language Processing of the Asian Federation of Natural Language Processing"]], "COMMENTS": "53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conferences on Natural Language Processing of the Asian Federation of Natural Language Processing", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["danushka bollegala", "takanori maehara", "ken-ichi kawarabayashi"], "accepted": true, "id": "1505.07184"}, "pdf": {"name": "1505.07184.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Cross-Domain Word Representation Learning", "authors": ["Danushka Bollegala", "Takanori Maehara", "Ken-ichi Kawarabayashi"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that it is a matter of a way in which people who are able to understand the world and understand what they do. (...) It is not as if people are able to understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world. (...) It is as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, as if they do not understand the world, if they do not understand the world, as if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, if they do not understand the world, why they do not understand the world, why they do not understand the world, why they do not understand the world, why they do not understand why they do not understand why they do not understand, why they do not understand why they do not understand, why they do not understand why they do not understand, why they do not understand"}, {"heading": "2 Related Work", "text": "In fact, most of us are able to survive on our own and are unable to save the world."}, {"heading": "3 Cross-Domain Representation Learning", "text": "We propose a method for learning word representations that are sensitive to semantic variations of words in different domains. We call this cross-domain word representation learning and provide a definition in Section 3.1. Next, in Section 3.2, given a number of pivot points occurring in both a source and target domain, we propose a method for learning cross-domain word representation. We move the discussion of pivot selection methods to Section 3.4. In Section 3.5, we propose a method for using the learned word representations to form a cross-domain mood classifier."}, {"heading": "3.1 Problem Definition", "text": "Suppose we get two sets of documents DS and DT for each source (S) and target (T) domain. Then, when we get a specific word w, we define cross-domain presentation learning as the task of learning two separate representations wS and wT that capture the semantics w in each of the source and target T domains. Unlike domain matching, where there is a clear distinction between the source (i.e., the domain on which we train) and the target domain (i.e., the domain on which we test), we make no distinction between the two domains for presentation learning purposes. In the uncontrolled setting of cross-domain representation that we examine in this paper, we do not assume that the availability of selected data for any domain for the purpose of learning word representations ains."}, {"heading": "3.2 Proposed Method", "text": "In order to describe our proposed method, we engage in an approach that identifies us not as a goal, but as a goal, as a goal and as a goal. (D) We are not able to achieve the goal we have set ourselves. (D) We are not able to achieve the goal we have set ourselves. (D) We are not able to achieve the goal we have set ourselves. (D) The definition of a non-pivot occurs only in a single domain. (D) We use notorious convenience in both domains when the domain is clear. (D) We use CS, CT, and WT to designate the phrases of word representation. (D) We are unable to achieve the goal we have set ourselves. (D) We are unable to achieve the goal we have set ourselves. (D) We are unable to achieve the goal we have represented in the target domain. (D) The definition of a non-pivot occurs only in a single domain. (D) We use notorious convenience in both domains when the domain is clear. (D) We use CS, CT, and WT to designate the phrases of word representation. (D)"}, {"heading": "3.3 Training", "text": "In order to derive parameter updates from this, we calculate the gradients of the total loss function in G.4 w.r.t. However, we calculate the gradients of the total loss function in G.4 w.r.t. The following parameters are applied to each parameter: G.4 wS = {0, if cS > (wS \u2212 w).6 wS = {0, if cS > (wT \u2212 w).7 - cT otherwise (5).6 wS \".7\".7 \".7.7.7.7.7.7\".7.7 \".7\".7 \".7\".7 \".7\".7 \".7\".7 \".7\".7 \".7\".7 \".7\".7 \".7\".7 \".7\".7 \"7\".7 \".7\" 7 \".7\".7 \"7\".7 \".7\" 7 \".7\" 7 \".7\" 7 \".7\" 7 \".7\" 7. \"7.\" 7 \"7.\" 7 \"7.\" 7 \"7.\" 7. \"7.\" 7. \"7.\" 7. \"7.\" 7 \"7.\" 7. \"7.\" 7 \"7.\" 7 \"7.\" 7. \"7.\" 7. \"7.\" 7. \"7.\" 7. \"7.\" 7. \"7.\" 7. \"7.\" 7. \"7.\" 7. \"7.\" 9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9 \"9.9.9\" 9.9.9.9.9 \"9.9.9\" 9.9 \"9.9.9\" 9.9.9 \"9\" 9.9.9 \"9.9.9\" 9.9 \"9.9\" 9.9.9 \"9\" 9.9.9 \"9.9\" 9 \"9\" 9.9 \"9\" 9 \"9.9.9 9\" 9.9.9"}, {"heading": "3.4 Pivot Selection", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "3.5 Cross-Domain Sentiment Classification", "text": "As a concrete application of cross-domain word representation, a method for learning a cross-domain sentiment classifier called \"representations\" can be found in the proposed method. (There is only one domain that is evaluated for its accuracy in measuring the semantic similarity between words, while we can perform an indirect extrinsic evaluation.) The train data available for unattended cross-domain classification consists of unmarked data for both the source and target domains, as well as marked data for the source domain. We train a binary sentient using these train data and apply it to the classification of the target domain."}, {"heading": "4 Experiments and Results", "text": "There are 1000 positive and 1000 negative ratings for each domain. In addition, each domain has on average 17, 547 blank ratings. We use the standard method of 800 positive and 800 negative ratings from each domain, and the rest (200 + 200) for the testing we use to use the film (source) and the computer (target), which are also by Blitzer et al. (2007), but not part of the train / test domains. Experiments conducted with this validation dataset have shown that the performance of the proposed method is relatively insensitive to the value of regulation."}, {"heading": "5 Conclusion", "text": "We proposed an unattended method for learning cross-domain word representation, selecting a specific set of pivot points from a source domain and a target domain. In addition, we proposed a domain matching method that uses the learned word representations. Experimental results of a cross-domain mood classification task showed that the proposed method outperforms several competitive baselines and achieves the best accuracy of mood classification for all domain pairs. In the future, we plan to extend the proposed method to other types of domain matching tasks such as cross-domain language tagging, entity recognition and relationship extraction. Source code and pre-processed data, etc. are publicly available for this publication3.3www.csc.liv.ac.uk / \u02dc danushka / prj / darep"}], "references": [{"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Computational Linguistics, 36(4):673 \u2013 721.", "citeRegEx": "Baroni and Lenci.,? 2010", "shortCiteRegEx": "Baroni and Lenci.", "year": 2010}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proc. of ACL, pages 238\u2013247.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137 \u2013 1155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proc. of EMNLP, pages 120 \u2013 128.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira."], "venue": "Proc. of ACL, pages 440 \u2013 447.", "citeRegEx": "Blitzer et al\\.,? 2007", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Relation adaptation: Learning to extract novel relations with minimum supervision", "author": ["Danushka Bollegala", "Yutaka Matsuo", "Mitsuru Ishizuka."], "venue": "Proc. of IJCAI, pages 2205 \u2013 2210.", "citeRegEx": "Bollegala et al\\.,? 2011a", "shortCiteRegEx": "Bollegala et al\\.", "year": 2011}, {"title": "Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification", "author": ["Danushka Bollegala", "David Weir", "John Carroll."], "venue": "ACL/HLT, pages 132 \u2013 141.", "citeRegEx": "Bollegala et al\\.,? 2011b", "shortCiteRegEx": "Bollegala et al\\.", "year": 2011}, {"title": "Mining for analogous tuples from an entity-relation graph", "author": ["Danushka Bollegala", "Mitsuru Kusumoto", "Yuichi Yoshida", "Ken ichi Kawarabayashi."], "venue": "Proc. of IJCAI, pages 2064 \u2013 2070.", "citeRegEx": "Bollegala et al\\.,? 2013a", "shortCiteRegEx": "Bollegala et al\\.", "year": 2013}, {"title": "Minimally supervised novel relation extraction using latent relational mapping", "author": ["Danushka Bollegala", "Yutaka Matsuo", "Mitsuru Ishizuka."], "venue": "IEEE Transactions on Knowledge and Data Engineering, 25(2):419 \u2013 432.", "citeRegEx": "Bollegala et al\\.,? 2013b", "shortCiteRegEx": "Bollegala et al\\.", "year": 2013}, {"title": "Learning to predict distributions of words across domains", "author": ["Danushka Bollegala", "David Weir", "John Carroll."], "venue": "Proc. of ACL, pages 613 \u2013 623.", "citeRegEx": "Bollegala et al\\.,? 2014", "shortCiteRegEx": "Bollegala et al\\.", "year": 2014}, {"title": "Normalized (pointwsie) mutual information in collocation extraction", "author": ["Gerlof Bouma."], "venue": "Proc. of GSCL, pages 31 \u2013 40.", "citeRegEx": "Bouma.,? 2009", "shortCiteRegEx": "Bouma.", "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuska."], "venue": "Journal of Machine Learning Research, 12:2493 \u2013 2537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Using relational similarity between word pairs for latent relational search on the web", "author": ["Nguyen Tuan Duc", "Danushka Bollegala", "Mitsuru Ishizuka."], "venue": "In", "citeRegEx": "Duc et al\\.,? 2010", "shortCiteRegEx": "Duc et al\\.", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121 \u2013 2159, July.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A re-examination of query expansion using lexical resources", "author": ["Hui Fang."], "venue": "Proc. of ACL, pages 139\u2013 147.", "citeRegEx": "Fang.,? 2008", "shortCiteRegEx": "Fang.", "year": 2008}, {"title": "A new minimally-supervised framework for domain word sense disambiguation", "author": ["Stefano Faralli", "Roberto Navigli."], "venue": "EMNLP, pages 1411 \u2013 1422.", "citeRegEx": "Faralli and Navigli.,? 2012", "shortCiteRegEx": "Faralli and Navigli.", "year": 2012}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "Proc. of ICML.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Stephan Gouws", "Yoshua Bengio", "Greg Corrado."], "venue": "Proc. of ICML.", "citeRegEx": "Gouws et al\\.,? 2015", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proc. of ICLR.", "citeRegEx": "Hermann and Blunsom.,? 2014", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proc. of ACL, pages 873 \u2013 882.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Instance weighting for domain adaptation in nlp", "author": ["Jing Jiang", "ChengXiang Zhai."], "venue": "ACL 2007, pages 264 \u2013 271.", "citeRegEx": "Jiang and Zhai.,? 2007a", "shortCiteRegEx": "Jiang and Zhai.", "year": 2007}, {"title": "A two-stage approach to domain adaptation for statistical classifiers", "author": ["Jing Jiang", "ChengXiang Zhai."], "venue": "CIKM 2007, pages 401\u2013410.", "citeRegEx": "Jiang and Zhai.,? 2007b", "shortCiteRegEx": "Jiang and Zhai.", "year": 2007}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "Proc. of COLING, pages 1459 \u2013 1474.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Automatic domain adaptation for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proc. of NAACL/HLT, pages 28 \u2013 36.", "citeRegEx": "McClosky et al\\.,? 2010", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "Efficient estimation of word representation in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Jeffrey Dean."], "venue": "CoRR.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Proc. of NIPS, pages 3111 \u2013 3119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continous space word representations", "author": ["Tomas Mikolov", "Wen tau Yih", "Geoffrey Zweig."], "venue": "NAACL\u201913, pages 746 \u2013 751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proc. of ACLHLT, pages 236 \u2013 244.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E. Hinton."], "venue": "Proc. of NIPS, pages 1081\u20131088.", "citeRegEx": "Mnih and Hinton.,? 2008", "shortCiteRegEx": "Mnih and Hinton.", "year": 2008}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "Proc. of NIPS.", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "Proc. of EMNLP, pages 1059\u20131069.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Cross-domain sentiment classification via spectral feature alignment", "author": ["Sinno Jialin Pan", "Xiaochuan Ni", "Jian-Tao Sun", "Qiang Yang", "Zheng Chen."], "venue": "Proc. of WWW, pages 751 \u2013 760.", "citeRegEx": "Pan et al\\.,? 2010", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Glove: global vectors for word representation", "author": ["Jeffery Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J. Mooney."], "venue": "Proc. of HLT-NAACL, pages 109 \u2013 117.", "citeRegEx": "Reisinger and Mooney.,? 2010", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Composition of word representations improves semantic role labelling", "author": ["Michael Roth", "Kristian Woodsend."], "venue": "Proc. of EMNLP, pages 407\u2013413.", "citeRegEx": "Roth and Woodsend.,? 2014", "shortCiteRegEx": "Roth and Woodsend.", "year": 2014}, {"title": "Towards robust cross-domain domain adaptation for part-ofspeech tagging", "author": ["Tobias Schnabel", "Hinrich Sch\u00fctze."], "venue": "Proc. of IJCNLP, pages 198 \u2013 206.", "citeRegEx": "Schnabel and Sch\u00fctze.,? 2013", "shortCiteRegEx": "Schnabel and Sch\u00fctze.", "year": 2013}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Ng", "Chris Manning."], "venue": "ICML\u201911.", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proc. of EMNLP, pages 151\u2013161.", "citeRegEx": "Socher et al\\.,? 2011b", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "Journal of Aritificial Intelligence Research, 37:141 \u2013 188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "Proc. of EMNLP\u201913, pages 1393 \u2013 1398.", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "merous higher-level NLP applications (Collobert et al., 2011).", "startOffset": 37, "endOffset": 61}, {"referenceID": 24, "context": "Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words (Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 132, "endOffset": 180}, {"referenceID": 32, "context": "Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words (Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 132, "endOffset": 180}, {"referenceID": 9, "context": "because movies that do not invoke deep thoughts in viewers are considered to be lightweight (Bollegala et al., 2014).", "startOffset": 92, "endOffset": 116}, {"referenceID": 32, "context": "Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010).", "startOffset": 88, "endOffset": 160}, {"referenceID": 11, "context": "Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010).", "startOffset": 88, "endOffset": 160}, {"referenceID": 24, "context": "Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010).", "startOffset": 88, "endOffset": 160}, {"referenceID": 33, "context": ", 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010).", "startOffset": 104, "endOffset": 132}, {"referenceID": 19, "context": "Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the do-", "startOffset": 82, "endOffset": 128}, {"referenceID": 30, "context": "Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the do-", "startOffset": 82, "endOffset": 128}, {"referenceID": 6, "context": "If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch\u00fctze, 2013), crossdomain dependency parsing (McClosky et al.", "startOffset": 200, "endOffset": 225}, {"referenceID": 35, "context": ", 2011b), cross-domain POS tagging (Schnabel and Sch\u00fctze, 2013), crossdomain dependency parsing (McClosky et al.", "startOffset": 35, "endOffset": 63}, {"referenceID": 23, "context": ", 2011b), cross-domain POS tagging (Schnabel and Sch\u00fctze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al.", "startOffset": 96, "endOffset": 119}, {"referenceID": 7, "context": ", 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b).", "startOffset": 54, "endOffset": 175}, {"referenceID": 8, "context": ", 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b).", "startOffset": 54, "endOffset": 175}, {"referenceID": 5, "context": ", 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b).", "startOffset": 54, "endOffset": 175}, {"referenceID": 20, "context": ", 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b).", "startOffset": 54, "endOffset": 175}, {"referenceID": 21, "context": ", 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b).", "startOffset": 54, "endOffset": 175}, {"referenceID": 3, "context": "This problem setting is closely related to unsupervised domain adaptation (Blitzer et al., 2006), which has found numerous useful applications such as, sentiment classification and POS tagging.", "startOffset": 74, "endOffset": 96}, {"referenceID": 3, "context": "For example, in unsupervised cross-domain sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007), we train a binary sentiment classifier using positive and negative labeled user reviews in the source domain, and apply the trained clas-", "startOffset": 67, "endOffset": 111}, {"referenceID": 4, "context": "For example, in unsupervised cross-domain sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007), we train a binary sentiment classifier using positive and negative labeled user reviews in the source domain, and apply the trained clas-", "startOffset": 67, "endOffset": 111}, {"referenceID": 3, "context": "Following prior work on domain adaptation (Blitzer et al., 2006), high-frequent features (unigrams/bigrams) common to both domains are referred to as domain-independent features or pivots.", "startOffset": 42, "endOffset": 64}, {"referenceID": 3, "context": "Consequently, prior work on domain adaptation (Blitzer et al., 2006; Pan et al., 2010) learn lower-dimensional mappings from non-pivots to pivots, thereby overcoming the feature mismatch problem.", "startOffset": 46, "endOffset": 86}, {"referenceID": 31, "context": "Consequently, prior work on domain adaptation (Blitzer et al., 2006; Pan et al., 2010) learn lower-dimensional mappings from non-pivots to pivots, thereby overcoming the feature mismatch problem.", "startOffset": 46, "endOffset": 86}, {"referenceID": 26, "context": "tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 11, "context": ", 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al.", "startOffset": 22, "endOffset": 46}, {"referenceID": 36, "context": ", 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 39, "context": ", 2011a), machine translation (Zou et al., 2013), sentiment classifica-", "startOffset": 30, "endOffset": 48}, {"referenceID": 37, "context": "tion (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification.", "startOffset": 5, "endOffset": 27}, {"referenceID": 34, "context": ", 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification.", "startOffset": 37, "endOffset": 62}, {"referenceID": 32, "context": "form a state-of-the-art domain-insensitive word representation learning method (Pennington et al., 2014), and several competitive baselines.", "startOffset": 79, "endOffset": 104}, {"referenceID": 31, "context": "Despite this taskindependent nature of the proposed method, it achieves the best sentiment classification accuracies on all domain-pairs, reporting statistically comparable results to the current state-of-the-art unsupervised cross-domain sentiment classification methods (Pan et al., 2010; Blitzer et al., 2006).", "startOffset": 272, "endOffset": 312}, {"referenceID": 3, "context": "Despite this taskindependent nature of the proposed method, it achieves the best sentiment classification accuracies on all domain-pairs, reporting statistically comparable results to the current state-of-the-art unsupervised cross-domain sentiment classification methods (Pan et al., 2010; Blitzer et al., 2006).", "startOffset": 272, "endOffset": 312}, {"referenceID": 38, "context": "tasks (Turney and Pantel, 2010).", "startOffset": 6, "endOffset": 31}, {"referenceID": 26, "context": "Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c).", "startOffset": 128, "endOffset": 151}, {"referenceID": 1, "context": "be identified in prior work (Baroni et al., 2014): counting-based and prediction-based.", "startOffset": 28, "endOffset": 49}, {"referenceID": 38, "context": "Numerous methods for selecting co-occurrence contexts such as proximity or dependency relations have been proposed (Turney and Pantel, 2010).", "startOffset": 115, "endOffset": 140}, {"referenceID": 2, "context": "The neural network language model (NNLM) (Bengio et al., 2003) uses a multi-layer feed-forward neural network to predict the next word in a sequence, and uses backpropagation to update the word vectors such that the prediction error is minimized.", "startOffset": 41, "endOffset": 62}, {"referenceID": 24, "context": "2013b) predicts the words c that appear in the local context of a word w, whereas the continuous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w\u2019s local context (Mikolov et al., 2013a).", "startOffset": 205, "endOffset": 228}, {"referenceID": 19, "context": "ods that use global co-occurrences in the entire corpus to learn word representations have shown to outperform methods that use only local cooccurrences (Huang et al., 2012; Pennington et al., 2014).", "startOffset": 153, "endOffset": 198}, {"referenceID": 32, "context": "ods that use global co-occurrences in the entire corpus to learn word representations have shown to outperform methods that use only local cooccurrences (Huang et al., 2012; Pennington et al., 2014).", "startOffset": 153, "endOffset": 198}, {"referenceID": 1, "context": "Overall, prediction-based methods have shown to outperform counting-based methods (Baroni et al., 2014).", "startOffset": 82, "endOffset": 103}, {"referenceID": 5, "context": "ing the problem of word representation variation across domains is due to Bollegala et al. (2014). Given a source and a target domain, they first select a set of pivots using pointwise mutual information, and create two distributional representa-", "startOffset": 74, "endOffset": 98}, {"referenceID": 18, "context": "Although in this paper we focus on the monolingual setting where source and target domains belong to the same language, the related setting where learning representations for words that are translational pairs across languages has been studied (Hermann and Blunsom, 2014; Klementiev et al., 2012; Gouws et al., 2015).", "startOffset": 244, "endOffset": 316}, {"referenceID": 22, "context": "Although in this paper we focus on the monolingual setting where source and target domains belong to the same language, the related setting where learning representations for words that are translational pairs across languages has been studied (Hermann and Blunsom, 2014; Klementiev et al., 2012; Gouws et al., 2015).", "startOffset": 244, "endOffset": 316}, {"referenceID": 17, "context": "Although in this paper we focus on the monolingual setting where source and target domains belong to the same language, the related setting where learning representations for words that are translational pairs across languages has been studied (Hermann and Blunsom, 2014; Klementiev et al., 2012; Gouws et al., 2015).", "startOffset": 244, "endOffset": 316}, {"referenceID": 12, "context": "Such representations are particularly useful for cross-lingual information retrieval (Duc et al., 2010).", "startOffset": 85, "endOffset": 103}, {"referenceID": 6, "context": "In fact, prior work on cross-domain sentiment analysis show that incorporating multiple source domains improves sentiment classification accuracy on a target domain (Bollegala et al., 2011b; Glorot et al., 2011).", "startOffset": 165, "endOffset": 211}, {"referenceID": 16, "context": "In fact, prior work on cross-domain sentiment analysis show that incorporating multiple source domains improves sentiment classification accuracy on a target domain (Bollegala et al., 2011b; Glorot et al., 2011).", "startOffset": 165, "endOffset": 211}, {"referenceID": 24, "context": "Following prior work on representation learning (Mikolov et al., 2013a), in our experiments, we set the window size to 10 tokens, without crossing sentence boundaries.", "startOffset": 48, "endOffset": 71}, {"referenceID": 29, "context": "Because nonoccurring non-pivots w\u2217 are randomly sampled, prior work on noise contrastive estimation has found that it requires more negative samples than positive samples to accurately learn a prediction model (Mnih and Kavukcuoglu, 2013).", "startOffset": 210, "endOffset": 238}, {"referenceID": 24, "context": "We raise p(w) to the 3/4-th power as proposed by Mikolov et al. (2013a), and normalize it to unit probability mass prior to sampling k non-pivots w\u2217 per each co-occurrence of (c, w) \u2208 d.", "startOffset": 49, "endOffset": 72}, {"referenceID": 13, "context": "AdaGrad (Duchi et al., 2011) is used to schedule the learning rate.", "startOffset": 8, "endOffset": 28}, {"referenceID": 11, "context": "1) is inspired by the prior work on word representation learning for a single domain (Collobert et al., 2011).", "startOffset": 85, "endOffset": 109}, {"referenceID": 24, "context": "Similar to the skip-gram model (Mikolov et al., 2013a), the proposed method predicts occurrences of contexts (non-pivots) w within a fixed-size contextual window of a target word (pivot) c.", "startOffset": 31, "endOffset": 54}, {"referenceID": 11, "context": "1) is inspired by the prior work on word representation learning for a single domain (Collobert et al., 2011). However, unlike the multilayer neural network in Collobert et al. (2011), the proposed method uses a computationally efficient single layer to reduce the number of parameters that must be learnt, thereby scaling to large datasets.", "startOffset": 86, "endOffset": 184}, {"referenceID": 28, "context": "Scoring the co-occurrences of two words c and w by the bilinear form given by the inner-product is similar to prior work on domain-insensitive word-representation learning (Mnih and Hinton, 2008; Mikolov et al., 2013a).", "startOffset": 172, "endOffset": 218}, {"referenceID": 24, "context": "Scoring the co-occurrences of two words c and w by the bilinear form given by the inner-product is similar to prior work on domain-insensitive word-representation learning (Mnih and Hinton, 2008; Mikolov et al., 2013a).", "startOffset": 172, "endOffset": 218}, {"referenceID": 31, "context": "set of pivots from a given pair of domains such as the minimum frequency of occurrence of a feature in the two domains, mutual information (MI), and the entropy of the feature distribution over the documents (Pan et al., 2010).", "startOffset": 208, "endOffset": 226}, {"referenceID": 10, "context": "In our preliminary experiments, we discovered that a normalized version of the PMI (NPMI) (Bouma, 2009) to work consistently well for selecting pivots from different pairs of domains.", "startOffset": 90, "endOffset": 103}, {"referenceID": 14, "context": "query expansion used in information retrieval to improve document recall (Fang, 2008).", "startOffset": 73, "endOffset": 85}, {"referenceID": 3, "context": "Amazon product reviews collected by Blitzer et al. (2007) for the four product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K).", "startOffset": 36, "endOffset": 58}, {"referenceID": 3, "context": "For validation purposes we use movie (source) and computer (target) domains, which were also collected by Blitzer et al. (2007), but not part of the train/test domains.", "startOffset": 106, "endOffset": 128}, {"referenceID": 31, "context": "Spectral Feature Alignment (SFA) (Pan et al., 2010) and Structural Correspondence Learning (SCL) (Blitzer et al.", "startOffset": 33, "endOffset": 51}, {"referenceID": 4, "context": ", 2010) and Structural Correspondence Learning (SCL) (Blitzer et al., 2007) are the state-ofthe-art methods for cross-domain sentiment classification.", "startOffset": 53, "endOffset": 75}, {"referenceID": 5, "context": "proposed by Bollegala et al. (2014). Although CS can be used to learn a vector-space translation matrix, it does not learn word representations.", "startOffset": 12, "endOffset": 36}], "year": 2015, "abstractText": "Meaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. Given a pair of source-target domains, we propose an unsupervised method for learning domain-specific word representations that accurately capture the domainspecific aspects of word semantics. First, we select a subset of frequent words that occur in both domains as pivots. Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, pivots that appear in a document must accurately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-theart domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset.", "creator": "LaTeX with hyperref package"}}}