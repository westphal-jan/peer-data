{"id": "1505.07634", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2015", "title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged", "abstract": "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the SLN-robustness of the unhinged loss.", "histories": [["v1", "Thu, 28 May 2015 10:38:56 GMT  (419kb,D)", "http://arxiv.org/abs/1505.07634v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["brendan van rooyen", "aditya krishna menon", "robert c williamson"], "accepted": true, "id": "1505.07634"}, "pdf": {"name": "1505.07634.pdf", "metadata": {"source": "CRF", "title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged", "authors": ["Brendan van Rooyen", "Aditya Krishna Menon", "Robert C. Williamson"], "emails": ["}@nicta.com.au"], "sections": [{"heading": "1 Learning with symmetric label noise", "text": "Considering an instance space X, and samples from any distribution D above X \u00d7 {\u00b1 1}, the goal is to have a scorer s: X \u2192 R with low misclassification error on future samples from D. Our interest is in the more realistic scenario in which the learner observes samples from a distribution D, which is a corruption of D in which labels have a constant probability of being reversed. The goal is still, in terms of the unobserved distribution D. This is known as the problem of learning from symmetrical label noise (SLN learning) [Angluin and Laird, 1988].Long and Servedio [2010] proved the following negative result on what is possible in SLN learning. There is a linear separableD, where the learner observes some corruption D with symmetrical label noise thing."}, {"heading": "2 Background and problem setup", "text": "Attach an instance space X. We use D to denote a distribution over X \u00b7 {\u00b1 1}, with random variables (X, Y) \u0445 D. Each D can be expressed via the class-related distributions (P, Q) = (P (X | Y = 1), P (X | Y = \u2212 1) and the base rate \u03c0 = P (Y = 1), or equivalent via the boundary distribution M = P (X) and the class probability function \u03b7: x 7 \u2192 P (Y = 1 | X = x)."}, {"heading": "2.1 Classifiers, scorers, and risks", "text": "A goal-scorer is any function s: X \u2192 R. A loss is any function ': {\u00b1 1} \u00b7 R \u2192 R. We use' \u2212 1, '1 to refer to' (\u2212 1, \u00b7) and '(1, \u00b7). The' -conditioned risk L ': [0, 1] \u00b7 R \u2192 R is defined as L': (\u03b7, v) 7 \u2192 \u03b7 \u00b7 '1 (v) + (1 \u2212 \u03b7) \u00b7 \"\u2212 1 (v). In a distribution D, the' risk of a goal-scorer s is defined as LD '(s). = E (X, Y) \u0445 D [' (Y, s (X)))], (1) or equivalent LD '(s) = E X-M [L' (\u03b7 (X), s (X)))]. In a group S, LD 'Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z is the group of goal-scorers with group F-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z, the group of goal-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z is the group of goal-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z with group F-Z-Z-Z-Z-Z-Z-Z-Z)."}, {"heading": "2.2 Learning with symmetric label noise (SLN learning)", "text": "The problem of learning with symmetric marking noise (SLN learning) is the following [Angluin and Laird, 1988, Kearns, 1998, Blum and Mitchell, 1998, Natarajan et al., 2013]. For some theoretical \"clean\" distribution D, which we would like to observe, we instead observe samples from some corrupt distributions SLN (D, \u03c1), but for others [0, 1 / 2. The distribution SLN (D, \u03c1) is such that the boundary distribution of the instances is unchanged, but each label is bent independently of the probability. The goal is to learn a goal shooter from these corrupt samples, so that LD01 (s) is small. For each quantity in D, we label their corrupted counterparts in SLN (D, \u03c1) with a bar, e.g. M for the corrupted boundary distribution, and also, if clearly taken from the context D, corruption (D, \u03c1) and corruption (L marginal distribution)."}, {"heading": "3 SLN-robustness: formalisation", "text": "For our purposes, a learner (\", F) includes a loss\" and a functional class F, where learning is the search for some s-F that minimizes the \"risk. Informal, the learner (\", F) is \"robust\" to symmetrical label noise (SLN-robust) if minimizing \"over F\" gives the same classifier for both the clean distribution D that the learner would like to observe and for every B [0, 1 / 2) that the learner actually observes. We are now formalizing this notion and reviewing what is known about the existence of SLN-resilient learners."}, {"heading": "3.1 SLN-robust learners: a formal definition", "text": "Considering a theoretical \"clean\" distribution D, Nsln: \u0445 \u2192 2 \u2206 returns the set of possible corrupt versions of D that the learner may be able to observe, with labels being inverted with unknown probability. Definition 1 (SLN robustness): Nsln: D 7 \u2192 {SLN (D, \u03c1) | \u03c1 [0, 12)} Equipped with this, we define our notion of SLN robustness. We say that a learner (\", F) SLN robustness is when (VP, Nsln (D, D) (VP, Nsln (D))) LD01 (S D, F) = L D 01 (S D, F). (3) That is, SLN robustness requires that the learner's classification performance (wt D) when we consider the learner directly as an SLN learner is not the same learner."}, {"heading": "3.2 Convex potentials with linear function classes are not SLN-robust", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "4 SLN-robustness: a noise-corrected loss perspective", "text": "The definition of SLN robustness (Eq.3) includes optimal scorers with the same loss \"over two different distributions. We are now expressing this again to think about optimal scorers on the same distribution, but with two different losses. This will help to characterize the number of losses that are SLN-robust."}, {"heading": "4.1 Reformulating SLN-robustness via noise-corrected losses", "text": "Natarajan et al. [2013, Lemma 1] have shown how to associate with a loss \"a noise-corrected counterpart,\" so that for each D, LD \"(s) = LD\" (s). The loss \"is defined as the following. Definition 2 (noise-corrected loss). In the face of a loss\" and a noise-corrected loss \"[0, 1 / 2), the noise-corrected loss\" depending on the unknown parameter \u03c1 \"is not directly usable to design an SLN-robust learner\" (y, v) = (1 \u2212 \u03c1) \u00b7 \"(y, v) \u00b7\" (Bayy, v) 1 \u2212 2\u03c1. \"(4) Since\" it depends on the unknown parameter \u03c1, it is not directly usable to design an SLN-robust learner. Nevertheless, it is a useful theoretical construct, since the risk equivalence between Ls \"() and LF\" means that the minimization of risk (D) above D is D (D)."}, {"heading": "4.2 Characterising a stronger notion of SLN-robustness", "text": "Manwani and Sastry [2013, Theorem 1] proved to be sufficient condition for \"such that equation holds 5 pairs, namely (\" C-R \") (\" V-R \") (\" V-R \")\" 1 (V) + \"\u2212 1 (V) = C. (6) For such a loss,\" a scaled and translated version of \"is trivial, so that SD, F-R\" = \"S-D, F-R.\" Ideally, one would like to characterize if equation holds 5. While this is an open question, we can interestingly show that under a stronger requirement for losses \"and\" the condition in equation 6 is also necessary. \"The stronger requirement is that the corresponding risk order orders all stochastic scorers identically. A stochastic ranking is simply a figure f: X-R, where\" R is the amount of distributions over values. \""}, {"heading": "5 The unhinged loss: a convex, classification-calibrated, strongly SLNrobust loss", "text": "Consider the following simple but non-standard convex loss: \"unh1 (v) = 1 \u2212 v and\" unh \u2212 1 (v) = 1 + v.A special property of loss is that it is negatively unlimited, a problem we discuss in \u00a7 5.3. Compared to the hinge loss, the loss is not stuck at zero, i.e. it has no hinge. Therefore, we call this the blurred loss2. The loss has a number of attractive properties, the most immediate of which is its SLN robustness."}, {"heading": "5.1 The unhinged loss is strongly SLN-robust", "text": "Since 'unh1 (v) +' unh \u2212 1 (v) = 0, we conclude from Proposition 3 that 'unh is strong SLN-robust, and that (' unh, F) is therefore robust for each choice of F. Furthermore, the following property of uniqueness is not difficult to show. Proposition 4. Select a convex loss. 'Then (' C'R) '1 (v) +' \u2212 1 (v) = C-robust. Coming back to the case of linear scorers, the above statement implies that ('unh, Flin) is SLN-robust. This does not contradict Proposition 1, since' unh is not a convex loss that is strong SLN-robust.Coming back to the case of linear scorers, this means that ('unh, Flin) is SLN-robust."}, {"heading": "5.2 The unhinged loss is classification calibrated", "text": "For example, a loss equal to zero is highly SLN-robust, but useless, since it is not classification calibrated. Fortunately, the loss that has fallen out of joint is classification calibrated, as we now find. For reasons that will be discussed below, we consider minimizing the risk to FB = [\u2212 B, + B] X, the group of scorers with a bandwidth limited by B-value. Proposal 5. Fix '= \"unh.\" Then, the limited Bayes-optimal scorer relative to FB has the same sign as the Bays-optimal scorer relative to the Bays-optimal scorer for 0-1. In the limited case where F = RX is the optimal scorer, which we cannot calibrate if we exceed the value."}, {"heading": "5.3 Enforcing boundedness of the loss", "text": "In orthodox decision theory, analogous theoretical problems arise when trying to establish basic theorems with unlimited losses [Ferguson, 1967, p. 78], [Schervish, 1995, p. 172]. We can get around this problem by limiting attention to limited scorers, so that \"unh is effectively limited.\" Proposal 5 does not affect the classification calibration of loss. In the context of linear scorers, the bounce of scorers can be achieved by regulation: instead of working with flin, you can instead use flin, which flin is considered unrobust (flin 7 = < w, x > | p\u03bb | | 2 \u2264 1 / local time) Less in the fluin the fluin (fluin the fluin the S4 = S4)."}, {"heading": "5.4 Unhinged loss minimisation on corrupted distribution is consistent", "text": "This shows the classification consistency of blurred loss minimization on the corrupt distribution.Proposal 6. Fix \"=\" unh. Then for all D, \u03c1 [0, 1 / 2), B [1, \u221e), and goal-chasing FB, regretD01 (s) \u2264 regretD01, FB \"(s) = 11 \u2212 2\u03c1 \u00b7 regretD, FB\" (s).Standard convergence rates over generalization limits are also trivial; see Appendix D. We now turn to the question of how to minimize blurred loss when using a kernel-based goal scorer."}, {"heading": "6 Learning with the unhinged loss and kernels", "text": "We now show that the optimal solution for the unhinged loss when using regularization and kernel scorers is a simple one, which sheds further light on SLN robustness and regularization."}, {"heading": "6.1 The centroid classifier optimises the unhinged loss", "text": "Consider minimizing the risk of blur over a ball in a reproducing Hilbert room H with kernel k, i.e., consider the functional class of kernel-based scorers FH, \u03bb = {s: x 7 \u2192 < w, \u03a6 (x) > H | | w | | H \u2264 1 / \u221a \u03bb} for some \u03bb > 0, where \u03a6: X \u2192 H is some characteristic sketch. Likewise, we can apply 3 D plugin estimates as appropriate for a distribution. First-order optimality condition implies that the gate X (X, Y) \u0445 D [1 \u2212 Y \u00b7 < w, \u03a6 (X) >] + \u03bb 2 < w > H. (7) 3Given a training sample X \u2022 Dn, we can apply plugin estimates as appropriate."}, {"heading": "6.2 Practical considerations", "text": "First, cross validation is not necessary to select \u03bb, since s \u0445 unh, \u03bb depends trivially on the regularization constant: the change of \u03bb only changes the size of the scorers, not their sign. Regularization thus simply controls the scale of predicted scorers, and for the purposes of classification you can simply \u03bb = 1. Second, we can slightly expand the scorers to use a bias regulated with strength 0 < \u03bbb 6 = \u03bb. Tuning is equivalent to calculating s unh according to Equation 9, and tuning a threshold to a specified target value. Third, if H = Rd for d is small, we can store w unh, \u03bb explicitly and use it to make predictions. For high (or infinite) dimensions H we can make predictions directly about equation 9."}, {"heading": "6.3 Equivalence to a highly regularised SVM and other convex potentials", "text": "There is an interesting equivalence between the unhinged solution and the highly regulated SVM solution."}, {"heading": "6.4 Equivalence to Fisher Linear Discriminant with whitened data", "text": "Remember that the Fisher Linear Discriminant (FLD) for the binary classification on DM, \u03b7, finds a weight vector proportional to the minimizer of the square loss \"sq\": (y, v) 7 \u2192 (1 \u2212 yv) 2 [Bishop, 2006, Section 4.1.5], w \u0445 sq, \u03bb = (EX \u0445 M [XXT] + \u03bbI) \u2212 1 \u00b7 E (X, Y) \u0445 D [Y \u00b7 X]. (11) By equation 10 and the fact that the corrupted limit measure M = M, we see that w \u0445 sq, \u03bb is changed only by one scaling factor under marking noise. This provides an alternative proof for the fact that (\"sq, Flin) is SLN-robust 5 [Manwani and Sastry, 2013, theorem 2]. Clearly, the blurred loss solution w \u0445 unh, \u03bb is equivalent to the WilliD and square loss solution, when universally F is transferred to the input data selective F = selective."}, {"heading": "7 SLN-robustness of unhinged loss: empirical illustration", "text": "We now clarify that the SLN robustness of the unequal loss is empirically manifest = 0.125 \u00b1 0.51 points of the noise level. We repeat that the unequal solution with high regularization corresponds to an SVM solution (and within the limit of any classified loss). Thus, the experiments are not aimed at asserting that the unequal loss is \"better\" than other losses, but rather at demonstrating that its SLN robustness is not purely theoretical. We first show that the unequal risk communicator works well using the example of Long and Servedio [2010]. Figure 1 shows the distribution D = (1, 0), (5), the unleashing is not purely theoretical."}, {"heading": "8 Conclusion and future work", "text": "We have proposed a convex, classification-calibrated loss, proven that it is robust to symmetric label noise (SLN-robust), demonstrated that it is the unique loss that fulfils an idea of strong SLN robustness, noted that it is optimized by the nearest centric classifier, and demonstrated that the nature of the optimal solution implies that most convex potentials, such as the SVM, are SLN-robust even at high regulation. Future work will include investigating losses that are robust to asymmetric noise, and outliers in the instance space."}, {"heading": "Acknowledgments", "text": "NICTA is funded by the Australian Government through the Ministry of Communications and the Australian Research Council through the ICT Centre of Excellence Program. The authors thank Cheng Soon Ong for valuable comments on a draft of this paper. Evidence for \"Learning with Symmetric Label Noise: The Importance of Being Unhinged\""}, {"heading": "A Proofs of results in main body", "text": "This result is implicitly given in Long and Servedio (2010, \u2212 x 2); the objective of this proof is simple, to make the result explicitly.Let X = (1, 0), (3), (3), (3), (3), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5, 5, 5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5, (5), (5), (5), (5, (5, (5), (5), (5, (5), (5, (5), (5), (5, (5), (5, (5), (5, (5), (5, (5), (5), (5, (5), (5, (5, (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5, (5, (5), (5"}, {"heading": "B Evidence that non-convex losses and linear scorers may not be SLNrobust", "text": "We present evidence that the TangentBoost losses \"(y, v) = (2 tan \u2212 1 (yv) \u2212 1) 2, or the t-logistic regression loss for t = 2,\" (y, v) = log (1 \u2212 yv + \u221a 1 + v2), (', Flin) is not, however, SLN robust. We do this by looking at the minimizers of these losses using the example of Long and Servedio [2010]. Of course, since these losses are not convex, the exact minimization of risk is a challenge. However, since the search space is R2, we construct a grid of resolution 0.025 above [\u2212 10] 2. We then exhaust the target for all grid points and seek the minimize misery. We apply this method to the Long and Servedio [2010] dataset with a noise rate of 30%."}, {"heading": "C Preservation of mean maps", "text": "Select any D and then select [0, 1 / 2]. Then (1, 1 / 2) 2\u03b7 (x) \u2212 1 = 2 \u00b7 (1 \u2212 2\u03c1) \u2212 1 = (1 \u2212 2\u03c1) \u00b7 (2\u03b7 (x) \u2212 1). Therefore, for each characteristic sketch the following applies: X \u2192 H, the middle map of the cleanliness distribution is E (X, Y) \u0445 D [Y \u00b7 \u03a6 (X)] = E X \u0445 M [(2\u03b7 (X) \u2212 1) \u00b7 \u03a6 (X)] = 1 (1 \u2212 2\u03c1 \u00b7 E X \u0445 M [(2\u03b7 (X) \u2212 1) \u00b7 \u03a6 (X) \u00b7 E (X \u2212 2\u043c) \u00b7 E (X, Y) \u2012 SLN (D) [Y \u00b7 \u0445 (X)], which is a scaled version of the mean value map of the noise distribution core. That is, the mean value map of the core is kept under symmetrical noise label."}, {"heading": "D Additional theoretical considerations", "text": "D.1 Generalization limits Generalization limits are easy to derive for the unregulated loss. For a training sample S \u0445 Dn, define the \"deviation of a goalie\": X \u2192 R to be the difference in his population and his empirical \"risk,\" devD, S \"(s) = L D\" (s) \u2212 LS \"(s). This quantity is of interest because a standard result states that for the empirical risk mixer sn has some functional class F, regretD, F\" (sn) \u2264 2 \u00b7 supports F \"dev D, S\" (s) | Boucheron et al., 2005, Eq.2]. For the unregulated loss we have the following radiator basis. Proposal 10. Select each D and n \"N +. Leave S, Dn, assign an empirical sample.\""}, {"heading": "E Additional relations to existing methods", "text": "We discuss some other links between the disordered loss and the existing methods as classified in 6.1. \"We can intuitively explain the disordered loss by examining the disordered courses of the disordered loss.\" \"We cannot consider the disordered distribution losses as such if we use these slightly disordered losses for learning purposes.\" \"Of course, we do not know how the noise ratio increases.\" \"The slightly disordered losses are not convex.\" \"We can exploit the loss completely unhindered.\" \"We can exploit the loss completely unhindered.\" (v) = 1 \u2212 and \"unh \u2212 1 (v) = 1 (v) Relative to the centrorobust distribution rates.\""}, {"heading": "F Example of poor classification with square loss", "text": "We illustrate that square losses with a linear function class can perform poorly even if the underlying distribution is linearly separable. We look at the data set of Long and Servedio [2010] without identification noise. That is, we have X = {(1, 0), (\u03b3, 5\u03b3), (\u03b3, \u2212 \u03b3), (\u03b3, \u2212 \u03b3), (1), (R2, and: x 7 \u2212 1. Let X \u2212 R4 \u00b7 2 be the characteristic matrix of the four data points. Then, the optimal weight vector learned by square loss is (XTX) \u2212 1XT 1 1 = [8\u03b3 + 3 \u2212 3 \u2212 3 \u2212 QU. \u2212 QU. \u2212 QU. \u2212 QU. \u2212 QU. \u2212 This refers to the convergence rate of the convergence of the prediction to true convergence. \u2212 QU."}, {"heading": "G Example of poor classification with unhinged loss", "text": "We illustrate that the uneven loss with a linear functional class can perform poorly, even if the underlying distribution is linearly separable. (For another example in which instances are on the unit sphere, see Balcan et al. [2008, Figure 1].) Let us consider a distribution DM, \u03b7 equally concentrated on X = {x1, x2, x3} with x1 = (1, 2), x2 = (1, \u2212 4), x3 = (x1, 1), with \u03b7 (x2, x3) = 0, i.e. the first two instances are positive, and the third instance negative. Then it is obvious that the optimal uneven hyperplane with regulatory strength 1, [x1, \u2212 1] is an optimal unit that we misclassify: the first instance as negative."}, {"heading": "H Additional experimental results", "text": "Table 4 indicates the 0-1 error for a series of losses on the Long and Servedio [2010] dataset. TanBoost refers to the loss of Masnadi-Shirazi et al. [2010]. As before, we find the unhinged loss to generally find a good classifier. Note that the relatively poor performance of the square and TanBoost loss are due to the results of Appendix B, F. Next, we report the 0-1 error and a minus of the AUC for a number of datasets. We start with a data set from Mease and Wyner [2008] where X = [0, 1] 20 and M is the uniform distribution. Furthermore, we have: x 7 \u2192 J < w *, x > 2.5 K for w \u0445 = [15 015], i.e. there is a sparse data set from Mease and Wyner [2008] where X = [0, 1] and M is equal distribution."}], "references": [{"title": "Learning from noisy examples", "author": ["Dana Angluin", "Philip Laird"], "venue": "Machine Learning,", "citeRegEx": "Angluin and Laird.,? \\Q1988\\E", "shortCiteRegEx": "Angluin and Laird.", "year": 1988}, {"title": "A theory of learning with similarity functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Nathan Srebro"], "venue": "Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L. Bartlett", "Michael I. Jordan", "Jon D. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "An efficient alternative to SVM based recursive feature elimination with applications in natural language processing and bioinformatics", "author": ["Justin Bedo", "Conrad Sanderson", "Adam Kowalczyk"], "venue": "AI 2006: Advances in Artificial Intelligence,", "citeRegEx": "Bedo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bedo et al\\.", "year": 2006}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["Avrim Blum", "Tom Mitchell"], "venue": "In Conference on Computational Learning Theory (COLT),", "citeRegEx": "Blum and Mitchell.,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "Theory of classification: a survey of some recent advances", "author": ["St\u00e9phane Boucheron", "Olivier Bousquet", "G\u00e1bor Lugosi"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "Boucheron et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2005}, {"title": "Super-samples from kernel herding", "author": ["Yutian Chen", "Max Welling", "Alex J. Smola"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Optimal Statistical Decisions", "author": ["Morris H. DeGroot"], "venue": null, "citeRegEx": "DeGroot.,? \\Q1970\\E", "shortCiteRegEx": "DeGroot.", "year": 1970}, {"title": "Robust classification with adiabatic quantum optimization", "author": ["Vasil Denchev", "Nan Ding", "Hartmut Neven", "S.V.N. Vishwanathan"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Denchev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denchev et al\\.", "year": 2012}, {"title": "A Probabilistic Theory of Pattern Recognition", "author": ["Luc Devroye", "L\u00e1szl\u00f3 Gy\u00f6rfi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "t-logistic regression. In Advances in Neural Information Processing Systems (NIPS), pages 514\u2013522", "author": ["Nan Ding", "S.V.N. Vishwanathan"], "venue": "Curran Associates, Inc.,", "citeRegEx": "Ding and Vishwanathan.,? \\Q2010\\E", "shortCiteRegEx": "Ding and Vishwanathan.", "year": 2010}, {"title": "Color retrieval in vector space model", "author": ["A. Doloc-Mihu", "V.V. Raghavan", "P. Bollmann-Sdorra"], "venue": "In ACM SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval,", "citeRegEx": "Doloc.Mihu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Doloc.Mihu et al\\.", "year": 2003}, {"title": "Mathematical Statistics: A Decision Theoretic Approach", "author": ["Thomas S. Ferguson"], "venue": null, "citeRegEx": "Ferguson.,? \\Q1967\\E", "shortCiteRegEx": "Ferguson.", "year": 1967}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael Kearns"], "venue": "Journal of the ACM,", "citeRegEx": "Kearns.,? \\Q1998\\E", "shortCiteRegEx": "Kearns.", "year": 1998}, {"title": "Estimating class membership probabilities using classifier learners", "author": ["John Langford", "Bianca Zadrozny"], "venue": null, "citeRegEx": "Langford and Zadrozny.,? \\Q2005\\E", "shortCiteRegEx": "Langford and Zadrozny.", "year": 2005}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["Philip M. Long", "Rocco A. Servedio"], "venue": "Machine Learning,", "citeRegEx": "Long and Servedio.,? \\Q2010\\E", "shortCiteRegEx": "Long and Servedio.", "year": 2010}, {"title": "Introduction to Information Retrieval", "author": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Noise tolerance under risk minimization", "author": ["Naresh Manwani", "P.S. Sastry"], "venue": "IEEE Transactions on Cybernetics,", "citeRegEx": "Manwani and Sastry.,? \\Q2013\\E", "shortCiteRegEx": "Manwani and Sastry.", "year": 2013}, {"title": "On the design of robust classifiers for computer vision", "author": ["Hamed Masnadi-Shirazi", "Vijay Mahadevan", "Nuno Vasconcelos"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Masnadi.Shirazi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Masnadi.Shirazi et al\\.", "year": 2010}, {"title": "Evidence contrary to the statistical view of boosting", "author": ["David Mease", "Abraham Wyner"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mease and Wyner.,? \\Q2008\\E", "shortCiteRegEx": "Mease and Wyner.", "year": 2008}, {"title": "Learning with noisy labels", "author": ["Nagarajan Natarajan", "Inderjit S. Dhillon", "Pradeep D. Ravikumar", "Ambuj Tewari"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Natarajan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Natarajan et al\\.", "year": 2013}, {"title": "On outlier rejection phenomena in bayes inference", "author": ["Anthony O\u2019Hagan"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), 41(3):pp", "citeRegEx": "O.Hagan.,? \\Q1979\\E", "shortCiteRegEx": "O.Hagan.", "year": 1979}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Composite binary losses", "author": ["Mark D. Reid", "Robert C. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid and Williamson.,? \\Q2010\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2010}, {"title": "Information, divergence and risk for binary experiments", "author": ["Mark D Reid", "Robert C Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid and Williamson.,? \\Q2011\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2011}, {"title": "Learning with kernels, volume 129", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Kernel Methods for Pattern Analysis, volume 47", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": "ISBN 0521813972", "citeRegEx": "Shawe.Taylor and Cristianini.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini.", "year": 2004}, {"title": "Kernel choice and classifiability for RKHS embeddings of probability distributions", "author": ["Bharath K. Sriperumbudur", "Kenji Fukumizu", "Arthur Gretton", "Gert R.G. Lanckriet", "Bernhard Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2009}, {"title": "Learning SVMs from sloppily labeled data", "author": ["Guillaume Stempfel", "Liva Ralaivola"], "venue": "In Artificial Neural Networks (ICANN),", "citeRegEx": "Stempfel and Ralaivola.,? \\Q2009\\E", "shortCiteRegEx": "Stempfel and Ralaivola.", "year": 2009}, {"title": "Diagnosis of multiple cancer types by shrunken centroids of gene expression", "author": ["Robert Tibshirani", "Trevor Hastie", "Balasubramanian Narasimhan", "Gilbert Chu"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Tibshirani et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2002}, {"title": "Characterizing the representer theorem", "author": ["Yaoliang Yu", "Hao Cheng", "Dale Schuurmans", "Csaba Szepesv\u00e1ri"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Yu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing.", "startOffset": 9, "endOffset": 34}, {"referenceID": 18, "context": "However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classificationcalibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded.", "startOffset": 9, "endOffset": 424}, {"referenceID": 0, "context": "This is known as the problem of learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988].", "startOffset": 83, "endOffset": 108}, {"referenceID": 0, "context": "This is known as the problem of learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988]. Long and Servedio [2010] proved the following negative result on what is possible in SLN learning: there exists a linearly separableD where, when the learner observes some corruptionD with symmetric label noise of any nonzero rate, minimisation of any convex potential over a linear function class results in classification performance on D that is equivalent to random guessing.", "startOffset": 84, "endOffset": 135}, {"referenceID": 0, "context": "This is known as the problem of learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988]. Long and Servedio [2010] proved the following negative result on what is possible in SLN learning: there exists a linearly separableD where, when the learner observes some corruptionD with symmetric label noise of any nonzero rate, minimisation of any convex potential over a linear function class results in classification performance on D that is equivalent to random guessing. Ostensibly, this establishes that convex losses are not \u201cSLN-robust\u201d and motivates the use of non-convex losses [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010, Denchev et al., 2012, Manwani and Sastry, 2013]. In this paper, we propose a convex loss and prove that it is SLN-robust. The loss avoids the result of Long and Servedio [2010] by virtue of being negatively unbounded.", "startOffset": 84, "endOffset": 869}, {"referenceID": 20, "context": "Manwani and Sastry [2013] demonstrated that square loss, `(y, v) = (1\u2212 yv), is one such loss.", "startOffset": 0, "endOffset": 26}, {"referenceID": 29, "context": "2This loss has been considered in Sriperumbudur et al. [2009], Reid and Williamson [2011] in the context of maximum mean discrepancy; see Appendix E.", "startOffset": 34, "endOffset": 62}, {"referenceID": 26, "context": "[2009], Reid and Williamson [2011] in the context of maximum mean discrepancy; see Appendix E.", "startOffset": 8, "endOffset": 35}, {"referenceID": 33, "context": "181] [Tibshirani et al., 2002] [Shawe-Taylor and Cristianini, 2004, Section 5.", "startOffset": 5, "endOffset": 30}, {"referenceID": 25, "context": "To alleviate this, for a translation-invariant kernel one can use random Fourier features [Rahimi and Recht, 2007] to find an approximate embedding of H into some low-dimensional R, and then store w\u2217 unh,\u03bb as usual.", "startOffset": 90, "endOffset": 114}, {"referenceID": 8, "context": "Alternately, one can post hoc search for a sparse approximation to w \u2217 unh,\u03bb, for example using kernel herding [Chen et al., 2012].", "startOffset": 111, "endOffset": 130}, {"referenceID": 18, "context": "We first show that the unhinged risk minimiser performs well on the example of Long and Servedio [2010]. Figure 1 shows the distribution D, where X = {(1, 0), (\u03b3, 5\u03b3), (\u03b3,\u2212\u03b3)} \u2282 R, with marginal distribution M = { 14 , 1 4 , 1 2} and all three instances are deterministically positive.", "startOffset": 79, "endOffset": 104}, {"referenceID": 18, "context": "Figure 1: Long and Servedio [2010] dataset.", "startOffset": 10, "endOffset": 35}, {"referenceID": 18, "context": "Table 1: Mean and standard deviation of the 0-1 error over 125 trials on Long and Servedio [2010]. Grayed cells denote the best performer at that noise rate.", "startOffset": 73, "endOffset": 98}, {"referenceID": 18, "context": "5Square loss escapes the result of Long and Servedio [2010] since it is not monotone decreasing.", "startOffset": 35, "endOffset": 60}, {"referenceID": 12, "context": "We compare the hinge, t-logistic (for t = 2) [Ding and Vishwanathan, 2010] and unhinged minimisers.", "startOffset": 45, "endOffset": 74}, {"referenceID": 4, "context": "With an unregularised bias term, Bedo et al. [2006] showed that the limiting solution of a soft-margin SVM is distribution dependent.", "startOffset": 33, "endOffset": 52}, {"referenceID": 18, "context": "We do this by looking at the minimisers of these losses on the 2D example of Long and Servedio [2010]. Of course, as these losses are non-convex, exact minimisation of the risk is challenging.", "startOffset": 77, "endOffset": 102}, {"referenceID": 18, "context": "We do this by looking at the minimisers of these losses on the 2D example of Long and Servedio [2010]. Of course, as these losses are non-convex, exact minimisation of the risk is challenging. However, as the search space is R, we construct a grid of resolution 0.025 over [\u221210, 10]. We then exhaustively compute the objective for all grid points, and seek the minimiser. We apply this procedure to the Long and Servedio [2010] dataset with \u03b3 = 1 60 , and with a 30% noise rate.", "startOffset": 77, "endOffset": 428}, {"referenceID": 18, "context": "Figure 2: Risk values for various weight vectors w = (w1, w2), TangentBoost, Long and Servedio [2010] dataset.", "startOffset": 77, "endOffset": 102}, {"referenceID": 12, "context": "For example, Ding and Vishwanathan [2010] defines", "startOffset": 13, "endOffset": 42}, {"referenceID": 18, "context": "Figure 3: Risk values for various weight vectors w = (w1, w2), t-logistic regression, Long and Servedio [2010] dataset.", "startOffset": 86, "endOffset": 111}, {"referenceID": 24, "context": "robustness to be a stability of the asymptotic maximum likelihood solution when adding a new labelled instance (chosen arbitrarily from X \u00d7 {\u00b11}), based on a definition in O\u2019Hagan [1979]. Intuitively, this captures robustness to outliers in the instance space, so that e.", "startOffset": 172, "endOffset": 187}, {"referenceID": 26, "context": "Recall that a loss ` is strictly proper composite [Reid and Williamson, 2010] if its (unique) Bayes-optimal scorer is some strictly monotone transformation \u03c8 of the class-probability function: (\u2200D) S ` = {\u03c8 \u25e6 \u03b7}.", "startOffset": 50, "endOffset": 77}, {"referenceID": 18, "context": "We further believe it likely that one can exhibit a scenario, possibly the same as the Long and Servedio [2010] example, where the resulting solution has accuracy 50%.", "startOffset": 87, "endOffset": 112}, {"referenceID": 33, "context": "181], and the nearest centroid classifier in computational genomics [Tibshirani et al., 2002].", "startOffset": 68, "endOffset": 93}, {"referenceID": 13, "context": "The optimal kernelised scorer for these approaches is [Doloc-Mihu et al., 2003]", "startOffset": 54, "endOffset": 79}, {"referenceID": 17, "context": "An alternative is to use the Probing reduction [Langford and Zadrozny, 2005], by computing an ensemble of cost-sensitive classifiers at varying cost ratios.", "startOffset": 47, "endOffset": 76}, {"referenceID": 15, "context": "When \u03c0 = 1 2 , ||w \u2217 1 ||H is precisely the maximum mean discrepancy (MMD) [Gretton et al., 2012] between P and Q, using all functions in the unit ball of H.", "startOffset": 75, "endOffset": 97}, {"referenceID": 18, "context": "We consider the dataset of Long and Servedio [2010], with no label noise.", "startOffset": 27, "endOffset": 52}, {"referenceID": 34, "context": "While this approach is suitable for this particular example, issues arise when dealing with infinite dimensional feature mappings (as we lose the existence of a representer theorem without regularisation based on the norm in the Hilbert space [Yu et al., 2013]).", "startOffset": 243, "endOffset": 260}, {"referenceID": 18, "context": "Table 4 reports the 0-1 error for a range of losses on the Long and Servedio [2010] dataset.", "startOffset": 59, "endOffset": 84}, {"referenceID": 18, "context": "Table 4 reports the 0-1 error for a range of losses on the Long and Servedio [2010] dataset. TanBoost refers to the loss of Masnadi-Shirazi et al. [2010]. As before, we find the unhinged loss to generally find a good classifier.", "startOffset": 59, "endOffset": 154}, {"referenceID": 18, "context": "Table 4 reports the 0-1 error for a range of losses on the Long and Servedio [2010] dataset. TanBoost refers to the loss of Masnadi-Shirazi et al. [2010]. As before, we find the unhinged loss to generally find a good classifier. Observe that the relatively poor performance of the square and TanBoost loss can be attributed to the findings of Appendix B, F. We next report the 0-1 error and one minus the AUC for a range of datasets. We begin with a dataset of Mease and Wyner [2008], where X = [0, 1], and M is the uniform distribution.", "startOffset": 59, "endOffset": 484}, {"referenceID": 18, "context": "Table 4: Results on Long and Servedio [2010] dataset.", "startOffset": 20, "endOffset": 45}], "year": 2015, "abstractText": "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classificationcalibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong `2 regularisation makes most standard learners SLN-robust. Experiments confirm the SLN-robustness of the unhinged loss. 1 Learning with symmetric label noise Binary classification is the canonical supervised learning problem. Given an instance space X, and samples from some distribution D over X \u00d7 {\u00b11}, the goal is to learn a scorer s : X \u2192 R with low misclassification error on future samples drawn from D. Our interest is in the more realistic scenario where the learner observes samples from a distribution D, which is a corruption of D where labels have some constant probability of being flipped. The goal is still to perform well with respect to the unobserved distribution D. This is known as the problem of learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988]. Long and Servedio [2010] proved the following negative result on what is possible in SLN learning: there exists a linearly separableD where, when the learner observes some corruptionD with symmetric label noise of any nonzero rate, minimisation of any convex potential over a linear function class results in classification performance on D that is equivalent to random guessing. Ostensibly, this establishes that convex losses are not \u201cSLN-robust\u201d and motivates the use of non-convex losses [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010, Denchev et al., 2012, Manwani and Sastry, 2013]. In this paper, we propose a convex loss and prove that it is SLN-robust. The loss avoids the result of Long and Servedio [2010] by virtue of being negatively unbounded. The loss is a modification of the hinge loss where one does not clamp at zero; thus, we call it the unhinged loss. We show that this is the unique convex loss (up to scaling and translation) that satisfies a notion of \u201cstrong SLN-robustness \u201d (Proposition 4). In addition to being SLN-robust, this loss has several attractive properties, such as being classification-calibrated (Proposition 5), consistent when minimised on the corrupted distribution (Proposition 6), and having an easily computable optimal solution that is the difference of two kernel means (Equation 9). Finally, we show that this optimal solution is equivalent to that of a strongly regularised SVM (Proposition 7), and such a result holds more generally for any twice-differentiable convex potential (Proposition 8), implying that strong `2 regularisation endows most standard learners with SLN-robustness. 1 ar X iv :1 50 5. 07 63 4v 1 [ cs .L G ] 2 8 M ay 2 01 5 The classifier resulting from minimising the unhinged loss is not new [Devroye et al., 1996, Chapter 10], [Sch\u00f6lkopf and Smola, 2002, Section 1.2], [Shawe-Taylor and Cristianini, 2004, Section 5.1]. However, establishing this classifier\u2019s SLN-robustness, its equivalence to a highly regularised SVM solution, and showing the underlying loss uniquely satisfies a notion of strong SLN-robustness, to our knowledge is novel. 2 Background and problem setup Fix an instance space X. We denote by D some distribution over X\u00d7{\u00b11}, with random variables (X,Y) \u223c D. Any D may be expressed via the class-conditional distributions (P,Q) = (P(X | Y = 1),P(X | Y = \u22121)) and base rate \u03c0 = P(Y = 1), or equivalently via the marginal distribution M = P(X) and classprobability function \u03b7 : x 7\u2192 P(Y = 1 | X = x). We interchangeably write D as DP,Q,\u03c0 or DM,\u03b7. 2.1 Classifiers, scorers, and risks A scorer is any function s : X \u2192 R. A loss is any function ` : {\u00b11} \u00d7 R \u2192 R. We use `\u22121, `1 to refer to `(\u22121, \u00b7) and `(1, \u00b7). The `-conditional risk L` : [0, 1] \u00d7 R \u2192 R is defined as L` : (\u03b7, v) 7\u2192 \u03b7 \u00b7 `1(v) + (1 \u2212 \u03b7) \u00b7 `\u22121(v). Given a distribution D, the `-risk of a scorer s is defined as L` (s) . = E (X,Y)\u223cD [`(Y, s(X))] , (1) or equivalently L` (s) = E X\u223cM [L`(\u03b7(X), s(X))]. For a set S, L` (S) is the set of `-risks for all scorers in S. A function class is any F \u2286 R. Given some F, the set of restricted Bayes-optimal scorers for a loss ` are those scorers in F that minimise the `-risk: S D,F,\u2217 ` . = Argmin s\u2208F L` (s). The set of (unrestricted) Bayes-optimal scorers is S ` = S D,F,\u2217 ` for F = R. The restricted `-regret of a scorer is its excess risk over that of any restricted Bayes-optimal scorer: regret ` (s) . = L` (s)\u2212 inf t\u2208F L` (t). Binary classification is concerned with the risk corresponding to the zero-one loss, ` : (y, v) 7\u2192 Jyv < 0K + 12Jv = 0K. A loss ` is classification-calibrated if all its Bayes-optimal scorers are also optimal for zeroone loss: (\u2200D) S ` \u2286 S D,\u2217 01 .A convex potential is any loss ` : (y, v) 7\u2192 \u03c6(yv), where \u03c6 : R\u2192 R+ is convex, non-increasing, differentiable with \u03c6\u2032(0) < 0, and \u03c6(+\u221e) = 0 [Long and Servedio, 2010, Definition 1]. All convex potential losses are classification-calibrated [Bartlett et al., 2006, Theorem 2.1]. 2.2 Learning with symmetric label noise (SLN learning) The problem of learning with symmetric label noise (SLN learning) is the following [Angluin and Laird, 1988, Kearns, 1998, Blum and Mitchell, 1998, Natarajan et al., 2013]. For some notional \u201cclean\u201d distribution D, which we would like to observe, we instead observe samples from some corrupted distribution SLN(D, \u03c1), for some \u03c1 \u2208 [0, 1/2). The distribution SLN(D, \u03c1) is such that the marginal distribution of instances is unchanged, but each label is independently flipped with probability \u03c1. The goal is to learn a scorer from these corrupted samples such that L01(s) is small. For any quantity in D, we denote its corrupted counterparts in SLN(D, \u03c1) with a bar, e.g. M for the corrupted marginal distribution, and \u03b7 for the corrupted class-probability function; additionally, when \u03c1 is clear from context, we will occasionally refer to SLN(D, \u03c1) by D. By definition of the corruption process, the corruption marginal distribution M = M , and [Natarajan et al., 2013, Lemma 7] (\u2200x \u2208 X) \u03b7(x) = (1\u2212 2\u03c1) \u00b7 \u03b7(x) + \u03c1. (2)", "creator": "LaTeX with hyperref package"}}}