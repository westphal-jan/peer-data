{"id": "1505.07909", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2015", "title": "Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding", "abstract": "Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Verbal comprehension questions appear very frequently in IQ tests, which measure human's verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by artificial intelligence technologies, especially the deep learning technologies that are recently developed and successfully applied in a number of fields. However, we found that the task was quite challenging, and simply applying existing technologies (e.g., word embedding) could not achieve a good performance, mainly due to the multiple senses of words and the complex relations among words. To tackle this challenge, we propose a novel framework consisting of three components. First, we build a classifier to recognize the specific type of a verbal question (e.g., analogy, classification, synonym, or antonym). Second, we obtain distributed representations of words and relations by leveraging a novel word embedding method that considers the multi-sense nature of words and the relational knowledge among words (or their senses) contained in dictionaries. Third, for each specific type of questions, we propose a simple yet effective solver based on the obtained distributed word representations and relation representations. According to our experimental results, our proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of human beings. The results are highly encouraging, indicating that with appropriate uses of the deep learning technologies, we could be a further step closer to the true human intelligence.", "histories": [["v1", "Fri, 29 May 2015 02:46:44 GMT  (138kb,D)", "http://arxiv.org/abs/1505.07909v1", null], ["v2", "Wed, 17 Jun 2015 13:29:41 GMT  (138kb,D)", "http://arxiv.org/abs/1505.07909v2", null], ["v3", "Mon, 6 Jul 2015 08:42:22 GMT  (102kb,D)", "http://arxiv.org/abs/1505.07909v3", null], ["v4", "Tue, 26 Apr 2016 11:37:32 GMT  (121kb)", "http://arxiv.org/abs/1505.07909v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["huazheng wang", "fei tian", "bin gao", "jiang bian", "tie-yan liu"], "accepted": true, "id": "1505.07909"}, "pdf": {"name": "1505.07909.pdf", "metadata": {"source": "CRF", "title": "Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding", "authors": ["Huazheng Wang", "Bin Gao", "Jiang Bian", "Fei Tian", "Tie-Yan Liu"], "emails": ["wanghzh@mail.ustc.edu.cn", "bingao@microsoft.com", "jibian@microsoft.com", "tianfei@mail.ustc.edu.cn", "tyliu@microsoft.com"], "sections": [{"heading": null, "text": "Permission to make digital or printed copies of all or part of this work for personal use or teaching is granted at no charge, provided that copies are not made or distributed for profit or commercial purposes, and that copies bear this notice and the full quote on the first page. Otherwise, reproduction, republication, server publication, or redistribution to lists requires prior express permission and / or fee. Copyright 20XX ACM X-XXXXX-XX-X / XX / XX... $15.00.Categories and subject descriptions H.3.5 [Information Systems]: Information Storage and Retrieval - Online Information ServicesKeywords Machine Learning, Deep Learning, Text Embedding, IQ Test."}, {"heading": "1. INTRODUCTION", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "2. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 IQ Test", "text": "Usually, such tests contain dozens of questions that a person has to solve within a limited period of time, and then an IQ score is calculated based on the correctness of the answers and several other factors, such as human age, with the results of each standard deviation defined upwards or downwards as 15 IQ points. By this definition, about 95% of the population is rated with an IQ between 70 and 130, which is within two standard deviations from meaning. Common IQ tests mainly contain three categories of questions [8]: verbal comprehensive questions, mathematical questions, and logical questions that include several types such as analogy, classification, synonym, and antonym."}, {"heading": "2.2 Deep Learning for Text Mining", "text": "In fact, it is so that it is able to hide in the position."}, {"heading": "3. VERBAL QUESTIONS IN IQ TEST", "text": "In standard IQ tests, a large proportion of questions are verbal comprehension questions, which play an important role in determining final IQ scores. In Wechsler's Adult Intelligence Scale [33], one of the most famous IQ test systems, the full IQ score is calculated from two IQ scores: verbal IQ and Performance IQ, and about 38% of questions in a typical test are verbal comprehension questions. In another popular system called WoodcockJohnson Tests of Cognitive Aabilities [35], the final IQ score is derived from three tests, including the Verbal Comprehension Test. Verbal questions can test not only verbal ability (e.g. understanding the polysemy of a word), but also a person's thinking ability and induction ability. According to previous studies [8], verbal questions consist mainly of the following types: analogy, classification, synonym, and antagonism."}, {"heading": "3.1 Analogy-I", "text": "Analogy I questions usually take the form \"A is to B as C is to?.\" One must select a word D from a given list of candidate words to form an analogous relationship between pair (A, B) and pair (C, D). Such questions test the ability to identify an implicit relationship of word pair (A, B) and apply it to word pair (C, D). Here is an example. Example 1. Isotherm is temperature like isotherm to atmosphere, (ii) wind, (iii) pressure, (iv) width, (v) current. The correct answer is pressure, because an isotherm is an isogram that connects points with the same temperature, while an isobarr is an isogram connecting points with the same pressure. Note that the Analogy I questions are also used as an important evaluation task in the word2vec models [21]."}, {"heading": "3.2 Analogy-II", "text": "Analogy II questions require identifying two words from two given lists to form an analogous relationship, such as \"A is to? how C is to?.\" Here is an example. Example 2. Identify two words (one from each set of brackets) that form a connection (analogy) when paired with the words in capital letters: CHAPTER (book, verse, read), ACT (stage, audience, game). The correct answer is book, play, because a book consists of several chapters and a play consists of several acts. Such questions are slightly more difficult than the Analogy I questions, since the analogous relationship cannot be observed directly from the questions, but must be sought in the word pairs from the candidates \"answers."}, {"heading": "3.3 Classification", "text": "Classification questions require identifying the word that differs from (or is dissimilar to) others in a given word list. Such questions are also known as odd-one-outs, which were examined in [24]. Classification questions test the ability to summarize the majority sense of words and identify the outlier. Here's a typical example. Example 3. What is the strange out? (i) quiet, (ii) calm, (iii) relaxed, (iv) serene, (v) unrounded. The correct answer is quiet, which means the absence of a sound, while the other words all have similar meanings to calm down."}, {"heading": "3.4 Synonym", "text": "Synonym questions require you to select a word from a list of words in such a way that it has the closest meaning to a particular word. Synonym questions test the ability to identify all the senses of the candidate words and select the right sense that can form a synonymous relationship with the given word. Here is a typical example. Example 4. Which word is closest to IRRATIONAL? (i) intransigent, (ii) unsalvageable, (iii) uncertain, (iv) lost, (v) nonsensible.The correct answer is nonsensible.The word has several senses, including (i) without reason, which is used in the context of psychology, (ii) unreasonable, which is also used in the context of psychology, and (iii) real number, which cannot be expressed as a quotient of two integers, which can be used in the context of mathematics."}, {"heading": "3.5 Antonym", "text": "Antonym questions require that one chooses a word from a list of words in such a way that it has the opposite meaning to a given word. Antonym questions test the ability to identify all the senses of the candidate words and select the right sense that can form an antonymous relationship to the given word. Here is a typical example. Example 5. Which word is the most counterproductive to MUSIC? (i) discordant, (ii) loud, (iii) lyrical, (iv) verbal, (v) euphonic. The correct answer is discordant. Musically, here, it means enjoying harmonic melody, while discordant means a lack of harmony. It is clear from the above explanations that for different types of questions, different kinds of relationships should be considered and the solvers should have different forms."}, {"heading": "4. SOLVING VERBAL QUESTIONS", "text": "In this section, we present our proposal for solving the verbal questions, which consists of the following three components:"}, {"heading": "4.1 Classification of Question Types", "text": "The first component of the framework is a question classifier that identifies different types of verbal questions. As different types of questions usually have their own expressions, the classification task is relatively simple, and we therefore take a simple approach to accomplish the task. Specifically, we treat each verbal question as a short document and use the TF \u00b7 IDF [1] function to build its representation. We then train an SVM [12] linear core classifier on a portion of the labeled question data and apply it to other questions. Question names include Analogy-I, Analogy-II, Classification, Synonym, and Antonym. We use the One-Against-Rest Training Strategy [4] to obtain a linear SVM classifier for each question type."}, {"heading": "4.2 Embedding of Word-Senses and Relations", "text": "The second component of our framework uses deep learning technologies to learn distributed representations of words (i.e. word embedding). Note that in the context of verbally answering questions, there are some specific demands placed on this learning process. Verbal questions in IQ tests usually take into account the multiple senses of a word (and focus on the rare senses) and the complex relationships between (ambiguous) words. Figure 1 shows an example of the multisense of words and the relationships between the sense of words. One can see that irrationality has three senses. Its first sense has an antonym relationship with the second sense of rationality, while its second sense has a synonym relationship with insensitive and an antonym relationship with the first sense of rationality. The above challenge has exceeded the ability of standard word embedding technologies. To address this problem, we present a novel approach that takes into account the multisense nature of words and the relational process of learning (or their meaning)."}, {"heading": "4.3 Solver for Each Type of Questions", "text": "In this sub-section, we define solutions to all types of verbal questions by using the embedded vectors for word pairs and relationships we have learned above. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5. EXPERIMENTS", "text": "In this section, we conduct experiments to investigate whether our proposed framework can achieve satisfactory results on verbal comprehension issues."}, {"heading": "5.1 Data Collection", "text": "5.1.1 Training Set for Word Embedding In our experiments, we trained Word Embedings on a pub-licensed text corpus called wiki2014 3, which is a large text snapshot from Wikipedia. After pre-processed2For our baseline embedding modes skip-gram, since it does not explicitly accept the relation representations, we use the first solver for this. 3http: / / en.wikipedia.org / wiki / Wikipedia: Database _ downloadby remove all the html meta-data and replace the digit numbers by English words, the final training corpus contains total more than 3.4 billion word tokens, and the number of unique words, i.e. the vocabulary size, is about 2 million.5.1.2 IQ Test Set According to our study, there is no online dataset specifi-based for verbal understanding questions, although there are many online IQ tests that users can play with."}, {"heading": "5.2 Compared Methods", "text": "In the following experiments we compare our new relationship model with several bases."}, {"heading": "5.3 Experimental Results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we explored how to automatically solve verbal comprehension issues in the intelligence quotient (IQ) test using AI technologies, in particular the deep learning techniques that have recently been developed and successfully applied in text mining and natural language processing. In order to accomplish the challenging task, especially in terms of the multiple senses of words and the complex relationships between words, we proposed a new framework consisting of three components: (i) the first component is a classifier that aims to identify the specific type of verbal comprehension question; (ii) the second component uses a novel deep learning technique to jointly learn the representations of both word-sense pairs and relationships between words (or their senses); (iii) the last component consists of dedicated solvers, based on the word-sense-pair representations and relationship representations received, in order to address each of the specific types of comprehension questions that may have better outcomes than previous experiments."}, {"heading": "7. REFERENCES", "text": "[1] A. Aizawa. An information-theoretical perspective oftf-idf measures. Information Processing & Management, 39 (1): 45-65, 2003. [2] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. Journal of Machine Learning Research, 3: 1137-1155, 2003. [3] J. Bian, B. Gao, and T.-Y. Liu. Knowledge-powered deep learning for word embedding. In Proceedings of ECML / PKDD, 2014. [4] C. M. Bishop et al. Pattern recognition and machine learning, volume 1. springer New York, 2006. [5] D. M. Blei, A. Y. Ng, and M. Jordan. Latent dirichlet allocation. the Journal of Machine Learning research, 3: 993-1022, 2003."}], "references": [{"title": "An information-theoretic perspective of tf\u2013idf measures", "author": ["A. Aizawa"], "venue": "Information Processing & Management, 39(1):45\u201365,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research, 3:1137\u20131155,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Knowledge-powered deep learning for word embedding", "author": ["J. Bian", "B. Gao", "T.-Y. Liu"], "venue": "Proceedings of ECML/PKDD,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern recognition and machine learning, volume 1. springer", "author": ["C.M. Bishop"], "venue": "New York,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems, pages 2787\u20132795,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "The complete book of intelligence tests", "author": ["P. Carter"], "venue": "John Wiley & Sons Ltd,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "The Ultimate IQ Test Book: 1,000 Practice Test Questions to Boost Your Brain Power", "author": ["P. Carter"], "venue": "Kogan Page Publishers,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of ICML, pages 160\u2013167. ACM,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Hern\u00c3 \u21aa  andez-Orallo", "author": ["J.D.L. Dowe"], "venue": "Iq tests are not for machines, yet. Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Using latent semantic analysis to improve access to textual information", "author": ["S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "S. Deerwester", "R. Harshman"], "venue": "Proceedings of SIGCHI, pages 281\u2013285. ACM,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1988}, {"title": "Wordrep: A benchmark for research on learning word representations", "author": ["B. Gao", "J. Bian", "T.-Y. Liu"], "venue": "arXiv preprint arXiv:1407.1640,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to solve arithmetic word problems with verb categorization", "author": ["M.J. Hosseini", "H. Hajishirzi", "O. Etzioni", "N. Kushman"], "venue": "http://www.freebase.com/", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of ACL, pages 873\u2013882,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "The Times Book of IQ Tests", "author": ["P.C. Ken Russell"], "venue": "Kogan Page Limited,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning to automatically solve algebra word problems", "author": ["N. Kushmany", "Y. Artziz", "L. Zettlemoyerz", "R. Barzilayy"], "venue": "Proceedings of the conference of the Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["M.-T. Luong", "R. Socher", "C.D. Manning"], "venue": "CoNLL-2013, 104,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Computing word-pair antonymy", "author": ["S. Mohammad", "B. Dorr", "G. Hirst"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 982\u2013991. Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "The Original Cambridge Self Scoring IQ Test", "author": ["D. Pape"], "venue": "The Magni Group, Inc,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1993}, {"title": "Automated word puzzle generation via topic dictionaries", "author": ["B. Pint\u00e9r", "G. V\u00f6r\u00f6s", "Z. Szab\u00f3", "A. L\u00f6rincz"], "venue": "CoRR, abs/1206.0377,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["J. Reisinger", "R.J. Mooney"], "venue": "Proc. of HLT,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "A computer program capable of passing i.q. tests", "author": ["P. Sanghi", "D. Dowe"], "venue": "In Proceedings of the Joint International Conference on Cognitive Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Diagram understanding in geometry questions", "author": ["M.J. Seo", "H. Hajishirzi", "A. Farhadi", "O. Etzioni"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "NIPS, pages 926\u2013934,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "The Psychological Methods of Testing Intelligence", "author": ["W. Stern"], "venue": "Warwick & York,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1914}, {"title": "An anthropomorphic method for number sequence problems", "author": ["C. Strannegard", "M. Amirghasemi", "S. Ulfsbacker"], "venue": "Cognitive Systems Research,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["F. Tian", "H. Dai", "J. Bian", "B. Gao", "R. Zhang", "E. Chen", "T.-Y. Liu"], "venue": "Proceedings of the 25th International Conference on Computational Linguistics,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind, 49:433\u2013460,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1950}, {"title": "Wechsler adult intelligence scale\u2013fourth edition (wais\u2013iv)", "author": ["D. Wechsler"], "venue": "San Antonio, TX: NCS Pearson,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["J. Weston", "A. Bordes", "O. Yakhnenko", "N. Usunier"], "venue": "arXiv preprint arXiv:1307.7973,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Woodcock-Johnson III tests of cognitive abilities", "author": ["R.W. Woodcock", "K.S. McGrew", "N. Mather"], "venue": "Riverside Pub.,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["M. Yu", "M. Dredze"], "venue": "Association for Computational Linguistics (ACL),", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "The most famous test is the Intelligence Quotient (IQ) test, which was first proposed about 100 years ago [29].", "startOffset": 106, "endOffset": 110}, {"referenceID": 31, "context": "Artificial Intelligence (AI) is the human-like intelligence exhibited by machines or software, which was first activated by the famous Turing test [32] and has attracted a lot of people to study afterwards.", "startOffset": 147, "endOffset": 151}, {"referenceID": 7, "context": "are verbal questions [8].", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "According to previous studies [8], verbal questions usually include sub-types like analogy, classification, synonym, and antonym.", "startOffset": 30, "endOffset": 33}, {"referenceID": 28, "context": "Intelligence Quotient Test was proposed by William Stern as a scoring method for human intelligence about a century ago [29].", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "Common IQ tests mainly contain three categories of questions [8]: verbal comprehensive questions, mathematical questions, and logic questions.", "startOffset": 61, "endOffset": 64}, {"referenceID": 25, "context": "Sanghi and Dowe [26] presented a fairly elementary WWW-based computer program that can solve a variety of IQ tests, regularly obtains a score close to the purported average human score of 100.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "[30] proposed an anthropomorphic method for number sequence solvers that targets performance on the level of human role models, and they also presented a method [30] for solving progressive matrix problems in logic questions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] proposed an anthropomorphic method for number sequence solvers that targets performance on the level of human role models, and they also presented a method [30] for solving progressive matrix problems in logic questions.", "startOffset": 161, "endOffset": 165}, {"referenceID": 18, "context": "[19] proposed an approach for automatically learning to solve algebra word problems, Seo et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] proposed a method to automatically solve geometry problems, and Hosseini et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] proposed an approach to learning to solve simple arithmetic word problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Besides the above efforts, there are also some debates on whether IQ tests are appropriate for evaluating the intelligence of machines or measuring the progress in AI [13], because some IQ test questions might be easily hacked and then correctly answered by some simple tricks.", "startOffset": 167, "endOffset": 171}, {"referenceID": 1, "context": "Building distributed word representations [2], a.", "startOffset": 42, "endOffset": 45}, {"referenceID": 13, "context": "Different with conventional one-hot representations of words or distributional word representations based on co-occurrence matrix between words such as LSA [14] and LDA [5], distributed word representations are usually low-dimensional dense vectors trained with neural networks by maximizing the likelihood of a text corpus.", "startOffset": 156, "endOffset": 160}, {"referenceID": 4, "context": "Different with conventional one-hot representations of words or distributional word representations based on co-occurrence matrix between words such as LSA [14] and LDA [5], distributed word representations are usually low-dimensional dense vectors trained with neural networks by maximizing the likelihood of a text corpus.", "startOffset": 169, "endOffset": 172}, {"referenceID": 9, "context": "[10, 11] proposed a neural network that can learn a unified word representations suited for several NLP tasks simultaneously.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[10, 11] proposed a neural network that can learn a unified word representations suited for several NLP tasks simultaneously.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "word2vec [21].", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "Some efforts have paid attention to learn word embedding in order to address knowledge base completion and enhancement [7, 28, 34]; however, they did not investigate the other side of the coin, i.", "startOffset": 119, "endOffset": 130}, {"referenceID": 27, "context": "Some efforts have paid attention to learn word embedding in order to address knowledge base completion and enhancement [7, 28, 34]; however, they did not investigate the other side of the coin, i.", "startOffset": 119, "endOffset": 130}, {"referenceID": 33, "context": "Some efforts have paid attention to learn word embedding in order to address knowledge base completion and enhancement [7, 28, 34]; however, they did not investigate the other side of the coin, i.", "startOffset": 119, "endOffset": 130}, {"referenceID": 19, "context": "[20] proposed a neural network model to learn word representations by leveraging morphological knowledge on words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] proposed a new learning objective that incorporates both a neural language model objective and a semantic prior knowledge objective to learn improved word representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] recently proposed to leverage morphological, syntactic, and semantic knowledge to advance the learning of word embeddings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] leveraged the global context information to train an initial word embedding and then proposed a clustering based method to produce multi-sense word embeddings for polysemous words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] proposed to model word polysemy from a probabilistic perspective and integrate it with the word2vec model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "For example, in Wechsler Adult Intelligence Scale [33], which is among the most famous IQ test systems, the full-scale IQ is calculated from two IQ scores: Verbal IQ and Performance IQ, and around 38% questions in a typical test are verbal comprehension questions.", "startOffset": 50, "endOffset": 54}, {"referenceID": 34, "context": "In another popular system named WoodcockJohnson Tests of Cognitive Abilities [35], the final IQ score is derived from three tests including the Verbal Comprehension Test.", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "According to previous studies [8], verbal questions mainly have the following types: analogy, classification, synonym, and antonym, which are elaborated in detailed as below.", "startOffset": 30, "endOffset": 33}, {"referenceID": 20, "context": "Note that the Analogy-I questions are also used as a major evaluation task in the word2vec models [21].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "Such questions are also known as Odd-One-Out, which have been studied in [24].", "startOffset": 73, "endOffset": 77}, {"referenceID": 0, "context": "Specifically, we regard each verbal question as a short document and use the TF\u00b7IDF [1] feature to build its representation.", "startOffset": 84, "endOffset": 87}, {"referenceID": 11, "context": "Then we train an SVM [12] classifier with linear kernel on a portion of labeled question data, and apply it to other questions.", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "We use the one-vs-rest training strategy [4] to obtain a linear SVM classifier for each question type.", "startOffset": 41, "endOffset": 44}, {"referenceID": 24, "context": "Several previous studies [25, 17, 31] have proposed using multiple representations to capture the different senses of a word.", "startOffset": 25, "endOffset": 37}, {"referenceID": 16, "context": "Several previous studies [25, 17, 31] have proposed using multiple representations to capture the different senses of a word.", "startOffset": 25, "endOffset": 37}, {"referenceID": 30, "context": "Several previous studies [25, 17, 31] have proposed using multiple representations to capture the different senses of a word.", "startOffset": 25, "endOffset": 37}, {"referenceID": 16, "context": "In this framework, we present a way, similar to [17], of using pre-learned singlesense embeddings to represent each context window, which can then be clustered to perform word sense discrimination.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "Beyond the method in [17], we also take advantages of additional knowledge in online dictionaries to regularize the word sense discrimination.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "First, we learn a single-sense word embedding by using the skip-gram method in word2vec [21] (see Figure 2).", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "Inspired by some recent work on multi-relation model [6, 36] that builds relationships between entities by interpreting them as translations operating on the low-dimensional representations of the entities, we propose to use a function Er as described below to capture the relational knowledge.", "startOffset": 53, "endOffset": 60}, {"referenceID": 6, "context": "To avoid this problem, we use an additional constraint on the norms, which is a commonly-used trick in the literature [7].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "However, instead of enforcing the L2-norm of the representations to 1 as used in [7], we adopt a soft norm constraint on the relation representations as below:", "startOffset": 81, "endOffset": 84}, {"referenceID": 20, "context": "[21] showed that such analogical relations can be reflected by word vector offsets between each pair of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We collected a set of verbal comprehension questions associated with correct answers from the published IQ test books, such as [8, 9, 23, 18], and we used this collection as the test set to evaluate the effectiveness of our new framework.", "startOffset": 127, "endOffset": 141}, {"referenceID": 8, "context": "We collected a set of verbal comprehension questions associated with correct answers from the published IQ test books, such as [8, 9, 23, 18], and we used this collection as the test set to evaluate the effectiveness of our new framework.", "startOffset": 127, "endOffset": 141}, {"referenceID": 22, "context": "We collected a set of verbal comprehension questions associated with correct answers from the published IQ test books, such as [8, 9, 23, 18], and we used this collection as the test set to evaluate the effectiveness of our new framework.", "startOffset": 127, "endOffset": 141}, {"referenceID": 17, "context": "We collected a set of verbal comprehension questions associated with correct answers from the published IQ test books, such as [8, 9, 23, 18], and we used this collection as the test set to evaluate the effectiveness of our new framework.", "startOffset": 127, "endOffset": 141}, {"referenceID": 21, "context": "In our experiments, we applied our framework to a public GRE Antonym Dataset [22] containing 162 questions.", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "Latent Dirichlet Allocation (LDA) [5].", "startOffset": 34, "endOffset": 37}, {"referenceID": 20, "context": "In this baseline, we applied the word embedding trained by skip-gram [21] (denoted by SG-1).", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "In this baseline, we applied the multi-sense word embedding models proposed in [17] and [31] (denoted by MS-1 and MS-2 respectively).", "startOffset": 79, "endOffset": 83}, {"referenceID": 30, "context": "In this baseline, we applied the multi-sense word embedding models proposed in [17] and [31] (denoted by MS-1 and MS-2 respectively).", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "We used a public relation knowledge set, WordRep [15], for relation training.", "startOffset": 49, "endOffset": 53}, {"referenceID": 25, "context": "[26] built an IQ test solver using WWW-based computer program.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Verbal comprehension questions appear very frequently in IQ tests, which measure human\u2019s verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by artificial intelligence technologies, especially the deep learning technologies that are recently developed and successfully applied in a number of fields. However, we found that the task was quite challenging, and simply applying existing technologies (e.g., word embedding) could not achieve a good performance, mainly due to the multiple senses of words and the complex relations among words. To tackle this challenge, we propose a novel framework consisting of three components. First, we build a classifier to recognize the specific type of a verbal question (e.g., analogy, classification, synonym, or antonym). Second, we obtain distributed representations of words and relations by leveraging a novel word embedding method that considers the multi-sense nature of words and the relational knowledge among words (or their senses) contained in dictionaries. Third, for each specific type of questions, we propose a simple yet effective solver based on the obtained distributed word representations and relation representations. According to our experimental results, our proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of human beings. The results are highly encouraging, indicating that with appropriate uses of the deep learning technologies, we could be a further step closer to the true human intelligence. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.", "creator": "LaTeX with hyperref package"}}}