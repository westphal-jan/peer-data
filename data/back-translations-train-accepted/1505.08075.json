{"id": "1505.08075", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2015", "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "abstract": "We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.", "histories": [["v1", "Fri, 29 May 2015 14:58:12 GMT  (343kb,D)", "http://arxiv.org/abs/1505.08075v1", "Proceedings of ACL 2015"]], "COMMENTS": "Proceedings of ACL 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["chris dyer", "miguel ballesteros", "wang ling", "austin matthews", "noah a smith"], "accepted": true, "id": "1505.08075"}, "pdf": {"name": "1505.08075.pdf", "metadata": {"source": "CRF", "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "authors": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "emails": ["chris@marianaslabs.com,", "miguel.ballesteros@upf.edu,", "lingwang@cs.cmu.edu", "austinma@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Stack LSTMs", "text": "We follow the convention that vectors are written with small, bold letters (e.g. v or vw), matrices with uppercase letters, bold letters (e.g. M, Ma or Mab) and scalars as lowercase letters (e.g. s or qz). Structured objects such as sequences of discrete symbols are written with lowercase letters, bold italic letters (e.g. w refers to a sequence of input words)."}, {"heading": "2.1 Long Short-Term Memories", "text": "LSTMs are a variant of recursive neural networks (RNNs) designed to deal with the disappearing gradient problem of RNNs (Hochreiter and Schmidhuber, 1997; Graves, 2013). RNNs read a vector xt at each time step and compute a new (hidden) state by linking a linear map to the concatenation of the previous time step \u2212 1 and applying the input and leading it through a logistic sigmodid nonlinearity. Although RNNs can in principle model far-reaching dependencies, their training is difficult in practice, since the repeated application of a squeezing nonlinearity at each step leads to an exponential decay of the error signal over time. LSTMs address this with an additional memory cell (ct) that arises as a linear combination of the previous state and the signal from the input of Wiht xxt."}, {"heading": "2.2 Stack Long Short-Term Memories", "text": "Conventional LSTM model sequences in a left-to-right order. (Our innovation here is to supplement the LSTM with a \"stack pointer.\" Like a conventional LSTM, new input is always added in the right position, but in stack LSTMs, the current position of the stack pointer determines which cell in the LSTM provides ct \u2212 1 and ht \u2212 1 when computing the new memory cell contents. In addition to adding elements to the end of the sequence, the LSTM stack provides a pop operation that shifts the stack pointer to the previous element (i.e. the previous element that is not necessarily the most element). Thus, the LSTM can be understood as a stack that is never overwritten, that is, push always adds a new entry at the end of the list that contains a back pointer to the previous element, and updates the stack only 1.9 in the control structure."}, {"heading": "3 Dependency Parser", "text": "We now turn to the problem of learning to display dependency parsers. We preserve the default data structures of a transition-based dependency parser, namely a buffer of words (B) to be processed, and a stack (S) of partially constructed syntactic elements. Each stack element is augmented by a continuous vector that represents one word and, in the case of S., all its syntactic dependencies. In addition, we introduce a third stack (A) to represent the history of actions taken by the parser. 3 Each of these stacks is associated with a stack LSTM that provides a coding of its current contents. Figure 3 illustrates the complete architecture, and we will check each of the components.3The A stack is always pushed; our use of a stack here is purely for implementation and expositional reasons."}, {"heading": "3.1 Parser Operation", "text": "The dependency parser is initialized by moving the words and their representations (we discuss word representations below in \u00a7 3.3) of the input set in reverse order to B, so that the first word is at the top of B and the ROOT symbol is at the bottom, and S and A each contain an emptystack token. At each step, the parser computes a composite representation of the stack states (as determined by the current configurations of B, S, and A) and uses this to predict an action to be taken that updates the stacks. Processing terminates when B is empty (except for the empty stack symbol), S contains two elements, one of which represents the complete parser tree with the ROOT symbol and the other the empty stack symbol, andA is the history of operations performed by the parser."}, {"heading": "3.2 Transition Operations", "text": "Our parser is based on the arc standard transition inventory described in Figure 3.5 (Nivre, 2004). For example, if S is empty and words remain in B, a SHIFT operation is mandatory (Sartorio et al., 2013).Why arc standard transitions? Arc standard transitions analyze a set from left to right, using a stack to store partially constructed syntactic structures and a buffer that holds the incoming tokens to be parsed.The parsing algorithm selects an action based on a score for each configuration. In arc standard parsing, the dependency tree is constructed from bottom to top, since right-dependent heads of a head are appended only after the subtree has been fully parsed under the dependent.Since our parser combines recursively representations of tree fragments, this construction order ensures that another structure is synchronized below that structure."}, {"heading": "3.3 Token Embeddings and OOVs", "text": "To represent each input character, we link three vectors: a learned vector representation for each word type (w); a fixed vector representation from a neural language model (w-LM); and a learned representation (t) of the POS tag of the token, which is provided to the parser as an auxiliary input; the alinear map (V) is applied to the resulting vector and passed on by a component-wise ReLU, x = max {0, V [w; w-LM; t] + b} This mapping can be schematic as in Figure 4.This architecture allows us to flexibly deal with vocabulary words present in both the very limited parsing data and in the pretraining LM, and words that are in both OOVs. To ensure that we have estimates of the OVs in the parsing training data, we replace them in an astical way (with 0.5) each singletype in the parsing data."}, {"heading": "3.4 Composition Functions", "text": "Recursive neural network models allow the compositional representation of complex phrases in relation to their parts and the relationships that connect them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this earlier line of work in embedding dependency tree fragments present in stack S in the same vector space as the token embedding discussed above. A particular challenge here is that a syntactic head can generally have an arbitrary number of dependencies. To simplify the parameterization of our composition function, we combine head change pairs one after the other and build more complex structures in the order in which they are \"reduced\" in the parser, as shown in Figure 5. Each node in this extended syntactic tree has a value that is calculated as a function of its three arguments: the syntactic head (h), the dependent (the) relationship (the tactical) and the (tactical) (the tactical) one."}, {"heading": "4 Training Procedure", "text": "We trained our parser to maximize the conditional log probability (equivalent. 1) of the tree bank parsing given sets. Our implementation constructs a calculation curve for each set and runs forwards and backwards to obtain the gradients of that target in terms of model parameters. Calculations for a single parsing model were performed on a single thread on a CPU. With the dimensions discussed in the next section, it took us between 8 and 12 hours to achieve convergence on a held scope of development.6Parameter optimization was performed with stochastic gradient pedigree with an initial learning rate of 0 = 0.1, and the learning rate was updated on each pass by the training data as it is supposed to happen 0 / (1 + 1), with the number of epochs completed."}, {"heading": "5 Experiments", "text": "We applied our parsing model and several variants of it to two parsing tasks and reported on the results below."}, {"heading": "5.1 Data", "text": "We used the same data structure as Chen and Manning (2014), namely an English and a Chinese parsing task. This basic configuration was chosen because they also used neural parameterization to predict actions in an arc standards-based transition parser. \u2022 For English, we used the Stanford Dependencency (SD) treebank (de Marneffe et al., 2006), which in (Chen and Manning, 2014) is the closest published model with the same splits.8 For Chinese, we use the Stanford Tagger (Toutanova et al., 2003) with an accuracy of 97.3%. This treebank contains a negligible amount of non-projective arcs (Chen and Manning, 2014). \u2022 For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5 et al., 2003) with an accuracy of 97.3%. This treebank contains a negligible amount of non-projective arcs (2014, Manning and 2014)."}, {"heading": "5.2 Experimental configurations", "text": "We report on the results of five experimental configurations per language and the baseline of Chen and Manning (2014): the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (\u2212 POS), the stack LSTM parsing model without pre-trained speech model embedding (\u2212 pretraining), the stack LSTM parsing model, which uses only headers on the stack instead of compound representations (\u2212 composition), and the full parsing model, which uses a classically recurrent neural network (S-RNN) instead of an LSTM."}, {"heading": "5.3 Results", "text": "According to Chen and Manning (2014), we exclude punctuation symbols for evaluation. Tables 1 and 2 show comparable results with Chen and Manning (2014) and we show that our model is better than their model in both development set and test set."}, {"heading": "5.4 Analysis", "text": "Overall, our parser significantly exceeds the neural network parser of Chen and Manning (2014), both in the full configuration and in the various lowered conditions we report on, the only exception being the \u2212 POS condition for the Chinese parsing task, where we underperform their baseline (where we used golden POS tags), although we still achieve reasonable parsing performance in this limited case. We note that predicted POS tags have very little added value - suggesting that we can analyze sentences directly without first marking them. We also note that the use of compiled representations of dependency tree fragments exceeds the representation of headers alone, which has an impact on scalp theories. Finally, we note that while LSTMs exceed baselines that use classical RNNs only, they are quite capable of learning good representations.The effects of beam size alone were determined to have a minimal impact on balances of \u2264% (small improvements were possible)."}, {"heading": "6 Related Work", "text": "The et al. (1992) proposed a neural network with an external stack memory based on recurring neural networks. In contrast to our model, in which the entire contents of the stack are summarized in a single value, the network could only see the contents of the top of the stack. Mikkulainen (1996) proposed an architecture with a stack in which the entire stack control is summarized in a single value, in their model the network could only see the contents of the top of the stack. Mikkulainen (1996) proposed an architecture with a stack that had a summarizing function even though the stack control was learned as latent variability. A multitude of authors have used neural networks to predict parser actions in displacement-reduced parsers. The earliest attempt we know of this is attributable to Mayberry and miiculaine parameters (1999). Resurgence of interest in neural networks has occurred 10Although we are overly similar to our scenario, parser actions are parameters as a structure of parrays adaptation to vinarios (a structure of parrays)."}, {"heading": "7 Conclusion", "text": "We introduced stack LSTMs, recurrent neural networks for sequences, with push and pop operations, and used them to implement a modern transition-based dependency parser. We conclude by noting that stack memory provides fascinating ways to learn to solve common problems of information processing (Mickulaines, 1996), where we learned from observable stack manipulation operations (i.e., monitoring from a tree base), and the calculated embedding of final parser states were not used for further predictions, but this could be reversed by giving only observed results to a device that learns to construct context-free programs (e.g. expression trees); one application would be unattended parsing. Such an expansion of the work would make it an alternative to architectures that have explicit external memory, such as neural turing machines (Graves et al., 2014 and West memory networks)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Lingpeng Kong and Jacob Eisenstein for their comments on an earlier version of this draft and Danqi Chen for their assistance in parsing the data sets. This work was partly sponsored by the US Army Research Laboratory and the US Army Research Office under contract / grant number W911NF-10-1-0533 and partly by the NSF CAREER Grant IIS-1054319. Miguel Ballesteros is supported by the European Commission under contract number FP7-ICT610411 (Project MULTISENSOR) and H2020RIA-645012 (Project KRISTINA)."}], "references": [{"title": "Automatic feature selection for agenda-based dependency parsing", "author": ["Ballesteros", "Bohnet2014] Miguel Ballesteros", "Bernd Bohnet"], "venue": "In Proc. COLING", "citeRegEx": "Ballesteros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2014}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proc. ACL", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bohnet", "Nivre2012] Bernd Bohnet", "Joakim Nivre"], "venue": "In Proc. EMNLP", "citeRegEx": "Bohnet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2012}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D. Manning"], "venue": "In Proc. EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Feature embedding for dependency parsing", "author": ["Chen et al.2014] Wenliang Chen", "Yue Zhang", "Min Zhang"], "venue": "In Proc. COLING", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with selectional branching", "author": ["Choi", "McCallum2013] Jinho D. Choi", "Andrew McCallum"], "venue": "In Proc. ACL", "citeRegEx": "Choi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2013}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["Das et al.1992] Sreerupa Das", "C. Lee Giles", "GuoZheng Sun"], "venue": "In Proc. Cognitive Science Society", "citeRegEx": "Das et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Das et al\\.", "year": 1992}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Bill MacCartney", "Christopher D. Manning"], "venue": "In Proc. LREC", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "In Proc. ICML", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proc. AISTATS", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Efficient implementation of beam-search incremental parsers", "author": ["Kai Zhao", "Liang Huang"], "venue": "In Proc. ACL", "citeRegEx": "Goldberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional LSTM networks", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. IJCNN", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Multi-dimensional recurrent neural networks", "author": ["Graves et al.2007] Alex Graves", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. ICANN", "citeRegEx": "Graves et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2007}, {"title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "New directions in vector space models of meaning", "author": ["Grefenstette", "Karl Moritz Hermann", "Georgiana Dinu", "Phil Blunsom."], "venue": "ACL Tutorial.", "citeRegEx": "Grefenstette et al\\.,? 2014", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2014}, {"title": "Discriminative training of a neural network discriminative parser", "author": ["James Henderson"], "venue": "In Proc. ACL", "citeRegEx": "Henderson.,? \\Q2004\\E", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Hermann", "Blunsom2013] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proc. ACL", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Forest reranking: Discriminative parsing with non-local features", "author": ["Huang", "Chiang2008] Liang Huang", "David Chiang"], "venue": "In Proc. ACL", "citeRegEx": "Huang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2008}, {"title": "Inside-outside recursive neural network model for dependency parsing", "author": ["Le", "Zuidema2014] Phong Le", "Willem Zuidema"], "venue": "In Proc. EMNLP", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso"], "venue": "In Proc. NAACL", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Turboparsers: Dependency parsing by approximate variational inference", "author": ["Noah A. Smith", "Eric P. Xing", "Pedro M.Q. Aguiar", "M\u00e1rio A.T. Figueiredo"], "venue": "In Proc. EMNLP", "citeRegEx": "Martins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "SARDSRN: A neural network shift-reduce parser", "author": ["Mayberry", "Risto Miikkulainen"], "venue": "In Proc. IJCAI", "citeRegEx": "Mayberry et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mayberry et al\\.", "year": 1999}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proc. NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proc. IWPT", "citeRegEx": "Nivre.,? \\Q2003\\E", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together", "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Incremental nonprojective dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proc. NAACL", "citeRegEx": "Nivre.,? \\Q2007\\E", "shortCiteRegEx": "Nivre.", "year": 2007}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre"], "venue": "In Proc. ACL", "citeRegEx": "Nivre.,? \\Q2009\\E", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "How to construct deep recurrent neural networks", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. ICLR", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "A transition-based dependency parser using a dynamic parsing strategy", "author": ["Giorgio Satta", "Joakim Nivre"], "venue": "In Proc. ACL", "citeRegEx": "Sartorio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sartorio et al\\.", "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proc. NIPS", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proc. ACL", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences. TACL", "author": ["Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proc. EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Transitionbased dependency parsing using recursive neural networks", "author": ["Pontus Stenetorp"], "venue": "In Proc. NIPS Deep Learning Workshop", "citeRegEx": "Stenetorp.,? \\Q2013\\E", "shortCiteRegEx": "Stenetorp.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Constituent parsing with incremental sigmoid belief networks", "author": ["Titov", "Henderson2007] Ivan Titov", "James Henderson"], "venue": "In Proc. ACL", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In Proc. NAACL", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "A conditional random field word segmenter for SIGHAN bakeoff", "author": ["Tseng et al.2005] Huihsin Tseng", "Pichuan Chang", "Galen Andrew", "Daniel Jurafsky", "Christopher Manning"], "venue": "In Proc. Fourth SIGHAN Workshop on Chinese Language", "citeRegEx": "Tseng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Grammar as a foreign language", "author": ["Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proc. ICLR", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Christopher Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proc. ACL", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Yamada", "Matsumoto2003] Hiroyasu Yamada", "Yuji Matsumoto"], "venue": "In Proc. IWPT", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "Reinforcement learning neural Turing machines", "author": ["Zaremba", "Sutskever2015] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "author": ["Zhang", "Clark2008] Yue Zhang", "Stephen Clark"], "venue": "In Proc. EMNLP", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": "In Proc. ACL", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Greed is good if randomized: New inference for dependency parsing", "author": ["Zhang et al.2014] Yuan Zhang", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proc. EMNLP", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004).", "startOffset": 192, "endOffset": 246}, {"referenceID": 25, "context": "Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004).", "startOffset": 192, "endOffset": 246}, {"referenceID": 26, "context": "This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment decisions (Nivre, 2007; Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013; Bohnet and Nivre, 2012), through feature engineering (Zhang and Nivre, 2011; Ballesteros and Nivre, 2014; Chen et al.", "startOffset": 153, "endOffset": 241}, {"referenceID": 27, "context": "This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment decisions (Nivre, 2007; Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013; Bohnet and Nivre, 2012), through feature engineering (Zhang and Nivre, 2011; Ballesteros and Nivre, 2014; Chen et al.", "startOffset": 153, "endOffset": 241}, {"referenceID": 28, "context": "This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment decisions (Nivre, 2007; Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013; Bohnet and Nivre, 2012), through feature engineering (Zhang and Nivre, 2011; Ballesteros and Nivre, 2014; Chen et al.", "startOffset": 153, "endOffset": 241}, {"referenceID": 3, "context": "This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment decisions (Nivre, 2007; Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013; Bohnet and Nivre, 2012), through feature engineering (Zhang and Nivre, 2011; Ballesteros and Nivre, 2014; Chen et al., 2014; Ballesteros and Bohnet, 2014) and more recently using neural networks (Chen and Manning, 2014; Stenetorp, 2013).", "startOffset": 271, "endOffset": 372}, {"referenceID": 35, "context": ", 2014; Ballesteros and Bohnet, 2014) and more recently using neural networks (Chen and Manning, 2014; Stenetorp, 2013).", "startOffset": 78, "endOffset": 119}, {"referenceID": 13, "context": "LSTMs are a variant of recurrent neural networks (RNNs) designed to cope with the vanishing gradient problem inherent in RNNs (Hochreiter and Schmidhuber, 1997; Graves, 2013).", "startOffset": 126, "endOffset": 174}, {"referenceID": 29, "context": "To improve the representational capacity of LSTMs (and RNNs generally), LSTMs can be stacked in \u201clayers\u201d (Pascanu et al., 2014).", "startOffset": 105, "endOffset": 127}, {"referenceID": 12, "context": "Ours is not the first deviation from a strict left-toright order: previous variations include bidirectional LSTMs (Graves and Schmidhuber, 2005) and multidimensional LSTMs (Graves et al., 2007).", "startOffset": 172, "endOffset": 193}, {"referenceID": 10, "context": "Goldberg et al. (2013) propose a similar stack construction to prevent stack operations from invalidating existing references to the stack in a beam-search parser that must (efficiently) maintain a priority queue of stacks.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "Although this architecture is to the best of our knowledge novel, it is reminiscent of the Recurrent Neural Network Pushdown Automaton (NNPDA) of Das et al. (1992), which added an external stack memory to an RNN.", "startOffset": 146, "endOffset": 164}, {"referenceID": 9, "context": "where W is a learned parameter matrix, bt is the stack LSTM encoding of the input buffer B, st is the stack LSTM encoding of S, at is the stack LSTM encoding of A, d is a bias term, then passed through a component-wise rectified linear unit (ReLU) nonlinearity (Glorot et al., 2011).", "startOffset": 261, "endOffset": 282}, {"referenceID": 25, "context": "Our parser is based on the arc-standard transition inventory (Nivre, 2004), given in Figure 3.", "startOffset": 61, "endOffset": 74}, {"referenceID": 30, "context": "For example, when S is empty and words remain in B, a SHIFT operation is obligatory (Sartorio et al., 2013).", "startOffset": 84, "endOffset": 107}, {"referenceID": 1, "context": "However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al.", "startOffset": 98, "endOffset": 119}, {"referenceID": 23, "context": "The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in \u00a75.", "startOffset": 94, "endOffset": 116}, {"referenceID": 1, "context": "However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al. (2015), named \u201cstructured skip n-gram,\u201d where a different set of parameters is used to predict each context word depending on its position relative to the target word.", "startOffset": 99, "endOffset": 207}, {"referenceID": 31, "context": "Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b).", "startOffset": 146, "endOffset": 238}, {"referenceID": 36, "context": "To mitigate the effects of \u201cexploding\u201d gradients, we clipped the `2 norm of the gradient to 5 before applying the weight update rule (Sutskever et al., 2014; Graves, 2013).", "startOffset": 133, "endOffset": 171}, {"referenceID": 13, "context": "To mitigate the effects of \u201cexploding\u201d gradients, we clipped the `2 norm of the gradient to 5 before applying the weight update rule (Sutskever et al., 2014; Graves, 2013).", "startOffset": 133, "endOffset": 171}, {"referenceID": 38, "context": "8 The part-of-speech tags are predicted by using the Stanford Tagger (Toutanova et al., 2003) with an accuracy of 97.", "startOffset": 69, "endOffset": 93}, {"referenceID": 39, "context": "as segmented by the Stanford Chinese Segmenter (Tseng et al., 2005).", "startOffset": 47, "endOffset": 67}, {"referenceID": 14, "context": "This finding is in line with previous work that generates sequences from recurrent networks (Grefenstette et al., 2014), although Vinyals et al.", "startOffset": 92, "endOffset": 119}, {"referenceID": 14, "context": "This finding is in line with previous work that generates sequences from recurrent networks (Grefenstette et al., 2014), although Vinyals et al. (2015) did report much more substantial improvements with beam search on their \u201cgrammar as a foreign language\u201d parser.", "startOffset": 93, "endOffset": 152}, {"referenceID": 6, "context": "Das et al. (1992) proposed a neural network with an external stack memory based on recurrent neural networks.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Das et al. (1992) proposed a neural network with an external stack memory based on recurrent neural networks. In contrast to our model, in which the entire contents of the stack are summarized in a single value, in their model, the network could only see the contents of the top of the stack. Mikkulainen (1996) proposed an architecture with a stack that had a summary feature, although the stack control was learned as a latent variable.", "startOffset": 0, "endOffset": 312}, {"referenceID": 40, "context": "Although superficially similar to ours, Vinyals et al. (2015) is a phrase-structure parser and adaptation to the dependency parsing scenario would have been nontrivial.", "startOffset": 40, "endOffset": 62}, {"referenceID": 41, "context": "in in several applications to transition-based dependency parsers (Weiss et al., 2015; Chen and Manning, 2014; Stenetorp, 2013).", "startOffset": 66, "endOffset": 127}, {"referenceID": 35, "context": "in in several applications to transition-based dependency parsers (Weiss et al., 2015; Chen and Manning, 2014; Stenetorp, 2013).", "startOffset": 66, "endOffset": 127}, {"referenceID": 15, "context": "Neural networks have also been used to learn representations for use in chart parsing (Henderson, 2004; Titov and Henderson, 2007; Socher et al., 2013a; Le and Zuidema, 2014).", "startOffset": 86, "endOffset": 174}, {"referenceID": 30, "context": ", 2015; Chen and Manning, 2014; Stenetorp, 2013). In these works, the conditioning structure was manually crafted and sensitive to only certain properties of the state, while we are conditioning on the global state object. Like us, Stenetorp (2013) used recursively composed representations of the tree fragments (a head and its dependents).", "startOffset": 32, "endOffset": 249}, {"referenceID": 40, "context": "Vinyals et al. (2015) proposed a phrasestructure parser based on LSTMs which operated by first reading the entire input sentence in so as to obtain a vector representation of it, and then generating bracketing structures sequentially conditioned on this representation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 21, "context": "An exhaustive summary is beyond the scope of this paper, but some of the important milestones in this tradition are the use of cube pruning to efficiently include nonlocal features in discriminative chart reranking (Huang and Chiang, 2008), approximate decoding techniques based on LP relaxations in graph-based parsing to include higherorder features (Martins et al., 2010), and randomized hill-climbing methods that enable arbitrary nonlocal features in global discriminative parsing models (Zhang et al.", "startOffset": 352, "endOffset": 374}, {"referenceID": 46, "context": ", 2010), and randomized hill-climbing methods that enable arbitrary nonlocal features in global discriminative parsing models (Zhang et al., 2014).", "startOffset": 126, "endOffset": 146}], "year": 2015, "abstractText": "We propose a technique for learning representations of parser states in transitionbased dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks\u2014 the stack LSTM. Like the conventional stack data structures used in transitionbased parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser\u2019s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.", "creator": "LaTeX with hyperref package"}}}