{"id": "1506.00275", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2015", "title": "Diversity in Spectral Learning for Natural Language Parsing", "abstract": "We describe an approach to incorporate diversity into spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the $F_1$ score of 90.18, and for German we achieve the $F_1$ score of 83.38.", "histories": [["v1", "Sun, 31 May 2015 19:21:26 GMT  (57kb,D)", "https://arxiv.org/abs/1506.00275v1", null], ["v2", "Sat, 15 Aug 2015 12:02:57 GMT  (95kb,D)", "http://arxiv.org/abs/1506.00275v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shashi narayan", "shay b cohen"], "accepted": true, "id": "1506.00275"}, "pdf": {"name": "1506.00275.pdf", "metadata": {"source": "CRF", "title": "Diversity in Spectral Learning for Natural Language Parsing", "authors": ["Shashi Narayan", "Shay B. Cohen"], "emails": ["snaraya2@inf.ed.ac.uk", "scohen@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "We have a long list of solutions, ranging from a decoder to a reconstruction, to improve accuracy in various areas (e.g. health, social, health and health), such as machine translation (Macho and Och, 2007), which means that there are a number of concepts that deal with the way they are put into practice (e.g. health, social and health)."}, {"heading": "2 Background and Notation", "text": "A L-PCFG is a 5-tuple tree (N, I, P, m, n) in which: \u2022 N is the set of nonterminal symbols in grammar. \u2022 I-N is a finite set of interminals. \u2022 P-N is a finite set of preterminals. We assume that N = I-P contains trees, and I-P = \u2205 P. Therefore we have divided the set of nonterminal symbols into two subsets. \u2022 [m] is the set of possible hidden states. \u2022 I, b-N, c-N, h1, h1, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h4, h5, h5, h5, h5, h5, h5, h5, h5, h3, h3, h3, h3, h3, h5, h5, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, 6, h3, h3, h3, 6, h3, h3, h5, h5, h5, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h5, h5, h3, h5, h5, h5, h5, h5, h5, h5, h5, h3, h5, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h5, h3, h5, h5, h5, h3, h3, h5, h5, h5, h5, h5, h5, h5, h3, h5, h3, h3, h5, h3, h3, h3, h3, h3, h3"}, {"heading": "3 Clustering Algorithm for Estimating L-PCFGs", "text": "We assume that this is an empirical estimate of the variance between the trees inside and outside the trees."}, {"heading": "4 Spectral Estimation with Noise", "text": "It has been shown that a variety of predictions can be used to improve the accuracy of decoders for various problems in NLP (Henderson and Brill, 1999). Typically, a k-best list of a single model is used to take advantage of model diversity. Instead, we estimate several models in which the underlying schemes are filtered with different noise schemes. We try to use three different types of noise schemes for the algorithm in Figure 2: dropout schemes. We put each element into the feature schemes, in which the underlying schemes are filtered with different noise schemes. We try three different types of noise schemes for the algorithm in Figure 2: dropout schemes. We draw a vector scheme of Gaussians with mean 0 and variance 2, and then use x (i)."}, {"heading": "5 Decoding with Multiple Models", "text": "The question that remains is how to combine these models to obtain a single best output sector tree that has given an input set. We define t (Gi, s) to decrypt the output tree according to the minimum Bayes risk. This means we follow Goodman (1996) who uses dynamic programming to calculate the tree that maximizes the sum of all output trees. Each output tree is marginal, for each output tree < a, j > (where there are a non-terminal and i and j endpoints in the set)."}, {"heading": "6 Experiments", "text": "In this section we describe parsing experiments in two languages: English and German."}, {"heading": "6.1 Results for English", "text": "We are dealing with a series of different models that deal with the question of what the future of humanity is like, and what the future of humanity is like. (...) We are dealing with a multitude of models that deal with the future of humanity. (...) We are dealing with a multitude of models that deal with the past. (...) We are dealing with a multitude of models that deal with the past. (...) We are dealing with the past. \"(...) We are dealing with the past.\" (...) We are dealing with the past. \"(...) We are dealing with the past.\" (...) We are dealing with the past. \"(...) We are dealing with the past.\" (...) We are dealing with the past. (...) We are dealing with the past. (...) We are dealing with the past. (...) We are dealing with the past."}, {"heading": "6.2 Results for German", "text": "For the German experiments we used the NEGRA-Corpus (Skut et al., 1997). We use the same setup as in Petrov (2010), and use the first 18,602 sets as training set, the next 1,000 sets as development set and the last 1,000 sets as test set. This corresponds to an 80% -10% split of the treebank. Our German experiments follow the same setting as in our English experiments. For the clustering algorithms we generated 80 models, 20 for each group. (0.1, 0.15, 0.2). For the spectral algorithms we generate 20 models, 5 for each group."}, {"heading": "7 Discussion", "text": "From a theoretical point of view, one of the great advantages of spectral learning techniques for latent-variable models is that they provide consistent parameter estimates. Our cluster algorithm for L-PCFG estimation breaks this, but there is a workaround to get an algorithm that would be statistically consistent. The main reason that our algorithm is not a consistent estimator is that it is based on k-mean clusters, which maximizes a non-convex target through hard cluster steps. The k-mean algorithm can be considered a \"hard EM\" for a Gaussian mixing model (GMM), in which any latent state is associated with one of the mixing components in the GMM. This means that instead of pursuing k-means, we could have identified the parameters and the rear parameters for a GMM where the observations correspond to the vectors that we cluster now have."}, {"heading": "8 Conclusion", "text": "We presented a novel estimation algorithm for latent-variable PCFGs. This algorithm is based on clustering continuous tree representations and also leads to sparse grammar estimates and compact models. We also showed how to use this algorithm and older spectral algorithms to obtain a diverse set of parse tree predictions. Any prediction in this algorithm is made by training an L-PCFG model after disturbing the underlying features that the estimation algorithm uses from the training data. We showed that such a diverse set of predictions can be used to improve the parsing accuracy of English and German."}, {"heading": "Acknowledgements", "text": "The authors thank David McClosky for his help in running the BLLIP parser and the three anonymous reviewers for their helpful comments. This research was supported by an EPSRC grant (EP / L02411X / 1)."}], "references": [{"title": "A spectral approach for probabilistic grammatical inference on trees", "author": ["Rapha\u00ebl Bailly", "Amaury Habrard", "Fran\u00e7ois Denis."], "venue": "Proceedings of ALT.", "citeRegEx": "Bailly et al\\.,? 2010", "shortCiteRegEx": "Bailly et al\\.", "year": 2010}, {"title": "A procedure for quantitatively comparing the syntactic coverage of English grammars", "author": ["torini", "Tomek Strzalkowski"], "venue": "In Proceedings of DARPA Workshop on Speech and Natural Language", "citeRegEx": "torini and Strzalkowski.,? \\Q1991\\E", "shortCiteRegEx": "torini and Strzalkowski.", "year": 1991}, {"title": "TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing", "author": ["Xavier Carreras", "Michael Collins", "Terry Koo."], "venue": "Proceedings of CoNLL.", "citeRegEx": "Carreras et al\\.,? 2008", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Coarseto-fine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "Syntactic parse fusion", "author": ["Do Kook Choe", "David McClosky", "Eugene Charniak."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Choe et al\\.,? 2015", "shortCiteRegEx": "Choe et al\\.", "year": 2015}, {"title": "A provably correct learning algorithm for latent-variable PCFGs", "author": ["Shay B. Cohen", "Michael Collins."], "venue": "Proceedings of ACL.", "citeRegEx": "Cohen and Collins.,? 2014", "shortCiteRegEx": "Cohen and Collins.", "year": 2014}, {"title": "Spectral learning of latent-variable PCFGs", "author": ["Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle Ungar."], "venue": "Proceedings of ACL.", "citeRegEx": "Cohen et al\\.,? 2012", "shortCiteRegEx": "Cohen et al\\.", "year": 2012}, {"title": "Experiments with spectral learning of latent-variable PCFGs", "author": ["Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle Ungar."], "venue": "Proceedings of NAACL.", "citeRegEx": "Cohen et al\\.,? 2013", "shortCiteRegEx": "Cohen et al\\.", "year": 2013}, {"title": "Head-driven statistical models for natural language processing", "author": ["Michael Collins."], "venue": "Computational Linguistics, 29:589\u2013637.", "citeRegEx": "Collins.,? 2003", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Eigenwords: Spectral word embeddings", "author": ["Paramveer Dhillon", "Dean Foster", "Lyle Ungar."], "venue": "Journal of Machine Learning Research (to appear).", "citeRegEx": "Dhillon et al\\.,? 2015", "shortCiteRegEx": "Dhillon et al\\.", "year": 2015}, {"title": "What to do when lexicalization fails: Parsing German with suffix analysis and smoothing", "author": ["Amit Dubey."], "venue": "Proceedings of ACL.", "citeRegEx": "Dubey.,? 2005", "shortCiteRegEx": "Dubey.", "year": 2005}, {"title": "Combining constituent parsers", "author": ["Victoria Fossum", "Kevin Knight."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Fossum and Knight.,? 2009", "shortCiteRegEx": "Fossum and Knight.", "year": 2009}, {"title": "Parsing algorithms and metrics", "author": ["Joshua Goodman."], "venue": "Proceedings of ACL.", "citeRegEx": "Goodman.,? 1996", "shortCiteRegEx": "Goodman.", "year": 1996}, {"title": "Exploiting diversity in natural language processing: Combining parsers", "author": ["John C. Henderson", "Eric Brill."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Henderson and Brill.,? 1999", "shortCiteRegEx": "Henderson and Brill.", "year": 1999}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Daniel Hsu", "Sham M. Kakade", "Tong Zhang."], "venue": "Proceedings of COLT.", "citeRegEx": "Hsu et al\\.,? 2009", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala."], "venue": "Learning Theory, volume 3559 of Lecture Notes in Computer Science, pages 444\u2013457. Springer.", "citeRegEx": "Kannan et al\\.,? 2005", "shortCiteRegEx": "Kannan et al\\.", "year": 2005}, {"title": "An overview of probabilistic tree transducers for natural language processing", "author": ["Kevin Knight", "Jonathan Graehl."], "venue": "Computational linguistics and intelligent text processing, volume 3406 of Lecture Notes in Computer Science, pages 1\u201324.", "citeRegEx": "Knight and Graehl.,? 2005", "shortCiteRegEx": "Knight and Graehl.", "year": 2005}, {"title": "Conversation trees: A grammar model for topic structure in forums", "author": ["Annie Louis", "Shay B. Cohen."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Louis and Cohen.,? 2015", "shortCiteRegEx": "Louis and Cohen.", "year": 2015}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of NAACL.", "citeRegEx": "Lu et al\\.,? 2015", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Spectral learning for nondeterministic dependency parsing", "author": ["Franco M. Luque", "Ariadna Quattoni", "Borja Balle", "Xavier Carreras."], "venue": "Proceedings of EACL.", "citeRegEx": "Luque et al\\.,? 2012", "shortCiteRegEx": "Luque et al\\.", "year": 2012}, {"title": "An empirical study on computing consensus translations from multiple machine translation systems", "author": ["Wolfgang Macherey", "Franz Josef Och."], "venue": "Proceedings of EMNLP-CoNLL.", "citeRegEx": "Macherey and Och.,? 2007", "shortCiteRegEx": "Macherey and Och.", "year": 2007}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary A. Marcinkiewicz."], "venue": "Computational Linguistics, 19:313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "TurboParsers: Dependency parsing by approximate variational inference", "author": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Eric P. Xing", "M\u00e1rio A.T. Figueiredo", "Pedro M.Q. Aguiar."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Martins et al\\.,? 2010", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "Probabilistic CFG with latent annotations", "author": ["Takuya Matsuzaki", "Yusuke Miyao", "Junichi Tsujii."], "venue": "Proceedings of ACL.", "citeRegEx": "Matsuzaki et al\\.,? 2005", "shortCiteRegEx": "Matsuzaki et al\\.", "year": 2005}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant."], "venue": "Proceedings of IEEE Symposium on Foundations of Computer Science (FOCS).", "citeRegEx": "Moitra and Valiant.,? 2010", "shortCiteRegEx": "Moitra and Valiant.", "year": 2010}, {"title": "Is your anchor going up or down? Fast and accurate supervised topic models", "author": ["Thang Nguyen", "Jordan Boyd-Graber", "Jeffrey Lund", "Kevin Seppi", "Eric Ringger."], "venue": "Proceedings of NAACL.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proceedings of HLTNAACL.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proceedings of COLING-ACL.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Products of random latent variable grammars", "author": ["Slav Petrov."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Petrov.,? 2010", "shortCiteRegEx": "Petrov.", "year": 2010}, {"title": "Multiview LSA: Representation learning via generalized CCA", "author": ["Pushpendre Rastogi", "Benjamin Van Durme", "Raman Arora."], "venue": "Proceedings of NAACL.", "citeRegEx": "Rastogi et al\\.,? 2015", "shortCiteRegEx": "Rastogi et al\\.", "year": 2015}, {"title": "Parser combination by reparsing", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Sagae and Lavie.,? 2006", "shortCiteRegEx": "Sagae and Lavie.", "year": 2006}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "Proceedings of ACL.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "An annotation scheme for free word order languages", "author": ["Wojciech Skut", "Brigitte Krenn", "Thorsten Brants", "Hans Uszkoreit."], "venue": "Proceedings of ANLP.", "citeRegEx": "Skut et al\\.,? 1997", "shortCiteRegEx": "Skut et al\\.", "year": 1997}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A spectral algorithm for learning class-based n-gram models of natural language", "author": ["Karl Stratos", "Do-kyum Kim", "Michael Collins", "Daniel Hsu."], "venue": "Proceedings of UAI.", "citeRegEx": "Stratos et al\\.,? 2014", "shortCiteRegEx": "Stratos et al\\.", "year": 2014}, {"title": "Improving accuracy in word class tagging through the combination of machine learning systems", "author": ["Hans Van Halteren", "Jakub Zavrel", "Walter Daelemans."], "venue": "Computational linguistics, 27(2):199\u2013229.", "citeRegEx": "Halteren et al\\.,? 2001", "shortCiteRegEx": "Halteren et al\\.", "year": 2001}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang."], "venue": "Journal of Computer and System Sciences, 68(4):841\u2013860.", "citeRegEx": "Vempala and Wang.,? 2004", "shortCiteRegEx": "Vempala and Wang.", "year": 2004}, {"title": "Feature noising for log-linear structured prediction", "author": ["Sida Wang", "Mengqiu Wang", "Stefan Wager", "Percy Liang", "Christopher D Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "K-best combination of syntactic parsers", "author": ["Hui Zhang", "Min Zhang", "Chew Lim Tan", "Haizhou Li."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zhang et al\\.,? 2009", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 13, "context": "It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999).", "startOffset": 165, "endOffset": 192}, {"referenceID": 20, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al.", "startOffset": 42, "endOffset": 66}, {"referenceID": 3, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 30, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 11, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 38, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 28, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 4, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 19, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 7, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 34, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 9, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 29, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 25, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 18, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 23, "context": "(2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005).", "startOffset": 149, "endOffset": 173}, {"referenceID": 27, "context": "Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006).", "startOffset": 112, "endOffset": 133}, {"referenceID": 26, "context": "We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes.", "startOffset": 166, "endOffset": 190}, {"referenceID": 6, "context": "First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of Cohen et al. (2012), but simpler to understand and implement.", "startOffset": 89, "endOffset": 109}, {"referenceID": 7, "context": "We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al., 2013), and repeat this process multiple times.", "startOffset": 80, "endOffset": 130}, {"referenceID": 35, "context": "Our noise schemes are similar to those described by Wang et al. (2013). We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 6, "context": "We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al., 2013), and repeat this process multiple times. We then use the set of parses we get from all models in a recombination step. The rest of the paper is organized as follows. In \u00a72 we describe notation and background about L-PCFG parsing. In \u00a73 we describe our new spectral algorithm for estimating L-PCFGs. It is based on similar intuitions as older spectral algorithms for L-PCFGs. In \u00a74 we describe the various noise schemes we use with our spectral algorithm and the spectral algorithm of Cohen et al. (2013). In \u00a75 we describe how to decode with multiple models, each arising from a different noise setting.", "startOffset": 111, "endOffset": 635}, {"referenceID": 16, "context": "Latent-variable PCFGs are essentially equivalent to probabilistic regular tree grammars (PRTGs; Knight and Graehl, 2005) where the righthand side trees are of depth 1.", "startOffset": 88, "endOffset": 120}, {"referenceID": 9, "context": "1 This step is akin to CCA, which has been used in various contexts in NLP, mostly to derive representations for words (Dhillon et al., 2015; Rastogi et al., 2015).", "startOffset": 119, "endOffset": 163}, {"referenceID": 29, "context": "1 This step is akin to CCA, which has been used in various contexts in NLP, mostly to derive representations for words (Dhillon et al., 2015; Rastogi et al., 2015).", "startOffset": 119, "endOffset": 163}, {"referenceID": 6, "context": "The features that we use for \u03c6 and \u03c8 are similar to those used in Cohen et al. (2013). These features look at the local neighborhood surrounding a given node.", "startOffset": 66, "endOffset": 86}, {"referenceID": 5, "context": "Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al.", "startOffset": 0, "endOffset": 198}, {"referenceID": 0, "context": "(2009) and Bailly et al. (2010).", "startOffset": 11, "endOffset": 32}, {"referenceID": 6, "context": "In our preliminary experiments, we found out that the clustering algorithm by itself performs worse than the spectral algorithm of Cohen et al. (2013). We believe that the reason is two-fold: (a) k-means finds a local maximum during clustering; (b) we do hard clustering instead of soft clustering.", "startOffset": 131, "endOffset": 151}, {"referenceID": 13, "context": "It has been shown that a diverse set of predictions can be used to help improve decoder accuracy for various problems in NLP (Henderson and Brill, 1999).", "startOffset": 125, "endOffset": 152}, {"referenceID": 33, "context": "Our use of dropout noise here is inspired by \u201cdropout\u201d as is used in neural network training, where various connections between units in the neural network are dropped during training in order to avoid overfitting of these units to the data (Srivastava et al., 2014).", "startOffset": 241, "endOffset": 266}, {"referenceID": 37, "context": "The three schemes we described were also used by Wang et al. (2013) to train log-linear models.", "startOffset": 49, "endOffset": 68}, {"referenceID": 28, "context": "the one of Petrov (2010), who builds a committee of latent-variable PCFGs in order to improve a natural language parser.", "startOffset": 11, "endOffset": 25}, {"referenceID": 6, "context": "We also use these perturbation schemes to create multiple models for the algorithm of Cohen et al. (2012). The dropout scheme stays the same, but for the Gaussian noising schemes, we follow a slightly different procedure.", "startOffset": 86, "endOffset": 106}, {"referenceID": 6, "context": "We also use these perturbation schemes to create multiple models for the algorithm of Cohen et al. (2012). The dropout scheme stays the same, but for the Gaussian noising schemes, we follow a slightly different procedure. After noising the projections of the inside and outside feature functions we get from the SVD step, we use these projected noised features as a new set of inside and outside feature functions, and re-run the spectral algorithm of Cohen et al. (2012) on them.", "startOffset": 86, "endOffset": 472}, {"referenceID": 12, "context": "This means we follow Goodman (1996), who uses dynamic programming to compute the tree that maximizes the sum of all marginals of all nonterminals in the output tree.", "startOffset": 21, "endOffset": 36}, {"referenceID": 3, "context": "We use the reranker of Charniak and Johnson (2005).3", "startOffset": 23, "endOffset": 51}, {"referenceID": 7, "context": "53 (Cohen et al., 2013) 86.", "startOffset": 3, "endOffset": 23}, {"referenceID": 7, "context": "47 (Cohen et al., 2013)", "startOffset": 3, "endOffset": 23}, {"referenceID": 6, "context": "The \u201cNo noise\u201d baseline for the spectral algorithm is taken from Cohen et al. (2013). The best figure in each algorithm block is in boldface.", "startOffset": 65, "endOffset": 85}, {"referenceID": 21, "context": "More specifically, we use the Penn WSJ treebank (Marcus et al., 1993) for our experiments, with sections 2\u201321 as the training data, and section 22 used as the development data.", "startOffset": 48, "endOffset": 69}, {"referenceID": 22, "context": "We tag all datasets using Turbo Tagger (Martins et al., 2010), trained on sections 2\u201321.", "startOffset": 39, "endOffset": 61}, {"referenceID": 6, "context": "For the rest of our experiments, both with the spectral algorithm of Cohen et al. (2012) and the clustering algorithm presented in this paper, we use m = 24.", "startOffset": 69, "endOffset": 89}, {"referenceID": 3, "context": "We found out that these oracle scores are comparable to the one Charniak and Johnson (2005) report.", "startOffset": 64, "endOffset": 92}, {"referenceID": 6, "context": "We also tested our oracle results, comparing the spectral algorithm of Cohen et al. (2013) to the clustering algorithm.", "startOffset": 71, "endOffset": 91}, {"referenceID": 6, "context": "It seems that dropout noise for the spectral algorithm acts as a regularizer, similarly to the backoff smoothing techniques that are used in Cohen et al. (2013). This is evident from the two spectral algorithm blocks in Table 1, where dropout noise does not substantially improve the smoothed spectral model (Cohen et al.", "startOffset": 141, "endOffset": 161}, {"referenceID": 29, "context": "For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 11, "context": "For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 11, "context": "For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.", "startOffset": 37, "endOffset": 86}, {"referenceID": 26, "context": "ing parsing recombination; Shindo et al. (2012) report an accuracy of 92.", "startOffset": 27, "endOffset": 48}, {"referenceID": 24, "context": "4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.", "startOffset": 49, "endOffset": 63}, {"referenceID": 2, "context": "0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.", "startOffset": 29, "endOffset": 57}, {"referenceID": 2, "context": "4 using a discriminative reranking model; Carreras et al. (2008) report 91.", "startOffset": 42, "endOffset": 65}, {"referenceID": 2, "context": "4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.", "startOffset": 42, "endOffset": 161}, {"referenceID": 2, "context": "4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.", "startOffset": 42, "endOffset": 207}, {"referenceID": 32, "context": "For the German experiments, we used the NEGRA corpus (Skut et al., 1997).", "startOffset": 53, "endOffset": 72}, {"referenceID": 28, "context": "We use the same setup as in Petrov (2010), and use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set.", "startOffset": 28, "endOffset": 42}, {"referenceID": 3, "context": "For the reranking experiment, we had to modify the BLLIP parser (Charniak and Johnson, 2005) to use the head features from the German treebank.", "startOffset": 64, "endOffset": 92}, {"referenceID": 6, "context": "We compared our oracle results to those given by the spectral algorithm of Cohen et al. (2013). With 20 models for each type of noising scheme, all spectral models combined achieve an oracle accuracy of 83.", "startOffset": 75, "endOffset": 95}, {"referenceID": 28, "context": "For example, Petrov (2010) reports an accuracy of 84.", "startOffset": 13, "endOffset": 27}, {"referenceID": 25, "context": "uct of L-PCFGs; Petrov and Klein (2007) report an accuracy of 80.", "startOffset": 16, "endOffset": 40}, {"referenceID": 10, "context": "1 F1; and Dubey (2005) reports an accuracy of 76.", "startOffset": 10, "endOffset": 23}, {"referenceID": 36, "context": "There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010).", "startOffset": 124, "endOffset": 195}, {"referenceID": 15, "context": "There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010).", "startOffset": 124, "endOffset": 195}, {"referenceID": 24, "context": "There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010).", "startOffset": 124, "endOffset": 195}], "year": 2015, "abstractText": "We describe an approach to create a diverse set of predictions with spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38.", "creator": "TeX"}}}